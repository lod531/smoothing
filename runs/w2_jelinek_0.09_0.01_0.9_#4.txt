Sender: LSF System <lsfadmin@eu-g2-04>
Subject: Job 202625151: <w2_jelinek_0.09_0.01_0.9_#4> in cluster <euler> Exited

Job <w2_jelinek_0.09_0.01_0.9_#4> was submitted from host <eu-login-14> by user <andriusb> in cluster <euler> at Mon Jan 31 08:52:12 2022
Job was executed on host(s) <eu-g2-04>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Mon Jan 31 08:52:43 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Mon Jan 31 08:52:43 2022
Terminated at Tue Feb  1 04:53:01 2022
Results reported at Tue Feb  1 04:53:01 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-2-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.09, 0.01, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 16 --save-interval 40 --max-update 50000
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   72957.57 sec.
    Max Memory :                                 5069 MB
    Average Memory :                             2696.59 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               14931.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   72017 sec.
    Turnaround time :                            72049 sec.

The output (if any) follows:

2022-01-31 08:52:48 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 40, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.5, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-2-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.09, 0.01, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.5, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-01-31 08:52:48 | INFO | fairseq.tasks.language_modeling | dictionary: 76624 types
2022-01-31 08:52:49 | INFO | fairseq.data.data_utils | loaded 36,718 examples from: data-bin/wikitext-2-raw-full/train
Calculating frequency stats:
  0%|          | 0/36718 [00:00<?, ?it/s]  4%|▍         | 1419/36718 [00:00<00:02, 14176.76it/s]  8%|▊         | 2837/36718 [00:00<00:02, 13568.78it/s] 12%|█▏        | 4410/36718 [00:00<00:02, 14523.47it/s] 16%|█▋        | 6023/36718 [00:00<00:02, 15143.40it/s] 21%|██        | 7541/36718 [00:00<00:02, 14323.72it/s] 24%|██▍       | 8982/36718 [00:00<00:01, 14044.62it/s] 28%|██▊       | 10399/36718 [00:00<00:01, 14079.91it/s] 32%|███▏      | 11811/36718 [00:00<00:01, 13824.59it/s] 36%|███▋      | 13311/36718 [00:00<00:01, 14166.29it/s] 40%|████      | 14753/36718 [00:01<00:01, 14242.20it/s] 44%|████▍     | 16180/36718 [00:01<00:01, 14060.03it/s] 48%|████▊     | 17591/36718 [00:01<00:01, 14069.40it/s] 52%|█████▏    | 19096/36718 [00:01<00:01, 14351.13it/s] 56%|█████▌    | 20533/36718 [00:01<00:01, 14304.94it/s] 60%|█████▉    | 21965/36718 [00:01<00:01, 14065.77it/s] 64%|██████▍   | 23443/36718 [00:01<00:00, 14273.09it/s] 68%|██████▊   | 25123/36718 [00:01<00:00, 15021.94it/s] 73%|███████▎  | 26628/36718 [00:01<00:00, 14570.95it/s] 77%|███████▋  | 28090/36718 [00:01<00:00, 14100.45it/s] 80%|████████  | 29506/36718 [00:02<00:00, 14096.51it/s] 84%|████████▍ | 30920/36718 [00:02<00:00, 13962.12it/s] 88%|████████▊ | 32319/36718 [00:02<00:00, 13663.97it/s] 92%|█████████▏| 33688/36718 [00:02<00:00, 13617.50it/s] 96%|█████████▌| 35152/36718 [00:02<00:00, 13911.50it/s]100%|█████████▉| 36595/36718 [00:02<00:00, 14060.87it/s]100%|██████████| 36718/36718 [00:02<00:00, 14146.55it/s]

gathering stats for n=1
  0%|          | 0/36718 [00:00<?, ?it/s]  7%|▋         | 2738/36718 [00:00<00:01, 27371.87it/s] 16%|█▌        | 5835/36718 [00:00<00:01, 29473.91it/s] 24%|██▍       | 8783/36718 [00:00<00:01, 27621.11it/s] 31%|███▏      | 11558/36718 [00:00<00:00, 27549.85it/s] 39%|███▉      | 14392/36718 [00:00<00:00, 27826.10it/s] 47%|████▋     | 17181/36718 [00:00<00:00, 27475.68it/s] 55%|█████▍    | 20099/36718 [00:00<00:00, 28020.07it/s] 62%|██████▏   | 22905/36718 [00:00<00:00, 27849.46it/s] 71%|███████   | 25938/36718 [00:00<00:00, 28612.84it/s] 78%|███████▊  | 28803/36718 [00:01<00:00, 28238.94it/s] 86%|████████▌ | 31630/36718 [00:01<00:00, 27627.46it/s] 94%|█████████▎| 34397/36718 [00:01<00:00, 27175.33it/s]100%|██████████| 36718/36718 [00:01<00:00, 27759.17it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 190.56it/s]2022-01-31 08:52:58 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(76624, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=76624, bias=False)
  )
)
2022-01-31 08:52:58 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-01-31 08:52:58 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-01-31 08:52:58 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-01-31 08:52:58 | INFO | fairseq_cli.train | num. shared model params: 58,145,792 (num. trained: 58,145,792)
2022-01-31 08:52:58 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-01-31 08:52:58 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-2-raw-full/valid
2022-01-31 08:52:58 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-01-31 08:52:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-31 08:52:58 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-01-31 08:52:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-31 08:52:58 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-01-31 08:52:58 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-01-31 08:52:58 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint_last.pt
2022-01-31 08:52:58 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint_last.pt
2022-01-31 08:52:58 | INFO | fairseq.trainer | loading train data for epoch 1
2022-01-31 08:52:58 | INFO | fairseq.data.data_utils | loaded 36,718 examples from: data-bin/wikitext-2-raw-full/train
2022-01-31 08:52:58 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-01-31 08:52:58 | INFO | fairseq.trainer | begin training epoch 1
2022-01-31 08:52:58 | INFO | fairseq_cli.train | Start iterating over samples

2022-01-31 08:58:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
/cluster/home/andriusb/fq/fairseq/fairseq/utils.py:372: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-01-31 08:58:55 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.679 | ppl 26227.6 | wps 7857.5 | wpb 2034.1 | bsz 4 | num_updates 64
2022-01-31 08:58:55 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-01-31 08:58:55 | INFO | train | epoch 001 | loss 16.131 | ppl 71746.6 | wps 5875.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 64 | lr 8.0984e-06 | gnorm 3.273 | train_wall 327 | gb_free 6.1 | wall 357
KL Stats: Epoch 1 Divergences: Uniform: 0.5172925508133422 Unigram: 3.685318486297919
2022-01-31 08:58:55 | INFO | fairseq.trainer | begin training epoch 2
2022-01-31 08:58:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:02:00 | INFO | train_inner | epoch 002:     36 / 64 loss=15.584, ppl=49132, wps=6047.7, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=100, lr=1.25975e-05, gnorm=2.682, train_wall=512, gb_free=6.1, wall=542
2022-01-31 09:04:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:04:50 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.667 | ppl 13005.3 | wps 7869.8 | wpb 2034.1 | bsz 4 | num_updates 128
2022-01-31 09:04:50 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-01-31 09:04:50 | INFO | train | epoch 002 | loss 14.402 | ppl 21650.6 | wps 5882.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 128 | lr 1.60968e-05 | gnorm 1.512 | train_wall 326 | gb_free 6.1 | wall 712
KL Stats: Epoch 2 Divergences: Uniform: 0.5355066972682596 Unigram: 2.4150737509746527
2022-01-31 09:04:50 | INFO | fairseq.trainer | begin training epoch 3
2022-01-31 09:04:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:10:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:10:44 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 12.836 | ppl 7313.14 | wps 7841.9 | wpb 2034.1 | bsz 4 | num_updates 192
2022-01-31 09:10:44 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-01-31 09:10:44 | INFO | train | epoch 003 | loss 13.489 | ppl 11496.4 | wps 5898.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 192 | lr 2.40952e-05 | gnorm 1.219 | train_wall 325 | gb_free 6.1 | wall 1066
KL Stats: Epoch 3 Divergences: Uniform: 0.5218326599133369 Unigram: 1.7313023709848727
2022-01-31 09:10:44 | INFO | fairseq.trainer | begin training epoch 4
2022-01-31 09:10:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:11:25 | INFO | train_inner | epoch 004:      8 / 64 loss=13.623, ppl=12620.1, wps=5768.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=200, lr=2.5095e-05, gnorm=1.248, train_wall=508, gb_free=6.1, wall=1107
2022-01-31 09:16:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:16:37 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.98 | ppl 4038.3 | wps 7975.8 | wpb 2034.1 | bsz 4 | num_updates 256
2022-01-31 09:16:37 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-01-31 09:16:37 | INFO | train | epoch 004 | loss 12.532 | ppl 5920.87 | wps 5909.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 256 | lr 3.20936e-05 | gnorm 0.972 | train_wall 325 | gb_free 6.1 | wall 1419
KL Stats: Epoch 4 Divergences: Uniform: 0.6079998372450861 Unigram: 1.1139984562616057
2022-01-31 09:16:37 | INFO | fairseq.trainer | begin training epoch 5
2022-01-31 09:16:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:20:23 | INFO | train_inner | epoch 005:     44 / 64 loss=12.178, ppl=4634.5, wps=6076, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=300, lr=3.75925e-05, gnorm=0.853, train_wall=509, gb_free=6.1, wall=1645
2022-01-31 09:22:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:22:31 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.455 | ppl 2807.14 | wps 7867 | wpb 2034.1 | bsz 4 | num_updates 320
2022-01-31 09:22:31 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-01-31 09:22:31 | INFO | train | epoch 005 | loss 11.725 | ppl 3385.99 | wps 5895.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 320 | lr 4.0092e-05 | gnorm 0.694 | train_wall 326 | gb_free 6.1 | wall 1774
KL Stats: Epoch 5 Divergences: Uniform: 0.8527021028177504 Unigram: 0.6570025670544285
2022-01-31 09:22:31 | INFO | fairseq.trainer | begin training epoch 6
2022-01-31 09:22:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:27:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:28:25 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.208 | ppl 2365.91 | wps 7865 | wpb 2034.1 | bsz 4 | num_updates 384
2022-01-31 09:28:25 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-01-31 09:28:25 | INFO | train | epoch 006 | loss 11.288 | ppl 2500.35 | wps 5903.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 384 | lr 4.80904e-05 | gnorm 0.587 | train_wall 325 | gb_free 6.1 | wall 2127
KL Stats: Epoch 6 Divergences: Uniform: 1.158623902415679 Unigram: 0.4516596964502991
2022-01-31 09:28:25 | INFO | fairseq.trainer | begin training epoch 7
2022-01-31 09:28:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:29:47 | INFO | train_inner | epoch 007:     16 / 64 loss=11.311, ppl=2540.2, wps=5771.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=400, lr=5.009e-05, gnorm=0.586, train_wall=508, gb_free=6.1, wall=2209
2022-01-31 09:33:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:34:20 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.065 | ppl 2142.7 | wps 7852.4 | wpb 2034.1 | bsz 4 | num_updates 448
2022-01-31 09:34:20 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-01-31 09:34:20 | INFO | train | epoch 007 | loss 11.085 | ppl 2172.54 | wps 5880 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 448 | lr 5.60888e-05 | gnorm 0.524 | train_wall 326 | gb_free 6.1 | wall 2483
KL Stats: Epoch 7 Divergences: Uniform: 1.392642527964186 Unigram: 0.45654338669681677
2022-01-31 09:34:20 | INFO | fairseq.trainer | begin training epoch 8
2022-01-31 09:34:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:38:47 | INFO | train_inner | epoch 008:     52 / 64 loss=11.024, ppl=2082.14, wps=6060.6, ups=0.19, wpb=32686.1, bsz=63.8, num_updates=500, lr=6.25875e-05, gnorm=0.517, train_wall=510, gb_free=6.1, wall=2749
2022-01-31 09:39:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:40:15 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.962 | ppl 1994.18 | wps 7872.6 | wpb 2034.1 | bsz 4 | num_updates 512
2022-01-31 09:40:15 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-01-31 09:40:15 | INFO | train | epoch 008 | loss 10.972 | ppl 2009.09 | wps 5895.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 512 | lr 6.40872e-05 | gnorm 0.512 | train_wall 326 | gb_free 6.1 | wall 2837
KL Stats: Epoch 8 Divergences: Uniform: 1.5156018024966074 Unigram: 0.5258306782085844
2022-01-31 09:40:15 | INFO | fairseq.trainer | begin training epoch 9
2022-01-31 09:40:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:45:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:46:10 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.846 | ppl 1841.01 | wps 7897.2 | wpb 2034.1 | bsz 4 | num_updates 576
2022-01-31 09:46:10 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-01-31 09:46:10 | INFO | train | epoch 009 | loss 10.868 | ppl 1868.46 | wps 5884.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 576 | lr 7.20856e-05 | gnorm 0.486 | train_wall 326 | gb_free 6.1 | wall 3192
KL Stats: Epoch 9 Divergences: Uniform: 1.5656219075321887 Unigram: 0.6215845768683442
2022-01-31 09:46:10 | INFO | fairseq.trainer | begin training epoch 10
2022-01-31 09:46:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:48:13 | INFO | train_inner | epoch 010:     24 / 64 loss=10.859, ppl=1856.76, wps=5756.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=600, lr=7.5085e-05, gnorm=0.488, train_wall=509, gb_free=6.1, wall=3315
2022-01-31 09:51:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:52:05 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.743 | ppl 1713.51 | wps 7841 | wpb 2034.1 | bsz 4 | num_updates 640
2022-01-31 09:52:05 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-01-31 09:52:05 | INFO | train | epoch 010 | loss 10.759 | ppl 1733.43 | wps 5880.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 640 | lr 8.0084e-05 | gnorm 0.484 | train_wall 326 | gb_free 6.1 | wall 3547
KL Stats: Epoch 10 Divergences: Uniform: 1.5928129414902545 Unigram: 0.726575061791323
2022-01-31 09:52:05 | INFO | fairseq.trainer | begin training epoch 11
2022-01-31 09:52:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:57:11 | INFO | train_inner | epoch 011:     60 / 64 loss=10.684, ppl=1645.03, wps=6069.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=700, lr=8.75825e-05, gnorm=0.494, train_wall=509, gb_free=6.1, wall=3853
2022-01-31 09:57:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:57:58 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.636 | ppl 1591.08 | wps 7839.3 | wpb 2034.1 | bsz 4 | num_updates 704
2022-01-31 09:57:58 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-01-31 09:57:58 | INFO | train | epoch 011 | loss 10.644 | ppl 1600.58 | wps 5907.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 704 | lr 8.80824e-05 | gnorm 0.499 | train_wall 325 | gb_free 6.1 | wall 3900
KL Stats: Epoch 11 Divergences: Uniform: 1.6123376178889608 Unigram: 0.8304129090247766
2022-01-31 09:57:58 | INFO | fairseq.trainer | begin training epoch 12
2022-01-31 09:57:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:03:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:03:53 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.529 | ppl 1477.1 | wps 7858.4 | wpb 2034.1 | bsz 4 | num_updates 768
2022-01-31 10:03:53 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-01-31 10:03:53 | INFO | train | epoch 012 | loss 10.528 | ppl 1476.5 | wps 5897.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 768 | lr 9.60808e-05 | gnorm 0.478 | train_wall 325 | gb_free 6.1 | wall 4255
KL Stats: Epoch 12 Divergences: Uniform: 1.6237976215068786 Unigram: 0.931897225138668
2022-01-31 10:03:53 | INFO | fairseq.trainer | begin training epoch 13
2022-01-31 10:03:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:06:36 | INFO | train_inner | epoch 013:     32 / 64 loss=10.504, ppl=1452.23, wps=5768.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=800, lr=0.00010008, gnorm=0.492, train_wall=508, gb_free=6.1, wall=4419
2022-01-31 10:09:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:09:47 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 10.437 | ppl 1386.69 | wps 7841.2 | wpb 2034.1 | bsz 4 | num_updates 832
2022-01-31 10:09:47 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-01-31 10:09:47 | INFO | train | epoch 013 | loss 10.414 | ppl 1364.48 | wps 5895.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 832 | lr 0.000104079 | gnorm 0.518 | train_wall 326 | gb_free 6.1 | wall 4609
KL Stats: Epoch 13 Divergences: Uniform: 1.6509153539214831 Unigram: 1.0206337089915303
2022-01-31 10:09:47 | INFO | fairseq.trainer | begin training epoch 14
2022-01-31 10:09:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:15:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:15:41 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 10.344 | ppl 1300.03 | wps 7877.2 | wpb 2034.1 | bsz 4 | num_updates 896
2022-01-31 10:15:41 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-01-31 10:15:41 | INFO | train | epoch 014 | loss 10.304 | ppl 1263.76 | wps 5889.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 896 | lr 0.000112078 | gnorm 0.554 | train_wall 326 | gb_free 6.1 | wall 4964
KL Stats: Epoch 14 Divergences: Uniform: 1.677293718416745 Unigram: 1.103458534226754
2022-01-31 10:15:41 | INFO | fairseq.trainer | begin training epoch 15
2022-01-31 10:15:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:16:02 | INFO | train_inner | epoch 015:      4 / 64 loss=10.326, ppl=1283.69, wps=5763, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=900, lr=0.000112578, gnorm=0.536, train_wall=509, gb_free=6.1, wall=4984
2022-01-31 10:21:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:21:36 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 10.275 | ppl 1239.39 | wps 7817.7 | wpb 2034.1 | bsz 4 | num_updates 960
2022-01-31 10:21:36 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-01-31 10:21:36 | INFO | train | epoch 015 | loss 10.192 | ppl 1169.65 | wps 5883.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 960 | lr 0.000120076 | gnorm 0.532 | train_wall 326 | gb_free 6.1 | wall 5319
KL Stats: Epoch 15 Divergences: Uniform: 1.7021763677656307 Unigram: 1.1782900924106943
2022-01-31 10:21:36 | INFO | fairseq.trainer | begin training epoch 16
2022-01-31 10:21:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:25:01 | INFO | train_inner | epoch 016:     40 / 64 loss=10.151, ppl=1136.82, wps=6060.1, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=1000, lr=0.000125075, gnorm=0.553, train_wall=510, gb_free=6.1, wall=5524
2022-01-31 10:27:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:27:32 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 10.196 | ppl 1172.82 | wps 7857.4 | wpb 2034.1 | bsz 4 | num_updates 1024
2022-01-31 10:27:32 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-01-31 10:27:32 | INFO | train | epoch 016 | loss 10.086 | ppl 1086.63 | wps 5877.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1024 | lr 0.000128074 | gnorm 0.553 | train_wall 327 | gb_free 6.1 | wall 5674
KL Stats: Epoch 16 Divergences: Uniform: 1.7317401962284507 Unigram: 1.2510396704586992
2022-01-31 10:27:32 | INFO | fairseq.trainer | begin training epoch 17
2022-01-31 10:27:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:32:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:33:26 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.102 | ppl 1099.16 | wps 7855 | wpb 2034.1 | bsz 4 | num_updates 1088
2022-01-31 10:33:26 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-01-31 10:33:26 | INFO | train | epoch 017 | loss 9.979 | ppl 1009.13 | wps 5894.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1088 | lr 0.000136073 | gnorm 0.544 | train_wall 326 | gb_free 6.1 | wall 6028
KL Stats: Epoch 17 Divergences: Uniform: 1.7665163546840297 Unigram: 1.3139824541843683
2022-01-31 10:33:26 | INFO | fairseq.trainer | begin training epoch 18
2022-01-31 10:33:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:34:27 | INFO | train_inner | epoch 018:     12 / 64 loss=9.993, ppl=1019.04, wps=5762.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=1100, lr=0.000137573, gnorm=0.548, train_wall=509, gb_free=6.1, wall=6089
2022-01-31 10:38:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:39:19 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.036 | ppl 1050.04 | wps 7837.8 | wpb 2034.1 | bsz 4 | num_updates 1152
2022-01-31 10:39:19 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-01-31 10:39:19 | INFO | train | epoch 018 | loss 9.878 | ppl 941.25 | wps 5914.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1152 | lr 0.000144071 | gnorm 0.569 | train_wall 324 | gb_free 6.1 | wall 6381
KL Stats: Epoch 18 Divergences: Uniform: 1.8017846695645565 Unigram: 1.375909205706343
2022-01-31 10:39:19 | INFO | fairseq.trainer | begin training epoch 19
2022-01-31 10:39:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:43:25 | INFO | train_inner | epoch 019:     48 / 64 loss=9.829, ppl=909.47, wps=6073.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=1200, lr=0.00015007, gnorm=0.541, train_wall=509, gb_free=6.1, wall=6627
2022-01-31 10:44:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:45:13 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.965 | ppl 999.23 | wps 7853.5 | wpb 2034.1 | bsz 4 | num_updates 1216
2022-01-31 10:45:13 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-01-31 10:45:13 | INFO | train | epoch 019 | loss 9.775 | ppl 876.26 | wps 5897.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1216 | lr 0.00015207 | gnorm 0.528 | train_wall 325 | gb_free 6.1 | wall 6736
KL Stats: Epoch 19 Divergences: Uniform: 1.8337019048581586 Unigram: 1.4378193049657308
2022-01-31 10:45:13 | INFO | fairseq.trainer | begin training epoch 20
2022-01-31 10:45:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:50:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:51:09 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.879 | ppl 941.94 | wps 7845.5 | wpb 2034.1 | bsz 4 | num_updates 1280
2022-01-31 10:51:09 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-01-31 10:51:09 | INFO | train | epoch 020 | loss 9.679 | ppl 819.72 | wps 5879 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1280 | lr 0.000160068 | gnorm 0.554 | train_wall 327 | gb_free 6.1 | wall 7091
KL Stats: Epoch 20 Divergences: Uniform: 1.8650812480484598 Unigram: 1.4935647577801334
2022-01-31 10:51:09 | INFO | fairseq.trainer | begin training epoch 21
2022-01-31 10:51:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:52:51 | INFO | train_inner | epoch 021:     20 / 64 loss=9.674, ppl=816.96, wps=5760.6, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=1300, lr=0.000162568, gnorm=0.549, train_wall=509, gb_free=6.1, wall=7193
2022-01-31 10:56:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:57:03 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.836 | ppl 914.09 | wps 7863.5 | wpb 2034.1 | bsz 4 | num_updates 1344
2022-01-31 10:57:03 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-01-31 10:57:03 | INFO | train | epoch 021 | loss 9.585 | ppl 767.88 | wps 5901.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1344 | lr 0.000168066 | gnorm 0.522 | train_wall 325 | gb_free 6.1 | wall 7445
KL Stats: Epoch 21 Divergences: Uniform: 1.8959466477684905 Unigram: 1.5474696312805731
2022-01-31 10:57:03 | INFO | fairseq.trainer | begin training epoch 22
2022-01-31 10:57:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:01:51 | INFO | train_inner | epoch 022:     56 / 64 loss=9.532, ppl=740.38, wps=6049.4, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=1400, lr=0.000175065, gnorm=0.529, train_wall=511, gb_free=6.1, wall=7733
2022-01-31 11:02:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:02:59 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.772 | ppl 874.3 | wps 7833.9 | wpb 2034.1 | bsz 4 | num_updates 1408
2022-01-31 11:02:59 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-01-31 11:02:59 | INFO | train | epoch 022 | loss 9.496 | ppl 721.95 | wps 5861.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1408 | lr 0.000176065 | gnorm 0.542 | train_wall 328 | gb_free 6.1 | wall 7801
KL Stats: Epoch 22 Divergences: Uniform: 1.9216239144417957 Unigram: 1.599239671066291
2022-01-31 11:02:59 | INFO | fairseq.trainer | begin training epoch 23
2022-01-31 11:02:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:08:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:08:54 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.713 | ppl 839.35 | wps 7791.2 | wpb 2034.1 | bsz 4 | num_updates 1472
2022-01-31 11:08:54 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-01-31 11:08:54 | INFO | train | epoch 023 | loss 9.409 | ppl 679.96 | wps 5878.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1472 | lr 0.000184063 | gnorm 0.515 | train_wall 326 | gb_free 6.1 | wall 8156
KL Stats: Epoch 23 Divergences: Uniform: 1.949994775913474 Unigram: 1.646232332910266
2022-01-31 11:08:54 | INFO | fairseq.trainer | begin training epoch 24
2022-01-31 11:08:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:11:18 | INFO | train_inner | epoch 024:     28 / 64 loss=9.394, ppl=672.87, wps=5757, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=1500, lr=0.000187563, gnorm=0.526, train_wall=509, gb_free=6.1, wall=8300
2022-01-31 11:14:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:14:48 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.654 | ppl 805.76 | wps 7849.8 | wpb 2034.1 | bsz 4 | num_updates 1536
2022-01-31 11:14:48 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-01-31 11:14:48 | INFO | train | epoch 024 | loss 9.326 | ppl 641.69 | wps 5898.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1536 | lr 0.000192062 | gnorm 0.546 | train_wall 325 | gb_free 6.1 | wall 8510
KL Stats: Epoch 24 Divergences: Uniform: 1.9727753792875775 Unigram: 1.6889862349654534
2022-01-31 11:14:48 | INFO | fairseq.trainer | begin training epoch 25
2022-01-31 11:14:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:20:15 | INFO | train_inner | epoch 025:     64 / 64 loss=9.272, ppl=618.13, wps=6065, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=1600, lr=0.00020006, gnorm=0.53, train_wall=508, gb_free=6.1, wall=8837
2022-01-31 11:20:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:20:43 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.624 | ppl 789.3 | wps 7849.7 | wpb 2034.1 | bsz 4 | num_updates 1600
2022-01-31 11:20:43 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-01-31 11:20:43 | INFO | train | epoch 025 | loss 9.244 | ppl 606.39 | wps 5890.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1600 | lr 0.00020006 | gnorm 0.521 | train_wall 326 | gb_free 6.1 | wall 8865
KL Stats: Epoch 25 Divergences: Uniform: 2.0026716159493776 Unigram: 1.7330858693671025
2022-01-31 11:20:43 | INFO | fairseq.trainer | begin training epoch 26
2022-01-31 11:20:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:26:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:26:38 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.576 | ppl 763.5 | wps 7955.5 | wpb 2034.1 | bsz 4 | num_updates 1664
2022-01-31 11:26:38 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-01-31 11:26:38 | INFO | train | epoch 026 | loss 9.163 | ppl 573.26 | wps 5875 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1664 | lr 0.000208058 | gnorm 0.534 | train_wall 327 | gb_free 6.1 | wall 9221
KL Stats: Epoch 26 Divergences: Uniform: 2.0169362838777287 Unigram: 1.7723366416299602
2022-01-31 11:26:38 | INFO | fairseq.trainer | begin training epoch 27
2022-01-31 11:26:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:29:42 | INFO | train_inner | epoch 027:     36 / 64 loss=9.135, ppl=562.15, wps=5761.7, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=1700, lr=0.000212558, gnorm=0.523, train_wall=510, gb_free=6.1, wall=9404
2022-01-31 11:32:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:32:32 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.543 | ppl 745.9 | wps 7851.9 | wpb 2034.1 | bsz 4 | num_updates 1728
2022-01-31 11:32:32 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-01-31 11:32:32 | INFO | train | epoch 027 | loss 9.082 | ppl 542.07 | wps 5908.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1728 | lr 0.000216057 | gnorm 0.514 | train_wall 325 | gb_free 6.1 | wall 9574
KL Stats: Epoch 27 Divergences: Uniform: 2.0451686198162173 Unigram: 1.8091918034813852
2022-01-31 11:32:32 | INFO | fairseq.trainer | begin training epoch 28
2022-01-31 11:32:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:37:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:38:26 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.514 | ppl 731.27 | wps 7814.3 | wpb 2034.1 | bsz 4 | num_updates 1792
2022-01-31 11:38:26 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-01-31 11:38:26 | INFO | train | epoch 028 | loss 9.005 | ppl 513.67 | wps 5904.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1792 | lr 0.000224055 | gnorm 0.522 | train_wall 325 | gb_free 6.1 | wall 9928
KL Stats: Epoch 28 Divergences: Uniform: 2.0734511726547016 Unigram: 1.846561328919263
2022-01-31 11:38:26 | INFO | fairseq.trainer | begin training epoch 29
2022-01-31 11:38:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:39:07 | INFO | train_inner | epoch 029:      8 / 64 loss=9.02, ppl=519.29, wps=5775.4, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=1800, lr=0.000225055, gnorm=0.527, train_wall=507, gb_free=6.1, wall=9969
2022-01-31 11:43:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:44:21 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.483 | ppl 715.84 | wps 7826.3 | wpb 2034.1 | bsz 4 | num_updates 1856
2022-01-31 11:44:21 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-01-31 11:44:21 | INFO | train | epoch 029 | loss 8.926 | ppl 486.44 | wps 5885.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1856 | lr 0.000232054 | gnorm 0.529 | train_wall 326 | gb_free 6.1 | wall 10283
KL Stats: Epoch 29 Divergences: Uniform: 2.0946086788516123 Unigram: 1.882260890309672
2022-01-31 11:44:21 | INFO | fairseq.trainer | begin training epoch 30
2022-01-31 11:44:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:48:06 | INFO | train_inner | epoch 030:     44 / 64 loss=8.893, ppl=475.35, wps=6060.3, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=1900, lr=0.000237553, gnorm=0.517, train_wall=510, gb_free=6.1, wall=10508
2022-01-31 11:49:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:50:15 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 9.462 | ppl 705.19 | wps 7841 | wpb 2034.1 | bsz 4 | num_updates 1920
2022-01-31 11:50:15 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-01-31 11:50:15 | INFO | train | epoch 030 | loss 8.848 | ppl 460.8 | wps 5899.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1920 | lr 0.000240052 | gnorm 0.516 | train_wall 325 | gb_free 6.1 | wall 10637
KL Stats: Epoch 30 Divergences: Uniform: 2.1129783086653977 Unigram: 1.9192933076442849
2022-01-31 11:50:15 | INFO | fairseq.trainer | begin training epoch 31
2022-01-31 11:50:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:55:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:56:09 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.411 | ppl 680.78 | wps 7972.6 | wpb 2034.1 | bsz 4 | num_updates 1984
2022-01-31 11:56:09 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-01-31 11:56:09 | INFO | train | epoch 031 | loss 8.769 | ppl 436.27 | wps 5888.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1984 | lr 0.00024805 | gnorm 0.494 | train_wall 326 | gb_free 6.1 | wall 10991
KL Stats: Epoch 31 Divergences: Uniform: 2.1328234181528987 Unigram: 1.951347608752532
2022-01-31 11:56:09 | INFO | fairseq.trainer | begin training epoch 32
2022-01-31 11:56:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:57:31 | INFO | train_inner | epoch 032:     16 / 64 loss=8.77, ppl=436.67, wps=5766.5, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=2000, lr=0.00025005, gnorm=0.502, train_wall=509, gb_free=6.1, wall=11073
2022-01-31 12:01:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:02:04 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 9.375 | ppl 663.8 | wps 7870.2 | wpb 2034.1 | bsz 4 | num_updates 2048
2022-01-31 12:02:04 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-01-31 12:02:04 | INFO | train | epoch 032 | loss 8.695 | ppl 414.53 | wps 5880.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2048 | lr 0.000256049 | gnorm 0.507 | train_wall 327 | gb_free 6.1 | wall 11347
KL Stats: Epoch 32 Divergences: Uniform: 2.1587278027032886 Unigram: 1.985949289624129
2022-01-31 12:02:04 | INFO | fairseq.trainer | begin training epoch 33
2022-01-31 12:02:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:06:31 | INFO | train_inner | epoch 033:     52 / 64 loss=8.658, ppl=404.03, wps=6052.9, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=2100, lr=0.000262548, gnorm=0.507, train_wall=511, gb_free=6.1, wall=11613
2022-01-31 12:07:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:07:59 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 9.362 | ppl 658.15 | wps 7819.9 | wpb 2034.1 | bsz 4 | num_updates 2112
2022-01-31 12:07:59 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-01-31 12:07:59 | INFO | train | epoch 033 | loss 8.621 | ppl 393.67 | wps 5893.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2112 | lr 0.000264047 | gnorm 0.505 | train_wall 326 | gb_free 6.1 | wall 11701
KL Stats: Epoch 33 Divergences: Uniform: 2.185260094275646 Unigram: 2.0201879666214566
2022-01-31 12:07:59 | INFO | fairseq.trainer | begin training epoch 34
2022-01-31 12:07:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:13:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:13:54 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 9.346 | ppl 650.72 | wps 7836.2 | wpb 2034.1 | bsz 4 | num_updates 2176
2022-01-31 12:13:54 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-01-31 12:13:54 | INFO | train | epoch 034 | loss 8.545 | ppl 373.54 | wps 5882.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2176 | lr 0.000272046 | gnorm 0.507 | train_wall 326 | gb_free 6.1 | wall 12056
KL Stats: Epoch 34 Divergences: Uniform: 2.2078339469416957 Unigram: 2.0538029326791505
2022-01-31 12:13:54 | INFO | fairseq.trainer | begin training epoch 35
2022-01-31 12:13:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:15:57 | INFO | train_inner | epoch 035:     24 / 64 loss=8.533, ppl=370.29, wps=5764.9, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=2200, lr=0.000275045, gnorm=0.507, train_wall=508, gb_free=6.1, wall=12179
2022-01-31 12:19:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:19:49 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 9.316 | ppl 637.46 | wps 7841.3 | wpb 2034.1 | bsz 4 | num_updates 2240
2022-01-31 12:19:49 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-01-31 12:19:49 | INFO | train | epoch 035 | loss 8.474 | ppl 355.45 | wps 5879.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2240 | lr 0.000280044 | gnorm 0.504 | train_wall 326 | gb_free 6.1 | wall 12411
KL Stats: Epoch 35 Divergences: Uniform: 2.229343265943193 Unigram: 2.0819022033249492
2022-01-31 12:19:49 | INFO | fairseq.trainer | begin training epoch 36
2022-01-31 12:19:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:24:57 | INFO | train_inner | epoch 036:     60 / 64 loss=8.429, ppl=344.68, wps=6047.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=2300, lr=0.000287543, gnorm=0.497, train_wall=511, gb_free=6.1, wall=12719
2022-01-31 12:25:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:25:44 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 9.286 | ppl 624.13 | wps 7999.5 | wpb 2034.1 | bsz 4 | num_updates 2304
2022-01-31 12:25:44 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-01-31 12:25:44 | INFO | train | epoch 036 | loss 8.399 | ppl 337.59 | wps 5893.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2304 | lr 0.000288042 | gnorm 0.494 | train_wall 326 | gb_free 6.1 | wall 12766
KL Stats: Epoch 36 Divergences: Uniform: 2.250290759235184 Unigram: 2.1151387002591924
2022-01-31 12:25:44 | INFO | fairseq.trainer | begin training epoch 37
2022-01-31 12:25:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:31:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:31:39 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.3 | ppl 630.17 | wps 7858.8 | wpb 2034.1 | bsz 4 | num_updates 2368
2022-01-31 12:31:39 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-01-31 12:31:39 | INFO | train | epoch 037 | loss 8.33 | ppl 321.79 | wps 5882.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2368 | lr 0.000296041 | gnorm 0.506 | train_wall 326 | gb_free 6.1 | wall 13121
KL Stats: Epoch 37 Divergences: Uniform: 2.271226338370874 Unigram: 2.1479327047977033
2022-01-31 12:31:39 | INFO | fairseq.trainer | begin training epoch 38
2022-01-31 12:31:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:34:22 | INFO | train_inner | epoch 038:     32 / 64 loss=8.309, ppl=317.05, wps=5767.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=2400, lr=0.00030004, gnorm=0.503, train_wall=509, gb_free=6.1, wall=13284
2022-01-31 12:37:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:37:33 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 9.282 | ppl 622.59 | wps 7861.7 | wpb 2034.1 | bsz 4 | num_updates 2432
2022-01-31 12:37:33 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-01-31 12:37:33 | INFO | train | epoch 038 | loss 8.262 | ppl 306.95 | wps 5887 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2432 | lr 0.000304039 | gnorm 0.507 | train_wall 326 | gb_free 6.1 | wall 13476
KL Stats: Epoch 38 Divergences: Uniform: 2.3018023846046796 Unigram: 2.1705607920289833
2022-01-31 12:37:33 | INFO | fairseq.trainer | begin training epoch 39
2022-01-31 12:37:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:43:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:43:28 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.266 | ppl 615.56 | wps 7866.7 | wpb 2034.1 | bsz 4 | num_updates 2496
2022-01-31 12:43:28 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-01-31 12:43:28 | INFO | train | epoch 039 | loss 8.193 | ppl 292.56 | wps 5896.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2496 | lr 0.000312038 | gnorm 0.494 | train_wall 326 | gb_free 6.1 | wall 13830
KL Stats: Epoch 39 Divergences: Uniform: 2.311048492981072 Unigram: 2.206612928570825
2022-01-31 12:43:28 | INFO | fairseq.trainer | begin training epoch 40
2022-01-31 12:43:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:43:48 | INFO | train_inner | epoch 040:      4 / 64 loss=8.215, ppl=297.05, wps=5758.1, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=2500, lr=0.000312538, gnorm=0.5, train_wall=509, gb_free=6.1, wall=13851
2022-01-31 12:48:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:49:23 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.246 | ppl 607.32 | wps 7855.3 | wpb 2034.1 | bsz 4 | num_updates 2560
2022-01-31 12:49:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 2560 updates
2022-01-31 12:49:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint40.pt
2022-01-31 12:49:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint40.pt
2022-01-31 12:49:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint40.pt (epoch 40 @ 2560 updates, score 9.246) (writing took 4.808969501405954 seconds)
2022-01-31 12:49:27 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-01-31 12:49:27 | INFO | train | epoch 040 | loss 8.125 | ppl 279.09 | wps 5807.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2560 | lr 0.000320036 | gnorm 0.498 | train_wall 326 | gb_free 6.1 | wall 14189
KL Stats: Epoch 40 Divergences: Uniform: 2.3392316898239276 Unigram: 2.233293056382214
2022-01-31 12:49:27 | INFO | fairseq.trainer | begin training epoch 41
2022-01-31 12:49:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:52:51 | INFO | train_inner | epoch 041:     40 / 64 loss=8.101, ppl=274.59, wps=6019, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=2600, lr=0.000325035, gnorm=0.497, train_wall=509, gb_free=6.1, wall=14393
2022-01-31 12:54:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:55:20 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.24 | ppl 604.83 | wps 8056.2 | wpb 2034.1 | bsz 4 | num_updates 2624 | best_loss 9.24
2022-01-31 12:55:20 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-01-31 12:55:20 | INFO | train | epoch 041 | loss 8.061 | ppl 267.01 | wps 5917.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2624 | lr 0.000328034 | gnorm 0.5 | train_wall 325 | gb_free 6.1 | wall 14542
KL Stats: Epoch 41 Divergences: Uniform: 2.3539291284267394 Unigram: 2.2586266373296975
2022-01-31 12:55:20 | INFO | fairseq.trainer | begin training epoch 42
2022-01-31 12:55:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:00:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:01:14 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.213 | ppl 593.27 | wps 7853.9 | wpb 2034.1 | bsz 4 | num_updates 2688 | best_loss 9.213
2022-01-31 13:01:14 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-01-31 13:01:14 | INFO | train | epoch 042 | loss 7.997 | ppl 255.42 | wps 5897.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2688 | lr 0.000336033 | gnorm 0.507 | train_wall 325 | gb_free 6.1 | wall 14897
KL Stats: Epoch 42 Divergences: Uniform: 2.373181215456418 Unigram: 2.2907259681200483
2022-01-31 13:01:14 | INFO | fairseq.trainer | begin training epoch 43
2022-01-31 13:01:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:02:16 | INFO | train_inner | epoch 043:     12 / 64 loss=8.003, ppl=256.56, wps=5772.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=2700, lr=0.000337533, gnorm=0.506, train_wall=508, gb_free=6.1, wall=14958
2022-01-31 13:06:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:07:09 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.242 | ppl 605.34 | wps 7877.3 | wpb 2034.1 | bsz 4 | num_updates 2752 | best_loss 9.242
2022-01-31 13:07:09 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-01-31 13:07:09 | INFO | train | epoch 043 | loss 7.932 | ppl 244.17 | wps 5885.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2752 | lr 0.000344031 | gnorm 0.497 | train_wall 326 | gb_free 6.1 | wall 15251
KL Stats: Epoch 43 Divergences: Uniform: 2.39493089001601 Unigram: 2.3174577780527588
2022-01-31 13:07:09 | INFO | fairseq.trainer | begin training epoch 44
2022-01-31 13:07:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:11:14 | INFO | train_inner | epoch 044:     48 / 64 loss=7.897, ppl=238.43, wps=6071.3, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=2800, lr=0.00035003, gnorm=0.499, train_wall=509, gb_free=6.1, wall=15497
2022-01-31 13:12:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:13:03 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.242 | ppl 605.69 | wps 7834.1 | wpb 2034.1 | bsz 4 | num_updates 2816 | best_loss 9.242
2022-01-31 13:13:03 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-01-31 13:13:03 | INFO | train | epoch 044 | loss 7.872 | ppl 234.26 | wps 5900.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2816 | lr 0.00035203 | gnorm 0.502 | train_wall 325 | gb_free 6.1 | wall 15605
KL Stats: Epoch 44 Divergences: Uniform: 2.412821130862619 Unigram: 2.340842516116444
2022-01-31 13:13:03 | INFO | fairseq.trainer | begin training epoch 45
2022-01-31 13:13:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:18:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:18:57 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.221 | ppl 596.66 | wps 7861.4 | wpb 2034.1 | bsz 4 | num_updates 2880 | best_loss 9.221
2022-01-31 13:18:57 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-01-31 13:18:57 | INFO | train | epoch 045 | loss 7.81 | ppl 224.36 | wps 5902.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2880 | lr 0.000360028 | gnorm 0.504 | train_wall 325 | gb_free 6.1 | wall 15959
KL Stats: Epoch 45 Divergences: Uniform: 2.4316907512099277 Unigram: 2.371139778331074
2022-01-31 13:18:57 | INFO | fairseq.trainer | begin training epoch 46
2022-01-31 13:18:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:20:39 | INFO | train_inner | epoch 046:     20 / 64 loss=7.809, ppl=224.28, wps=5770.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=2900, lr=0.000362528, gnorm=0.504, train_wall=508, gb_free=6.1, wall=16061
2022-01-31 13:24:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:24:51 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.227 | ppl 599.36 | wps 8030 | wpb 2034.1 | bsz 4 | num_updates 2944 | best_loss 9.227
2022-01-31 13:24:51 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-01-31 13:24:51 | INFO | train | epoch 046 | loss 7.751 | ppl 215.37 | wps 5902.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2944 | lr 0.000368026 | gnorm 0.507 | train_wall 326 | gb_free 6.1 | wall 16313
KL Stats: Epoch 46 Divergences: Uniform: 2.44571838345043 Unigram: 2.389719107073781
2022-01-31 13:24:51 | INFO | fairseq.trainer | begin training epoch 47
2022-01-31 13:24:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:29:39 | INFO | train_inner | epoch 047:     56 / 64 loss=7.72, ppl=210.81, wps=6055.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3000, lr=0.000375025, gnorm=0.499, train_wall=511, gb_free=6.1, wall=16601
2022-01-31 13:30:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:30:47 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.208 | ppl 591.33 | wps 7874.2 | wpb 2034.1 | bsz 4 | num_updates 3008 | best_loss 9.208
2022-01-31 13:30:47 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-01-31 13:30:47 | INFO | train | epoch 047 | loss 7.692 | ppl 206.72 | wps 5870.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3008 | lr 0.000376025 | gnorm 0.492 | train_wall 327 | gb_free 6.1 | wall 16669
KL Stats: Epoch 47 Divergences: Uniform: 2.4685463246058954 Unigram: 2.412590927406782
2022-01-31 13:30:47 | INFO | fairseq.trainer | begin training epoch 48
2022-01-31 13:30:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:36:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:36:42 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.206 | ppl 590.43 | wps 7837.7 | wpb 2034.1 | bsz 4 | num_updates 3072 | best_loss 9.206
2022-01-31 13:36:42 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-01-31 13:36:42 | INFO | train | epoch 048 | loss 7.635 | ppl 198.75 | wps 5885 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3072 | lr 0.000384023 | gnorm 0.508 | train_wall 326 | gb_free 6.1 | wall 17024
KL Stats: Epoch 48 Divergences: Uniform: 2.4871440295690546 Unigram: 2.442350126085311
2022-01-31 13:36:42 | INFO | fairseq.trainer | begin training epoch 49
2022-01-31 13:36:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:39:06 | INFO | train_inner | epoch 049:     28 / 64 loss=7.617, ppl=196.34, wps=5753.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=3100, lr=0.000387523, gnorm=0.504, train_wall=509, gb_free=6.1, wall=17168
2022-01-31 13:42:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:42:37 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.233 | ppl 601.7 | wps 7825.1 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 9.233
2022-01-31 13:42:37 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-01-31 13:42:37 | INFO | train | epoch 049 | loss 7.578 | ppl 191.08 | wps 5884.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3136 | lr 0.000392022 | gnorm 0.507 | train_wall 326 | gb_free 6.1 | wall 17379
KL Stats: Epoch 49 Divergences: Uniform: 2.492513298935431 Unigram: 2.4626103808389894
2022-01-31 13:42:37 | INFO | fairseq.trainer | begin training epoch 50
2022-01-31 13:42:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:48:03 | INFO | train_inner | epoch 050:     64 / 64 loss=7.553, ppl=187.82, wps=6068.8, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=3200, lr=0.00040002, gnorm=0.52, train_wall=508, gb_free=6.1, wall=17705
2022-01-31 13:48:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:48:31 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.242 | ppl 605.51 | wps 7880.1 | wpb 2034.1 | bsz 4 | num_updates 3200 | best_loss 9.242
2022-01-31 13:48:31 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-01-31 13:48:31 | INFO | train | epoch 050 | loss 7.526 | ppl 184.35 | wps 5903.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3200 | lr 0.00040002 | gnorm 0.524 | train_wall 325 | gb_free 6.1 | wall 17733
KL Stats: Epoch 50 Divergences: Uniform: 2.5132236073295435 Unigram: 2.4790649263794244
2022-01-31 13:48:31 | INFO | fairseq.trainer | begin training epoch 51
2022-01-31 13:48:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:53:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:54:25 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.26 | ppl 613.29 | wps 8021.9 | wpb 2034.1 | bsz 4 | num_updates 3264 | best_loss 9.246
2022-01-31 13:54:25 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-01-31 13:54:25 | INFO | train | epoch 051 | loss 7.47 | ppl 177.31 | wps 5889.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3264 | lr 0.000408018 | gnorm 0.505 | train_wall 326 | gb_free 6.1 | wall 18087
KL Stats: Epoch 51 Divergences: Uniform: 2.5384572158640775 Unigram: 2.5002152715531447
2022-01-31 13:54:25 | INFO | fairseq.trainer | begin training epoch 52
2022-01-31 13:54:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:57:30 | INFO | train_inner | epoch 052:     36 / 64 loss=7.446, ppl=174.39, wps=5760.4, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=3300, lr=0.000412518, gnorm=0.504, train_wall=511, gb_free=6.1, wall=18272
2022-01-31 13:59:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:00:20 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.248 | ppl 607.87 | wps 7857.2 | wpb 2034.1 | bsz 4 | num_updates 3328 | best_loss 9.246
2022-01-31 14:00:20 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-01-31 14:00:20 | INFO | train | epoch 052 | loss 7.418 | ppl 171.02 | wps 5883.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3328 | lr 0.000416017 | gnorm 0.512 | train_wall 326 | gb_free 6.1 | wall 18442
KL Stats: Epoch 52 Divergences: Uniform: 2.5459241317845533 Unigram: 2.5297980139216367
2022-01-31 14:00:20 | INFO | fairseq.trainer | begin training epoch 53
2022-01-31 14:00:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:05:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:06:15 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.215 | ppl 594.48 | wps 7712.6 | wpb 2034.1 | bsz 4 | num_updates 3392 | best_loss 9.215
2022-01-31 14:06:15 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-01-31 14:06:15 | INFO | train | epoch 053 | loss 7.367 | ppl 165.04 | wps 5887.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3392 | lr 0.000424015 | gnorm 0.499 | train_wall 325 | gb_free 6.1 | wall 18797
KL Stats: Epoch 53 Divergences: Uniform: 2.571177017408321 Unigram: 2.546479536065831
2022-01-31 14:06:15 | INFO | fairseq.trainer | begin training epoch 54
2022-01-31 14:06:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:06:57 | INFO | train_inner | epoch 054:      8 / 64 loss=7.38, ppl=166.53, wps=5754.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=3400, lr=0.000425015, gnorm=0.51, train_wall=509, gb_free=6.1, wall=18839
2022-01-31 14:11:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:12:10 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.28 | ppl 621.83 | wps 7882.1 | wpb 2034.1 | bsz 4 | num_updates 3456 | best_loss 9.246
2022-01-31 14:12:10 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-01-31 14:12:10 | INFO | train | epoch 054 | loss 7.316 | ppl 159.36 | wps 5876.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3456 | lr 0.000432014 | gnorm 0.515 | train_wall 327 | gb_free 6.1 | wall 19152
KL Stats: Epoch 54 Divergences: Uniform: 2.575243285746771 Unigram: 2.5651617547537664
2022-01-31 14:12:10 | INFO | fairseq.trainer | begin training epoch 55
2022-01-31 14:12:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:15:56 | INFO | train_inner | epoch 055:     44 / 64 loss=7.289, ppl=156.37, wps=6057.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3500, lr=0.000437513, gnorm=0.516, train_wall=510, gb_free=6.1, wall=19378
2022-01-31 14:17:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:18:05 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.271 | ppl 617.99 | wps 7811.1 | wpb 2034.1 | bsz 4 | num_updates 3520 | best_loss 9.246
2022-01-31 14:18:05 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-01-31 14:18:05 | INFO | train | epoch 055 | loss 7.27 | ppl 154.31 | wps 5886.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3520 | lr 0.000440012 | gnorm 0.533 | train_wall 326 | gb_free 6.1 | wall 19507
KL Stats: Epoch 55 Divergences: Uniform: 2.592504079321782 Unigram: 2.5896378388893746
2022-01-31 14:18:05 | INFO | fairseq.trainer | begin training epoch 56
2022-01-31 14:18:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:23:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:24:00 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.38 | ppl 666.5 | wps 7864.2 | wpb 2034.1 | bsz 4 | num_updates 3584 | best_loss 9.246
2022-01-31 14:24:00 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-01-31 14:24:00 | INFO | train | epoch 056 | loss 7.22 | ppl 149.05 | wps 5894 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3584 | lr 0.00044801 | gnorm 0.515 | train_wall 326 | gb_free 6.1 | wall 19862
KL Stats: Epoch 56 Divergences: Uniform: 2.588788986840991 Unigram: 2.6029715724272884
2022-01-31 14:24:00 | INFO | fairseq.trainer | begin training epoch 57
2022-01-31 14:24:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:25:21 | INFO | train_inner | epoch 057:     16 / 64 loss=7.224, ppl=149.48, wps=5770.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=3600, lr=0.00045001, gnorm=0.525, train_wall=508, gb_free=6.1, wall=19943
2022-01-31 14:29:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:29:53 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.356 | ppl 655.47 | wps 7829.7 | wpb 2034.1 | bsz 4 | num_updates 3648 | best_loss 9.246
2022-01-31 14:29:53 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-01-31 14:29:53 | INFO | train | epoch 057 | loss 7.173 | ppl 144.27 | wps 5907 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3648 | lr 0.000456009 | gnorm 0.526 | train_wall 325 | gb_free 6.1 | wall 20215
KL Stats: Epoch 57 Divergences: Uniform: 2.6248513855407145 Unigram: 2.6314225590498883
2022-01-31 14:29:53 | INFO | fairseq.trainer | begin training epoch 58
2022-01-31 14:29:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:34:18 | INFO | train_inner | epoch 058:     52 / 64 loss=7.149, ppl=141.94, wps=6081.5, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3700, lr=0.000462508, gnorm=0.528, train_wall=508, gb_free=6.1, wall=20481
2022-01-31 14:35:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:35:47 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.348 | ppl 651.56 | wps 7853.5 | wpb 2034.1 | bsz 4 | num_updates 3712 | best_loss 9.246
2022-01-31 14:35:47 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-01-31 14:35:47 | INFO | train | epoch 058 | loss 7.129 | ppl 140 | wps 5908.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3712 | lr 0.000464007 | gnorm 0.529 | train_wall 325 | gb_free 6.1 | wall 20569
KL Stats: Epoch 58 Divergences: Uniform: 2.6320366394874313 Unigram: 2.6464217880594427
2022-01-31 14:35:47 | INFO | fairseq.trainer | begin training epoch 59
2022-01-31 14:35:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:41:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:41:42 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.444 | ppl 696.67 | wps 7845.1 | wpb 2034.1 | bsz 4 | num_updates 3776 | best_loss 9.246
2022-01-31 14:41:42 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-01-31 14:41:42 | INFO | train | epoch 059 | loss 7.084 | ppl 135.7 | wps 5873.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3776 | lr 0.000472006 | gnorm 0.527 | train_wall 327 | gb_free 6.1 | wall 20924
KL Stats: Epoch 59 Divergences: Uniform: 2.6483546816609564 Unigram: 2.661073766599165
2022-01-31 14:41:42 | INFO | fairseq.trainer | begin training epoch 60
2022-01-31 14:41:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:43:45 | INFO | train_inner | epoch 060:     24 / 64 loss=7.078, ppl=135.14, wps=5759.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=3800, lr=0.000475005, gnorm=0.533, train_wall=509, gb_free=6.1, wall=21047
2022-01-31 14:47:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:47:36 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.374 | ppl 663.38 | wps 7837.5 | wpb 2034.1 | bsz 4 | num_updates 3840 | best_loss 9.246
2022-01-31 14:47:36 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-01-31 14:47:36 | INFO | train | epoch 060 | loss 7.039 | ppl 131.52 | wps 5898 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3840 | lr 0.000480004 | gnorm 0.536 | train_wall 325 | gb_free 6.1 | wall 21278
KL Stats: Epoch 60 Divergences: Uniform: 2.663031051172662 Unigram: 2.686090431264174
2022-01-31 14:47:36 | INFO | fairseq.trainer | begin training epoch 61
2022-01-31 14:47:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:52:44 | INFO | train_inner | epoch 061:     60 / 64 loss=7.019, ppl=129.71, wps=6062.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3900, lr=0.000487503, gnorm=0.535, train_wall=510, gb_free=6.1, wall=21586
2022-01-31 14:53:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:53:31 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.434 | ppl 691.88 | wps 7800.2 | wpb 2034.1 | bsz 4 | num_updates 3904 | best_loss 9.246
2022-01-31 14:53:31 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-01-31 14:53:31 | INFO | train | epoch 061 | loss 6.996 | ppl 127.67 | wps 5889.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3904 | lr 0.000488002 | gnorm 0.542 | train_wall 326 | gb_free 6.1 | wall 21633
KL Stats: Epoch 61 Divergences: Uniform: 2.6812445053882827 Unigram: 2.6953474898577476
2022-01-31 14:53:31 | INFO | fairseq.trainer | begin training epoch 62
2022-01-31 14:53:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:58:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:59:26 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.409 | ppl 679.96 | wps 7840 | wpb 2034.1 | bsz 4 | num_updates 3968 | best_loss 9.246
2022-01-31 14:59:26 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-01-31 14:59:26 | INFO | train | epoch 062 | loss 6.955 | ppl 124.04 | wps 5879.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3968 | lr 0.000496001 | gnorm 0.538 | train_wall 326 | gb_free 6.1 | wall 21988
KL Stats: Epoch 62 Divergences: Uniform: 2.687262237415915 Unigram: 2.7204856309581746
2022-01-31 14:59:26 | INFO | fairseq.trainer | begin training epoch 63
2022-01-31 14:59:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:02:10 | INFO | train_inner | epoch 063:     32 / 64 loss=6.929, ppl=121.85, wps=5753.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4000, lr=0.0005, gnorm=0.538, train_wall=509, gb_free=6.1, wall=22152
2022-01-31 15:04:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:05:21 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.431 | ppl 690.24 | wps 7835.6 | wpb 2034.1 | bsz 4 | num_updates 4032 | best_loss 9.246
2022-01-31 15:05:21 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-01-31 15:05:21 | INFO | train | epoch 063 | loss 6.912 | ppl 120.4 | wps 5881.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4032 | lr 0.000498012 | gnorm 0.536 | train_wall 326 | gb_free 6.1 | wall 22343
KL Stats: Epoch 63 Divergences: Uniform: 2.7028314626010532 Unigram: 2.7365208688017213
2022-01-31 15:05:21 | INFO | fairseq.trainer | begin training epoch 64
2022-01-31 15:05:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:10:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:11:16 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.458 | ppl 703.14 | wps 7840.1 | wpb 2034.1 | bsz 4 | num_updates 4096 | best_loss 9.246
2022-01-31 15:11:16 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-01-31 15:11:16 | INFO | train | epoch 064 | loss 6.869 | ppl 116.89 | wps 5880.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4096 | lr 0.000494106 | gnorm 0.546 | train_wall 326 | gb_free 6.1 | wall 22699
KL Stats: Epoch 64 Divergences: Uniform: 2.71180065716313 Unigram: 2.753621684356737
2022-01-31 15:11:16 | INFO | fairseq.trainer | begin training epoch 65
2022-01-31 15:11:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:11:37 | INFO | train_inner | epoch 065:      4 / 64 loss=6.896, ppl=119.1, wps=5752.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4100, lr=0.000493865, gnorm=0.544, train_wall=509, gb_free=6.1, wall=22719
2022-01-31 15:16:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:17:11 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.504 | ppl 726.05 | wps 7707.7 | wpb 2034.1 | bsz 4 | num_updates 4160 | best_loss 9.246
2022-01-31 15:17:11 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-01-31 15:17:11 | INFO | train | epoch 065 | loss 6.826 | ppl 113.49 | wps 5888.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4160 | lr 0.00049029 | gnorm 0.552 | train_wall 325 | gb_free 6.1 | wall 23053
KL Stats: Epoch 65 Divergences: Uniform: 2.7159273904850365 Unigram: 2.771189072160683
2022-01-31 15:17:11 | INFO | fairseq.trainer | begin training epoch 66
2022-01-31 15:17:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:20:36 | INFO | train_inner | epoch 066:     40 / 64 loss=6.802, ppl=111.57, wps=6062.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4200, lr=0.00048795, gnorm=0.555, train_wall=509, gb_free=6.1, wall=23258
2022-01-31 15:22:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:23:06 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.478 | ppl 713.33 | wps 7879.1 | wpb 2034.1 | bsz 4 | num_updates 4224 | best_loss 9.246
2022-01-31 15:23:06 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-01-31 15:23:06 | INFO | train | epoch 066 | loss 6.786 | ppl 110.38 | wps 5892.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4224 | lr 0.000486562 | gnorm 0.558 | train_wall 326 | gb_free 6.1 | wall 23408
KL Stats: Epoch 66 Divergences: Uniform: 2.7363678512708693 Unigram: 2.7854401653626732
2022-01-31 15:23:06 | INFO | fairseq.trainer | begin training epoch 67
2022-01-31 15:23:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:28:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:28:59 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.455 | ppl 701.94 | wps 7834.1 | wpb 2034.1 | bsz 4 | num_updates 4288 | best_loss 9.246
2022-01-31 15:28:59 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-01-31 15:28:59 | INFO | train | epoch 067 | loss 6.745 | ppl 107.3 | wps 5918.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4288 | lr 0.000482917 | gnorm 0.554 | train_wall 324 | gb_free 6.1 | wall 23761
KL Stats: Epoch 67 Divergences: Uniform: 2.7525707326144895 Unigram: 2.8078312264301486
2022-01-31 15:28:59 | INFO | fairseq.trainer | begin training epoch 68
2022-01-31 15:28:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:30:00 | INFO | train_inner | epoch 068:     12 / 64 loss=6.755, ppl=107.98, wps=5780.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4300, lr=0.000482243, gnorm=0.556, train_wall=507, gb_free=6.1, wall=23822
2022-01-31 15:34:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:34:53 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.527 | ppl 737.87 | wps 7849.7 | wpb 2034.1 | bsz 4 | num_updates 4352 | best_loss 9.246
2022-01-31 15:34:53 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-01-31 15:34:53 | INFO | train | epoch 068 | loss 6.707 | ppl 104.48 | wps 5894 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4352 | lr 0.000479353 | gnorm 0.562 | train_wall 326 | gb_free 6.1 | wall 24115
KL Stats: Epoch 68 Divergences: Uniform: 2.769761139056548 Unigram: 2.826800064674442
2022-01-31 15:34:53 | INFO | fairseq.trainer | begin training epoch 69
2022-01-31 15:34:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:38:58 | INFO | train_inner | epoch 069:     48 / 64 loss=6.689, ppl=103.2, wps=6069.6, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4400, lr=0.000476731, gnorm=0.555, train_wall=509, gb_free=6.1, wall=24360
2022-01-31 15:40:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:40:48 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.544 | ppl 746.61 | wps 7784.2 | wpb 2034.1 | bsz 4 | num_updates 4416 | best_loss 9.246
2022-01-31 15:40:48 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-01-31 15:40:48 | INFO | train | epoch 069 | loss 6.67 | ppl 101.83 | wps 5887.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4416 | lr 0.000475867 | gnorm 0.556 | train_wall 326 | gb_free 6.1 | wall 24470
KL Stats: Epoch 69 Divergences: Uniform: 2.778441064473104 Unigram: 2.8393852453574677
2022-01-31 15:40:48 | INFO | fairseq.trainer | begin training epoch 70
2022-01-31 15:40:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:46:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:46:43 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.603 | ppl 777.69 | wps 7643.9 | wpb 2034.1 | bsz 4 | num_updates 4480 | best_loss 9.246
2022-01-31 15:46:43 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-01-31 15:46:43 | INFO | train | epoch 070 | loss 6.635 | ppl 99.37 | wps 5884.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4480 | lr 0.000472456 | gnorm 0.554 | train_wall 325 | gb_free 6.1 | wall 24825
KL Stats: Epoch 70 Divergences: Uniform: 2.7869951213882387 Unigram: 2.847321428408159
2022-01-31 15:46:43 | INFO | fairseq.trainer | begin training epoch 71
2022-01-31 15:46:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:48:26 | INFO | train_inner | epoch 071:     20 / 64 loss=6.63, ppl=99.02, wps=5746, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4500, lr=0.000471405, gnorm=0.562, train_wall=509, gb_free=6.1, wall=24928
2022-01-31 15:52:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:52:38 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.576 | ppl 763.45 | wps 7754.7 | wpb 2034.1 | bsz 4 | num_updates 4544 | best_loss 9.246
2022-01-31 15:52:38 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-01-31 15:52:38 | INFO | train | epoch 071 | loss 6.602 | ppl 97.14 | wps 5875.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4544 | lr 0.000469117 | gnorm 0.576 | train_wall 326 | gb_free 6.1 | wall 25180
KL Stats: Epoch 71 Divergences: Uniform: 2.7953830080671516 Unigram: 2.872068287899798
2022-01-31 15:52:38 | INFO | fairseq.trainer | begin training epoch 72
2022-01-31 15:52:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:57:24 | INFO | train_inner | epoch 072:     56 / 64 loss=6.586, ppl=96.07, wps=6075.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4600, lr=0.000466252, gnorm=0.568, train_wall=508, gb_free=6.1, wall=25466
2022-01-31 15:58:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:58:32 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.558 | ppl 753.89 | wps 7755.2 | wpb 2034.1 | bsz 4 | num_updates 4608 | best_loss 9.246
2022-01-31 15:58:32 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-01-31 15:58:32 | INFO | train | epoch 072 | loss 6.566 | ppl 94.72 | wps 5908.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4608 | lr 0.000465847 | gnorm 0.562 | train_wall 324 | gb_free 6.1 | wall 25534
KL Stats: Epoch 72 Divergences: Uniform: 2.8144443693343026 Unigram: 2.887326286995723
2022-01-31 15:58:32 | INFO | fairseq.trainer | begin training epoch 73
2022-01-31 15:58:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:03:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:04:26 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.631 | ppl 792.77 | wps 7786.3 | wpb 2034.1 | bsz 4 | num_updates 4672 | best_loss 9.246
2022-01-31 16:04:26 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-01-31 16:04:26 | INFO | train | epoch 073 | loss 6.534 | ppl 92.67 | wps 5893.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4672 | lr 0.000462646 | gnorm 0.565 | train_wall 325 | gb_free 6.1 | wall 25888
KL Stats: Epoch 73 Divergences: Uniform: 2.8112855059170374 Unigram: 2.9000206296305313
2022-01-31 16:04:26 | INFO | fairseq.trainer | begin training epoch 74
2022-01-31 16:04:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:06:49 | INFO | train_inner | epoch 074:     28 / 64 loss=6.523, ppl=91.95, wps=5767.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4700, lr=0.000461266, gnorm=0.565, train_wall=507, gb_free=6.1, wall=26031
2022-01-31 16:09:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:10:20 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.583 | ppl 766.81 | wps 7753.7 | wpb 2034.1 | bsz 4 | num_updates 4736 | best_loss 9.246
2022-01-31 16:10:20 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-01-31 16:10:20 | INFO | train | epoch 074 | loss 6.501 | ppl 90.57 | wps 5905.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4736 | lr 0.000459509 | gnorm 0.568 | train_wall 325 | gb_free 6.1 | wall 26242
KL Stats: Epoch 74 Divergences: Uniform: 2.8285110012669064 Unigram: 2.9194100935936396
2022-01-31 16:10:20 | INFO | fairseq.trainer | begin training epoch 75
2022-01-31 16:10:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:15:47 | INFO | train_inner | epoch 075:     64 / 64 loss=6.492, ppl=90.01, wps=6060.8, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=4800, lr=0.000456435, gnorm=0.575, train_wall=508, gb_free=6.1, wall=26569
2022-01-31 16:15:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:16:15 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.724 | ppl 845.67 | wps 7668 | wpb 2034.1 | bsz 4 | num_updates 4800 | best_loss 9.246
2022-01-31 16:16:15 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-01-31 16:16:15 | INFO | train | epoch 075 | loss 6.472 | ppl 88.76 | wps 5876.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4800 | lr 0.000456435 | gnorm 0.576 | train_wall 326 | gb_free 6.1 | wall 26597
KL Stats: Epoch 75 Divergences: Uniform: 2.8262790088594305 Unigram: 2.9311146187983352
2022-01-31 16:16:15 | INFO | fairseq.trainer | begin training epoch 76
2022-01-31 16:16:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:21:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:22:09 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 9.696 | ppl 829.72 | wps 7856.8 | wpb 2034.1 | bsz 4 | num_updates 4864 | best_loss 9.246
2022-01-31 16:22:09 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-01-31 16:22:09 | INFO | train | epoch 076 | loss 6.442 | ppl 86.97 | wps 5903 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4864 | lr 0.000453423 | gnorm 0.584 | train_wall 325 | gb_free 6.1 | wall 26951
KL Stats: Epoch 76 Divergences: Uniform: 2.839954368072354 Unigram: 2.951171327049977
2022-01-31 16:22:09 | INFO | fairseq.trainer | begin training epoch 77
2022-01-31 16:22:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:25:13 | INFO | train_inner | epoch 077:     36 / 64 loss=6.417, ppl=85.47, wps=5769.5, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=4900, lr=0.000451754, gnorm=0.585, train_wall=509, gb_free=6.1, wall=27135
2022-01-31 16:27:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:28:03 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 9.683 | ppl 822.03 | wps 7775.6 | wpb 2034.1 | bsz 4 | num_updates 4928 | best_loss 9.246
2022-01-31 16:28:03 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-01-31 16:28:03 | INFO | train | epoch 077 | loss 6.413 | ppl 85.24 | wps 5904.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4928 | lr 0.000450469 | gnorm 0.593 | train_wall 325 | gb_free 6.1 | wall 27305
KL Stats: Epoch 77 Divergences: Uniform: 2.8446341214897473 Unigram: 2.97305975360337
2022-01-31 16:28:03 | INFO | fairseq.trainer | begin training epoch 78
2022-01-31 16:28:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:33:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:33:57 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 9.665 | ppl 811.8 | wps 7819.8 | wpb 2034.1 | bsz 4 | num_updates 4992 | best_loss 9.246
2022-01-31 16:33:57 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-01-31 16:33:57 | INFO | train | epoch 078 | loss 6.385 | ppl 83.55 | wps 5889 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4992 | lr 0.000447572 | gnorm 0.595 | train_wall 326 | gb_free 6.1 | wall 27659
KL Stats: Epoch 78 Divergences: Uniform: 2.8578261340717397 Unigram: 2.981098338227456
2022-01-31 16:33:57 | INFO | fairseq.trainer | begin training epoch 79
2022-01-31 16:33:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:34:38 | INFO | train_inner | epoch 079:      8 / 64 loss=6.4, ppl=84.44, wps=5770.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=5000, lr=0.000447214, gnorm=0.597, train_wall=507, gb_free=6.1, wall=27700
2022-01-31 16:39:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:39:52 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 9.737 | ppl 853.54 | wps 7828.7 | wpb 2034.1 | bsz 4 | num_updates 5056 | best_loss 9.246
2022-01-31 16:39:52 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-01-31 16:39:52 | INFO | train | epoch 079 | loss 6.354 | ppl 81.82 | wps 5888.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5056 | lr 0.00044473 | gnorm 0.584 | train_wall 326 | gb_free 6.1 | wall 28014
KL Stats: Epoch 79 Divergences: Uniform: 2.8578794912608316 Unigram: 2.988907619100436
2022-01-31 16:39:52 | INFO | fairseq.trainer | begin training epoch 80
2022-01-31 16:39:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:43:37 | INFO | train_inner | epoch 080:     44 / 64 loss=6.338, ppl=80.89, wps=6058.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5100, lr=0.000442807, gnorm=0.583, train_wall=510, gb_free=6.1, wall=28240
2022-01-31 16:45:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:45:46 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 9.67 | ppl 814.8 | wps 7699.5 | wpb 2034.1 | bsz 4 | num_updates 5120 | best_loss 9.246
2022-01-31 16:45:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 5120 updates
2022-01-31 16:45:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint80.pt
2022-01-31 16:45:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint80.pt
2022-01-31 16:45:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint80.pt (epoch 80 @ 5120 updates, score 9.67) (writing took 3.481197975575924 seconds)
2022-01-31 16:45:50 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-01-31 16:45:50 | INFO | train | epoch 080 | loss 6.329 | ppl 80.4 | wps 5834.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5120 | lr 0.000441942 | gnorm 0.595 | train_wall 325 | gb_free 6.1 | wall 28372
KL Stats: Epoch 80 Divergences: Uniform: 2.8694258485430884 Unigram: 3.003587363404377
2022-01-31 16:45:50 | INFO | fairseq.trainer | begin training epoch 81
2022-01-31 16:45:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:51:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:51:44 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 9.679 | ppl 819.77 | wps 7836.7 | wpb 2034.1 | bsz 4 | num_updates 5184 | best_loss 9.246
2022-01-31 16:51:44 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-01-31 16:51:44 | INFO | train | epoch 081 | loss 6.303 | ppl 78.95 | wps 5895.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5184 | lr 0.000439205 | gnorm 0.606 | train_wall 325 | gb_free 6.1 | wall 28726
KL Stats: Epoch 81 Divergences: Uniform: 2.890615850356331 Unigram: 3.0291461778438413
2022-01-31 16:51:44 | INFO | fairseq.trainer | begin training epoch 82
2022-01-31 16:51:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:53:06 | INFO | train_inner | epoch 082:     16 / 64 loss=6.31, ppl=79.32, wps=5735.6, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5200, lr=0.000438529, gnorm=0.61, train_wall=507, gb_free=6.1, wall=28808
2022-01-31 16:57:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:57:38 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.682 | ppl 821.24 | wps 7861.9 | wpb 2034.1 | bsz 4 | num_updates 5248 | best_loss 9.246
2022-01-31 16:57:38 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-01-31 16:57:38 | INFO | train | epoch 082 | loss 6.278 | ppl 77.61 | wps 5909.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5248 | lr 0.000436519 | gnorm 0.608 | train_wall 325 | gb_free 6.1 | wall 29080
KL Stats: Epoch 82 Divergences: Uniform: 2.8985036248055978 Unigram: 3.0428841215633997
2022-01-31 16:57:38 | INFO | fairseq.trainer | begin training epoch 83
2022-01-31 16:57:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:02:03 | INFO | train_inner | epoch 083:     52 / 64 loss=6.264, ppl=76.86, wps=6080.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5300, lr=0.000434372, gnorm=0.612, train_wall=508, gb_free=6.1, wall=29345
2022-01-31 17:03:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:03:31 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 9.62 | ppl 787.11 | wps 7859.8 | wpb 2034.1 | bsz 4 | num_updates 5312 | best_loss 9.246
2022-01-31 17:03:31 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-01-31 17:03:31 | INFO | train | epoch 083 | loss 6.254 | ppl 76.32 | wps 5910.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5312 | lr 0.000433881 | gnorm 0.618 | train_wall 325 | gb_free 6.1 | wall 29433
KL Stats: Epoch 83 Divergences: Uniform: 2.90211026498174 Unigram: 3.0555249795985655
2022-01-31 17:03:31 | INFO | fairseq.trainer | begin training epoch 84
2022-01-31 17:03:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:08:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:09:26 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 9.696 | ppl 829.53 | wps 7847.7 | wpb 2034.1 | bsz 4 | num_updates 5376 | best_loss 9.246
2022-01-31 17:09:26 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-01-31 17:09:26 | INFO | train | epoch 084 | loss 6.228 | ppl 74.94 | wps 5886.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5376 | lr 0.000431291 | gnorm 0.602 | train_wall 326 | gb_free 6.1 | wall 29788
KL Stats: Epoch 84 Divergences: Uniform: 2.90920185862887 Unigram: 3.0673680220822783
2022-01-31 17:09:26 | INFO | fairseq.trainer | begin training epoch 85
2022-01-31 17:09:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:11:29 | INFO | train_inner | epoch 085:     24 / 64 loss=6.217, ppl=74.41, wps=5762.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5400, lr=0.000430331, gnorm=0.606, train_wall=509, gb_free=6.1, wall=29911
2022-01-31 17:14:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:15:21 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 9.713 | ppl 839.5 | wps 7841.4 | wpb 2034.1 | bsz 4 | num_updates 5440 | best_loss 9.246
2022-01-31 17:15:21 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-01-31 17:15:21 | INFO | train | epoch 085 | loss 6.204 | ppl 73.74 | wps 5887.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5440 | lr 0.000428746 | gnorm 0.614 | train_wall 326 | gb_free 6.1 | wall 30143
KL Stats: Epoch 85 Divergences: Uniform: 2.922017007841304 Unigram: 3.0761343846572964
2022-01-31 17:15:21 | INFO | fairseq.trainer | begin training epoch 86
2022-01-31 17:15:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:20:28 | INFO | train_inner | epoch 086:     60 / 64 loss=6.201, ppl=73.59, wps=6059.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5500, lr=0.000426401, gnorm=0.615, train_wall=510, gb_free=6.1, wall=30450
2022-01-31 17:20:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:21:15 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 9.732 | ppl 850.35 | wps 7842.3 | wpb 2034.1 | bsz 4 | num_updates 5504 | best_loss 9.246
2022-01-31 17:21:15 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-01-31 17:21:15 | INFO | train | epoch 086 | loss 6.18 | ppl 72.51 | wps 5888.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5504 | lr 0.000426246 | gnorm 0.617 | train_wall 326 | gb_free 6.1 | wall 30497
KL Stats: Epoch 86 Divergences: Uniform: 2.919603231373333 Unigram: 3.0963559125298348
2022-01-31 17:21:15 | INFO | fairseq.trainer | begin training epoch 87
2022-01-31 17:21:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:26:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:27:09 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 9.739 | ppl 854.44 | wps 8041.3 | wpb 2034.1 | bsz 4 | num_updates 5568 | best_loss 9.246
2022-01-31 17:27:09 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-01-31 17:27:09 | INFO | train | epoch 087 | loss 6.159 | ppl 71.45 | wps 5898.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5568 | lr 0.00042379 | gnorm 0.638 | train_wall 326 | gb_free 6.1 | wall 30852
KL Stats: Epoch 87 Divergences: Uniform: 2.9265122665493317 Unigram: 3.109895170713815
2022-01-31 17:27:09 | INFO | fairseq.trainer | begin training epoch 88
2022-01-31 17:27:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:29:54 | INFO | train_inner | epoch 088:     32 / 64 loss=6.146, ppl=70.81, wps=5764.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=5600, lr=0.000422577, gnorm=0.635, train_wall=509, gb_free=6.1, wall=31016
2022-01-31 17:32:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:33:04 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 9.717 | ppl 841.33 | wps 7832 | wpb 2034.1 | bsz 4 | num_updates 5632 | best_loss 9.246
2022-01-31 17:33:04 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-01-31 17:33:04 | INFO | train | epoch 088 | loss 6.137 | ppl 70.36 | wps 5883.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5632 | lr 0.000421375 | gnorm 0.635 | train_wall 326 | gb_free 6.1 | wall 31207
KL Stats: Epoch 88 Divergences: Uniform: 2.9324901219320147 Unigram: 3.1185036241029347
2022-01-31 17:33:04 | INFO | fairseq.trainer | begin training epoch 89
2022-01-31 17:33:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:38:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:38:59 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 9.737 | ppl 853.06 | wps 7877.5 | wpb 2034.1 | bsz 4 | num_updates 5696 | best_loss 9.246
2022-01-31 17:38:59 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-01-31 17:38:59 | INFO | train | epoch 089 | loss 6.118 | ppl 69.44 | wps 5885 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5696 | lr 0.000419001 | gnorm 0.644 | train_wall 326 | gb_free 6.1 | wall 31561
KL Stats: Epoch 89 Divergences: Uniform: 2.9409127239645505 Unigram: 3.131990047438749
2022-01-31 17:38:59 | INFO | fairseq.trainer | begin training epoch 90
2022-01-31 17:38:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:39:20 | INFO | train_inner | epoch 090:      4 / 64 loss=6.128, ppl=69.96, wps=5757.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5700, lr=0.000418854, gnorm=0.644, train_wall=509, gb_free=6.1, wall=31582
2022-01-31 17:44:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:44:54 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 9.781 | ppl 879.89 | wps 7833.9 | wpb 2034.1 | bsz 4 | num_updates 5760 | best_loss 9.246
2022-01-31 17:44:54 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-01-31 17:44:54 | INFO | train | epoch 090 | loss 6.095 | ppl 68.38 | wps 5897.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5760 | lr 0.000416667 | gnorm 0.631 | train_wall 325 | gb_free 6.1 | wall 31916
KL Stats: Epoch 90 Divergences: Uniform: 2.9464852430002457 Unigram: 3.1402575321002795
2022-01-31 17:44:54 | INFO | fairseq.trainer | begin training epoch 91
2022-01-31 17:44:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:48:19 | INFO | train_inner | epoch 091:     40 / 64 loss=6.077, ppl=67.51, wps=6064.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5800, lr=0.000415227, gnorm=0.626, train_wall=510, gb_free=6.1, wall=32121
2022-01-31 17:50:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:50:48 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 9.804 | ppl 894 | wps 7854.4 | wpb 2034.1 | bsz 4 | num_updates 5824 | best_loss 9.246
2022-01-31 17:50:48 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-01-31 17:50:48 | INFO | train | epoch 091 | loss 6.073 | ppl 67.33 | wps 5888.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5824 | lr 0.000414371 | gnorm 0.633 | train_wall 326 | gb_free 6.1 | wall 32270
KL Stats: Epoch 91 Divergences: Uniform: 2.9541651263958713 Unigram: 3.162081193140862
2022-01-31 17:50:48 | INFO | fairseq.trainer | begin training epoch 92
2022-01-31 17:50:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:56:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:56:42 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 9.805 | ppl 894.32 | wps 8039.2 | wpb 2034.1 | bsz 4 | num_updates 5888 | best_loss 9.246
2022-01-31 17:56:42 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-01-31 17:56:42 | INFO | train | epoch 092 | loss 6.054 | ppl 66.45 | wps 5900.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5888 | lr 0.000412113 | gnorm 0.665 | train_wall 326 | gb_free 6.1 | wall 32624
KL Stats: Epoch 92 Divergences: Uniform: 2.9608603573565055 Unigram: 3.1701059523445734
2022-01-31 17:56:42 | INFO | fairseq.trainer | begin training epoch 93
2022-01-31 17:56:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:57:44 | INFO | train_inner | epoch 093:     12 / 64 loss=6.063, ppl=66.88, wps=5769.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5900, lr=0.000411693, gnorm=0.656, train_wall=509, gb_free=6.1, wall=32686
2022-01-31 18:02:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:02:36 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 9.788 | ppl 884.31 | wps 7829.9 | wpb 2034.1 | bsz 4 | num_updates 5952 | best_loss 9.246
2022-01-31 18:02:36 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-01-31 18:02:36 | INFO | train | epoch 093 | loss 6.036 | ppl 65.63 | wps 5900.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5952 | lr 0.000409891 | gnorm 0.649 | train_wall 325 | gb_free 6.1 | wall 32978
KL Stats: Epoch 93 Divergences: Uniform: 2.9630128706607404 Unigram: 3.1805304436486823
2022-01-31 18:02:36 | INFO | fairseq.trainer | begin training epoch 94
2022-01-31 18:02:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:06:43 | INFO | train_inner | epoch 094:     48 / 64 loss=6.022, ppl=64.99, wps=6065.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=6000, lr=0.000408248, gnorm=0.656, train_wall=509, gb_free=6.1, wall=33225
2022-01-31 18:08:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:08:31 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 9.828 | ppl 908.81 | wps 7854.2 | wpb 2034.1 | bsz 4 | num_updates 6016 | best_loss 9.246
2022-01-31 18:08:31 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-01-31 18:08:31 | INFO | train | epoch 094 | loss 6.015 | ppl 64.66 | wps 5889.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6016 | lr 0.000407705 | gnorm 0.658 | train_wall 326 | gb_free 6.1 | wall 33333
KL Stats: Epoch 94 Divergences: Uniform: 2.966390253118658 Unigram: 3.198641726878607
2022-01-31 18:08:31 | INFO | fairseq.trainer | begin training epoch 95
2022-01-31 18:08:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:13:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:14:26 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 9.896 | ppl 952.69 | wps 7876.4 | wpb 2034.1 | bsz 4 | num_updates 6080 | best_loss 9.246
2022-01-31 18:14:26 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-01-31 18:14:26 | INFO | train | epoch 095 | loss 5.996 | ppl 63.84 | wps 5887 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6080 | lr 0.000405554 | gnorm 0.648 | train_wall 326 | gb_free 6.1 | wall 33688
KL Stats: Epoch 95 Divergences: Uniform: 2.9713336326152127 Unigram: 3.2089014736959474
2022-01-31 18:14:26 | INFO | fairseq.trainer | begin training epoch 96
2022-01-31 18:14:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:16:08 | INFO | train_inner | epoch 096:     20 / 64 loss=5.995, ppl=63.79, wps=5772.3, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6100, lr=0.000404888, gnorm=0.654, train_wall=508, gb_free=6.1, wall=33790
2022-01-31 18:19:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:20:19 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 9.799 | ppl 890.73 | wps 7836.8 | wpb 2034.1 | bsz 4 | num_updates 6144 | best_loss 9.246
2022-01-31 18:20:19 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-01-31 18:20:19 | INFO | train | epoch 096 | loss 5.98 | ppl 63.13 | wps 5910.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6144 | lr 0.000403436 | gnorm 0.673 | train_wall 325 | gb_free 6.1 | wall 34041
KL Stats: Epoch 96 Divergences: Uniform: 2.981195968783701 Unigram: 3.2182568683658133
2022-01-31 18:20:19 | INFO | fairseq.trainer | begin training epoch 97
2022-01-31 18:20:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:25:07 | INFO | train_inner | epoch 097:     56 / 64 loss=5.975, ppl=62.88, wps=6061.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=6200, lr=0.00040161, gnorm=0.683, train_wall=510, gb_free=6.1, wall=34329
2022-01-31 18:25:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:26:14 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 9.845 | ppl 919.52 | wps 8020.1 | wpb 2034.1 | bsz 4 | num_updates 6208 | best_loss 9.246
2022-01-31 18:26:14 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-01-31 18:26:14 | INFO | train | epoch 097 | loss 5.963 | ppl 62.37 | wps 5884.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6208 | lr 0.000401351 | gnorm 0.684 | train_wall 327 | gb_free 6.1 | wall 34396
KL Stats: Epoch 97 Divergences: Uniform: 2.9905779922585247 Unigram: 3.23319567323948
2022-01-31 18:26:14 | INFO | fairseq.trainer | begin training epoch 98
2022-01-31 18:26:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:31:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:32:09 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 9.856 | ppl 926.53 | wps 7847.5 | wpb 2034.1 | bsz 4 | num_updates 6272 | best_loss 9.246
2022-01-31 18:32:09 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-01-31 18:32:09 | INFO | train | epoch 098 | loss 5.942 | ppl 61.49 | wps 5887 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6272 | lr 0.000399298 | gnorm 0.674 | train_wall 326 | gb_free 6.1 | wall 34751
KL Stats: Epoch 98 Divergences: Uniform: 2.986913658490035 Unigram: 3.2348868696625055
2022-01-31 18:32:09 | INFO | fairseq.trainer | begin training epoch 99
2022-01-31 18:32:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:34:32 | INFO | train_inner | epoch 099:     28 / 64 loss=5.933, ppl=61.1, wps=5766.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=6300, lr=0.00039841, gnorm=0.674, train_wall=509, gb_free=6.1, wall=34894
2022-01-31 18:37:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:38:03 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 9.821 | ppl 904.67 | wps 7832.3 | wpb 2034.1 | bsz 4 | num_updates 6336 | best_loss 9.246
2022-01-31 18:38:03 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-01-31 18:38:03 | INFO | train | epoch 099 | loss 5.925 | ppl 60.74 | wps 5901 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6336 | lr 0.000397276 | gnorm 0.687 | train_wall 325 | gb_free 6.1 | wall 35105
KL Stats: Epoch 99 Divergences: Uniform: 2.9997902692184413 Unigram: 3.2533463750264406
2022-01-31 18:38:03 | INFO | fairseq.trainer | begin training epoch 100
2022-01-31 18:38:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:43:29 | INFO | train_inner | epoch 100:     64 / 64 loss=5.928, ppl=60.89, wps=6069.6, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=6400, lr=0.000395285, gnorm=0.694, train_wall=508, gb_free=6.1, wall=35431
2022-01-31 18:43:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:43:57 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 9.791 | ppl 886.05 | wps 7862.2 | wpb 2034.1 | bsz 4 | num_updates 6400 | best_loss 9.246
2022-01-31 18:43:57 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-01-31 18:43:57 | INFO | train | epoch 100 | loss 5.911 | ppl 60.17 | wps 5895.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6400 | lr 0.000395285 | gnorm 0.691 | train_wall 326 | gb_free 6.1 | wall 35459
KL Stats: Epoch 100 Divergences: Uniform: 2.9996437611603177 Unigram: 3.258376529573389
2022-01-31 18:43:57 | INFO | fairseq.trainer | begin training epoch 101
2022-01-31 18:43:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:49:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:49:51 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 9.87 | ppl 935.89 | wps 7850.3 | wpb 2034.1 | bsz 4 | num_updates 6464 | best_loss 9.246
2022-01-31 18:49:51 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-01-31 18:49:51 | INFO | train | epoch 101 | loss 5.891 | ppl 59.33 | wps 5892.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6464 | lr 0.000393323 | gnorm 0.691 | train_wall 326 | gb_free 6.1 | wall 35814
KL Stats: Epoch 101 Divergences: Uniform: 3.0136383977721546 Unigram: 3.2770153218991904
2022-01-31 18:49:51 | INFO | fairseq.trainer | begin training epoch 102
2022-01-31 18:49:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:52:56 | INFO | train_inner | epoch 102:     36 / 64 loss=5.877, ppl=58.77, wps=5764.1, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=6500, lr=0.000392232, gnorm=0.704, train_wall=510, gb_free=6.1, wall=35998
2022-01-31 18:55:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:55:45 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 9.869 | ppl 935.37 | wps 8023.7 | wpb 2034.1 | bsz 4 | num_updates 6528 | best_loss 9.246
2022-01-31 18:55:45 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-01-31 18:55:45 | INFO | train | epoch 102 | loss 5.877 | ppl 58.79 | wps 5901.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6528 | lr 0.00039139 | gnorm 0.713 | train_wall 326 | gb_free 6.1 | wall 36167
KL Stats: Epoch 102 Divergences: Uniform: 3.0074707747458143 Unigram: 3.285721747969396
2022-01-31 18:55:45 | INFO | fairseq.trainer | begin training epoch 103
2022-01-31 18:55:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:01:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:01:40 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 9.877 | ppl 940.37 | wps 7841.3 | wpb 2034.1 | bsz 4 | num_updates 6592 | best_loss 9.246
2022-01-31 19:01:40 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-01-31 19:01:40 | INFO | train | epoch 103 | loss 5.86 | ppl 58.1 | wps 5896.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6592 | lr 0.000389486 | gnorm 0.699 | train_wall 325 | gb_free 6.1 | wall 36522
KL Stats: Epoch 103 Divergences: Uniform: 3.019424094658558 Unigram: 3.2963978197645516
2022-01-31 19:01:40 | INFO | fairseq.trainer | begin training epoch 104
2022-01-31 19:01:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:02:21 | INFO | train_inner | epoch 104:      8 / 64 loss=5.868, ppl=58.39, wps=5775.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6600, lr=0.000389249, gnorm=0.699, train_wall=508, gb_free=6.1, wall=36563
2022-01-31 19:07:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:07:34 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 9.893 | ppl 950.83 | wps 7861.1 | wpb 2034.1 | bsz 4 | num_updates 6656 | best_loss 9.246
2022-01-31 19:07:34 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-01-31 19:07:34 | INFO | train | epoch 104 | loss 5.847 | ppl 57.56 | wps 5899.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6656 | lr 0.000387609 | gnorm 0.72 | train_wall 325 | gb_free 6.1 | wall 36876
KL Stats: Epoch 104 Divergences: Uniform: 3.0147308982640864 Unigram: 3.309288526290372
2022-01-31 19:07:34 | INFO | fairseq.trainer | begin training epoch 105
2022-01-31 19:07:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:11:20 | INFO | train_inner | epoch 105:     44 / 64 loss=5.835, ppl=57.07, wps=6058, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=6700, lr=0.000386334, gnorm=0.72, train_wall=510, gb_free=6.1, wall=37102
2022-01-31 19:13:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:13:29 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 9.835 | ppl 913.61 | wps 7868.7 | wpb 2034.1 | bsz 4 | num_updates 6720 | best_loss 9.246
2022-01-31 19:13:29 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-01-31 19:13:29 | INFO | train | epoch 105 | loss 5.831 | ppl 56.91 | wps 5873.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6720 | lr 0.000385758 | gnorm 0.717 | train_wall 327 | gb_free 6.1 | wall 37231
KL Stats: Epoch 105 Divergences: Uniform: 3.02241235725413 Unigram: 3.313443933547046
2022-01-31 19:13:29 | INFO | fairseq.trainer | begin training epoch 106
2022-01-31 19:13:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:18:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:19:23 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 9.876 | ppl 939.47 | wps 7872.6 | wpb 2034.1 | bsz 4 | num_updates 6784 | best_loss 9.246
2022-01-31 19:19:23 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-01-31 19:19:23 | INFO | train | epoch 106 | loss 5.815 | ppl 56.29 | wps 5897.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6784 | lr 0.000383934 | gnorm 0.72 | train_wall 325 | gb_free 6.1 | wall 37585
KL Stats: Epoch 106 Divergences: Uniform: 3.02000442061274 Unigram: 3.3225626371936485
2022-01-31 19:19:23 | INFO | fairseq.trainer | begin training epoch 107
2022-01-31 19:19:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:20:46 | INFO | train_inner | epoch 107:     16 / 64 loss=5.818, ppl=56.42, wps=5761.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6800, lr=0.000383482, gnorm=0.717, train_wall=509, gb_free=6.1, wall=37668
2022-01-31 19:24:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:25:17 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 9.881 | ppl 943.24 | wps 8015.7 | wpb 2034.1 | bsz 4 | num_updates 6848 | best_loss 9.246
2022-01-31 19:25:17 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-01-31 19:25:17 | INFO | train | epoch 107 | loss 5.799 | ppl 55.67 | wps 5902.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6848 | lr 0.000382136 | gnorm 0.709 | train_wall 326 | gb_free 6.1 | wall 37939
KL Stats: Epoch 107 Divergences: Uniform: 3.0309695071556066 Unigram: 3.338660753625158
2022-01-31 19:25:17 | INFO | fairseq.trainer | begin training epoch 108
2022-01-31 19:25:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:29:44 | INFO | train_inner | epoch 108:     52 / 64 loss=5.795, ppl=55.5, wps=6073.1, ups=0.19, wpb=32686.1, bsz=63.8, num_updates=6900, lr=0.000380693, gnorm=0.721, train_wall=510, gb_free=6.1, wall=38206
2022-01-31 19:30:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:31:12 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 9.919 | ppl 968.25 | wps 7854.7 | wpb 2034.1 | bsz 4 | num_updates 6912 | best_loss 9.246
2022-01-31 19:31:12 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-01-31 19:31:12 | INFO | train | epoch 108 | loss 5.788 | ppl 55.26 | wps 5889.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6912 | lr 0.000380363 | gnorm 0.737 | train_wall 326 | gb_free 6.1 | wall 38294
KL Stats: Epoch 108 Divergences: Uniform: 3.03347689154092 Unigram: 3.3434450013659767
2022-01-31 19:31:12 | INFO | fairseq.trainer | begin training epoch 109
2022-01-31 19:31:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:36:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:37:06 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 9.946 | ppl 986.28 | wps 7864.9 | wpb 2034.1 | bsz 4 | num_updates 6976 | best_loss 9.246
2022-01-31 19:37:06 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-01-31 19:37:06 | INFO | train | epoch 109 | loss 5.773 | ppl 54.68 | wps 5902.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6976 | lr 0.000378614 | gnorm 0.73 | train_wall 325 | gb_free 6.1 | wall 38648
KL Stats: Epoch 109 Divergences: Uniform: 3.0385399153642108 Unigram: 3.3572704321111617
2022-01-31 19:37:06 | INFO | fairseq.trainer | begin training epoch 110
2022-01-31 19:37:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:39:09 | INFO | train_inner | epoch 110:     24 / 64 loss=5.767, ppl=54.47, wps=5769.9, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=7000, lr=0.000377964, gnorm=0.738, train_wall=508, gb_free=6.1, wall=38771
2022-01-31 19:42:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:43:01 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 9.966 | ppl 1000.12 | wps 7841.4 | wpb 2034.1 | bsz 4 | num_updates 7040 | best_loss 9.246
2022-01-31 19:43:01 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-01-31 19:43:01 | INFO | train | epoch 110 | loss 5.759 | ppl 54.17 | wps 5882.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7040 | lr 0.000376889 | gnorm 0.734 | train_wall 326 | gb_free 6.1 | wall 39003
KL Stats: Epoch 110 Divergences: Uniform: 3.0451966794501186 Unigram: 3.3682384829599488
2022-01-31 19:43:01 | INFO | fairseq.trainer | begin training epoch 111
2022-01-31 19:43:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:48:07 | INFO | train_inner | epoch 111:     60 / 64 loss=5.759, ppl=54.15, wps=6071.7, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7100, lr=0.000375293, gnorm=0.744, train_wall=509, gb_free=6.1, wall=39309
2022-01-31 19:48:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:48:54 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 9.937 | ppl 980.21 | wps 7862.2 | wpb 2034.1 | bsz 4 | num_updates 7104 | best_loss 9.246
2022-01-31 19:48:54 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-01-31 19:48:54 | INFO | train | epoch 111 | loss 5.747 | ppl 53.69 | wps 5912.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7104 | lr 0.000375188 | gnorm 0.758 | train_wall 325 | gb_free 6.1 | wall 39356
KL Stats: Epoch 111 Divergences: Uniform: 3.0494284835117846 Unigram: 3.3816936833034132
2022-01-31 19:48:54 | INFO | fairseq.trainer | begin training epoch 112
2022-01-31 19:48:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:54:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:54:48 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 9.938 | ppl 980.67 | wps 8049.4 | wpb 2034.1 | bsz 4 | num_updates 7168 | best_loss 9.246
2022-01-31 19:54:48 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-01-31 19:54:48 | INFO | train | epoch 112 | loss 5.733 | ppl 53.17 | wps 5902.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7168 | lr 0.000373509 | gnorm 0.747 | train_wall 326 | gb_free 6.1 | wall 39710
KL Stats: Epoch 112 Divergences: Uniform: 3.0453683405096035 Unigram: 3.39058479212995
2022-01-31 19:54:48 | INFO | fairseq.trainer | begin training epoch 113
2022-01-31 19:54:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:57:33 | INFO | train_inner | epoch 113:     32 / 64 loss=5.721, ppl=52.74, wps=5766.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7200, lr=0.000372678, gnorm=0.748, train_wall=509, gb_free=6.1, wall=39875
2022-01-31 20:00:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:00:43 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 9.962 | ppl 997.22 | wps 7866 | wpb 2034.1 | bsz 4 | num_updates 7232 | best_loss 9.246
2022-01-31 20:00:43 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-01-31 20:00:43 | INFO | train | epoch 113 | loss 5.717 | ppl 52.6 | wps 5888.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7232 | lr 0.000371853 | gnorm 0.748 | train_wall 326 | gb_free 6.1 | wall 40065
KL Stats: Epoch 113 Divergences: Uniform: 3.0625494583627675 Unigram: 3.399464567237783
2022-01-31 20:00:43 | INFO | fairseq.trainer | begin training epoch 114
2022-01-31 20:00:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:06:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:06:36 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 9.954 | ppl 991.75 | wps 7832.5 | wpb 2034.1 | bsz 4 | num_updates 7296 | best_loss 9.246
2022-01-31 20:06:36 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-01-31 20:06:36 | INFO | train | epoch 114 | loss 5.705 | ppl 52.16 | wps 5904.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7296 | lr 0.000370218 | gnorm 0.752 | train_wall 325 | gb_free 6.1 | wall 40419
KL Stats: Epoch 114 Divergences: Uniform: 3.056717262340026 Unigram: 3.4063503651210483
2022-01-31 20:06:36 | INFO | fairseq.trainer | begin training epoch 115
2022-01-31 20:06:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:06:57 | INFO | train_inner | epoch 115:      4 / 64 loss=5.717, ppl=52.59, wps=5775, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=7300, lr=0.000370117, gnorm=0.753, train_wall=507, gb_free=6.1, wall=40439
2022-01-31 20:12:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:12:31 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 9.966 | ppl 999.98 | wps 7869.5 | wpb 2034.1 | bsz 4 | num_updates 7360 | best_loss 9.246
2022-01-31 20:12:31 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-01-31 20:12:31 | INFO | train | epoch 115 | loss 5.692 | ppl 51.71 | wps 5890.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7360 | lr 0.000368605 | gnorm 0.763 | train_wall 326 | gb_free 6.1 | wall 40773
KL Stats: Epoch 115 Divergences: Uniform: 3.062692138495786 Unigram: 3.413147659657322
2022-01-31 20:12:31 | INFO | fairseq.trainer | begin training epoch 116
2022-01-31 20:12:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:15:56 | INFO | train_inner | epoch 116:     40 / 64 loss=5.679, ppl=51.23, wps=6062.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7400, lr=0.000367607, gnorm=0.759, train_wall=510, gb_free=6.1, wall=40978
2022-01-31 20:17:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:18:27 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 9.935 | ppl 979.11 | wps 7737.7 | wpb 2034.1 | bsz 4 | num_updates 7424 | best_loss 9.246
2022-01-31 20:18:27 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-01-31 20:18:27 | INFO | train | epoch 116 | loss 5.68 | ppl 51.27 | wps 5870.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7424 | lr 0.000367013 | gnorm 0.766 | train_wall 327 | gb_free 6.1 | wall 41129
KL Stats: Epoch 116 Divergences: Uniform: 3.064045496744629 Unigram: 3.4177973114856868
2022-01-31 20:18:27 | INFO | fairseq.trainer | begin training epoch 117
2022-01-31 20:18:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:23:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:24:22 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 10 | ppl 1024.18 | wps 7871.1 | wpb 2034.1 | bsz 4 | num_updates 7488 | best_loss 9.246
2022-01-31 20:24:22 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-01-31 20:24:22 | INFO | train | epoch 117 | loss 5.669 | ppl 50.88 | wps 5887 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7488 | lr 0.000365441 | gnorm 0.773 | train_wall 326 | gb_free 6.1 | wall 41484
KL Stats: Epoch 117 Divergences: Uniform: 3.0629582802983744 Unigram: 3.4318073903831756
2022-01-31 20:24:22 | INFO | fairseq.trainer | begin training epoch 118
2022-01-31 20:24:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:25:23 | INFO | train_inner | epoch 118:     12 / 64 loss=5.674, ppl=51.05, wps=5748.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7500, lr=0.000365148, gnorm=0.78, train_wall=510, gb_free=6.1, wall=41545
2022-01-31 20:29:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:30:17 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 9.986 | ppl 1014.15 | wps 7824.7 | wpb 2034.1 | bsz 4 | num_updates 7552 | best_loss 9.246
2022-01-31 20:30:17 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-01-31 20:30:17 | INFO | train | epoch 118 | loss 5.656 | ppl 50.42 | wps 5879.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7552 | lr 0.000363889 | gnorm 0.781 | train_wall 326 | gb_free 6.1 | wall 41839
KL Stats: Epoch 118 Divergences: Uniform: 3.0735088211665724 Unigram: 3.4409717074299833
2022-01-31 20:30:17 | INFO | fairseq.trainer | begin training epoch 119
2022-01-31 20:30:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:34:23 | INFO | train_inner | epoch 119:     48 / 64 loss=5.646, ppl=50.08, wps=6058, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7600, lr=0.000362738, gnorm=0.775, train_wall=510, gb_free=6.1, wall=42085
2022-01-31 20:35:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:36:11 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 9.971 | ppl 1003.45 | wps 7847.2 | wpb 2034.1 | bsz 4 | num_updates 7616 | best_loss 9.246
2022-01-31 20:36:11 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-01-31 20:36:11 | INFO | train | epoch 119 | loss 5.644 | ppl 50.01 | wps 5898 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7616 | lr 0.000362357 | gnorm 0.783 | train_wall 325 | gb_free 6.1 | wall 42193
KL Stats: Epoch 119 Divergences: Uniform: 3.078349062351984 Unigram: 3.453722884694153
2022-01-31 20:36:11 | INFO | fairseq.trainer | begin training epoch 120
2022-01-31 20:36:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:41:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:42:06 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 9.984 | ppl 1012.61 | wps 7867.8 | wpb 2034.1 | bsz 4 | num_updates 7680 | best_loss 9.246
2022-01-31 20:42:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 7680 updates
2022-01-31 20:42:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint120.pt
2022-01-31 20:42:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint120.pt
2022-01-31 20:42:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint120.pt (epoch 120 @ 7680 updates, score 9.984) (writing took 3.4669819297268987 seconds)
2022-01-31 20:42:09 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-01-31 20:42:09 | INFO | train | epoch 120 | loss 5.634 | ppl 49.67 | wps 5829.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7680 | lr 0.000360844 | gnorm 0.786 | train_wall 326 | gb_free 6.1 | wall 42551
KL Stats: Epoch 120 Divergences: Uniform: 3.0841357559825493 Unigram: 3.457134184284947
2022-01-31 20:42:09 | INFO | fairseq.trainer | begin training epoch 121
2022-01-31 20:42:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:43:52 | INFO | train_inner | epoch 121:     20 / 64 loss=5.635, ppl=49.71, wps=5729.6, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7700, lr=0.000360375, gnorm=0.785, train_wall=508, gb_free=6.1, wall=42654
2022-01-31 20:47:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:48:04 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 9.998 | ppl 1022.53 | wps 7806.7 | wpb 2034.1 | bsz 4 | num_updates 7744 | best_loss 9.246
2022-01-31 20:48:04 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-01-31 20:48:04 | INFO | train | epoch 121 | loss 5.621 | ppl 49.23 | wps 5890.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7744 | lr 0.00035935 | gnorm 0.772 | train_wall 326 | gb_free 6.1 | wall 42906
KL Stats: Epoch 121 Divergences: Uniform: 3.0837452827534886 Unigram: 3.4635553600202917
2022-01-31 20:48:04 | INFO | fairseq.trainer | begin training epoch 122
2022-01-31 20:48:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:52:51 | INFO | train_inner | epoch 122:     56 / 64 loss=5.619, ppl=49.16, wps=6057.6, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7800, lr=0.000358057, gnorm=0.794, train_wall=510, gb_free=6.1, wall=43193
2022-01-31 20:53:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:53:59 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 9.95 | ppl 989.2 | wps 7866.2 | wpb 2034.1 | bsz 4 | num_updates 7808 | best_loss 9.246
2022-01-31 20:53:59 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-01-31 20:53:59 | INFO | train | epoch 122 | loss 5.612 | ppl 48.89 | wps 5886.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7808 | lr 0.000357874 | gnorm 0.81 | train_wall 326 | gb_free 6.1 | wall 43261
KL Stats: Epoch 122 Divergences: Uniform: 3.089669705495483 Unigram: 3.4775312468574886
2022-01-31 20:53:59 | INFO | fairseq.trainer | begin training epoch 123
2022-01-31 20:53:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:59:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:59:53 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 10.023 | ppl 1040.73 | wps 7859.7 | wpb 2034.1 | bsz 4 | num_updates 7872 | best_loss 9.246
2022-01-31 20:59:53 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-01-31 20:59:53 | INFO | train | epoch 123 | loss 5.6 | ppl 48.51 | wps 5891.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7872 | lr 0.000356416 | gnorm 0.811 | train_wall 326 | gb_free 6.1 | wall 43615
KL Stats: Epoch 123 Divergences: Uniform: 3.086775258018131 Unigram: 3.4861835337087306
2022-01-31 20:59:53 | INFO | fairseq.trainer | begin training epoch 124
2022-01-31 20:59:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:02:17 | INFO | train_inner | epoch 124:     28 / 64 loss=5.593, ppl=48.26, wps=5763.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7900, lr=0.000355784, gnorm=0.806, train_wall=509, gb_free=6.1, wall=43759
2022-01-31 21:05:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:05:48 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 10.005 | ppl 1027.41 | wps 7864.7 | wpb 2034.1 | bsz 4 | num_updates 7936 | best_loss 9.246
2022-01-31 21:05:48 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-01-31 21:05:48 | INFO | train | epoch 124 | loss 5.588 | ppl 48.1 | wps 5890.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7936 | lr 0.000354976 | gnorm 0.801 | train_wall 326 | gb_free 6.1 | wall 43970
KL Stats: Epoch 124 Divergences: Uniform: 3.076812669218142 Unigram: 3.4853046724545496
2022-01-31 21:05:48 | INFO | fairseq.trainer | begin training epoch 125
2022-01-31 21:05:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:11:14 | INFO | train_inner | epoch 125:     64 / 64 loss=5.591, ppl=48.21, wps=6062.7, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=8000, lr=0.000353553, gnorm=0.813, train_wall=508, gb_free=6.1, wall=44296
2022-01-31 21:11:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:11:42 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 10.049 | ppl 1059.55 | wps 7855.6 | wpb 2034.1 | bsz 4 | num_updates 8000 | best_loss 9.246
2022-01-31 21:11:42 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-01-31 21:11:42 | INFO | train | epoch 125 | loss 5.577 | ppl 47.75 | wps 5892.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8000 | lr 0.000353553 | gnorm 0.816 | train_wall 326 | gb_free 6.1 | wall 44324
KL Stats: Epoch 125 Divergences: Uniform: 3.090824957905586 Unigram: 3.4981979701553936
2022-01-31 21:11:42 | INFO | fairseq.trainer | begin training epoch 126
2022-01-31 21:11:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:17:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:17:37 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 9.973 | ppl 1005.3 | wps 7748.4 | wpb 2034.1 | bsz 4 | num_updates 8064 | best_loss 9.246
2022-01-31 21:17:37 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-01-31 21:17:37 | INFO | train | epoch 126 | loss 5.567 | ppl 47.4 | wps 5885.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8064 | lr 0.000352148 | gnorm 0.801 | train_wall 326 | gb_free 6.1 | wall 44679
KL Stats: Epoch 126 Divergences: Uniform: 3.097918617952891 Unigram: 3.5052430565178367
2022-01-31 21:17:37 | INFO | fairseq.trainer | begin training epoch 127
2022-01-31 21:17:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:20:42 | INFO | train_inner | epoch 127:     36 / 64 loss=5.555, ppl=47.02, wps=5760.3, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=8100, lr=0.000351364, gnorm=0.824, train_wall=510, gb_free=6.1, wall=44864
2022-01-31 21:23:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:23:31 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 10.003 | ppl 1026.13 | wps 7854.3 | wpb 2034.1 | bsz 4 | num_updates 8128 | best_loss 9.246
2022-01-31 21:23:31 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-01-31 21:23:31 | INFO | train | epoch 127 | loss 5.558 | ppl 47.11 | wps 5896.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8128 | lr 0.000350758 | gnorm 0.844 | train_wall 325 | gb_free 6.1 | wall 45033
KL Stats: Epoch 127 Divergences: Uniform: 3.0916964797194777 Unigram: 3.510811431430151
2022-01-31 21:23:31 | INFO | fairseq.trainer | begin training epoch 128
2022-01-31 21:23:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:28:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:29:25 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 10.019 | ppl 1037.89 | wps 7859.1 | wpb 2034.1 | bsz 4 | num_updates 8192 | best_loss 9.246
2022-01-31 21:29:25 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-01-31 21:29:25 | INFO | train | epoch 128 | loss 5.546 | ppl 46.72 | wps 5898.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8192 | lr 0.000349386 | gnorm 0.838 | train_wall 325 | gb_free 6.1 | wall 45387
KL Stats: Epoch 128 Divergences: Uniform: 3.0960420328301956 Unigram: 3.5200384903419724
2022-01-31 21:29:25 | INFO | fairseq.trainer | begin training epoch 129
2022-01-31 21:29:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:30:06 | INFO | train_inner | epoch 129:      8 / 64 loss=5.554, ppl=46.97, wps=5771.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8200, lr=0.000349215, gnorm=0.835, train_wall=508, gb_free=6.1, wall=45429
2022-01-31 21:34:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:35:21 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 10.013 | ppl 1033.11 | wps 7875.7 | wpb 2034.1 | bsz 4 | num_updates 8256 | best_loss 9.246
2022-01-31 21:35:21 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-01-31 21:35:21 | INFO | train | epoch 129 | loss 5.54 | ppl 46.52 | wps 5877.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8256 | lr 0.000348029 | gnorm 0.841 | train_wall 327 | gb_free 6.1 | wall 45743
KL Stats: Epoch 129 Divergences: Uniform: 3.0944421038435026 Unigram: 3.525352858777476
2022-01-31 21:35:21 | INFO | fairseq.trainer | begin training epoch 130
2022-01-31 21:35:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:39:06 | INFO | train_inner | epoch 130:     44 / 64 loss=5.527, ppl=46.11, wps=6055, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=8300, lr=0.000347105, gnorm=0.831, train_wall=511, gb_free=6.1, wall=45968
2022-01-31 21:40:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:41:16 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 10.087 | ppl 1087.68 | wps 7857 | wpb 2034.1 | bsz 4 | num_updates 8320 | best_loss 9.246
2022-01-31 21:41:16 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-01-31 21:41:16 | INFO | train | epoch 130 | loss 5.526 | ppl 46.08 | wps 5884.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8320 | lr 0.000346688 | gnorm 0.824 | train_wall 326 | gb_free 6.1 | wall 46098
KL Stats: Epoch 130 Divergences: Uniform: 3.096336222943654 Unigram: 3.5374585601207262
2022-01-31 21:41:16 | INFO | fairseq.trainer | begin training epoch 131
2022-01-31 21:41:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:46:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:47:09 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 10.055 | ppl 1063.81 | wps 7752.3 | wpb 2034.1 | bsz 4 | num_updates 8384 | best_loss 9.246
2022-01-31 21:47:09 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-01-31 21:47:09 | INFO | train | epoch 131 | loss 5.518 | ppl 45.83 | wps 5902.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8384 | lr 0.000345362 | gnorm 0.845 | train_wall 325 | gb_free 6.1 | wall 46452
KL Stats: Epoch 131 Divergences: Uniform: 3.096469563798697 Unigram: 3.539068610838557
2022-01-31 21:47:09 | INFO | fairseq.trainer | begin training epoch 132
2022-01-31 21:47:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:48:31 | INFO | train_inner | epoch 132:     16 / 64 loss=5.521, ppl=45.93, wps=5769.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=8400, lr=0.000345033, gnorm=0.847, train_wall=508, gb_free=6.1, wall=46533
2022-01-31 21:52:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:53:04 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 10.078 | ppl 1080.82 | wps 7845.9 | wpb 2034.1 | bsz 4 | num_updates 8448 | best_loss 9.246
2022-01-31 21:53:04 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-01-31 21:53:04 | INFO | train | epoch 132 | loss 5.509 | ppl 45.55 | wps 5897.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8448 | lr 0.000344051 | gnorm 0.876 | train_wall 325 | gb_free 6.1 | wall 46806
KL Stats: Epoch 132 Divergences: Uniform: 3.1037304387640754 Unigram: 3.5483974909919977
2022-01-31 21:53:04 | INFO | fairseq.trainer | begin training epoch 133
2022-01-31 21:53:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:57:30 | INFO | train_inner | epoch 133:     52 / 64 loss=5.503, ppl=45.35, wps=6068.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=8500, lr=0.000342997, gnorm=0.865, train_wall=509, gb_free=6.1, wall=47072
2022-01-31 21:58:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:58:58 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 10.048 | ppl 1058.94 | wps 7836.8 | wpb 2034.1 | bsz 4 | num_updates 8512 | best_loss 9.246
2022-01-31 21:58:58 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-01-31 21:58:58 | INFO | train | epoch 133 | loss 5.498 | ppl 45.19 | wps 5899.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8512 | lr 0.000342755 | gnorm 0.856 | train_wall 325 | gb_free 6.1 | wall 47160
KL Stats: Epoch 133 Divergences: Uniform: 3.114140632784319 Unigram: 3.5550237927392385
2022-01-31 21:58:58 | INFO | fairseq.trainer | begin training epoch 134
2022-01-31 21:58:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:04:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:04:52 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 10.132 | ppl 1122.15 | wps 7878.6 | wpb 2034.1 | bsz 4 | num_updates 8576 | best_loss 9.246
2022-01-31 22:04:52 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-01-31 22:04:52 | INFO | train | epoch 134 | loss 5.49 | ppl 44.95 | wps 5898.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8576 | lr 0.000341474 | gnorm 0.879 | train_wall 325 | gb_free 6.1 | wall 47514
KL Stats: Epoch 134 Divergences: Uniform: 3.1078433724863497 Unigram: 3.558811212929947
2022-01-31 22:04:52 | INFO | fairseq.trainer | begin training epoch 135
2022-01-31 22:04:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:06:55 | INFO | train_inner | epoch 135:     24 / 64 loss=5.489, ppl=44.92, wps=5769.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8600, lr=0.000340997, gnorm=0.872, train_wall=508, gb_free=6.1, wall=47637
2022-01-31 22:10:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:10:46 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 10.029 | ppl 1044.72 | wps 7851.9 | wpb 2034.1 | bsz 4 | num_updates 8640 | best_loss 9.246
2022-01-31 22:10:46 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-01-31 22:10:46 | INFO | train | epoch 135 | loss 5.481 | ppl 44.65 | wps 5899.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8640 | lr 0.000340207 | gnorm 0.859 | train_wall 325 | gb_free 6.1 | wall 47868
KL Stats: Epoch 135 Divergences: Uniform: 3.111670229840268 Unigram: 3.5685190033817418
2022-01-31 22:10:46 | INFO | fairseq.trainer | begin training epoch 136
2022-01-31 22:10:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:15:53 | INFO | train_inner | epoch 136:     60 / 64 loss=5.481, ppl=44.67, wps=6071.3, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=8700, lr=0.000339032, gnorm=0.867, train_wall=509, gb_free=6.1, wall=48175
2022-01-31 22:16:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:16:40 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 10.076 | ppl 1079.26 | wps 7725.1 | wpb 2034.1 | bsz 4 | num_updates 8704 | best_loss 9.246
2022-01-31 22:16:40 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-01-31 22:16:40 | INFO | train | epoch 136 | loss 5.472 | ppl 44.39 | wps 5895.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8704 | lr 0.000338954 | gnorm 0.872 | train_wall 325 | gb_free 6.1 | wall 48222
KL Stats: Epoch 136 Divergences: Uniform: 3.1115972082508017 Unigram: 3.5743016246367283
2022-01-31 22:16:40 | INFO | fairseq.trainer | begin training epoch 137
2022-01-31 22:16:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:22:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:22:35 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 10.121 | ppl 1113.42 | wps 7880.2 | wpb 2034.1 | bsz 4 | num_updates 8768 | best_loss 9.246
2022-01-31 22:22:35 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-01-31 22:22:35 | INFO | train | epoch 137 | loss 5.465 | ppl 44.16 | wps 5887.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8768 | lr 0.000337715 | gnorm 0.887 | train_wall 326 | gb_free 6.1 | wall 48577
KL Stats: Epoch 137 Divergences: Uniform: 3.114532469221607 Unigram: 3.5864768893889094
2022-01-31 22:22:35 | INFO | fairseq.trainer | begin training epoch 138
2022-01-31 22:22:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:25:19 | INFO | train_inner | epoch 138:     32 / 64 loss=5.455, ppl=43.87, wps=5761.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8800, lr=0.0003371, gnorm=0.875, train_wall=508, gb_free=6.1, wall=48741
2022-01-31 22:28:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:28:29 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 10.124 | ppl 1115.96 | wps 7855.2 | wpb 2034.1 | bsz 4 | num_updates 8832 | best_loss 9.246
2022-01-31 22:28:29 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-01-31 22:28:29 | INFO | train | epoch 138 | loss 5.454 | ppl 43.83 | wps 5900.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8832 | lr 0.000336489 | gnorm 0.889 | train_wall 325 | gb_free 6.1 | wall 48931
KL Stats: Epoch 138 Divergences: Uniform: 3.117839864238971 Unigram: 3.5910906533479583
2022-01-31 22:28:29 | INFO | fairseq.trainer | begin training epoch 139
2022-01-31 22:28:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:33:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:34:23 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 10.07 | ppl 1074.55 | wps 7910.7 | wpb 2034.1 | bsz 4 | num_updates 8896 | best_loss 9.246
2022-01-31 22:34:23 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-01-31 22:34:23 | INFO | train | epoch 139 | loss 5.445 | ppl 43.55 | wps 5896.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8896 | lr 0.000335276 | gnorm 0.895 | train_wall 326 | gb_free 6.1 | wall 49285
KL Stats: Epoch 139 Divergences: Uniform: 3.1199963792292107 Unigram: 3.6010009246670402
2022-01-31 22:34:23 | INFO | fairseq.trainer | begin training epoch 140
2022-01-31 22:34:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:34:43 | INFO | train_inner | epoch 140:      4 / 64 loss=5.453, ppl=43.79, wps=5775.5, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=8900, lr=0.000335201, gnorm=0.903, train_wall=508, gb_free=6.1, wall=49306
2022-01-31 22:39:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:40:17 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 10.085 | ppl 1086.12 | wps 7850 | wpb 2034.1 | bsz 4 | num_updates 8960 | best_loss 9.246
2022-01-31 22:40:17 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-01-31 22:40:17 | INFO | train | epoch 140 | loss 5.437 | ppl 43.31 | wps 5893.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8960 | lr 0.000334077 | gnorm 0.907 | train_wall 326 | gb_free 6.1 | wall 49640
KL Stats: Epoch 140 Divergences: Uniform: 3.114179465805643 Unigram: 3.6032917672033262
2022-01-31 22:40:17 | INFO | fairseq.trainer | begin training epoch 141
2022-01-31 22:40:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:43:42 | INFO | train_inner | epoch 141:     40 / 64 loss=5.429, ppl=43.09, wps=6063.5, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9000, lr=0.000333333, gnorm=0.913, train_wall=510, gb_free=6.1, wall=49844
2022-01-31 22:45:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:46:12 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 10.13 | ppl 1120.89 | wps 7736.5 | wpb 2034.1 | bsz 4 | num_updates 9024 | best_loss 9.246
2022-01-31 22:46:12 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-01-31 22:46:12 | INFO | train | epoch 141 | loss 5.429 | ppl 43.08 | wps 5898.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9024 | lr 0.00033289 | gnorm 0.9 | train_wall 325 | gb_free 6.1 | wall 49994
KL Stats: Epoch 141 Divergences: Uniform: 3.1216654506620705 Unigram: 3.6082183314703116
2022-01-31 22:46:12 | INFO | fairseq.trainer | begin training epoch 142
2022-01-31 22:46:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:51:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:52:06 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 10.069 | ppl 1073.79 | wps 7887 | wpb 2034.1 | bsz 4 | num_updates 9088 | best_loss 9.246
2022-01-31 22:52:06 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-01-31 22:52:06 | INFO | train | epoch 142 | loss 5.419 | ppl 42.79 | wps 5895.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9088 | lr 0.000331716 | gnorm 0.915 | train_wall 326 | gb_free 6.1 | wall 50348
KL Stats: Epoch 142 Divergences: Uniform: 3.1228926772283967 Unigram: 3.6120658966365835
2022-01-31 22:52:06 | INFO | fairseq.trainer | begin training epoch 143
2022-01-31 22:52:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:53:07 | INFO | train_inner | epoch 143:     12 / 64 loss=5.421, ppl=42.84, wps=5775.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=9100, lr=0.000331497, gnorm=0.907, train_wall=507, gb_free=6.1, wall=50409
2022-01-31 22:57:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:58:00 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 10.074 | ppl 1077.97 | wps 7833.1 | wpb 2034.1 | bsz 4 | num_updates 9152 | best_loss 9.246
2022-01-31 22:58:00 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-01-31 22:58:00 | INFO | train | epoch 143 | loss 5.414 | ppl 42.62 | wps 5891.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9152 | lr 0.000330554 | gnorm 0.917 | train_wall 326 | gb_free 6.1 | wall 50702
KL Stats: Epoch 143 Divergences: Uniform: 3.13257279959034 Unigram: 3.6190608785885785
2022-01-31 22:58:00 | INFO | fairseq.trainer | begin training epoch 144
2022-01-31 22:58:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:02:06 | INFO | train_inner | epoch 144:     48 / 64 loss=5.41, ppl=42.51, wps=6061.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9200, lr=0.00032969, gnorm=0.91, train_wall=510, gb_free=6.1, wall=50948
2022-01-31 23:03:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:03:54 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 10.129 | ppl 1119.96 | wps 7915.9 | wpb 2034.1 | bsz 4 | num_updates 9216 | best_loss 9.246
2022-01-31 23:03:54 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-01-31 23:03:54 | INFO | train | epoch 144 | loss 5.406 | ppl 42.39 | wps 5903.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9216 | lr 0.000329404 | gnorm 0.907 | train_wall 325 | gb_free 6.1 | wall 51056
KL Stats: Epoch 144 Divergences: Uniform: 3.122829273442295 Unigram: 3.620045999924024
2022-01-31 23:03:54 | INFO | fairseq.trainer | begin training epoch 145
2022-01-31 23:03:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:09:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:09:49 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 10.148 | ppl 1134.8 | wps 7844.7 | wpb 2034.1 | bsz 4 | num_updates 9280 | best_loss 9.246
2022-01-31 23:09:49 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-01-31 23:09:49 | INFO | train | epoch 145 | loss 5.398 | ppl 42.17 | wps 5884.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9280 | lr 0.000328266 | gnorm 0.937 | train_wall 326 | gb_free 6.1 | wall 51411
KL Stats: Epoch 145 Divergences: Uniform: 3.133467826428437 Unigram: 3.6309601271147205
2022-01-31 23:09:49 | INFO | fairseq.trainer | begin training epoch 146
2022-01-31 23:09:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:11:31 | INFO | train_inner | epoch 146:     20 / 64 loss=5.394, ppl=42.06, wps=5770, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=9300, lr=0.000327913, gnorm=0.933, train_wall=508, gb_free=6.1, wall=51513
2022-01-31 23:15:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:15:44 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 10.106 | ppl 1102.12 | wps 7798.5 | wpb 2034.1 | bsz 4 | num_updates 9344 | best_loss 9.246
2022-01-31 23:15:44 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-01-31 23:15:44 | INFO | train | epoch 146 | loss 5.388 | ppl 41.88 | wps 5887.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9344 | lr 0.00032714 | gnorm 0.945 | train_wall 326 | gb_free 6.1 | wall 51766
KL Stats: Epoch 146 Divergences: Uniform: 3.1301713620510516 Unigram: 3.635953028467672
2022-01-31 23:15:44 | INFO | fairseq.trainer | begin training epoch 147
2022-01-31 23:15:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:20:31 | INFO | train_inner | epoch 147:     56 / 64 loss=5.39, ppl=41.93, wps=6056.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9400, lr=0.000326164, gnorm=0.942, train_wall=510, gb_free=6.1, wall=52053
2022-01-31 23:21:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:21:38 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 10.126 | ppl 1117.28 | wps 7838.1 | wpb 2034.1 | bsz 4 | num_updates 9408 | best_loss 9.246
2022-01-31 23:21:38 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-01-31 23:21:38 | INFO | train | epoch 147 | loss 5.381 | ppl 41.67 | wps 5891.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9408 | lr 0.000326025 | gnorm 0.943 | train_wall 326 | gb_free 6.1 | wall 52120
KL Stats: Epoch 147 Divergences: Uniform: 3.1374342874373746 Unigram: 3.64663195049186
2022-01-31 23:21:38 | INFO | fairseq.trainer | begin training epoch 148
2022-01-31 23:21:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:27:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:27:33 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 10.11 | ppl 1105.44 | wps 7976 | wpb 2034.1 | bsz 4 | num_updates 9472 | best_loss 9.246
2022-01-31 23:27:33 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-01-31 23:27:33 | INFO | train | epoch 148 | loss 5.373 | ppl 41.45 | wps 5884.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9472 | lr 0.000324922 | gnorm 0.941 | train_wall 327 | gb_free 6.1 | wall 52475
KL Stats: Epoch 148 Divergences: Uniform: 3.1352670913371203 Unigram: 3.650489253887747
2022-01-31 23:27:33 | INFO | fairseq.trainer | begin training epoch 149
2022-01-31 23:27:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:29:57 | INFO | train_inner | epoch 149:     28 / 64 loss=5.368, ppl=41.31, wps=5751.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=9500, lr=0.000324443, gnorm=0.943, train_wall=510, gb_free=6.1, wall=52620
2022-01-31 23:33:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:33:29 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 10.123 | ppl 1114.99 | wps 7853.4 | wpb 2034.1 | bsz 4 | num_updates 9536 | best_loss 9.246
2022-01-31 23:33:29 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-01-31 23:33:29 | INFO | train | epoch 149 | loss 5.368 | ppl 41.29 | wps 5866.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9536 | lr 0.00032383 | gnorm 0.975 | train_wall 327 | gb_free 6.1 | wall 52831
KL Stats: Epoch 149 Divergences: Uniform: 3.1419838030392255 Unigram: 3.6569394030361453
2022-01-31 23:33:29 | INFO | fairseq.trainer | begin training epoch 150
2022-01-31 23:33:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:38:57 | INFO | train_inner | epoch 150:     64 / 64 loss=5.37, ppl=41.34, wps=6043, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=9600, lr=0.000322749, gnorm=0.981, train_wall=510, gb_free=6.1, wall=53159
2022-01-31 23:38:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:39:25 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 10.162 | ppl 1145.35 | wps 7851 | wpb 2034.1 | bsz 4 | num_updates 9600 | best_loss 9.246
2022-01-31 23:39:25 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-01-31 23:39:25 | INFO | train | epoch 150 | loss 5.359 | ppl 41.03 | wps 5876 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9600 | lr 0.000322749 | gnorm 0.967 | train_wall 327 | gb_free 6.1 | wall 53187
KL Stats: Epoch 150 Divergences: Uniform: 3.139678197175833 Unigram: 3.6605557191123235
2022-01-31 23:39:25 | INFO | fairseq.trainer | begin training epoch 151
2022-01-31 23:39:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:44:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:45:20 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 10.176 | ppl 1156.71 | wps 7858.4 | wpb 2034.1 | bsz 4 | num_updates 9664 | best_loss 9.246
2022-01-31 23:45:20 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-01-31 23:45:20 | INFO | train | epoch 151 | loss 5.351 | ppl 40.82 | wps 5875.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9664 | lr 0.000321678 | gnorm 0.969 | train_wall 327 | gb_free 6.1 | wall 53542
KL Stats: Epoch 151 Divergences: Uniform: 3.138224029879648 Unigram: 3.665370863699881
2022-01-31 23:45:20 | INFO | fairseq.trainer | begin training epoch 152
2022-01-31 23:45:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:48:25 | INFO | train_inner | epoch 152:     36 / 64 loss=5.339, ppl=40.47, wps=5752.7, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=9700, lr=0.000321081, gnorm=0.97, train_wall=511, gb_free=6.1, wall=53727
2022-01-31 23:50:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:51:15 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 10.146 | ppl 1132.94 | wps 7846.5 | wpb 2034.1 | bsz 4 | num_updates 9728 | best_loss 9.246
2022-01-31 23:51:15 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-01-31 23:51:15 | INFO | train | epoch 152 | loss 5.344 | ppl 40.62 | wps 5887.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9728 | lr 0.000320618 | gnorm 0.975 | train_wall 326 | gb_free 6.1 | wall 53897
KL Stats: Epoch 152 Divergences: Uniform: 3.1477865279646804 Unigram: 3.6749283161578155
2022-01-31 23:51:15 | INFO | fairseq.trainer | begin training epoch 153
2022-01-31 23:51:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:56:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:57:08 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 10.137 | ppl 1125.81 | wps 7908.7 | wpb 2034.1 | bsz 4 | num_updates 9792 | best_loss 9.246
2022-01-31 23:57:08 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-01-31 23:57:08 | INFO | train | epoch 153 | loss 5.336 | ppl 40.39 | wps 5911.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9792 | lr 0.000319569 | gnorm 0.954 | train_wall 325 | gb_free 6.1 | wall 54250
KL Stats: Epoch 153 Divergences: Uniform: 3.1489724360515527 Unigram: 3.6805198993674395
2022-01-31 23:57:08 | INFO | fairseq.trainer | begin training epoch 154
2022-01-31 23:57:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:57:50 | INFO | train_inner | epoch 154:      8 / 64 loss=5.344, ppl=40.61, wps=5774, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=9800, lr=0.000319438, gnorm=0.961, train_wall=508, gb_free=6.1, wall=54292
2022-02-01 00:02:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:03:03 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 10.194 | ppl 1171.08 | wps 7856.5 | wpb 2034.1 | bsz 4 | num_updates 9856 | best_loss 9.246
2022-02-01 00:03:03 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-02-01 00:03:03 | INFO | train | epoch 154 | loss 5.332 | ppl 40.27 | wps 5883.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9856 | lr 0.000318529 | gnorm 0.997 | train_wall 326 | gb_free 6.1 | wall 54605
KL Stats: Epoch 154 Divergences: Uniform: 3.1526098467221115 Unigram: 3.6799221807091276
2022-02-01 00:03:03 | INFO | fairseq.trainer | begin training epoch 155
2022-02-01 00:03:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:06:48 | INFO | train_inner | epoch 155:     44 / 64 loss=5.323, ppl=40.02, wps=6064, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=9900, lr=0.000317821, gnorm=0.996, train_wall=510, gb_free=6.1, wall=54831
2022-02-01 00:08:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:08:57 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 10.185 | ppl 1164.28 | wps 7857.4 | wpb 2034.1 | bsz 4 | num_updates 9920 | best_loss 9.246
2022-02-01 00:08:57 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-02-01 00:08:57 | INFO | train | epoch 155 | loss 5.324 | ppl 40.06 | wps 5902.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9920 | lr 0.0003175 | gnorm 0.998 | train_wall 325 | gb_free 6.1 | wall 54959
KL Stats: Epoch 155 Divergences: Uniform: 3.144106154752952 Unigram: 3.6947574393250813
2022-02-01 00:08:57 | INFO | fairseq.trainer | begin training epoch 156
2022-02-01 00:08:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:14:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:14:51 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 10.157 | ppl 1142.04 | wps 7834.8 | wpb 2034.1 | bsz 4 | num_updates 9984 | best_loss 9.246
2022-02-01 00:14:51 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-02-01 00:14:51 | INFO | train | epoch 156 | loss 5.318 | ppl 39.9 | wps 5899.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9984 | lr 0.000316481 | gnorm 1.012 | train_wall 325 | gb_free 6.1 | wall 55313
KL Stats: Epoch 156 Divergences: Uniform: 3.1490253064561515 Unigram: 3.6845379121241124
2022-02-01 00:14:51 | INFO | fairseq.trainer | begin training epoch 157
2022-02-01 00:14:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:16:13 | INFO | train_inner | epoch 157:     16 / 64 loss=5.324, ppl=40.05, wps=5776.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10000, lr=0.000316228, gnorm=1.011, train_wall=507, gb_free=6.1, wall=55395
2022-02-01 00:20:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:20:44 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 10.128 | ppl 1119.04 | wps 7857.6 | wpb 2034.1 | bsz 4 | num_updates 10048 | best_loss 9.246
2022-02-01 00:20:44 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-02-01 00:20:44 | INFO | train | epoch 157 | loss 5.31 | ppl 39.68 | wps 5914 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10048 | lr 0.000315472 | gnorm 1.017 | train_wall 324 | gb_free 6.1 | wall 55666
KL Stats: Epoch 157 Divergences: Uniform: 3.1475109914330126 Unigram: 3.6988670225943454
2022-02-01 00:20:44 | INFO | fairseq.trainer | begin training epoch 158
2022-02-01 00:20:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:25:10 | INFO | train_inner | epoch 158:     52 / 64 loss=5.302, ppl=39.46, wps=6082.4, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=10100, lr=0.000314658, gnorm=0.99, train_wall=508, gb_free=6.1, wall=55932
2022-02-01 00:26:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:26:38 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 10.212 | ppl 1186.47 | wps 7898.5 | wpb 2034.1 | bsz 4 | num_updates 10112 | best_loss 9.246
2022-02-01 00:26:38 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-02-01 00:26:38 | INFO | train | epoch 158 | loss 5.302 | ppl 39.46 | wps 5904.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10112 | lr 0.000314472 | gnorm 0.967 | train_wall 325 | gb_free 6.1 | wall 56020
KL Stats: Epoch 158 Divergences: Uniform: 3.1480411072598526 Unigram: 3.7040085168187478
2022-02-01 00:26:38 | INFO | fairseq.trainer | begin training epoch 159
2022-02-01 00:26:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:32:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:32:32 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 10.23 | ppl 1201.39 | wps 7855.6 | wpb 2034.1 | bsz 4 | num_updates 10176 | best_loss 9.246
2022-02-01 00:32:32 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-02-01 00:32:32 | INFO | train | epoch 159 | loss 5.296 | ppl 39.29 | wps 5896.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10176 | lr 0.000313481 | gnorm 1.015 | train_wall 325 | gb_free 6.1 | wall 56374
KL Stats: Epoch 159 Divergences: Uniform: 3.1430791221191976 Unigram: 3.7161618646282224
2022-02-01 00:32:32 | INFO | fairseq.trainer | begin training epoch 160
2022-02-01 00:32:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:34:35 | INFO | train_inner | epoch 160:     24 / 64 loss=5.296, ppl=39.29, wps=5768.1, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=10200, lr=0.000313112, gnorm=1.013, train_wall=508, gb_free=6.1, wall=56497
2022-02-01 00:37:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:38:27 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 10.195 | ppl 1172.52 | wps 7854.7 | wpb 2034.1 | bsz 4 | num_updates 10240 | best_loss 9.246
2022-02-01 00:38:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 10240 updates
2022-02-01 00:38:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint160.pt
2022-02-01 00:38:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint160.pt
2022-02-01 00:38:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint160.pt (epoch 160 @ 10240 updates, score 10.195) (writing took 3.5066676074638963 seconds)
2022-02-01 00:38:30 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-02-01 00:38:30 | INFO | train | epoch 160 | loss 5.292 | ppl 39.18 | wps 5831 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10240 | lr 0.0003125 | gnorm 1.032 | train_wall 326 | gb_free 6.1 | wall 56733
KL Stats: Epoch 160 Divergences: Uniform: 3.1510534993394765 Unigram: 3.71617796872914
2022-02-01 00:38:30 | INFO | fairseq.trainer | begin training epoch 161
2022-02-01 00:38:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:43:38 | INFO | train_inner | epoch 161:     60 / 64 loss=5.293, ppl=39.2, wps=6017.6, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=10300, lr=0.000311588, gnorm=1.031, train_wall=510, gb_free=6.1, wall=57041
2022-02-01 00:43:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:44:25 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 10.156 | ppl 1141.02 | wps 7855.8 | wpb 2034.1 | bsz 4 | num_updates 10304 | best_loss 9.246
2022-02-01 00:44:25 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-02-01 00:44:25 | INFO | train | epoch 161 | loss 5.285 | ppl 38.99 | wps 5885.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10304 | lr 0.000311528 | gnorm 1.021 | train_wall 326 | gb_free 6.1 | wall 57087
KL Stats: Epoch 161 Divergences: Uniform: 3.158335517577685 Unigram: 3.72012645617306
2022-02-01 00:44:25 | INFO | fairseq.trainer | begin training epoch 162
2022-02-01 00:44:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:49:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:50:21 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 10.195 | ppl 1172.35 | wps 7871.5 | wpb 2034.1 | bsz 4 | num_updates 10368 | best_loss 9.246
2022-02-01 00:50:21 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-02-01 00:50:21 | INFO | train | epoch 162 | loss 5.277 | ppl 38.78 | wps 5875.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10368 | lr 0.000310565 | gnorm 1.03 | train_wall 327 | gb_free 6.1 | wall 57443
KL Stats: Epoch 162 Divergences: Uniform: 3.161009652107703 Unigram: 3.726535426527989
2022-02-01 00:50:21 | INFO | fairseq.trainer | begin training epoch 163
2022-02-01 00:50:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:53:05 | INFO | train_inner | epoch 163:     32 / 64 loss=5.264, ppl=38.44, wps=5751.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10400, lr=0.000310087, gnorm=1.026, train_wall=510, gb_free=6.1, wall=57607
2022-02-01 00:55:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:56:16 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 10.261 | ppl 1226.92 | wps 7786.5 | wpb 2034.1 | bsz 4 | num_updates 10432 | best_loss 9.246
2022-02-01 00:56:16 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-02-01 00:56:16 | INFO | train | epoch 163 | loss 5.273 | ppl 38.68 | wps 5873.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10432 | lr 0.000309611 | gnorm 1.032 | train_wall 327 | gb_free 6.1 | wall 57798
KL Stats: Epoch 163 Divergences: Uniform: 3.15149764108806 Unigram: 3.7293601410684194
2022-02-01 00:56:16 | INFO | fairseq.trainer | begin training epoch 164
2022-02-01 00:56:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:01:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:02:11 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 10.168 | ppl 1150.78 | wps 7852.9 | wpb 2034.1 | bsz 4 | num_updates 10496 | best_loss 9.246
2022-02-01 01:02:11 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-02-01 01:02:11 | INFO | train | epoch 164 | loss 5.266 | ppl 38.49 | wps 5891.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10496 | lr 0.000308665 | gnorm 1.053 | train_wall 326 | gb_free 6.1 | wall 58153
KL Stats: Epoch 164 Divergences: Uniform: 3.1513544582971402 Unigram: 3.733933988910646
2022-02-01 01:02:11 | INFO | fairseq.trainer | begin training epoch 165
2022-02-01 01:02:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:02:31 | INFO | train_inner | epoch 165:      4 / 64 loss=5.281, ppl=38.88, wps=5758, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=10500, lr=0.000308607, gnorm=1.05, train_wall=509, gb_free=6.1, wall=58173
2022-02-01 01:07:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:08:05 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 10.201 | ppl 1176.75 | wps 7828.3 | wpb 2034.1 | bsz 4 | num_updates 10560 | best_loss 9.246
2022-02-01 01:08:05 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-02-01 01:08:05 | INFO | train | epoch 165 | loss 5.258 | ppl 38.27 | wps 5904.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10560 | lr 0.000307729 | gnorm 1.054 | train_wall 325 | gb_free 6.1 | wall 58507
KL Stats: Epoch 165 Divergences: Uniform: 3.156471710547944 Unigram: 3.741887934304215
2022-02-01 01:08:05 | INFO | fairseq.trainer | begin training epoch 166
2022-02-01 01:08:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:11:29 | INFO | train_inner | epoch 166:     40 / 64 loss=5.251, ppl=38.09, wps=6073.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=10600, lr=0.000307148, gnorm=1.073, train_wall=509, gb_free=6.1, wall=58712
2022-02-01 01:13:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:13:59 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 10.114 | ppl 1107.97 | wps 7863.5 | wpb 2034.1 | bsz 4 | num_updates 10624 | best_loss 9.246
2022-02-01 01:13:59 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-02-01 01:13:59 | INFO | train | epoch 166 | loss 5.254 | ppl 38.16 | wps 5889.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10624 | lr 0.0003068 | gnorm 1.079 | train_wall 326 | gb_free 6.1 | wall 58861
KL Stats: Epoch 166 Divergences: Uniform: 3.159389989195111 Unigram: 3.7476906918230664
2022-02-01 01:13:59 | INFO | fairseq.trainer | begin training epoch 167
2022-02-01 01:13:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:19:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:19:54 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 10.139 | ppl 1127.82 | wps 7841.2 | wpb 2034.1 | bsz 4 | num_updates 10688 | best_loss 9.246
2022-02-01 01:19:54 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-02-01 01:19:54 | INFO | train | epoch 167 | loss 5.247 | ppl 37.98 | wps 5881.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10688 | lr 0.00030588 | gnorm 1.065 | train_wall 326 | gb_free 6.1 | wall 59216
KL Stats: Epoch 167 Divergences: Uniform: 3.1548258114326706 Unigram: 3.7444339634522406
2022-02-01 01:19:54 | INFO | fairseq.trainer | begin training epoch 168
2022-02-01 01:19:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:20:56 | INFO | train_inner | epoch 168:     12 / 64 loss=5.249, ppl=38.03, wps=5754.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10700, lr=0.000305709, gnorm=1.055, train_wall=509, gb_free=6.1, wall=59278
2022-02-01 01:25:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:25:49 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 10.22 | ppl 1192.55 | wps 7781.7 | wpb 2034.1 | bsz 4 | num_updates 10752 | best_loss 9.246
2022-02-01 01:25:49 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-02-01 01:25:49 | INFO | train | epoch 168 | loss 5.241 | ppl 37.82 | wps 5882.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10752 | lr 0.000304969 | gnorm 1.056 | train_wall 326 | gb_free 6.1 | wall 59572
KL Stats: Epoch 168 Divergences: Uniform: 3.16180785549941 Unigram: 3.7506524654054876
2022-02-01 01:25:49 | INFO | fairseq.trainer | begin training epoch 169
2022-02-01 01:25:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:29:55 | INFO | train_inner | epoch 169:     48 / 64 loss=5.241, ppl=37.81, wps=6061.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=10800, lr=0.00030429, gnorm=1.076, train_wall=510, gb_free=6.1, wall=59817
2022-02-01 01:31:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:31:44 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 10.181 | ppl 1160.99 | wps 7865.9 | wpb 2034.1 | bsz 4 | num_updates 10816 | best_loss 9.246
2022-02-01 01:31:44 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-02-01 01:31:44 | INFO | train | epoch 169 | loss 5.236 | ppl 37.68 | wps 5897.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10816 | lr 0.000304065 | gnorm 1.082 | train_wall 325 | gb_free 6.1 | wall 59926
KL Stats: Epoch 169 Divergences: Uniform: 3.16005977058326 Unigram: 3.7566339754814897
2022-02-01 01:31:44 | INFO | fairseq.trainer | begin training epoch 170
2022-02-01 01:31:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:37:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:37:38 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 10.22 | ppl 1192.61 | wps 7832.7 | wpb 2034.1 | bsz 4 | num_updates 10880 | best_loss 9.246
2022-02-01 01:37:38 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-02-01 01:37:38 | INFO | train | epoch 170 | loss 5.232 | ppl 37.57 | wps 5886.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10880 | lr 0.00030317 | gnorm 1.119 | train_wall 326 | gb_free 6.1 | wall 60280
KL Stats: Epoch 170 Divergences: Uniform: 3.167935037422434 Unigram: 3.7641906196468944
2022-02-01 01:37:38 | INFO | fairseq.trainer | begin training epoch 171
2022-02-01 01:37:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:39:21 | INFO | train_inner | epoch 171:     20 / 64 loss=5.225, ppl=37.41, wps=5760.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10900, lr=0.000302891, gnorm=1.113, train_wall=509, gb_free=6.1, wall=60383
2022-02-01 01:43:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:43:33 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 10.207 | ppl 1182.25 | wps 7857.1 | wpb 2034.1 | bsz 4 | num_updates 10944 | best_loss 9.246
2022-02-01 01:43:33 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-02-01 01:43:33 | INFO | train | epoch 171 | loss 5.225 | ppl 37.41 | wps 5888.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10944 | lr 0.000302282 | gnorm 1.125 | train_wall 326 | gb_free 6.1 | wall 60635
KL Stats: Epoch 171 Divergences: Uniform: 3.162335560261489 Unigram: 3.7644736666283607
2022-02-01 01:43:33 | INFO | fairseq.trainer | begin training epoch 172
2022-02-01 01:43:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:48:20 | INFO | train_inner | epoch 172:     56 / 64 loss=5.227, ppl=37.46, wps=6064.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11000, lr=0.000301511, gnorm=1.099, train_wall=510, gb_free=6.1, wall=60922
2022-02-01 01:48:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:49:27 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 10.204 | ppl 1179.87 | wps 7874.1 | wpb 2034.1 | bsz 4 | num_updates 11008 | best_loss 9.246
2022-02-01 01:49:27 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-02-01 01:49:27 | INFO | train | epoch 172 | loss 5.218 | ppl 37.21 | wps 5898 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11008 | lr 0.000301402 | gnorm 1.094 | train_wall 325 | gb_free 6.1 | wall 60989
KL Stats: Epoch 172 Divergences: Uniform: 3.1693925541134025 Unigram: 3.7705039571668113
2022-02-01 01:49:27 | INFO | fairseq.trainer | begin training epoch 173
2022-02-01 01:49:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:54:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:55:22 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 10.167 | ppl 1149.27 | wps 7734.7 | wpb 2034.1 | bsz 4 | num_updates 11072 | best_loss 9.246
2022-02-01 01:55:22 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-02-01 01:55:22 | INFO | train | epoch 173 | loss 5.214 | ppl 37.13 | wps 5883.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11072 | lr 0.000300529 | gnorm 1.147 | train_wall 326 | gb_free 6.1 | wall 61344
KL Stats: Epoch 173 Divergences: Uniform: 3.167246454089047 Unigram: 3.7749430577701006
2022-02-01 01:55:22 | INFO | fairseq.trainer | begin training epoch 174
2022-02-01 01:55:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:57:46 | INFO | train_inner | epoch 174:     28 / 64 loss=5.208, ppl=36.97, wps=5757.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=11100, lr=0.00030015, gnorm=1.139, train_wall=509, gb_free=6.1, wall=61488
2022-02-01 02:00:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:01:18 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 10.221 | ppl 1193.88 | wps 7868.6 | wpb 2034.1 | bsz 4 | num_updates 11136 | best_loss 9.246
2022-02-01 02:01:18 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-02-01 02:01:18 | INFO | train | epoch 174 | loss 5.209 | ppl 37 | wps 5877.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11136 | lr 0.000299665 | gnorm 1.133 | train_wall 327 | gb_free 6.1 | wall 61700
KL Stats: Epoch 174 Divergences: Uniform: 3.1646548739762186 Unigram: 3.783897610701194
2022-02-01 02:01:18 | INFO | fairseq.trainer | begin training epoch 175
2022-02-01 02:01:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:06:44 | INFO | train_inner | epoch 175:     64 / 64 loss=5.213, ppl=37.09, wps=6059.2, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=11200, lr=0.000298807, gnorm=1.136, train_wall=509, gb_free=6.1, wall=62026
2022-02-01 02:06:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:07:12 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 10.246 | ppl 1214.58 | wps 7875.2 | wpb 2034.1 | bsz 4 | num_updates 11200 | best_loss 9.246
2022-02-01 02:07:12 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-02-01 02:07:12 | INFO | train | epoch 175 | loss 5.203 | ppl 36.82 | wps 5893.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11200 | lr 0.000298807 | gnorm 1.128 | train_wall 326 | gb_free 6.1 | wall 62054
KL Stats: Epoch 175 Divergences: Uniform: 3.1747484809284576 Unigram: 3.7846221077246396
2022-02-01 02:07:12 | INFO | fairseq.trainer | begin training epoch 176
2022-02-01 02:07:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:12:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:13:06 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 10.231 | ppl 1201.99 | wps 7861.3 | wpb 2034.1 | bsz 4 | num_updates 11264 | best_loss 9.246
2022-02-01 02:13:06 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-02-01 02:13:06 | INFO | train | epoch 176 | loss 5.198 | ppl 36.71 | wps 5895.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11264 | lr 0.000297957 | gnorm 1.136 | train_wall 326 | gb_free 6.1 | wall 62408
KL Stats: Epoch 176 Divergences: Uniform: 3.1697322154013357 Unigram: 3.7887824058956006
2022-02-01 02:13:06 | INFO | fairseq.trainer | begin training epoch 177
2022-02-01 02:13:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:16:11 | INFO | train_inner | epoch 177:     36 / 64 loss=5.185, ppl=36.38, wps=5761, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=11300, lr=0.000297482, gnorm=1.127, train_wall=510, gb_free=6.1, wall=62594
2022-02-01 02:18:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:19:01 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 10.224 | ppl 1196.03 | wps 7852.5 | wpb 2034.1 | bsz 4 | num_updates 11328 | best_loss 9.246
2022-02-01 02:19:01 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-02-01 02:19:01 | INFO | train | epoch 177 | loss 5.192 | ppl 36.56 | wps 5883 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11328 | lr 0.000297114 | gnorm 1.13 | train_wall 326 | gb_free 6.1 | wall 62763
KL Stats: Epoch 177 Divergences: Uniform: 3.170895565471488 Unigram: 3.7928213436128404
2022-02-01 02:19:01 | INFO | fairseq.trainer | begin training epoch 178
2022-02-01 02:19:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:24:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:24:55 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 10.23 | ppl 1201.39 | wps 7725.6 | wpb 2034.1 | bsz 4 | num_updates 11392 | best_loss 9.246
2022-02-01 02:24:55 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-02-01 02:24:55 | INFO | train | epoch 178 | loss 5.192 | ppl 36.54 | wps 5905.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11392 | lr 0.000296278 | gnorm 1.203 | train_wall 325 | gb_free 6.1 | wall 63117
KL Stats: Epoch 178 Divergences: Uniform: 3.1791057690376183 Unigram: 3.7987952126538125
2022-02-01 02:24:55 | INFO | fairseq.trainer | begin training epoch 179
2022-02-01 02:24:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:25:36 | INFO | train_inner | epoch 179:      8 / 64 loss=5.198, ppl=36.71, wps=5773.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=11400, lr=0.000296174, gnorm=1.185, train_wall=507, gb_free=6.1, wall=63158
2022-02-01 02:30:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:30:51 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 10.187 | ppl 1165.68 | wps 7830 | wpb 2034.1 | bsz 4 | num_updates 11456 | best_loss 9.246
2022-02-01 02:30:51 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-02-01 02:30:51 | INFO | train | epoch 179 | loss 5.182 | ppl 36.29 | wps 5869.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11456 | lr 0.00029545 | gnorm 1.16 | train_wall 327 | gb_free 6.1 | wall 63473
KL Stats: Epoch 179 Divergences: Uniform: 3.179375766808332 Unigram: 3.8035008846999165
2022-02-01 02:30:51 | INFO | fairseq.trainer | begin training epoch 180
2022-02-01 02:30:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:34:36 | INFO | train_inner | epoch 180:     44 / 64 loss=5.177, ppl=36.18, wps=6048.6, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11500, lr=0.000294884, gnorm=1.155, train_wall=511, gb_free=6.1, wall=63698
2022-02-01 02:36:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:36:45 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 10.172 | ppl 1153.89 | wps 8043.9 | wpb 2034.1 | bsz 4 | num_updates 11520 | best_loss 9.246
2022-02-01 02:36:45 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-02-01 02:36:45 | INFO | train | epoch 180 | loss 5.178 | ppl 36.19 | wps 5897.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11520 | lr 0.000294628 | gnorm 1.17 | train_wall 326 | gb_free 6.1 | wall 63827
KL Stats: Epoch 180 Divergences: Uniform: 3.1717641813935993 Unigram: 3.8072895474609565
2022-02-01 02:36:45 | INFO | fairseq.trainer | begin training epoch 181
2022-02-01 02:36:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:42:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:42:39 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 10.19 | ppl 1168.22 | wps 7848 | wpb 2034.1 | bsz 4 | num_updates 11584 | best_loss 9.246
2022-02-01 02:42:39 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-02-01 02:42:39 | INFO | train | epoch 181 | loss 5.17 | ppl 36.01 | wps 5896.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11584 | lr 0.000293813 | gnorm 1.16 | train_wall 325 | gb_free 6.1 | wall 64181
KL Stats: Epoch 181 Divergences: Uniform: 3.177378616595448 Unigram: 3.8118337992292695
2022-02-01 02:42:39 | INFO | fairseq.trainer | begin training epoch 182
2022-02-01 02:42:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:44:01 | INFO | train_inner | epoch 182:     16 / 64 loss=5.171, ppl=36.03, wps=5775.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=11600, lr=0.00029361, gnorm=1.167, train_wall=508, gb_free=6.1, wall=64263
2022-02-01 02:48:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:48:34 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 10.198 | ppl 1174.72 | wps 7831.7 | wpb 2034.1 | bsz 4 | num_updates 11648 | best_loss 9.246
2022-02-01 02:48:34 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-02-01 02:48:34 | INFO | train | epoch 182 | loss 5.169 | ppl 35.97 | wps 5887 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11648 | lr 0.000293005 | gnorm 1.193 | train_wall 326 | gb_free 6.1 | wall 64536
KL Stats: Epoch 182 Divergences: Uniform: 3.182958410519576 Unigram: 3.811672530912898
2022-02-01 02:48:34 | INFO | fairseq.trainer | begin training epoch 183
2022-02-01 02:48:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:53:01 | INFO | train_inner | epoch 183:     52 / 64 loss=5.166, ppl=35.89, wps=6049.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=11700, lr=0.000292353, gnorm=1.197, train_wall=511, gb_free=6.1, wall=64803
2022-02-01 02:54:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:54:29 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 10.206 | ppl 1181.2 | wps 7814.9 | wpb 2034.1 | bsz 4 | num_updates 11712 | best_loss 9.246
2022-02-01 02:54:29 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-02-01 02:54:29 | INFO | train | epoch 183 | loss 5.161 | ppl 35.77 | wps 5882.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11712 | lr 0.000292203 | gnorm 1.186 | train_wall 326 | gb_free 6.1 | wall 64891
KL Stats: Epoch 183 Divergences: Uniform: 3.180219092794487 Unigram: 3.8182755778008337
2022-02-01 02:54:29 | INFO | fairseq.trainer | begin training epoch 184
2022-02-01 02:54:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:59:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:00:25 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 10.158 | ppl 1142.63 | wps 7843.4 | wpb 2034.1 | bsz 4 | num_updates 11776 | best_loss 9.246
2022-02-01 03:00:25 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-02-01 03:00:25 | INFO | train | epoch 184 | loss 5.159 | ppl 35.72 | wps 5875.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11776 | lr 0.000291408 | gnorm 1.237 | train_wall 327 | gb_free 6.1 | wall 65247
KL Stats: Epoch 184 Divergences: Uniform: 3.179069445919472 Unigram: 3.8201804522985467
2022-02-01 03:00:25 | INFO | fairseq.trainer | begin training epoch 185
2022-02-01 03:00:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:02:27 | INFO | train_inner | epoch 185:     24 / 64 loss=5.159, ppl=35.72, wps=5759.6, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=11800, lr=0.000291111, gnorm=1.225, train_wall=509, gb_free=6.1, wall=65369
2022-02-01 03:05:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:06:18 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 10.211 | ppl 1184.92 | wps 7989.5 | wpb 2034.1 | bsz 4 | num_updates 11840 | best_loss 9.246
2022-02-01 03:06:18 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-02-01 03:06:18 | INFO | train | epoch 185 | loss 5.154 | ppl 35.6 | wps 5903.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11840 | lr 0.000290619 | gnorm 1.195 | train_wall 326 | gb_free 6.1 | wall 65600
KL Stats: Epoch 185 Divergences: Uniform: 3.178993884690645 Unigram: 3.825198452683274
2022-02-01 03:06:18 | INFO | fairseq.trainer | begin training epoch 186
2022-02-01 03:06:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:11:26 | INFO | train_inner | epoch 186:     60 / 64 loss=5.154, ppl=35.61, wps=6067.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11900, lr=0.000289886, gnorm=1.214, train_wall=510, gb_free=6.1, wall=65908
2022-02-01 03:11:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:12:13 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 10.152 | ppl 1137.74 | wps 7860.4 | wpb 2034.1 | bsz 4 | num_updates 11904 | best_loss 9.246
2022-02-01 03:12:13 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-02-01 03:12:13 | INFO | train | epoch 186 | loss 5.151 | ppl 35.52 | wps 5895.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11904 | lr 0.000289837 | gnorm 1.235 | train_wall 326 | gb_free 6.1 | wall 65955
KL Stats: Epoch 186 Divergences: Uniform: 3.1775096567588226 Unigram: 3.8243792362865
2022-02-01 03:12:13 | INFO | fairseq.trainer | begin training epoch 187
2022-02-01 03:12:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:17:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:18:07 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 10.269 | ppl 1233.48 | wps 7855.6 | wpb 2034.1 | bsz 4 | num_updates 11968 | best_loss 9.246
2022-02-01 03:18:07 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-02-01 03:18:07 | INFO | train | epoch 187 | loss 5.144 | ppl 35.36 | wps 5890.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11968 | lr 0.000289061 | gnorm 1.215 | train_wall 326 | gb_free 6.1 | wall 66309
KL Stats: Epoch 187 Divergences: Uniform: 3.1799654653746448 Unigram: 3.834803896166196
2022-02-01 03:18:07 | INFO | fairseq.trainer | begin training epoch 188
2022-02-01 03:18:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:20:51 | INFO | train_inner | epoch 188:     32 / 64 loss=5.137, ppl=35.18, wps=5767.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12000, lr=0.000288675, gnorm=1.203, train_wall=508, gb_free=6.1, wall=66473
2022-02-01 03:23:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:24:01 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 10.219 | ppl 1191.62 | wps 7847.5 | wpb 2034.1 | bsz 4 | num_updates 12032 | best_loss 9.246
2022-02-01 03:24:01 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-02-01 03:24:01 | INFO | train | epoch 188 | loss 5.138 | ppl 35.2 | wps 5895.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12032 | lr 0.000288291 | gnorm 1.222 | train_wall 325 | gb_free 6.1 | wall 66664
KL Stats: Epoch 188 Divergences: Uniform: 3.1785500497221153 Unigram: 3.8384672724507514
2022-02-01 03:24:01 | INFO | fairseq.trainer | begin training epoch 189
2022-02-01 03:24:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:29:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:29:57 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 10.201 | ppl 1177.41 | wps 7838.1 | wpb 2034.1 | bsz 4 | num_updates 12096 | best_loss 9.246
2022-02-01 03:29:57 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-02-01 03:29:57 | INFO | train | epoch 189 | loss 5.134 | ppl 35.12 | wps 5880 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12096 | lr 0.000287527 | gnorm 1.268 | train_wall 326 | gb_free 6.1 | wall 67019
KL Stats: Epoch 189 Divergences: Uniform: 3.183505595973794 Unigram: 3.8421271093887484
2022-02-01 03:29:57 | INFO | fairseq.trainer | begin training epoch 190
2022-02-01 03:29:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:30:17 | INFO | train_inner | epoch 190:      4 / 64 loss=5.14, ppl=35.27, wps=5754.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=12100, lr=0.00028748, gnorm=1.268, train_wall=509, gb_free=6.1, wall=67040
2022-02-01 03:35:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:35:51 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 10.245 | ppl 1213.46 | wps 7966.4 | wpb 2034.1 | bsz 4 | num_updates 12160 | best_loss 9.246
2022-02-01 03:35:51 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-02-01 03:35:51 | INFO | train | epoch 190 | loss 5.129 | ppl 34.99 | wps 5899 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12160 | lr 0.00028677 | gnorm 1.235 | train_wall 326 | gb_free 6.1 | wall 67373
KL Stats: Epoch 190 Divergences: Uniform: 3.1838642796356837 Unigram: 3.8461705168458655
2022-02-01 03:35:51 | INFO | fairseq.trainer | begin training epoch 191
2022-02-01 03:35:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:39:15 | INFO | train_inner | epoch 191:     40 / 64 loss=5.119, ppl=34.74, wps=6074.9, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=12200, lr=0.000286299, gnorm=1.251, train_wall=509, gb_free=6.1, wall=67578
2022-02-01 03:41:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:41:45 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 10.203 | ppl 1178.97 | wps 7841.6 | wpb 2034.1 | bsz 4 | num_updates 12224 | best_loss 9.246
2022-02-01 03:41:45 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-02-01 03:41:45 | INFO | train | epoch 191 | loss 5.126 | ppl 34.91 | wps 5889.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12224 | lr 0.000286018 | gnorm 1.261 | train_wall 326 | gb_free 6.1 | wall 67727
KL Stats: Epoch 191 Divergences: Uniform: 3.184101226009161 Unigram: 3.846745972732835
2022-02-01 03:41:45 | INFO | fairseq.trainer | begin training epoch 192
2022-02-01 03:41:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:47:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:47:40 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 10.195 | ppl 1172.39 | wps 7874.8 | wpb 2034.1 | bsz 4 | num_updates 12288 | best_loss 9.246
2022-02-01 03:47:40 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-02-01 03:47:40 | INFO | train | epoch 192 | loss 5.122 | ppl 34.83 | wps 5891.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12288 | lr 0.000285272 | gnorm 1.272 | train_wall 326 | gb_free 6.1 | wall 68082
KL Stats: Epoch 192 Divergences: Uniform: 3.1823351248814378 Unigram: 3.8438328492392766
2022-02-01 03:47:40 | INFO | fairseq.trainer | begin training epoch 193
2022-02-01 03:47:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:48:42 | INFO | train_inner | epoch 193:     12 / 64 loss=5.128, ppl=34.97, wps=5758.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12300, lr=0.000285133, gnorm=1.268, train_wall=509, gb_free=6.1, wall=68144
2022-02-01 03:53:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:53:35 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 10.15 | ppl 1136.23 | wps 7856.2 | wpb 2034.1 | bsz 4 | num_updates 12352 | best_loss 9.246
2022-02-01 03:53:35 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-02-01 03:53:35 | INFO | train | epoch 193 | loss 5.118 | ppl 34.72 | wps 5887.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12352 | lr 0.000284532 | gnorm 1.294 | train_wall 326 | gb_free 6.1 | wall 68437
KL Stats: Epoch 193 Divergences: Uniform: 3.1914390736667517 Unigram: 3.852600357345195
2022-02-01 03:53:35 | INFO | fairseq.trainer | begin training epoch 194
2022-02-01 03:53:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:57:41 | INFO | train_inner | epoch 194:     48 / 64 loss=5.114, ppl=34.63, wps=6062.3, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=12400, lr=0.000283981, gnorm=1.293, train_wall=510, gb_free=6.1, wall=68683
2022-02-01 03:59:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:59:29 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 10.182 | ppl 1161.79 | wps 7843 | wpb 2034.1 | bsz 4 | num_updates 12416 | best_loss 9.246
2022-02-01 03:59:29 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-02-01 03:59:29 | INFO | train | epoch 194 | loss 5.113 | ppl 34.6 | wps 5889.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12416 | lr 0.000283798 | gnorm 1.318 | train_wall 326 | gb_free 6.1 | wall 68791
KL Stats: Epoch 194 Divergences: Uniform: 3.1890637539145263 Unigram: 3.8575371569969152
2022-02-01 03:59:29 | INFO | fairseq.trainer | begin training epoch 195
2022-02-01 03:59:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:04:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:05:24 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 10.218 | ppl 1191.36 | wps 7866.1 | wpb 2034.1 | bsz 4 | num_updates 12480 | best_loss 9.246
2022-02-01 04:05:24 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-02-01 04:05:24 | INFO | train | epoch 195 | loss 5.108 | ppl 34.49 | wps 5886.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12480 | lr 0.000283069 | gnorm 1.298 | train_wall 326 | gb_free 6.1 | wall 69146
KL Stats: Epoch 195 Divergences: Uniform: 3.1885844526743816 Unigram: 3.858804741066612
2022-02-01 04:05:24 | INFO | fairseq.trainer | begin training epoch 196
2022-02-01 04:05:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:07:07 | INFO | train_inner | epoch 196:     20 / 64 loss=5.106, ppl=34.45, wps=5755.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12500, lr=0.000282843, gnorm=1.311, train_wall=509, gb_free=6.1, wall=69249
2022-02-01 04:10:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:11:19 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 10.21 | ppl 1184.84 | wps 7864.2 | wpb 2034.1 | bsz 4 | num_updates 12544 | best_loss 9.246
2022-02-01 04:11:19 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-02-01 04:11:19 | INFO | train | epoch 196 | loss 5.106 | ppl 34.43 | wps 5880 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12544 | lr 0.000282346 | gnorm 1.336 | train_wall 327 | gb_free 6.1 | wall 69501
KL Stats: Epoch 196 Divergences: Uniform: 3.1877433700239823 Unigram: 3.8604513818933017
2022-02-01 04:11:19 | INFO | fairseq.trainer | begin training epoch 197
2022-02-01 04:11:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:16:06 | INFO | train_inner | epoch 197:     56 / 64 loss=5.105, ppl=34.41, wps=6058, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=12600, lr=0.000281718, gnorm=1.34, train_wall=510, gb_free=6.1, wall=69789
2022-02-01 04:16:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:17:14 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 10.211 | ppl 1185.54 | wps 7860.5 | wpb 2034.1 | bsz 4 | num_updates 12608 | best_loss 9.246
2022-02-01 04:17:14 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-02-01 04:17:14 | INFO | train | epoch 197 | loss 5.099 | ppl 34.28 | wps 5883.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12608 | lr 0.000281629 | gnorm 1.328 | train_wall 326 | gb_free 6.1 | wall 69856
KL Stats: Epoch 197 Divergences: Uniform: 3.1865663248012184 Unigram: 3.8635427002582983
2022-02-01 04:17:14 | INFO | fairseq.trainer | begin training epoch 198
2022-02-01 04:17:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:22:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:23:09 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 10.205 | ppl 1180.13 | wps 7843.9 | wpb 2034.1 | bsz 4 | num_updates 12672 | best_loss 9.246
2022-02-01 04:23:09 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-02-01 04:23:09 | INFO | train | epoch 198 | loss 5.095 | ppl 34.18 | wps 5891 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12672 | lr 0.000280917 | gnorm 1.337 | train_wall 326 | gb_free 6.1 | wall 70211
KL Stats: Epoch 198 Divergences: Uniform: 3.1929393927891203 Unigram: 3.8711975379567964
2022-02-01 04:23:09 | INFO | fairseq.trainer | begin training epoch 199
2022-02-01 04:23:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:25:32 | INFO | train_inner | epoch 199:     28 / 64 loss=5.091, ppl=34.07, wps=5759.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=12700, lr=0.000280607, gnorm=1.332, train_wall=509, gb_free=6.1, wall=70355
2022-02-01 04:28:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:29:04 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 10.221 | ppl 1193.68 | wps 7861.3 | wpb 2034.1 | bsz 4 | num_updates 12736 | best_loss 9.246
2022-02-01 04:29:04 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-02-01 04:29:04 | INFO | train | epoch 199 | loss 5.091 | ppl 34.09 | wps 5885.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12736 | lr 0.00028021 | gnorm 1.314 | train_wall 326 | gb_free 6.1 | wall 70566
KL Stats: Epoch 199 Divergences: Uniform: 3.191141017214542 Unigram: 3.8772058656254824
2022-02-01 04:29:04 | INFO | fairseq.trainer | begin training epoch 200
2022-02-01 04:29:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:34:31 | INFO | train_inner | epoch 200:     64 / 64 loss=5.099, ppl=34.27, wps=6055.2, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=12800, lr=0.000279508, gnorm=1.341, train_wall=509, gb_free=6.1, wall=70893
2022-02-01 04:34:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:34:59 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 10.241 | ppl 1209.92 | wps 7801.5 | wpb 2034.1 | bsz 4 | num_updates 12800 | best_loss 9.246
2022-02-01 04:34:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 12800 updates
2022-02-01 04:34:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint200.pt
2022-02-01 04:35:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint200.pt
2022-02-01 04:35:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint200.pt (epoch 200 @ 12800 updates, score 10.241) (writing took 3.605265788733959 seconds)
2022-02-01 04:35:02 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-02-01 04:35:02 | INFO | train | epoch 200 | loss 5.087 | ppl 33.99 | wps 5823.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12800 | lr 0.000279508 | gnorm 1.357 | train_wall 326 | gb_free 6.1 | wall 70925
KL Stats: Epoch 200 Divergences: Uniform: 3.190097251998759 Unigram: 3.876024122403243
2022-02-01 04:35:02 | INFO | fairseq.trainer | begin training epoch 201
2022-02-01 04:35:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:40:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:40:57 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 10.238 | ppl 1207.87 | wps 7877.5 | wpb 2034.1 | bsz 4 | num_updates 12864 | best_loss 9.246
2022-02-01 04:40:57 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-02-01 04:40:57 | INFO | train | epoch 201 | loss 5.084 | ppl 33.93 | wps 5890.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12864 | lr 0.000278812 | gnorm 1.335 | train_wall 326 | gb_free 6.1 | wall 71279
KL Stats: Epoch 201 Divergences: Uniform: 3.188080827236387 Unigram: 3.8778765306333094
2022-02-01 04:40:57 | INFO | fairseq.trainer | begin training epoch 202
2022-02-01 04:40:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:44:01 | INFO | train_inner | epoch 202:     36 / 64 loss=5.071, ppl=33.62, wps=5731.4, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=12900, lr=0.000278423, gnorm=1.352, train_wall=509, gb_free=6.1, wall=71463
2022-02-01 04:46:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:46:51 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 10.206 | ppl 1181.42 | wps 7850.8 | wpb 2034.1 | bsz 4 | num_updates 12928 | best_loss 9.246
2022-02-01 04:46:51 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-02-01 04:46:51 | INFO | train | epoch 202 | loss 5.08 | ppl 33.82 | wps 5899.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12928 | lr 0.000278121 | gnorm 1.358 | train_wall 325 | gb_free 6.1 | wall 71633
KL Stats: Epoch 202 Divergences: Uniform: 3.1914965566718942 Unigram: 3.887394019142554
2022-02-01 04:46:51 | INFO | fairseq.trainer | begin training epoch 203
2022-02-01 04:46:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:52:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:52:40 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 10.225 | ppl 1196.91 | wps 8149.4 | wpb 2034.1 | bsz 4 | num_updates 12992 | best_loss 9.246
2022-02-01 04:52:40 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-02-01 04:52:40 | INFO | train | epoch 203 | loss 5.074 | ppl 33.68 | wps 5979 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12992 | lr 0.000277435 | gnorm 1.364 | train_wall 322 | gb_free 6.1 | wall 71982
KL Stats: Epoch 203 Divergences: Uniform: 3.19078468916631 Unigram: 3.887174998290582
2022-02-01 04:52:40 | INFO | fairseq.trainer | begin training epoch 204
2022-02-01 04:52:40 | INFO | fairseq_cli.train | Start iterating over samples
User defined signal 2
Sender: LSF System <lsfadmin@eu-g3-025>
Subject: Job 202993782: <w2_jelinek_0.09_0.01_0.9_#4> in cluster <euler> Exited

Job <w2_jelinek_0.09_0.01_0.9_#4> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Wed Feb  2 06:10:42 2022
Job was executed on host(s) <eu-g3-025>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Wed Feb  2 06:11:20 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Feb  2 06:11:20 2022
Terminated at Thu Feb  3 02:11:26 2022
Results reported at Thu Feb  3 02:11:26 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-2-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.09, 0.01, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 16 --save-interval 40 --seed 40002 --max-update 50000
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   71965.00 sec.
    Max Memory :                                 5561 MB
    Average Memory :                             3160.92 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               14439.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   72006 sec.
    Turnaround time :                            72044 sec.

The output (if any) follows:

2022-02-02 06:11:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 40002, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 40, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.5, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-2-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 40002, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.09, 0.01, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.5, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-02 06:11:30 | INFO | fairseq.tasks.language_modeling | dictionary: 76624 types
2022-02-02 06:11:31 | INFO | fairseq.data.data_utils | loaded 36,718 examples from: data-bin/wikitext-2-raw-full/train
Calculating frequency stats:
  0%|          | 0/36718 [00:00<?, ?it/s]  3%|▎         | 1108/36718 [00:00<00:03, 11077.24it/s]  6%|▌         | 2216/36718 [00:00<00:03, 10134.49it/s]  9%|▉         | 3425/36718 [00:00<00:03, 10983.20it/s] 13%|█▎        | 4648/36718 [00:00<00:02, 11460.04it/s] 16%|█▌        | 5934/36718 [00:00<00:02, 11954.52it/s] 19%|█▉        | 7135/36718 [00:00<00:02, 11151.40it/s] 23%|██▎       | 8271/36718 [00:00<00:02, 11214.61it/s] 26%|██▌       | 9401/36718 [00:00<00:02, 11188.98it/s] 29%|██▊       | 10526/36718 [00:00<00:02, 11085.81it/s] 32%|███▏      | 11639/36718 [00:01<00:02, 11012.55it/s] 35%|███▍      | 12759/36718 [00:01<00:02, 11067.05it/s] 38%|███▊      | 13966/36718 [00:01<00:02, 11363.54it/s] 41%|████▏     | 15148/36718 [00:01<00:01, 11499.86it/s] 44%|████▍     | 16300/36718 [00:01<00:01, 11086.01it/s] 47%|████▋     | 17436/36718 [00:01<00:01, 11160.08it/s] 51%|█████     | 18555/36718 [00:01<00:01, 10938.57it/s] 54%|█████▍    | 19839/36718 [00:01<00:01, 11491.89it/s] 57%|█████▋    | 20992/36718 [00:01<00:01, 11239.62it/s] 60%|██████    | 22120/36718 [00:01<00:01, 11011.20it/s] 63%|██████▎   | 23298/36718 [00:02<00:01, 11231.31it/s] 67%|██████▋   | 24728/36718 [00:02<00:00, 12122.60it/s] 71%|███████   | 25945/36718 [00:02<00:00, 11841.14it/s] 74%|███████▍  | 27134/36718 [00:02<00:00, 11241.24it/s] 77%|███████▋  | 28266/36718 [00:02<00:00, 11199.50it/s] 80%|████████  | 29392/36718 [00:02<00:00, 11145.92it/s] 83%|████████▎ | 30531/36718 [00:02<00:00, 11210.18it/s] 86%|████████▌ | 31655/36718 [00:02<00:00, 10966.05it/s] 89%|████████▉ | 32755/36718 [00:02<00:00, 10705.19it/s] 92%|█████████▏| 33828/36718 [00:03<00:00, 10622.65it/s] 95%|█████████▌| 35004/36718 [00:03<00:00, 10950.37it/s] 98%|█████████▊| 36102/36718 [00:03<00:00, 10886.98it/s]100%|██████████| 36718/36718 [00:03<00:00, 11159.16it/s]

gathering stats for n=1
  0%|          | 0/36718 [00:00<?, ?it/s]  5%|▌         | 1981/36718 [00:00<00:01, 19806.95it/s] 11%|█▏        | 4149/36718 [00:00<00:01, 20907.98it/s] 17%|█▋        | 6371/36718 [00:00<00:01, 21503.61it/s] 23%|██▎       | 8522/36718 [00:00<00:01, 20462.17it/s] 29%|██▉       | 10585/36718 [00:00<00:01, 20512.52it/s] 34%|███▍      | 12642/36718 [00:00<00:01, 20520.84it/s] 40%|████      | 14752/36718 [00:00<00:01, 20706.58it/s] 46%|████▌     | 16826/36718 [00:00<00:00, 20422.99it/s] 52%|█████▏    | 18946/36718 [00:00<00:00, 20657.69it/s] 57%|█████▋    | 21014/36718 [00:01<00:00, 20575.83it/s] 63%|██████▎   | 23073/36718 [00:01<00:00, 20492.93it/s] 69%|██████▉   | 25420/36718 [00:01<00:00, 21388.26it/s] 75%|███████▌  | 27561/36718 [00:01<00:00, 20443.61it/s] 81%|████████  | 29658/36718 [00:01<00:00, 20590.79it/s] 86%|████████▋ | 31725/36718 [00:01<00:00, 20316.28it/s] 92%|█████████▏| 33762/36718 [00:01<00:00, 19712.90it/s] 97%|█████████▋| 35763/36718 [00:01<00:00, 19793.90it/s]100%|██████████| 36718/36718 [00:01<00:00, 20396.46it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 234.45it/s]2022-02-02 06:11:44 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(76624, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=76624, bias=False)
  )
)
2022-02-02 06:11:44 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-02 06:11:44 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-02 06:11:44 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-02-02 06:11:44 | INFO | fairseq_cli.train | num. shared model params: 58,145,792 (num. trained: 58,145,792)
2022-02-02 06:11:44 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-02 06:11:44 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-2-raw-full/valid
2022-02-02 06:11:45 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-02 06:11:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-02 06:11:45 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-02-02 06:11:45 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-02 06:11:45 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-02 06:11:45 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-02-02 06:11:45 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint_last.pt
2022-02-02 06:11:45 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint_last.pt
2022-02-02 06:11:45 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-02 06:11:45 | INFO | fairseq.data.data_utils | loaded 36,718 examples from: data-bin/wikitext-2-raw-full/train
2022-02-02 06:11:45 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-02-02 06:11:45 | INFO | fairseq.trainer | begin training epoch 1
2022-02-02 06:11:45 | INFO | fairseq_cli.train | Start iterating over samples

2022-02-02 06:17:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
/cluster/home/andriusb/fq/fairseq/fairseq/utils.py:372: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 06:17:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.707 | ppl 26745 | wps 7970.7 | wpb 2034.1 | bsz 4 | num_updates 64
2022-02-02 06:17:42 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-02 06:17:42 | INFO | train | epoch 001 | loss 16.136 | ppl 72021.5 | wps 5894 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 64 | lr 8.0984e-06 | gnorm 3.03 | train_wall 326 | gb_free 6.1 | wall 357
KL Stats: Epoch 1 Divergences: Uniform: 0.5102809553563303 Unigram: 3.7042221936235626
2022-02-02 06:17:42 | INFO | fairseq.trainer | begin training epoch 2
2022-02-02 06:17:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 06:20:45 | INFO | train_inner | epoch 002:     36 / 64 loss=15.616, ppl=50209.6, wps=6076.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=100, lr=1.25975e-05, gnorm=2.563, train_wall=508, gb_free=6.1, wall=541
2022-02-02 06:23:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 06:23:35 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.65 | ppl 12851.6 | wps 7955.1 | wpb 2034.1 | bsz 4 | num_updates 128
2022-02-02 06:23:35 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-02 06:23:35 | INFO | train | epoch 002 | loss 14.455 | ppl 22456.6 | wps 5920.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 128 | lr 1.60968e-05 | gnorm 1.633 | train_wall 324 | gb_free 6.1 | wall 710
KL Stats: Epoch 2 Divergences: Uniform: 0.5081234080390405 Unigram: 2.4600226511915047
2022-02-02 06:23:35 | INFO | fairseq.trainer | begin training epoch 3
2022-02-02 06:23:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 06:29:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 06:29:29 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 12.794 | ppl 7103.44 | wps 7912.7 | wpb 2034.1 | bsz 4 | num_updates 192
2022-02-02 06:29:29 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-02 06:29:29 | INFO | train | epoch 003 | loss 13.502 | ppl 11598.6 | wps 5898 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 192 | lr 2.40952e-05 | gnorm 1.359 | train_wall 325 | gb_free 6.1 | wall 1064
KL Stats: Epoch 3 Divergences: Uniform: 0.49177638419828995 Unigram: 1.7454502137036414
2022-02-02 06:29:29 | INFO | fairseq.trainer | begin training epoch 4
2022-02-02 06:29:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 06:30:10 | INFO | train_inner | epoch 004:      8 / 64 loss=13.638, ppl=12746.3, wps=5778.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=200, lr=2.5095e-05, gnorm=1.389, train_wall=507, gb_free=6.1, wall=1105
2022-02-02 06:34:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 06:35:22 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.939 | ppl 3927.52 | wps 7974.7 | wpb 2034.1 | bsz 4 | num_updates 256
2022-02-02 06:35:22 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-02 06:35:22 | INFO | train | epoch 004 | loss 12.501 | ppl 5796.81 | wps 5909.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 256 | lr 3.20936e-05 | gnorm 1.05 | train_wall 324 | gb_free 6.1 | wall 1418
KL Stats: Epoch 4 Divergences: Uniform: 0.5889339303367147 Unigram: 1.1129560590689838
2022-02-02 06:35:22 | INFO | fairseq.trainer | begin training epoch 5
2022-02-02 06:35:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 06:39:08 | INFO | train_inner | epoch 005:     44 / 64 loss=12.145, ppl=4530.19, wps=6075.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=300, lr=3.75925e-05, gnorm=0.902, train_wall=508, gb_free=6.1, wall=1643
2022-02-02 06:40:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 06:41:16 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.456 | ppl 2809.92 | wps 7952.7 | wpb 2034.1 | bsz 4 | num_updates 320
2022-02-02 06:41:16 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-02 06:41:16 | INFO | train | epoch 005 | loss 11.696 | ppl 3318.82 | wps 5903.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 320 | lr 4.0092e-05 | gnorm 0.71 | train_wall 325 | gb_free 6.1 | wall 1771
KL Stats: Epoch 5 Divergences: Uniform: 0.8490835813961715 Unigram: 0.6394476462775636
2022-02-02 06:41:16 | INFO | fairseq.trainer | begin training epoch 6
2022-02-02 06:41:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 06:46:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 06:47:09 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.23 | ppl 2402.08 | wps 7956.8 | wpb 2034.1 | bsz 4 | num_updates 384
2022-02-02 06:47:09 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-02 06:47:09 | INFO | train | epoch 006 | loss 11.282 | ppl 2489.58 | wps 5913.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 384 | lr 4.80904e-05 | gnorm 0.589 | train_wall 324 | gb_free 6.1 | wall 2125
KL Stats: Epoch 6 Divergences: Uniform: 1.168289467064053 Unigram: 0.42709169679358966
2022-02-02 06:47:09 | INFO | fairseq.trainer | begin training epoch 7
2022-02-02 06:47:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 06:48:31 | INFO | train_inner | epoch 007:     16 / 64 loss=11.305, ppl=2529.92, wps=5785.3, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=400, lr=5.009e-05, gnorm=0.596, train_wall=506, gb_free=6.1, wall=2206
2022-02-02 06:52:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 06:53:02 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.082 | ppl 2167.67 | wps 7951.4 | wpb 2034.1 | bsz 4 | num_updates 448
2022-02-02 06:53:02 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-02 06:53:02 | INFO | train | epoch 007 | loss 11.088 | ppl 2176.19 | wps 5912.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 448 | lr 5.60888e-05 | gnorm 0.525 | train_wall 324 | gb_free 6.1 | wall 2478
KL Stats: Epoch 7 Divergences: Uniform: 1.3960343454342337 Unigram: 0.44867718955224956
2022-02-02 06:53:02 | INFO | fairseq.trainer | begin training epoch 8
2022-02-02 06:53:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 06:57:29 | INFO | train_inner | epoch 008:     52 / 64 loss=11.023, ppl=2081.07, wps=6079.3, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=500, lr=6.25875e-05, gnorm=0.493, train_wall=508, gb_free=6.1, wall=2744
2022-02-02 06:58:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 06:58:56 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.966 | ppl 1999.95 | wps 7935.3 | wpb 2034.1 | bsz 4 | num_updates 512
2022-02-02 06:58:56 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-02 06:58:56 | INFO | train | epoch 008 | loss 10.974 | ppl 2011.04 | wps 5905.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 512 | lr 6.40872e-05 | gnorm 0.494 | train_wall 324 | gb_free 6.1 | wall 2832
KL Stats: Epoch 8 Divergences: Uniform: 1.5165904575170965 Unigram: 0.5229995347000576
2022-02-02 06:58:56 | INFO | fairseq.trainer | begin training epoch 9
2022-02-02 06:58:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:04:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:04:50 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.857 | ppl 1855.12 | wps 7970.2 | wpb 2034.1 | bsz 4 | num_updates 576
2022-02-02 07:04:50 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-02 07:04:50 | INFO | train | epoch 009 | loss 10.872 | ppl 1873.8 | wps 5907.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 576 | lr 7.20856e-05 | gnorm 0.496 | train_wall 324 | gb_free 6.1 | wall 3185
KL Stats: Epoch 9 Divergences: Uniform: 1.5683732503686965 Unigram: 0.6204937519732974
2022-02-02 07:04:50 | INFO | fairseq.trainer | begin training epoch 10
2022-02-02 07:04:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:06:53 | INFO | train_inner | epoch 010:     24 / 64 loss=10.86, ppl=1858.54, wps=5781.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=600, lr=7.5085e-05, gnorm=0.502, train_wall=506, gb_free=6.1, wall=3308
2022-02-02 07:10:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:10:44 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.746 | ppl 1717.58 | wps 7894.8 | wpb 2034.1 | bsz 4 | num_updates 640
2022-02-02 07:10:44 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-02 07:10:44 | INFO | train | epoch 010 | loss 10.763 | ppl 1737.94 | wps 5896.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 640 | lr 8.0084e-05 | gnorm 0.506 | train_wall 325 | gb_free 6.1 | wall 3539
KL Stats: Epoch 10 Divergences: Uniform: 1.59566604313298 Unigram: 0.7254283146703714
2022-02-02 07:10:44 | INFO | fairseq.trainer | begin training epoch 11
2022-02-02 07:10:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:15:51 | INFO | train_inner | epoch 011:     60 / 64 loss=10.688, ppl=1649.5, wps=6069.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=700, lr=8.75825e-05, gnorm=0.487, train_wall=508, gb_free=6.1, wall=3846
2022-02-02 07:16:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:16:38 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.637 | ppl 1592.52 | wps 7949.6 | wpb 2034.1 | bsz 4 | num_updates 704
2022-02-02 07:16:38 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-02 07:16:38 | INFO | train | epoch 011 | loss 10.646 | ppl 1602.89 | wps 5903.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 704 | lr 8.80824e-05 | gnorm 0.472 | train_wall 325 | gb_free 6.1 | wall 3893
KL Stats: Epoch 11 Divergences: Uniform: 1.6113807795791695 Unigram: 0.8326583913946173
2022-02-02 07:16:38 | INFO | fairseq.trainer | begin training epoch 12
2022-02-02 07:16:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:22:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:22:31 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.544 | ppl 1493.02 | wps 7920 | wpb 2034.1 | bsz 4 | num_updates 768
2022-02-02 07:22:31 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-02 07:22:31 | INFO | train | epoch 012 | loss 10.53 | ppl 1478.34 | wps 5903.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 768 | lr 9.60808e-05 | gnorm 0.516 | train_wall 325 | gb_free 6.1 | wall 4247
KL Stats: Epoch 12 Divergences: Uniform: 1.6283917716488117 Unigram: 0.9334174638528905
2022-02-02 07:22:31 | INFO | fairseq.trainer | begin training epoch 13
2022-02-02 07:22:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:25:15 | INFO | train_inner | epoch 013:     32 / 64 loss=10.503, ppl=1450.71, wps=5775.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=800, lr=0.00010008, gnorm=0.509, train_wall=507, gb_free=6.1, wall=4411
2022-02-02 07:27:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:28:26 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 10.45 | ppl 1398.96 | wps 7923.1 | wpb 2034.1 | bsz 4 | num_updates 832
2022-02-02 07:28:26 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-02 07:28:26 | INFO | train | epoch 013 | loss 10.414 | ppl 1364.16 | wps 5894.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 832 | lr 0.000104079 | gnorm 0.481 | train_wall 325 | gb_free 6.1 | wall 4601
KL Stats: Epoch 13 Divergences: Uniform: 1.648809465418061 Unigram: 1.0220222897493623
2022-02-02 07:28:26 | INFO | fairseq.trainer | begin training epoch 14
2022-02-02 07:28:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:33:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:34:20 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 10.373 | ppl 1326.56 | wps 7962.7 | wpb 2034.1 | bsz 4 | num_updates 896
2022-02-02 07:34:20 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-02 07:34:20 | INFO | train | epoch 014 | loss 10.3 | ppl 1260.32 | wps 5898.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 896 | lr 0.000112078 | gnorm 0.521 | train_wall 325 | gb_free 6.1 | wall 4955
KL Stats: Epoch 14 Divergences: Uniform: 1.6783578114355082 Unigram: 1.1072281119667897
2022-02-02 07:34:20 | INFO | fairseq.trainer | begin training epoch 15
2022-02-02 07:34:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:34:40 | INFO | train_inner | epoch 015:      4 / 64 loss=10.326, ppl=1283.77, wps=5770.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=900, lr=0.000112578, gnorm=0.509, train_wall=507, gb_free=6.1, wall=4976
2022-02-02 07:39:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:40:14 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 10.272 | ppl 1236.57 | wps 7928.6 | wpb 2034.1 | bsz 4 | num_updates 960
2022-02-02 07:40:14 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-02 07:40:14 | INFO | train | epoch 015 | loss 10.189 | ppl 1167.35 | wps 5899.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 960 | lr 0.000120076 | gnorm 0.531 | train_wall 325 | gb_free 6.1 | wall 5309
KL Stats: Epoch 15 Divergences: Uniform: 1.706756702804904 Unigram: 1.1785904878623952
2022-02-02 07:40:14 | INFO | fairseq.trainer | begin training epoch 16
2022-02-02 07:40:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:43:38 | INFO | train_inner | epoch 016:     40 / 64 loss=10.149, ppl=1135.7, wps=6074.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=1000, lr=0.000125075, gnorm=0.508, train_wall=508, gb_free=6.1, wall=5514
2022-02-02 07:45:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:46:07 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 10.183 | ppl 1162.09 | wps 7935.4 | wpb 2034.1 | bsz 4 | num_updates 1024
2022-02-02 07:46:07 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-02 07:46:07 | INFO | train | epoch 016 | loss 10.078 | ppl 1080.8 | wps 5909.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1024 | lr 0.000128074 | gnorm 0.509 | train_wall 324 | gb_free 6.1 | wall 5663
KL Stats: Epoch 16 Divergences: Uniform: 1.7443948092589736 Unigram: 1.247359462489142
2022-02-02 07:46:07 | INFO | fairseq.trainer | begin training epoch 17
2022-02-02 07:46:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:51:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 07:52:01 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.098 | ppl 1096.08 | wps 7960.6 | wpb 2034.1 | bsz 4 | num_updates 1088
2022-02-02 07:52:01 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-02 07:52:01 | INFO | train | epoch 017 | loss 9.971 | ppl 1003.56 | wps 5903.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1088 | lr 0.000136073 | gnorm 0.527 | train_wall 325 | gb_free 6.1 | wall 6017
KL Stats: Epoch 17 Divergences: Uniform: 1.7709228469697942 Unigram: 1.316588718733468
2022-02-02 07:52:01 | INFO | fairseq.trainer | begin training epoch 18
2022-02-02 07:52:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:53:02 | INFO | train_inner | epoch 018:     12 / 64 loss=9.98, ppl=1010.13, wps=5779.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=1100, lr=0.000137573, gnorm=0.529, train_wall=506, gb_free=6.1, wall=6078
2022-02-02 07:57:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:57:55 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.011 | ppl 1031.81 | wps 7979.6 | wpb 2034.1 | bsz 4 | num_updates 1152
2022-02-02 07:57:55 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-02 07:57:55 | INFO | train | epoch 018 | loss 9.868 | ppl 934.39 | wps 5905 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1152 | lr 0.000144071 | gnorm 0.539 | train_wall 325 | gb_free 6.1 | wall 6370
KL Stats: Epoch 18 Divergences: Uniform: 1.7998237247949593 Unigram: 1.3815408701691372
2022-02-02 07:57:55 | INFO | fairseq.trainer | begin training epoch 19
2022-02-02 07:57:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:02:01 | INFO | train_inner | epoch 019:     48 / 64 loss=9.817, ppl=902.27, wps=6072.3, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=1200, lr=0.00015007, gnorm=0.558, train_wall=508, gb_free=6.1, wall=6616
2022-02-02 08:03:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:03:49 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.946 | ppl 986.55 | wps 7945.7 | wpb 2034.1 | bsz 4 | num_updates 1216
2022-02-02 08:03:49 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-02 08:03:49 | INFO | train | epoch 019 | loss 9.767 | ppl 871.36 | wps 5902.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1216 | lr 0.00015207 | gnorm 0.55 | train_wall 325 | gb_free 6.1 | wall 6724
KL Stats: Epoch 19 Divergences: Uniform: 1.8359983431938431 Unigram: 1.4416301014473107
2022-02-02 08:03:49 | INFO | fairseq.trainer | begin training epoch 20
2022-02-02 08:03:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:09:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:09:42 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.918 | ppl 967.38 | wps 7926.7 | wpb 2034.1 | bsz 4 | num_updates 1280
2022-02-02 08:09:42 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-02 08:09:42 | INFO | train | epoch 020 | loss 9.67 | ppl 814.77 | wps 5910.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1280 | lr 0.000160068 | gnorm 0.526 | train_wall 324 | gb_free 6.1 | wall 7077
KL Stats: Epoch 20 Divergences: Uniform: 1.8615541341724895 Unigram: 1.4991039074213304
2022-02-02 08:09:42 | INFO | fairseq.trainer | begin training epoch 21
2022-02-02 08:09:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:11:25 | INFO | train_inner | epoch 021:     20 / 64 loss=9.672, ppl=815.61, wps=5780.2, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=1300, lr=0.000162568, gnorm=0.531, train_wall=506, gb_free=6.1, wall=7180
2022-02-02 08:15:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:15:36 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.833 | ppl 911.77 | wps 7949.1 | wpb 2034.1 | bsz 4 | num_updates 1344
2022-02-02 08:15:36 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-02 08:15:36 | INFO | train | epoch 021 | loss 9.581 | ppl 765.73 | wps 5908 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1344 | lr 0.000168066 | gnorm 0.546 | train_wall 324 | gb_free 6.1 | wall 7431
KL Stats: Epoch 21 Divergences: Uniform: 1.8914546183464636 Unigram: 1.5521247433494367
2022-02-02 08:15:36 | INFO | fairseq.trainer | begin training epoch 22
2022-02-02 08:15:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:20:22 | INFO | train_inner | epoch 022:     56 / 64 loss=9.529, ppl=738.82, wps=6084.2, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=1400, lr=0.000175065, gnorm=0.516, train_wall=507, gb_free=6.1, wall=7717
2022-02-02 08:21:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:21:29 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.786 | ppl 882.66 | wps 7930.7 | wpb 2034.1 | bsz 4 | num_updates 1408
2022-02-02 08:21:29 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-02 08:21:29 | INFO | train | epoch 022 | loss 9.49 | ppl 719.12 | wps 5911 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1408 | lr 0.000176065 | gnorm 0.51 | train_wall 324 | gb_free 6.1 | wall 7784
KL Stats: Epoch 22 Divergences: Uniform: 1.928953993124693 Unigram: 1.6012635498749537
2022-02-02 08:21:29 | INFO | fairseq.trainer | begin training epoch 23
2022-02-02 08:21:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:26:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:27:22 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.714 | ppl 840.02 | wps 7943.5 | wpb 2034.1 | bsz 4 | num_updates 1472
2022-02-02 08:27:22 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-02 08:27:22 | INFO | train | epoch 023 | loss 9.406 | ppl 678.28 | wps 5911.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1472 | lr 0.000184063 | gnorm 0.547 | train_wall 324 | gb_free 6.1 | wall 8138
KL Stats: Epoch 23 Divergences: Uniform: 1.9556524827646458 Unigram: 1.6418226020261018
2022-02-02 08:27:22 | INFO | fairseq.trainer | begin training epoch 24
2022-02-02 08:27:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:29:45 | INFO | train_inner | epoch 024:     28 / 64 loss=9.387, ppl=669.55, wps=5784.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=1500, lr=0.000187563, gnorm=0.553, train_wall=506, gb_free=6.1, wall=8281
2022-02-02 08:32:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:33:15 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.67 | ppl 814.65 | wps 7963.4 | wpb 2034.1 | bsz 4 | num_updates 1536
2022-02-02 08:33:15 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-02 08:33:15 | INFO | train | epoch 024 | loss 9.321 | ppl 639.69 | wps 5914.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1536 | lr 0.000192062 | gnorm 0.526 | train_wall 324 | gb_free 6.1 | wall 8491
KL Stats: Epoch 24 Divergences: Uniform: 1.9829457294025583 Unigram: 1.687255324335348
2022-02-02 08:33:15 | INFO | fairseq.trainer | begin training epoch 25
2022-02-02 08:33:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:38:41 | INFO | train_inner | epoch 025:     64 / 64 loss=9.268, ppl=616.7, wps=6080.4, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=1600, lr=0.00020006, gnorm=0.522, train_wall=506, gb_free=6.1, wall=8817
2022-02-02 08:38:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:39:09 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.621 | ppl 787.31 | wps 7953.7 | wpb 2034.1 | bsz 4 | num_updates 1600
2022-02-02 08:39:09 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-02 08:39:09 | INFO | train | epoch 025 | loss 9.242 | ppl 605.43 | wps 5907.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1600 | lr 0.00020006 | gnorm 0.537 | train_wall 324 | gb_free 6.1 | wall 8844
KL Stats: Epoch 25 Divergences: Uniform: 1.997791162131894 Unigram: 1.7273155398438678
2022-02-02 08:39:09 | INFO | fairseq.trainer | begin training epoch 26
2022-02-02 08:39:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:44:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:45:03 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.592 | ppl 772 | wps 7939.5 | wpb 2034.1 | bsz 4 | num_updates 1664
2022-02-02 08:45:03 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-02 08:45:03 | INFO | train | epoch 026 | loss 9.158 | ppl 571.26 | wps 5898.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1664 | lr 0.000208058 | gnorm 0.502 | train_wall 325 | gb_free 6.1 | wall 9198
KL Stats: Epoch 26 Divergences: Uniform: 2.0202515358084634 Unigram: 1.7672894692029415
2022-02-02 08:45:03 | INFO | fairseq.trainer | begin training epoch 27
2022-02-02 08:45:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:48:07 | INFO | train_inner | epoch 027:     36 / 64 loss=9.129, ppl=559.78, wps=5779.6, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=1700, lr=0.000212558, gnorm=0.502, train_wall=508, gb_free=6.1, wall=9382
2022-02-02 08:50:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:50:56 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.533 | ppl 741.04 | wps 7953.2 | wpb 2034.1 | bsz 4 | num_updates 1728
2022-02-02 08:50:56 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-02 08:50:56 | INFO | train | epoch 027 | loss 9.078 | ppl 540.62 | wps 5915.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1728 | lr 0.000216057 | gnorm 0.515 | train_wall 324 | gb_free 6.1 | wall 9552
KL Stats: Epoch 27 Divergences: Uniform: 2.0498542419497934 Unigram: 1.80211929527855
2022-02-02 08:50:56 | INFO | fairseq.trainer | begin training epoch 28
2022-02-02 08:50:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:56:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 08:56:49 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.506 | ppl 727.35 | wps 7943.1 | wpb 2034.1 | bsz 4 | num_updates 1792
2022-02-02 08:56:49 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-02 08:56:49 | INFO | train | epoch 028 | loss 8.996 | ppl 510.62 | wps 5910.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1792 | lr 0.000224055 | gnorm 0.516 | train_wall 324 | gb_free 6.1 | wall 9905
KL Stats: Epoch 28 Divergences: Uniform: 2.0773593573055953 Unigram: 1.8416155458713603
2022-02-02 08:56:49 | INFO | fairseq.trainer | begin training epoch 29
2022-02-02 08:56:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:57:30 | INFO | train_inner | epoch 029:      8 / 64 loss=9.011, ppl=516.06, wps=5785.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=1800, lr=0.000225055, gnorm=0.524, train_wall=506, gb_free=6.1, wall=9946
2022-02-02 09:02:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:02:43 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.467 | ppl 707.78 | wps 7943.7 | wpb 2034.1 | bsz 4 | num_updates 1856
2022-02-02 09:02:43 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-02 09:02:43 | INFO | train | epoch 029 | loss 8.92 | ppl 484.44 | wps 5907.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1856 | lr 0.000232054 | gnorm 0.529 | train_wall 324 | gb_free 6.1 | wall 10258
KL Stats: Epoch 29 Divergences: Uniform: 2.099591394662328 Unigram: 1.8794010618872712
2022-02-02 09:02:43 | INFO | fairseq.trainer | begin training epoch 30
2022-02-02 09:02:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:06:29 | INFO | train_inner | epoch 030:     44 / 64 loss=8.888, ppl=473.7, wps=6070.5, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=1900, lr=0.000237553, gnorm=0.52, train_wall=508, gb_free=6.1, wall=10484
2022-02-02 09:08:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 09:08:37 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 9.435 | ppl 692.32 | wps 7956.3 | wpb 2034.1 | bsz 4 | num_updates 1920
2022-02-02 09:08:37 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-02 09:08:37 | INFO | train | epoch 030 | loss 8.842 | ppl 458.98 | wps 5896.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1920 | lr 0.000240052 | gnorm 0.517 | train_wall 325 | gb_free 6.1 | wall 10613
KL Stats: Epoch 30 Divergences: Uniform: 2.1191437516379663 Unigram: 1.9119449332720218
2022-02-02 09:08:37 | INFO | fairseq.trainer | begin training epoch 31
2022-02-02 09:08:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:14:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 09:14:30 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.416 | ppl 683.3 | wps 7973.9 | wpb 2034.1 | bsz 4 | num_updates 1984
2022-02-02 09:14:30 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-02 09:14:30 | INFO | train | epoch 031 | loss 8.767 | ppl 435.59 | wps 5916.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1984 | lr 0.00024805 | gnorm 0.511 | train_wall 324 | gb_free 6.1 | wall 10966
KL Stats: Epoch 31 Divergences: Uniform: 2.1407710868837144 Unigram: 1.9505314449178237
2022-02-02 09:14:30 | INFO | fairseq.trainer | begin training epoch 32
2022-02-02 09:14:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:15:52 | INFO | train_inner | epoch 032:     16 / 64 loss=8.766, ppl=435.38, wps=5786, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=2000, lr=0.00025005, gnorm=0.517, train_wall=506, gb_free=6.1, wall=11048
2022-02-02 09:19:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:20:23 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 9.406 | ppl 678.19 | wps 7952.9 | wpb 2034.1 | bsz 4 | num_updates 2048
2022-02-02 09:20:23 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-02 09:20:23 | INFO | train | epoch 032 | loss 8.692 | ppl 413.48 | wps 5921.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2048 | lr 0.000256049 | gnorm 0.528 | train_wall 324 | gb_free 6.1 | wall 11318
KL Stats: Epoch 32 Divergences: Uniform: 2.1656032279497563 Unigram: 1.9851846481184403
2022-02-02 09:20:23 | INFO | fairseq.trainer | begin training epoch 33
2022-02-02 09:20:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:24:49 | INFO | train_inner | epoch 033:     52 / 64 loss=8.655, ppl=403.01, wps=6088, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=2100, lr=0.000262548, gnorm=0.517, train_wall=507, gb_free=6.1, wall=11584
2022-02-02 09:25:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:26:16 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 9.361 | ppl 657.58 | wps 7988.8 | wpb 2034.1 | bsz 4 | num_updates 2112
2022-02-02 09:26:16 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-02 09:26:16 | INFO | train | epoch 033 | loss 8.616 | ppl 392.46 | wps 5914.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2112 | lr 0.000264047 | gnorm 0.51 | train_wall 324 | gb_free 6.1 | wall 11672
KL Stats: Epoch 33 Divergences: Uniform: 2.1858896659947584 Unigram: 2.017573608486482
2022-02-02 09:26:16 | INFO | fairseq.trainer | begin training epoch 34
2022-02-02 09:26:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:31:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:32:09 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 9.341 | ppl 648.5 | wps 7967.7 | wpb 2034.1 | bsz 4 | num_updates 2176
2022-02-02 09:32:09 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-02 09:32:09 | INFO | train | epoch 034 | loss 8.542 | ppl 372.73 | wps 5917.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2176 | lr 0.000272046 | gnorm 0.495 | train_wall 324 | gb_free 6.1 | wall 12024
KL Stats: Epoch 34 Divergences: Uniform: 2.206327581850095 Unigram: 2.052552952641548
2022-02-02 09:32:09 | INFO | fairseq.trainer | begin training epoch 35
2022-02-02 09:32:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:34:11 | INFO | train_inner | epoch 035:     24 / 64 loss=8.532, ppl=370.1, wps=5796.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=2200, lr=0.000275045, gnorm=0.502, train_wall=505, gb_free=6.1, wall=12147
2022-02-02 09:37:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:38:01 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 9.325 | ppl 641.52 | wps 7977 | wpb 2034.1 | bsz 4 | num_updates 2240
2022-02-02 09:38:01 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-02 09:38:01 | INFO | train | epoch 035 | loss 8.471 | ppl 354.81 | wps 5930.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2240 | lr 0.000280044 | gnorm 0.515 | train_wall 323 | gb_free 6.1 | wall 12377
KL Stats: Epoch 35 Divergences: Uniform: 2.2347655180978925 Unigram: 2.080042771323755
2022-02-02 09:38:01 | INFO | fairseq.trainer | begin training epoch 36
2022-02-02 09:38:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:43:07 | INFO | train_inner | epoch 036:     60 / 64 loss=8.43, ppl=344.93, wps=6099.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=2300, lr=0.000287543, gnorm=0.507, train_wall=506, gb_free=6.1, wall=12683
2022-02-02 09:43:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:43:54 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 9.287 | ppl 624.66 | wps 7957.8 | wpb 2034.1 | bsz 4 | num_updates 2304
2022-02-02 09:43:54 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-02 09:43:54 | INFO | train | epoch 036 | loss 8.399 | ppl 337.66 | wps 5926.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2304 | lr 0.000288042 | gnorm 0.504 | train_wall 323 | gb_free 6.1 | wall 12729
KL Stats: Epoch 36 Divergences: Uniform: 2.254250422404493 Unigram: 2.1104596199800953
2022-02-02 09:43:54 | INFO | fairseq.trainer | begin training epoch 37
2022-02-02 09:43:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:49:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:49:46 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.28 | ppl 621.66 | wps 7963.2 | wpb 2034.1 | bsz 4 | num_updates 2368
2022-02-02 09:49:46 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-02-02 09:49:46 | INFO | train | epoch 037 | loss 8.33 | ppl 321.85 | wps 5926.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2368 | lr 0.000296041 | gnorm 0.508 | train_wall 323 | gb_free 6.1 | wall 13082
KL Stats: Epoch 37 Divergences: Uniform: 2.2745723533884346 Unigram: 2.1410076032994962
2022-02-02 09:49:46 | INFO | fairseq.trainer | begin training epoch 38
2022-02-02 09:49:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:52:29 | INFO | train_inner | epoch 038:     32 / 64 loss=8.313, ppl=318.01, wps=5798, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=2400, lr=0.00030004, gnorm=0.512, train_wall=505, gb_free=6.1, wall=13245
2022-02-02 09:55:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:55:39 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 9.272 | ppl 618.18 | wps 7977.4 | wpb 2034.1 | bsz 4 | num_updates 2432
2022-02-02 09:55:39 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-02-02 09:55:39 | INFO | train | epoch 038 | loss 8.262 | ppl 306.89 | wps 5920.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2432 | lr 0.000304039 | gnorm 0.517 | train_wall 324 | gb_free 6.1 | wall 13434
KL Stats: Epoch 38 Divergences: Uniform: 2.2981635717831344 Unigram: 2.175973227856903
2022-02-02 09:55:39 | INFO | fairseq.trainer | begin training epoch 39
2022-02-02 09:55:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:01:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:01:32 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.249 | ppl 608.25 | wps 7968.3 | wpb 2034.1 | bsz 4 | num_updates 2496
2022-02-02 10:01:32 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-02-02 10:01:32 | INFO | train | epoch 039 | loss 8.193 | ppl 292.67 | wps 5919.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2496 | lr 0.000312038 | gnorm 0.519 | train_wall 324 | gb_free 6.1 | wall 13787
KL Stats: Epoch 39 Divergences: Uniform: 2.3165171984508732 Unigram: 2.201922105462404
2022-02-02 10:01:32 | INFO | fairseq.trainer | begin training epoch 40
2022-02-02 10:01:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:01:52 | INFO | train_inner | epoch 040:      4 / 64 loss=8.211, ppl=296.39, wps=5792.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=2500, lr=0.000312538, gnorm=0.519, train_wall=505, gb_free=6.1, wall=13808
2022-02-02 10:06:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 10:07:25 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.237 | ppl 603.4 | wps 7971.2 | wpb 2034.1 | bsz 4 | num_updates 2560
2022-02-02 10:07:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 2560 updates
2022-02-02 10:07:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint40.pt
2022-02-02 10:07:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint40.pt
2022-02-02 10:07:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint40.pt (epoch 40 @ 2560 updates, score 9.237) (writing took 4.343812194652855 seconds)
2022-02-02 10:07:29 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-02-02 10:07:29 | INFO | train | epoch 040 | loss 8.128 | ppl 279.82 | wps 5844.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2560 | lr 0.000320036 | gnorm 0.51 | train_wall 324 | gb_free 6.1 | wall 14145
KL Stats: Epoch 40 Divergences: Uniform: 2.3407538630303097 Unigram: 2.2243209651349334
2022-02-02 10:07:29 | INFO | fairseq.trainer | begin training epoch 41
2022-02-02 10:07:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:10:53 | INFO | train_inner | epoch 041:     40 / 64 loss=8.097, ppl=273.77, wps=6047.4, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=2600, lr=0.000325035, gnorm=0.507, train_wall=506, gb_free=6.1, wall=14348
2022-02-02 10:12:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:13:21 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.228 | ppl 599.76 | wps 7964.3 | wpb 2034.1 | bsz 4 | num_updates 2624 | best_loss 9.228
2022-02-02 10:13:21 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-02-02 10:13:21 | INFO | train | epoch 041 | loss 8.061 | ppl 267.07 | wps 5934.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2624 | lr 0.000328034 | gnorm 0.507 | train_wall 323 | gb_free 6.1 | wall 14496
KL Stats: Epoch 41 Divergences: Uniform: 2.361501103552299 Unigram: 2.2603890395302058
2022-02-02 10:13:21 | INFO | fairseq.trainer | begin training epoch 42
2022-02-02 10:13:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:18:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:19:14 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.235 | ppl 602.38 | wps 7967.6 | wpb 2034.1 | bsz 4 | num_updates 2688 | best_loss 9.235
2022-02-02 10:19:14 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-02-02 10:19:14 | INFO | train | epoch 042 | loss 8.001 | ppl 256.17 | wps 5917.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2688 | lr 0.000336033 | gnorm 0.507 | train_wall 324 | gb_free 6.1 | wall 14849
KL Stats: Epoch 42 Divergences: Uniform: 2.3748612680066916 Unigram: 2.283418102365994
2022-02-02 10:19:14 | INFO | fairseq.trainer | begin training epoch 43
2022-02-02 10:19:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:20:15 | INFO | train_inner | epoch 043:     12 / 64 loss=8.008, ppl=257.47, wps=5795, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=2700, lr=0.000337533, gnorm=0.508, train_wall=505, gb_free=6.1, wall=14911
2022-02-02 10:24:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:25:06 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.229 | ppl 599.97 | wps 7970.9 | wpb 2034.1 | bsz 4 | num_updates 2752 | best_loss 9.229
2022-02-02 10:25:06 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-02-02 10:25:06 | INFO | train | epoch 043 | loss 7.936 | ppl 244.91 | wps 5930.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2752 | lr 0.000344031 | gnorm 0.494 | train_wall 323 | gb_free 6.1 | wall 15202
KL Stats: Epoch 43 Divergences: Uniform: 2.397686465919891 Unigram: 2.311229893540045
2022-02-02 10:25:06 | INFO | fairseq.trainer | begin training epoch 44
2022-02-02 10:25:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:29:11 | INFO | train_inner | epoch 044:     48 / 64 loss=7.906, ppl=239.82, wps=6096.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=2800, lr=0.00035003, gnorm=0.494, train_wall=506, gb_free=6.1, wall=15447
2022-02-02 10:30:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:30:59 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.24 | ppl 604.63 | wps 7931.3 | wpb 2034.1 | bsz 4 | num_updates 2816 | best_loss 9.237
2022-02-02 10:30:59 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-02-02 10:30:59 | INFO | train | epoch 044 | loss 7.875 | ppl 234.76 | wps 5915.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2816 | lr 0.00035203 | gnorm 0.497 | train_wall 324 | gb_free 6.1 | wall 15555
KL Stats: Epoch 44 Divergences: Uniform: 2.423807836457247 Unigram: 2.333640750651353
2022-02-02 10:30:59 | INFO | fairseq.trainer | begin training epoch 45
2022-02-02 10:30:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:36:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:36:52 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.268 | ppl 616.34 | wps 7990.3 | wpb 2034.1 | bsz 4 | num_updates 2880 | best_loss 9.237
2022-02-02 10:36:52 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-02-02 10:36:52 | INFO | train | epoch 045 | loss 7.816 | ppl 225.38 | wps 5926.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2880 | lr 0.000360028 | gnorm 0.516 | train_wall 323 | gb_free 6.1 | wall 15907
KL Stats: Epoch 45 Divergences: Uniform: 2.432058756583223 Unigram: 2.3607096915450354
2022-02-02 10:36:52 | INFO | fairseq.trainer | begin training epoch 46
2022-02-02 10:36:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:38:33 | INFO | train_inner | epoch 046:     20 / 64 loss=7.814, ppl=225.1, wps=5798.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=2900, lr=0.000362528, gnorm=0.508, train_wall=505, gb_free=6.1, wall=16009
2022-02-02 10:42:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 10:42:44 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.197 | ppl 587.05 | wps 7969.2 | wpb 2034.1 | bsz 4 | num_updates 2944 | best_loss 9.197
2022-02-02 10:42:44 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-02-02 10:42:44 | INFO | train | epoch 046 | loss 7.755 | ppl 215.94 | wps 5928.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2944 | lr 0.000368026 | gnorm 0.493 | train_wall 323 | gb_free 6.1 | wall 16259
KL Stats: Epoch 46 Divergences: Uniform: 2.4490572421009995 Unigram: 2.381561197707561
2022-02-02 10:42:44 | INFO | fairseq.trainer | begin training epoch 47
2022-02-02 10:42:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:47:30 | INFO | train_inner | epoch 047:     56 / 64 loss=7.724, ppl=211.42, wps=6096.3, ups=0.19, wpb=32686.1, bsz=63.8, num_updates=3000, lr=0.000375025, gnorm=0.501, train_wall=506, gb_free=6.1, wall=16545
2022-02-02 10:48:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:48:37 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.233 | ppl 601.66 | wps 7960.1 | wpb 2034.1 | bsz 4 | num_updates 3008 | best_loss 9.233
2022-02-02 10:48:37 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-02-02 10:48:37 | INFO | train | epoch 047 | loss 7.699 | ppl 207.73 | wps 5923 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3008 | lr 0.000376025 | gnorm 0.507 | train_wall 324 | gb_free 6.1 | wall 16612
KL Stats: Epoch 47 Divergences: Uniform: 2.476501645588949 Unigram: 2.410084366639667
2022-02-02 10:48:37 | INFO | fairseq.trainer | begin training epoch 48
2022-02-02 10:48:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:54:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:54:29 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.257 | ppl 611.85 | wps 7982.6 | wpb 2034.1 | bsz 4 | num_updates 3072 | best_loss 9.237
2022-02-02 10:54:29 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-02-02 10:54:29 | INFO | train | epoch 048 | loss 7.64 | ppl 199.48 | wps 5928.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3072 | lr 0.000384023 | gnorm 0.505 | train_wall 323 | gb_free 6.1 | wall 16964
KL Stats: Epoch 48 Divergences: Uniform: 2.482136415048964 Unigram: 2.4275280409159787
2022-02-02 10:54:29 | INFO | fairseq.trainer | begin training epoch 49
2022-02-02 10:54:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:56:52 | INFO | train_inner | epoch 049:     28 / 64 loss=7.627, ppl=197.74, wps=5796.3, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=3100, lr=0.000387523, gnorm=0.506, train_wall=505, gb_free=6.1, wall=17107
2022-02-02 10:59:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:00:21 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.245 | ppl 606.71 | wps 8001.4 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 9.237
2022-02-02 11:00:21 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-02-02 11:00:21 | INFO | train | epoch 049 | loss 7.585 | ppl 192 | wps 5925.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3136 | lr 0.000392022 | gnorm 0.505 | train_wall 323 | gb_free 6.1 | wall 17317
KL Stats: Epoch 49 Divergences: Uniform: 2.499831385682331 Unigram: 2.44958098983426
2022-02-02 11:00:21 | INFO | fairseq.trainer | begin training epoch 50
2022-02-02 11:00:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:05:46 | INFO | train_inner | epoch 050:     64 / 64 loss=7.555, ppl=188.01, wps=6098.7, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=3200, lr=0.00040002, gnorm=0.512, train_wall=505, gb_free=6.1, wall=17642
2022-02-02 11:05:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:06:14 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.225 | ppl 598.33 | wps 7957.9 | wpb 2034.1 | bsz 4 | num_updates 3200 | best_loss 9.225
2022-02-02 11:06:14 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-02-02 11:06:14 | INFO | train | epoch 050 | loss 7.531 | ppl 184.96 | wps 5924.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3200 | lr 0.00040002 | gnorm 0.518 | train_wall 323 | gb_free 6.1 | wall 17669
KL Stats: Epoch 50 Divergences: Uniform: 2.5204132263574714 Unigram: 2.4776365190836995
2022-02-02 11:06:14 | INFO | fairseq.trainer | begin training epoch 51
2022-02-02 11:06:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:11:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:12:06 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.213 | ppl 593.36 | wps 7985.4 | wpb 2034.1 | bsz 4 | num_updates 3264 | best_loss 9.213
2022-02-02 11:12:06 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-02-02 11:12:06 | INFO | train | epoch 051 | loss 7.478 | ppl 178.26 | wps 5929.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3264 | lr 0.000408018 | gnorm 0.519 | train_wall 323 | gb_free 6.1 | wall 18022
KL Stats: Epoch 51 Divergences: Uniform: 2.5378170136182083 Unigram: 2.4993133385129576
2022-02-02 11:12:06 | INFO | fairseq.trainer | begin training epoch 52
2022-02-02 11:12:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:15:10 | INFO | train_inner | epoch 052:     36 / 64 loss=7.451, ppl=174.99, wps=5800.7, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=3300, lr=0.000412518, gnorm=0.515, train_wall=506, gb_free=6.1, wall=18205
2022-02-02 11:17:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:17:59 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.217 | ppl 595.13 | wps 7972.5 | wpb 2034.1 | bsz 4 | num_updates 3328 | best_loss 9.217
2022-02-02 11:17:59 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-02-02 11:17:59 | INFO | train | epoch 052 | loss 7.424 | ppl 171.74 | wps 5922.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3328 | lr 0.000416017 | gnorm 0.509 | train_wall 324 | gb_free 6.1 | wall 18374
KL Stats: Epoch 52 Divergences: Uniform: 2.5430375646073924 Unigram: 2.51715089604953
2022-02-02 11:17:59 | INFO | fairseq.trainer | begin training epoch 53
2022-02-02 11:17:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:23:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:23:52 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.274 | ppl 619.24 | wps 7981.9 | wpb 2034.1 | bsz 4 | num_updates 3392 | best_loss 9.237
2022-02-02 11:23:52 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-02-02 11:23:52 | INFO | train | epoch 053 | loss 7.373 | ppl 165.77 | wps 5921.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3392 | lr 0.000424015 | gnorm 0.514 | train_wall 324 | gb_free 6.1 | wall 18727
KL Stats: Epoch 53 Divergences: Uniform: 2.5594747752229003 Unigram: 2.5379045156706614
2022-02-02 11:23:52 | INFO | fairseq.trainer | begin training epoch 54
2022-02-02 11:23:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:24:32 | INFO | train_inner | epoch 054:      8 / 64 loss=7.386, ppl=167.29, wps=5796.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=3400, lr=0.000425015, gnorm=0.512, train_wall=505, gb_free=6.1, wall=18768
2022-02-02 11:29:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:29:44 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.29 | ppl 626.02 | wps 7971.5 | wpb 2034.1 | bsz 4 | num_updates 3456 | best_loss 9.237
2022-02-02 11:29:44 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-02-02 11:29:44 | INFO | train | epoch 054 | loss 7.322 | ppl 160.05 | wps 5921.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3456 | lr 0.000432014 | gnorm 0.514 | train_wall 324 | gb_free 6.1 | wall 19080
KL Stats: Epoch 54 Divergences: Uniform: 2.582012738643939 Unigram: 2.5586485947390107
2022-02-02 11:29:44 | INFO | fairseq.trainer | begin training epoch 55
2022-02-02 11:29:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:33:29 | INFO | train_inner | epoch 055:     44 / 64 loss=7.295, ppl=157.01, wps=6093.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3500, lr=0.000437513, gnorm=0.521, train_wall=506, gb_free=6.1, wall=19304
2022-02-02 11:35:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:35:37 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.268 | ppl 616.41 | wps 8011.5 | wpb 2034.1 | bsz 4 | num_updates 3520 | best_loss 9.237
2022-02-02 11:35:37 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-02-02 11:35:37 | INFO | train | epoch 055 | loss 7.276 | ppl 155 | wps 5926.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3520 | lr 0.000440012 | gnorm 0.525 | train_wall 323 | gb_free 6.1 | wall 19432
KL Stats: Epoch 55 Divergences: Uniform: 2.5990063599147533 Unigram: 2.5771928140443006
2022-02-02 11:35:37 | INFO | fairseq.trainer | begin training epoch 56
2022-02-02 11:35:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:41:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:41:29 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.293 | ppl 627.47 | wps 7956.3 | wpb 2034.1 | bsz 4 | num_updates 3584 | best_loss 9.237
2022-02-02 11:41:29 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-02-02 11:41:29 | INFO | train | epoch 056 | loss 7.226 | ppl 149.69 | wps 5923.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3584 | lr 0.00044801 | gnorm 0.524 | train_wall 323 | gb_free 6.1 | wall 19785
KL Stats: Epoch 56 Divergences: Uniform: 2.61104650313305 Unigram: 2.5999437724543952
2022-02-02 11:41:29 | INFO | fairseq.trainer | begin training epoch 57
2022-02-02 11:41:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:42:51 | INFO | train_inner | epoch 057:     16 / 64 loss=7.233, ppl=150.4, wps=5798.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=3600, lr=0.00045001, gnorm=0.525, train_wall=505, gb_free=6.1, wall=19866
2022-02-02 11:46:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:47:22 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.299 | ppl 629.74 | wps 7977.4 | wpb 2034.1 | bsz 4 | num_updates 3648 | best_loss 9.237
2022-02-02 11:47:22 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-02-02 11:47:22 | INFO | train | epoch 057 | loss 7.179 | ppl 144.95 | wps 5925 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3648 | lr 0.000456009 | gnorm 0.539 | train_wall 323 | gb_free 6.1 | wall 20137
KL Stats: Epoch 57 Divergences: Uniform: 2.6147517157934317 Unigram: 2.6158991259995217
2022-02-02 11:47:22 | INFO | fairseq.trainer | begin training epoch 58
2022-02-02 11:47:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:51:46 | INFO | train_inner | epoch 058:     52 / 64 loss=7.158, ppl=142.78, wps=6100.7, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3700, lr=0.000462508, gnorm=0.531, train_wall=506, gb_free=6.1, wall=20402
2022-02-02 11:52:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:53:14 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.346 | ppl 650.94 | wps 7987.5 | wpb 2034.1 | bsz 4 | num_updates 3712 | best_loss 9.237
2022-02-02 11:53:14 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-02-02 11:53:14 | INFO | train | epoch 058 | loss 7.133 | ppl 140.36 | wps 5933.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3712 | lr 0.000464007 | gnorm 0.529 | train_wall 323 | gb_free 6.1 | wall 20489
KL Stats: Epoch 58 Divergences: Uniform: 2.6287788384975563 Unigram: 2.6335598562116154
2022-02-02 11:53:14 | INFO | fairseq.trainer | begin training epoch 59
2022-02-02 11:53:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:58:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:59:07 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.282 | ppl 622.42 | wps 7981.7 | wpb 2034.1 | bsz 4 | num_updates 3776 | best_loss 9.237
2022-02-02 11:59:07 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-02-02 11:59:07 | INFO | train | epoch 059 | loss 7.091 | ppl 136.29 | wps 5919 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3776 | lr 0.000472006 | gnorm 0.529 | train_wall 324 | gb_free 6.1 | wall 20842
KL Stats: Epoch 59 Divergences: Uniform: 2.6558583422972792 Unigram: 2.658993124803119
2022-02-02 11:59:07 | INFO | fairseq.trainer | begin training epoch 60
2022-02-02 11:59:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:01:09 | INFO | train_inner | epoch 060:     24 / 64 loss=7.083, ppl=135.53, wps=5793.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=3800, lr=0.000475005, gnorm=0.535, train_wall=505, gb_free=6.1, wall=20965
2022-02-02 12:04:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:04:59 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.4 | ppl 675.39 | wps 8007.6 | wpb 2034.1 | bsz 4 | num_updates 3840 | best_loss 9.237
2022-02-02 12:04:59 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-02-02 12:04:59 | INFO | train | epoch 060 | loss 7.044 | ppl 131.98 | wps 5922.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3840 | lr 0.000480004 | gnorm 0.543 | train_wall 324 | gb_free 6.1 | wall 21195
KL Stats: Epoch 60 Divergences: Uniform: 2.6585970991619736 Unigram: 2.672662308519806
2022-02-02 12:04:59 | INFO | fairseq.trainer | begin training epoch 61
2022-02-02 12:04:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:10:05 | INFO | train_inner | epoch 061:     60 / 64 loss=7.026, ppl=130.29, wps=6098.1, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=3900, lr=0.000487503, gnorm=0.552, train_wall=506, gb_free=6.1, wall=21501
2022-02-02 12:10:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:10:52 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.346 | ppl 650.7 | wps 8003.2 | wpb 2034.1 | bsz 4 | num_updates 3904 | best_loss 9.237
2022-02-02 12:10:52 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-02-02 12:10:52 | INFO | train | epoch 061 | loss 7.003 | ppl 128.23 | wps 5929.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3904 | lr 0.000488002 | gnorm 0.556 | train_wall 323 | gb_free 6.1 | wall 21547
KL Stats: Epoch 61 Divergences: Uniform: 2.6780925076430075 Unigram: 2.6957720712296864
2022-02-02 12:10:52 | INFO | fairseq.trainer | begin training epoch 62
2022-02-02 12:10:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:16:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:16:44 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.357 | ppl 655.69 | wps 7984.3 | wpb 2034.1 | bsz 4 | num_updates 3968 | best_loss 9.237
2022-02-02 12:16:44 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-02-02 12:16:44 | INFO | train | epoch 062 | loss 6.958 | ppl 124.31 | wps 5932.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3968 | lr 0.000496001 | gnorm 0.538 | train_wall 323 | gb_free 6.1 | wall 21899
KL Stats: Epoch 62 Divergences: Uniform: 2.692160365531776 Unigram: 2.7073814156740124
2022-02-02 12:16:44 | INFO | fairseq.trainer | begin training epoch 63
2022-02-02 12:16:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:19:26 | INFO | train_inner | epoch 063:     32 / 64 loss=6.938, ppl=122.58, wps=5807.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4000, lr=0.0005, gnorm=0.542, train_wall=504, gb_free=6.1, wall=22062
2022-02-02 12:22:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 12:22:36 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.357 | ppl 655.97 | wps 7964.5 | wpb 2034.1 | bsz 4 | num_updates 4032 | best_loss 9.237
2022-02-02 12:22:36 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-02-02 12:22:36 | INFO | train | epoch 063 | loss 6.919 | ppl 121.05 | wps 5931.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4032 | lr 0.000498012 | gnorm 0.551 | train_wall 323 | gb_free 6.1 | wall 22251
KL Stats: Epoch 63 Divergences: Uniform: 2.707843293627262 Unigram: 2.7222503086645444
2022-02-02 12:22:36 | INFO | fairseq.trainer | begin training epoch 64
2022-02-02 12:22:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:28:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:28:29 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.376 | ppl 664.54 | wps 7988.5 | wpb 2034.1 | bsz 4 | num_updates 4096 | best_loss 9.237
2022-02-02 12:28:29 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-02-02 12:28:29 | INFO | train | epoch 064 | loss 6.875 | ppl 117.38 | wps 5914.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4096 | lr 0.000494106 | gnorm 0.546 | train_wall 324 | gb_free 6.1 | wall 22604
KL Stats: Epoch 64 Divergences: Uniform: 2.719751812752437 Unigram: 2.7466979058541248
2022-02-02 12:28:29 | INFO | fairseq.trainer | begin training epoch 65
2022-02-02 12:28:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:28:49 | INFO | train_inner | epoch 065:      4 / 64 loss=6.895, ppl=119.04, wps=5791.9, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4100, lr=0.000493865, gnorm=0.549, train_wall=505, gb_free=6.1, wall=22625
2022-02-02 12:33:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:34:21 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.38 | ppl 666.36 | wps 7981.7 | wpb 2034.1 | bsz 4 | num_updates 4160 | best_loss 9.237
2022-02-02 12:34:21 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-02-02 12:34:21 | INFO | train | epoch 065 | loss 6.831 | ppl 113.83 | wps 5928.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4160 | lr 0.00049029 | gnorm 0.546 | train_wall 323 | gb_free 6.1 | wall 22957
KL Stats: Epoch 65 Divergences: Uniform: 2.73391211133399 Unigram: 2.762948697691645
2022-02-02 12:34:21 | INFO | fairseq.trainer | begin training epoch 66
2022-02-02 12:34:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:37:45 | INFO | train_inner | epoch 066:     40 / 64 loss=6.806, ppl=111.87, wps=6102.6, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4200, lr=0.00048795, gnorm=0.542, train_wall=506, gb_free=6.1, wall=23160
2022-02-02 12:39:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:40:13 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.429 | ppl 689.39 | wps 7967.8 | wpb 2034.1 | bsz 4 | num_updates 4224 | best_loss 9.237
2022-02-02 12:40:13 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-02-02 12:40:13 | INFO | train | epoch 066 | loss 6.792 | ppl 110.81 | wps 5930.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4224 | lr 0.000486562 | gnorm 0.542 | train_wall 323 | gb_free 6.1 | wall 23309
KL Stats: Epoch 66 Divergences: Uniform: 2.7384187422801904 Unigram: 2.7848564865215586
2022-02-02 12:40:13 | INFO | fairseq.trainer | begin training epoch 67
2022-02-02 12:40:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:45:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:46:06 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.436 | ppl 692.49 | wps 7966.4 | wpb 2034.1 | bsz 4 | num_updates 4288 | best_loss 9.237
2022-02-02 12:46:06 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-02-02 12:46:06 | INFO | train | epoch 067 | loss 6.754 | ppl 107.92 | wps 5923.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4288 | lr 0.000482917 | gnorm 0.559 | train_wall 323 | gb_free 6.1 | wall 23661
KL Stats: Epoch 67 Divergences: Uniform: 2.7577133428452547 Unigram: 2.8031434405394995
2022-02-02 12:46:06 | INFO | fairseq.trainer | begin training epoch 68
2022-02-02 12:46:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:47:07 | INFO | train_inner | epoch 068:     12 / 64 loss=6.762, ppl=108.56, wps=5796.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4300, lr=0.000482243, gnorm=0.556, train_wall=505, gb_free=6.1, wall=23723
2022-02-02 12:51:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:51:59 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.401 | ppl 676.26 | wps 7985.7 | wpb 2034.1 | bsz 4 | num_updates 4352 | best_loss 9.237
2022-02-02 12:51:59 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-02-02 12:51:59 | INFO | train | epoch 068 | loss 6.712 | ppl 104.85 | wps 5919.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4352 | lr 0.000479353 | gnorm 0.55 | train_wall 324 | gb_free 6.1 | wall 24014
KL Stats: Epoch 68 Divergences: Uniform: 2.766863779972989 Unigram: 2.820899787924843
2022-02-02 12:51:59 | INFO | fairseq.trainer | begin training epoch 69
2022-02-02 12:51:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:56:03 | INFO | train_inner | epoch 069:     48 / 64 loss=6.694, ppl=103.53, wps=6101.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4400, lr=0.000476731, gnorm=0.549, train_wall=506, gb_free=6.1, wall=24258
2022-02-02 12:57:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:57:50 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.523 | ppl 735.74 | wps 7986.7 | wpb 2034.1 | bsz 4 | num_updates 4416 | best_loss 9.237
2022-02-02 12:57:50 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-02-02 12:57:50 | INFO | train | epoch 069 | loss 6.676 | ppl 102.22 | wps 5943.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4416 | lr 0.000475867 | gnorm 0.552 | train_wall 322 | gb_free 6.1 | wall 24366
KL Stats: Epoch 69 Divergences: Uniform: 2.777923279214953 Unigram: 2.833468091472011
2022-02-02 12:57:50 | INFO | fairseq.trainer | begin training epoch 70
2022-02-02 12:57:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:03:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:03:42 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.493 | ppl 720.36 | wps 7982.3 | wpb 2034.1 | bsz 4 | num_updates 4480 | best_loss 9.237
2022-02-02 13:03:42 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-02-02 13:03:42 | INFO | train | epoch 070 | loss 6.639 | ppl 99.69 | wps 5937.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4480 | lr 0.000472456 | gnorm 0.556 | train_wall 323 | gb_free 6.1 | wall 24717
KL Stats: Epoch 70 Divergences: Uniform: 2.795820994319992 Unigram: 2.8543574311603086
2022-02-02 13:03:42 | INFO | fairseq.trainer | begin training epoch 71
2022-02-02 13:03:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:05:24 | INFO | train_inner | epoch 071:     20 / 64 loss=6.633, ppl=99.26, wps=5810.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4500, lr=0.000471405, gnorm=0.554, train_wall=504, gb_free=6.1, wall=24819
2022-02-02 13:09:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:09:34 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.448 | ppl 698.34 | wps 7981 | wpb 2034.1 | bsz 4 | num_updates 4544 | best_loss 9.237
2022-02-02 13:09:34 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-02-02 13:09:34 | INFO | train | epoch 071 | loss 6.602 | ppl 97.12 | wps 5929.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4544 | lr 0.000469117 | gnorm 0.563 | train_wall 323 | gb_free 6.1 | wall 25070
KL Stats: Epoch 71 Divergences: Uniform: 2.8038368409229184 Unigram: 2.8692781955890005
2022-02-02 13:09:34 | INFO | fairseq.trainer | begin training epoch 72
2022-02-02 13:09:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:14:19 | INFO | train_inner | epoch 072:     56 / 64 loss=6.591, ppl=96.4, wps=6102.7, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=4600, lr=0.000466252, gnorm=0.561, train_wall=506, gb_free=6.1, wall=25355
2022-02-02 13:14:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:15:26 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.466 | ppl 707.22 | wps 8001.2 | wpb 2034.1 | bsz 4 | num_updates 4608 | best_loss 9.237
2022-02-02 13:15:26 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-02-02 13:15:26 | INFO | train | epoch 072 | loss 6.569 | ppl 94.92 | wps 5932.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4608 | lr 0.000465847 | gnorm 0.555 | train_wall 323 | gb_free 6.1 | wall 25422
KL Stats: Epoch 72 Divergences: Uniform: 2.8151443344299074 Unigram: 2.8883699613097926
2022-02-02 13:15:26 | INFO | fairseq.trainer | begin training epoch 73
2022-02-02 13:15:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:20:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:21:19 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.465 | ppl 706.55 | wps 7936.4 | wpb 2034.1 | bsz 4 | num_updates 4672 | best_loss 9.237
2022-02-02 13:21:19 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-02-02 13:21:19 | INFO | train | epoch 073 | loss 6.538 | ppl 92.95 | wps 5925.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4672 | lr 0.000462646 | gnorm 0.584 | train_wall 323 | gb_free 6.1 | wall 25774
KL Stats: Epoch 73 Divergences: Uniform: 2.824962241777673 Unigram: 2.904060898174247
2022-02-02 13:21:19 | INFO | fairseq.trainer | begin training epoch 74
2022-02-02 13:21:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:23:42 | INFO | train_inner | epoch 074:     28 / 64 loss=6.525, ppl=92.09, wps=5799.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4700, lr=0.000461266, gnorm=0.579, train_wall=505, gb_free=6.1, wall=25917
2022-02-02 13:26:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 13:27:11 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.502 | ppl 725.28 | wps 7974.1 | wpb 2034.1 | bsz 4 | num_updates 4736 | best_loss 9.237
2022-02-02 13:27:11 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-02-02 13:27:11 | INFO | train | epoch 074 | loss 6.503 | ppl 90.71 | wps 5928.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4736 | lr 0.000459509 | gnorm 0.572 | train_wall 323 | gb_free 6.1 | wall 26126
KL Stats: Epoch 74 Divergences: Uniform: 2.8281019973162276 Unigram: 2.9193324318897234
2022-02-02 13:27:11 | INFO | fairseq.trainer | begin training epoch 75
2022-02-02 13:27:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:32:36 | INFO | train_inner | epoch 075:     64 / 64 loss=6.492, ppl=90.04, wps=6102, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=4800, lr=0.000456435, gnorm=0.567, train_wall=504, gb_free=6.1, wall=26451
2022-02-02 13:32:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:33:03 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.54 | ppl 744.69 | wps 7965.7 | wpb 2034.1 | bsz 4 | num_updates 4800 | best_loss 9.237
2022-02-02 13:33:03 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-02-02 13:33:03 | INFO | train | epoch 075 | loss 6.471 | ppl 88.68 | wps 5929.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4800 | lr 0.000456435 | gnorm 0.566 | train_wall 323 | gb_free 6.1 | wall 26479
KL Stats: Epoch 75 Divergences: Uniform: 2.8318730590082093 Unigram: 2.9388553187204796
2022-02-02 13:33:03 | INFO | fairseq.trainer | begin training epoch 76
2022-02-02 13:33:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:38:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:38:56 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 9.596 | ppl 773.74 | wps 7981.6 | wpb 2034.1 | bsz 4 | num_updates 4864 | best_loss 9.237
2022-02-02 13:38:56 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-02-02 13:38:56 | INFO | train | epoch 076 | loss 6.442 | ppl 86.97 | wps 5924.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4864 | lr 0.000453423 | gnorm 0.578 | train_wall 323 | gb_free 6.1 | wall 26831
KL Stats: Epoch 76 Divergences: Uniform: 2.843884879834801 Unigram: 2.9555096109866885
2022-02-02 13:38:56 | INFO | fairseq.trainer | begin training epoch 77
2022-02-02 13:38:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:41:59 | INFO | train_inner | epoch 077:     36 / 64 loss=6.42, ppl=85.6, wps=5800, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=4900, lr=0.000451754, gnorm=0.581, train_wall=506, gb_free=6.1, wall=27015
2022-02-02 13:44:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:44:48 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 9.534 | ppl 741.36 | wps 8017.5 | wpb 2034.1 | bsz 4 | num_updates 4928 | best_loss 9.237
2022-02-02 13:44:48 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-02-02 13:44:48 | INFO | train | epoch 077 | loss 6.412 | ppl 85.17 | wps 5930.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4928 | lr 0.000450469 | gnorm 0.579 | train_wall 323 | gb_free 6.1 | wall 27183
KL Stats: Epoch 77 Divergences: Uniform: 2.8593954318267105 Unigram: 2.9706256459460914
2022-02-02 13:44:48 | INFO | fairseq.trainer | begin training epoch 78
2022-02-02 13:44:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:50:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 13:50:41 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 9.579 | ppl 764.62 | wps 7990.8 | wpb 2034.1 | bsz 4 | num_updates 4992 | best_loss 9.237
2022-02-02 13:50:41 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-02-02 13:50:41 | INFO | train | epoch 078 | loss 6.382 | ppl 83.42 | wps 5920.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4992 | lr 0.000447572 | gnorm 0.58 | train_wall 324 | gb_free 6.1 | wall 27536
KL Stats: Epoch 78 Divergences: Uniform: 2.8652479293025763 Unigram: 2.9863340557282942
2022-02-02 13:50:41 | INFO | fairseq.trainer | begin training epoch 79
2022-02-02 13:50:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:51:21 | INFO | train_inner | epoch 079:      8 / 64 loss=6.397, ppl=84.26, wps=5798.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5000, lr=0.000447214, gnorm=0.578, train_wall=505, gb_free=6.1, wall=27577
2022-02-02 13:56:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:56:33 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 9.656 | ppl 806.74 | wps 7972 | wpb 2034.1 | bsz 4 | num_updates 5056 | best_loss 9.237
2022-02-02 13:56:33 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-02-02 13:56:33 | INFO | train | epoch 079 | loss 6.357 | ppl 81.98 | wps 5920.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5056 | lr 0.00044473 | gnorm 0.579 | train_wall 324 | gb_free 6.1 | wall 27889
KL Stats: Epoch 79 Divergences: Uniform: 2.8697691511906784 Unigram: 3.007059862917389
2022-02-02 13:56:33 | INFO | fairseq.trainer | begin training epoch 80
2022-02-02 13:56:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:00:18 | INFO | train_inner | epoch 080:     44 / 64 loss=6.338, ppl=80.87, wps=6095.7, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=5100, lr=0.000442807, gnorm=0.589, train_wall=506, gb_free=6.1, wall=28113
2022-02-02 14:01:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 14:02:26 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 9.598 | ppl 775.1 | wps 7967.3 | wpb 2034.1 | bsz 4 | num_updates 5120 | best_loss 9.237
2022-02-02 14:02:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 5120 updates
2022-02-02 14:02:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint80.pt
2022-02-02 14:02:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint80.pt
2022-02-02 14:02:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint80.pt (epoch 80 @ 5120 updates, score 9.598) (writing took 2.886670551262796 seconds)
2022-02-02 14:02:29 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-02-02 14:02:29 | INFO | train | epoch 080 | loss 6.329 | ppl 80.41 | wps 5881.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5120 | lr 0.000441942 | gnorm 0.603 | train_wall 323 | gb_free 6.1 | wall 28244
KL Stats: Epoch 80 Divergences: Uniform: 2.883521333250849 Unigram: 3.0111077463579243
2022-02-02 14:02:29 | INFO | fairseq.trainer | begin training epoch 81
2022-02-02 14:02:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:07:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 14:08:20 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 9.629 | ppl 791.6 | wps 7980.8 | wpb 2034.1 | bsz 4 | num_updates 5184 | best_loss 9.237
2022-02-02 14:08:20 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-02-02 14:08:20 | INFO | train | epoch 081 | loss 6.304 | ppl 79.02 | wps 5940.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5184 | lr 0.000439205 | gnorm 0.601 | train_wall 323 | gb_free 6.1 | wall 28596
KL Stats: Epoch 81 Divergences: Uniform: 2.8912157922440826 Unigram: 3.0325155946026365
2022-02-02 14:08:20 | INFO | fairseq.trainer | begin training epoch 82
2022-02-02 14:08:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:09:42 | INFO | train_inner | epoch 082:     16 / 64 loss=6.307, ppl=79.16, wps=5777.5, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=5200, lr=0.000438529, gnorm=0.598, train_wall=504, gb_free=6.1, wall=28677
2022-02-02 14:13:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 14:14:13 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.596 | ppl 774 | wps 7972.3 | wpb 2034.1 | bsz 4 | num_updates 5248 | best_loss 9.237
2022-02-02 14:14:13 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-02-02 14:14:13 | INFO | train | epoch 082 | loss 6.277 | ppl 77.57 | wps 5926.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5248 | lr 0.000436519 | gnorm 0.618 | train_wall 323 | gb_free 6.1 | wall 28948
KL Stats: Epoch 82 Divergences: Uniform: 2.9052231774243364 Unigram: 3.0413745136404904
2022-02-02 14:14:13 | INFO | fairseq.trainer | begin training epoch 83
2022-02-02 14:14:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:18:38 | INFO | train_inner | epoch 083:     52 / 64 loss=6.266, ppl=76.96, wps=6095.2, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=5300, lr=0.000434372, gnorm=0.611, train_wall=506, gb_free=6.1, wall=29213
2022-02-02 14:19:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 14:20:05 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 9.609 | ppl 780.96 | wps 7987.5 | wpb 2034.1 | bsz 4 | num_updates 5312 | best_loss 9.237
2022-02-02 14:20:05 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-02-02 14:20:05 | INFO | train | epoch 083 | loss 6.253 | ppl 76.25 | wps 5922.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5312 | lr 0.000433881 | gnorm 0.596 | train_wall 324 | gb_free 6.1 | wall 29301
KL Stats: Epoch 83 Divergences: Uniform: 2.9031376324385683 Unigram: 3.061476788307051
2022-02-02 14:20:05 | INFO | fairseq.trainer | begin training epoch 84
2022-02-02 14:20:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:25:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 14:25:58 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 9.556 | ppl 752.8 | wps 7973.7 | wpb 2034.1 | bsz 4 | num_updates 5376 | best_loss 9.237
2022-02-02 14:25:58 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-02-02 14:25:58 | INFO | train | epoch 084 | loss 6.229 | ppl 74.99 | wps 5921.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5376 | lr 0.000431291 | gnorm 0.604 | train_wall 324 | gb_free 6.1 | wall 29653
KL Stats: Epoch 84 Divergences: Uniform: 2.9154606413640183 Unigram: 3.0729277084204973
2022-02-02 14:25:58 | INFO | fairseq.trainer | begin training epoch 85
2022-02-02 14:25:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:28:00 | INFO | train_inner | epoch 085:     24 / 64 loss=6.221, ppl=74.6, wps=5799.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5400, lr=0.000430331, gnorm=0.607, train_wall=505, gb_free=6.1, wall=29776
2022-02-02 14:31:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 14:31:50 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 9.628 | ppl 791.19 | wps 7970.9 | wpb 2034.1 | bsz 4 | num_updates 5440 | best_loss 9.237
2022-02-02 14:31:50 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-02-02 14:31:50 | INFO | train | epoch 085 | loss 6.205 | ppl 73.76 | wps 5937.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5440 | lr 0.000428746 | gnorm 0.616 | train_wall 323 | gb_free 6.1 | wall 30005
KL Stats: Epoch 85 Divergences: Uniform: 2.9197820916580084 Unigram: 3.0770150189710836
2022-02-02 14:31:50 | INFO | fairseq.trainer | begin training epoch 86
2022-02-02 14:31:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:36:55 | INFO | train_inner | epoch 086:     60 / 64 loss=6.2, ppl=73.54, wps=6107.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=5500, lr=0.000426401, gnorm=0.62, train_wall=505, gb_free=6.1, wall=30311
2022-02-02 14:37:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 14:37:42 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 9.6 | ppl 776.03 | wps 8025.2 | wpb 2034.1 | bsz 4 | num_updates 5504 | best_loss 9.237
2022-02-02 14:37:42 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-02-02 14:37:42 | INFO | train | epoch 086 | loss 6.181 | ppl 72.57 | wps 5938.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5504 | lr 0.000426246 | gnorm 0.623 | train_wall 323 | gb_free 6.1 | wall 30357
KL Stats: Epoch 86 Divergences: Uniform: 2.930600420295319 Unigram: 3.102590586733829
2022-02-02 14:37:42 | INFO | fairseq.trainer | begin training epoch 87
2022-02-02 14:37:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:43:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 14:43:34 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 9.697 | ppl 830.28 | wps 7965.1 | wpb 2034.1 | bsz 4 | num_updates 5568 | best_loss 9.237
2022-02-02 14:43:34 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-02-02 14:43:34 | INFO | train | epoch 087 | loss 6.158 | ppl 71.42 | wps 5918.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5568 | lr 0.00042379 | gnorm 0.62 | train_wall 324 | gb_free 6.1 | wall 30710
KL Stats: Epoch 87 Divergences: Uniform: 2.93599080309252 Unigram: 3.1157697579408823
2022-02-02 14:43:34 | INFO | fairseq.trainer | begin training epoch 88
2022-02-02 14:43:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:46:18 | INFO | train_inner | epoch 088:     32 / 64 loss=6.141, ppl=70.57, wps=5793.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=5600, lr=0.000422577, gnorm=0.631, train_wall=505, gb_free=6.1, wall=30873
2022-02-02 14:49:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 14:49:27 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 9.704 | ppl 833.98 | wps 7972.8 | wpb 2034.1 | bsz 4 | num_updates 5632 | best_loss 9.237
2022-02-02 14:49:27 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-02-02 14:49:27 | INFO | train | epoch 088 | loss 6.136 | ppl 70.33 | wps 5919.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5632 | lr 0.000421375 | gnorm 0.637 | train_wall 324 | gb_free 6.1 | wall 31063
KL Stats: Epoch 88 Divergences: Uniform: 2.9320324903268893 Unigram: 3.1269893485389706
2022-02-02 14:49:27 | INFO | fairseq.trainer | begin training epoch 89
2022-02-02 14:49:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:54:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 14:55:20 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 9.694 | ppl 828.35 | wps 7959.1 | wpb 2034.1 | bsz 4 | num_updates 5696 | best_loss 9.237
2022-02-02 14:55:20 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-02-02 14:55:20 | INFO | train | epoch 089 | loss 6.115 | ppl 69.31 | wps 5921.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5696 | lr 0.000419001 | gnorm 0.634 | train_wall 324 | gb_free 6.1 | wall 31415
KL Stats: Epoch 89 Divergences: Uniform: 2.9414119299284573 Unigram: 3.137682577900182
2022-02-02 14:55:20 | INFO | fairseq.trainer | begin training epoch 90
2022-02-02 14:55:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:55:40 | INFO | train_inner | epoch 090:      4 / 64 loss=6.133, ppl=70.17, wps=5794.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=5700, lr=0.000418854, gnorm=0.63, train_wall=505, gb_free=6.1, wall=31436
2022-02-02 15:00:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:01:12 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 9.684 | ppl 822.48 | wps 8004.1 | wpb 2034.1 | bsz 4 | num_updates 5760 | best_loss 9.237
2022-02-02 15:01:12 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-02-02 15:01:12 | INFO | train | epoch 090 | loss 6.092 | ppl 68.22 | wps 5932.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5760 | lr 0.000416667 | gnorm 0.627 | train_wall 323 | gb_free 6.1 | wall 31767
KL Stats: Epoch 90 Divergences: Uniform: 2.9545406960827267 Unigram: 3.149264395739627
2022-02-02 15:01:12 | INFO | fairseq.trainer | begin training epoch 91
2022-02-02 15:01:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:04:36 | INFO | train_inner | epoch 091:     40 / 64 loss=6.076, ppl=67.47, wps=6103.5, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5800, lr=0.000415227, gnorm=0.631, train_wall=506, gb_free=6.1, wall=31971
2022-02-02 15:06:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:07:04 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 9.642 | ppl 799.22 | wps 7990.8 | wpb 2034.1 | bsz 4 | num_updates 5824 | best_loss 9.237
2022-02-02 15:07:04 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-02-02 15:07:04 | INFO | train | epoch 091 | loss 6.074 | ppl 67.37 | wps 5933 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5824 | lr 0.000414371 | gnorm 0.641 | train_wall 323 | gb_free 6.1 | wall 32120
KL Stats: Epoch 91 Divergences: Uniform: 2.960334194854689 Unigram: 3.1627031990648957
2022-02-02 15:07:04 | INFO | fairseq.trainer | begin training epoch 92
2022-02-02 15:07:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:12:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 15:12:57 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 9.683 | ppl 821.98 | wps 7988.9 | wpb 2034.1 | bsz 4 | num_updates 5888 | best_loss 9.237
2022-02-02 15:12:57 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-02-02 15:12:57 | INFO | train | epoch 092 | loss 6.053 | ppl 66.38 | wps 5914.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5888 | lr 0.000412113 | gnorm 0.632 | train_wall 324 | gb_free 6.1 | wall 32473
KL Stats: Epoch 92 Divergences: Uniform: 2.961339622934395 Unigram: 3.1753527513885835
2022-02-02 15:12:57 | INFO | fairseq.trainer | begin training epoch 93
2022-02-02 15:12:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:13:58 | INFO | train_inner | epoch 093:     12 / 64 loss=6.057, ppl=66.6, wps=5795, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5900, lr=0.000411693, gnorm=0.638, train_wall=505, gb_free=6.1, wall=32534
2022-02-02 15:18:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:18:50 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 9.752 | ppl 862.28 | wps 7965.3 | wpb 2034.1 | bsz 4 | num_updates 5952 | best_loss 9.237
2022-02-02 15:18:50 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-02-02 15:18:50 | INFO | train | epoch 093 | loss 6.033 | ppl 65.46 | wps 5920.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5952 | lr 0.000409891 | gnorm 0.652 | train_wall 324 | gb_free 6.1 | wall 32825
KL Stats: Epoch 93 Divergences: Uniform: 2.965495467556354 Unigram: 3.194317117776545
2022-02-02 15:18:50 | INFO | fairseq.trainer | begin training epoch 94
2022-02-02 15:18:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:22:55 | INFO | train_inner | epoch 094:     48 / 64 loss=6.024, ppl=65.06, wps=6089, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=6000, lr=0.000408248, gnorm=0.647, train_wall=507, gb_free=6.1, wall=33071
2022-02-02 15:24:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:24:43 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 9.743 | ppl 856.67 | wps 7987.2 | wpb 2034.1 | bsz 4 | num_updates 6016 | best_loss 9.237
2022-02-02 15:24:43 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-02-02 15:24:43 | INFO | train | epoch 094 | loss 6.014 | ppl 64.61 | wps 5920.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6016 | lr 0.000407705 | gnorm 0.656 | train_wall 324 | gb_free 6.1 | wall 33178
KL Stats: Epoch 94 Divergences: Uniform: 2.974013046852176 Unigram: 3.2016684516927056
2022-02-02 15:24:43 | INFO | fairseq.trainer | begin training epoch 95
2022-02-02 15:24:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:30:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:30:35 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 9.777 | ppl 877.27 | wps 8003.8 | wpb 2034.1 | bsz 4 | num_updates 6080 | best_loss 9.237
2022-02-02 15:30:35 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-02-02 15:30:35 | INFO | train | epoch 095 | loss 5.995 | ppl 63.76 | wps 5926.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6080 | lr 0.000405554 | gnorm 0.657 | train_wall 323 | gb_free 6.1 | wall 33531
KL Stats: Epoch 95 Divergences: Uniform: 2.9751777819300353 Unigram: 3.2088781319561925
2022-02-02 15:30:35 | INFO | fairseq.trainer | begin training epoch 96
2022-02-02 15:30:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:32:17 | INFO | train_inner | epoch 096:     20 / 64 loss=5.996, ppl=63.82, wps=5797.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6100, lr=0.000404888, gnorm=0.66, train_wall=505, gb_free=6.1, wall=33633
2022-02-02 15:36:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:36:28 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 9.698 | ppl 830.65 | wps 7987.2 | wpb 2034.1 | bsz 4 | num_updates 6144 | best_loss 9.237
2022-02-02 15:36:28 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-02-02 15:36:28 | INFO | train | epoch 096 | loss 5.977 | ppl 62.99 | wps 5920.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6144 | lr 0.000403436 | gnorm 0.653 | train_wall 324 | gb_free 6.1 | wall 33883
KL Stats: Epoch 96 Divergences: Uniform: 2.9864716555243644 Unigram: 3.2260876171757156
2022-02-02 15:36:28 | INFO | fairseq.trainer | begin training epoch 97
2022-02-02 15:36:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:41:14 | INFO | train_inner | epoch 097:     56 / 64 loss=5.968, ppl=62.6, wps=6092.1, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=6200, lr=0.00040161, gnorm=0.661, train_wall=507, gb_free=6.1, wall=34169
2022-02-02 15:41:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:42:21 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 9.785 | ppl 882 | wps 8010.7 | wpb 2034.1 | bsz 4 | num_updates 6208 | best_loss 9.237
2022-02-02 15:42:21 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-02-02 15:42:21 | INFO | train | epoch 097 | loss 5.959 | ppl 62.21 | wps 5921 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6208 | lr 0.000401351 | gnorm 0.666 | train_wall 324 | gb_free 6.1 | wall 34236
KL Stats: Epoch 97 Divergences: Uniform: 2.989558448063444 Unigram: 3.2415527101627624
2022-02-02 15:42:21 | INFO | fairseq.trainer | begin training epoch 98
2022-02-02 15:42:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:47:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:48:14 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 9.795 | ppl 888.52 | wps 7969.4 | wpb 2034.1 | bsz 4 | num_updates 6272 | best_loss 9.237
2022-02-02 15:48:14 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-02-02 15:48:14 | INFO | train | epoch 098 | loss 5.943 | ppl 61.52 | wps 5918.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6272 | lr 0.000399298 | gnorm 0.688 | train_wall 324 | gb_free 6.1 | wall 34589
KL Stats: Epoch 98 Divergences: Uniform: 2.9911407132282077 Unigram: 3.2476118998267203
2022-02-02 15:48:14 | INFO | fairseq.trainer | begin training epoch 99
2022-02-02 15:48:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:50:36 | INFO | train_inner | epoch 099:     28 / 64 loss=5.933, ppl=61.11, wps=5794.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=6300, lr=0.00039841, gnorm=0.684, train_wall=505, gb_free=6.1, wall=34732
2022-02-02 15:53:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:54:06 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 9.778 | ppl 878.1 | wps 8000.9 | wpb 2034.1 | bsz 4 | num_updates 6336 | best_loss 9.237
2022-02-02 15:54:06 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-02-02 15:54:06 | INFO | train | epoch 099 | loss 5.923 | ppl 60.69 | wps 5928.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6336 | lr 0.000397276 | gnorm 0.669 | train_wall 323 | gb_free 6.1 | wall 34941
KL Stats: Epoch 99 Divergences: Uniform: 2.999038164000801 Unigram: 3.2659070112945447
2022-02-02 15:54:06 | INFO | fairseq.trainer | begin training epoch 100
2022-02-02 15:54:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:59:31 | INFO | train_inner | epoch 100:     64 / 64 loss=5.923, ppl=60.66, wps=6097.9, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=6400, lr=0.000395285, gnorm=0.677, train_wall=505, gb_free=6.1, wall=35266
2022-02-02 15:59:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:59:58 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 9.836 | ppl 913.97 | wps 7984.2 | wpb 2034.1 | bsz 4 | num_updates 6400 | best_loss 9.237
2022-02-02 15:59:58 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-02-02 15:59:58 | INFO | train | epoch 100 | loss 5.906 | ppl 59.97 | wps 5925.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6400 | lr 0.000395285 | gnorm 0.684 | train_wall 323 | gb_free 6.1 | wall 35294
KL Stats: Epoch 100 Divergences: Uniform: 3.000761668310661 Unigram: 3.2673068583090994
2022-02-02 15:59:58 | INFO | fairseq.trainer | begin training epoch 101
2022-02-02 15:59:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:05:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:05:51 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 9.804 | ppl 894.08 | wps 7964.3 | wpb 2034.1 | bsz 4 | num_updates 6464 | best_loss 9.237
2022-02-02 16:05:51 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-02-02 16:05:51 | INFO | train | epoch 101 | loss 5.891 | ppl 59.36 | wps 5920.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6464 | lr 0.000393323 | gnorm 0.688 | train_wall 324 | gb_free 6.1 | wall 35647
KL Stats: Epoch 101 Divergences: Uniform: 3.0089626648772096 Unigram: 3.283817078767752
2022-02-02 16:05:51 | INFO | fairseq.trainer | begin training epoch 102
2022-02-02 16:05:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:08:55 | INFO | train_inner | epoch 102:     36 / 64 loss=5.876, ppl=58.71, wps=5797.6, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=6500, lr=0.000392232, gnorm=0.683, train_wall=506, gb_free=6.1, wall=35830
2022-02-02 16:11:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:11:43 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 9.842 | ppl 917.69 | wps 7996.1 | wpb 2034.1 | bsz 4 | num_updates 6528 | best_loss 9.237
2022-02-02 16:11:43 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-02-02 16:11:43 | INFO | train | epoch 102 | loss 5.874 | ppl 58.63 | wps 5932.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6528 | lr 0.00039139 | gnorm 0.69 | train_wall 323 | gb_free 6.1 | wall 35999
KL Stats: Epoch 102 Divergences: Uniform: 3.0069424679583037 Unigram: 3.2882898748053266
2022-02-02 16:11:43 | INFO | fairseq.trainer | begin training epoch 103
2022-02-02 16:11:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:17:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:17:36 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 9.791 | ppl 885.79 | wps 7988.4 | wpb 2034.1 | bsz 4 | num_updates 6592 | best_loss 9.237
2022-02-02 16:17:36 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-02-02 16:17:36 | INFO | train | epoch 103 | loss 5.86 | ppl 58.07 | wps 5914.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6592 | lr 0.000389486 | gnorm 0.699 | train_wall 324 | gb_free 6.1 | wall 36352
KL Stats: Epoch 103 Divergences: Uniform: 3.012491727372672 Unigram: 3.3042884247598256
2022-02-02 16:17:36 | INFO | fairseq.trainer | begin training epoch 104
2022-02-02 16:17:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:18:17 | INFO | train_inner | epoch 104:      8 / 64 loss=5.867, ppl=58.35, wps=5796.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6600, lr=0.000389249, gnorm=0.7, train_wall=505, gb_free=6.1, wall=36392
2022-02-02 16:23:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 16:23:28 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 9.801 | ppl 892.3 | wps 8012 | wpb 2034.1 | bsz 4 | num_updates 6656 | best_loss 9.237
2022-02-02 16:23:28 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-02-02 16:23:28 | INFO | train | epoch 104 | loss 5.843 | ppl 57.4 | wps 5933.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6656 | lr 0.000387609 | gnorm 0.688 | train_wall 323 | gb_free 6.1 | wall 36704
KL Stats: Epoch 104 Divergences: Uniform: 3.0214072278061286 Unigram: 3.3150661974761735
2022-02-02 16:23:28 | INFO | fairseq.trainer | begin training epoch 105
2022-02-02 16:23:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:27:12 | INFO | train_inner | epoch 105:     44 / 64 loss=5.834, ppl=57.04, wps=6106.6, ups=0.19, wpb=32686.1, bsz=63.8, num_updates=6700, lr=0.000386334, gnorm=0.694, train_wall=505, gb_free=6.1, wall=36928
2022-02-02 16:28:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:29:20 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 9.823 | ppl 905.68 | wps 7964.2 | wpb 2034.1 | bsz 4 | num_updates 6720 | best_loss 9.237
2022-02-02 16:29:20 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-02-02 16:29:20 | INFO | train | epoch 105 | loss 5.828 | ppl 56.81 | wps 5932.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6720 | lr 0.000385758 | gnorm 0.713 | train_wall 323 | gb_free 6.1 | wall 37056
KL Stats: Epoch 105 Divergences: Uniform: 3.020457653590853 Unigram: 3.3264195160287993
2022-02-02 16:29:20 | INFO | fairseq.trainer | begin training epoch 106
2022-02-02 16:29:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:34:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:35:13 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 9.859 | ppl 928.84 | wps 7966.4 | wpb 2034.1 | bsz 4 | num_updates 6784 | best_loss 9.237
2022-02-02 16:35:13 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-02-02 16:35:13 | INFO | train | epoch 106 | loss 5.815 | ppl 56.29 | wps 5925.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6784 | lr 0.000383934 | gnorm 0.725 | train_wall 323 | gb_free 6.1 | wall 37408
KL Stats: Epoch 106 Divergences: Uniform: 3.024795130311887 Unigram: 3.3365511028596258
2022-02-02 16:35:13 | INFO | fairseq.trainer | begin training epoch 107
2022-02-02 16:35:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:36:35 | INFO | train_inner | epoch 107:     16 / 64 loss=5.814, ppl=56.25, wps=5797.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6800, lr=0.000383482, gnorm=0.728, train_wall=505, gb_free=6.1, wall=37490
2022-02-02 16:40:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:41:05 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 9.799 | ppl 890.78 | wps 7985.7 | wpb 2034.1 | bsz 4 | num_updates 6848 | best_loss 9.237
2022-02-02 16:41:05 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-02-02 16:41:05 | INFO | train | epoch 107 | loss 5.798 | ppl 55.64 | wps 5931.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6848 | lr 0.000382136 | gnorm 0.717 | train_wall 323 | gb_free 6.1 | wall 37760
KL Stats: Epoch 107 Divergences: Uniform: 3.037430341858574 Unigram: 3.344716469552641
2022-02-02 16:41:05 | INFO | fairseq.trainer | begin training epoch 108
2022-02-02 16:41:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:45:31 | INFO | train_inner | epoch 108:     52 / 64 loss=5.795, ppl=55.51, wps=6092, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=6900, lr=0.000380693, gnorm=0.716, train_wall=506, gb_free=6.1, wall=38026
2022-02-02 16:46:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:46:58 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 9.797 | ppl 889.61 | wps 7971.8 | wpb 2034.1 | bsz 4 | num_updates 6912 | best_loss 9.237
2022-02-02 16:46:58 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-02-02 16:46:58 | INFO | train | epoch 108 | loss 5.784 | ppl 55.12 | wps 5911.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6912 | lr 0.000380363 | gnorm 0.717 | train_wall 324 | gb_free 6.1 | wall 38114
KL Stats: Epoch 108 Divergences: Uniform: 3.039425806950528 Unigram: 3.3595764049677332
2022-02-02 16:46:58 | INFO | fairseq.trainer | begin training epoch 109
2022-02-02 16:46:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:52:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:52:51 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 9.879 | ppl 941.3 | wps 7977.2 | wpb 2034.1 | bsz 4 | num_updates 6976 | best_loss 9.237
2022-02-02 16:52:51 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-02-02 16:52:51 | INFO | train | epoch 109 | loss 5.77 | ppl 54.58 | wps 5921.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6976 | lr 0.000378614 | gnorm 0.727 | train_wall 324 | gb_free 6.1 | wall 38467
KL Stats: Epoch 109 Divergences: Uniform: 3.0371204003513097 Unigram: 3.3661580707991963
2022-02-02 16:52:51 | INFO | fairseq.trainer | begin training epoch 110
2022-02-02 16:52:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:54:53 | INFO | train_inner | epoch 110:     24 / 64 loss=5.76, ppl=54.19, wps=5795.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7000, lr=0.000377964, gnorm=0.716, train_wall=505, gb_free=6.1, wall=38589
2022-02-02 16:58:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:58:44 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 9.885 | ppl 945.42 | wps 7983.2 | wpb 2034.1 | bsz 4 | num_updates 7040 | best_loss 9.237
2022-02-02 16:58:44 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-02-02 16:58:44 | INFO | train | epoch 110 | loss 5.754 | ppl 53.98 | wps 5921.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7040 | lr 0.000376889 | gnorm 0.711 | train_wall 324 | gb_free 6.1 | wall 38819
KL Stats: Epoch 110 Divergences: Uniform: 3.0399672835866873 Unigram: 3.375976376463665
2022-02-02 16:58:44 | INFO | fairseq.trainer | begin training epoch 111
2022-02-02 16:58:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:03:49 | INFO | train_inner | epoch 111:     60 / 64 loss=5.757, ppl=54.07, wps=6097.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7100, lr=0.000375293, gnorm=0.73, train_wall=506, gb_free=6.1, wall=39125
2022-02-02 17:04:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:04:36 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 9.885 | ppl 945.36 | wps 7976.4 | wpb 2034.1 | bsz 4 | num_updates 7104 | best_loss 9.237
2022-02-02 17:04:36 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-02-02 17:04:36 | INFO | train | epoch 111 | loss 5.742 | ppl 53.51 | wps 5931.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7104 | lr 0.000375188 | gnorm 0.732 | train_wall 323 | gb_free 6.1 | wall 39171
KL Stats: Epoch 111 Divergences: Uniform: 3.051887909042216 Unigram: 3.3867244015679696
2022-02-02 17:04:36 | INFO | fairseq.trainer | begin training epoch 112
2022-02-02 17:04:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:10:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:10:28 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 9.867 | ppl 934.14 | wps 7984.4 | wpb 2034.1 | bsz 4 | num_updates 7168 | best_loss 9.237
2022-02-02 17:10:28 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-02-02 17:10:28 | INFO | train | epoch 112 | loss 5.729 | ppl 53.02 | wps 5940.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7168 | lr 0.000373509 | gnorm 0.737 | train_wall 323 | gb_free 6.1 | wall 39523
KL Stats: Epoch 112 Divergences: Uniform: 3.049944213481146 Unigram: 3.396051223787053
2022-02-02 17:10:28 | INFO | fairseq.trainer | begin training epoch 113
2022-02-02 17:10:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:13:11 | INFO | train_inner | epoch 113:     32 / 64 loss=5.715, ppl=52.51, wps=5808.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=7200, lr=0.000372678, gnorm=0.733, train_wall=504, gb_free=6.1, wall=39686
2022-02-02 17:15:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:16:20 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 9.873 | ppl 937.93 | wps 7989.4 | wpb 2034.1 | bsz 4 | num_updates 7232 | best_loss 9.237
2022-02-02 17:16:20 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-02-02 17:16:20 | INFO | train | epoch 113 | loss 5.716 | ppl 52.56 | wps 5919.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7232 | lr 0.000371853 | gnorm 0.737 | train_wall 324 | gb_free 6.1 | wall 39876
KL Stats: Epoch 113 Divergences: Uniform: 3.058913691436456 Unigram: 3.402556523925402
2022-02-02 17:16:20 | INFO | fairseq.trainer | begin training epoch 114
2022-02-02 17:16:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:21:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:22:13 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 9.85 | ppl 922.6 | wps 7932.6 | wpb 2034.1 | bsz 4 | num_updates 7296 | best_loss 9.237
2022-02-02 17:22:13 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-02-02 17:22:13 | INFO | train | epoch 114 | loss 5.704 | ppl 52.14 | wps 5924.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7296 | lr 0.000370218 | gnorm 0.774 | train_wall 323 | gb_free 6.1 | wall 40228
KL Stats: Epoch 114 Divergences: Uniform: 3.061341795136121 Unigram: 3.4144468232360987
2022-02-02 17:22:13 | INFO | fairseq.trainer | begin training epoch 115
2022-02-02 17:22:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:22:33 | INFO | train_inner | epoch 115:      4 / 64 loss=5.717, ppl=52.62, wps=5794, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7300, lr=0.000370117, gnorm=0.765, train_wall=505, gb_free=6.1, wall=40249
2022-02-02 17:27:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:28:06 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 9.881 | ppl 942.83 | wps 7986.4 | wpb 2034.1 | bsz 4 | num_updates 7360 | best_loss 9.237
2022-02-02 17:28:06 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-02-02 17:28:06 | INFO | train | epoch 115 | loss 5.689 | ppl 51.6 | wps 5916.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7360 | lr 0.000368605 | gnorm 0.746 | train_wall 324 | gb_free 6.1 | wall 40581
KL Stats: Epoch 115 Divergences: Uniform: 3.0554994546303575 Unigram: 3.420853060084452
2022-02-02 17:28:06 | INFO | fairseq.trainer | begin training epoch 116
2022-02-02 17:28:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:31:30 | INFO | train_inner | epoch 116:     40 / 64 loss=5.676, ppl=51.11, wps=6092.2, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=7400, lr=0.000367607, gnorm=0.749, train_wall=507, gb_free=6.1, wall=40785
2022-02-02 17:33:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 17:33:58 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 9.889 | ppl 948.08 | wps 7972.6 | wpb 2034.1 | bsz 4 | num_updates 7424 | best_loss 9.237
2022-02-02 17:33:58 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-02-02 17:33:58 | INFO | train | epoch 116 | loss 5.675 | ppl 51.09 | wps 5924.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7424 | lr 0.000367013 | gnorm 0.762 | train_wall 323 | gb_free 6.1 | wall 40934
KL Stats: Epoch 116 Divergences: Uniform: 3.0640692087580668 Unigram: 3.4253473768644844
2022-02-02 17:33:58 | INFO | fairseq.trainer | begin training epoch 117
2022-02-02 17:33:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:39:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:39:50 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 9.888 | ppl 947.68 | wps 7974.1 | wpb 2034.1 | bsz 4 | num_updates 7488 | best_loss 9.237
2022-02-02 17:39:50 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-02-02 17:39:50 | INFO | train | epoch 117 | loss 5.666 | ppl 50.76 | wps 5933.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7488 | lr 0.000365441 | gnorm 0.759 | train_wall 323 | gb_free 6.1 | wall 41286
KL Stats: Epoch 117 Divergences: Uniform: 3.0673272385366537 Unigram: 3.4383693154102635
2022-02-02 17:39:50 | INFO | fairseq.trainer | begin training epoch 118
2022-02-02 17:39:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:40:52 | INFO | train_inner | epoch 118:     12 / 64 loss=5.672, ppl=51, wps=5801.1, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=7500, lr=0.000365148, gnorm=0.762, train_wall=504, gb_free=6.1, wall=41347
2022-02-02 17:45:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 17:45:43 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 9.875 | ppl 939.12 | wps 7984.7 | wpb 2034.1 | bsz 4 | num_updates 7552 | best_loss 9.237
2022-02-02 17:45:43 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-02-02 17:45:43 | INFO | train | epoch 118 | loss 5.651 | ppl 50.25 | wps 5928.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7552 | lr 0.000363889 | gnorm 0.775 | train_wall 323 | gb_free 6.1 | wall 41638
KL Stats: Epoch 118 Divergences: Uniform: 3.070541978252765 Unigram: 3.4481181644409222
2022-02-02 17:45:43 | INFO | fairseq.trainer | begin training epoch 119
2022-02-02 17:45:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:49:47 | INFO | train_inner | epoch 119:     48 / 64 loss=5.644, ppl=50.02, wps=6099.6, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=7600, lr=0.000362738, gnorm=0.781, train_wall=506, gb_free=6.1, wall=41883
2022-02-02 17:51:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:51:35 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 9.957 | ppl 993.88 | wps 7964.5 | wpb 2034.1 | bsz 4 | num_updates 7616 | best_loss 9.237
2022-02-02 17:51:35 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-02-02 17:51:35 | INFO | train | epoch 119 | loss 5.642 | ppl 49.95 | wps 5925.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7616 | lr 0.000362357 | gnorm 0.771 | train_wall 323 | gb_free 6.1 | wall 41991
KL Stats: Epoch 119 Divergences: Uniform: 3.0730389646038287 Unigram: 3.4492956274517685
2022-02-02 17:51:35 | INFO | fairseq.trainer | begin training epoch 120
2022-02-02 17:51:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:57:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:57:28 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 9.917 | ppl 966.45 | wps 7972.9 | wpb 2034.1 | bsz 4 | num_updates 7680 | best_loss 9.237
2022-02-02 17:57:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 7680 updates
2022-02-02 17:57:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint120.pt
2022-02-02 17:57:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint120.pt
2022-02-02 17:57:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint120.pt (epoch 120 @ 7680 updates, score 9.917) (writing took 2.950313001871109 seconds)
2022-02-02 17:57:31 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-02-02 17:57:31 | INFO | train | epoch 120 | loss 5.63 | ppl 49.51 | wps 5871 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7680 | lr 0.000360844 | gnorm 0.787 | train_wall 324 | gb_free 6.1 | wall 42346
KL Stats: Epoch 120 Divergences: Uniform: 3.0752862201390982 Unigram: 3.4644730428669437
2022-02-02 17:57:31 | INFO | fairseq.trainer | begin training epoch 121
2022-02-02 17:57:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:59:13 | INFO | train_inner | epoch 121:     20 / 64 loss=5.626, ppl=49.4, wps=5765.3, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=7700, lr=0.000360375, gnorm=0.78, train_wall=505, gb_free=6.1, wall=42448
2022-02-02 18:02:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 18:03:23 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 9.936 | ppl 979.67 | wps 7997.9 | wpb 2034.1 | bsz 4 | num_updates 7744 | best_loss 9.237
2022-02-02 18:03:23 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-02-02 18:03:23 | INFO | train | epoch 121 | loss 5.618 | ppl 49.1 | wps 5927 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7744 | lr 0.00035935 | gnorm 0.794 | train_wall 323 | gb_free 6.1 | wall 42699
KL Stats: Epoch 121 Divergences: Uniform: 3.0806126609358637 Unigram: 3.465638729820133
2022-02-02 18:03:23 | INFO | fairseq.trainer | begin training epoch 122
2022-02-02 18:03:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:08:09 | INFO | train_inner | epoch 122:     56 / 64 loss=5.617, ppl=49.08, wps=6097.4, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=7800, lr=0.000358057, gnorm=0.798, train_wall=506, gb_free=6.1, wall=42984
2022-02-02 18:08:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:09:16 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 9.966 | ppl 1000.27 | wps 7989.8 | wpb 2034.1 | bsz 4 | num_updates 7808 | best_loss 9.237
2022-02-02 18:09:16 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-02-02 18:09:16 | INFO | train | epoch 122 | loss 5.608 | ppl 48.76 | wps 5925.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7808 | lr 0.000357874 | gnorm 0.803 | train_wall 323 | gb_free 6.1 | wall 43051
KL Stats: Epoch 122 Divergences: Uniform: 3.0829805112975044 Unigram: 3.487716973859683
2022-02-02 18:09:16 | INFO | fairseq.trainer | begin training epoch 123
2022-02-02 18:09:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:14:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:15:08 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 9.976 | ppl 1007.12 | wps 7958.4 | wpb 2034.1 | bsz 4 | num_updates 7872 | best_loss 9.237
2022-02-02 18:15:08 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-02-02 18:15:08 | INFO | train | epoch 123 | loss 5.597 | ppl 48.4 | wps 5931.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7872 | lr 0.000356416 | gnorm 0.801 | train_wall 323 | gb_free 6.1 | wall 43403
KL Stats: Epoch 123 Divergences: Uniform: 3.081397046420609 Unigram: 3.487603210418742
2022-02-02 18:15:08 | INFO | fairseq.trainer | begin training epoch 124
2022-02-02 18:15:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:17:31 | INFO | train_inner | epoch 124:     28 / 64 loss=5.591, ppl=48.22, wps=5802.5, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7900, lr=0.000355784, gnorm=0.801, train_wall=504, gb_free=6.1, wall=43546
2022-02-02 18:20:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:21:01 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 9.914 | ppl 964.93 | wps 8005.2 | wpb 2034.1 | bsz 4 | num_updates 7936 | best_loss 9.237
2022-02-02 18:21:01 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-02-02 18:21:01 | INFO | train | epoch 124 | loss 5.587 | ppl 48.06 | wps 5921.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7936 | lr 0.000354976 | gnorm 0.806 | train_wall 324 | gb_free 6.1 | wall 43756
KL Stats: Epoch 124 Divergences: Uniform: 3.091667494771748 Unigram: 3.493588653501806
2022-02-02 18:21:01 | INFO | fairseq.trainer | begin training epoch 125
2022-02-02 18:21:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:26:26 | INFO | train_inner | epoch 125:     64 / 64 loss=5.588, ppl=48.11, wps=6092.9, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=8000, lr=0.000353553, gnorm=0.814, train_wall=505, gb_free=6.1, wall=44081
2022-02-02 18:26:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:26:53 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 9.982 | ppl 1011.33 | wps 7971.9 | wpb 2034.1 | bsz 4 | num_updates 8000 | best_loss 9.237
2022-02-02 18:26:53 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-02-02 18:26:53 | INFO | train | epoch 125 | loss 5.575 | ppl 47.68 | wps 5925.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8000 | lr 0.000353553 | gnorm 0.813 | train_wall 323 | gb_free 6.1 | wall 44109
KL Stats: Epoch 125 Divergences: Uniform: 3.087874129355033 Unigram: 3.506091103996111
2022-02-02 18:26:53 | INFO | fairseq.trainer | begin training epoch 126
2022-02-02 18:26:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:32:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:32:45 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 9.904 | ppl 958.03 | wps 7999.4 | wpb 2034.1 | bsz 4 | num_updates 8064 | best_loss 9.237
2022-02-02 18:32:45 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-02-02 18:32:45 | INFO | train | epoch 126 | loss 5.563 | ppl 47.28 | wps 5929.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8064 | lr 0.000352148 | gnorm 0.81 | train_wall 323 | gb_free 6.1 | wall 44461
KL Stats: Epoch 126 Divergences: Uniform: 3.0901511993823028 Unigram: 3.5093424880023116
2022-02-02 18:32:45 | INFO | fairseq.trainer | begin training epoch 127
2022-02-02 18:32:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:35:49 | INFO | train_inner | epoch 127:     36 / 64 loss=5.551, ppl=46.9, wps=5807.3, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=8100, lr=0.000351364, gnorm=0.811, train_wall=505, gb_free=6.1, wall=44644
2022-02-02 18:38:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:38:37 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 10.015 | ppl 1034.8 | wps 7985.7 | wpb 2034.1 | bsz 4 | num_updates 8128 | best_loss 9.237
2022-02-02 18:38:37 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-02-02 18:38:37 | INFO | train | epoch 127 | loss 5.556 | ppl 47.04 | wps 5937.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8128 | lr 0.000350758 | gnorm 0.835 | train_wall 323 | gb_free 6.1 | wall 44813
KL Stats: Epoch 127 Divergences: Uniform: 3.0879275452757327 Unigram: 3.5162950306006446
2022-02-02 18:38:37 | INFO | fairseq.trainer | begin training epoch 128
2022-02-02 18:38:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:44:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:44:29 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 10.041 | ppl 1053.49 | wps 8012.4 | wpb 2034.1 | bsz 4 | num_updates 8192 | best_loss 9.237
2022-02-02 18:44:29 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-02-02 18:44:29 | INFO | train | epoch 128 | loss 5.543 | ppl 46.62 | wps 5932.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8192 | lr 0.000349386 | gnorm 0.799 | train_wall 323 | gb_free 6.1 | wall 45165
KL Stats: Epoch 128 Divergences: Uniform: 3.098435830286858 Unigram: 3.5271524778747896
2022-02-02 18:44:29 | INFO | fairseq.trainer | begin training epoch 129
2022-02-02 18:44:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:45:10 | INFO | train_inner | epoch 129:      8 / 64 loss=5.55, ppl=46.86, wps=5808.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8200, lr=0.000349215, gnorm=0.822, train_wall=504, gb_free=6.1, wall=45205
2022-02-02 18:49:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:50:20 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 10.002 | ppl 1025.2 | wps 8042.5 | wpb 2034.1 | bsz 4 | num_updates 8256 | best_loss 9.237
2022-02-02 18:50:20 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-02-02 18:50:20 | INFO | train | epoch 129 | loss 5.535 | ppl 46.36 | wps 5961.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8256 | lr 0.000348029 | gnorm 0.847 | train_wall 322 | gb_free 6.1 | wall 45515
KL Stats: Epoch 129 Divergences: Uniform: 3.0979714725899052 Unigram: 3.5290523690749844
2022-02-02 18:50:20 | INFO | fairseq.trainer | begin training epoch 130
2022-02-02 18:50:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:54:02 | INFO | train_inner | epoch 130:     44 / 64 loss=5.527, ppl=46.1, wps=6139.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=8300, lr=0.000347105, gnorm=0.846, train_wall=503, gb_free=6.1, wall=45737
2022-02-02 18:55:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:56:09 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 10.014 | ppl 1033.67 | wps 8072.9 | wpb 2034.1 | bsz 4 | num_updates 8320 | best_loss 9.237
2022-02-02 18:56:09 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-02-02 18:56:09 | INFO | train | epoch 130 | loss 5.526 | ppl 46.09 | wps 5975.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8320 | lr 0.000346688 | gnorm 0.853 | train_wall 321 | gb_free 6.1 | wall 45865
KL Stats: Epoch 130 Divergences: Uniform: 3.096222231348938 Unigram: 3.535266052496682
2022-02-02 18:56:09 | INFO | fairseq.trainer | begin training epoch 131
2022-02-02 18:56:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:01:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:02:00 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 9.953 | ppl 991.11 | wps 8000.4 | wpb 2034.1 | bsz 4 | num_updates 8384 | best_loss 9.237
2022-02-02 19:02:00 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-02-02 19:02:00 | INFO | train | epoch 131 | loss 5.514 | ppl 45.68 | wps 5953.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8384 | lr 0.000345362 | gnorm 0.834 | train_wall 322 | gb_free 6.1 | wall 46215
KL Stats: Epoch 131 Divergences: Uniform: 3.105708507765572 Unigram: 3.5471177317013596
2022-02-02 19:02:00 | INFO | fairseq.trainer | begin training epoch 132
2022-02-02 19:02:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:03:21 | INFO | train_inner | epoch 132:     16 / 64 loss=5.517, ppl=45.79, wps=5831.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=8400, lr=0.000345033, gnorm=0.842, train_wall=502, gb_free=6.1, wall=46297
2022-02-02 19:07:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:07:50 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 9.97 | ppl 1003.24 | wps 8031.1 | wpb 2034.1 | bsz 4 | num_updates 8448 | best_loss 9.237
2022-02-02 19:07:50 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-02-02 19:07:50 | INFO | train | epoch 132 | loss 5.506 | ppl 45.44 | wps 5971.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8448 | lr 0.000344051 | gnorm 0.85 | train_wall 321 | gb_free 6.1 | wall 46565
KL Stats: Epoch 132 Divergences: Uniform: 3.1049426058055136 Unigram: 3.5510112820230373
2022-02-02 19:07:50 | INFO | fairseq.trainer | begin training epoch 133
2022-02-02 19:07:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:12:14 | INFO | train_inner | epoch 133:     52 / 64 loss=5.502, ppl=45.32, wps=6133.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=8500, lr=0.000342997, gnorm=0.851, train_wall=503, gb_free=6.1, wall=46829
2022-02-02 19:13:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:13:41 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 10 | ppl 1024.18 | wps 8009.5 | wpb 2034.1 | bsz 4 | num_updates 8512 | best_loss 9.237
2022-02-02 19:13:41 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-02-02 19:13:41 | INFO | train | epoch 133 | loss 5.496 | ppl 45.14 | wps 5950.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8512 | lr 0.000342755 | gnorm 0.859 | train_wall 322 | gb_free 6.1 | wall 46916
KL Stats: Epoch 133 Divergences: Uniform: 3.1078958780142236 Unigram: 3.56362497335638
2022-02-02 19:13:41 | INFO | fairseq.trainer | begin training epoch 134
2022-02-02 19:13:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:19:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:19:32 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 9.995 | ppl 1020.58 | wps 7998.8 | wpb 2034.1 | bsz 4 | num_updates 8576 | best_loss 9.237
2022-02-02 19:19:32 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-02-02 19:19:32 | INFO | train | epoch 134 | loss 5.488 | ppl 44.87 | wps 5940.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8576 | lr 0.000341474 | gnorm 0.844 | train_wall 323 | gb_free 6.1 | wall 47268
KL Stats: Epoch 134 Divergences: Uniform: 3.1038766389839183 Unigram: 3.5596196009114855
2022-02-02 19:19:32 | INFO | fairseq.trainer | begin training epoch 135
2022-02-02 19:19:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:21:34 | INFO | train_inner | epoch 135:     24 / 64 loss=5.482, ppl=44.68, wps=5818.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=8600, lr=0.000340997, gnorm=0.844, train_wall=503, gb_free=6.1, wall=47390
2022-02-02 19:24:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:25:23 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 10.003 | ppl 1025.87 | wps 8026.2 | wpb 2034.1 | bsz 4 | num_updates 8640 | best_loss 9.237
2022-02-02 19:25:23 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-02-02 19:25:23 | INFO | train | epoch 135 | loss 5.477 | ppl 44.55 | wps 5958.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8640 | lr 0.000340207 | gnorm 0.855 | train_wall 322 | gb_free 6.1 | wall 47618
KL Stats: Epoch 135 Divergences: Uniform: 3.115541343312236 Unigram: 3.574267806024801
2022-02-02 19:25:23 | INFO | fairseq.trainer | begin training epoch 136
2022-02-02 19:25:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:30:26 | INFO | train_inner | epoch 136:     60 / 64 loss=5.481, ppl=44.66, wps=6141.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=8700, lr=0.000339032, gnorm=0.878, train_wall=502, gb_free=6.1, wall=47922
2022-02-02 19:30:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 19:31:12 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 9.992 | ppl 1018.26 | wps 8051.6 | wpb 2034.1 | bsz 4 | num_updates 8704 | best_loss 9.237
2022-02-02 19:31:12 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-02-02 19:31:12 | INFO | train | epoch 136 | loss 5.47 | ppl 44.32 | wps 5974.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8704 | lr 0.000338954 | gnorm 0.88 | train_wall 321 | gb_free 6.1 | wall 47968
KL Stats: Epoch 136 Divergences: Uniform: 3.1150723595752536 Unigram: 3.5823822823620564
2022-02-02 19:31:12 | INFO | fairseq.trainer | begin training epoch 137
2022-02-02 19:31:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:36:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:37:02 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 9.992 | ppl 1018.69 | wps 8074.5 | wpb 2034.1 | bsz 4 | num_updates 8768 | best_loss 9.237
2022-02-02 19:37:02 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-02-02 19:37:02 | INFO | train | epoch 137 | loss 5.459 | ppl 43.99 | wps 5974.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8768 | lr 0.000337715 | gnorm 0.88 | train_wall 321 | gb_free 6.1 | wall 48317
KL Stats: Epoch 137 Divergences: Uniform: 3.1088802656807797 Unigram: 3.58897379690205
2022-02-02 19:37:02 | INFO | fairseq.trainer | begin training epoch 138
2022-02-02 19:37:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:39:44 | INFO | train_inner | epoch 138:     32 / 64 loss=5.447, ppl=43.63, wps=5844.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=8800, lr=0.0003371, gnorm=0.872, train_wall=501, gb_free=6.1, wall=48480
2022-02-02 19:42:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:42:52 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 10.049 | ppl 1059.45 | wps 8026.6 | wpb 2034.1 | bsz 4 | num_updates 8832 | best_loss 9.237
2022-02-02 19:42:52 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-02-02 19:42:52 | INFO | train | epoch 138 | loss 5.45 | ppl 43.72 | wps 5962.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8832 | lr 0.000336489 | gnorm 0.867 | train_wall 321 | gb_free 6.1 | wall 48668
KL Stats: Epoch 138 Divergences: Uniform: 3.1119207159127864 Unigram: 3.590885040445747
2022-02-02 19:42:52 | INFO | fairseq.trainer | begin training epoch 139
2022-02-02 19:42:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:48:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:48:43 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 10.048 | ppl 1058.32 | wps 8046.7 | wpb 2034.1 | bsz 4 | num_updates 8896 | best_loss 9.237
2022-02-02 19:48:43 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-02-02 19:48:43 | INFO | train | epoch 139 | loss 5.442 | ppl 43.46 | wps 5958.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8896 | lr 0.000335276 | gnorm 0.883 | train_wall 322 | gb_free 6.1 | wall 49018
KL Stats: Epoch 139 Divergences: Uniform: 3.114665231755507 Unigram: 3.5986177604885246
2022-02-02 19:48:43 | INFO | fairseq.trainer | begin training epoch 140
2022-02-02 19:48:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:49:03 | INFO | train_inner | epoch 140:      4 / 64 loss=5.453, ppl=43.79, wps=5831.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8900, lr=0.000335201, gnorm=0.881, train_wall=502, gb_free=6.1, wall=49039
2022-02-02 19:54:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:54:33 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 10.007 | ppl 1029.16 | wps 8036 | wpb 2034.1 | bsz 4 | num_updates 8960 | best_loss 9.237
2022-02-02 19:54:33 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-02-02 19:54:33 | INFO | train | epoch 140 | loss 5.434 | ppl 43.24 | wps 5968.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8960 | lr 0.000334077 | gnorm 0.893 | train_wall 321 | gb_free 6.1 | wall 49368
KL Stats: Epoch 140 Divergences: Uniform: 3.121550476397613 Unigram: 3.6038361281434326
2022-02-02 19:54:33 | INFO | fairseq.trainer | begin training epoch 141
2022-02-02 19:54:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:57:55 | INFO | train_inner | epoch 141:     40 / 64 loss=5.425, ppl=42.95, wps=6138.5, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9000, lr=0.000333333, gnorm=0.903, train_wall=503, gb_free=6.1, wall=49571
2022-02-02 19:59:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:00:23 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 10.069 | ppl 1073.95 | wps 8037.9 | wpb 2034.1 | bsz 4 | num_updates 9024 | best_loss 9.237
2022-02-02 20:00:23 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-02-02 20:00:23 | INFO | train | epoch 141 | loss 5.426 | ppl 42.99 | wps 5961.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9024 | lr 0.00033289 | gnorm 0.897 | train_wall 322 | gb_free 6.1 | wall 49718
KL Stats: Epoch 141 Divergences: Uniform: 3.127178707706944 Unigram: 3.6161002935010806
2022-02-02 20:00:23 | INFO | fairseq.trainer | begin training epoch 142
2022-02-02 20:00:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:05:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:06:13 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 10.045 | ppl 1056.48 | wps 8079.9 | wpb 2034.1 | bsz 4 | num_updates 9088 | best_loss 9.237
2022-02-02 20:06:13 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-02-02 20:06:13 | INFO | train | epoch 142 | loss 5.419 | ppl 42.78 | wps 5971.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9088 | lr 0.000331716 | gnorm 0.932 | train_wall 321 | gb_free 6.1 | wall 50068
KL Stats: Epoch 142 Divergences: Uniform: 3.126035606284381 Unigram: 3.6187513326466267
2022-02-02 20:06:13 | INFO | fairseq.trainer | begin training epoch 143
2022-02-02 20:06:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:07:14 | INFO | train_inner | epoch 143:     12 / 64 loss=5.42, ppl=42.82, wps=5841.5, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=9100, lr=0.000331497, gnorm=0.916, train_wall=501, gb_free=6.1, wall=50129
2022-02-02 20:11:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:12:03 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 9.99 | ppl 1017.09 | wps 8020 | wpb 2034.1 | bsz 4 | num_updates 9152 | best_loss 9.237
2022-02-02 20:12:03 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-02-02 20:12:03 | INFO | train | epoch 143 | loss 5.41 | ppl 42.5 | wps 5965.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9152 | lr 0.000330554 | gnorm 0.905 | train_wall 321 | gb_free 6.1 | wall 50418
KL Stats: Epoch 143 Divergences: Uniform: 3.1324268061480836 Unigram: 3.62549983222274
2022-02-02 20:12:03 | INFO | fairseq.trainer | begin training epoch 144
2022-02-02 20:12:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:16:07 | INFO | train_inner | epoch 144:     48 / 64 loss=5.407, ppl=42.43, wps=6131.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9200, lr=0.00032969, gnorm=0.897, train_wall=503, gb_free=6.1, wall=50662
2022-02-02 20:17:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:17:54 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 10.027 | ppl 1043.05 | wps 8060.3 | wpb 2034.1 | bsz 4 | num_updates 9216 | best_loss 9.237
2022-02-02 20:17:54 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-02-02 20:17:54 | INFO | train | epoch 144 | loss 5.4 | ppl 42.23 | wps 5957.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9216 | lr 0.000329404 | gnorm 0.894 | train_wall 322 | gb_free 6.1 | wall 50769
KL Stats: Epoch 144 Divergences: Uniform: 3.1305237929661223 Unigram: 3.631652051059606
2022-02-02 20:17:54 | INFO | fairseq.trainer | begin training epoch 145
2022-02-02 20:17:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:23:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:23:43 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 10.072 | ppl 1076.21 | wps 8053.9 | wpb 2034.1 | bsz 4 | num_updates 9280 | best_loss 9.237
2022-02-02 20:23:43 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-02-02 20:23:43 | INFO | train | epoch 145 | loss 5.393 | ppl 42.03 | wps 5971.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9280 | lr 0.000328266 | gnorm 0.919 | train_wall 321 | gb_free 6.1 | wall 51119
KL Stats: Epoch 145 Divergences: Uniform: 3.130995216869112 Unigram: 3.6372187661426203
2022-02-02 20:23:43 | INFO | fairseq.trainer | begin training epoch 146
2022-02-02 20:23:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:25:24 | INFO | train_inner | epoch 146:     20 / 64 loss=5.39, ppl=41.93, wps=5844.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=9300, lr=0.000327913, gnorm=0.912, train_wall=501, gb_free=6.1, wall=51220
2022-02-02 20:29:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:29:33 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 10.092 | ppl 1091.78 | wps 8065 | wpb 2034.1 | bsz 4 | num_updates 9344 | best_loss 9.237
2022-02-02 20:29:33 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-02-02 20:29:33 | INFO | train | epoch 146 | loss 5.387 | ppl 41.84 | wps 5977.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9344 | lr 0.00032714 | gnorm 0.914 | train_wall 321 | gb_free 6.1 | wall 51468
KL Stats: Epoch 146 Divergences: Uniform: 3.1355286867841845 Unigram: 3.645965582944529
2022-02-02 20:29:33 | INFO | fairseq.trainer | begin training epoch 147
2022-02-02 20:29:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:34:16 | INFO | train_inner | epoch 147:     56 / 64 loss=5.389, ppl=41.9, wps=6141.7, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9400, lr=0.000326164, gnorm=0.944, train_wall=502, gb_free=6.1, wall=51752
2022-02-02 20:34:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:35:23 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 10.042 | ppl 1053.92 | wps 8055 | wpb 2034.1 | bsz 4 | num_updates 9408 | best_loss 9.237
2022-02-02 20:35:23 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-02-02 20:35:23 | INFO | train | epoch 147 | loss 5.379 | ppl 41.63 | wps 5965 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9408 | lr 0.000326025 | gnorm 0.959 | train_wall 321 | gb_free 6.1 | wall 51818
KL Stats: Epoch 147 Divergences: Uniform: 3.1275842245874856 Unigram: 3.647282490135562
2022-02-02 20:35:23 | INFO | fairseq.trainer | begin training epoch 148
2022-02-02 20:35:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:40:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:41:13 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 10.118 | ppl 1111.23 | wps 8053.2 | wpb 2034.1 | bsz 4 | num_updates 9472 | best_loss 9.237
2022-02-02 20:41:13 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-02-02 20:41:13 | INFO | train | epoch 148 | loss 5.37 | ppl 41.36 | wps 5958.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9472 | lr 0.000324922 | gnorm 0.961 | train_wall 322 | gb_free 6.1 | wall 52169
KL Stats: Epoch 148 Divergences: Uniform: 3.1320894762404063 Unigram: 3.6529394546403813
2022-02-02 20:41:13 | INFO | fairseq.trainer | begin training epoch 149
2022-02-02 20:41:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:43:36 | INFO | train_inner | epoch 149:     28 / 64 loss=5.361, ppl=41.09, wps=5829.9, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=9500, lr=0.000324443, gnorm=0.961, train_wall=502, gb_free=6.1, wall=52311
2022-02-02 20:46:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:47:04 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 10.122 | ppl 1114.42 | wps 8045 | wpb 2034.1 | bsz 4 | num_updates 9536 | best_loss 9.237
2022-02-02 20:47:04 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-02-02 20:47:04 | INFO | train | epoch 149 | loss 5.365 | ppl 41.2 | wps 5954.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9536 | lr 0.00032383 | gnorm 0.951 | train_wall 322 | gb_free 6.1 | wall 52520
KL Stats: Epoch 149 Divergences: Uniform: 3.136526761984116 Unigram: 3.661348402791165
2022-02-02 20:47:04 | INFO | fairseq.trainer | begin training epoch 150
2022-02-02 20:47:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:52:27 | INFO | train_inner | epoch 150:     64 / 64 loss=5.37, ppl=41.35, wps=6137.3, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=9600, lr=0.000322749, gnorm=0.949, train_wall=501, gb_free=6.1, wall=52842
2022-02-02 20:52:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:52:54 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 10.066 | ppl 1071.62 | wps 8043.8 | wpb 2034.1 | bsz 4 | num_updates 9600 | best_loss 9.237
2022-02-02 20:52:54 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-02-02 20:52:54 | INFO | train | epoch 150 | loss 5.354 | ppl 40.89 | wps 5970.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9600 | lr 0.000322749 | gnorm 0.953 | train_wall 321 | gb_free 6.1 | wall 52869
KL Stats: Epoch 150 Divergences: Uniform: 3.1329427619431214 Unigram: 3.6671712073653313
2022-02-02 20:52:54 | INFO | fairseq.trainer | begin training epoch 151
2022-02-02 20:52:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:58:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 20:58:44 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 10.101 | ppl 1098.48 | wps 8049.2 | wpb 2034.1 | bsz 4 | num_updates 9664 | best_loss 9.237
2022-02-02 20:58:44 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-02-02 20:58:44 | INFO | train | epoch 151 | loss 5.349 | ppl 40.77 | wps 5963.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9664 | lr 0.000321678 | gnorm 0.95 | train_wall 321 | gb_free 6.1 | wall 53220
KL Stats: Epoch 151 Divergences: Uniform: 3.131496024459825 Unigram: 3.6719510264254867
2022-02-02 20:58:44 | INFO | fairseq.trainer | begin training epoch 152
2022-02-02 20:58:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:01:47 | INFO | train_inner | epoch 152:     36 / 64 loss=5.337, ppl=40.42, wps=5838.4, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=9700, lr=0.000321081, gnorm=0.958, train_wall=503, gb_free=6.1, wall=53402
2022-02-02 21:04:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:04:34 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 10.085 | ppl 1085.82 | wps 8040.5 | wpb 2034.1 | bsz 4 | num_updates 9728 | best_loss 9.237
2022-02-02 21:04:34 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-02-02 21:04:34 | INFO | train | epoch 152 | loss 5.343 | ppl 40.6 | wps 5965.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9728 | lr 0.000320618 | gnorm 0.972 | train_wall 321 | gb_free 6.1 | wall 53570
KL Stats: Epoch 152 Divergences: Uniform: 3.1367584197615392 Unigram: 3.683749289074528
2022-02-02 21:04:34 | INFO | fairseq.trainer | begin training epoch 153
2022-02-02 21:04:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:09:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 21:10:24 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 10.104 | ppl 1100.72 | wps 8061.9 | wpb 2034.1 | bsz 4 | num_updates 9792 | best_loss 9.237
2022-02-02 21:10:24 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-02-02 21:10:24 | INFO | train | epoch 153 | loss 5.336 | ppl 40.39 | wps 5966.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9792 | lr 0.000319569 | gnorm 0.98 | train_wall 321 | gb_free 6.1 | wall 53920
KL Stats: Epoch 153 Divergences: Uniform: 3.140009067347459 Unigram: 3.6816438483599288
2022-02-02 21:10:24 | INFO | fairseq.trainer | begin training epoch 154
2022-02-02 21:10:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:11:05 | INFO | train_inner | epoch 154:      8 / 64 loss=5.344, ppl=40.63, wps=5838.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=9800, lr=0.000319438, gnorm=0.974, train_wall=502, gb_free=6.1, wall=53960
2022-02-02 21:15:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:16:15 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 10.157 | ppl 1141.73 | wps 8069.8 | wpb 2034.1 | bsz 4 | num_updates 9856 | best_loss 9.237
2022-02-02 21:16:15 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-02-02 21:16:15 | INFO | train | epoch 154 | loss 5.326 | ppl 40.13 | wps 5965.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9856 | lr 0.000318529 | gnorm 0.956 | train_wall 321 | gb_free 6.1 | wall 54270
KL Stats: Epoch 154 Divergences: Uniform: 3.135631267938728 Unigram: 3.6867488267630195
2022-02-02 21:16:15 | INFO | fairseq.trainer | begin training epoch 155
2022-02-02 21:16:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:19:58 | INFO | train_inner | epoch 155:     44 / 64 loss=5.317, ppl=39.87, wps=6135.6, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=9900, lr=0.000317821, gnorm=0.977, train_wall=503, gb_free=6.1, wall=54493
2022-02-02 21:21:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:22:05 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 10.108 | ppl 1103.9 | wps 8055.4 | wpb 2034.1 | bsz 4 | num_updates 9920 | best_loss 9.237
2022-02-02 21:22:05 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-02-02 21:22:05 | INFO | train | epoch 155 | loss 5.322 | ppl 40.01 | wps 5965.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9920 | lr 0.0003175 | gnorm 1.009 | train_wall 321 | gb_free 6.1 | wall 54620
KL Stats: Epoch 155 Divergences: Uniform: 3.141477680925893 Unigram: 3.689713730369734
2022-02-02 21:22:05 | INFO | fairseq.trainer | begin training epoch 156
2022-02-02 21:22:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:27:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:27:55 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 10.137 | ppl 1125.82 | wps 8027.6 | wpb 2034.1 | bsz 4 | num_updates 9984 | best_loss 9.237
2022-02-02 21:27:55 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-02-02 21:27:55 | INFO | train | epoch 156 | loss 5.316 | ppl 39.82 | wps 5965.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9984 | lr 0.000316481 | gnorm 1.002 | train_wall 321 | gb_free 6.1 | wall 54970
KL Stats: Epoch 156 Divergences: Uniform: 3.139984128531275 Unigram: 3.695359342848519
2022-02-02 21:27:55 | INFO | fairseq.trainer | begin training epoch 157
2022-02-02 21:27:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:29:16 | INFO | train_inner | epoch 157:     16 / 64 loss=5.319, ppl=39.92, wps=5839.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10000, lr=0.000316228, gnorm=1.007, train_wall=501, gb_free=6.1, wall=55051
2022-02-02 21:33:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:33:45 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 10.131 | ppl 1121.35 | wps 8017.8 | wpb 2034.1 | bsz 4 | num_updates 10048 | best_loss 9.237
2022-02-02 21:33:45 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-02-02 21:33:45 | INFO | train | epoch 157 | loss 5.308 | ppl 39.62 | wps 5965.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10048 | lr 0.000315472 | gnorm 0.999 | train_wall 321 | gb_free 6.1 | wall 55320
KL Stats: Epoch 157 Divergences: Uniform: 3.1508286478675 Unigram: 3.702916616659084
2022-02-02 21:33:45 | INFO | fairseq.trainer | begin training epoch 158
2022-02-02 21:33:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:38:08 | INFO | train_inner | epoch 158:     52 / 64 loss=5.304, ppl=39.51, wps=6135.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=10100, lr=0.000314658, gnorm=1.009, train_wall=503, gb_free=6.1, wall=55584
2022-02-02 21:39:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:39:35 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 10.123 | ppl 1115.13 | wps 8041.8 | wpb 2034.1 | bsz 4 | num_updates 10112 | best_loss 9.237
2022-02-02 21:39:35 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-02-02 21:39:35 | INFO | train | epoch 158 | loss 5.301 | ppl 39.42 | wps 5961.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10112 | lr 0.000314472 | gnorm 1.013 | train_wall 322 | gb_free 6.1 | wall 55671
KL Stats: Epoch 158 Divergences: Uniform: 3.144250881814766 Unigram: 3.710353929627169
2022-02-02 21:39:35 | INFO | fairseq.trainer | begin training epoch 159
2022-02-02 21:39:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:44:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:45:25 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 10.142 | ppl 1129.56 | wps 8052.4 | wpb 2034.1 | bsz 4 | num_updates 10176 | best_loss 9.237
2022-02-02 21:45:25 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-02-02 21:45:25 | INFO | train | epoch 159 | loss 5.294 | ppl 39.23 | wps 5967.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10176 | lr 0.000313481 | gnorm 1.022 | train_wall 321 | gb_free 6.1 | wall 56021
KL Stats: Epoch 159 Divergences: Uniform: 3.1519891137395284 Unigram: 3.7117005040049817
2022-02-02 21:45:25 | INFO | fairseq.trainer | begin training epoch 160
2022-02-02 21:45:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:47:27 | INFO | train_inner | epoch 160:     24 / 64 loss=5.29, ppl=39.13, wps=5836.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=10200, lr=0.000313112, gnorm=1.029, train_wall=502, gb_free=6.1, wall=56142
2022-02-02 21:50:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:51:16 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 10.125 | ppl 1116.43 | wps 8008.5 | wpb 2034.1 | bsz 4 | num_updates 10240 | best_loss 9.237
2022-02-02 21:51:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 10240 updates
2022-02-02 21:51:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint160.pt
2022-02-02 21:51:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint160.pt
2022-02-02 21:51:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint160.pt (epoch 160 @ 10240 updates, score 10.125) (writing took 3.0395522294566035 seconds)
2022-02-02 21:51:19 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-02-02 21:51:19 | INFO | train | epoch 160 | loss 5.289 | ppl 39.1 | wps 5908.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10240 | lr 0.0003125 | gnorm 1.032 | train_wall 322 | gb_free 6.1 | wall 56374
KL Stats: Epoch 160 Divergences: Uniform: 3.153923454784151 Unigram: 3.719318855300883
2022-02-02 21:51:19 | INFO | fairseq.trainer | begin training epoch 161
2022-02-02 21:51:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:56:23 | INFO | train_inner | epoch 161:     60 / 64 loss=5.292, ppl=39.17, wps=6092.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=10300, lr=0.000311588, gnorm=1.016, train_wall=503, gb_free=6.1, wall=56679
2022-02-02 21:56:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:57:09 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 10.147 | ppl 1133.74 | wps 8045.9 | wpb 2034.1 | bsz 4 | num_updates 10304 | best_loss 9.237
2022-02-02 21:57:09 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-02-02 21:57:09 | INFO | train | epoch 161 | loss 5.281 | ppl 38.88 | wps 5954.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10304 | lr 0.000311528 | gnorm 1.019 | train_wall 322 | gb_free 6.1 | wall 56725
KL Stats: Epoch 161 Divergences: Uniform: 3.1490002331392954 Unigram: 3.721137934243261
2022-02-02 21:57:10 | INFO | fairseq.trainer | begin training epoch 162
2022-02-02 21:57:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:02:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:03:00 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 10.209 | ppl 1183.79 | wps 8053.1 | wpb 2034.1 | bsz 4 | num_updates 10368 | best_loss 9.237
2022-02-02 22:03:00 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-02-02 22:03:00 | INFO | train | epoch 162 | loss 5.277 | ppl 38.77 | wps 5958.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10368 | lr 0.000310565 | gnorm 1.072 | train_wall 322 | gb_free 6.1 | wall 57075
KL Stats: Epoch 162 Divergences: Uniform: 3.1569342641792346 Unigram: 3.731372960450858
2022-02-02 22:03:00 | INFO | fairseq.trainer | begin training epoch 163
2022-02-02 22:03:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:05:42 | INFO | train_inner | epoch 163:     32 / 64 loss=5.265, ppl=38.46, wps=5831.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10400, lr=0.000310087, gnorm=1.057, train_wall=502, gb_free=6.1, wall=57238
2022-02-02 22:08:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:08:51 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 10.18 | ppl 1160.27 | wps 8022 | wpb 2034.1 | bsz 4 | num_updates 10432 | best_loss 9.237
2022-02-02 22:08:51 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-02-02 22:08:51 | INFO | train | epoch 163 | loss 5.267 | ppl 38.51 | wps 5959.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10432 | lr 0.000309611 | gnorm 1.041 | train_wall 322 | gb_free 6.1 | wall 57426
KL Stats: Epoch 163 Divergences: Uniform: 3.1554346428518554 Unigram: 3.7341695536627433
2022-02-02 22:08:51 | INFO | fairseq.trainer | begin training epoch 164
2022-02-02 22:08:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:14:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:14:41 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 10.186 | ppl 1164.83 | wps 8048.9 | wpb 2034.1 | bsz 4 | num_updates 10496 | best_loss 9.237
2022-02-02 22:14:41 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-02-02 22:14:41 | INFO | train | epoch 164 | loss 5.264 | ppl 38.41 | wps 5962.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10496 | lr 0.000308665 | gnorm 1.043 | train_wall 322 | gb_free 6.1 | wall 57776
KL Stats: Epoch 164 Divergences: Uniform: 3.1555971233674383 Unigram: 3.7364088046619535
2022-02-02 22:14:41 | INFO | fairseq.trainer | begin training epoch 165
2022-02-02 22:14:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:15:01 | INFO | train_inner | epoch 165:      4 / 64 loss=5.273, ppl=38.67, wps=5834.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=10500, lr=0.000308607, gnorm=1.045, train_wall=502, gb_free=6.1, wall=57797
2022-02-02 22:20:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:20:31 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 10.129 | ppl 1119.69 | wps 8042.2 | wpb 2034.1 | bsz 4 | num_updates 10560 | best_loss 9.237
2022-02-02 22:20:31 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-02-02 22:20:31 | INFO | train | epoch 165 | loss 5.257 | ppl 38.24 | wps 5961.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10560 | lr 0.000307729 | gnorm 1.056 | train_wall 322 | gb_free 6.1 | wall 58127
KL Stats: Epoch 165 Divergences: Uniform: 3.1592862329851297 Unigram: 3.741504373309139
2022-02-02 22:20:31 | INFO | fairseq.trainer | begin training epoch 166
2022-02-02 22:20:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:23:54 | INFO | train_inner | epoch 166:     40 / 64 loss=5.249, ppl=38.04, wps=6136.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=10600, lr=0.000307148, gnorm=1.057, train_wall=503, gb_free=6.1, wall=58329
2022-02-02 22:25:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:26:21 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 10.162 | ppl 1145.75 | wps 8052.5 | wpb 2034.1 | bsz 4 | num_updates 10624 | best_loss 9.237
2022-02-02 22:26:21 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-02-02 22:26:21 | INFO | train | epoch 166 | loss 5.252 | ppl 38.12 | wps 5968.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10624 | lr 0.0003068 | gnorm 1.048 | train_wall 321 | gb_free 6.1 | wall 58477
KL Stats: Epoch 166 Divergences: Uniform: 3.1591674034933064 Unigram: 3.748129625189214
2022-02-02 22:26:21 | INFO | fairseq.trainer | begin training epoch 167
2022-02-02 22:26:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:31:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:32:12 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 10.146 | ppl 1133.42 | wps 8033.2 | wpb 2034.1 | bsz 4 | num_updates 10688 | best_loss 9.237
2022-02-02 22:32:12 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-02-02 22:32:12 | INFO | train | epoch 167 | loss 5.249 | ppl 38.03 | wps 5958 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10688 | lr 0.00030588 | gnorm 1.117 | train_wall 322 | gb_free 6.1 | wall 58827
KL Stats: Epoch 167 Divergences: Uniform: 3.1629679666536945 Unigram: 3.750308283582773
2022-02-02 22:32:12 | INFO | fairseq.trainer | begin training epoch 168
2022-02-02 22:32:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:33:12 | INFO | train_inner | epoch 168:     12 / 64 loss=5.25, ppl=38.06, wps=5835.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10700, lr=0.000305709, gnorm=1.089, train_wall=502, gb_free=6.1, wall=58888
2022-02-02 22:37:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:38:02 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 10.159 | ppl 1143.2 | wps 8043.2 | wpb 2034.1 | bsz 4 | num_updates 10752 | best_loss 9.237
2022-02-02 22:38:02 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-02-02 22:38:02 | INFO | train | epoch 168 | loss 5.24 | ppl 37.78 | wps 5960 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10752 | lr 0.000304969 | gnorm 1.066 | train_wall 322 | gb_free 6.1 | wall 59178
KL Stats: Epoch 168 Divergences: Uniform: 3.160610389243853 Unigram: 3.7555405074524657
2022-02-02 22:38:02 | INFO | fairseq.trainer | begin training epoch 169
2022-02-02 22:38:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:42:05 | INFO | train_inner | epoch 169:     48 / 64 loss=5.24, ppl=37.8, wps=6131.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=10800, lr=0.00030429, gnorm=1.072, train_wall=503, gb_free=6.1, wall=59421
2022-02-02 22:43:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:43:52 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 10.153 | ppl 1138.6 | wps 8045.7 | wpb 2034.1 | bsz 4 | num_updates 10816 | best_loss 9.237
2022-02-02 22:43:52 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-02-02 22:43:52 | INFO | train | epoch 169 | loss 5.235 | ppl 37.67 | wps 5960.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10816 | lr 0.000304065 | gnorm 1.073 | train_wall 322 | gb_free 6.1 | wall 59528
KL Stats: Epoch 169 Divergences: Uniform: 3.1672614303314575 Unigram: 3.7662684095207446
2022-02-02 22:43:52 | INFO | fairseq.trainer | begin training epoch 170
2022-02-02 22:43:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:49:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:49:43 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 10.164 | ppl 1147.5 | wps 8038.5 | wpb 2034.1 | bsz 4 | num_updates 10880 | best_loss 9.237
2022-02-02 22:49:43 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-02-02 22:49:43 | INFO | train | epoch 170 | loss 5.228 | ppl 37.49 | wps 5961.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10880 | lr 0.00030317 | gnorm 1.081 | train_wall 322 | gb_free 6.1 | wall 59878
KL Stats: Epoch 170 Divergences: Uniform: 3.1636638423362626 Unigram: 3.764366298900728
2022-02-02 22:49:43 | INFO | fairseq.trainer | begin training epoch 171
2022-02-02 22:49:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:51:24 | INFO | train_inner | epoch 171:     20 / 64 loss=5.225, ppl=37.41, wps=5834.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10900, lr=0.000302891, gnorm=1.075, train_wall=502, gb_free=6.1, wall=59980
2022-02-02 22:55:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:55:33 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 10.181 | ppl 1161.15 | wps 8041.5 | wpb 2034.1 | bsz 4 | num_updates 10944 | best_loss 9.237
2022-02-02 22:55:33 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-02-02 22:55:33 | INFO | train | epoch 171 | loss 5.223 | ppl 37.34 | wps 5969.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10944 | lr 0.000302282 | gnorm 1.07 | train_wall 321 | gb_free 6.1 | wall 60228
KL Stats: Epoch 171 Divergences: Uniform: 3.1676659064455874 Unigram: 3.766353557017767
2022-02-02 22:55:33 | INFO | fairseq.trainer | begin training epoch 172
2022-02-02 22:55:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:00:16 | INFO | train_inner | epoch 172:     56 / 64 loss=5.225, ppl=37.4, wps=6145.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11000, lr=0.000301511, gnorm=1.105, train_wall=502, gb_free=6.1, wall=60511
2022-02-02 23:00:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:01:22 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 10.239 | ppl 1208.36 | wps 8021.8 | wpb 2034.1 | bsz 4 | num_updates 11008 | best_loss 9.237
2022-02-02 23:01:22 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-02-02 23:01:22 | INFO | train | epoch 172 | loss 5.22 | ppl 37.28 | wps 5973.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11008 | lr 0.000301402 | gnorm 1.126 | train_wall 321 | gb_free 6.1 | wall 60578
KL Stats: Epoch 172 Divergences: Uniform: 3.161484149631478 Unigram: 3.771502182846645
2022-02-02 23:01:22 | INFO | fairseq.trainer | begin training epoch 173
2022-02-02 23:01:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:06:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:07:13 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 10.221 | ppl 1193.73 | wps 8020 | wpb 2034.1 | bsz 4 | num_updates 11072 | best_loss 9.237
2022-02-02 23:07:13 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-02-02 23:07:13 | INFO | train | epoch 173 | loss 5.214 | ppl 37.12 | wps 5953.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11072 | lr 0.000300529 | gnorm 1.135 | train_wall 322 | gb_free 6.1 | wall 60929
KL Stats: Epoch 173 Divergences: Uniform: 3.168134791661621 Unigram: 3.777363731233586
2022-02-02 23:07:13 | INFO | fairseq.trainer | begin training epoch 174
2022-02-02 23:07:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:09:35 | INFO | train_inner | epoch 174:     28 / 64 loss=5.206, ppl=36.92, wps=5831.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=11100, lr=0.00030015, gnorm=1.131, train_wall=502, gb_free=6.1, wall=61071
2022-02-02 23:12:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:13:03 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 10.19 | ppl 1168.26 | wps 8027.5 | wpb 2034.1 | bsz 4 | num_updates 11136 | best_loss 9.237
2022-02-02 23:13:03 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-02-02 23:13:03 | INFO | train | epoch 174 | loss 5.207 | ppl 36.93 | wps 5973 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11136 | lr 0.000299665 | gnorm 1.143 | train_wall 321 | gb_free 6.1 | wall 61278
KL Stats: Epoch 174 Divergences: Uniform: 3.170206019997967 Unigram: 3.778427070688789
2022-02-02 23:13:03 | INFO | fairseq.trainer | begin training epoch 175
2022-02-02 23:13:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:18:26 | INFO | train_inner | epoch 175:     64 / 64 loss=5.213, ppl=37.1, wps=6134.1, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=11200, lr=0.000298807, gnorm=1.165, train_wall=502, gb_free=6.1, wall=61602
2022-02-02 23:18:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:18:54 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 10.204 | ppl 1179.5 | wps 8029.6 | wpb 2034.1 | bsz 4 | num_updates 11200 | best_loss 9.237
2022-02-02 23:18:54 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-02-02 23:18:54 | INFO | train | epoch 175 | loss 5.202 | ppl 36.82 | wps 5954.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11200 | lr 0.000298807 | gnorm 1.169 | train_wall 322 | gb_free 6.1 | wall 61629
KL Stats: Epoch 175 Divergences: Uniform: 3.164189732238763 Unigram: 3.7867579432630136
2022-02-02 23:18:54 | INFO | fairseq.trainer | begin training epoch 176
2022-02-02 23:18:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:24:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:24:44 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 10.177 | ppl 1157.49 | wps 8013.9 | wpb 2034.1 | bsz 4 | num_updates 11264 | best_loss 9.237
2022-02-02 23:24:44 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-02-02 23:24:44 | INFO | train | epoch 176 | loss 5.197 | ppl 36.67 | wps 5964.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11264 | lr 0.000297957 | gnorm 1.158 | train_wall 321 | gb_free 6.1 | wall 61979
KL Stats: Epoch 176 Divergences: Uniform: 3.171902604278754 Unigram: 3.789222106688085
2022-02-02 23:24:44 | INFO | fairseq.trainer | begin training epoch 177
2022-02-02 23:24:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:27:46 | INFO | train_inner | epoch 177:     36 / 64 loss=5.184, ppl=36.35, wps=5837.1, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=11300, lr=0.000297482, gnorm=1.149, train_wall=503, gb_free=6.1, wall=62162
2022-02-02 23:30:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:30:34 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 10.168 | ppl 1150.13 | wps 8039.9 | wpb 2034.1 | bsz 4 | num_updates 11328 | best_loss 9.237
2022-02-02 23:30:34 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-02-02 23:30:34 | INFO | train | epoch 177 | loss 5.191 | ppl 36.54 | wps 5963.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11328 | lr 0.000297114 | gnorm 1.136 | train_wall 321 | gb_free 6.1 | wall 62330
KL Stats: Epoch 177 Divergences: Uniform: 3.178030201026825 Unigram: 3.795148260266422
2022-02-02 23:30:34 | INFO | fairseq.trainer | begin training epoch 178
2022-02-02 23:30:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:35:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:36:25 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 10.189 | ppl 1167.46 | wps 8048.8 | wpb 2034.1 | bsz 4 | num_updates 11392 | best_loss 9.237
2022-02-02 23:36:25 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-02-02 23:36:25 | INFO | train | epoch 178 | loss 5.186 | ppl 36.41 | wps 5959.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11392 | lr 0.000296278 | gnorm 1.179 | train_wall 322 | gb_free 6.1 | wall 62680
KL Stats: Epoch 178 Divergences: Uniform: 3.1716145633116755 Unigram: 3.8011885747558214
2022-02-02 23:36:25 | INFO | fairseq.trainer | begin training epoch 179
2022-02-02 23:36:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:37:05 | INFO | train_inner | epoch 179:      8 / 64 loss=5.193, ppl=36.59, wps=5834, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=11400, lr=0.000296174, gnorm=1.164, train_wall=502, gb_free=6.1, wall=62721
2022-02-02 23:41:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:42:15 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 10.172 | ppl 1153.71 | wps 8039.1 | wpb 2034.1 | bsz 4 | num_updates 11456 | best_loss 9.237
2022-02-02 23:42:15 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-02-02 23:42:15 | INFO | train | epoch 179 | loss 5.179 | ppl 36.22 | wps 5967.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11456 | lr 0.00029545 | gnorm 1.135 | train_wall 321 | gb_free 6.1 | wall 63030
KL Stats: Epoch 179 Divergences: Uniform: 3.178450842724871 Unigram: 3.80171778971388
2022-02-02 23:42:15 | INFO | fairseq.trainer | begin training epoch 180
2022-02-02 23:42:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:45:57 | INFO | train_inner | epoch 180:     44 / 64 loss=5.173, ppl=36.08, wps=6141.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11500, lr=0.000294884, gnorm=1.162, train_wall=502, gb_free=6.1, wall=63253
2022-02-02 23:47:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:48:04 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 10.223 | ppl 1195.28 | wps 8047.6 | wpb 2034.1 | bsz 4 | num_updates 11520 | best_loss 9.237
2022-02-02 23:48:04 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-02-02 23:48:04 | INFO | train | epoch 180 | loss 5.177 | ppl 36.19 | wps 5974.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11520 | lr 0.000294628 | gnorm 1.19 | train_wall 321 | gb_free 6.1 | wall 63380
KL Stats: Epoch 180 Divergences: Uniform: 3.177723207317137 Unigram: 3.810189776456573
2022-02-02 23:48:04 | INFO | fairseq.trainer | begin training epoch 181
2022-02-02 23:48:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:53:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:53:55 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 10.2 | ppl 1176.12 | wps 8036.9 | wpb 2034.1 | bsz 4 | num_updates 11584 | best_loss 9.237
2022-02-02 23:53:55 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-02-02 23:53:55 | INFO | train | epoch 181 | loss 5.172 | ppl 36.04 | wps 5952.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11584 | lr 0.000293813 | gnorm 1.185 | train_wall 322 | gb_free 6.1 | wall 63731
KL Stats: Epoch 181 Divergences: Uniform: 3.1772708308680873 Unigram: 3.8105919163226023
2022-02-02 23:53:55 | INFO | fairseq.trainer | begin training epoch 182
2022-02-02 23:53:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:55:16 | INFO | train_inner | epoch 182:     16 / 64 loss=5.172, ppl=36.06, wps=5831.5, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=11600, lr=0.00029361, gnorm=1.179, train_wall=502, gb_free=6.1, wall=63812
2022-02-02 23:59:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:59:45 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 10.199 | ppl 1175.56 | wps 8030.2 | wpb 2034.1 | bsz 4 | num_updates 11648 | best_loss 9.237
2022-02-02 23:59:45 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-02-02 23:59:45 | INFO | train | epoch 182 | loss 5.164 | ppl 35.86 | wps 5962.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11648 | lr 0.000293005 | gnorm 1.185 | train_wall 321 | gb_free 6.1 | wall 64081
KL Stats: Epoch 182 Divergences: Uniform: 3.1800318316676806 Unigram: 3.815005060730116
2022-02-02 23:59:45 | INFO | fairseq.trainer | begin training epoch 183
2022-02-02 23:59:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:04:09 | INFO | train_inner | epoch 183:     52 / 64 loss=5.165, ppl=35.87, wps=6136.5, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11700, lr=0.000292353, gnorm=1.205, train_wall=503, gb_free=6.1, wall=64344
2022-02-03 00:05:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-03 00:05:35 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 10.218 | ppl 1191.18 | wps 8048.3 | wpb 2034.1 | bsz 4 | num_updates 11712 | best_loss 9.237
2022-02-03 00:05:35 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-02-03 00:05:35 | INFO | train | epoch 183 | loss 5.162 | ppl 35.8 | wps 5968.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11712 | lr 0.000292203 | gnorm 1.217 | train_wall 321 | gb_free 6.1 | wall 64431
KL Stats: Epoch 183 Divergences: Uniform: 3.175875979109018 Unigram: 3.8168518470225736
2022-02-03 00:05:35 | INFO | fairseq.trainer | begin training epoch 184
2022-02-03 00:05:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:10:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:11:25 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 10.194 | ppl 1171.78 | wps 8050.6 | wpb 2034.1 | bsz 4 | num_updates 11776 | best_loss 9.237
2022-02-03 00:11:25 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-02-03 00:11:25 | INFO | train | epoch 184 | loss 5.155 | ppl 35.62 | wps 5974.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11776 | lr 0.000291408 | gnorm 1.199 | train_wall 321 | gb_free 6.1 | wall 64780
KL Stats: Epoch 184 Divergences: Uniform: 3.174364197693345 Unigram: 3.822921434826603
2022-02-03 00:11:25 | INFO | fairseq.trainer | begin training epoch 185
2022-02-03 00:11:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:13:26 | INFO | train_inner | epoch 185:     24 / 64 loss=5.153, ppl=35.59, wps=5848.5, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=11800, lr=0.000291111, gnorm=1.212, train_wall=501, gb_free=6.1, wall=64902
2022-02-03 00:16:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:17:14 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 10.205 | ppl 1180.62 | wps 8053.8 | wpb 2034.1 | bsz 4 | num_updates 11840 | best_loss 9.237
2022-02-03 00:17:14 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-02-03 00:17:14 | INFO | train | epoch 185 | loss 5.152 | ppl 35.55 | wps 5975.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11840 | lr 0.000290619 | gnorm 1.248 | train_wall 321 | gb_free 6.1 | wall 65130
KL Stats: Epoch 185 Divergences: Uniform: 3.178412747076232 Unigram: 3.819909323071354
2022-02-03 00:17:14 | INFO | fairseq.trainer | begin training epoch 186
2022-02-03 00:17:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:22:18 | INFO | train_inner | epoch 186:     60 / 64 loss=5.155, ppl=35.62, wps=6145.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11900, lr=0.000289886, gnorm=1.24, train_wall=502, gb_free=6.1, wall=65433
2022-02-03 00:22:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:23:04 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 10.268 | ppl 1233.15 | wps 8033 | wpb 2034.1 | bsz 4 | num_updates 11904 | best_loss 9.237
2022-02-03 00:23:04 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-02-03 00:23:04 | INFO | train | epoch 186 | loss 5.147 | ppl 35.43 | wps 5970.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11904 | lr 0.000289837 | gnorm 1.228 | train_wall 321 | gb_free 6.1 | wall 65480
KL Stats: Epoch 186 Divergences: Uniform: 3.174593791504516 Unigram: 3.8286688936269924
2022-02-03 00:23:04 | INFO | fairseq.trainer | begin training epoch 187
2022-02-03 00:23:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:28:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:28:54 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 10.235 | ppl 1205.34 | wps 8040.9 | wpb 2034.1 | bsz 4 | num_updates 11968 | best_loss 9.237
2022-02-03 00:28:54 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-02-03 00:28:54 | INFO | train | epoch 187 | loss 5.141 | ppl 35.29 | wps 5965.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11968 | lr 0.000289061 | gnorm 1.235 | train_wall 321 | gb_free 6.1 | wall 65830
KL Stats: Epoch 187 Divergences: Uniform: 3.1789345817464327 Unigram: 3.8351253265282654
2022-02-03 00:28:54 | INFO | fairseq.trainer | begin training epoch 188
2022-02-03 00:28:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:31:36 | INFO | train_inner | epoch 188:     32 / 64 loss=5.137, ppl=35.19, wps=5840.6, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=12000, lr=0.000288675, gnorm=1.225, train_wall=501, gb_free=6.1, wall=65992
2022-02-03 00:34:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-03 00:34:44 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 10.174 | ppl 1155.21 | wps 8027.4 | wpb 2034.1 | bsz 4 | num_updates 12032 | best_loss 9.237
2022-02-03 00:34:44 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-02-03 00:34:44 | INFO | train | epoch 188 | loss 5.136 | ppl 35.17 | wps 5977.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12032 | lr 0.000288291 | gnorm 1.231 | train_wall 321 | gb_free 6.1 | wall 66179
KL Stats: Epoch 188 Divergences: Uniform: 3.1817230170851434 Unigram: 3.83672351197989
2022-02-03 00:34:44 | INFO | fairseq.trainer | begin training epoch 189
2022-02-03 00:34:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:40:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-03 00:40:33 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 10.19 | ppl 1167.81 | wps 8074.9 | wpb 2034.1 | bsz 4 | num_updates 12096 | best_loss 9.237
2022-02-03 00:40:33 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-02-03 00:40:33 | INFO | train | epoch 189 | loss 5.133 | ppl 35.09 | wps 5972.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12096 | lr 0.000287527 | gnorm 1.255 | train_wall 321 | gb_free 6.1 | wall 66529
KL Stats: Epoch 189 Divergences: Uniform: 3.186392379346764 Unigram: 3.8414403563650565
2022-02-03 00:40:33 | INFO | fairseq.trainer | begin training epoch 190
2022-02-03 00:40:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:40:54 | INFO | train_inner | epoch 190:      4 / 64 loss=5.137, ppl=35.18, wps=5846.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12100, lr=0.00028748, gnorm=1.252, train_wall=501, gb_free=6.1, wall=66549
2022-02-03 00:45:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:46:23 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 10.269 | ppl 1233.8 | wps 8031.4 | wpb 2034.1 | bsz 4 | num_updates 12160 | best_loss 9.237
2022-02-03 00:46:23 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-02-03 00:46:23 | INFO | train | epoch 190 | loss 5.129 | ppl 34.98 | wps 5975.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12160 | lr 0.00028677 | gnorm 1.264 | train_wall 321 | gb_free 6.1 | wall 66878
KL Stats: Epoch 190 Divergences: Uniform: 3.180259538342357 Unigram: 3.849686434043549
2022-02-03 00:46:23 | INFO | fairseq.trainer | begin training epoch 191
2022-02-03 00:46:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:49:46 | INFO | train_inner | epoch 191:     40 / 64 loss=5.121, ppl=34.81, wps=6145.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=12200, lr=0.000286299, gnorm=1.276, train_wall=502, gb_free=6.1, wall=67081
2022-02-03 00:51:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:52:13 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 10.227 | ppl 1198.53 | wps 8038.9 | wpb 2034.1 | bsz 4 | num_updates 12224 | best_loss 9.237
2022-02-03 00:52:13 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-02-03 00:52:13 | INFO | train | epoch 191 | loss 5.125 | ppl 34.9 | wps 5965.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12224 | lr 0.000286018 | gnorm 1.294 | train_wall 321 | gb_free 6.1 | wall 67229
KL Stats: Epoch 191 Divergences: Uniform: 3.1875798699065307 Unigram: 3.8482399323332643
2022-02-03 00:52:13 | INFO | fairseq.trainer | begin training epoch 192
2022-02-03 00:52:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:57:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:58:04 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 10.234 | ppl 1204.63 | wps 8043 | wpb 2034.1 | bsz 4 | num_updates 12288 | best_loss 9.237
2022-02-03 00:58:04 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-02-03 00:58:04 | INFO | train | epoch 192 | loss 5.121 | ppl 34.81 | wps 5959.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12288 | lr 0.000285272 | gnorm 1.267 | train_wall 322 | gb_free 6.1 | wall 67579
KL Stats: Epoch 192 Divergences: Uniform: 3.1924121984100005 Unigram: 3.8542058487971307
2022-02-03 00:58:04 | INFO | fairseq.trainer | begin training epoch 193
2022-02-03 00:58:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:59:04 | INFO | train_inner | epoch 193:     12 / 64 loss=5.124, ppl=34.87, wps=5833.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=12300, lr=0.000285133, gnorm=1.285, train_wall=502, gb_free=6.1, wall=67640
2022-02-03 01:03:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:03:53 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 10.188 | ppl 1166.41 | wps 8019.5 | wpb 2034.1 | bsz 4 | num_updates 12352 | best_loss 9.237
2022-02-03 01:03:53 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-02-03 01:03:53 | INFO | train | epoch 193 | loss 5.115 | ppl 34.65 | wps 5968 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12352 | lr 0.000284532 | gnorm 1.312 | train_wall 321 | gb_free 6.1 | wall 67929
KL Stats: Epoch 193 Divergences: Uniform: 3.1857823754118857 Unigram: 3.851945333993017
2022-02-03 01:03:53 | INFO | fairseq.trainer | begin training epoch 194
2022-02-03 01:03:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:07:57 | INFO | train_inner | epoch 194:     48 / 64 loss=5.111, ppl=34.57, wps=6139.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=12400, lr=0.000283981, gnorm=1.298, train_wall=502, gb_free=6.1, wall=68172
2022-02-03 01:09:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:09:43 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 10.191 | ppl 1169.16 | wps 8045 | wpb 2034.1 | bsz 4 | num_updates 12416 | best_loss 9.237
2022-02-03 01:09:43 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-02-03 01:09:43 | INFO | train | epoch 194 | loss 5.111 | ppl 34.57 | wps 5968.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12416 | lr 0.000283798 | gnorm 1.308 | train_wall 321 | gb_free 6.1 | wall 68279
KL Stats: Epoch 194 Divergences: Uniform: 3.1843903704955214 Unigram: 3.8594247459889095
2022-02-03 01:09:43 | INFO | fairseq.trainer | begin training epoch 195
2022-02-03 01:09:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:15:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:15:34 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 10.219 | ppl 1191.56 | wps 8029 | wpb 2034.1 | bsz 4 | num_updates 12480 | best_loss 9.237
2022-02-03 01:15:34 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-02-03 01:15:34 | INFO | train | epoch 195 | loss 5.107 | ppl 34.47 | wps 5957.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12480 | lr 0.000283069 | gnorm 1.337 | train_wall 322 | gb_free 6.1 | wall 68629
KL Stats: Epoch 195 Divergences: Uniform: 3.1903251732819036 Unigram: 3.8655210936064197
2022-02-03 01:15:34 | INFO | fairseq.trainer | begin training epoch 196
2022-02-03 01:15:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:17:15 | INFO | train_inner | epoch 196:     20 / 64 loss=5.107, ppl=34.45, wps=5835.6, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=12500, lr=0.000282843, gnorm=1.332, train_wall=502, gb_free=6.1, wall=68731
2022-02-03 01:20:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:21:23 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 10.244 | ppl 1212.73 | wps 8065.2 | wpb 2034.1 | bsz 4 | num_updates 12544 | best_loss 9.237
2022-02-03 01:21:23 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-02-03 01:21:23 | INFO | train | epoch 196 | loss 5.103 | ppl 34.36 | wps 5977.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12544 | lr 0.000282346 | gnorm 1.309 | train_wall 321 | gb_free 6.1 | wall 68979
KL Stats: Epoch 196 Divergences: Uniform: 3.184983323427819 Unigram: 3.8676421736485684
2022-02-03 01:21:23 | INFO | fairseq.trainer | begin training epoch 197
2022-02-03 01:21:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:26:07 | INFO | train_inner | epoch 197:     56 / 64 loss=5.102, ppl=34.33, wps=6141.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=12600, lr=0.000281718, gnorm=1.313, train_wall=502, gb_free=6.1, wall=69263
2022-02-03 01:26:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:27:14 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 10.217 | ppl 1190.01 | wps 8041.7 | wpb 2034.1 | bsz 4 | num_updates 12608 | best_loss 9.237
2022-02-03 01:27:14 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-02-03 01:27:14 | INFO | train | epoch 197 | loss 5.098 | ppl 34.25 | wps 5961 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12608 | lr 0.000281629 | gnorm 1.308 | train_wall 322 | gb_free 6.1 | wall 69329
KL Stats: Epoch 197 Divergences: Uniform: 3.1880642450284387 Unigram: 3.8699423878445818
2022-02-03 01:27:14 | INFO | fairseq.trainer | begin training epoch 198
2022-02-03 01:27:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:32:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:33:04 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 10.294 | ppl 1255.22 | wps 8044.2 | wpb 2034.1 | bsz 4 | num_updates 12672 | best_loss 9.237
2022-02-03 01:33:04 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-02-03 01:33:04 | INFO | train | epoch 198 | loss 5.095 | ppl 34.18 | wps 5968.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12672 | lr 0.000280917 | gnorm 1.33 | train_wall 321 | gb_free 6.1 | wall 69679
KL Stats: Epoch 198 Divergences: Uniform: 3.194134373586547 Unigram: 3.8758951676881317
2022-02-03 01:33:04 | INFO | fairseq.trainer | begin training epoch 199
2022-02-03 01:33:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:35:25 | INFO | train_inner | epoch 199:     28 / 64 loss=5.092, ppl=34.1, wps=5844.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=12700, lr=0.000280607, gnorm=1.331, train_wall=501, gb_free=6.1, wall=69821
2022-02-03 01:38:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:38:53 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 10.271 | ppl 1235.49 | wps 8055.1 | wpb 2034.1 | bsz 4 | num_updates 12736 | best_loss 9.237
2022-02-03 01:38:53 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-02-03 01:38:53 | INFO | train | epoch 199 | loss 5.09 | ppl 34.05 | wps 5981.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12736 | lr 0.00028021 | gnorm 1.363 | train_wall 320 | gb_free 6.1 | wall 70028
KL Stats: Epoch 199 Divergences: Uniform: 3.197228686311115 Unigram: 3.876054758291164
2022-02-03 01:38:53 | INFO | fairseq.trainer | begin training epoch 200
2022-02-03 01:38:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:44:16 | INFO | train_inner | epoch 200:     64 / 64 loss=5.095, ppl=34.18, wps=6141.8, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=12800, lr=0.000279508, gnorm=1.346, train_wall=501, gb_free=6.1, wall=70351
2022-02-03 01:44:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:44:43 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 10.295 | ppl 1256.49 | wps 8030.6 | wpb 2034.1 | bsz 4 | num_updates 12800 | best_loss 9.237
2022-02-03 01:44:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 12800 updates
2022-02-03 01:44:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint200.pt
2022-02-03 01:44:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint200.pt
2022-02-03 01:44:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#4/checkpoint200.pt (epoch 200 @ 12800 updates, score 10.295) (writing took 3.0663319611921906 seconds)
2022-02-03 01:44:46 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-02-03 01:44:46 | INFO | train | epoch 200 | loss 5.084 | ppl 33.92 | wps 5911.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12800 | lr 0.000279508 | gnorm 1.329 | train_wall 321 | gb_free 6.1 | wall 70382
KL Stats: Epoch 200 Divergences: Uniform: 3.194295311736045 Unigram: 3.8806494783942456
2022-02-03 01:44:46 | INFO | fairseq.trainer | begin training epoch 201
2022-02-03 01:44:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:50:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:50:37 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 10.246 | ppl 1214.65 | wps 8020.5 | wpb 2034.1 | bsz 4 | num_updates 12864 | best_loss 9.237
2022-02-03 01:50:37 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-02-03 01:50:37 | INFO | train | epoch 201 | loss 5.081 | ppl 33.85 | wps 5955.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12864 | lr 0.000278812 | gnorm 1.345 | train_wall 322 | gb_free 6.1 | wall 70732
KL Stats: Epoch 201 Divergences: Uniform: 3.1962436451892877 Unigram: 3.881643113696305
2022-02-03 01:50:37 | INFO | fairseq.trainer | begin training epoch 202
2022-02-03 01:50:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:53:40 | INFO | train_inner | epoch 202:     36 / 64 loss=5.072, ppl=33.65, wps=5795.7, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=12900, lr=0.000278423, gnorm=1.343, train_wall=504, gb_free=6.1, wall=70915
2022-02-03 01:56:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:56:28 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 10.273 | ppl 1236.99 | wps 8007.9 | wpb 2034.1 | bsz 4 | num_updates 12928 | best_loss 9.237
2022-02-03 01:56:28 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-02-03 01:56:28 | INFO | train | epoch 202 | loss 5.079 | ppl 33.79 | wps 5949.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12928 | lr 0.000278121 | gnorm 1.335 | train_wall 322 | gb_free 6.1 | wall 71083
KL Stats: Epoch 202 Divergences: Uniform: 3.1951821286361906 Unigram: 3.8868661777633484
2022-02-03 01:56:28 | INFO | fairseq.trainer | begin training epoch 203
2022-02-03 01:56:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 02:01:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 02:02:18 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 10.238 | ppl 1207.86 | wps 8059.8 | wpb 2034.1 | bsz 4 | num_updates 12992 | best_loss 9.237
2022-02-03 02:02:18 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-02-03 02:02:18 | INFO | train | epoch 203 | loss 5.075 | ppl 33.72 | wps 5963.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12992 | lr 0.000277435 | gnorm 1.382 | train_wall 322 | gb_free 6.1 | wall 71434
KL Stats: Epoch 203 Divergences: Uniform: 3.1940101681949296 Unigram: 3.88724055969604
2022-02-03 02:02:18 | INFO | fairseq.trainer | begin training epoch 204
2022-02-03 02:02:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 02:02:59 | INFO | train_inner | epoch 204:      8 / 64 loss=5.08, ppl=33.82, wps=5832.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=13000, lr=0.00027735, gnorm=1.37, train_wall=502, gb_free=6.1, wall=71474
2022-02-03 02:07:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 02:08:08 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 10.224 | ppl 1195.59 | wps 8019.3 | wpb 2034.1 | bsz 4 | num_updates 13056 | best_loss 9.237
2022-02-03 02:08:08 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-02-03 02:08:08 | INFO | train | epoch 204 | loss 5.072 | ppl 33.63 | wps 5973.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 13056 | lr 0.000276755 | gnorm 1.404 | train_wall 321 | gb_free 6.1 | wall 71783
KL Stats: Epoch 204 Divergences: Uniform: 3.1963995413096598 Unigram: 3.889352887564641
2022-02-03 02:08:08 | INFO | fairseq.trainer | begin training epoch 205
2022-02-03 02:08:08 | INFO | fairseq_cli.train | Start iterating over samples
User defined signal 2
