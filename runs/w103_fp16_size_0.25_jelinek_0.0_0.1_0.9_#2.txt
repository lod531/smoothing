Sender: LSF System <lsfadmin@eu-g3-071>
Subject: Job 207019766: <w103_fp16_size_0.25_jelinek_0.0_0.1_0.9_#2> in cluster <euler> Exited

Job <w103_fp16_size_0.25_jelinek_0.0_0.1_0.9_#2> was submitted from host <eu-login-19> by user <andriusb> in cluster <euler> at Thu Mar  3 11:53:31 2022
Job was executed on host(s) <eu-g3-071>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Thu Mar  3 20:37:41 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Mar  3 20:37:41 2022
Terminated at Sat Mar  5 08:35:25 2022
Results reported at Sat Mar  5 08:35:25 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.0, 0.1, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321672 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   129411.04 sec.
    Max Memory :                                 8322 MB
    Average Memory :                             2880.33 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11678.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   129466 sec.
    Turnaround time :                            160914 sec.

The output (if any) follows:

2022-03-03 20:37:48 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321672, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321672, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.0, 0.1, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-03 20:37:48 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-03 20:37:53 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
Calculating frequency stats:
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 685/450337 [00:00<01:05, 6838.30it/s]  0%|          | 1369/450337 [00:00<01:15, 5960.53it/s]  0%|          | 1973/450337 [00:00<01:18, 5680.15it/s]  1%|          | 2560/450337 [00:00<01:17, 5743.26it/s]  1%|          | 3238/450337 [00:00<01:13, 6091.31it/s]  1%|          | 3875/450337 [00:00<01:12, 6180.82it/s]  1%|          | 4582/450337 [00:00<01:09, 6459.71it/s]  1%|          | 5276/450337 [00:00<01:07, 6609.74it/s]  1%|▏         | 5975/450337 [00:00<01:06, 6722.35it/s]  1%|▏         | 6649/450337 [00:01<01:12, 6103.79it/s]  2%|▏         | 7271/450337 [00:01<01:12, 6113.63it/s]  2%|▏         | 7891/450337 [00:01<01:12, 6061.61it/s]  2%|▏         | 8503/450337 [00:01<01:15, 5827.70it/s]  2%|▏         | 9152/450337 [00:01<01:13, 6015.32it/s]  2%|▏         | 9759/450337 [00:01<01:14, 5907.62it/s]  2%|▏         | 10399/450337 [00:01<01:12, 6044.28it/s]  2%|▏         | 11007/450337 [00:01<01:13, 6006.31it/s]  3%|▎         | 11610/450337 [00:01<01:14, 5883.48it/s]  3%|▎         | 12236/450337 [00:02<01:13, 5986.16it/s]  3%|▎         | 12846/450337 [00:02<01:12, 6018.05it/s]  3%|▎         | 13507/450337 [00:02<01:10, 6188.04it/s]  3%|▎         | 14127/450337 [00:02<01:10, 6153.74it/s]  3%|▎         | 14744/450337 [00:02<01:10, 6157.14it/s]  3%|▎         | 15374/450337 [00:02<01:10, 6197.16it/s]  4%|▎         | 15995/450337 [00:02<01:13, 5927.84it/s]  4%|▎         | 16591/450337 [00:02<01:13, 5879.27it/s]  4%|▍         | 17221/450337 [00:02<01:12, 6000.15it/s]  4%|▍         | 17846/450337 [00:02<01:11, 6073.29it/s]  4%|▍         | 18455/450337 [00:03<01:12, 5916.37it/s]  4%|▍         | 19123/450337 [00:03<01:10, 6134.81it/s]  4%|▍         | 19793/450337 [00:03<01:08, 6297.83it/s]  5%|▍         | 20425/450337 [00:03<01:11, 6031.35it/s]  5%|▍         | 21042/450337 [00:03<01:10, 6069.45it/s]  5%|▍         | 21652/450337 [00:03<01:13, 5832.81it/s]  5%|▍         | 22311/450337 [00:03<01:10, 6045.95it/s]  5%|▌         | 22970/450337 [00:03<01:08, 6202.49it/s]  5%|▌         | 23630/450337 [00:03<01:07, 6318.41it/s]  5%|▌         | 24341/450337 [00:03<01:05, 6549.10it/s]  6%|▌         | 25069/450337 [00:04<01:02, 6760.20it/s]  6%|▌         | 25747/450337 [00:04<01:04, 6548.11it/s]  6%|▌         | 26405/450337 [00:04<01:09, 6133.23it/s]  6%|▌         | 27025/450337 [00:04<01:10, 5982.42it/s]  6%|▌         | 27628/450337 [00:04<01:12, 5819.87it/s]  6%|▋         | 28265/450337 [00:04<01:10, 5973.71it/s]  6%|▋         | 28923/450337 [00:04<01:08, 6145.69it/s]  7%|▋         | 29541/450337 [00:04<01:10, 5988.47it/s]  7%|▋         | 30159/450337 [00:04<01:09, 6040.57it/s]  7%|▋         | 30766/450337 [00:05<01:13, 5715.74it/s]  7%|▋         | 31354/450337 [00:05<01:12, 5761.72it/s]  7%|▋         | 31937/450337 [00:05<01:12, 5779.03it/s]  7%|▋         | 32518/450337 [00:05<01:12, 5756.46it/s]  7%|▋         | 33096/450337 [00:05<01:15, 5558.80it/s]  7%|▋         | 33690/450337 [00:05<01:13, 5667.97it/s]  8%|▊         | 34269/450337 [00:05<01:12, 5700.94it/s]  8%|▊         | 34981/450337 [00:05<01:07, 6115.17it/s]  8%|▊         | 35595/450337 [00:05<01:09, 5944.63it/s]  8%|▊         | 36214/450337 [00:05<01:08, 6015.61it/s]  8%|▊         | 36818/450337 [00:06<01:08, 5999.31it/s]  8%|▊         | 37420/450337 [00:06<01:13, 5590.34it/s]  8%|▊         | 37992/450337 [00:06<01:13, 5623.04it/s]  9%|▊         | 38598/450337 [00:06<01:11, 5737.13it/s]  9%|▊         | 39213/450337 [00:06<01:10, 5851.47it/s]  9%|▉         | 39802/450337 [00:06<01:10, 5838.70it/s]  9%|▉         | 40466/450337 [00:06<01:07, 6071.26it/s]  9%|▉         | 41075/450337 [00:06<01:07, 6027.29it/s]  9%|▉         | 41680/450337 [00:06<01:08, 5925.03it/s]  9%|▉         | 42274/450337 [00:07<01:12, 5651.22it/s] 10%|▉         | 42843/450337 [00:07<01:12, 5657.63it/s] 10%|▉         | 43411/450337 [00:07<01:12, 5579.06it/s] 10%|▉         | 44071/450337 [00:07<01:09, 5870.27it/s] 10%|▉         | 44733/450337 [00:07<01:06, 6086.71it/s] 10%|█         | 45344/450337 [00:07<01:07, 5972.39it/s] 10%|█         | 46009/450337 [00:07<01:05, 6162.28it/s] 10%|█         | 46672/450337 [00:07<01:04, 6297.66it/s] 11%|█         | 47555/450337 [00:07<00:57, 7044.28it/s] 11%|█         | 48292/450337 [00:07<00:56, 7129.50it/s] 11%|█         | 49007/450337 [00:08<00:57, 6970.73it/s] 11%|█         | 49706/450337 [00:08<00:58, 6898.31it/s] 11%|█         | 50398/450337 [00:08<01:02, 6351.67it/s] 11%|█▏        | 51042/450337 [00:08<01:03, 6271.25it/s] 11%|█▏        | 51675/450337 [00:08<01:03, 6277.99it/s] 12%|█▏        | 52488/450337 [00:08<00:58, 6801.88it/s] 12%|█▏        | 53174/450337 [00:08<01:00, 6538.38it/s] 12%|█▏        | 53834/450337 [00:08<01:02, 6365.88it/s] 12%|█▏        | 54475/450337 [00:08<01:06, 5915.40it/s] 12%|█▏        | 55108/450337 [00:09<01:05, 6025.50it/s] 12%|█▏        | 55780/450337 [00:09<01:03, 6215.25it/s] 13%|█▎        | 56408/450337 [00:09<01:03, 6175.28it/s] 13%|█▎        | 57030/450337 [00:09<01:07, 5835.74it/s] 13%|█▎        | 57620/450337 [00:09<01:07, 5784.05it/s] 13%|█▎        | 58240/450337 [00:09<01:06, 5899.68it/s] 13%|█▎        | 58958/450337 [00:09<01:02, 6267.18it/s] 13%|█▎        | 59589/450337 [00:09<01:04, 6019.88it/s] 13%|█▎        | 60196/450337 [00:09<01:05, 5934.21it/s] 14%|█▎        | 60924/450337 [00:10<01:01, 6318.87it/s] 14%|█▎        | 61580/450337 [00:10<01:00, 6379.34it/s] 14%|█▍        | 62221/450337 [00:10<01:02, 6168.73it/s] 14%|█▍        | 62860/450337 [00:10<01:02, 6230.16it/s] 14%|█▍        | 63486/450337 [00:10<01:03, 6133.03it/s] 14%|█▍        | 64102/450337 [00:10<01:03, 6035.95it/s] 14%|█▍        | 64714/450337 [00:10<01:03, 6057.63it/s] 15%|█▍        | 65321/450337 [00:10<01:04, 5926.95it/s] 15%|█▍        | 65976/450337 [00:10<01:02, 6106.18it/s] 15%|█▍        | 66628/450337 [00:10<01:01, 6219.51it/s] 15%|█▍        | 67252/450337 [00:11<01:04, 5984.57it/s] 15%|█▌        | 67853/450337 [00:11<01:06, 5725.37it/s] 15%|█▌        | 68439/450337 [00:11<01:06, 5763.13it/s] 15%|█▌        | 69172/450337 [00:11<01:01, 6204.49it/s] 16%|█▌        | 69851/450337 [00:11<00:59, 6372.85it/s] 16%|█▌        | 70492/450337 [00:11<00:59, 6367.66it/s] 16%|█▌        | 71131/450337 [00:11<01:02, 6049.13it/s] 16%|█▌        | 71748/450337 [00:11<01:02, 6079.47it/s] 16%|█▌        | 72360/450337 [00:11<01:02, 6012.93it/s] 16%|█▌        | 73113/450337 [00:11<00:58, 6452.43it/s] 16%|█▋        | 73802/450337 [00:12<00:57, 6576.27it/s] 17%|█▋        | 74585/450337 [00:12<00:54, 6940.70it/s] 17%|█▋        | 75282/450337 [00:12<00:56, 6612.50it/s] 17%|█▋        | 75948/450337 [00:12<00:57, 6521.63it/s] 17%|█▋        | 76604/450337 [00:12<00:57, 6469.60it/s] 17%|█▋        | 77254/450337 [00:12<01:00, 6198.63it/s] 17%|█▋        | 77958/450337 [00:12<00:57, 6433.37it/s] 17%|█▋        | 78605/450337 [00:12<01:02, 5979.16it/s] 18%|█▊        | 79211/450337 [00:12<01:05, 5647.34it/s] 18%|█▊        | 79808/450337 [00:13<01:04, 5734.48it/s] 18%|█▊        | 80400/450337 [00:13<01:03, 5780.59it/s] 18%|█▊        | 81030/450337 [00:13<01:02, 5928.22it/s] 18%|█▊        | 81683/450337 [00:13<01:00, 6095.03it/s] 18%|█▊        | 82298/450337 [00:13<01:00, 6109.75it/s] 18%|█▊        | 82914/450337 [00:13<01:00, 6119.37it/s] 19%|█▊        | 83593/450337 [00:13<00:58, 6310.70it/s] 19%|█▊        | 84226/450337 [00:13<00:59, 6147.36it/s] 19%|█▉        | 84905/450337 [00:13<00:57, 6328.64it/s] 19%|█▉        | 85540/450337 [00:14<01:00, 6071.74it/s] 19%|█▉        | 86151/450337 [00:14<01:00, 6038.44it/s] 19%|█▉        | 86813/450337 [00:14<00:58, 6203.65it/s] 19%|█▉        | 87496/450337 [00:14<00:56, 6382.51it/s] 20%|█▉        | 88176/450337 [00:14<00:55, 6498.54it/s] 20%|█▉        | 88828/450337 [00:14<01:01, 5909.12it/s] 20%|█▉        | 89430/450337 [00:14<01:02, 5817.32it/s] 20%|█▉        | 90020/450337 [00:14<01:02, 5739.27it/s] 20%|██        | 90603/450337 [00:14<01:02, 5758.20it/s] 20%|██        | 91259/450337 [00:14<00:59, 5987.70it/s] 20%|██        | 91862/450337 [00:15<01:00, 5960.53it/s] 21%|██        | 92556/450337 [00:15<00:57, 6242.70it/s] 21%|██        | 93183/450337 [00:15<00:59, 5984.85it/s] 21%|██        | 93795/450337 [00:15<00:59, 6017.63it/s] 21%|██        | 94400/450337 [00:15<00:59, 5997.78it/s] 21%|██        | 95089/450337 [00:15<00:56, 6250.95it/s] 21%|██▏       | 95716/450337 [00:15<00:58, 6025.01it/s] 21%|██▏       | 96410/450337 [00:15<00:56, 6286.74it/s] 22%|██▏       | 97183/450337 [00:15<00:52, 6697.97it/s] 22%|██▏       | 97856/450337 [00:16<00:55, 6311.12it/s] 22%|██▏       | 98494/450337 [00:16<00:56, 6247.74it/s] 22%|██▏       | 99124/450337 [00:16<00:57, 6122.10it/s] 22%|██▏       | 99740/450337 [00:16<00:58, 5956.26it/s] 22%|██▏       | 100339/450337 [00:16<01:00, 5800.73it/s] 22%|██▏       | 100959/450337 [00:16<00:59, 5910.12it/s] 23%|██▎       | 101553/450337 [00:16<00:58, 5912.67it/s] 23%|██▎       | 102216/450337 [00:16<00:56, 6118.78it/s] 23%|██▎       | 102830/450337 [00:16<00:57, 6063.76it/s] 23%|██▎       | 103438/450337 [00:16<00:58, 5957.21it/s] 23%|██▎       | 104035/450337 [00:17<00:59, 5792.27it/s] 23%|██▎       | 104671/450337 [00:17<00:58, 5947.82it/s] 23%|██▎       | 105268/450337 [00:17<00:58, 5896.94it/s] 24%|██▎       | 105951/450337 [00:17<00:55, 6157.52it/s] 24%|██▎       | 106569/450337 [00:17<00:57, 5997.35it/s] 24%|██▍       | 107171/450337 [00:17<00:59, 5803.60it/s] 24%|██▍       | 107796/450337 [00:17<00:57, 5927.78it/s] 24%|██▍       | 108399/450337 [00:17<00:57, 5954.92it/s] 24%|██▍       | 108996/450337 [00:17<01:00, 5671.23it/s] 24%|██▍       | 109604/450337 [00:18<00:58, 5783.74it/s] 24%|██▍       | 110203/450337 [00:18<00:58, 5837.85it/s] 25%|██▍       | 110853/450337 [00:18<00:56, 6027.76it/s] 25%|██▍       | 111512/450337 [00:18<00:54, 6184.93it/s] 25%|██▍       | 112133/450337 [00:18<00:55, 6064.78it/s] 25%|██▌       | 112790/450337 [00:18<00:54, 6210.37it/s] 25%|██▌       | 113413/450337 [00:18<00:56, 6012.02it/s] 25%|██▌       | 114017/450337 [00:18<00:56, 5969.50it/s] 25%|██▌       | 114616/450337 [00:18<00:56, 5910.61it/s] 26%|██▌       | 115231/450337 [00:18<00:56, 5976.43it/s] 26%|██▌       | 115830/450337 [00:19<00:59, 5663.47it/s] 26%|██▌       | 116417/450337 [00:19<00:58, 5716.96it/s] 26%|██▌       | 116997/450337 [00:19<00:58, 5739.17it/s] 26%|██▌       | 117578/450337 [00:19<00:57, 5758.53it/s] 26%|██▌       | 118156/450337 [00:19<00:57, 5748.82it/s] 26%|██▋       | 118796/450337 [00:19<00:55, 5936.36it/s] 27%|██▋       | 119391/450337 [00:19<00:56, 5821.96it/s] 27%|██▋       | 120037/450337 [00:19<00:54, 6008.98it/s] 27%|██▋       | 120709/450337 [00:19<00:53, 6212.01it/s] 27%|██▋       | 121332/450337 [00:19<00:53, 6095.82it/s] 27%|██▋       | 121943/450337 [00:20<00:56, 5806.79it/s] 27%|██▋       | 122575/450337 [00:20<00:55, 5950.58it/s] 27%|██▋       | 123174/450337 [00:20<00:55, 5869.27it/s] 27%|██▋       | 123769/450337 [00:20<00:55, 5892.44it/s] 28%|██▊       | 124361/450337 [00:20<00:55, 5895.82it/s] 28%|██▊       | 124953/450337 [00:20<00:55, 5896.09it/s] 28%|██▊       | 125557/450337 [00:20<00:54, 5937.44it/s] 28%|██▊       | 126221/450337 [00:20<00:52, 6141.98it/s] 28%|██▊       | 126924/450337 [00:20<00:50, 6401.02it/s] 28%|██▊       | 127565/450337 [00:21<00:53, 6031.69it/s] 28%|██▊       | 128193/450337 [00:21<00:52, 6100.85it/s] 29%|██▊       | 128807/450337 [00:21<00:53, 5992.87it/s] 29%|██▉       | 129489/450337 [00:21<00:51, 6228.14it/s] 29%|██▉       | 130115/450337 [00:21<00:54, 5853.12it/s] 29%|██▉       | 130707/450337 [00:21<00:55, 5807.02it/s] 29%|██▉       | 131331/450337 [00:21<00:53, 5928.08it/s] 29%|██▉       | 131928/450337 [00:21<00:59, 5319.41it/s] 29%|██▉       | 132617/450337 [00:21<00:55, 5741.43it/s] 30%|██▉       | 133310/450337 [00:21<00:52, 6065.70it/s] 30%|██▉       | 133929/450337 [00:22<00:52, 6072.28it/s] 30%|██▉       | 134619/450337 [00:22<00:50, 6308.65it/s] 30%|███       | 135351/450337 [00:22<00:47, 6600.46it/s] 30%|███       | 136017/450337 [00:22<00:49, 6408.40it/s] 30%|███       | 136744/450337 [00:22<00:47, 6653.55it/s] 31%|███       | 137463/450337 [00:22<00:45, 6809.39it/s] 31%|███       | 138148/450337 [00:22<00:49, 6357.83it/s] 31%|███       | 138795/450337 [00:22<00:48, 6388.90it/s] 31%|███       | 139440/450337 [00:22<00:50, 6195.55it/s] 31%|███       | 140065/450337 [00:23<00:50, 6177.29it/s] 31%|███       | 140686/450337 [00:23<00:50, 6178.35it/s] 31%|███▏      | 141307/450337 [00:23<00:52, 5895.41it/s] 32%|███▏      | 141940/450337 [00:23<00:51, 6017.79it/s] 32%|███▏      | 142546/450337 [00:23<00:51, 5990.38it/s] 32%|███▏      | 143148/450337 [00:23<00:53, 5692.60it/s] 32%|███▏      | 143942/450337 [00:23<00:48, 6325.23it/s] 32%|███▏      | 144582/450337 [00:23<00:48, 6301.92it/s] 32%|███▏      | 145249/450337 [00:23<00:47, 6405.66it/s] 32%|███▏      | 145938/450337 [00:23<00:46, 6545.78it/s] 33%|███▎      | 146596/450337 [00:24<00:48, 6300.81it/s] 33%|███▎      | 147230/450337 [00:24<00:49, 6102.93it/s] 33%|███▎      | 147844/450337 [00:24<00:52, 5735.04it/s] 33%|███▎      | 148460/450337 [00:24<00:51, 5848.61it/s] 33%|███▎      | 149117/450337 [00:24<00:49, 6048.88it/s] 33%|███▎      | 149727/450337 [00:24<00:51, 5875.89it/s] 33%|███▎      | 150319/450337 [00:24<00:53, 5605.70it/s] 34%|███▎      | 150915/450337 [00:24<00:52, 5702.65it/s] 34%|███▎      | 151533/450337 [00:24<00:51, 5833.56it/s] 34%|███▍      | 152120/450337 [00:25<00:53, 5613.77it/s] 34%|███▍      | 152770/450337 [00:25<00:50, 5864.38it/s] 34%|███▍      | 153469/450337 [00:25<00:47, 6186.04it/s] 34%|███▍      | 154092/450337 [00:25<00:50, 5854.20it/s] 34%|███▍      | 154684/450337 [00:25<00:50, 5816.03it/s] 34%|███▍      | 155324/450337 [00:25<00:49, 5982.14it/s] 35%|███▍      | 155933/450337 [00:25<00:48, 6011.30it/s] 35%|███▍      | 156606/450337 [00:25<00:47, 6220.50it/s] 35%|███▍      | 157267/450337 [00:25<00:46, 6332.23it/s] 35%|███▌      | 157919/450337 [00:26<00:45, 6384.29it/s] 35%|███▌      | 158570/450337 [00:26<00:45, 6420.57it/s] 35%|███▌      | 159214/450337 [00:26<00:48, 5949.11it/s] 36%|███▌      | 159922/450337 [00:26<00:46, 6268.47it/s] 36%|███▌      | 160556/450337 [00:26<00:47, 6157.56it/s] 36%|███▌      | 161218/450337 [00:26<00:45, 6287.63it/s] 36%|███▌      | 161851/450337 [00:26<00:48, 5932.07it/s] 36%|███▌      | 162528/450337 [00:26<00:46, 6166.18it/s] 36%|███▌      | 163194/450337 [00:26<00:45, 6304.47it/s] 36%|███▋      | 163830/450337 [00:26<00:48, 5941.62it/s] 37%|███▋      | 164443/450337 [00:27<00:47, 5991.41it/s] 37%|███▋      | 165110/450337 [00:27<00:46, 6181.73it/s] 37%|███▋      | 165807/450337 [00:27<00:44, 6410.12it/s] 37%|███▋      | 166452/450337 [00:27<00:44, 6330.19it/s] 37%|███▋      | 167141/450337 [00:27<00:43, 6490.00it/s] 37%|███▋      | 167793/450337 [00:27<00:45, 6278.26it/s] 37%|███▋      | 168424/450337 [00:27<00:47, 5959.30it/s] 38%|███▊      | 169114/450337 [00:27<00:45, 6221.53it/s] 38%|███▊      | 169742/450337 [00:27<00:46, 6044.73it/s] 38%|███▊      | 170351/450337 [00:28<00:48, 5718.64it/s] 38%|███▊      | 170960/450337 [00:28<00:48, 5816.18it/s] 38%|███▊      | 171564/450337 [00:28<00:47, 5869.94it/s] 38%|███▊      | 172155/450337 [00:28<00:48, 5722.65it/s] 38%|███▊      | 172803/450337 [00:28<00:46, 5933.00it/s] 39%|███▊      | 173400/450337 [00:28<00:46, 5899.55it/s] 39%|███▊      | 174018/450337 [00:28<00:46, 5980.76it/s] 39%|███▉      | 174682/450337 [00:28<00:44, 6173.86it/s] 39%|███▉      | 175301/450337 [00:28<00:46, 5907.93it/s] 39%|███▉      | 175896/450337 [00:28<00:46, 5883.73it/s] 39%|███▉      | 176487/450337 [00:29<00:47, 5811.28it/s] 39%|███▉      | 177070/450337 [00:29<00:47, 5761.36it/s] 39%|███▉      | 177648/450337 [00:29<00:51, 5264.79it/s] 40%|███▉      | 178292/450337 [00:29<00:48, 5582.32it/s] 40%|███▉      | 178934/450337 [00:29<00:46, 5818.11it/s] 40%|███▉      | 179562/450337 [00:29<00:45, 5946.56it/s] 40%|████      | 180163/450337 [00:29<00:47, 5734.44it/s] 40%|████      | 180802/450337 [00:29<00:45, 5921.41it/s] 40%|████      | 181399/450337 [00:29<00:46, 5764.80it/s] 40%|████      | 181980/450337 [00:30<00:47, 5640.52it/s] 41%|████      | 182571/450337 [00:30<00:46, 5716.49it/s] 41%|████      | 183177/450337 [00:30<00:45, 5814.94it/s] 41%|████      | 183761/450337 [00:30<00:49, 5377.08it/s] 41%|████      | 184321/450337 [00:30<00:48, 5435.83it/s] 41%|████      | 184941/450337 [00:30<00:46, 5650.91it/s] 41%|████      | 185512/450337 [00:30<00:48, 5504.76it/s] 41%|████▏     | 186076/450337 [00:30<00:47, 5542.75it/s] 41%|████▏     | 186679/450337 [00:30<00:46, 5680.05it/s] 42%|████▏     | 187314/450337 [00:30<00:44, 5867.95it/s] 42%|████▏     | 187903/450337 [00:31<00:45, 5775.35it/s] 42%|████▏     | 188639/450337 [00:31<00:42, 6230.90it/s] 42%|████▏     | 189265/450337 [00:31<00:43, 5979.87it/s] 42%|████▏     | 189918/450337 [00:31<00:42, 6136.32it/s] 42%|████▏     | 190535/450337 [00:31<00:42, 6130.79it/s] 42%|████▏     | 191155/450337 [00:31<00:42, 6147.11it/s] 43%|████▎     | 191772/450337 [00:31<00:43, 6006.20it/s] 43%|████▎     | 192410/450337 [00:31<00:42, 6113.33it/s] 43%|████▎     | 193023/450337 [00:31<00:42, 6064.85it/s] 43%|████▎     | 193645/450337 [00:32<00:42, 6108.95it/s] 43%|████▎     | 194267/450337 [00:32<00:41, 6138.84it/s] 43%|████▎     | 194882/450337 [00:32<00:42, 6066.64it/s] 43%|████▎     | 195497/450337 [00:32<00:41, 6090.91it/s] 44%|████▎     | 196107/450337 [00:32<00:42, 6015.28it/s] 44%|████▎     | 196709/450337 [00:32<00:43, 5770.21it/s] 44%|████▍     | 197665/450337 [00:32<00:36, 6859.14it/s] 44%|████▍     | 198357/450337 [00:32<00:40, 6281.57it/s] 44%|████▍     | 198998/450337 [00:32<00:40, 6266.37it/s] 44%|████▍     | 199634/450337 [00:32<00:40, 6133.33it/s] 44%|████▍     | 200254/450337 [00:33<00:41, 5995.84it/s] 45%|████▍     | 200858/450337 [00:33<00:42, 5838.12it/s] 45%|████▍     | 201445/450337 [00:33<00:43, 5761.36it/s] 45%|████▍     | 202079/450337 [00:33<00:41, 5912.57it/s] 45%|████▌     | 202673/450337 [00:33<00:42, 5892.01it/s] 45%|████▌     | 203356/450337 [00:33<00:40, 6159.81it/s] 45%|████▌     | 204023/450337 [00:33<00:39, 6308.24it/s] 45%|████▌     | 204656/450337 [00:33<00:40, 6031.32it/s] 46%|████▌     | 205263/450337 [00:33<00:41, 5967.64it/s] 46%|████▌     | 205863/450337 [00:34<00:41, 5853.17it/s] 46%|████▌     | 206518/450337 [00:34<00:40, 6042.75it/s] 46%|████▌     | 207125/450337 [00:34<00:42, 5772.55it/s] 46%|████▌     | 207723/450337 [00:34<00:41, 5829.01it/s] 46%|████▋     | 208309/450337 [00:34<00:42, 5753.71it/s] 46%|████▋     | 208887/450337 [00:34<00:43, 5545.89it/s] 47%|████▋     | 209450/450337 [00:34<00:43, 5565.53it/s] 47%|████▋     | 210159/450337 [00:34<00:40, 6000.63it/s] 47%|████▋     | 210789/450337 [00:34<00:39, 6087.68it/s] 47%|████▋     | 211400/450337 [00:34<00:40, 5957.83it/s] 47%|████▋     | 211998/450337 [00:35<00:40, 5952.03it/s] 47%|████▋     | 212710/450337 [00:35<00:37, 6290.24it/s] 47%|████▋     | 213341/450337 [00:35<00:38, 6106.67it/s] 48%|████▊     | 213954/450337 [00:35<00:40, 5803.69it/s] 48%|████▊     | 214539/450337 [00:35<00:40, 5762.83it/s] 48%|████▊     | 215118/450337 [00:35<00:42, 5499.56it/s] 48%|████▊     | 215672/450337 [00:35<00:43, 5392.14it/s] 48%|████▊     | 216294/450337 [00:35<00:41, 5621.01it/s] 48%|████▊     | 216860/450337 [00:35<00:41, 5606.39it/s] 48%|████▊     | 217475/450337 [00:36<00:40, 5755.72it/s] 48%|████▊     | 218121/450337 [00:36<00:38, 5959.79it/s] 49%|████▊     | 218750/450337 [00:36<00:38, 6056.40it/s] 49%|████▊     | 219384/450337 [00:36<00:37, 6136.33it/s] 49%|████▉     | 219999/450337 [00:36<00:38, 6009.44it/s] 49%|████▉     | 220602/450337 [00:36<00:39, 5792.66it/s] 49%|████▉     | 221293/450337 [00:36<00:37, 6112.94it/s] 49%|████▉     | 221908/450337 [00:36<00:38, 5869.04it/s] 49%|████▉     | 222499/450337 [00:36<00:40, 5621.87it/s] 50%|████▉     | 223094/450337 [00:37<00:39, 5709.54it/s] 50%|████▉     | 223669/450337 [00:37<00:41, 5441.59it/s] 50%|████▉     | 224344/450337 [00:37<00:38, 5806.04it/s] 50%|████▉     | 225003/450337 [00:37<00:37, 6027.57it/s] 50%|█████     | 225676/450337 [00:37<00:36, 6224.50it/s] 50%|█████     | 226383/450337 [00:37<00:34, 6468.73it/s] 50%|█████     | 227034/450337 [00:37<00:36, 6152.81it/s] 51%|█████     | 227655/450337 [00:37<00:36, 6058.65it/s] 51%|█████     | 228265/450337 [00:37<00:37, 5924.42it/s] 51%|█████     | 228861/450337 [00:37<00:38, 5722.81it/s] 51%|█████     | 229436/450337 [00:38<00:39, 5599.54it/s] 51%|█████     | 229998/450337 [00:38<00:40, 5487.71it/s] 51%|█████     | 230668/450337 [00:38<00:37, 5830.65it/s] 51%|█████▏    | 231254/450337 [00:38<00:37, 5787.92it/s] 51%|█████▏    | 231835/450337 [00:38<00:38, 5612.39it/s] 52%|█████▏    | 232418/450337 [00:38<00:38, 5674.07it/s] 52%|█████▏    | 233154/450337 [00:38<00:35, 6162.91it/s] 52%|█████▏    | 233826/450337 [00:38<00:34, 6323.73it/s] 52%|█████▏    | 234531/450337 [00:38<00:33, 6537.60it/s] 52%|█████▏    | 235187/450337 [00:39<00:33, 6515.19it/s] 52%|█████▏    | 235840/450337 [00:39<00:33, 6388.12it/s] 53%|█████▎    | 236481/450337 [00:39<00:35, 5952.38it/s] 53%|█████▎    | 237083/450337 [00:39<00:35, 5961.51it/s] 53%|█████▎    | 237713/450337 [00:39<00:35, 6055.84it/s] 53%|█████▎    | 238323/450337 [00:39<00:35, 5977.32it/s] 53%|█████▎    | 238975/450337 [00:39<00:34, 6129.43it/s] 53%|█████▎    | 239591/450337 [00:39<00:34, 6079.84it/s] 53%|█████▎    | 240201/450337 [00:39<00:36, 5810.87it/s] 54%|█████▎    | 240982/450337 [00:39<00:32, 6379.59it/s] 54%|█████▎    | 241633/450337 [00:40<00:32, 6404.52it/s] 54%|█████▍    | 242283/450337 [00:40<00:32, 6430.46it/s] 54%|█████▍    | 242929/450337 [00:40<00:33, 6158.01it/s] 54%|█████▍    | 243582/450337 [00:40<00:33, 6263.59it/s] 54%|█████▍    | 244212/450337 [00:40<00:34, 6015.67it/s] 54%|█████▍    | 244825/450337 [00:40<00:34, 6042.28it/s] 54%|█████▍    | 245433/450337 [00:40<00:33, 6046.20it/s] 55%|█████▍    | 246130/450337 [00:40<00:32, 6303.13it/s] 55%|█████▍    | 246916/450337 [00:40<00:30, 6758.03it/s] 55%|█████▍    | 247595/450337 [00:41<00:31, 6491.45it/s] 55%|█████▌    | 248248/450337 [00:41<00:32, 6192.83it/s] 55%|█████▌    | 248960/450337 [00:41<00:31, 6447.21it/s] 55%|█████▌    | 249675/450337 [00:41<00:30, 6646.56it/s] 56%|█████▌    | 250344/450337 [00:41<00:31, 6285.63it/s] 56%|█████▌    | 250999/450337 [00:41<00:31, 6353.75it/s] 56%|█████▌    | 251640/450337 [00:41<00:31, 6313.68it/s] 56%|█████▌    | 252275/450337 [00:41<00:31, 6288.35it/s] 56%|█████▌    | 252907/450337 [00:41<00:32, 6118.34it/s] 56%|█████▋    | 253521/450337 [00:41<00:32, 6099.22it/s] 56%|█████▋    | 254203/450337 [00:42<00:31, 6303.86it/s] 57%|█████▋    | 254836/450337 [00:42<00:32, 6065.06it/s] 57%|█████▋    | 255446/450337 [00:42<00:32, 6042.51it/s] 57%|█████▋    | 256137/450337 [00:42<00:30, 6287.42it/s] 57%|█████▋    | 256775/450337 [00:42<00:30, 6312.46it/s] 57%|█████▋    | 257408/450337 [00:42<00:30, 6259.06it/s] 57%|█████▋    | 258065/450337 [00:42<00:30, 6340.81it/s] 57%|█████▋    | 258700/450337 [00:42<00:33, 5678.32it/s] 58%|█████▊    | 259281/450337 [00:42<00:34, 5577.27it/s] 58%|█████▊    | 259933/450337 [00:43<00:32, 5833.91it/s] 58%|█████▊    | 260669/450337 [00:43<00:30, 6262.46it/s] 58%|█████▊    | 261304/450337 [00:43<00:32, 5903.54it/s] 58%|█████▊    | 261973/450337 [00:43<00:30, 6122.33it/s] 58%|█████▊    | 262594/450337 [00:43<00:32, 5826.98it/s] 58%|█████▊    | 263185/450337 [00:43<00:32, 5839.91it/s] 59%|█████▊    | 263815/450337 [00:43<00:31, 5970.40it/s] 59%|█████▊    | 264417/450337 [00:43<00:32, 5705.47it/s] 59%|█████▉    | 265062/450337 [00:43<00:31, 5913.84it/s] 59%|█████▉    | 265659/450337 [00:44<00:31, 5797.42it/s] 59%|█████▉    | 266243/450337 [00:44<00:32, 5656.11it/s] 59%|█████▉    | 266812/450337 [00:44<00:32, 5627.85it/s] 59%|█████▉    | 267432/450337 [00:44<00:31, 5786.59it/s] 60%|█████▉    | 268013/450337 [00:44<00:31, 5720.17it/s] 60%|█████▉    | 268671/450337 [00:44<00:30, 5962.19it/s] 60%|█████▉    | 269331/450337 [00:44<00:29, 6147.81it/s] 60%|█████▉    | 269948/450337 [00:44<00:31, 5811.47it/s] 60%|██████    | 270631/450337 [00:44<00:29, 6095.68it/s] 60%|██████    | 271246/450337 [00:44<00:29, 6020.45it/s] 60%|██████    | 271902/450337 [00:45<00:28, 6170.78it/s] 61%|██████    | 272636/450337 [00:45<00:27, 6511.67it/s] 61%|██████    | 273293/450337 [00:45<00:27, 6526.52it/s] 61%|██████    | 273948/450337 [00:45<00:27, 6510.81it/s] 61%|██████    | 274601/450337 [00:45<00:27, 6284.87it/s] 61%|██████    | 275232/450337 [00:45<00:29, 5922.35it/s] 61%|██████    | 275830/450337 [00:45<00:29, 5914.36it/s] 61%|██████▏   | 276426/450337 [00:45<00:30, 5628.85it/s] 62%|██████▏   | 277035/450337 [00:45<00:30, 5754.04it/s] 62%|██████▏   | 277615/450337 [00:46<00:30, 5721.35it/s] 62%|██████▏   | 278251/450337 [00:46<00:29, 5904.08it/s] 62%|██████▏   | 278937/450337 [00:46<00:27, 6179.69it/s] 62%|██████▏   | 279558/450337 [00:46<00:28, 6047.52it/s] 62%|██████▏   | 280204/450337 [00:46<00:27, 6162.54it/s] 62%|██████▏   | 280823/450337 [00:46<00:29, 5736.45it/s] 62%|██████▏   | 281452/450337 [00:46<00:28, 5888.72it/s] 63%|██████▎   | 282092/450337 [00:46<00:27, 6032.79it/s] 63%|██████▎   | 282700/450337 [00:46<00:29, 5754.80it/s] 63%|██████▎   | 283384/450337 [00:46<00:27, 6061.28it/s] 63%|██████▎   | 283996/450337 [00:47<00:27, 6058.06it/s] 63%|██████▎   | 284606/450337 [00:47<00:27, 6002.18it/s] 63%|██████▎   | 285218/450337 [00:47<00:27, 6035.87it/s] 63%|██████▎   | 285824/450337 [00:47<00:27, 6004.21it/s] 64%|██████▎   | 286426/450337 [00:47<00:27, 5989.22it/s] 64%|██████▎   | 287026/450337 [00:47<00:28, 5813.37it/s] 64%|██████▍   | 287624/450337 [00:47<00:27, 5848.66it/s] 64%|██████▍   | 288287/450337 [00:47<00:26, 6075.83it/s] 64%|██████▍   | 288949/450337 [00:47<00:25, 6234.98it/s] 64%|██████▍   | 289574/450337 [00:47<00:27, 5895.85it/s] 64%|██████▍   | 290169/450337 [00:48<00:28, 5661.70it/s] 65%|██████▍   | 290815/450337 [00:48<00:27, 5883.07it/s] 65%|██████▍   | 291408/450337 [00:48<00:27, 5853.13it/s] 65%|██████▍   | 292008/450337 [00:48<00:26, 5889.11it/s] 65%|██████▍   | 292653/450337 [00:48<00:26, 6048.62it/s] 65%|██████▌   | 293260/450337 [00:48<00:26, 5952.26it/s] 65%|██████▌   | 293857/450337 [00:48<00:26, 5808.80it/s] 65%|██████▌   | 294506/450337 [00:48<00:25, 6005.75it/s] 66%|██████▌   | 295155/450337 [00:48<00:25, 6143.33it/s] 66%|██████▌   | 295771/450337 [00:49<00:25, 6130.59it/s] 66%|██████▌   | 296386/450337 [00:49<00:25, 5990.73it/s] 66%|██████▌   | 297015/450337 [00:49<00:25, 6076.52it/s] 66%|██████▌   | 297624/450337 [00:49<00:25, 6038.59it/s] 66%|██████▌   | 298258/450337 [00:49<00:24, 6125.70it/s] 66%|██████▋   | 298872/450337 [00:49<00:24, 6074.51it/s] 67%|██████▋   | 299529/450337 [00:49<00:24, 6214.81it/s] 67%|██████▋   | 300152/450337 [00:49<00:24, 6186.80it/s] 67%|██████▋   | 300772/450337 [00:49<00:24, 6063.31it/s] 67%|██████▋   | 301380/450337 [00:49<00:25, 5829.37it/s] 67%|██████▋   | 301965/450337 [00:50<00:26, 5592.70it/s] 67%|██████▋   | 302663/450337 [00:50<00:24, 5979.50it/s] 67%|██████▋   | 303319/450337 [00:50<00:23, 6142.53it/s] 67%|██████▋   | 303939/450337 [00:50<00:23, 6155.11it/s] 68%|██████▊   | 304558/450337 [00:50<00:24, 6021.04it/s] 68%|██████▊   | 305167/450337 [00:50<00:24, 6031.22it/s] 68%|██████▊   | 305772/450337 [00:50<00:24, 5889.04it/s] 68%|██████▊   | 306389/450337 [00:50<00:24, 5963.77it/s] 68%|██████▊   | 306987/450337 [00:50<00:24, 5786.40it/s] 68%|██████▊   | 307622/450337 [00:51<00:23, 5948.74it/s] 68%|██████▊   | 308219/450337 [00:51<00:24, 5744.46it/s] 69%|██████▊   | 308796/450337 [00:51<00:24, 5663.34it/s] 69%|██████▉   | 309724/450337 [00:51<00:20, 6702.19it/s] 69%|██████▉   | 310400/450337 [00:51<00:21, 6442.15it/s] 69%|██████▉   | 311050/450337 [00:51<00:22, 6238.72it/s] 69%|██████▉   | 311679/450337 [00:51<00:22, 6126.67it/s] 69%|██████▉   | 312328/450337 [00:51<00:22, 6225.14it/s] 69%|██████▉   | 312983/450337 [00:51<00:21, 6316.26it/s] 70%|██████▉   | 313644/450337 [00:51<00:21, 6397.41it/s] 70%|██████▉   | 314318/450337 [00:52<00:20, 6497.96it/s] 70%|██████▉   | 314970/450337 [00:52<00:22, 5947.68it/s] 70%|███████   | 315575/450337 [00:52<00:22, 5929.17it/s] 70%|███████   | 316200/450337 [00:52<00:22, 6019.83it/s] 70%|███████   | 316808/450337 [00:52<00:22, 5823.96it/s] 70%|███████   | 317416/450337 [00:52<00:22, 5891.95it/s] 71%|███████   | 318009/450337 [00:52<00:23, 5683.75it/s] 71%|███████   | 318581/450337 [00:52<00:23, 5672.53it/s] 71%|███████   | 319211/450337 [00:52<00:22, 5851.35it/s] 71%|███████   | 319829/450337 [00:53<00:22, 5930.16it/s] 71%|███████   | 320424/450337 [00:53<00:22, 5680.39it/s] 71%|███████▏  | 321161/450337 [00:53<00:20, 6162.17it/s] 71%|███████▏  | 321782/450337 [00:53<00:21, 5964.73it/s] 72%|███████▏  | 322383/450337 [00:53<00:22, 5741.91it/s] 72%|███████▏  | 322962/450337 [00:53<00:22, 5703.70it/s] 72%|███████▏  | 323613/450337 [00:53<00:21, 5932.34it/s] 72%|███████▏  | 324246/450337 [00:53<00:20, 6045.78it/s] 72%|███████▏  | 324874/450337 [00:53<00:20, 6114.08it/s] 72%|███████▏  | 325488/450337 [00:54<00:22, 5552.07it/s] 72%|███████▏  | 326067/450337 [00:54<00:22, 5616.06it/s] 73%|███████▎  | 326787/450337 [00:54<00:20, 6062.70it/s] 73%|███████▎  | 327424/450337 [00:54<00:19, 6148.07it/s] 73%|███████▎  | 328228/450337 [00:54<00:18, 6688.16it/s] 73%|███████▎  | 328903/450337 [00:54<00:18, 6645.89it/s] 73%|███████▎  | 329572/450337 [00:54<00:19, 6174.56it/s] 73%|███████▎  | 330199/450337 [00:54<00:20, 5828.31it/s] 73%|███████▎  | 330791/450337 [00:54<00:20, 5795.57it/s] 74%|███████▎  | 331377/450337 [00:54<00:20, 5783.14it/s] 74%|███████▎  | 331966/450337 [00:55<00:20, 5812.69it/s] 74%|███████▍  | 332613/450337 [00:55<00:19, 5990.68it/s] 74%|███████▍  | 333215/450337 [00:55<00:19, 5907.53it/s] 74%|███████▍  | 333865/450337 [00:55<00:19, 6079.20it/s] 74%|███████▍  | 334475/450337 [00:55<00:19, 6000.66it/s] 74%|███████▍  | 335086/450337 [00:55<00:19, 6026.75it/s] 75%|███████▍  | 335720/450337 [00:55<00:18, 6117.46it/s] 75%|███████▍  | 336333/450337 [00:55<00:18, 6095.59it/s] 75%|███████▍  | 336975/450337 [00:55<00:18, 6188.63it/s] 75%|███████▍  | 337615/450337 [00:55<00:18, 6249.21it/s] 75%|███████▌  | 338241/450337 [00:56<00:17, 6242.52it/s] 75%|███████▌  | 338936/450337 [00:56<00:17, 6450.36it/s] 75%|███████▌  | 339582/450337 [00:56<00:17, 6175.45it/s] 76%|███████▌  | 340203/450337 [00:56<00:19, 5657.46it/s] 76%|███████▌  | 340827/450337 [00:56<00:18, 5817.03it/s] 76%|███████▌  | 341433/450337 [00:56<00:18, 5877.21it/s] 76%|███████▌  | 342045/450337 [00:56<00:18, 5946.16it/s] 76%|███████▌  | 342658/450337 [00:56<00:17, 5997.71it/s] 76%|███████▌  | 343267/450337 [00:56<00:17, 6017.67it/s] 76%|███████▋  | 343977/450337 [00:57<00:16, 6332.28it/s] 77%|███████▋  | 344613/450337 [00:57<00:17, 5963.76it/s] 77%|███████▋  | 345289/450337 [00:57<00:16, 6187.87it/s] 77%|███████▋  | 345913/450337 [00:57<00:18, 5634.68it/s] 77%|███████▋  | 346621/450337 [00:57<00:17, 6028.70it/s] 77%|███████▋  | 347237/450337 [00:57<00:17, 5955.56it/s] 77%|███████▋  | 347842/450337 [00:57<00:17, 5907.16it/s] 77%|███████▋  | 348439/450337 [00:57<00:17, 5812.05it/s] 78%|███████▊  | 349061/450337 [00:57<00:17, 5923.74it/s] 78%|███████▊  | 349657/450337 [00:58<00:17, 5916.82it/s] 78%|███████▊  | 350496/450337 [00:58<00:15, 6626.15it/s] 78%|███████▊  | 351162/450337 [00:58<00:15, 6245.92it/s] 78%|███████▊  | 351793/450337 [00:58<00:15, 6171.06it/s] 78%|███████▊  | 352415/450337 [00:58<00:17, 5653.13it/s] 78%|███████▊  | 353022/450337 [00:58<00:16, 5765.17it/s] 79%|███████▊  | 353607/450337 [00:58<00:16, 5745.86it/s] 79%|███████▊  | 354188/450337 [00:58<00:16, 5677.29it/s] 79%|███████▉  | 354902/450337 [00:58<00:15, 6085.59it/s] 79%|███████▉  | 355516/450337 [00:58<00:16, 5829.51it/s] 79%|███████▉  | 356137/450337 [00:59<00:15, 5931.27it/s] 79%|███████▉  | 356735/450337 [00:59<00:15, 5911.21it/s] 79%|███████▉  | 357329/450337 [00:59<00:17, 5452.46it/s] 79%|███████▉  | 358013/450337 [00:59<00:15, 5830.23it/s] 80%|███████▉  | 358605/450337 [00:59<00:15, 5851.00it/s] 80%|███████▉  | 359203/450337 [00:59<00:15, 5881.73it/s] 80%|███████▉  | 359809/450337 [00:59<00:15, 5929.80it/s] 80%|████████  | 360443/450337 [00:59<00:14, 6048.51it/s] 80%|████████  | 361126/450337 [00:59<00:14, 6278.66it/s] 80%|████████  | 361868/450337 [01:00<00:13, 6613.51it/s] 81%|████████  | 362532/450337 [01:00<00:13, 6442.85it/s] 81%|████████  | 363179/450337 [01:00<00:13, 6418.30it/s] 81%|████████  | 363823/450337 [01:00<00:13, 6390.24it/s] 81%|████████  | 364478/450337 [01:00<00:13, 6436.26it/s] 81%|████████  | 365166/450337 [01:00<00:12, 6567.42it/s] 81%|████████  | 365824/450337 [01:00<00:13, 6440.68it/s] 81%|████████▏ | 366470/450337 [01:00<00:13, 6274.90it/s] 82%|████████▏ | 367116/450337 [01:00<00:13, 6324.00it/s] 82%|████████▏ | 367750/450337 [01:00<00:14, 5889.27it/s] 82%|████████▏ | 368346/450337 [01:01<00:14, 5830.47it/s] 82%|████████▏ | 368986/450337 [01:01<00:13, 5989.58it/s] 82%|████████▏ | 369607/450337 [01:01<00:13, 6047.10it/s] 82%|████████▏ | 370238/450337 [01:01<00:13, 6115.94it/s] 82%|████████▏ | 370852/450337 [01:01<00:13, 5830.36it/s] 82%|████████▏ | 371447/450337 [01:01<00:13, 5861.82it/s] 83%|████████▎ | 372037/450337 [01:01<00:13, 5790.12it/s] 83%|████████▎ | 372655/450337 [01:01<00:13, 5902.37it/s] 83%|████████▎ | 373286/450337 [01:01<00:12, 6011.86it/s] 83%|████████▎ | 373889/450337 [01:02<00:13, 5871.81it/s] 83%|████████▎ | 374621/450337 [01:02<00:12, 6291.26it/s] 83%|████████▎ | 375253/450337 [01:02<00:12, 6159.19it/s] 83%|████████▎ | 375871/450337 [01:02<00:13, 5715.10it/s] 84%|████████▎ | 376460/450337 [01:02<00:12, 5763.04it/s] 84%|████████▎ | 377042/450337 [01:02<00:13, 5566.91it/s] 84%|████████▍ | 377616/450337 [01:02<00:12, 5609.54it/s] 84%|████████▍ | 378324/450337 [01:02<00:11, 6030.10it/s] 84%|████████▍ | 378932/450337 [01:02<00:11, 6022.62it/s] 84%|████████▍ | 379559/450337 [01:02<00:11, 6089.88it/s] 84%|████████▍ | 380262/450337 [01:03<00:11, 6358.33it/s] 85%|████████▍ | 380949/450337 [01:03<00:10, 6508.04it/s] 85%|████████▍ | 381602/450337 [01:03<00:10, 6305.58it/s] 85%|████████▍ | 382235/450337 [01:03<00:11, 5911.63it/s] 85%|████████▌ | 382833/450337 [01:03<00:11, 5764.51it/s] 85%|████████▌ | 383424/450337 [01:03<00:11, 5795.86it/s] 85%|████████▌ | 384027/450337 [01:03<00:11, 5862.55it/s] 85%|████████▌ | 384724/450337 [01:03<00:10, 6178.10it/s] 86%|████████▌ | 385356/450337 [01:03<00:10, 6217.38it/s] 86%|████████▌ | 385980/450337 [01:04<00:10, 5897.40it/s] 86%|████████▌ | 386575/450337 [01:04<00:10, 5855.16it/s] 86%|████████▌ | 387334/450337 [01:04<00:09, 6351.65it/s] 86%|████████▌ | 387993/450337 [01:04<00:09, 6413.34it/s] 86%|████████▋ | 388638/450337 [01:04<00:10, 6138.57it/s] 86%|████████▋ | 389278/450337 [01:04<00:09, 6213.20it/s] 87%|████████▋ | 389903/450337 [01:04<00:09, 6211.91it/s] 87%|████████▋ | 390527/450337 [01:04<00:09, 6145.50it/s] 87%|████████▋ | 391151/450337 [01:04<00:09, 6160.76it/s] 87%|████████▋ | 391769/450337 [01:04<00:10, 5761.62it/s] 87%|████████▋ | 392351/450337 [01:05<00:10, 5765.07it/s] 87%|████████▋ | 392932/450337 [01:05<00:10, 5705.48it/s] 87%|████████▋ | 393506/450337 [01:05<00:10, 5612.38it/s] 88%|████████▊ | 394177/450337 [01:05<00:09, 5923.04it/s] 88%|████████▊ | 394772/450337 [01:05<00:09, 5844.96it/s] 88%|████████▊ | 395407/450337 [01:05<00:09, 5983.05it/s] 88%|████████▊ | 396032/450337 [01:05<00:08, 6051.72it/s] 88%|████████▊ | 396639/450337 [01:05<00:09, 5805.10it/s] 88%|████████▊ | 397223/450337 [01:05<00:09, 5637.48it/s] 88%|████████▊ | 397835/450337 [01:06<00:09, 5770.07it/s] 88%|████████▊ | 398518/450337 [01:06<00:08, 6073.12it/s] 89%|████████▊ | 399165/450337 [01:06<00:08, 6188.46it/s] 89%|████████▉ | 399906/450337 [01:06<00:07, 6543.13it/s] 89%|████████▉ | 400563/450337 [01:06<00:07, 6265.37it/s] 89%|████████▉ | 401194/450337 [01:06<00:08, 6040.29it/s] 89%|████████▉ | 401802/450337 [01:06<00:08, 5817.67it/s] 89%|████████▉ | 402388/450337 [01:06<00:08, 5497.94it/s] 89%|████████▉ | 402954/450337 [01:06<00:08, 5537.82it/s] 90%|████████▉ | 403596/450337 [01:06<00:08, 5784.21it/s] 90%|████████▉ | 404179/450337 [01:07<00:08, 5643.74it/s] 90%|████████▉ | 404851/450337 [01:07<00:07, 5949.42it/s] 90%|█████████ | 405450/450337 [01:07<00:08, 5605.82it/s] 90%|█████████ | 406057/450337 [01:07<00:07, 5733.16it/s] 90%|█████████ | 406636/450337 [01:07<00:07, 5577.97it/s] 90%|█████████ | 407282/450337 [01:07<00:07, 5823.50it/s] 91%|█████████ | 407869/450337 [01:07<00:07, 5660.78it/s] 91%|█████████ | 408494/450337 [01:07<00:07, 5828.77it/s] 91%|█████████ | 409162/450337 [01:07<00:06, 6070.81it/s] 91%|█████████ | 409773/450337 [01:08<00:06, 5917.30it/s] 91%|█████████ | 410368/450337 [01:08<00:06, 5828.38it/s] 91%|█████████▏| 411011/450337 [01:08<00:06, 5995.49it/s] 91%|█████████▏| 411613/450337 [01:08<00:06, 5855.68it/s] 92%|█████████▏| 412201/450337 [01:08<00:06, 5681.53it/s] 92%|█████████▏| 412772/450337 [01:08<00:07, 5323.05it/s] 92%|█████████▏| 413319/450337 [01:08<00:06, 5360.30it/s] 92%|█████████▏| 413885/450337 [01:08<00:06, 5443.37it/s] 92%|█████████▏| 414433/450337 [01:08<00:06, 5303.02it/s] 92%|█████████▏| 414966/450337 [01:09<00:06, 5284.80it/s] 92%|█████████▏| 415586/450337 [01:09<00:06, 5534.00it/s] 92%|█████████▏| 416150/450337 [01:09<00:06, 5562.56it/s] 93%|█████████▎| 416733/450337 [01:09<00:05, 5640.14it/s] 93%|█████████▎| 417299/450337 [01:09<00:05, 5640.71it/s] 93%|█████████▎| 417975/450337 [01:09<00:05, 5966.95it/s] 93%|█████████▎| 418573/450337 [01:09<00:05, 5694.83it/s] 93%|█████████▎| 419255/450337 [01:09<00:05, 6018.54it/s] 93%|█████████▎| 419861/450337 [01:09<00:05, 5929.79it/s] 93%|█████████▎| 420468/450337 [01:09<00:05, 5965.00it/s] 94%|█████████▎| 421067/450337 [01:10<00:04, 5914.50it/s] 94%|█████████▎| 421660/450337 [01:10<00:05, 5559.44it/s] 94%|█████████▍| 422238/450337 [01:10<00:04, 5620.66it/s] 94%|█████████▍| 422936/450337 [01:10<00:04, 6005.15it/s] 94%|█████████▍| 423541/450337 [01:10<00:04, 6007.85it/s] 94%|█████████▍| 424145/450337 [01:10<00:04, 5825.62it/s] 94%|█████████▍| 424731/450337 [01:10<00:04, 5790.25it/s] 94%|█████████▍| 425313/450337 [01:10<00:04, 5709.02it/s] 95%|█████████▍| 425961/450337 [01:10<00:04, 5927.62it/s] 95%|█████████▍| 426707/450337 [01:10<00:03, 6367.13it/s] 95%|█████████▍| 427346/450337 [01:11<00:03, 6195.33it/s] 95%|█████████▌| 427968/450337 [01:11<00:03, 6041.77it/s] 95%|█████████▌| 428746/450337 [01:11<00:03, 6541.49it/s] 95%|█████████▌| 429404/450337 [01:11<00:03, 6278.46it/s] 95%|█████████▌| 430037/450337 [01:11<00:03, 6237.41it/s] 96%|█████████▌| 430664/450337 [01:11<00:03, 6122.11it/s] 96%|█████████▌| 431279/450337 [01:11<00:03, 5695.43it/s] 96%|█████████▌| 431855/450337 [01:11<00:03, 5532.69it/s] 96%|█████████▌| 432674/450337 [01:11<00:02, 6261.68it/s] 96%|█████████▌| 433310/450337 [01:12<00:02, 6184.20it/s] 96%|█████████▋| 433935/450337 [01:12<00:02, 6106.51it/s] 96%|█████████▋| 434550/450337 [01:12<00:02, 5663.35it/s] 97%|█████████▋| 435125/450337 [01:12<00:02, 5643.47it/s] 97%|█████████▋| 435754/450337 [01:12<00:02, 5821.72it/s] 97%|█████████▋| 436342/450337 [01:12<00:02, 5509.91it/s] 97%|█████████▋| 436971/450337 [01:12<00:02, 5725.89it/s] 97%|█████████▋| 437573/450337 [01:12<00:02, 5804.89it/s] 97%|█████████▋| 438159/450337 [01:12<00:02, 5762.77it/s] 97%|█████████▋| 438749/450337 [01:13<00:01, 5795.44it/s] 98%|█████████▊| 439331/450337 [01:13<00:01, 5570.97it/s] 98%|█████████▊| 439892/450337 [01:13<00:01, 5420.62it/s] 98%|█████████▊| 440446/450337 [01:13<00:01, 5454.33it/s] 98%|█████████▊| 441014/450337 [01:13<00:01, 5515.91it/s] 98%|█████████▊| 441620/450337 [01:13<00:01, 5674.58it/s] 98%|█████████▊| 442244/450337 [01:13<00:01, 5836.84it/s] 98%|█████████▊| 442830/450337 [01:13<00:01, 5678.14it/s] 98%|█████████▊| 443491/450337 [01:13<00:01, 5946.63it/s] 99%|█████████▊| 444133/450337 [01:13<00:01, 6084.11it/s] 99%|█████████▉| 444744/450337 [01:14<00:01, 5582.87it/s] 99%|█████████▉| 445312/450337 [01:14<00:00, 5369.46it/s] 99%|█████████▉| 445915/450337 [01:14<00:00, 5550.37it/s] 99%|█████████▉| 446477/450337 [01:14<00:00, 5537.00it/s] 99%|█████████▉| 447055/450337 [01:14<00:00, 5599.01it/s] 99%|█████████▉| 447748/450337 [01:14<00:00, 5980.70it/s]100%|█████████▉| 448378/450337 [01:14<00:00, 6066.50it/s]100%|█████████▉| 448988/450337 [01:14<00:00, 6073.27it/s]100%|█████████▉| 449598/450337 [01:14<00:00, 5804.78it/s]100%|█████████▉| 450236/450337 [01:15<00:00, 5966.03it/s]100%|██████████| 450337/450337 [01:15<00:00, 6000.27it/s]

gathering stats for n=1
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 1896/450337 [00:00<00:23, 18945.61it/s]  1%|          | 3945/450337 [00:00<00:22, 19842.31it/s]  1%|▏         | 6165/450337 [00:00<00:21, 20915.93it/s]  2%|▏         | 8257/450337 [00:00<00:22, 19885.12it/s]  2%|▏         | 10271/450337 [00:00<00:22, 19968.20it/s]  3%|▎         | 12273/450337 [00:00<00:22, 19666.55it/s]  3%|▎         | 14340/450337 [00:00<00:21, 19985.04it/s]  4%|▎         | 16342/450337 [00:00<00:21, 19766.35it/s]  4%|▍         | 18321/450337 [00:00<00:21, 19657.81it/s]  5%|▍         | 20380/450337 [00:01<00:21, 19936.54it/s]  5%|▍         | 22376/450337 [00:01<00:21, 19780.05it/s]  5%|▌         | 24574/450337 [00:01<00:20, 20437.54it/s]  6%|▌         | 26620/450337 [00:01<00:21, 20173.59it/s]  6%|▋         | 28640/450337 [00:01<00:21, 20032.16it/s]  7%|▋         | 30645/450337 [00:01<00:21, 19513.00it/s]  7%|▋         | 32600/450337 [00:01<00:21, 19185.73it/s]  8%|▊         | 34558/450337 [00:01<00:21, 19299.87it/s]  8%|▊         | 36491/450337 [00:01<00:21, 19244.74it/s]  9%|▊         | 38417/450337 [00:01<00:21, 18889.68it/s]  9%|▉         | 40423/450337 [00:02<00:21, 19224.43it/s]  9%|▉         | 42348/450337 [00:02<00:21, 18928.71it/s] 10%|▉         | 44262/450337 [00:02<00:21, 18985.35it/s] 10%|█         | 46392/450337 [00:02<00:20, 19662.67it/s] 11%|█         | 48721/450337 [00:02<00:19, 20735.38it/s] 11%|█▏        | 50798/450337 [00:02<00:19, 20462.07it/s] 12%|█▏        | 53010/450337 [00:02<00:18, 20946.77it/s] 12%|█▏        | 55108/450337 [00:02<00:19, 20241.25it/s] 13%|█▎        | 57139/450337 [00:02<00:19, 19840.78it/s] 13%|█▎        | 59189/450337 [00:02<00:19, 20030.97it/s] 14%|█▎        | 61313/450337 [00:03<00:19, 20379.13it/s] 14%|█▍        | 63355/450337 [00:03<00:19, 19905.85it/s] 15%|█▍        | 65351/450337 [00:03<00:19, 19800.67it/s] 15%|█▍        | 67357/450337 [00:03<00:19, 19875.00it/s] 15%|█▌        | 69347/450337 [00:03<00:19, 19870.13it/s] 16%|█▌        | 71368/450337 [00:03<00:18, 19970.18it/s] 16%|█▋        | 73550/450337 [00:03<00:18, 20518.56it/s] 17%|█▋        | 75667/450337 [00:03<00:18, 20698.30it/s] 17%|█▋        | 77746/450337 [00:03<00:17, 20723.95it/s] 18%|█▊        | 79820/450337 [00:04<00:18, 19752.55it/s] 18%|█▊        | 81853/450337 [00:04<00:18, 19909.51it/s] 19%|█▊        | 83943/450337 [00:04<00:18, 20198.67it/s] 19%|█▉        | 85969/450337 [00:04<00:18, 20077.84it/s] 20%|█▉        | 88081/450337 [00:04<00:17, 20378.99it/s] 20%|██        | 90123/450337 [00:04<00:18, 19485.34it/s] 20%|██        | 92180/450337 [00:04<00:18, 19788.80it/s] 21%|██        | 94168/450337 [00:04<00:17, 19789.57it/s] 21%|██▏       | 96248/450337 [00:04<00:17, 20085.63it/s] 22%|██▏       | 98366/450337 [00:04<00:17, 20398.85it/s] 22%|██▏       | 100410/450337 [00:05<00:17, 19797.82it/s] 23%|██▎       | 102406/450337 [00:05<00:17, 19840.04it/s] 23%|██▎       | 104395/450337 [00:05<00:17, 19797.77it/s] 24%|██▎       | 106378/450337 [00:05<00:17, 19617.17it/s] 24%|██▍       | 108342/450337 [00:05<00:17, 19491.16it/s] 24%|██▍       | 110293/450337 [00:05<00:17, 19351.91it/s] 25%|██▍       | 112341/450337 [00:05<00:17, 19678.15it/s] 25%|██▌       | 114311/450337 [00:05<00:17, 19582.18it/s] 26%|██▌       | 116271/450337 [00:05<00:17, 19251.31it/s] 26%|██▌       | 118198/450337 [00:05<00:17, 19194.37it/s] 27%|██▋       | 120296/450337 [00:06<00:16, 19715.71it/s] 27%|██▋       | 122270/450337 [00:06<00:16, 19534.37it/s] 28%|██▊       | 124225/450337 [00:06<00:16, 19429.44it/s] 28%|██▊       | 126259/450337 [00:06<00:16, 19697.90it/s] 28%|██▊       | 128243/450337 [00:06<00:16, 19736.34it/s] 29%|██▉       | 130218/450337 [00:06<00:16, 19535.51it/s] 29%|██▉       | 132173/450337 [00:06<00:16, 19057.83it/s] 30%|██▉       | 134315/450337 [00:06<00:16, 19742.82it/s] 30%|███       | 136499/450337 [00:06<00:15, 20356.48it/s] 31%|███       | 138621/450337 [00:06<00:15, 20603.16it/s] 31%|███       | 140685/450337 [00:07<00:15, 20332.13it/s] 32%|███▏      | 142721/450337 [00:07<00:15, 19884.20it/s] 32%|███▏      | 144873/450337 [00:07<00:15, 20357.86it/s] 33%|███▎      | 146913/450337 [00:07<00:15, 20179.96it/s] 33%|███▎      | 148934/450337 [00:07<00:15, 19851.27it/s] 34%|███▎      | 150922/450337 [00:07<00:15, 19481.19it/s] 34%|███▍      | 152873/450337 [00:07<00:15, 19466.48it/s] 34%|███▍      | 154835/450337 [00:07<00:15, 19506.16it/s] 35%|███▍      | 156903/450337 [00:07<00:14, 19851.98it/s] 35%|███▌      | 158967/450337 [00:08<00:14, 20079.73it/s] 36%|███▌      | 161013/450337 [00:08<00:14, 20191.18it/s] 36%|███▌      | 163036/450337 [00:08<00:14, 20201.99it/s] 37%|███▋      | 165057/450337 [00:08<00:14, 20051.52it/s] 37%|███▋      | 167193/450337 [00:08<00:13, 20440.03it/s] 38%|███▊      | 169238/450337 [00:08<00:14, 20040.24it/s] 38%|███▊      | 171245/450337 [00:08<00:14, 19540.96it/s] 38%|███▊      | 173203/450337 [00:08<00:14, 19475.34it/s] 39%|███▉      | 175206/450337 [00:08<00:14, 19434.35it/s] 39%|███▉      | 177152/450337 [00:08<00:14, 19330.65it/s] 40%|███▉      | 179087/450337 [00:09<00:14, 19124.22it/s] 40%|████      | 181001/450337 [00:09<00:14, 18998.89it/s] 41%|████      | 182902/450337 [00:09<00:14, 18998.15it/s] 41%|████      | 184803/450337 [00:09<00:14, 18764.23it/s] 41%|████▏     | 186680/450337 [00:09<00:14, 18574.55it/s] 42%|████▏     | 188747/450337 [00:09<00:13, 19188.65it/s] 42%|████▏     | 190717/450337 [00:09<00:13, 19335.86it/s] 43%|████▎     | 192707/450337 [00:09<00:13, 19501.27it/s] 43%|████▎     | 194666/450337 [00:09<00:13, 19526.86it/s] 44%|████▎     | 196656/450337 [00:09<00:12, 19637.04it/s] 44%|████▍     | 198830/450337 [00:10<00:12, 20265.16it/s] 45%|████▍     | 200858/450337 [00:10<00:12, 19840.31it/s] 45%|████▌     | 202845/450337 [00:10<00:12, 19729.79it/s] 45%|████▌     | 204835/450337 [00:10<00:12, 19775.05it/s] 46%|████▌     | 206814/450337 [00:10<00:12, 19739.71it/s] 46%|████▋     | 208789/450337 [00:10<00:12, 19231.35it/s] 47%|████▋     | 210833/450337 [00:10<00:12, 19580.27it/s] 47%|████▋     | 212877/450337 [00:10<00:11, 19826.12it/s] 48%|████▊     | 214863/450337 [00:10<00:12, 19246.05it/s] 48%|████▊     | 216793/450337 [00:10<00:12, 19107.83it/s] 49%|████▊     | 218815/450337 [00:11<00:11, 19427.01it/s] 49%|████▉     | 220762/450337 [00:11<00:11, 19437.89it/s] 49%|████▉     | 222709/450337 [00:11<00:11, 19185.15it/s] 50%|████▉     | 224640/450337 [00:11<00:11, 19217.24it/s] 50%|█████     | 226716/450337 [00:11<00:11, 19666.50it/s] 51%|█████     | 228685/450337 [00:11<00:11, 19415.02it/s] 51%|█████     | 230629/450337 [00:11<00:11, 19152.55it/s] 52%|█████▏    | 232546/450337 [00:11<00:11, 19104.85it/s] 52%|█████▏    | 234783/450337 [00:11<00:10, 20058.15it/s] 53%|█████▎    | 236792/450337 [00:11<00:10, 19769.20it/s] 53%|█████▎    | 238862/450337 [00:12<00:10, 20041.85it/s] 53%|█████▎    | 240921/450337 [00:12<00:10, 20199.92it/s] 54%|█████▍    | 242943/450337 [00:12<00:10, 20179.90it/s] 54%|█████▍    | 244963/450337 [00:12<00:10, 20095.10it/s] 55%|█████▍    | 247193/450337 [00:12<00:09, 20740.90it/s] 55%|█████▌    | 249269/450337 [00:12<00:09, 20658.35it/s] 56%|█████▌    | 251340/450337 [00:12<00:09, 20672.90it/s] 56%|█████▋    | 253408/450337 [00:12<00:09, 20385.04it/s] 57%|█████▋    | 255448/450337 [00:12<00:09, 20345.50it/s] 57%|█████▋    | 257600/450337 [00:13<00:09, 20693.28it/s] 58%|█████▊    | 259671/450337 [00:13<00:09, 19741.95it/s] 58%|█████▊    | 261774/450337 [00:13<00:09, 20110.15it/s] 59%|█████▊    | 263793/450337 [00:13<00:09, 19852.80it/s] 59%|█████▉    | 265785/450337 [00:13<00:09, 19445.27it/s] 59%|█████▉    | 267735/450337 [00:13<00:09, 19271.74it/s] 60%|█████▉    | 269738/450337 [00:13<00:09, 19491.52it/s] 60%|██████    | 271769/450337 [00:13<00:09, 19729.92it/s] 61%|██████    | 273977/450337 [00:13<00:08, 20417.66it/s] 61%|██████▏   | 276022/450337 [00:13<00:08, 19936.93it/s] 62%|██████▏   | 278020/450337 [00:14<00:08, 19661.57it/s] 62%|██████▏   | 279999/450337 [00:14<00:08, 19697.58it/s] 63%|██████▎   | 281974/450337 [00:14<00:08, 19709.88it/s] 63%|██████▎   | 283954/450337 [00:14<00:08, 19732.75it/s] 63%|██████▎   | 285954/450337 [00:14<00:08, 19811.72it/s] 64%|██████▍   | 287937/450337 [00:14<00:08, 19648.59it/s] 64%|██████▍   | 289903/450337 [00:14<00:08, 19508.37it/s] 65%|██████▍   | 291855/450337 [00:14<00:08, 19399.93it/s] 65%|██████▌   | 293796/450337 [00:14<00:08, 19376.64it/s] 66%|██████▌   | 295849/450337 [00:14<00:07, 19717.05it/s] 66%|██████▌   | 297887/450337 [00:15<00:07, 19902.31it/s] 67%|██████▋   | 299915/450337 [00:15<00:07, 20012.06it/s] 67%|██████▋   | 301917/450337 [00:15<00:07, 19342.23it/s] 68%|██████▊   | 304057/450337 [00:15<00:07, 19936.25it/s] 68%|██████▊   | 306056/450337 [00:15<00:07, 19766.40it/s] 68%|██████▊   | 308037/450337 [00:15<00:07, 19630.65it/s] 69%|██████▉   | 310144/450337 [00:15<00:06, 20050.98it/s] 69%|██████▉   | 312169/450337 [00:15<00:06, 20103.26it/s] 70%|██████▉   | 314312/450337 [00:15<00:06, 20492.43it/s] 70%|███████   | 316363/450337 [00:15<00:06, 19628.60it/s] 71%|███████   | 318335/450337 [00:16<00:06, 19563.63it/s] 71%|███████   | 320298/450337 [00:16<00:06, 19501.33it/s] 72%|███████▏  | 322253/450337 [00:16<00:06, 19455.72it/s] 72%|███████▏  | 324252/450337 [00:16<00:06, 19609.94it/s] 72%|███████▏  | 326216/450337 [00:16<00:06, 19273.48it/s] 73%|███████▎  | 328501/450337 [00:16<00:05, 20322.48it/s] 73%|███████▎  | 330538/450337 [00:16<00:06, 19601.85it/s] 74%|███████▍  | 332572/450337 [00:16<00:05, 19806.73it/s] 74%|███████▍  | 334559/450337 [00:16<00:05, 19498.64it/s] 75%|███████▍  | 336612/450337 [00:17<00:05, 19797.76it/s] 75%|███████▌  | 338702/450337 [00:17<00:05, 20120.03it/s] 76%|███████▌  | 340718/450337 [00:17<00:05, 19617.28it/s] 76%|███████▌  | 342728/450337 [00:17<00:05, 19755.83it/s] 77%|███████▋  | 344732/450337 [00:17<00:05, 19838.51it/s] 77%|███████▋  | 346719/450337 [00:17<00:05, 19587.09it/s] 77%|███████▋  | 348681/450337 [00:17<00:05, 19590.59it/s] 78%|███████▊  | 350833/450337 [00:17<00:04, 20152.72it/s] 78%|███████▊  | 352851/450337 [00:17<00:05, 19396.61it/s] 79%|███████▉  | 354892/450337 [00:17<00:04, 19689.76it/s] 79%|███████▉  | 356867/450337 [00:18<00:04, 19397.35it/s] 80%|███████▉  | 358812/450337 [00:18<00:04, 19297.77it/s] 80%|████████  | 360811/450337 [00:18<00:04, 19496.24it/s] 81%|████████  | 362982/450337 [00:18<00:04, 20147.09it/s] 81%|████████  | 365075/450337 [00:18<00:04, 20376.58it/s] 82%|████████▏ | 367115/450337 [00:18<00:04, 20294.51it/s] 82%|████████▏ | 369147/450337 [00:18<00:04, 19829.69it/s] 82%|████████▏ | 371134/450337 [00:18<00:04, 19668.99it/s] 83%|████████▎ | 373104/450337 [00:18<00:03, 19475.23it/s] 83%|████████▎ | 375149/450337 [00:18<00:03, 19757.80it/s] 84%|████████▎ | 377127/450337 [00:19<00:03, 19053.99it/s] 84%|████████▍ | 379120/450337 [00:19<00:03, 19306.74it/s] 85%|████████▍ | 381178/450337 [00:19<00:03, 19674.59it/s] 85%|████████▌ | 383150/450337 [00:19<00:03, 19046.83it/s] 86%|████████▌ | 385099/450337 [00:19<00:03, 19174.37it/s] 86%|████████▌ | 387022/450337 [00:19<00:03, 18860.66it/s] 86%|████████▋ | 388966/450337 [00:19<00:03, 19028.18it/s] 87%|████████▋ | 390950/450337 [00:19<00:03, 19265.67it/s] 87%|████████▋ | 392880/450337 [00:19<00:03, 18568.49it/s] 88%|████████▊ | 394776/450337 [00:20<00:02, 18677.47it/s] 88%|████████▊ | 396678/450337 [00:20<00:02, 18777.34it/s] 89%|████████▊ | 398656/450337 [00:20<00:02, 19070.93it/s] 89%|████████▉ | 400743/450337 [00:20<00:02, 19601.77it/s] 89%|████████▉ | 402707/450337 [00:20<00:02, 18784.75it/s] 90%|████████▉ | 404716/450337 [00:20<00:02, 19157.86it/s] 90%|█████████ | 406640/450337 [00:20<00:02, 18818.62it/s] 91%|█████████ | 408605/450337 [00:20<00:02, 19059.71it/s] 91%|█████████ | 410580/450337 [00:20<00:02, 19260.88it/s] 92%|█████████▏| 412510/450337 [00:20<00:02, 18797.57it/s] 92%|█████████▏| 414395/450337 [00:21<00:01, 18522.88it/s] 92%|█████████▏| 416251/450337 [00:21<00:01, 18270.60it/s] 93%|█████████▎| 418267/450337 [00:21<00:01, 18816.99it/s] 93%|█████████▎| 420211/450337 [00:21<00:01, 18994.70it/s] 94%|█████████▎| 422114/450337 [00:21<00:01, 18762.77it/s] 94%|█████████▍| 424111/450337 [00:21<00:01, 19113.68it/s] 95%|█████████▍| 426078/450337 [00:21<00:01, 19269.94it/s] 95%|█████████▌| 428112/450337 [00:21<00:01, 19585.49it/s] 96%|█████████▌| 430271/450337 [00:21<00:00, 20176.14it/s] 96%|█████████▌| 432291/450337 [00:21<00:00, 19642.84it/s] 96%|█████████▋| 434363/450337 [00:22<00:00, 19958.00it/s] 97%|█████████▋| 436363/450337 [00:22<00:00, 19727.30it/s] 97%|█████████▋| 438365/450337 [00:22<00:00, 19810.70it/s] 98%|█████████▊| 440349/450337 [00:22<00:00, 19410.65it/s] 98%|█████████▊| 442397/450337 [00:22<00:00, 19719.86it/s] 99%|█████████▊| 444382/450337 [00:22<00:00, 19757.85it/s] 99%|█████████▉| 446360/450337 [00:22<00:00, 19212.05it/s]100%|█████████▉| 448414/450337 [00:22<00:00, 19592.58it/s]100%|██████████| 450337/450337 [00:22<00:00, 19673.46it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00,  9.97it/s]100%|██████████| 1/1 [00:00<00:00,  9.94it/s]2022-03-03 20:39:39 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-03 20:39:39 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-03 20:39:39 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-03 20:39:39 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-03 20:39:39 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-03 20:39:39 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-03 20:39:39 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-03 20:39:39 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-03 20:39:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 20:39:39 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-03 20:39:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 20:39:39 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-03 20:39:39 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-03 20:39:39 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_last.pt
2022-03-03 20:39:39 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_last.pt
2022-03-03 20:39:39 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-03 20:39:39 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-03 20:39:39 | INFO | fairseq.trainer | begin training epoch 1
2022-03-03 20:39:39 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-03 20:39:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-03 20:39:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 20:40:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 20:40:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:40:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 20:47:44 | INFO | train_inner | epoch 001:    105 / 393 loss=17.015, ppl=132464, wps=14670.6, ups=0.22, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.574, loss_scale=4, train_wall=479, gb_free=10.1, wall=485
2022-03-03 20:55:10 | INFO | train_inner | epoch 001:    205 / 393 loss=14.497, ppl=23122.4, wps=14671.2, ups=0.22, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.635, loss_scale=4, train_wall=442, gb_free=10.1, wall=931
2022-03-03 21:02:37 | INFO | train_inner | epoch 001:    305 / 393 loss=12.373, ppl=5305.46, wps=14683.7, ups=0.22, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.114, loss_scale=4, train_wall=441, gb_free=10.1, wall=1378
2022-03-03 21:09:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:09:14 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.499 | ppl 1447.61 | wps 33163.2 | wpb 2034.1 | bsz 4 | num_updates 388
2022-03-03 21:09:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-03 21:09:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-03 21:09:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-03 21:09:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 1 @ 388 updates, score 10.499) (writing took 4.41420784406364 seconds)
2022-03-03 21:09:18 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-03 21:09:18 | INFO | train | epoch 001 | loss 13.789 | ppl 14153.6 | wps 14583.9 | ups 0.22 | wpb 65460.6 | bsz 127.9 | num_updates 388 | lr 4.85903e-05 | gnorm 1.772 | loss_scale 4 | train_wall 1749 | gb_free 10.1 | wall 1779
2022-03-03 21:09:18 | INFO | fairseq.trainer | begin training epoch 2
2022-03-03 21:09:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:10:12 | INFO | train_inner | epoch 002:     12 / 393 loss=10.862, ppl=1861.08, wps=14323.8, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.613, loss_scale=4, train_wall=440, gb_free=10.1, wall=1833
2022-03-03 21:17:38 | INFO | train_inner | epoch 002:    112 / 393 loss=10.269, ppl=1234.18, wps=14682.7, ups=0.22, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.451, loss_scale=4, train_wall=441, gb_free=10.1, wall=2279
2022-03-03 21:25:05 | INFO | train_inner | epoch 002:    212 / 393 loss=9.982, ppl=1011.31, wps=14686, ups=0.22, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.524, loss_scale=8, train_wall=441, gb_free=10.1, wall=2726
2022-03-03 21:32:31 | INFO | train_inner | epoch 002:    312 / 393 loss=9.758, ppl=865.6, wps=14685.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.547, loss_scale=8, train_wall=441, gb_free=10.1, wall=3172
2022-03-03 21:38:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:38:37 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.458 | ppl 703.11 | wps 33419.9 | wpb 2034.1 | bsz 4 | num_updates 781 | best_loss 9.458
2022-03-03 21:38:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 781 updates
2022-03-03 21:38:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-03 21:38:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-03 21:38:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 2 @ 781 updates, score 9.458) (writing took 4.392232791520655 seconds)
2022-03-03 21:38:42 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-03 21:38:42 | INFO | train | epoch 002 | loss 9.929 | ppl 974.59 | wps 14591.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 0.531 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 3543
2022-03-03 21:38:42 | INFO | fairseq.trainer | begin training epoch 3
2022-03-03 21:38:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:40:06 | INFO | train_inner | epoch 003:     19 / 393 loss=9.543, ppl=746.21, wps=14325.3, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.634, loss_scale=8, train_wall=440, gb_free=10.1, wall=3627
2022-03-03 21:47:33 | INFO | train_inner | epoch 003:    119 / 393 loss=9.333, ppl=645.03, wps=14685.1, ups=0.22, wpb=65530.2, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.707, loss_scale=8, train_wall=441, gb_free=10.1, wall=4074
2022-03-03 21:54:59 | INFO | train_inner | epoch 003:    219 / 393 loss=9.169, ppl=575.79, wps=14684.9, ups=0.22, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.782, loss_scale=8, train_wall=441, gb_free=10.1, wall=4520
2022-03-03 22:02:25 | INFO | train_inner | epoch 003:    319 / 393 loss=9.02, ppl=519.23, wps=14680.9, ups=0.22, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.795, loss_scale=16, train_wall=442, gb_free=10.1, wall=4966
2022-03-03 22:07:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:08:00 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.819 | ppl 451.54 | wps 33569 | wpb 2034.1 | bsz 4 | num_updates 1174 | best_loss 8.819
2022-03-03 22:08:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1174 updates
2022-03-03 22:08:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-03 22:08:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-03 22:08:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 3 @ 1174 updates, score 8.819) (writing took 4.358379683457315 seconds)
2022-03-03 22:08:05 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-03 22:08:05 | INFO | train | epoch 003 | loss 9.135 | ppl 562.22 | wps 14591.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1174 | lr 0.000146821 | gnorm 0.762 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 5306
2022-03-03 22:08:05 | INFO | fairseq.trainer | begin training epoch 4
2022-03-03 22:08:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:10:01 | INFO | train_inner | epoch 004:     26 / 393 loss=8.869, ppl=467.58, wps=14329.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.791, loss_scale=16, train_wall=440, gb_free=10.1, wall=5422
2022-03-03 22:17:27 | INFO | train_inner | epoch 004:    126 / 393 loss=8.716, ppl=420.43, wps=14682.2, ups=0.22, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.793, loss_scale=16, train_wall=441, gb_free=10.1, wall=5868
2022-03-03 22:24:54 | INFO | train_inner | epoch 004:    226 / 393 loss=8.609, ppl=390.37, wps=14679.2, ups=0.22, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.821, loss_scale=16, train_wall=442, gb_free=10.1, wall=6315
2022-03-03 22:32:20 | INFO | train_inner | epoch 004:    326 / 393 loss=8.498, ppl=361.58, wps=14679.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.839, loss_scale=16, train_wall=441, gb_free=10.1, wall=6761
2022-03-03 22:37:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:37:24 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.36 | ppl 328.57 | wps 33668 | wpb 2034.1 | bsz 4 | num_updates 1567 | best_loss 8.36
2022-03-03 22:37:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1567 updates
2022-03-03 22:37:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-03 22:37:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-03 22:37:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 4 @ 1567 updates, score 8.36) (writing took 4.558433969505131 seconds)
2022-03-03 22:37:28 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-03 22:37:28 | INFO | train | epoch 004 | loss 8.586 | ppl 384.26 | wps 14588.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1567 | lr 0.000195936 | gnorm 0.817 | loss_scale 32 | train_wall 1733 | gb_free 10.1 | wall 7069
2022-03-03 22:37:28 | INFO | fairseq.trainer | begin training epoch 5
2022-03-03 22:37:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:39:55 | INFO | train_inner | epoch 005:     33 / 393 loss=8.375, ppl=331.98, wps=14324.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.814, loss_scale=32, train_wall=440, gb_free=10.1, wall=7216
2022-03-03 22:47:22 | INFO | train_inner | epoch 005:    133 / 393 loss=8.248, ppl=303.98, wps=14684.3, ups=0.22, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.813, loss_scale=32, train_wall=441, gb_free=10.1, wall=7663
2022-03-03 22:54:48 | INFO | train_inner | epoch 005:    233 / 393 loss=8.165, ppl=287.04, wps=14682.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.828, loss_scale=32, train_wall=441, gb_free=10.1, wall=8109
2022-03-03 23:02:14 | INFO | train_inner | epoch 005:    333 / 393 loss=8.083, ppl=271.25, wps=14685.4, ups=0.22, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.807, loss_scale=32, train_wall=441, gb_free=10.1, wall=8555
2022-03-03 23:06:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:06:47 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.008 | ppl 257.47 | wps 33630.6 | wpb 2034.1 | bsz 4 | num_updates 1960 | best_loss 8.008
2022-03-03 23:06:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1960 updates
2022-03-03 23:06:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-03 23:06:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-03 23:06:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 5 @ 1960 updates, score 8.008) (writing took 4.626609423197806 seconds)
2022-03-03 23:06:51 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-03 23:06:51 | INFO | train | epoch 005 | loss 8.154 | ppl 284.9 | wps 14591 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1960 | lr 0.000245051 | gnorm 0.812 | loss_scale 32 | train_wall 1733 | gb_free 10.1 | wall 8832
2022-03-03 23:06:51 | INFO | fairseq.trainer | begin training epoch 6
2022-03-03 23:06:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:09:50 | INFO | train_inner | epoch 006:     40 / 393 loss=7.965, ppl=249.82, wps=14321.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.795, loss_scale=32, train_wall=440, gb_free=10.1, wall=9011
2022-03-03 23:14:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 23:17:21 | INFO | train_inner | epoch 006:    141 / 393 loss=7.866, ppl=233.33, wps=14535.1, ups=0.22, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.767, loss_scale=32, train_wall=446, gb_free=10.1, wall=9462
2022-03-03 23:22:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 23:24:52 | INFO | train_inner | epoch 006:    242 / 393 loss=7.793, ppl=221.74, wps=14541.1, ups=0.22, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.762, loss_scale=16, train_wall=446, gb_free=10.1, wall=9913
2022-03-03 23:32:18 | INFO | train_inner | epoch 006:    342 / 393 loss=7.738, ppl=213.49, wps=14681, ups=0.22, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.771, loss_scale=16, train_wall=441, gb_free=10.1, wall=10359
2022-03-03 23:36:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:36:10 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.737 | ppl 213.38 | wps 33650.3 | wpb 2034.1 | bsz 4 | num_updates 2351 | best_loss 7.737
2022-03-03 23:36:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2351 updates
2022-03-03 23:36:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-03 23:36:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-03 23:36:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 6 @ 2351 updates, score 7.737) (writing took 4.52438076864928 seconds)
2022-03-03 23:36:15 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-03 23:36:15 | INFO | train | epoch 006 | loss 7.794 | ppl 221.94 | wps 14515.3 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 2351 | lr 0.000293916 | gnorm 0.762 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 10596
2022-03-03 23:36:15 | INFO | fairseq.trainer | begin training epoch 7
2022-03-03 23:36:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:39:53 | INFO | train_inner | epoch 007:     49 / 393 loss=7.628, ppl=197.77, wps=14323.1, ups=0.22, wpb=65238.4, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.738, loss_scale=16, train_wall=439, gb_free=10.1, wall=10814
2022-03-03 23:47:20 | INFO | train_inner | epoch 007:    149 / 393 loss=7.535, ppl=185.5, wps=14685.1, ups=0.22, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.736, loss_scale=16, train_wall=441, gb_free=10.1, wall=11261
2022-03-03 23:54:46 | INFO | train_inner | epoch 007:    249 / 393 loss=7.489, ppl=179.67, wps=14684.3, ups=0.22, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.739, loss_scale=16, train_wall=441, gb_free=10.1, wall=11707
2022-03-04 00:02:12 | INFO | train_inner | epoch 007:    349 / 393 loss=7.443, ppl=174.02, wps=14681.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.716, loss_scale=32, train_wall=441, gb_free=10.1, wall=12153
2022-03-04 00:05:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:05:33 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.513 | ppl 182.71 | wps 33593.1 | wpb 2034.1 | bsz 4 | num_updates 2744 | best_loss 7.513
2022-03-04 00:05:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2744 updates
2022-03-04 00:05:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 00:05:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 00:05:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 7 @ 2744 updates, score 7.513) (writing took 4.549623498693109 seconds)
2022-03-04 00:05:38 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 00:05:38 | INFO | train | epoch 007 | loss 7.488 | ppl 179.58 | wps 14590.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 2744 | lr 0.000343031 | gnorm 0.733 | loss_scale 32 | train_wall 1733 | gb_free 10.1 | wall 12359
2022-03-04 00:05:38 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 00:05:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:09:48 | INFO | train_inner | epoch 008:     56 / 393 loss=7.323, ppl=160.16, wps=14323.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.721, loss_scale=32, train_wall=440, gb_free=10.1, wall=12609
2022-03-04 00:17:14 | INFO | train_inner | epoch 008:    156 / 393 loss=7.256, ppl=152.81, wps=14683.1, ups=0.22, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.711, loss_scale=32, train_wall=441, gb_free=10.1, wall=13055
2022-03-04 00:24:40 | INFO | train_inner | epoch 008:    256 / 393 loss=7.232, ppl=150.35, wps=14684.7, ups=0.22, wpb=65530.2, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.682, loss_scale=32, train_wall=441, gb_free=10.1, wall=13502
2022-03-04 00:25:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 00:32:11 | INFO | train_inner | epoch 008:    357 / 393 loss=7.205, ppl=147.56, wps=14540.5, ups=0.22, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.699, loss_scale=16, train_wall=446, gb_free=10.1, wall=13952
2022-03-04 00:34:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:34:56 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.347 | ppl 162.84 | wps 33584 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 7.347
2022-03-04 00:34:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3136 updates
2022-03-04 00:34:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 00:35:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 00:35:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 8 @ 3136 updates, score 7.347) (writing took 4.4967862563207746 seconds)
2022-03-04 00:35:01 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 00:35:01 | INFO | train | epoch 008 | loss 7.232 | ppl 150.29 | wps 14554.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3136 | lr 0.000392022 | gnorm 0.705 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 14122
2022-03-04 00:35:01 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 00:35:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:39:47 | INFO | train_inner | epoch 009:     64 / 393 loss=7.09, ppl=136.25, wps=14326.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.7, loss_scale=16, train_wall=439, gb_free=10.1, wall=14408
2022-03-04 00:47:13 | INFO | train_inner | epoch 009:    164 / 393 loss=7.03, ppl=130.66, wps=14682.3, ups=0.22, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.682, loss_scale=16, train_wall=441, gb_free=10.1, wall=14854
2022-03-04 00:54:39 | INFO | train_inner | epoch 009:    264 / 393 loss=7.014, ppl=129.23, wps=14686.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.675, loss_scale=16, train_wall=441, gb_free=10.1, wall=15300
2022-03-04 01:02:06 | INFO | train_inner | epoch 009:    364 / 393 loss=7, ppl=128.02, wps=14676.1, ups=0.22, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.675, loss_scale=16, train_wall=442, gb_free=10.1, wall=15747
2022-03-04 01:04:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:04:20 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.223 | ppl 149.37 | wps 33632.4 | wpb 2034.1 | bsz 4 | num_updates 3529 | best_loss 7.223
2022-03-04 01:04:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3529 updates
2022-03-04 01:04:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 01:04:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 01:04:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 9 @ 3529 updates, score 7.223) (writing took 4.473246512003243 seconds)
2022-03-04 01:04:24 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 01:04:24 | INFO | train | epoch 009 | loss 7.016 | ppl 129.41 | wps 14590.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 3529 | lr 0.000441137 | gnorm 0.674 | loss_scale 32 | train_wall 1733 | gb_free 10.1 | wall 15885
2022-03-04 01:04:24 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 01:04:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:04:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 01:09:46 | INFO | train_inner | epoch 010:     72 / 393 loss=6.878, ppl=117.65, wps=14189.2, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.673, loss_scale=16, train_wall=444, gb_free=10.1, wall=16207
2022-03-04 01:17:12 | INFO | train_inner | epoch 010:    172 / 393 loss=6.842, ppl=114.74, wps=14685.1, ups=0.22, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.652, loss_scale=16, train_wall=441, gb_free=10.1, wall=16653
2022-03-04 01:24:38 | INFO | train_inner | epoch 010:    272 / 393 loss=6.835, ppl=114.18, wps=14684.6, ups=0.22, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.709, loss_scale=16, train_wall=441, gb_free=10.1, wall=17099
2022-03-04 01:32:04 | INFO | train_inner | epoch 010:    372 / 393 loss=6.828, ppl=113.65, wps=14687.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.617, loss_scale=16, train_wall=441, gb_free=10.1, wall=17545
2022-03-04 01:33:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:33:43 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.142 | ppl 141.24 | wps 33631.1 | wpb 2034.1 | bsz 4 | num_updates 3921 | best_loss 7.142
2022-03-04 01:33:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3921 updates
2022-03-04 01:33:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 01:33:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 01:33:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 10 @ 3921 updates, score 7.142) (writing took 4.492722067981958 seconds)
2022-03-04 01:33:47 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 01:33:47 | INFO | train | epoch 010 | loss 6.835 | ppl 114.18 | wps 14555.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3921 | lr 0.000490127 | gnorm 0.659 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 17648
2022-03-04 01:33:47 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 01:33:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:39:40 | INFO | train_inner | epoch 011:     79 / 393 loss=6.702, ppl=104.08, wps=14321.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.637, loss_scale=16, train_wall=440, gb_free=10.1, wall=18001
2022-03-04 01:43:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 01:47:11 | INFO | train_inner | epoch 011:    180 / 393 loss=6.667, ppl=101.65, wps=14537.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.627, loss_scale=16, train_wall=446, gb_free=10.1, wall=18452
2022-03-04 01:54:37 | INFO | train_inner | epoch 011:    280 / 393 loss=6.674, ppl=102.08, wps=14683.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.609, loss_scale=16, train_wall=441, gb_free=10.1, wall=18898
2022-03-04 02:02:03 | INFO | train_inner | epoch 011:    380 / 393 loss=6.679, ppl=102.48, wps=14684.2, ups=0.22, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.613, loss_scale=16, train_wall=441, gb_free=10.1, wall=19344
2022-03-04 02:02:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:03:06 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.047 | ppl 132.24 | wps 33706 | wpb 2034.1 | bsz 4 | num_updates 4313 | best_loss 7.047
2022-03-04 02:03:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4313 updates
2022-03-04 02:03:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 02:03:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 02:03:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 11 @ 4313 updates, score 7.047) (writing took 4.435142632573843 seconds)
2022-03-04 02:03:10 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 02:03:10 | INFO | train | epoch 011 | loss 6.673 | ppl 102.02 | wps 14553.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 4313 | lr 0.000481515 | gnorm 0.627 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 19411
2022-03-04 02:03:10 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 02:03:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:09:39 | INFO | train_inner | epoch 012:     87 / 393 loss=6.509, ppl=91.07, wps=14321.8, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.598, loss_scale=16, train_wall=440, gb_free=10.1, wall=19800
2022-03-04 02:17:05 | INFO | train_inner | epoch 012:    187 / 393 loss=6.511, ppl=91.2, wps=14684, ups=0.22, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.579, loss_scale=16, train_wall=441, gb_free=10.1, wall=20246
2022-03-04 02:22:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 02:24:36 | INFO | train_inner | epoch 012:    288 / 393 loss=6.534, ppl=92.66, wps=14538.3, ups=0.22, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.587, loss_scale=16, train_wall=446, gb_free=10.1, wall=20697
2022-03-04 02:25:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 02:32:07 | INFO | train_inner | epoch 012:    389 / 393 loss=6.539, ppl=93.02, wps=14538.2, ups=0.22, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.594, loss_scale=8, train_wall=446, gb_free=10.1, wall=21148
2022-03-04 02:32:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:32:29 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.995 | ppl 127.55 | wps 33741.6 | wpb 2034.1 | bsz 4 | num_updates 4704 | best_loss 6.995
2022-03-04 02:32:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4704 updates
2022-03-04 02:32:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 02:32:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 02:32:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 12 @ 4704 updates, score 6.995) (writing took 4.405526912771165 seconds)
2022-03-04 02:32:34 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 02:32:34 | INFO | train | epoch 012 | loss 6.519 | ppl 91.71 | wps 14516 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 4704 | lr 0.000461069 | gnorm 0.587 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 21175
2022-03-04 02:32:34 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 02:32:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:39:42 | INFO | train_inner | epoch 013:     96 / 393 loss=6.365, ppl=82.41, wps=14332.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.591, loss_scale=8, train_wall=439, gb_free=10.1, wall=21603
2022-03-04 02:47:08 | INFO | train_inner | epoch 013:    196 / 393 loss=6.389, ppl=83.84, wps=14688.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.57, loss_scale=8, train_wall=441, gb_free=10.1, wall=22049
2022-03-04 02:54:34 | INFO | train_inner | epoch 013:    296 / 393 loss=6.4, ppl=84.42, wps=14688, ups=0.22, wpb=65535.4, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.575, loss_scale=8, train_wall=441, gb_free=10.1, wall=22495
2022-03-04 03:01:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:01:52 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.951 | ppl 123.72 | wps 33702.7 | wpb 2034.1 | bsz 4 | num_updates 5097 | best_loss 6.951
2022-03-04 03:01:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5097 updates
2022-03-04 03:01:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 03:01:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 03:01:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 13 @ 5097 updates, score 6.951) (writing took 4.478983947075903 seconds)
2022-03-04 03:01:56 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 03:01:56 | INFO | train | epoch 013 | loss 6.391 | ppl 83.9 | wps 14595.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 5097 | lr 0.000442938 | gnorm 0.576 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 22937
2022-03-04 03:01:56 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 03:01:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:02:10 | INFO | train_inner | epoch 014:      3 / 393 loss=6.409, ppl=84.95, wps=14330.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.566, loss_scale=8, train_wall=439, gb_free=10.1, wall=22951
2022-03-04 03:09:36 | INFO | train_inner | epoch 014:    103 / 393 loss=6.252, ppl=76.22, wps=14679.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.561, loss_scale=16, train_wall=442, gb_free=10.1, wall=23397
2022-03-04 03:17:02 | INFO | train_inner | epoch 014:    203 / 393 loss=6.275, ppl=77.42, wps=14682, ups=0.22, wpb=65530.9, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.574, loss_scale=16, train_wall=441, gb_free=10.1, wall=23843
2022-03-04 03:24:29 | INFO | train_inner | epoch 014:    303 / 393 loss=6.291, ppl=78.29, wps=14682.4, ups=0.22, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.556, loss_scale=16, train_wall=441, gb_free=10.1, wall=24290
2022-03-04 03:31:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:31:15 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.926 | ppl 121.6 | wps 33631.9 | wpb 2034.1 | bsz 4 | num_updates 5490 | best_loss 6.926
2022-03-04 03:31:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5490 updates
2022-03-04 03:31:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 03:31:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 03:31:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 14 @ 5490 updates, score 6.926) (writing took 4.132393008098006 seconds)
2022-03-04 03:31:19 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 03:31:19 | INFO | train | epoch 014 | loss 6.281 | ppl 77.74 | wps 14591 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 5490 | lr 0.00042679 | gnorm 0.559 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 24700
2022-03-04 03:31:19 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 03:31:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:32:04 | INFO | train_inner | epoch 015:     10 / 393 loss=6.291, ppl=78.31, wps=14332.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.553, loss_scale=16, train_wall=440, gb_free=10.1, wall=24745
2022-03-04 03:39:30 | INFO | train_inner | epoch 015:    110 / 393 loss=6.151, ppl=71.05, wps=14681.1, ups=0.22, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.578, loss_scale=16, train_wall=442, gb_free=10.1, wall=25192
2022-03-04 03:41:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 03:44:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 03:47:05 | INFO | train_inner | epoch 015:    212 / 393 loss=6.178, ppl=72.4, wps=14401.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.547, loss_scale=8, train_wall=450, gb_free=10.1, wall=25647
2022-03-04 03:54:32 | INFO | train_inner | epoch 015:    312 / 393 loss=6.2, ppl=73.5, wps=14686.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.557, loss_scale=8, train_wall=441, gb_free=10.1, wall=26093
2022-03-04 04:00:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:00:38 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.942 | ppl 122.92 | wps 33770.3 | wpb 2034.1 | bsz 4 | num_updates 5881 | best_loss 6.926
2022-03-04 04:00:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5881 updates
2022-03-04 04:00:38 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 04:00:38 | INFO | train | epoch 015 | loss 6.184 | ppl 72.7 | wps 14557 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 5881 | lr 0.000412358 | gnorm 0.559 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 26459
2022-03-04 04:00:38 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 04:00:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:02:03 | INFO | train_inner | epoch 016:     19 / 393 loss=6.185, ppl=72.75, wps=14474.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.554, loss_scale=8, train_wall=439, gb_free=10.1, wall=26544
2022-03-04 04:09:29 | INFO | train_inner | epoch 016:    119 / 393 loss=6.063, ppl=66.87, wps=14685.5, ups=0.22, wpb=65530.2, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.567, loss_scale=8, train_wall=441, gb_free=10.1, wall=26990
2022-03-04 04:16:55 | INFO | train_inner | epoch 016:    219 / 393 loss=6.098, ppl=68.48, wps=14681.7, ups=0.22, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.557, loss_scale=8, train_wall=441, gb_free=10.1, wall=27436
2022-03-04 04:24:21 | INFO | train_inner | epoch 016:    319 / 393 loss=6.121, ppl=69.58, wps=14682.5, ups=0.22, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.59, loss_scale=16, train_wall=441, gb_free=10.1, wall=27883
2022-03-04 04:25:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 04:29:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:29:56 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.925 | ppl 121.48 | wps 33879.2 | wpb 2034.1 | bsz 4 | num_updates 6273 | best_loss 6.925
2022-03-04 04:29:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6273 updates
2022-03-04 04:29:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 04:30:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 04:30:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 16 @ 6273 updates, score 6.925) (writing took 4.239850669167936 seconds)
2022-03-04 04:30:01 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 04:30:01 | INFO | train | epoch 016 | loss 6.1 | ppl 68.61 | wps 14556.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6273 | lr 0.000399266 | gnorm 0.57 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 28222
2022-03-04 04:30:01 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 04:30:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:32:01 | INFO | train_inner | epoch 017:     27 / 393 loss=6.09, ppl=68.13, wps=14194.7, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.565, loss_scale=8, train_wall=444, gb_free=10.1, wall=28342
2022-03-04 04:39:27 | INFO | train_inner | epoch 017:    127 / 393 loss=5.993, ppl=63.7, wps=14684.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.556, loss_scale=8, train_wall=441, gb_free=10.1, wall=28788
2022-03-04 04:46:54 | INFO | train_inner | epoch 017:    227 / 393 loss=6.015, ppl=64.67, wps=14685.1, ups=0.22, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.557, loss_scale=8, train_wall=441, gb_free=10.1, wall=29235
2022-03-04 04:54:20 | INFO | train_inner | epoch 017:    327 / 393 loss=6.05, ppl=66.27, wps=14683.6, ups=0.22, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.568, loss_scale=8, train_wall=441, gb_free=10.1, wall=29681
2022-03-04 04:59:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:59:19 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.921 | ppl 121.19 | wps 33921.3 | wpb 2034.1 | bsz 4 | num_updates 6666 | best_loss 6.921
2022-03-04 04:59:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6666 updates
2022-03-04 04:59:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 04:59:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt
2022-03-04 04:59:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#2/checkpoint_best.pt (epoch 17 @ 6666 updates, score 6.921) (writing took 4.241985118016601 seconds)
2022-03-04 04:59:23 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 04:59:23 | INFO | train | epoch 017 | loss 6.024 | ppl 65.08 | wps 14594.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 6666 | lr 0.000387318 | gnorm 0.562 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 29984
2022-03-04 04:59:23 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 04:59:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:01:55 | INFO | train_inner | epoch 018:     34 / 393 loss=6.014, ppl=64.61, wps=14337.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.591, loss_scale=8, train_wall=439, gb_free=10.1, wall=30136
2022-03-04 05:09:22 | INFO | train_inner | epoch 018:    134 / 393 loss=5.919, ppl=60.51, wps=14680, ups=0.22, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.552, loss_scale=16, train_wall=442, gb_free=10.1, wall=30583
2022-03-04 05:12:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 05:16:52 | INFO | train_inner | epoch 018:    235 / 393 loss=5.955, ppl=62.02, wps=14541.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.558, loss_scale=8, train_wall=446, gb_free=10.1, wall=31033
2022-03-04 05:24:18 | INFO | train_inner | epoch 018:    335 / 393 loss=5.988, ppl=63.46, wps=14684.6, ups=0.22, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.589, loss_scale=8, train_wall=441, gb_free=10.1, wall=31480
2022-03-04 05:28:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:28:42 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.933 | ppl 122.16 | wps 33984.6 | wpb 2034.1 | bsz 4 | num_updates 7058 | best_loss 6.921
2022-03-04 05:28:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7058 updates
2022-03-04 05:28:42 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 05:28:42 | INFO | train | epoch 018 | loss 5.955 | ppl 62.04 | wps 14591.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7058 | lr 0.000376408 | gnorm 0.575 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 31743
2022-03-04 05:28:42 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 05:28:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:31:49 | INFO | train_inner | epoch 019:     42 / 393 loss=5.926, ppl=60.81, wps=14471.9, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.578, loss_scale=8, train_wall=439, gb_free=10.1, wall=31930
2022-03-04 05:39:16 | INFO | train_inner | epoch 019:    142 / 393 loss=5.853, ppl=57.81, wps=14686.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.558, loss_scale=8, train_wall=441, gb_free=10.1, wall=32377
2022-03-04 05:46:42 | INFO | train_inner | epoch 019:    242 / 393 loss=5.902, ppl=59.79, wps=14681.1, ups=0.22, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.566, loss_scale=8, train_wall=441, gb_free=10.1, wall=32823
2022-03-04 05:54:08 | INFO | train_inner | epoch 019:    342 / 393 loss=5.927, ppl=60.84, wps=14685.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.578, loss_scale=16, train_wall=441, gb_free=10.1, wall=33269
2022-03-04 05:56:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 05:57:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:58:00 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.963 | ppl 124.78 | wps 33959.7 | wpb 2034.1 | bsz 4 | num_updates 7450 | best_loss 6.921
2022-03-04 05:58:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7450 updates
2022-03-04 05:58:00 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 05:58:00 | INFO | train | epoch 019 | loss 5.892 | ppl 59.39 | wps 14593.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7450 | lr 0.000366372 | gnorm 0.569 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 33501
2022-03-04 05:58:00 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 05:58:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:01:43 | INFO | train_inner | epoch 020:     50 / 393 loss=5.846, ppl=57.52, wps=14331.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.572, loss_scale=8, train_wall=444, gb_free=10.1, wall=33725
2022-03-04 06:09:10 | INFO | train_inner | epoch 020:    150 / 393 loss=5.807, ppl=56, wps=14687.3, ups=0.22, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.579, loss_scale=8, train_wall=441, gb_free=10.1, wall=34171
2022-03-04 06:16:36 | INFO | train_inner | epoch 020:    250 / 393 loss=5.838, ppl=57.21, wps=14684.9, ups=0.22, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.562, loss_scale=8, train_wall=441, gb_free=10.1, wall=34617
2022-03-04 06:24:02 | INFO | train_inner | epoch 020:    350 / 393 loss=5.874, ppl=58.67, wps=14685, ups=0.22, wpb=65530.9, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.584, loss_scale=8, train_wall=441, gb_free=10.1, wall=35063
2022-03-04 06:27:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:27:19 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.954 | ppl 124 | wps 33929.7 | wpb 2034.1 | bsz 4 | num_updates 7843 | best_loss 6.921
2022-03-04 06:27:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7843 updates
2022-03-04 06:27:19 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 06:27:19 | INFO | train | epoch 020 | loss 5.834 | ppl 57.05 | wps 14630.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 7843 | lr 0.000357075 | gnorm 0.57 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 35260
2022-03-04 06:27:19 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 06:27:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:31:33 | INFO | train_inner | epoch 021:     57 / 393 loss=5.782, ppl=55.04, wps=14471.7, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.556, loss_scale=8, train_wall=439, gb_free=10.1, wall=35514
2022-03-04 06:36:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 06:39:04 | INFO | train_inner | epoch 021:    158 / 393 loss=5.755, ppl=54, wps=14542.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.581, loss_scale=8, train_wall=446, gb_free=10.1, wall=35965
2022-03-04 06:46:30 | INFO | train_inner | epoch 021:    258 / 393 loss=5.781, ppl=54.99, wps=14685.1, ups=0.22, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.575, loss_scale=8, train_wall=441, gb_free=10.1, wall=36411
2022-03-04 06:53:56 | INFO | train_inner | epoch 021:    358 / 393 loss=5.828, ppl=56.79, wps=14688, ups=0.22, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.568, loss_scale=8, train_wall=441, gb_free=10.1, wall=36857
2022-03-04 06:56:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:56:37 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.981 | ppl 126.33 | wps 33929.6 | wpb 2034.1 | bsz 4 | num_updates 8235 | best_loss 6.921
2022-03-04 06:56:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8235 updates
2022-03-04 06:56:37 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 06:56:37 | INFO | train | epoch 021 | loss 5.781 | ppl 54.98 | wps 14594.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8235 | lr 0.000348472 | gnorm 0.579 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 37018
2022-03-04 06:56:37 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 06:56:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:01:27 | INFO | train_inner | epoch 022:     65 / 393 loss=5.73, ppl=53.06, wps=14473, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.612, loss_scale=8, train_wall=439, gb_free=10.1, wall=37308
2022-03-04 07:08:53 | INFO | train_inner | epoch 022:    165 / 393 loss=5.709, ppl=52.3, wps=14685.3, ups=0.22, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.598, loss_scale=8, train_wall=441, gb_free=10.1, wall=37754
2022-03-04 07:16:19 | INFO | train_inner | epoch 022:    265 / 393 loss=5.739, ppl=53.42, wps=14686.9, ups=0.22, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.573, loss_scale=16, train_wall=441, gb_free=10.1, wall=38201
2022-03-04 07:17:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 07:23:50 | INFO | train_inner | epoch 022:    366 / 393 loss=5.772, ppl=54.64, wps=14543.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.601, loss_scale=8, train_wall=446, gb_free=10.1, wall=38651
2022-03-04 07:25:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:25:55 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.027 | ppl 130.4 | wps 33950.8 | wpb 2034.1 | bsz 4 | num_updates 8627 | best_loss 6.921
2022-03-04 07:25:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8627 updates
2022-03-04 07:25:55 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 07:25:55 | INFO | train | epoch 022 | loss 5.731 | ppl 53.1 | wps 14595 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8627 | lr 0.000340463 | gnorm 0.589 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 38776
2022-03-04 07:25:55 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 07:25:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:31:21 | INFO | train_inner | epoch 023:     73 / 393 loss=5.664, ppl=50.7, wps=14476.9, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.584, loss_scale=8, train_wall=439, gb_free=10.1, wall=39102
2022-03-04 07:38:47 | INFO | train_inner | epoch 023:    173 / 393 loss=5.656, ppl=50.41, wps=14686.6, ups=0.22, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.573, loss_scale=8, train_wall=441, gb_free=10.1, wall=39548
2022-03-04 07:46:13 | INFO | train_inner | epoch 023:    273 / 393 loss=5.701, ppl=52.02, wps=14687.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.618, loss_scale=8, train_wall=441, gb_free=10.1, wall=39994
2022-03-04 07:53:40 | INFO | train_inner | epoch 023:    373 / 393 loss=5.732, ppl=53.15, wps=14681.4, ups=0.22, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.576, loss_scale=8, train_wall=442, gb_free=10.1, wall=40441
2022-03-04 07:55:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:55:13 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.042 | ppl 131.81 | wps 34111.8 | wpb 2034.1 | bsz 4 | num_updates 9020 | best_loss 6.921
2022-03-04 07:55:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9020 updates
2022-03-04 07:55:13 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 07:55:13 | INFO | train | epoch 023 | loss 5.685 | ppl 51.44 | wps 14632 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9020 | lr 0.000332964 | gnorm 0.59 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 40534
2022-03-04 07:55:13 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 07:55:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:56:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 08:01:15 | INFO | train_inner | epoch 024:     81 / 393 loss=5.615, ppl=48.99, wps=14332.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.594, loss_scale=8, train_wall=444, gb_free=10.1, wall=40896
2022-03-04 08:08:41 | INFO | train_inner | epoch 024:    181 / 393 loss=5.62, ppl=49.18, wps=14685.9, ups=0.22, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.591, loss_scale=8, train_wall=441, gb_free=10.1, wall=41342
2022-03-04 08:13:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 08:16:12 | INFO | train_inner | epoch 024:    282 / 393 loss=5.651, ppl=50.26, wps=14541.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.615, loss_scale=4, train_wall=446, gb_free=10.1, wall=41793
2022-03-04 08:23:38 | INFO | train_inner | epoch 024:    382 / 393 loss=5.69, ppl=51.61, wps=14688.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.584, loss_scale=4, train_wall=441, gb_free=10.1, wall=42239
2022-03-04 08:24:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:24:31 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.051 | ppl 132.62 | wps 33975.1 | wpb 2034.1 | bsz 4 | num_updates 9411 | best_loss 6.921
2022-03-04 08:24:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9411 updates
2022-03-04 08:24:31 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 08:24:31 | INFO | train | epoch 024 | loss 5.641 | ppl 49.9 | wps 14558.2 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 9411 | lr 0.000325973 | gnorm 0.596 | loss_scale 4 | train_wall 1733 | gb_free 10.1 | wall 42292
2022-03-04 08:24:31 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 08:24:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:31:09 | INFO | train_inner | epoch 025:     89 / 393 loss=5.556, ppl=47.03, wps=14479.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.583, loss_scale=4, train_wall=439, gb_free=10.1, wall=42690
2022-03-04 08:38:35 | INFO | train_inner | epoch 025:    189 / 393 loss=5.58, ppl=47.82, wps=14689.6, ups=0.22, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.603, loss_scale=4, train_wall=441, gb_free=10.1, wall=43136
2022-03-04 08:46:01 | INFO | train_inner | epoch 025:    289 / 393 loss=5.623, ppl=49.27, wps=14690.9, ups=0.22, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.586, loss_scale=4, train_wall=441, gb_free=10.1, wall=43582
2022-03-04 08:53:27 | INFO | train_inner | epoch 025:    389 / 393 loss=5.656, ppl=50.43, wps=14691.5, ups=0.22, wpb=65530.2, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.595, loss_scale=8, train_wall=441, gb_free=10.1, wall=44028
2022-03-04 08:53:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:53:49 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.072 | ppl 134.59 | wps 34034.2 | wpb 2034.1 | bsz 4 | num_updates 9804 | best_loss 6.921
2022-03-04 08:53:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9804 updates
2022-03-04 08:53:49 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 08:53:49 | INFO | train | epoch 025 | loss 5.601 | ppl 48.52 | wps 14636.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9804 | lr 0.000319373 | gnorm 0.593 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 44050
2022-03-04 08:53:49 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 08:53:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:00:57 | INFO | train_inner | epoch 026:     96 / 393 loss=5.505, ppl=45.4, wps=14478.5, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.61, loss_scale=8, train_wall=439, gb_free=10.1, wall=44478
2022-03-04 09:08:24 | INFO | train_inner | epoch 026:    196 / 393 loss=5.543, ppl=46.62, wps=14686.4, ups=0.22, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.6, loss_scale=8, train_wall=441, gb_free=10.1, wall=44925
2022-03-04 09:15:50 | INFO | train_inner | epoch 026:    296 / 393 loss=5.582, ppl=47.91, wps=14685.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.595, loss_scale=8, train_wall=441, gb_free=10.1, wall=45371
2022-03-04 09:19:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 09:23:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:23:07 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.076 | ppl 134.94 | wps 33885.2 | wpb 2034.1 | bsz 4 | num_updates 10196 | best_loss 6.921
2022-03-04 09:23:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10196 updates
2022-03-04 09:23:07 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 09:23:07 | INFO | train | epoch 026 | loss 5.561 | ppl 47.2 | wps 14595 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10196 | lr 0.000313174 | gnorm 0.608 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 45808
2022-03-04 09:23:07 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 09:23:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:23:25 | INFO | train_inner | epoch 027:      4 / 393 loss=5.61, ppl=48.84, wps=14330.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.621, loss_scale=4, train_wall=444, gb_free=10.1, wall=45826
2022-03-04 09:30:51 | INFO | train_inner | epoch 027:    104 / 393 loss=5.467, ppl=44.23, wps=14689.1, ups=0.22, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.59, loss_scale=4, train_wall=441, gb_free=10.1, wall=46272
2022-03-04 09:38:18 | INFO | train_inner | epoch 027:    204 / 393 loss=5.512, ppl=45.65, wps=14686.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.62, loss_scale=4, train_wall=441, gb_free=10.1, wall=46719
2022-03-04 09:45:44 | INFO | train_inner | epoch 027:    304 / 393 loss=5.553, ppl=46.94, wps=14691.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.632, loss_scale=4, train_wall=441, gb_free=10.1, wall=47165
2022-03-04 09:52:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:52:25 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.113 | ppl 138.42 | wps 34076.1 | wpb 2034.1 | bsz 4 | num_updates 10589 | best_loss 6.921
2022-03-04 09:52:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10589 updates
2022-03-04 09:52:25 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 09:52:25 | INFO | train | epoch 027 | loss 5.524 | ppl 46.03 | wps 14633.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 10589 | lr 0.000307307 | gnorm 0.615 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 47566
2022-03-04 09:52:25 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 09:52:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:53:14 | INFO | train_inner | epoch 028:     11 / 393 loss=5.556, ppl=47.06, wps=14473.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.637, loss_scale=4, train_wall=439, gb_free=10.1, wall=47616
2022-03-04 10:00:41 | INFO | train_inner | epoch 028:    111 / 393 loss=5.431, ppl=43.16, wps=14687, ups=0.22, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.587, loss_scale=8, train_wall=441, gb_free=10.1, wall=48062
2022-03-04 10:08:07 | INFO | train_inner | epoch 028:    211 / 393 loss=5.471, ppl=44.36, wps=14682.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.619, loss_scale=8, train_wall=441, gb_free=10.1, wall=48508
2022-03-04 10:15:33 | INFO | train_inner | epoch 028:    311 / 393 loss=5.526, ppl=46.08, wps=14682.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.593, loss_scale=8, train_wall=441, gb_free=10.1, wall=48954
2022-03-04 10:21:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:21:44 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.129 | ppl 139.98 | wps 33956.4 | wpb 2034.1 | bsz 4 | num_updates 10982 | best_loss 6.921
2022-03-04 10:21:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10982 updates
2022-03-04 10:21:44 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 10:21:44 | INFO | train | epoch 028 | loss 5.49 | ppl 44.94 | wps 14630.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 10982 | lr 0.000301758 | gnorm 0.608 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 49325
2022-03-04 10:21:44 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 10:21:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:22:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 10:23:09 | INFO | train_inner | epoch 029:     19 / 393 loss=5.521, ppl=45.91, wps=14332.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.652, loss_scale=4, train_wall=444, gb_free=10.1, wall=49410
2022-03-04 10:30:35 | INFO | train_inner | epoch 029:    119 / 393 loss=5.411, ppl=42.54, wps=14688.1, ups=0.22, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.596, loss_scale=4, train_wall=441, gb_free=10.1, wall=49856
2022-03-04 10:38:01 | INFO | train_inner | epoch 029:    219 / 393 loss=5.451, ppl=43.74, wps=14687.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.617, loss_scale=4, train_wall=441, gb_free=10.1, wall=50302
2022-03-04 10:45:27 | INFO | train_inner | epoch 029:    319 / 393 loss=5.48, ppl=44.65, wps=14685.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.62, loss_scale=4, train_wall=441, gb_free=10.1, wall=50748
2022-03-04 10:50:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:51:02 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.159 | ppl 142.91 | wps 33965.9 | wpb 2034.1 | bsz 4 | num_updates 11374 | best_loss 6.921
2022-03-04 10:51:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11374 updates
2022-03-04 10:51:02 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 10:51:02 | INFO | train | epoch 029 | loss 5.456 | ppl 43.9 | wps 14595.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 11374 | lr 0.000296513 | gnorm 0.627 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 51083
2022-03-04 10:51:02 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 10:51:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:52:58 | INFO | train_inner | epoch 030:     26 / 393 loss=5.467, ppl=44.22, wps=14479.2, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.628, loss_scale=4, train_wall=439, gb_free=10.1, wall=51199
2022-03-04 11:00:24 | INFO | train_inner | epoch 030:    126 / 393 loss=5.379, ppl=41.6, wps=14696.7, ups=0.22, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.616, loss_scale=4, train_wall=441, gb_free=10.1, wall=51645
2022-03-04 11:07:50 | INFO | train_inner | epoch 030:    226 / 393 loss=5.424, ppl=42.92, wps=14684.1, ups=0.22, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.621, loss_scale=8, train_wall=441, gb_free=10.1, wall=52091
2022-03-04 11:15:16 | INFO | train_inner | epoch 030:    326 / 393 loss=5.456, ppl=43.9, wps=14684.8, ups=0.22, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.603, loss_scale=8, train_wall=441, gb_free=10.1, wall=52537
2022-03-04 11:20:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:20:20 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.148 | ppl 141.82 | wps 33885.6 | wpb 2034.1 | bsz 4 | num_updates 11767 | best_loss 6.921
2022-03-04 11:20:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11767 updates
2022-03-04 11:20:20 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 11:20:20 | INFO | train | epoch 030 | loss 5.425 | ppl 42.96 | wps 14634.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 11767 | lr 0.000291519 | gnorm 0.612 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 52841
2022-03-04 11:20:20 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 11:20:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:22:47 | INFO | train_inner | epoch 031:     33 / 393 loss=5.427, ppl=43.01, wps=14471.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.626, loss_scale=8, train_wall=439, gb_free=10.1, wall=52988
2022-03-04 11:27:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 11:30:18 | INFO | train_inner | epoch 031:    134 / 393 loss=5.353, ppl=40.86, wps=14544.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.651, loss_scale=4, train_wall=446, gb_free=10.1, wall=53439
2022-03-04 11:37:44 | INFO | train_inner | epoch 031:    234 / 393 loss=5.397, ppl=42.12, wps=14691.8, ups=0.22, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.595, loss_scale=4, train_wall=441, gb_free=10.1, wall=53885
2022-03-04 11:45:10 | INFO | train_inner | epoch 031:    334 / 393 loss=5.428, ppl=43.04, wps=14693, ups=0.22, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.645, loss_scale=4, train_wall=441, gb_free=10.1, wall=54331
2022-03-04 11:49:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:49:38 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.192 | ppl 146.18 | wps 33914.8 | wpb 2034.1 | bsz 4 | num_updates 12159 | best_loss 6.921
2022-03-04 11:49:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12159 updates
2022-03-04 11:49:38 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 11:49:38 | INFO | train | epoch 031 | loss 5.395 | ppl 42.07 | wps 14599 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12159 | lr 0.000286781 | gnorm 0.626 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 54599
2022-03-04 11:49:38 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 11:49:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:52:40 | INFO | train_inner | epoch 032:     41 / 393 loss=5.391, ppl=41.97, wps=14481.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.616, loss_scale=4, train_wall=439, gb_free=10.1, wall=54782
2022-03-04 12:00:06 | INFO | train_inner | epoch 032:    141 / 393 loss=5.321, ppl=39.96, wps=14695.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.631, loss_scale=4, train_wall=441, gb_free=10.1, wall=55227
2022-03-04 12:07:33 | INFO | train_inner | epoch 032:    241 / 393 loss=5.365, ppl=41.23, wps=14688, ups=0.22, wpb=65530.9, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.634, loss_scale=8, train_wall=441, gb_free=10.1, wall=55674
2022-03-04 12:14:59 | INFO | train_inner | epoch 032:    341 / 393 loss=5.408, ppl=42.47, wps=14685.8, ups=0.22, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.63, loss_scale=8, train_wall=441, gb_free=10.1, wall=56120
2022-03-04 12:18:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:18:55 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.208 | ppl 147.84 | wps 33923.3 | wpb 2034.1 | bsz 4 | num_updates 12552 | best_loss 6.921
2022-03-04 12:18:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12552 updates
2022-03-04 12:18:55 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 12:18:55 | INFO | train | epoch 032 | loss 5.366 | ppl 41.25 | wps 14634.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 12552 | lr 0.000282256 | gnorm 0.627 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 56357
2022-03-04 12:18:55 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 12:18:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:22:30 | INFO | train_inner | epoch 033:     48 / 393 loss=5.353, ppl=40.87, wps=14470.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.631, loss_scale=8, train_wall=439, gb_free=10.1, wall=56571
2022-03-04 12:29:56 | INFO | train_inner | epoch 033:    148 / 393 loss=5.297, ppl=39.31, wps=14683.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.647, loss_scale=8, train_wall=441, gb_free=10.1, wall=57017
2022-03-04 12:37:22 | INFO | train_inner | epoch 033:    248 / 393 loss=5.342, ppl=40.57, wps=14688, ups=0.22, wpb=65535.4, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.653, loss_scale=8, train_wall=441, gb_free=10.1, wall=57463
2022-03-04 12:44:49 | INFO | train_inner | epoch 033:    348 / 393 loss=5.384, ppl=41.76, wps=14684, ups=0.22, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.64, loss_scale=16, train_wall=441, gb_free=10.1, wall=57910
2022-03-04 12:45:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 12:48:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:48:14 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.239 | ppl 151.05 | wps 33961.1 | wpb 2034.1 | bsz 4 | num_updates 12944 | best_loss 6.921
2022-03-04 12:48:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12944 updates
2022-03-04 12:48:14 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 12:48:14 | INFO | train | epoch 033 | loss 5.339 | ppl 40.49 | wps 14594.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12944 | lr 0.000277949 | gnorm 0.651 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 58115
2022-03-04 12:48:14 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 12:48:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:52:24 | INFO | train_inner | epoch 034:     56 / 393 loss=5.309, ppl=39.65, wps=14335.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.629, loss_scale=8, train_wall=444, gb_free=10.1, wall=58365
2022-03-04 12:59:50 | INFO | train_inner | epoch 034:    156 / 393 loss=5.274, ppl=38.68, wps=14686.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.662, loss_scale=8, train_wall=441, gb_free=10.1, wall=58811
2022-03-04 13:07:16 | INFO | train_inner | epoch 034:    256 / 393 loss=5.324, ppl=40.06, wps=14682, ups=0.22, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.655, loss_scale=8, train_wall=441, gb_free=10.1, wall=59257
2022-03-04 13:14:43 | INFO | train_inner | epoch 034:    356 / 393 loss=5.363, ppl=41.16, wps=14677.9, ups=0.22, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.655, loss_scale=8, train_wall=442, gb_free=10.1, wall=59704
2022-03-04 13:17:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:17:32 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.268 | ppl 154.17 | wps 33915.8 | wpb 2034.1 | bsz 4 | num_updates 13337 | best_loss 6.921
2022-03-04 13:17:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13337 updates
2022-03-04 13:17:32 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 13:17:32 | INFO | train | epoch 034 | loss 5.314 | ppl 39.79 | wps 14628.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 13337 | lr 0.000273824 | gnorm 0.655 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 59873
2022-03-04 13:17:32 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 13:17:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:22:14 | INFO | train_inner | epoch 035:     63 / 393 loss=5.28, ppl=38.84, wps=14468.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.649, loss_scale=8, train_wall=439, gb_free=10.1, wall=60155
2022-03-04 13:23:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 13:29:44 | INFO | train_inner | epoch 035:    164 / 393 loss=5.26, ppl=38.31, wps=14538.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.651, loss_scale=8, train_wall=446, gb_free=10.1, wall=60606
2022-03-04 13:32:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 13:37:15 | INFO | train_inner | epoch 035:    265 / 393 loss=5.3, ppl=39.39, wps=14545.1, ups=0.22, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.676, loss_scale=4, train_wall=446, gb_free=10.1, wall=61056
2022-03-04 13:44:41 | INFO | train_inner | epoch 035:    365 / 393 loss=5.331, ppl=40.26, wps=14695.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.646, loss_scale=4, train_wall=441, gb_free=10.1, wall=61502
2022-03-04 13:46:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:46:50 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.275 | ppl 154.87 | wps 33940.8 | wpb 2034.1 | bsz 4 | num_updates 13728 | best_loss 6.921
2022-03-04 13:46:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13728 updates
2022-03-04 13:46:50 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 13:46:50 | INFO | train | epoch 035 | loss 5.289 | ppl 39.1 | wps 14559.5 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 13728 | lr 0.000269896 | gnorm 0.651 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 61631
2022-03-04 13:46:50 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 13:46:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:52:12 | INFO | train_inner | epoch 036:     72 / 393 loss=5.242, ppl=37.86, wps=14479.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.666, loss_scale=4, train_wall=439, gb_free=10.1, wall=61953
2022-03-04 13:59:38 | INFO | train_inner | epoch 036:    172 / 393 loss=5.238, ppl=37.75, wps=14689.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.645, loss_scale=4, train_wall=441, gb_free=10.1, wall=62399
2022-03-04 14:07:04 | INFO | train_inner | epoch 036:    272 / 393 loss=5.278, ppl=38.8, wps=14689.6, ups=0.22, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.645, loss_scale=4, train_wall=441, gb_free=10.1, wall=62845
2022-03-04 14:13:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 14:14:34 | INFO | train_inner | epoch 036:    373 / 393 loss=5.315, ppl=39.8, wps=14546, ups=0.22, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.644, loss_scale=4, train_wall=446, gb_free=10.1, wall=63295
2022-03-04 14:16:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:16:08 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.274 | ppl 154.73 | wps 33930.7 | wpb 2034.1 | bsz 4 | num_updates 14120 | best_loss 6.921
2022-03-04 14:16:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14120 updates
2022-03-04 14:16:08 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 14:16:08 | INFO | train | epoch 036 | loss 5.266 | ppl 38.47 | wps 14598.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14120 | lr 0.000266123 | gnorm 0.651 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 63389
2022-03-04 14:16:08 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 14:16:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:22:05 | INFO | train_inner | epoch 037:     80 / 393 loss=5.209, ppl=36.98, wps=14478.8, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.664, loss_scale=4, train_wall=439, gb_free=10.1, wall=63746
2022-03-04 14:29:31 | INFO | train_inner | epoch 037:    180 / 393 loss=5.213, ppl=37.09, wps=14692.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.643, loss_scale=4, train_wall=441, gb_free=10.1, wall=64192
2022-03-04 14:36:57 | INFO | train_inner | epoch 037:    280 / 393 loss=5.263, ppl=38.39, wps=14692.9, ups=0.22, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.659, loss_scale=4, train_wall=441, gb_free=10.1, wall=64638
2022-03-04 14:44:23 | INFO | train_inner | epoch 037:    380 / 393 loss=5.299, ppl=39.38, wps=14694.3, ups=0.22, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.644, loss_scale=4, train_wall=441, gb_free=10.1, wall=65084
2022-03-04 14:45:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:45:26 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.319 | ppl 159.66 | wps 34009 | wpb 2034.1 | bsz 4 | num_updates 14513 | best_loss 6.921
2022-03-04 14:45:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14513 updates
2022-03-04 14:45:26 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 14:45:26 | INFO | train | epoch 037 | loss 5.243 | ppl 37.86 | wps 14638.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 14513 | lr 0.000262495 | gnorm 0.654 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 65147
2022-03-04 14:45:26 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 14:45:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:51:54 | INFO | train_inner | epoch 038:     87 / 393 loss=5.168, ppl=35.96, wps=14475, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.646, loss_scale=8, train_wall=439, gb_free=10.1, wall=65535
2022-03-04 14:59:20 | INFO | train_inner | epoch 038:    187 / 393 loss=5.196, ppl=36.67, wps=14683.8, ups=0.22, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.661, loss_scale=8, train_wall=441, gb_free=10.1, wall=65981
2022-03-04 15:06:46 | INFO | train_inner | epoch 038:    287 / 393 loss=5.244, ppl=37.9, wps=14687.3, ups=0.22, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.698, loss_scale=8, train_wall=441, gb_free=10.1, wall=66427
2022-03-04 15:13:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 15:14:17 | INFO | train_inner | epoch 038:    388 / 393 loss=5.277, ppl=38.78, wps=14543.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.667, loss_scale=4, train_wall=446, gb_free=10.1, wall=66878
2022-03-04 15:14:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:14:44 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.339 | ppl 161.92 | wps 33905.3 | wpb 2034.1 | bsz 4 | num_updates 14905 | best_loss 6.921
2022-03-04 15:14:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14905 updates
2022-03-04 15:14:44 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 15:14:44 | INFO | train | epoch 038 | loss 5.22 | ppl 37.28 | wps 14594.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14905 | lr 0.00025902 | gnorm 0.665 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 66905
2022-03-04 15:14:44 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 15:14:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:21:48 | INFO | train_inner | epoch 039:     95 / 393 loss=5.149, ppl=35.48, wps=14477.8, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.653, loss_scale=4, train_wall=439, gb_free=10.1, wall=67329
2022-03-04 15:29:14 | INFO | train_inner | epoch 039:    195 / 393 loss=5.186, ppl=36.39, wps=14694.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.651, loss_scale=4, train_wall=441, gb_free=10.1, wall=67775
2022-03-04 15:36:40 | INFO | train_inner | epoch 039:    295 / 393 loss=5.221, ppl=37.29, wps=14689.8, ups=0.22, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.646, loss_scale=4, train_wall=441, gb_free=10.1, wall=68221
2022-03-04 15:43:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:44:01 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.378 | ppl 166.37 | wps 33891.8 | wpb 2034.1 | bsz 4 | num_updates 15298 | best_loss 6.921
2022-03-04 15:44:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15298 updates
2022-03-04 15:44:01 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 15:44:01 | INFO | train | epoch 039 | loss 5.2 | ppl 36.75 | wps 14637.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 15298 | lr 0.000255672 | gnorm 0.655 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 68662
2022-03-04 15:44:01 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 15:44:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:44:10 | INFO | train_inner | epoch 040:      2 / 393 loss=5.247, ppl=37.99, wps=14479, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.669, loss_scale=4, train_wall=439, gb_free=10.1, wall=68671
2022-03-04 15:51:36 | INFO | train_inner | epoch 040:    102 / 393 loss=5.119, ppl=34.76, wps=14695.5, ups=0.22, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.678, loss_scale=8, train_wall=441, gb_free=10.1, wall=69117
2022-03-04 15:59:03 | INFO | train_inner | epoch 040:    202 / 393 loss=5.169, ppl=35.97, wps=14682.6, ups=0.22, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.655, loss_scale=8, train_wall=441, gb_free=10.1, wall=69564
2022-03-04 16:06:29 | INFO | train_inner | epoch 040:    302 / 393 loss=5.2, ppl=36.76, wps=14679.3, ups=0.22, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.696, loss_scale=8, train_wall=442, gb_free=10.1, wall=70010
2022-03-04 16:13:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:13:20 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.36 | ppl 164.24 | wps 33811.3 | wpb 2034.1 | bsz 4 | num_updates 15691 | best_loss 6.921
2022-03-04 16:13:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15691 updates
2022-03-04 16:13:20 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 16:13:20 | INFO | train | epoch 040 | loss 5.179 | ppl 36.22 | wps 14630.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 15691 | lr 0.00025245 | gnorm 0.673 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 70421
2022-03-04 16:13:20 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 16:13:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:14:00 | INFO | train_inner | epoch 041:      9 / 393 loss=5.219, ppl=37.25, wps=14471.5, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.658, loss_scale=8, train_wall=439, gb_free=10.1, wall=70461
2022-03-04 16:21:26 | INFO | train_inner | epoch 041:    109 / 393 loss=5.105, ppl=34.4, wps=14685.4, ups=0.22, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.651, loss_scale=8, train_wall=441, gb_free=10.1, wall=70907
2022-03-04 16:28:53 | INFO | train_inner | epoch 041:    209 / 393 loss=5.149, ppl=35.49, wps=14683.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.677, loss_scale=8, train_wall=441, gb_free=10.1, wall=71354
2022-03-04 16:31:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:36:23 | INFO | train_inner | epoch 041:    310 / 393 loss=5.187, ppl=36.42, wps=14537.6, ups=0.22, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.666, loss_scale=8, train_wall=446, gb_free=10.1, wall=71804
2022-03-04 16:39:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 16:42:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:42:38 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.4 | ppl 168.94 | wps 33929.1 | wpb 2034.1 | bsz 4 | num_updates 16082 | best_loss 6.921
2022-03-04 16:42:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16082 updates
2022-03-04 16:42:38 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 16:42:38 | INFO | train | epoch 041 | loss 5.159 | ppl 35.73 | wps 14556.1 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 16082 | lr 0.000249362 | gnorm 0.673 | loss_scale 4 | train_wall 1733 | gb_free 10.1 | wall 72179
2022-03-04 16:42:38 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 16:42:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:43:59 | INFO | train_inner | epoch 042:     18 / 393 loss=5.185, ppl=36.39, wps=14334.3, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.694, loss_scale=4, train_wall=444, gb_free=10.1, wall=72260
2022-03-04 16:51:25 | INFO | train_inner | epoch 042:    118 / 393 loss=5.09, ppl=34.05, wps=14691, ups=0.22, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.667, loss_scale=4, train_wall=441, gb_free=10.1, wall=72706
2022-03-04 16:58:51 | INFO | train_inner | epoch 042:    218 / 393 loss=5.132, ppl=35.07, wps=14697.8, ups=0.22, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.669, loss_scale=4, train_wall=441, gb_free=10.1, wall=73152
2022-03-04 17:06:17 | INFO | train_inner | epoch 042:    318 / 393 loss=5.176, ppl=36.15, wps=14686.7, ups=0.22, wpb=65530.2, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.678, loss_scale=4, train_wall=441, gb_free=10.1, wall=73598
2022-03-04 17:11:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:11:56 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.415 | ppl 170.68 | wps 33903.2 | wpb 2034.1 | bsz 4 | num_updates 16475 | best_loss 6.921
2022-03-04 17:11:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16475 updates
2022-03-04 17:11:56 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 17:11:56 | INFO | train | epoch 042 | loss 5.141 | ppl 35.29 | wps 14637.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 16475 | lr 0.00024637 | gnorm 0.668 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 73937
2022-03-04 17:11:56 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 17:11:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:13:47 | INFO | train_inner | epoch 043:     25 / 393 loss=5.157, ppl=35.68, wps=14479.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.669, loss_scale=4, train_wall=439, gb_free=10.1, wall=74048
2022-03-04 17:21:13 | INFO | train_inner | epoch 043:    125 / 393 loss=5.076, ppl=33.72, wps=14691.5, ups=0.22, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.699, loss_scale=8, train_wall=441, gb_free=10.1, wall=74494
2022-03-04 17:28:40 | INFO | train_inner | epoch 043:    225 / 393 loss=5.117, ppl=34.7, wps=14680.8, ups=0.22, wpb=65530.2, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.676, loss_scale=8, train_wall=441, gb_free=10.1, wall=74941
2022-03-04 17:36:06 | INFO | train_inner | epoch 043:    325 / 393 loss=5.153, ppl=35.57, wps=14686.6, ups=0.22, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.685, loss_scale=8, train_wall=441, gb_free=10.1, wall=75387
2022-03-04 17:41:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:41:14 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.432 | ppl 172.72 | wps 33967.4 | wpb 2034.1 | bsz 4 | num_updates 16868 | best_loss 6.921
2022-03-04 17:41:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16868 updates
2022-03-04 17:41:14 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 17:41:14 | INFO | train | epoch 043 | loss 5.123 | ppl 34.84 | wps 14632.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 16868 | lr 0.000243483 | gnorm 0.685 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 75695
2022-03-04 17:41:14 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 17:41:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:43:37 | INFO | train_inner | epoch 044:     32 / 393 loss=5.136, ppl=35.16, wps=14474.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.695, loss_scale=8, train_wall=439, gb_free=10.1, wall=75838
2022-03-04 17:51:03 | INFO | train_inner | epoch 044:    132 / 393 loss=5.059, ppl=33.33, wps=14686.6, ups=0.22, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.679, loss_scale=8, train_wall=441, gb_free=10.1, wall=76284
2022-03-04 17:56:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:58:34 | INFO | train_inner | epoch 044:    233 / 393 loss=5.107, ppl=34.46, wps=14534.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.692, loss_scale=8, train_wall=446, gb_free=10.1, wall=76735
2022-03-04 17:59:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 18:06:05 | INFO | train_inner | epoch 044:    334 / 393 loss=5.137, ppl=35.2, wps=14542.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.672, loss_scale=4, train_wall=446, gb_free=10.1, wall=77186
2022-03-04 18:10:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:10:32 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.428 | ppl 172.25 | wps 34049.2 | wpb 2034.1 | bsz 4 | num_updates 17259 | best_loss 6.921
2022-03-04 18:10:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17259 updates
2022-03-04 18:10:32 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 18:10:32 | INFO | train | epoch 044 | loss 5.106 | ppl 34.43 | wps 14556.7 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 17259 | lr 0.000240709 | gnorm 0.686 | loss_scale 4 | train_wall 1733 | gb_free 10.1 | wall 77453
2022-03-04 18:10:32 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 18:10:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:13:35 | INFO | train_inner | epoch 045:     41 / 393 loss=5.108, ppl=34.49, wps=14483.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.695, loss_scale=4, train_wall=439, gb_free=10.1, wall=77636
2022-03-04 18:21:01 | INFO | train_inner | epoch 045:    141 / 393 loss=5.051, ppl=33.15, wps=14688.7, ups=0.22, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.676, loss_scale=4, train_wall=441, gb_free=10.1, wall=78082
2022-03-04 18:28:27 | INFO | train_inner | epoch 045:    241 / 393 loss=5.086, ppl=33.96, wps=14693.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.681, loss_scale=4, train_wall=441, gb_free=10.1, wall=78528
2022-03-04 18:35:53 | INFO | train_inner | epoch 045:    341 / 393 loss=5.13, ppl=35.01, wps=14690.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.701, loss_scale=4, train_wall=441, gb_free=10.1, wall=78974
2022-03-04 18:39:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:39:50 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.471 | ppl 177.39 | wps 33912.6 | wpb 2034.1 | bsz 4 | num_updates 17652 | best_loss 6.921
2022-03-04 18:39:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17652 updates
2022-03-04 18:39:50 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 18:39:50 | INFO | train | epoch 045 | loss 5.089 | ppl 34.03 | wps 14637.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 17652 | lr 0.000238014 | gnorm 0.689 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 79211
2022-03-04 18:39:50 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 18:39:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:43:24 | INFO | train_inner | epoch 046:     48 / 393 loss=5.079, ppl=33.8, wps=14477.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.686, loss_scale=8, train_wall=439, gb_free=10.1, wall=79425
2022-03-04 18:50:51 | INFO | train_inner | epoch 046:    148 / 393 loss=5.031, ppl=32.69, wps=14676.4, ups=0.22, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.661, loss_scale=8, train_wall=442, gb_free=10.1, wall=79872
2022-03-04 18:58:17 | INFO | train_inner | epoch 046:    248 / 393 loss=5.074, ppl=33.68, wps=14689.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.696, loss_scale=8, train_wall=441, gb_free=10.1, wall=80318
2022-03-04 19:05:43 | INFO | train_inner | epoch 046:    348 / 393 loss=5.112, ppl=34.59, wps=14686.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.694, loss_scale=8, train_wall=441, gb_free=10.1, wall=80764
2022-03-04 19:09:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:09:08 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.44 | ppl 173.59 | wps 33974.1 | wpb 2034.1 | bsz 4 | num_updates 18045 | best_loss 6.921
2022-03-04 19:09:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18045 updates
2022-03-04 19:09:08 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 19:09:08 | INFO | train | epoch 046 | loss 5.072 | ppl 33.65 | wps 14629.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 18045 | lr 0.000235408 | gnorm 0.691 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 80969
2022-03-04 19:09:08 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 19:09:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:13:14 | INFO | train_inner | epoch 047:     55 / 393 loss=5.056, ppl=33.28, wps=14471.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.708, loss_scale=8, train_wall=440, gb_free=10.1, wall=81215
2022-03-04 19:18:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:20:45 | INFO | train_inner | epoch 047:    156 / 393 loss=5.024, ppl=32.53, wps=14538.6, ups=0.22, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.702, loss_scale=8, train_wall=446, gb_free=10.1, wall=81666
2022-03-04 19:28:11 | INFO | train_inner | epoch 047:    256 / 393 loss=5.062, ppl=33.4, wps=14684, ups=0.22, wpb=65535.4, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.666, loss_scale=8, train_wall=441, gb_free=10.1, wall=82112
2022-03-04 19:29:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 19:35:42 | INFO | train_inner | epoch 047:    357 / 393 loss=5.098, ppl=34.24, wps=14543.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.694, loss_scale=4, train_wall=446, gb_free=10.1, wall=82563
2022-03-04 19:38:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:38:27 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.468 | ppl 177.03 | wps 33923.3 | wpb 2034.1 | bsz 4 | num_updates 18436 | best_loss 6.921
2022-03-04 19:38:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18436 updates
2022-03-04 19:38:27 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 19:38:27 | INFO | train | epoch 047 | loss 5.056 | ppl 33.28 | wps 14557.7 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 18436 | lr 0.000232898 | gnorm 0.69 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 82728
2022-03-04 19:38:27 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 19:38:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:43:12 | INFO | train_inner | epoch 048:     64 / 393 loss=5.022, ppl=32.49, wps=14479.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.694, loss_scale=4, train_wall=439, gb_free=10.1, wall=83013
2022-03-04 19:50:38 | INFO | train_inner | epoch 048:    164 / 393 loss=5.013, ppl=32.3, wps=14687.7, ups=0.22, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.695, loss_scale=4, train_wall=441, gb_free=10.1, wall=83459
2022-03-04 19:58:05 | INFO | train_inner | epoch 048:    264 / 393 loss=5.057, ppl=33.29, wps=14688.2, ups=0.22, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.691, loss_scale=4, train_wall=441, gb_free=10.1, wall=83906
2022-03-04 20:05:31 | INFO | train_inner | epoch 048:    364 / 393 loss=5.087, ppl=33.98, wps=14687.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.735, loss_scale=4, train_wall=441, gb_free=10.1, wall=84352
2022-03-04 20:07:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:07:45 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.48 | ppl 178.59 | wps 33936.7 | wpb 2034.1 | bsz 4 | num_updates 18829 | best_loss 6.921
2022-03-04 20:07:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18829 updates
2022-03-04 20:07:45 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 20:07:45 | INFO | train | epoch 048 | loss 5.042 | ppl 32.95 | wps 14633.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 18829 | lr 0.000230455 | gnorm 0.704 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 84486
2022-03-04 20:07:45 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 20:07:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:13:02 | INFO | train_inner | epoch 049:     71 / 393 loss=5.003, ppl=32.07, wps=14470.9, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.684, loss_scale=8, train_wall=439, gb_free=10.1, wall=84803
2022-03-04 20:20:28 | INFO | train_inner | epoch 049:    171 / 393 loss=4.994, ppl=31.88, wps=14684.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.7, loss_scale=8, train_wall=441, gb_free=10.1, wall=85249
2022-03-04 20:27:54 | INFO | train_inner | epoch 049:    271 / 393 loss=5.039, ppl=32.88, wps=14685.1, ups=0.22, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.698, loss_scale=8, train_wall=441, gb_free=10.1, wall=85695
2022-03-04 20:35:21 | INFO | train_inner | epoch 049:    371 / 393 loss=5.08, ppl=33.83, wps=14681.2, ups=0.22, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.696, loss_scale=8, train_wall=442, gb_free=10.1, wall=86142
2022-03-04 20:36:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:37:03 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.488 | ppl 179.47 | wps 33898.5 | wpb 2034.1 | bsz 4 | num_updates 19222 | best_loss 6.921
2022-03-04 20:37:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19222 updates
2022-03-04 20:37:03 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 20:37:03 | INFO | train | epoch 049 | loss 5.027 | ppl 32.6 | wps 14628.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 19222 | lr 0.000228087 | gnorm 0.696 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 86244
2022-03-04 20:37:03 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 20:37:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:42:51 | INFO | train_inner | epoch 050:     78 / 393 loss=4.978, ppl=31.53, wps=14472.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=19300, lr=0.000227626, gnorm=0.714, loss_scale=8, train_wall=439, gb_free=10.1, wall=86592
2022-03-04 20:48:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:50:22 | INFO | train_inner | epoch 050:    179 / 393 loss=4.988, ppl=31.73, wps=14538, ups=0.22, wpb=65530.2, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.679, loss_scale=8, train_wall=446, gb_free=10.1, wall=87043
2022-03-04 20:57:48 | INFO | train_inner | epoch 050:    279 / 393 loss=5.031, ppl=32.7, wps=14687.3, ups=0.22, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.684, loss_scale=8, train_wall=441, gb_free=10.1, wall=87489
2022-03-04 21:05:15 | INFO | train_inner | epoch 050:    379 / 393 loss=5.061, ppl=33.38, wps=14677, ups=0.22, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.724, loss_scale=8, train_wall=442, gb_free=10.1, wall=87936
2022-03-04 21:06:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:06:22 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.501 | ppl 181.18 | wps 33958.9 | wpb 2034.1 | bsz 4 | num_updates 19614 | best_loss 6.921
2022-03-04 21:06:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19614 updates
2022-03-04 21:06:22 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 21:06:22 | INFO | train | epoch 050 | loss 5.013 | ppl 32.28 | wps 14591.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 19614 | lr 0.000225796 | gnorm 0.694 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 88003
2022-03-04 21:06:22 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 21:06:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:11:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 21:12:50 | INFO | train_inner | epoch 051:     87 / 393 loss=4.965, ppl=31.24, wps=14331.5, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.708, loss_scale=4, train_wall=444, gb_free=10.1, wall=88391
2022-03-04 21:20:16 | INFO | train_inner | epoch 051:    187 / 393 loss=4.969, ppl=31.32, wps=14694.3, ups=0.22, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.672, loss_scale=4, train_wall=441, gb_free=10.1, wall=88837
2022-03-04 21:27:42 | INFO | train_inner | epoch 051:    287 / 393 loss=5.02, ppl=32.45, wps=14693.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.725, loss_scale=4, train_wall=441, gb_free=10.1, wall=89283
2022-03-04 21:35:08 | INFO | train_inner | epoch 051:    387 / 393 loss=5.048, ppl=33.09, wps=14694, ups=0.22, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.7, loss_scale=4, train_wall=441, gb_free=10.1, wall=89729
2022-03-04 21:35:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:35:39 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.527 | ppl 184.38 | wps 33967.1 | wpb 2034.1 | bsz 4 | num_updates 20006 | best_loss 6.921
2022-03-04 21:35:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20006 updates
2022-03-04 21:35:39 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 21:35:39 | INFO | train | epoch 051 | loss 4.999 | ppl 31.98 | wps 14600.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20006 | lr 0.000223573 | gnorm 0.703 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 89760
2022-03-04 21:35:39 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 21:35:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:42:39 | INFO | train_inner | epoch 052:     94 / 393 loss=4.929, ppl=30.46, wps=14484.5, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=20100, lr=0.00022305, gnorm=0.719, loss_scale=4, train_wall=439, gb_free=10.1, wall=90180
2022-03-04 21:50:05 | INFO | train_inner | epoch 052:    194 / 393 loss=4.971, ppl=31.36, wps=14696.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.712, loss_scale=8, train_wall=441, gb_free=10.1, wall=90626
2022-03-04 21:57:31 | INFO | train_inner | epoch 052:    294 / 393 loss=5.008, ppl=32.18, wps=14687, ups=0.22, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.71, loss_scale=8, train_wall=441, gb_free=10.1, wall=91072
2022-03-04 22:04:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:04:57 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.533 | ppl 185.22 | wps 34002.6 | wpb 2034.1 | bsz 4 | num_updates 20399 | best_loss 6.921
2022-03-04 22:04:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20399 updates
2022-03-04 22:04:57 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 22:04:57 | INFO | train | epoch 052 | loss 4.986 | ppl 31.68 | wps 14635.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 20399 | lr 0.000221409 | gnorm 0.71 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 91518
2022-03-04 22:04:57 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 22:04:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:05:02 | INFO | train_inner | epoch 053:      1 / 393 loss=5.039, ppl=32.87, wps=14468.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.701, loss_scale=8, train_wall=440, gb_free=10.1, wall=91523
2022-03-04 22:12:28 | INFO | train_inner | epoch 053:    101 / 393 loss=4.909, ppl=30.05, wps=14683.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.718, loss_scale=8, train_wall=441, gb_free=10.1, wall=91969
2022-03-04 22:19:54 | INFO | train_inner | epoch 053:    201 / 393 loss=4.964, ppl=31.21, wps=14684.1, ups=0.22, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.703, loss_scale=8, train_wall=441, gb_free=10.1, wall=92415
2022-03-04 22:27:20 | INFO | train_inner | epoch 053:    301 / 393 loss=4.988, ppl=31.74, wps=14687.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.698, loss_scale=8, train_wall=441, gb_free=10.1, wall=92862
2022-03-04 22:29:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:34:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:34:15 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.563 | ppl 189.08 | wps 33845.7 | wpb 2034.1 | bsz 4 | num_updates 20791 | best_loss 6.921
2022-03-04 22:34:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20791 updates
2022-03-04 22:34:15 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 22:34:15 | INFO | train | epoch 053 | loss 4.972 | ppl 31.39 | wps 14593.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20791 | lr 0.000219312 | gnorm 0.708 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 93277
2022-03-04 22:34:16 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 22:34:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:34:56 | INFO | train_inner | epoch 054:      9 / 393 loss=5.018, ppl=32.41, wps=14333, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=20800, lr=0.000219265, gnorm=0.722, loss_scale=8, train_wall=444, gb_free=10.1, wall=93317
2022-03-04 22:42:22 | INFO | train_inner | epoch 054:    109 / 393 loss=4.903, ppl=29.92, wps=14684.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.701, loss_scale=8, train_wall=441, gb_free=10.1, wall=93763
2022-03-04 22:49:48 | INFO | train_inner | epoch 054:    209 / 393 loss=4.95, ppl=30.91, wps=14681.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.731, loss_scale=8, train_wall=441, gb_free=10.1, wall=94209
2022-03-04 22:57:15 | INFO | train_inner | epoch 054:    309 / 393 loss=4.99, ppl=31.78, wps=14689.2, ups=0.22, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.7, loss_scale=8, train_wall=441, gb_free=10.1, wall=94656
2022-03-04 23:03:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:03:34 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.56 | ppl 188.66 | wps 33996.8 | wpb 2034.1 | bsz 4 | num_updates 21184 | best_loss 6.921
2022-03-04 23:03:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21184 updates
2022-03-04 23:03:34 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 23:03:34 | INFO | train | epoch 054 | loss 4.96 | ppl 31.13 | wps 14632.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 21184 | lr 0.000217268 | gnorm 0.716 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 95035
2022-03-04 23:03:34 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 23:03:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:04:45 | INFO | train_inner | epoch 055:     16 / 393 loss=4.991, ppl=31.81, wps=14478.1, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.714, loss_scale=8, train_wall=439, gb_free=10.1, wall=95106
2022-03-04 23:08:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:12:16 | INFO | train_inner | epoch 055:    117 / 393 loss=4.9, ppl=29.86, wps=14538.5, ups=0.22, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.698, loss_scale=8, train_wall=446, gb_free=10.1, wall=95557
2022-03-04 23:19:42 | INFO | train_inner | epoch 055:    217 / 393 loss=4.933, ppl=30.55, wps=14687.1, ups=0.22, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.73, loss_scale=8, train_wall=441, gb_free=10.1, wall=96003
2022-03-04 23:27:09 | INFO | train_inner | epoch 055:    317 / 393 loss=4.979, ppl=31.54, wps=14680.9, ups=0.22, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.728, loss_scale=8, train_wall=442, gb_free=10.1, wall=96450
2022-03-04 23:32:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:32:52 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.586 | ppl 192.19 | wps 33999.3 | wpb 2034.1 | bsz 4 | num_updates 21576 | best_loss 6.921
2022-03-04 23:32:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 21576 updates
2022-03-04 23:32:52 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 23:32:52 | INFO | train | epoch 055 | loss 4.948 | ppl 30.86 | wps 14592.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 21576 | lr 0.000215285 | gnorm 0.72 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 96793
2022-03-04 23:32:52 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 23:32:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:34:39 | INFO | train_inner | epoch 056:     24 / 393 loss=4.968, ppl=31.29, wps=14471.4, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=21600, lr=0.000215166, gnorm=0.731, loss_scale=8, train_wall=440, gb_free=10.1, wall=96900
2022-03-04 23:42:06 | INFO | train_inner | epoch 056:    124 / 393 loss=4.885, ppl=29.55, wps=14684, ups=0.22, wpb=65530.9, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.706, loss_scale=8, train_wall=441, gb_free=10.1, wall=97347
2022-03-04 23:47:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:49:36 | INFO | train_inner | epoch 056:    225 / 393 loss=4.935, ppl=30.58, wps=14538.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.723, loss_scale=8, train_wall=446, gb_free=10.1, wall=97798
2022-03-04 23:57:03 | INFO | train_inner | epoch 056:    325 / 393 loss=4.972, ppl=31.39, wps=14681.1, ups=0.22, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.742, loss_scale=8, train_wall=442, gb_free=10.1, wall=98244
2022-03-05 00:02:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:02:11 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.601 | ppl 194.11 | wps 33917.7 | wpb 2034.1 | bsz 4 | num_updates 21968 | best_loss 6.921
2022-03-05 00:02:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 21968 updates
2022-03-05 00:02:11 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-05 00:02:11 | INFO | train | epoch 056 | loss 4.936 | ppl 30.61 | wps 14591.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 21968 | lr 0.000213356 | gnorm 0.719 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 98552
2022-03-05 00:02:11 | INFO | fairseq.trainer | begin training epoch 57
2022-03-05 00:02:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:04:34 | INFO | train_inner | epoch 057:     32 / 393 loss=4.946, ppl=30.82, wps=14475.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=22000, lr=0.000213201, gnorm=0.709, loss_scale=8, train_wall=439, gb_free=10.1, wall=98695
2022-03-05 00:12:00 | INFO | train_inner | epoch 057:    132 / 393 loss=4.886, ppl=29.56, wps=14684.1, ups=0.22, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.723, loss_scale=8, train_wall=441, gb_free=10.1, wall=99141
2022-03-05 00:19:26 | INFO | train_inner | epoch 057:    232 / 393 loss=4.928, ppl=30.44, wps=14683.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.749, loss_scale=8, train_wall=441, gb_free=10.1, wall=99587
2022-03-05 00:26:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:26:57 | INFO | train_inner | epoch 057:    333 / 393 loss=4.956, ppl=31.05, wps=14536.1, ups=0.22, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.699, loss_scale=8, train_wall=446, gb_free=10.1, wall=100038
2022-03-05 00:31:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:31:29 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.597 | ppl 193.67 | wps 33858.4 | wpb 2034.1 | bsz 4 | num_updates 22360 | best_loss 6.921
2022-03-05 00:31:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 22360 updates
2022-03-05 00:31:29 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-05 00:31:29 | INFO | train | epoch 057 | loss 4.925 | ppl 30.37 | wps 14592.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 22360 | lr 0.000211477 | gnorm 0.725 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 100310
2022-03-05 00:31:29 | INFO | fairseq.trainer | begin training epoch 58
2022-03-05 00:31:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:34:28 | INFO | train_inner | epoch 058:     40 / 393 loss=4.921, ppl=30.3, wps=14475, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=22400, lr=0.000211289, gnorm=0.725, loss_scale=8, train_wall=439, gb_free=10.1, wall=100489
2022-03-05 00:41:54 | INFO | train_inner | epoch 058:    140 / 393 loss=4.873, ppl=29.3, wps=14684.3, ups=0.22, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.712, loss_scale=8, train_wall=441, gb_free=10.1, wall=100935
2022-03-05 00:49:20 | INFO | train_inner | epoch 058:    240 / 393 loss=4.915, ppl=30.16, wps=14688.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.758, loss_scale=8, train_wall=441, gb_free=10.1, wall=101381
2022-03-05 00:56:46 | INFO | train_inner | epoch 058:    340 / 393 loss=4.949, ppl=30.88, wps=14693.1, ups=0.22, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.725, loss_scale=8, train_wall=441, gb_free=10.1, wall=101827
2022-03-05 01:00:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:00:47 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.627 | ppl 197.68 | wps 33942.3 | wpb 2034.1 | bsz 4 | num_updates 22753 | best_loss 6.921
2022-03-05 01:00:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 22753 updates
2022-03-05 01:00:47 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-05 01:00:47 | INFO | train | epoch 058 | loss 4.915 | ppl 30.16 | wps 14634 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 22753 | lr 0.000209643 | gnorm 0.729 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 102068
2022-03-05 01:00:47 | INFO | fairseq.trainer | begin training epoch 59
2022-03-05 01:00:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:04:17 | INFO | train_inner | epoch 059:     47 / 393 loss=4.914, ppl=30.15, wps=14472.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=22800, lr=0.000209427, gnorm=0.739, loss_scale=8, train_wall=439, gb_free=10.1, wall=102278
2022-03-05 01:05:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:11:48 | INFO | train_inner | epoch 059:    148 / 393 loss=4.864, ppl=29.12, wps=14538.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.743, loss_scale=8, train_wall=446, gb_free=10.1, wall=102729
2022-03-05 01:19:14 | INFO | train_inner | epoch 059:    248 / 393 loss=4.904, ppl=29.94, wps=14681.6, ups=0.22, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.723, loss_scale=8, train_wall=441, gb_free=10.1, wall=103175
2022-03-05 01:26:41 | INFO | train_inner | epoch 059:    348 / 393 loss=4.939, ppl=30.67, wps=14682.4, ups=0.22, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.747, loss_scale=8, train_wall=441, gb_free=10.1, wall=103622
2022-03-05 01:30:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:30:06 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.631 | ppl 198.25 | wps 33848.1 | wpb 2034.1 | bsz 4 | num_updates 23145 | best_loss 6.921
2022-03-05 01:30:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 23145 updates
2022-03-05 01:30:06 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-05 01:30:06 | INFO | train | epoch 059 | loss 4.903 | ppl 29.92 | wps 14590.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 23145 | lr 0.00020786 | gnorm 0.738 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 103827
2022-03-05 01:30:06 | INFO | fairseq.trainer | begin training epoch 60
2022-03-05 01:30:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:34:12 | INFO | train_inner | epoch 060:     55 / 393 loss=4.889, ppl=29.62, wps=14469.9, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=23200, lr=0.000207614, gnorm=0.732, loss_scale=8, train_wall=439, gb_free=10.1, wall=104073
2022-03-05 01:41:38 | INFO | train_inner | epoch 060:    155 / 393 loss=4.86, ppl=29.03, wps=14689, ups=0.22, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.735, loss_scale=8, train_wall=441, gb_free=10.1, wall=104519
2022-03-05 01:44:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:49:08 | INFO | train_inner | epoch 060:    256 / 393 loss=4.9, ppl=29.86, wps=14543.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.734, loss_scale=8, train_wall=446, gb_free=10.1, wall=104969
2022-03-05 01:56:35 | INFO | train_inner | epoch 060:    356 / 393 loss=4.93, ppl=30.48, wps=14685.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.741, loss_scale=8, train_wall=441, gb_free=10.1, wall=105416
2022-03-05 01:59:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:59:24 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.661 | ppl 202.44 | wps 33835.6 | wpb 2034.1 | bsz 4 | num_updates 23537 | best_loss 6.921
2022-03-05 01:59:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 23537 updates
2022-03-05 01:59:24 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-05 01:59:24 | INFO | train | epoch 060 | loss 4.892 | ppl 29.7 | wps 14595 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 23537 | lr 0.000206122 | gnorm 0.736 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 105585
2022-03-05 01:59:24 | INFO | fairseq.trainer | begin training epoch 61
2022-03-05 01:59:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:04:05 | INFO | train_inner | epoch 061:     63 / 393 loss=4.873, ppl=29.3, wps=14472.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=23600, lr=0.000205847, gnorm=0.738, loss_scale=8, train_wall=439, gb_free=10.1, wall=105866
2022-03-05 02:11:32 | INFO | train_inner | epoch 061:    163 / 393 loss=4.852, ppl=28.89, wps=14686.2, ups=0.22, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.733, loss_scale=8, train_wall=441, gb_free=10.1, wall=106313
2022-03-05 02:18:58 | INFO | train_inner | epoch 061:    263 / 393 loss=4.892, ppl=29.69, wps=14684.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.744, loss_scale=8, train_wall=441, gb_free=10.1, wall=106759
2022-03-05 02:24:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:26:29 | INFO | train_inner | epoch 061:    364 / 393 loss=4.926, ppl=30.4, wps=14539.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.736, loss_scale=8, train_wall=446, gb_free=10.1, wall=107210
2022-03-05 02:28:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:28:43 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.646 | ppl 200.3 | wps 33855.2 | wpb 2034.1 | bsz 4 | num_updates 23929 | best_loss 6.921
2022-03-05 02:28:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 23929 updates
2022-03-05 02:28:43 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-05 02:28:43 | INFO | train | epoch 061 | loss 4.883 | ppl 29.5 | wps 14593.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 23929 | lr 0.000204427 | gnorm 0.738 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 107344
2022-03-05 02:28:43 | INFO | fairseq.trainer | begin training epoch 62
2022-03-05 02:28:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:33:59 | INFO | train_inner | epoch 062:     71 / 393 loss=4.845, ppl=28.75, wps=14474.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=24000, lr=0.000204124, gnorm=0.713, loss_scale=8, train_wall=439, gb_free=10.1, wall=107661
2022-03-05 02:41:26 | INFO | train_inner | epoch 062:    171 / 393 loss=4.85, ppl=28.84, wps=14690.4, ups=0.22, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.742, loss_scale=8, train_wall=441, gb_free=10.1, wall=108107
2022-03-05 02:48:52 | INFO | train_inner | epoch 062:    271 / 393 loss=4.882, ppl=29.48, wps=14685.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.745, loss_scale=8, train_wall=441, gb_free=10.1, wall=108553
2022-03-05 02:56:18 | INFO | train_inner | epoch 062:    371 / 393 loss=4.92, ppl=30.28, wps=14685.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.736, loss_scale=8, train_wall=441, gb_free=10.1, wall=108999
2022-03-05 02:57:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:58:01 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 7.682 | ppl 205.39 | wps 33869.2 | wpb 2034.1 | bsz 4 | num_updates 24322 | best_loss 6.921
2022-03-05 02:58:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 24322 updates
2022-03-05 02:58:01 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-05 02:58:01 | INFO | train | epoch 062 | loss 4.873 | ppl 29.31 | wps 14631.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 24322 | lr 0.000202768 | gnorm 0.738 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 109102
2022-03-05 02:58:01 | INFO | fairseq.trainer | begin training epoch 63
2022-03-05 02:58:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:03:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:03:53 | INFO | train_inner | epoch 063:     79 / 393 loss=4.837, ppl=28.59, wps=14335.4, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=24400, lr=0.000202444, gnorm=0.733, loss_scale=8, train_wall=444, gb_free=10.1, wall=109454
2022-03-05 03:11:19 | INFO | train_inner | epoch 063:    179 / 393 loss=4.834, ppl=28.53, wps=14686.4, ups=0.22, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.744, loss_scale=8, train_wall=441, gb_free=10.1, wall=109900
2022-03-05 03:18:46 | INFO | train_inner | epoch 063:    279 / 393 loss=4.877, ppl=29.38, wps=14686.3, ups=0.22, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.777, loss_scale=8, train_wall=441, gb_free=10.1, wall=110347
2022-03-05 03:26:12 | INFO | train_inner | epoch 063:    379 / 393 loss=4.911, ppl=30.08, wps=14687.7, ups=0.22, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.728, loss_scale=8, train_wall=441, gb_free=10.1, wall=110793
2022-03-05 03:27:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:27:19 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 7.684 | ppl 205.64 | wps 33804.8 | wpb 2034.1 | bsz 4 | num_updates 24714 | best_loss 6.921
2022-03-05 03:27:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 24714 updates
2022-03-05 03:27:19 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-05 03:27:19 | INFO | train | epoch 063 | loss 4.863 | ppl 29.1 | wps 14595.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 24714 | lr 0.000201154 | gnorm 0.742 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 110860
2022-03-05 03:27:19 | INFO | fairseq.trainer | begin training epoch 64
2022-03-05 03:27:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:33:43 | INFO | train_inner | epoch 064:     86 / 393 loss=4.815, ppl=28.14, wps=14470.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=24800, lr=0.000200805, gnorm=0.746, loss_scale=8, train_wall=439, gb_free=10.1, wall=111244
2022-03-05 03:41:09 | INFO | train_inner | epoch 064:    186 / 393 loss=4.837, ppl=28.58, wps=14685.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.733, loss_scale=8, train_wall=441, gb_free=10.1, wall=111690
2022-03-05 03:42:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:48:40 | INFO | train_inner | epoch 064:    287 / 393 loss=4.876, ppl=29.36, wps=14539.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.753, loss_scale=8, train_wall=446, gb_free=10.1, wall=112141
2022-03-05 03:56:06 | INFO | train_inner | epoch 064:    387 / 393 loss=4.896, ppl=29.77, wps=14686, ups=0.22, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.758, loss_scale=8, train_wall=441, gb_free=10.1, wall=112587
2022-03-05 03:56:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-05 03:56:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:56:37 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 7.697 | ppl 207.45 | wps 34000.3 | wpb 2034.1 | bsz 4 | num_updates 25105 | best_loss 6.921
2022-03-05 03:56:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 25105 updates
2022-03-05 03:56:37 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-05 03:56:37 | INFO | train | epoch 064 | loss 4.854 | ppl 28.91 | wps 14556.7 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 25105 | lr 0.000199581 | gnorm 0.752 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 112618
2022-03-05 03:56:37 | INFO | fairseq.trainer | begin training epoch 65
2022-03-05 03:56:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:03:41 | INFO | train_inner | epoch 065:     95 / 393 loss=4.792, ppl=27.71, wps=14336.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=25200, lr=0.000199205, gnorm=0.762, loss_scale=4, train_wall=444, gb_free=10.1, wall=113042
2022-03-05 04:11:07 | INFO | train_inner | epoch 065:    195 / 393 loss=4.832, ppl=28.47, wps=14690.4, ups=0.22, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.735, loss_scale=4, train_wall=441, gb_free=10.1, wall=113488
2022-03-05 04:18:33 | INFO | train_inner | epoch 065:    295 / 393 loss=4.859, ppl=29.03, wps=14694.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.741, loss_scale=4, train_wall=441, gb_free=10.1, wall=113934
2022-03-05 04:25:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:25:55 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 7.688 | ppl 206.18 | wps 33962.5 | wpb 2034.1 | bsz 4 | num_updates 25498 | best_loss 6.921
2022-03-05 04:25:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 25498 updates
2022-03-05 04:25:55 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-05 04:25:55 | INFO | train | epoch 065 | loss 4.845 | ppl 28.74 | wps 14636.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 25498 | lr 0.000198037 | gnorm 0.745 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 114376
2022-03-05 04:25:55 | INFO | fairseq.trainer | begin training epoch 66
2022-03-05 04:25:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:26:04 | INFO | train_inner | epoch 066:      2 / 393 loss=4.899, ppl=29.83, wps=14477.4, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=25500, lr=0.00019803, gnorm=0.754, loss_scale=4, train_wall=439, gb_free=10.1, wall=114385
2022-03-05 04:33:30 | INFO | train_inner | epoch 066:    102 / 393 loss=4.785, ppl=27.57, wps=14695.9, ups=0.22, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.734, loss_scale=4, train_wall=441, gb_free=10.1, wall=114831
2022-03-05 04:40:56 | INFO | train_inner | epoch 066:    202 / 393 loss=4.822, ppl=28.28, wps=14685.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.771, loss_scale=8, train_wall=441, gb_free=10.1, wall=115277
2022-03-05 04:48:22 | INFO | train_inner | epoch 066:    302 / 393 loss=4.854, ppl=28.92, wps=14681.7, ups=0.22, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.737, loss_scale=8, train_wall=441, gb_free=10.1, wall=115723
2022-03-05 04:55:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:55:13 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 7.72 | ppl 210.8 | wps 34013.4 | wpb 2034.1 | bsz 4 | num_updates 25891 | best_loss 6.921
2022-03-05 04:55:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 25891 updates
2022-03-05 04:55:13 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-05 04:55:13 | INFO | train | epoch 066 | loss 4.835 | ppl 28.54 | wps 14633.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 25891 | lr 0.000196529 | gnorm 0.754 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 116134
2022-03-05 04:55:13 | INFO | fairseq.trainer | begin training epoch 67
2022-03-05 04:55:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:55:53 | INFO | train_inner | epoch 067:      9 / 393 loss=4.873, ppl=29.31, wps=14477, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=25900, lr=0.000196494, gnorm=0.78, loss_scale=8, train_wall=439, gb_free=10.1, wall=116174
2022-03-05 05:03:19 | INFO | train_inner | epoch 067:    109 / 393 loss=4.777, ppl=27.43, wps=14689.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.737, loss_scale=8, train_wall=441, gb_free=10.1, wall=116620
2022-03-05 05:10:46 | INFO | train_inner | epoch 067:    209 / 393 loss=4.817, ppl=28.18, wps=14684.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.755, loss_scale=8, train_wall=441, gb_free=10.1, wall=117067
2022-03-05 05:13:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:18:16 | INFO | train_inner | epoch 067:    310 / 393 loss=4.849, ppl=28.81, wps=14542.2, ups=0.22, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.744, loss_scale=8, train_wall=446, gb_free=10.1, wall=117517
2022-03-05 05:23:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-05 05:24:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:24:31 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 7.73 | ppl 212.31 | wps 33983.6 | wpb 2034.1 | bsz 4 | num_updates 26282 | best_loss 6.921
2022-03-05 05:24:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 26282 updates
2022-03-05 05:24:31 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-05 05:24:31 | INFO | train | epoch 067 | loss 4.825 | ppl 28.35 | wps 14559 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 26282 | lr 0.000195061 | gnorm 0.754 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 117892
2022-03-05 05:24:31 | INFO | fairseq.trainer | begin training epoch 68
2022-03-05 05:24:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:25:51 | INFO | train_inner | epoch 068:     18 / 393 loss=4.847, ppl=28.78, wps=14334.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=26300, lr=0.000194994, gnorm=0.771, loss_scale=4, train_wall=444, gb_free=10.1, wall=117972
2022-03-05 05:33:17 | INFO | train_inner | epoch 068:    118 / 393 loss=4.768, ppl=27.25, wps=14693.1, ups=0.22, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.744, loss_scale=4, train_wall=441, gb_free=10.1, wall=118418
2022-03-05 05:40:44 | INFO | train_inner | epoch 068:    218 / 393 loss=4.809, ppl=28.03, wps=14690.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.759, loss_scale=4, train_wall=441, gb_free=10.1, wall=118865
2022-03-05 05:48:09 | INFO | train_inner | epoch 068:    318 / 393 loss=4.85, ppl=28.84, wps=14696.4, ups=0.22, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.755, loss_scale=4, train_wall=441, gb_free=10.1, wall=119310
2022-03-05 05:53:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:53:48 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 7.732 | ppl 212.6 | wps 33880.7 | wpb 2034.1 | bsz 4 | num_updates 26675 | best_loss 6.921
2022-03-05 05:53:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 26675 updates
2022-03-05 05:53:48 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-05 05:53:48 | INFO | train | epoch 068 | loss 4.817 | ppl 28.2 | wps 14638.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 26675 | lr 0.000193619 | gnorm 0.75 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 119649
2022-03-05 05:53:48 | INFO | fairseq.trainer | begin training epoch 69
2022-03-05 05:53:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:55:40 | INFO | train_inner | epoch 069:     25 / 393 loss=4.84, ppl=28.64, wps=14482.7, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=26700, lr=0.000193528, gnorm=0.743, loss_scale=4, train_wall=439, gb_free=10.1, wall=119761
2022-03-05 06:03:06 | INFO | train_inner | epoch 069:    125 / 393 loss=4.762, ppl=27.14, wps=14693, ups=0.22, wpb=65530.9, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.755, loss_scale=8, train_wall=441, gb_free=10.1, wall=120207
2022-03-05 06:10:32 | INFO | train_inner | epoch 069:    225 / 393 loss=4.801, ppl=27.88, wps=14687.1, ups=0.22, wpb=65536, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.73, loss_scale=8, train_wall=441, gb_free=10.1, wall=120653
2022-03-05 06:17:58 | INFO | train_inner | epoch 069:    325 / 393 loss=4.84, ppl=28.63, wps=14687.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.784, loss_scale=8, train_wall=441, gb_free=10.1, wall=121099
2022-03-05 06:23:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:23:06 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 7.752 | ppl 215.52 | wps 33970.8 | wpb 2034.1 | bsz 4 | num_updates 27068 | best_loss 6.921
2022-03-05 06:23:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 27068 updates
2022-03-05 06:23:06 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-05 06:23:06 | INFO | train | epoch 069 | loss 4.809 | ppl 28.02 | wps 14635 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 27068 | lr 0.000192208 | gnorm 0.757 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 121407
2022-03-05 06:23:06 | INFO | fairseq.trainer | begin training epoch 70
2022-03-05 06:23:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:25:29 | INFO | train_inner | epoch 070:     32 / 393 loss=4.816, ppl=28.17, wps=14475.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=27100, lr=0.000192095, gnorm=0.77, loss_scale=8, train_wall=439, gb_free=10.1, wall=121550
2022-03-05 06:32:55 | INFO | train_inner | epoch 070:    132 / 393 loss=4.76, ppl=27.1, wps=14690.2, ups=0.22, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.75, loss_scale=8, train_wall=441, gb_free=10.1, wall=121996
2022-03-05 06:40:22 | INFO | train_inner | epoch 070:    232 / 393 loss=4.801, ppl=27.87, wps=14685.3, ups=0.22, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.752, loss_scale=16, train_wall=441, gb_free=10.1, wall=122443
2022-03-05 06:41:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:47:52 | INFO | train_inner | epoch 070:    333 / 393 loss=4.836, ppl=28.56, wps=14541, ups=0.22, wpb=65535.4, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.762, loss_scale=8, train_wall=446, gb_free=10.1, wall=122893
2022-03-05 06:52:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-05 06:52:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:52:24 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 7.735 | ppl 212.97 | wps 33872.6 | wpb 2034.1 | bsz 4 | num_updates 27459 | best_loss 6.921
2022-03-05 06:52:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 27459 updates
2022-03-05 06:52:24 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-05 06:52:24 | INFO | train | epoch 070 | loss 4.8 | ppl 27.86 | wps 14557.7 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 27459 | lr 0.000190835 | gnorm 0.756 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 123166
2022-03-05 06:52:24 | INFO | fairseq.trainer | begin training epoch 71
2022-03-05 06:52:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:55:27 | INFO | train_inner | epoch 071:     41 / 393 loss=4.799, ppl=27.83, wps=14332.1, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=27500, lr=0.000190693, gnorm=0.758, loss_scale=4, train_wall=444, gb_free=10.1, wall=123349
2022-03-05 07:02:53 | INFO | train_inner | epoch 071:    141 / 393 loss=4.761, ppl=27.11, wps=14692.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.744, loss_scale=4, train_wall=441, gb_free=10.1, wall=123795
2022-03-05 07:10:20 | INFO | train_inner | epoch 071:    241 / 393 loss=4.796, ppl=27.77, wps=14693.3, ups=0.22, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.745, loss_scale=4, train_wall=441, gb_free=10.1, wall=124241
2022-03-05 07:17:45 | INFO | train_inner | epoch 071:    341 / 393 loss=4.82, ppl=28.25, wps=14695, ups=0.22, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.768, loss_scale=4, train_wall=441, gb_free=10.1, wall=124687
2022-03-05 07:21:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:21:42 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 7.781 | ppl 219.98 | wps 33913.1 | wpb 2034.1 | bsz 4 | num_updates 27852 | best_loss 6.921
2022-03-05 07:21:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 27852 updates
2022-03-05 07:21:42 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-05 07:21:42 | INFO | train | epoch 071 | loss 4.793 | ppl 27.71 | wps 14638.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 27852 | lr 0.000189484 | gnorm 0.758 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 124923
2022-03-05 07:21:42 | INFO | fairseq.trainer | begin training epoch 72
2022-03-05 07:21:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:25:16 | INFO | train_inner | epoch 072:     48 / 393 loss=4.792, ppl=27.69, wps=14479.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=27900, lr=0.000189321, gnorm=0.783, loss_scale=4, train_wall=439, gb_free=10.1, wall=125137
2022-03-05 07:32:42 | INFO | train_inner | epoch 072:    148 / 393 loss=4.75, ppl=26.91, wps=14686.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.764, loss_scale=8, train_wall=441, gb_free=10.1, wall=125583
2022-03-05 07:40:09 | INFO | train_inner | epoch 072:    248 / 393 loss=4.785, ppl=27.57, wps=14688.3, ups=0.22, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.774, loss_scale=8, train_wall=441, gb_free=10.1, wall=126030
2022-03-05 07:47:35 | INFO | train_inner | epoch 072:    348 / 393 loss=4.82, ppl=28.25, wps=14684.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.75, loss_scale=8, train_wall=441, gb_free=10.1, wall=126476
2022-03-05 07:50:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:51:00 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 7.791 | ppl 221.45 | wps 33813.9 | wpb 2034.1 | bsz 4 | num_updates 28245 | best_loss 6.921
2022-03-05 07:51:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 28245 updates
2022-03-05 07:51:00 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-05 07:51:00 | INFO | train | epoch 072 | loss 4.785 | ppl 27.56 | wps 14632.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 28245 | lr 0.000188161 | gnorm 0.768 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 126681
2022-03-05 07:51:00 | INFO | fairseq.trainer | begin training epoch 73
2022-03-05 07:51:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:52:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-05 07:55:10 | INFO | train_inner | epoch 073:     56 / 393 loss=4.772, ppl=27.32, wps=14336.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=28300, lr=0.000187978, gnorm=0.786, loss_scale=4, train_wall=444, gb_free=10.1, wall=126931
2022-03-05 08:02:36 | INFO | train_inner | epoch 073:    156 / 393 loss=4.735, ppl=26.63, wps=14692, ups=0.22, wpb=65535.4, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.769, loss_scale=4, train_wall=441, gb_free=10.1, wall=127377
2022-03-05 08:10:02 | INFO | train_inner | epoch 073:    256 / 393 loss=4.787, ppl=27.6, wps=14688.2, ups=0.22, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.764, loss_scale=4, train_wall=441, gb_free=10.1, wall=127823
2022-03-05 08:17:28 | INFO | train_inner | epoch 073:    356 / 393 loss=4.821, ppl=28.27, wps=14691.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.777, loss_scale=4, train_wall=441, gb_free=10.1, wall=128269
2022-03-05 08:20:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:20:18 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 7.792 | ppl 221.69 | wps 33914.8 | wpb 2034.1 | bsz 4 | num_updates 28637 | best_loss 6.921
2022-03-05 08:20:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 28637 updates
2022-03-05 08:20:18 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-05 08:20:18 | INFO | train | epoch 073 | loss 4.776 | ppl 27.4 | wps 14599.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 28637 | lr 0.000186869 | gnorm 0.77 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 128439
2022-03-05 08:20:18 | INFO | fairseq.trainer | begin training epoch 74
2022-03-05 08:20:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:24:59 | INFO | train_inner | epoch 074:     63 / 393 loss=4.755, ppl=26.99, wps=14482.7, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=28700, lr=0.000186663, gnorm=0.77, loss_scale=4, train_wall=439, gb_free=10.1, wall=128720
2022-03-05 08:32:25 | INFO | train_inner | epoch 074:    163 / 393 loss=4.738, ppl=26.68, wps=14692.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.757, loss_scale=8, train_wall=441, gb_free=10.1, wall=129166
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/jelinek_mercer.py", line 99, in forward
    loss, _ = self.compute_loss(model, net_output, sample, reduce=reduce)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/jelinek_mercer.py", line 113, in compute_loss
    lprobs = model.get_normalized_probs(net_output, log_probs=True)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 69, in get_normalized_probs
    return self.get_normalized_probs_scriptable(net_output, log_probs, sample)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 83, in get_normalized_probs_scriptable
    return self.decoder.get_normalized_probs(net_output, log_probs, sample)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_decoder.py", line 67, in get_normalized_probs
    return self.get_normalized_probs_scriptable(net_output, log_probs, sample)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_decoder.py", line 92, in get_normalized_probs_scriptable
    return utils.log_softmax(logits, dim=-1, onnx_trace=self.onnx_trace)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/utils.py", line 521, in log_softmax
    return F.log_softmax(x, dim=dim, dtype=torch.float32)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 1607, in log_softmax
    ret = input.log_softmax(dim, dtype=dtype)
KeyboardInterrupt
