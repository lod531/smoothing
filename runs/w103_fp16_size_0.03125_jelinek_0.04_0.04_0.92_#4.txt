Sender: LSF System <lsfadmin@eu-lo-g2-010>
Subject: Job 207540445: <w103_fp16_size_0.03125_jelinek_0.04_0.04_0.92_#4> in cluster <euler> Exited

Job <w103_fp16_size_0.03125_jelinek_0.04_0.04_0.92_#4> was submitted from host <eu-login-13> by user <andriusb> in cluster <euler> at Tue Mar  8 11:14:29 2022
Job was executed on host(s) <eu-lo-g2-010>, in queue <gpu.120h>, as user <andriusb> in cluster <euler> at Tue Mar  8 11:14:53 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar  8 11:14:53 2022
Terminated at Tue Mar  8 16:28:06 2022
Results reported at Tue Mar  8 16:28:06 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.03125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.04, 0.04, 0.92)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --no-epoch-checkpoints --no-last-checkpoints --seed 66575624 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   19478.05 sec.
    Max Memory :                                 5909 MB
    Average Memory :                             3152.61 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               14091.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   18796 sec.
    Turnaround time :                            18817 sec.

The output (if any) follows:

2022-03-08 11:15:22 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575624, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.03125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575624, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.04, 0.04, 0.92)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-08 11:15:22 | INFO | fairseq.tasks.language_modeling | dictionary: 96056 types
2022-03-08 11:15:24 | INFO | fairseq.data.data_utils | loaded 56,292 examples from: data-bin/wikitext-103-raw-size-0.03125/train
Calculating frequency stats:
  0%|          | 0/56292 [00:00<?, ?it/s]  1%|          | 615/56292 [00:00<00:09, 6135.52it/s]  2%|▏         | 1229/56292 [00:00<00:09, 5895.19it/s]  3%|▎         | 1820/56292 [00:00<00:09, 5699.77it/s]  4%|▍         | 2391/56292 [00:00<00:09, 5569.80it/s]  6%|▌         | 3167/56292 [00:00<00:08, 6326.21it/s]  7%|▋         | 3803/56292 [00:00<00:08, 6109.34it/s]  8%|▊         | 4536/56292 [00:00<00:07, 6491.71it/s]  9%|▉         | 5251/56292 [00:00<00:07, 6688.53it/s] 11%|█         | 5960/56292 [00:00<00:07, 6807.89it/s] 12%|█▏        | 6644/56292 [00:01<00:07, 6241.28it/s] 13%|█▎        | 7279/56292 [00:01<00:07, 6263.41it/s] 14%|█▍        | 7913/56292 [00:01<00:07, 6205.91it/s] 15%|█▌        | 8539/56292 [00:01<00:07, 5974.83it/s] 16%|█▋        | 9187/56292 [00:01<00:07, 6116.10it/s] 17%|█▋        | 9845/56292 [00:01<00:07, 6245.86it/s] 19%|█▊        | 10473/56292 [00:01<00:07, 6218.58it/s] 20%|█▉        | 11098/56292 [00:01<00:07, 6112.78it/s] 21%|██        | 11712/56292 [00:01<00:07, 6005.31it/s] 22%|██▏       | 12364/56292 [00:01<00:07, 6142.62it/s] 23%|██▎       | 12996/56292 [00:02<00:06, 6189.06it/s] 24%|██▍       | 13674/56292 [00:02<00:06, 6362.78it/s] 25%|██▌       | 14312/56292 [00:02<00:06, 6360.14it/s] 27%|██▋       | 14949/56292 [00:02<00:06, 6357.65it/s] 28%|██▊       | 15586/56292 [00:02<00:06, 6277.29it/s] 29%|██▉       | 16215/56292 [00:02<00:06, 6067.93it/s] 30%|██▉       | 16824/56292 [00:02<00:06, 6058.21it/s] 31%|███       | 17489/56292 [00:02<00:06, 6227.28it/s] 32%|███▏      | 18113/56292 [00:02<00:06, 6122.05it/s] 33%|███▎      | 18741/56292 [00:03<00:06, 6160.83it/s] 35%|███▍      | 19528/56292 [00:03<00:05, 6663.56it/s] 36%|███▌      | 20196/56292 [00:03<00:05, 6326.48it/s] 37%|███▋      | 20833/56292 [00:03<00:05, 6246.23it/s] 38%|███▊      | 21461/56292 [00:03<00:05, 5966.55it/s] 39%|███▉      | 22113/56292 [00:03<00:05, 6119.28it/s] 41%|████      | 22799/56292 [00:03<00:05, 6327.69it/s] 42%|████▏     | 23438/56292 [00:03<00:05, 6343.99it/s] 43%|████▎     | 24217/56292 [00:03<00:04, 6762.93it/s] 44%|████▍     | 24944/56292 [00:03<00:04, 6908.84it/s] 46%|████▌     | 25637/56292 [00:04<00:04, 6795.71it/s] 47%|████▋     | 26319/56292 [00:04<00:04, 6385.00it/s] 48%|████▊     | 26964/56292 [00:04<00:04, 6231.05it/s] 49%|████▉     | 27592/56292 [00:04<00:04, 6032.71it/s] 50%|█████     | 28247/56292 [00:04<00:04, 6174.44it/s] 51%|█████▏    | 28925/56292 [00:04<00:04, 6343.76it/s] 53%|█████▎    | 29563/56292 [00:04<00:04, 6243.04it/s] 54%|█████▎    | 30190/56292 [00:04<00:04, 6249.47it/s] 55%|█████▍    | 30817/56292 [00:04<00:04, 5929.87it/s] 56%|█████▌    | 31437/56292 [00:05<00:04, 6005.90it/s] 57%|█████▋    | 32041/56292 [00:05<00:04, 5880.71it/s] 58%|█████▊    | 32652/56292 [00:05<00:03, 5945.75it/s] 59%|█████▉    | 33249/56292 [00:05<00:03, 5798.53it/s] 60%|██████    | 33831/56292 [00:05<00:03, 5708.55it/s] 61%|██████    | 34478/56292 [00:05<00:03, 5927.54it/s] 62%|██████▏   | 35096/56292 [00:05<00:03, 5999.73it/s] 63%|██████▎   | 35698/56292 [00:05<00:03, 5990.26it/s] 65%|██████▍   | 36344/56292 [00:05<00:03, 6127.08it/s] 66%|██████▌   | 36958/56292 [00:05<00:03, 5930.16it/s] 67%|██████▋   | 37553/56292 [00:06<00:03, 5747.36it/s] 68%|██████▊   | 38176/56292 [00:06<00:03, 5885.18it/s] 69%|██████▉   | 38770/56292 [00:06<00:02, 5901.02it/s] 70%|██████▉   | 39362/56292 [00:06<00:02, 5900.18it/s] 71%|███████   | 40003/56292 [00:06<00:02, 6050.36it/s] 72%|███████▏  | 40703/56292 [00:06<00:02, 6329.56it/s] 73%|███████▎  | 41337/56292 [00:06<00:02, 6310.65it/s] 75%|███████▍  | 41969/56292 [00:06<00:02, 5818.94it/s] 76%|███████▌  | 42559/56292 [00:06<00:02, 5790.98it/s] 77%|███████▋  | 43144/56292 [00:07<00:02, 5740.05it/s] 78%|███████▊  | 43840/56292 [00:07<00:02, 6084.07it/s] 79%|███████▉  | 44453/56292 [00:07<00:01, 6060.62it/s] 80%|████████  | 45070/56292 [00:07<00:01, 6086.58it/s] 81%|████████  | 45718/56292 [00:07<00:01, 6195.29it/s] 83%|████████▎ | 46457/56292 [00:07<00:01, 6545.23it/s] 84%|████████▎ | 47114/56292 [00:07<00:01, 6444.05it/s] 85%|████████▌ | 48079/56292 [00:07<00:01, 7386.76it/s] 87%|████████▋ | 48821/56292 [00:07<00:01, 7156.58it/s] 88%|████████▊ | 49584/56292 [00:07<00:00, 7293.03it/s] 89%|████████▉ | 50317/56292 [00:08<00:00, 6692.09it/s] 91%|█████████ | 50998/56292 [00:08<00:00, 6470.62it/s] 92%|█████████▏| 51653/56292 [00:08<00:00, 6456.90it/s] 93%|█████████▎| 52487/56292 [00:08<00:00, 6986.84it/s] 94%|█████████▍| 53193/56292 [00:08<00:00, 6736.43it/s] 96%|█████████▌| 53873/56292 [00:08<00:00, 6523.99it/s] 97%|█████████▋| 54531/56292 [00:08<00:00, 6080.31it/s] 98%|█████████▊| 55181/56292 [00:08<00:00, 6189.06it/s] 99%|█████████▉| 55807/56292 [00:08<00:00, 6206.42it/s]100%|██████████| 56292/56292 [00:09<00:00, 6234.14it/s]

gathering stats for n=1
  0%|          | 0/56292 [00:00<?, ?it/s]  4%|▍         | 2227/56292 [00:00<00:02, 22255.38it/s]  8%|▊         | 4782/56292 [00:00<00:02, 24190.54it/s] 13%|█▎        | 7202/56292 [00:00<00:02, 23789.27it/s] 17%|█▋        | 9582/56292 [00:00<00:02, 23309.18it/s] 21%|██        | 11915/56292 [00:00<00:01, 23272.38it/s] 25%|██▌       | 14244/56292 [00:00<00:01, 23276.23it/s] 29%|██▉       | 16573/56292 [00:00<00:01, 23144.16it/s] 34%|███▍      | 19013/56292 [00:00<00:01, 23530.59it/s] 38%|███▊      | 21367/56292 [00:00<00:01, 23194.31it/s] 42%|████▏     | 23855/56292 [00:01<00:01, 23706.17it/s] 47%|████▋     | 26283/56292 [00:01<00:01, 23876.49it/s] 51%|█████     | 28672/56292 [00:01<00:01, 23692.94it/s] 55%|█████▌    | 31043/56292 [00:01<00:01, 23072.83it/s] 59%|█████▉    | 33355/56292 [00:01<00:01, 22726.42it/s] 63%|██████▎   | 35707/56292 [00:01<00:00, 22956.26it/s] 68%|██████▊   | 38006/56292 [00:01<00:00, 22607.09it/s] 72%|███████▏  | 40366/56292 [00:01<00:00, 22897.69it/s] 76%|███████▌  | 42659/56292 [00:01<00:00, 22568.88it/s] 80%|███████▉  | 44992/56292 [00:01<00:00, 22791.34it/s] 85%|████████▍ | 47609/56292 [00:02<00:00, 23785.90it/s] 89%|████████▉ | 50104/56292 [00:02<00:00, 24129.39it/s] 93%|█████████▎| 52549/56292 [00:02<00:00, 24224.00it/s] 98%|█████████▊| 54974/56292 [00:02<00:00, 23535.49it/s]100%|██████████| 56292/56292 [00:02<00:00, 23384.17it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 122.44it/s]2022-03-08 11:15:41 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(96056, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=96056, bias=False)
  )
)
2022-03-08 11:15:41 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-08 11:15:41 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-08 11:15:41 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-08 11:15:41 | INFO | fairseq_cli.train | num. shared model params: 68,094,976 (num. trained: 68,094,976)
2022-03-08 11:15:41 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-08 11:15:41 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.03125/valid
2022-03-08 11:15:42 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-08 11:15:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-08 11:15:42 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-08 11:15:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-08 11:15:42 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-08 11:15:42 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-08 11:15:42 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_last.pt
2022-03-08 11:15:42 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_last.pt
2022-03-08 11:15:42 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-08 11:15:42 | INFO | fairseq.data.data_utils | loaded 56,292 examples from: data-bin/wikitext-103-raw-size-0.03125/train
2022-03-08 11:15:42 | INFO | fairseq.trainer | begin training epoch 1
2022-03-08 11:15:42 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-08 11:15:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-08 11:15:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 11:16:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 11:16:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 11:19:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:20:04 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 15.313 | ppl 40712.1 | wps 18670.1 | wpb 510.9 | bsz 1 | num_updates 45
2022-03-08 11:20:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 45 updates
2022-03-08 11:20:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:20:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:20:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 1 @ 45 updates, score 15.313) (writing took 2.8263323339633644 seconds)
2022-03-08 11:20:06 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-08 11:20:06 | INFO | train | epoch 001 | loss 16.552 | ppl 96079.9 | wps 12711.6 | ups 0.2 | wpb 64807.8 | bsz 126.6 | num_updates 45 | lr 5.72388e-06 | gnorm 5.433 | loss_scale 8 | train_wall 236 | gb_free 8.3 | wall 265
KL Stats: Epoch 1 Divergences: Uniform: 0.5599188628158661 Unigram: 4.103865072906033
2022-03-08 11:20:06 | INFO | fairseq.trainer | begin training epoch 2
2022-03-08 11:20:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:24:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:24:12 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.877 | ppl 15048.4 | wps 18781.8 | wpb 510.9 | bsz 1 | num_updates 94 | best_loss 13.877
2022-03-08 11:24:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 94 updates
2022-03-08 11:24:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:24:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:24:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 2 @ 94 updates, score 13.877) (writing took 2.7580699517857283 seconds)
2022-03-08 11:24:15 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-08 11:24:15 | INFO | train | epoch 002 | loss 14.56 | ppl 24146.1 | wps 12785.5 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 94 | lr 1.18477e-05 | gnorm 2.139 | loss_scale 8 | train_wall 220 | gb_free 8.3 | wall 513
KL Stats: Epoch 2 Divergences: Uniform: 0.5531828898638176 Unigram: 2.654830184000964
2022-03-08 11:24:15 | INFO | fairseq.trainer | begin training epoch 3
2022-03-08 11:24:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:24:44 | INFO | train_inner | epoch 003:      6 / 49 loss=15.419, ppl=43810.2, wps=12793.1, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=100, lr=1.25975e-05, gnorm=3.58, loss_scale=8, train_wall=483, gb_free=8.3, wall=542
2022-03-08 11:28:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:28:21 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 13.245 | ppl 9709.36 | wps 18779.6 | wpb 510.9 | bsz 1 | num_updates 143 | best_loss 13.245
2022-03-08 11:28:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 143 updates
2022-03-08 11:28:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:28:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:28:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 3 @ 143 updates, score 13.245) (writing took 2.8639833689667284 seconds)
2022-03-08 11:28:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-08 11:28:24 | INFO | train | epoch 003 | loss 13.677 | ppl 13097.1 | wps 12760.4 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 143 | lr 1.79714e-05 | gnorm 1.364 | loss_scale 16 | train_wall 220 | gb_free 8.3 | wall 762
KL Stats: Epoch 3 Divergences: Uniform: 0.5430411411012079 Unigram: 2.0325248445951853
2022-03-08 11:28:24 | INFO | fairseq.trainer | begin training epoch 4
2022-03-08 11:28:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:32:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:32:30 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 12.451 | ppl 5598.57 | wps 18853.2 | wpb 510.9 | bsz 1 | num_updates 192 | best_loss 12.451
2022-03-08 11:32:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 192 updates
2022-03-08 11:32:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:32:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:32:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 4 @ 192 updates, score 12.451) (writing took 2.82510979892686 seconds)
2022-03-08 11:32:32 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-08 11:32:32 | INFO | train | epoch 004 | loss 12.966 | ppl 8000.95 | wps 12796.3 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 192 | lr 2.40952e-05 | gnorm 1.176 | loss_scale 16 | train_wall 220 | gb_free 8.3 | wall 1011
KL Stats: Epoch 4 Divergences: Uniform: 0.5477300646538399 Unigram: 1.5932401997952417
2022-03-08 11:32:32 | INFO | fairseq.trainer | begin training epoch 5
2022-03-08 11:32:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:33:11 | INFO | train_inner | epoch 005:      8 / 49 loss=13.213, ppl=9492.79, wps=12794.9, ups=0.2, wpb=64876.2, bsz=126.7, num_updates=200, lr=2.5095e-05, gnorm=1.238, loss_scale=16, train_wall=449, gb_free=8.3, wall=1050
2022-03-08 11:36:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:36:38 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.723 | ppl 3380.52 | wps 18860.3 | wpb 510.9 | bsz 1 | num_updates 241 | best_loss 11.723
2022-03-08 11:36:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 241 updates
2022-03-08 11:36:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:36:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:36:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 5 @ 241 updates, score 11.723) (writing took 2.707207137020305 seconds)
2022-03-08 11:36:41 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-08 11:36:41 | INFO | train | epoch 005 | loss 12.147 | ppl 4535.83 | wps 12788.8 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 241 | lr 3.0219e-05 | gnorm 0.897 | loss_scale 16 | train_wall 220 | gb_free 8.3 | wall 1259
KL Stats: Epoch 5 Divergences: Uniform: 0.6457325630773275 Unigram: 1.1721200702171934
2022-03-08 11:36:41 | INFO | fairseq.trainer | begin training epoch 6
2022-03-08 11:36:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:40:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:40:47 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.144 | ppl 2263.23 | wps 18891.4 | wpb 510.9 | bsz 1 | num_updates 290 | best_loss 11.144
2022-03-08 11:40:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 290 updates
2022-03-08 11:40:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:40:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:40:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 6 @ 290 updates, score 11.144) (writing took 2.8685850820038468 seconds)
2022-03-08 11:40:50 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-08 11:40:50 | INFO | train | epoch 006 | loss 11.451 | ppl 2800.06 | wps 12782 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 290 | lr 3.63428e-05 | gnorm 0.705 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 1508
KL Stats: Epoch 6 Divergences: Uniform: 0.8207494196793629 Unigram: 0.7911918369759083
2022-03-08 11:40:50 | INFO | fairseq.trainer | begin training epoch 7
2022-03-08 11:40:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:41:38 | INFO | train_inner | epoch 007:     10 / 49 loss=11.676, ppl=3271.42, wps=12802.9, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=300, lr=3.75925e-05, gnorm=0.763, loss_scale=32, train_wall=448, gb_free=8.3, wall=1556
2022-03-08 11:44:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:44:55 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.78 | ppl 1758.4 | wps 18890.3 | wpb 510.9 | bsz 1 | num_updates 339 | best_loss 10.78
2022-03-08 11:44:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 339 updates
2022-03-08 11:44:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:44:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:44:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 7 @ 339 updates, score 10.78) (writing took 2.774759426014498 seconds)
2022-03-08 11:44:58 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-08 11:44:58 | INFO | train | epoch 007 | loss 10.943 | ppl 1968.16 | wps 12787.9 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 339 | lr 4.24665e-05 | gnorm 0.54 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 1756
KL Stats: Epoch 7 Divergences: Uniform: 1.084976886960802 Unigram: 0.552526712757709
2022-03-08 11:44:58 | INFO | fairseq.trainer | begin training epoch 8
2022-03-08 11:44:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:48:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:49:04 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.572 | ppl 1522.03 | wps 18876.7 | wpb 510.9 | bsz 1 | num_updates 388 | best_loss 10.572
2022-03-08 11:49:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 388 updates
2022-03-08 11:49:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:49:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:49:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 8 @ 388 updates, score 10.572) (writing took 2.888345666928217 seconds)
2022-03-08 11:49:07 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-08 11:49:07 | INFO | train | epoch 008 | loss 10.637 | ppl 1592.33 | wps 12759.3 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 388 | lr 4.85903e-05 | gnorm 0.453 | loss_scale 64 | train_wall 220 | gb_free 8.3 | wall 2005
KL Stats: Epoch 8 Divergences: Uniform: 1.3753591682022248 Unigram: 0.4701469003345013
2022-03-08 11:49:07 | INFO | fairseq.trainer | begin training epoch 9
2022-03-08 11:49:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:50:05 | INFO | train_inner | epoch 009:     12 / 49 loss=10.723, ppl=1689.84, wps=12787.7, ups=0.2, wpb=64867.4, bsz=126.7, num_updates=400, lr=5.009e-05, gnorm=0.481, loss_scale=64, train_wall=449, gb_free=8.3, wall=2063
2022-03-08 11:51:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 11:53:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:53:13 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.418 | ppl 1368.28 | wps 18820.7 | wpb 510.9 | bsz 1 | num_updates 436 | best_loss 10.418
2022-03-08 11:53:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 436 updates
2022-03-08 11:53:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:53:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:53:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 9 @ 436 updates, score 10.418) (writing took 2.9723457219079137 seconds)
2022-03-08 11:53:16 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-08 11:53:16 | INFO | train | epoch 009 | loss 10.446 | ppl 1394.74 | wps 12493.3 | ups 0.19 | wpb 64844.1 | bsz 126.7 | num_updates 436 | lr 5.45891e-05 | gnorm 0.447 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 2255
KL Stats: Epoch 9 Divergences: Uniform: 1.6097594800087962 Unigram: 0.5027322117508654
2022-03-08 11:53:16 | INFO | fairseq.trainer | begin training epoch 10
2022-03-08 11:53:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:57:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:57:23 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.284 | ppl 1247.2 | wps 18873.5 | wpb 510.9 | bsz 1 | num_updates 485 | best_loss 10.284
2022-03-08 11:57:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 485 updates
2022-03-08 11:57:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:57:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 11:57:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 10 @ 485 updates, score 10.284) (writing took 2.8160046411212534 seconds)
2022-03-08 11:57:25 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-08 11:57:25 | INFO | train | epoch 010 | loss 10.286 | ppl 1248.32 | wps 12757.6 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 485 | lr 6.07129e-05 | gnorm 0.482 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 2504
KL Stats: Epoch 10 Divergences: Uniform: 1.7628757041672547 Unigram: 0.5851397216111157
2022-03-08 11:57:25 | INFO | fairseq.trainer | begin training epoch 11
2022-03-08 11:57:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:58:38 | INFO | train_inner | epoch 011:     15 / 49 loss=10.321, ppl=1279.52, wps=12648, ups=0.19, wpb=64876.2, bsz=126.7, num_updates=500, lr=6.25875e-05, gnorm=0.474, loss_scale=32, train_wall=454, gb_free=8.3, wall=2576
2022-03-08 12:01:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:01:32 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.147 | ppl 1133.7 | wps 18802.5 | wpb 510.9 | bsz 1 | num_updates 534 | best_loss 10.147
2022-03-08 12:01:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 534 updates
2022-03-08 12:01:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:01:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:01:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 11 @ 534 updates, score 10.147) (writing took 2.685168912168592 seconds)
2022-03-08 12:01:35 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-08 12:01:35 | INFO | train | epoch 011 | loss 10.132 | ppl 1121.95 | wps 12738.7 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 534 | lr 6.68367e-05 | gnorm 0.526 | loss_scale 32 | train_wall 221 | gb_free 8.3 | wall 2753
KL Stats: Epoch 11 Divergences: Uniform: 1.8567618096808254 Unigram: 0.6883602921405325
2022-03-08 12:01:35 | INFO | fairseq.trainer | begin training epoch 12
2022-03-08 12:01:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:02:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 12:05:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:05:41 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.018 | ppl 1036.66 | wps 18834 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 10.018
2022-03-08 12:05:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 582 updates
2022-03-08 12:05:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:05:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:05:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 12 @ 582 updates, score 10.018) (writing took 3.010923549067229 seconds)
2022-03-08 12:05:44 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-08 12:05:44 | INFO | train | epoch 012 | loss 9.986 | ppl 1014.15 | wps 12475.3 | ups 0.19 | wpb 64844.1 | bsz 126.7 | num_updates 582 | lr 7.28355e-05 | gnorm 0.582 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 3003
KL Stats: Epoch 12 Divergences: Uniform: 1.9237059266479395 Unigram: 0.7886351551029284
2022-03-08 12:05:44 | INFO | fairseq.trainer | begin training epoch 13
2022-03-08 12:05:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:07:11 | INFO | train_inner | epoch 013:     18 / 49 loss=10.007, ppl=1028.66, wps=12635.5, ups=0.19, wpb=64867.4, bsz=126.7, num_updates=600, lr=7.5085e-05, gnorm=0.564, loss_scale=32, train_wall=454, gb_free=8.3, wall=3090
2022-03-08 12:09:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:09:51 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 9.898 | ppl 954.02 | wps 18845.2 | wpb 510.9 | bsz 1 | num_updates 631 | best_loss 9.898
2022-03-08 12:09:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 631 updates
2022-03-08 12:09:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:09:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:09:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 13 @ 631 updates, score 9.898) (writing took 3.1451289851684123 seconds)
2022-03-08 12:09:54 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-08 12:09:54 | INFO | train | epoch 013 | loss 9.846 | ppl 920.35 | wps 12728.2 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 631 | lr 7.89592e-05 | gnorm 0.629 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 3252
KL Stats: Epoch 13 Divergences: Uniform: 1.9778430279694874 Unigram: 0.8861577929220888
2022-03-08 12:09:54 | INFO | fairseq.trainer | begin training epoch 14
2022-03-08 12:09:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:13:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:14:01 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 9.798 | ppl 890.19 | wps 18841.7 | wpb 510.9 | bsz 1 | num_updates 680 | best_loss 9.798
2022-03-08 12:14:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 680 updates
2022-03-08 12:14:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:14:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:14:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 14 @ 680 updates, score 9.798) (writing took 3.041350114159286 seconds)
2022-03-08 12:14:04 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-08 12:14:04 | INFO | train | epoch 014 | loss 9.714 | ppl 839.94 | wps 12726.7 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 680 | lr 8.5083e-05 | gnorm 0.678 | loss_scale 64 | train_wall 220 | gb_free 8.3 | wall 3502
KL Stats: Epoch 14 Divergences: Uniform: 2.0262288045637438 Unigram: 0.9750209970005509
2022-03-08 12:14:04 | INFO | fairseq.trainer | begin training epoch 15
2022-03-08 12:14:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:14:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 12:15:45 | INFO | train_inner | epoch 015:     21 / 49 loss=9.729, ppl=848.62, wps=12624.3, ups=0.19, wpb=64876.2, bsz=126.7, num_updates=700, lr=8.75825e-05, gnorm=0.667, loss_scale=32, train_wall=454, gb_free=8.3, wall=3604
2022-03-08 12:17:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:18:10 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 9.701 | ppl 832.6 | wps 18812.1 | wpb 510.9 | bsz 1 | num_updates 728 | best_loss 9.701
2022-03-08 12:18:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 728 updates
2022-03-08 12:18:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:18:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:18:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 15 @ 728 updates, score 9.701) (writing took 3.338879160117358 seconds)
2022-03-08 12:18:13 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-08 12:18:13 | INFO | train | epoch 015 | loss 9.591 | ppl 771.09 | wps 12462.7 | ups 0.19 | wpb 64844.1 | bsz 126.7 | num_updates 728 | lr 9.10818e-05 | gnorm 0.715 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 3752
KL Stats: Epoch 15 Divergences: Uniform: 2.0802810364776088 Unigram: 1.0518988926632475
2022-03-08 12:18:13 | INFO | fairseq.trainer | begin training epoch 16
2022-03-08 12:18:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:22:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:22:20 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.598 | ppl 774.98 | wps 18842.3 | wpb 510.9 | bsz 1 | num_updates 777 | best_loss 9.598
2022-03-08 12:22:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 777 updates
2022-03-08 12:22:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:22:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:22:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 16 @ 777 updates, score 9.598) (writing took 3.3711738439742476 seconds)
2022-03-08 12:22:23 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-08 12:22:23 | INFO | train | epoch 016 | loss 9.467 | ppl 707.68 | wps 12718.2 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 777 | lr 9.72056e-05 | gnorm 0.677 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 4002
KL Stats: Epoch 16 Divergences: Uniform: 2.136243065746056 Unigram: 1.1255851093589597
2022-03-08 12:22:23 | INFO | fairseq.trainer | begin training epoch 17
2022-03-08 12:22:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:24:15 | INFO | train_inner | epoch 017:     23 / 49 loss=9.475, ppl=711.69, wps=12732.8, ups=0.2, wpb=64867.4, bsz=126.7, num_updates=800, lr=0.00010008, gnorm=0.742, loss_scale=32, train_wall=450, gb_free=8.3, wall=4113
2022-03-08 12:25:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 12:26:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:26:30 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 9.515 | ppl 731.52 | wps 18813.4 | wpb 510.9 | bsz 1 | num_updates 825 | best_loss 9.515
2022-03-08 12:26:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 825 updates
2022-03-08 12:26:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:26:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:26:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 17 @ 825 updates, score 9.515) (writing took 2.791648982092738 seconds)
2022-03-08 12:26:33 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-08 12:26:33 | INFO | train | epoch 017 | loss 9.35 | ppl 652.67 | wps 12474.2 | ups 0.19 | wpb 64844.1 | bsz 126.7 | num_updates 825 | lr 0.000103204 | gnorm 0.818 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 4251
KL Stats: Epoch 17 Divergences: Uniform: 2.1904227183939877 Unigram: 1.1958169860057963
2022-03-08 12:26:33 | INFO | fairseq.trainer | begin training epoch 18
2022-03-08 12:26:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:30:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:30:40 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 9.439 | ppl 694.25 | wps 18813.3 | wpb 510.9 | bsz 1 | num_updates 874 | best_loss 9.439
2022-03-08 12:30:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 874 updates
2022-03-08 12:30:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:30:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:30:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 18 @ 874 updates, score 9.439) (writing took 3.2502032099291682 seconds)
2022-03-08 12:30:43 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-08 12:30:43 | INFO | train | epoch 018 | loss 9.234 | ppl 601.95 | wps 12714 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 874 | lr 0.000109328 | gnorm 0.772 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 4501
KL Stats: Epoch 18 Divergences: Uniform: 2.2413011025032974 Unigram: 1.2641441216721434
2022-03-08 12:30:43 | INFO | fairseq.trainer | begin training epoch 19
2022-03-08 12:30:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:32:49 | INFO | train_inner | epoch 019:     26 / 49 loss=9.233, ppl=601.91, wps=12618.4, ups=0.19, wpb=64876.2, bsz=126.7, num_updates=900, lr=0.000112578, gnorm=0.801, loss_scale=32, train_wall=455, gb_free=8.3, wall=4627
2022-03-08 12:34:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:34:50 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.349 | ppl 651.97 | wps 18764.3 | wpb 510.9 | bsz 1 | num_updates 923 | best_loss 9.349
2022-03-08 12:34:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 923 updates
2022-03-08 12:34:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:34:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:34:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 19 @ 923 updates, score 9.349) (writing took 2.8777057470288128 seconds)
2022-03-08 12:34:53 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-08 12:34:53 | INFO | train | epoch 019 | loss 9.123 | ppl 557.59 | wps 12724 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 923 | lr 0.000115452 | gnorm 0.888 | loss_scale 32 | train_wall 221 | gb_free 8.3 | wall 4751
KL Stats: Epoch 19 Divergences: Uniform: 2.288861045076867 Unigram: 1.329464426821125
2022-03-08 12:34:53 | INFO | fairseq.trainer | begin training epoch 20
2022-03-08 12:34:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:36:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 12:38:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 12:38:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:38:59 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.281 | ppl 622.29 | wps 18835 | wpb 510.9 | bsz 1 | num_updates 970 | best_loss 9.281
2022-03-08 12:38:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 970 updates
2022-03-08 12:38:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:39:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:39:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 20 @ 970 updates, score 9.281) (writing took 3.6437264061532915 seconds)
2022-03-08 12:39:03 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-08 12:39:03 | INFO | train | epoch 020 | loss 9.015 | ppl 517.19 | wps 12176.1 | ups 0.19 | wpb 64829.4 | bsz 126.6 | num_updates 970 | lr 0.000121326 | gnorm 0.824 | loss_scale 16 | train_wall 220 | gb_free 8.3 | wall 5001
KL Stats: Epoch 20 Divergences: Uniform: 2.3402922268538986 Unigram: 1.3881407363985516
2022-03-08 12:39:03 | INFO | fairseq.trainer | begin training epoch 21
2022-03-08 12:39:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:41:28 | INFO | train_inner | epoch 021:     30 / 49 loss=9.01, ppl=515.44, wps=12493.5, ups=0.19, wpb=64867.4, bsz=126.7, num_updates=1000, lr=0.000125075, gnorm=0.855, loss_scale=16, train_wall=459, gb_free=8.3, wall=5147
2022-03-08 12:42:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:43:09 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.213 | ppl 593.53 | wps 18826 | wpb 510.9 | bsz 1 | num_updates 1019 | best_loss 9.213
2022-03-08 12:43:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1019 updates
2022-03-08 12:43:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:43:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:43:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 21 @ 1019 updates, score 9.213) (writing took 3.456633073044941 seconds)
2022-03-08 12:43:13 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-08 12:43:13 | INFO | train | epoch 021 | loss 8.916 | ppl 482.98 | wps 12710.8 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1019 | lr 0.00012745 | gnorm 0.854 | loss_scale 16 | train_wall 220 | gb_free 8.3 | wall 5251
KL Stats: Epoch 21 Divergences: Uniform: 2.378749796486569 Unigram: 1.4467288896488941
2022-03-08 12:43:13 | INFO | fairseq.trainer | begin training epoch 22
2022-03-08 12:43:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:47:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:47:19 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.161 | ppl 572.63 | wps 18927.8 | wpb 510.9 | bsz 1 | num_updates 1068 | best_loss 9.161
2022-03-08 12:47:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1068 updates
2022-03-08 12:47:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:47:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:47:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 22 @ 1068 updates, score 9.161) (writing took 2.8388179449830204 seconds)
2022-03-08 12:47:22 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-08 12:47:22 | INFO | train | epoch 022 | loss 8.816 | ppl 450.57 | wps 12754.1 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1068 | lr 0.000133573 | gnorm 0.855 | loss_scale 16 | train_wall 220 | gb_free 8.3 | wall 5500
KL Stats: Epoch 22 Divergences: Uniform: 2.4194697058444823 Unigram: 1.5042830346734868
2022-03-08 12:47:22 | INFO | fairseq.trainer | begin training epoch 23
2022-03-08 12:47:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:49:57 | INFO | train_inner | epoch 023:     32 / 49 loss=8.8, ppl=445.84, wps=12760.4, ups=0.2, wpb=64876.2, bsz=126.7, num_updates=1100, lr=0.000137573, gnorm=0.855, loss_scale=32, train_wall=449, gb_free=8.3, wall=5655
2022-03-08 12:51:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:51:28 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.114 | ppl 554.03 | wps 18941.5 | wpb 510.9 | bsz 1 | num_updates 1117 | best_loss 9.114
2022-03-08 12:51:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1117 updates
2022-03-08 12:51:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:51:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:51:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 23 @ 1117 updates, score 9.114) (writing took 2.842010285006836 seconds)
2022-03-08 12:51:31 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-08 12:51:31 | INFO | train | epoch 023 | loss 8.718 | ppl 421.12 | wps 12790.2 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1117 | lr 0.000139697 | gnorm 0.918 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 5749
KL Stats: Epoch 23 Divergences: Uniform: 2.468024658221998 Unigram: 1.5563420666630805
2022-03-08 12:51:31 | INFO | fairseq.trainer | begin training epoch 24
2022-03-08 12:51:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:55:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:55:36 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.064 | ppl 535.18 | wps 18923.2 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 9.064
2022-03-08 12:55:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1166 updates
2022-03-08 12:55:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:55:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:55:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 24 @ 1166 updates, score 9.064) (writing took 2.867169707082212 seconds)
2022-03-08 12:55:39 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-08 12:55:39 | INFO | train | epoch 024 | loss 8.623 | ppl 394.17 | wps 12781.1 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1166 | lr 0.000145821 | gnorm 0.86 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 5998
KL Stats: Epoch 24 Divergences: Uniform: 2.5078743593115007 Unigram: 1.6067336813768889
2022-03-08 12:55:39 | INFO | fairseq.trainer | begin training epoch 25
2022-03-08 12:55:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:58:23 | INFO | train_inner | epoch 025:     34 / 49 loss=8.606, ppl=389.58, wps=12801, ups=0.2, wpb=64867.4, bsz=126.7, num_updates=1200, lr=0.00015007, gnorm=0.905, loss_scale=32, train_wall=448, gb_free=8.3, wall=6162
2022-03-08 12:59:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:59:45 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.011 | ppl 515.81 | wps 18880 | wpb 510.9 | bsz 1 | num_updates 1215 | best_loss 9.011
2022-03-08 12:59:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1215 updates
2022-03-08 12:59:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:59:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 12:59:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 25 @ 1215 updates, score 9.011) (writing took 2.78512232308276 seconds)
2022-03-08 12:59:48 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-08 12:59:48 | INFO | train | epoch 025 | loss 8.529 | ppl 369.47 | wps 12786.8 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1215 | lr 0.000151945 | gnorm 0.924 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 6246
KL Stats: Epoch 25 Divergences: Uniform: 2.54086733550266 Unigram: 1.65547090484505
2022-03-08 12:59:48 | INFO | fairseq.trainer | begin training epoch 26
2022-03-08 12:59:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:02:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 13:03:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:03:53 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.971 | ppl 501.75 | wps 18923 | wpb 510.9 | bsz 1 | num_updates 1263 | best_loss 8.971
2022-03-08 13:03:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1263 updates
2022-03-08 13:03:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:03:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:03:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 26 @ 1263 updates, score 8.971) (writing took 2.9284632420167327 seconds)
2022-03-08 13:03:56 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-08 13:03:56 | INFO | train | epoch 026 | loss 8.436 | ppl 346.42 | wps 12525.3 | ups 0.19 | wpb 64844.1 | bsz 126.7 | num_updates 1263 | lr 0.000157943 | gnorm 0.876 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 6495
KL Stats: Epoch 26 Divergences: Uniform: 2.578608845573897 Unigram: 1.7027157213719
2022-03-08 13:03:56 | INFO | fairseq.trainer | begin training epoch 27
2022-03-08 13:03:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:06:55 | INFO | train_inner | epoch 027:     37 / 49 loss=8.415, ppl=341.36, wps=12684.5, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=1300, lr=0.000162568, gnorm=0.881, loss_scale=32, train_wall=453, gb_free=8.3, wall=6673
2022-03-08 13:07:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:08:02 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.919 | ppl 484.2 | wps 18906.3 | wpb 510.9 | bsz 1 | num_updates 1312 | best_loss 8.919
2022-03-08 13:08:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1312 updates
2022-03-08 13:08:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:08:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:08:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 27 @ 1312 updates, score 8.919) (writing took 2.9381661489605904 seconds)
2022-03-08 13:08:05 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-08 13:08:05 | INFO | train | epoch 027 | loss 8.345 | ppl 325.17 | wps 12788.7 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1312 | lr 0.000164067 | gnorm 0.89 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 6743
KL Stats: Epoch 27 Divergences: Uniform: 2.622854919110057 Unigram: 1.7473956173862162
2022-03-08 13:08:05 | INFO | fairseq.trainer | begin training epoch 28
2022-03-08 13:08:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:11:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:12:10 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.887 | ppl 473.53 | wps 18833.9 | wpb 510.9 | bsz 1 | num_updates 1361 | best_loss 8.887
2022-03-08 13:12:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1361 updates
2022-03-08 13:12:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:12:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:12:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 28 @ 1361 updates, score 8.887) (writing took 2.947790668113157 seconds)
2022-03-08 13:12:13 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-08 13:12:13 | INFO | train | epoch 028 | loss 8.253 | ppl 305.06 | wps 12780.3 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1361 | lr 0.000170191 | gnorm 0.906 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 6992
KL Stats: Epoch 28 Divergences: Uniform: 2.6520274658364893 Unigram: 1.7929183735687249
2022-03-08 13:12:13 | INFO | fairseq.trainer | begin training epoch 29
2022-03-08 13:12:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:13:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 13:15:26 | INFO | train_inner | epoch 029:     40 / 49 loss=8.228, ppl=299.78, wps=12680.1, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=1400, lr=0.000175065, gnorm=0.929, loss_scale=32, train_wall=453, gb_free=8.3, wall=7185
2022-03-08 13:16:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:16:19 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.855 | ppl 462.94 | wps 18924.2 | wpb 510.9 | bsz 1 | num_updates 1409 | best_loss 8.855
2022-03-08 13:16:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1409 updates
2022-03-08 13:16:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:16:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:16:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 29 @ 1409 updates, score 8.855) (writing took 2.7525061189662665 seconds)
2022-03-08 13:16:22 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-08 13:16:22 | INFO | train | epoch 029 | loss 8.162 | ppl 286.34 | wps 12537.1 | ups 0.19 | wpb 64844.1 | bsz 126.7 | num_updates 1409 | lr 0.00017619 | gnorm 0.924 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 7240
KL Stats: Epoch 29 Divergences: Uniform: 2.6998797476254794 Unigram: 1.8368732082639816
2022-03-08 13:16:22 | INFO | fairseq.trainer | begin training epoch 30
2022-03-08 13:16:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:20:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:20:28 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.827 | ppl 454.03 | wps 18957.5 | wpb 510.9 | bsz 1 | num_updates 1458 | best_loss 8.827
2022-03-08 13:20:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1458 updates
2022-03-08 13:20:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:20:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:20:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 30 @ 1458 updates, score 8.827) (writing took 2.8340762730222195 seconds)
2022-03-08 13:20:30 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-08 13:20:30 | INFO | train | epoch 030 | loss 8.071 | ppl 268.9 | wps 12778.1 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1458 | lr 0.000182314 | gnorm 0.924 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 7489
KL Stats: Epoch 30 Divergences: Uniform: 2.7244161695447824 Unigram: 1.8790211022541563
2022-03-08 13:20:30 | INFO | fairseq.trainer | begin training epoch 31
2022-03-08 13:20:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:23:53 | INFO | train_inner | epoch 031:     42 / 49 loss=8.04, ppl=263.15, wps=12801.4, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=1500, lr=0.000187563, gnorm=0.915, loss_scale=64, train_wall=448, gb_free=8.3, wall=7691
2022-03-08 13:24:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:24:36 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.789 | ppl 442.46 | wps 18918.9 | wpb 510.9 | bsz 1 | num_updates 1507 | best_loss 8.789
2022-03-08 13:24:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1507 updates
2022-03-08 13:24:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:24:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:24:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 31 @ 1507 updates, score 8.789) (writing took 2.9401790800038725 seconds)
2022-03-08 13:24:39 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-08 13:24:39 | INFO | train | epoch 031 | loss 7.979 | ppl 252.36 | wps 12785 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1507 | lr 0.000188437 | gnorm 0.912 | loss_scale 64 | train_wall 220 | gb_free 8.3 | wall 7737
KL Stats: Epoch 31 Divergences: Uniform: 2.766786765842172 Unigram: 1.9234793941339292
2022-03-08 13:24:39 | INFO | fairseq.trainer | begin training epoch 32
2022-03-08 13:24:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:25:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 13:28:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:28:44 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.769 | ppl 436.28 | wps 18910.4 | wpb 510.9 | bsz 1 | num_updates 1555 | best_loss 8.769
2022-03-08 13:28:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 1555 updates
2022-03-08 13:28:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:28:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:28:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 32 @ 1555 updates, score 8.769) (writing took 2.7625103869941086 seconds)
2022-03-08 13:28:47 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-08 13:28:47 | INFO | train | epoch 032 | loss 7.887 | ppl 236.76 | wps 12535.8 | ups 0.19 | wpb 64844.1 | bsz 126.7 | num_updates 1555 | lr 0.000194436 | gnorm 0.953 | loss_scale 32 | train_wall 219 | gb_free 8.3 | wall 7986
KL Stats: Epoch 32 Divergences: Uniform: 2.8132565282722855 Unigram: 1.9635161580488125
2022-03-08 13:28:47 | INFO | fairseq.trainer | begin training epoch 33
2022-03-08 13:28:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:32:25 | INFO | train_inner | epoch 033:     45 / 49 loss=7.853, ppl=231.17, wps=12683.8, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=1600, lr=0.00020006, gnorm=0.941, loss_scale=32, train_wall=453, gb_free=8.3, wall=8203
2022-03-08 13:32:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:32:53 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.735 | ppl 425.96 | wps 18873.2 | wpb 510.9 | bsz 1 | num_updates 1604 | best_loss 8.735
2022-03-08 13:32:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1604 updates
2022-03-08 13:32:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:32:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:32:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 33 @ 1604 updates, score 8.735) (writing took 2.8749607380013913 seconds)
2022-03-08 13:32:56 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-08 13:32:56 | INFO | train | epoch 033 | loss 7.798 | ppl 222.62 | wps 12777.8 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1604 | lr 0.00020056 | gnorm 0.934 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 8234
KL Stats: Epoch 33 Divergences: Uniform: 2.8502621327625532 Unigram: 2.0066110060893734
2022-03-08 13:32:56 | INFO | fairseq.trainer | begin training epoch 34
2022-03-08 13:32:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:36:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:37:02 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.721 | ppl 421.94 | wps 18884 | wpb 510.9 | bsz 1 | num_updates 1653 | best_loss 8.721
2022-03-08 13:37:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1653 updates
2022-03-08 13:37:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:37:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:37:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 34 @ 1653 updates, score 8.721) (writing took 2.9893291119951755 seconds)
2022-03-08 13:37:05 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-08 13:37:05 | INFO | train | epoch 034 | loss 7.708 | ppl 209.11 | wps 12761.3 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1653 | lr 0.000206684 | gnorm 0.925 | loss_scale 64 | train_wall 220 | gb_free 8.3 | wall 8483
KL Stats: Epoch 34 Divergences: Uniform: 2.887308708923593 Unigram: 2.0490995964497176
2022-03-08 13:37:05 | INFO | fairseq.trainer | begin training epoch 35
2022-03-08 13:37:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:37:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 13:40:56 | INFO | train_inner | epoch 035:     48 / 49 loss=7.669, ppl=203.54, wps=12671.8, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=1700, lr=0.000212558, gnorm=0.961, loss_scale=32, train_wall=453, gb_free=8.3, wall=8715
2022-03-08 13:40:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:41:10 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.713 | ppl 419.61 | wps 18918.9 | wpb 510.9 | bsz 1 | num_updates 1701 | best_loss 8.713
2022-03-08 13:41:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1701 updates
2022-03-08 13:41:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:41:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:41:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 35 @ 1701 updates, score 8.713) (writing took 2.7250719419680536 seconds)
2022-03-08 13:41:13 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-08 13:41:13 | INFO | train | epoch 035 | loss 7.621 | ppl 196.92 | wps 12539.8 | ups 0.19 | wpb 64844.1 | bsz 126.7 | num_updates 1701 | lr 0.000212682 | gnorm 1 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 8732
KL Stats: Epoch 35 Divergences: Uniform: 2.919954388834132 Unigram: 2.091498453208954
2022-03-08 13:41:13 | INFO | fairseq.trainer | begin training epoch 36
2022-03-08 13:41:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:45:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:45:19 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.714 | ppl 419.87 | wps 18910.4 | wpb 510.9 | bsz 1 | num_updates 1750 | best_loss 8.713
2022-03-08 13:45:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1750 updates
2022-03-08 13:45:19 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-08 13:45:19 | INFO | train | epoch 036 | loss 7.531 | ppl 184.92 | wps 12922.5 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1750 | lr 0.000218806 | gnorm 0.913 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 8978
KL Stats: Epoch 36 Divergences: Uniform: 2.9621532940521225 Unigram: 2.130317139935465
2022-03-08 13:45:19 | INFO | fairseq.trainer | begin training epoch 37
2022-03-08 13:45:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:49:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:49:25 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.682 | ppl 410.81 | wps 18921.7 | wpb 510.9 | bsz 1 | num_updates 1799 | best_loss 8.682
2022-03-08 13:49:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1799 updates
2022-03-08 13:49:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:49:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt
2022-03-08 13:49:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#4/checkpoint_best.pt (epoch 37 @ 1799 updates, score 8.682) (writing took 2.912972084013745 seconds)
2022-03-08 13:49:28 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-08 13:49:28 | INFO | train | epoch 037 | loss 7.444 | ppl 174.19 | wps 12781.1 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1799 | lr 0.00022493 | gnorm 1.014 | loss_scale 64 | train_wall 220 | gb_free 8.3 | wall 9226
KL Stats: Epoch 37 Divergences: Uniform: 3.001147668216536 Unigram: 2.1704162131673854
2022-03-08 13:49:28 | INFO | fairseq.trainer | begin training epoch 38
2022-03-08 13:49:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:49:33 | INFO | train_inner | epoch 038:      1 / 49 loss=7.488, ppl=179.46, wps=12502.9, ups=0.19, wpb=64544.1, bsz=126.1, num_updates=1800, lr=0.000225055, gnorm=0.964, loss_scale=64, train_wall=446, gb_free=8.3, wall=9231
2022-03-08 13:50:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 13:53:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:53:33 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.724 | ppl 422.92 | wps 18900.7 | wpb 510.9 | bsz 1 | num_updates 1847 | best_loss 8.682
2022-03-08 13:53:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1847 updates
2022-03-08 13:53:33 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-08 13:53:33 | INFO | train | epoch 038 | loss 7.355 | ppl 163.75 | wps 12671.8 | ups 0.2 | wpb 64844.1 | bsz 126.7 | num_updates 1847 | lr 0.000230929 | gnorm 0.97 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 9472
KL Stats: Epoch 38 Divergences: Uniform: 3.0246026865034645 Unigram: 2.2113982543576842
2022-03-08 13:53:33 | INFO | fairseq.trainer | begin training epoch 39
2022-03-08 13:53:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:57:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:57:39 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.711 | ppl 419.08 | wps 18885.4 | wpb 510.9 | bsz 1 | num_updates 1896 | best_loss 8.682
2022-03-08 13:57:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1896 updates
2022-03-08 13:57:39 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-08 13:57:39 | INFO | train | epoch 039 | loss 7.274 | ppl 154.75 | wps 12923 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1896 | lr 0.000237053 | gnorm 1.008 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 9718
KL Stats: Epoch 39 Divergences: Uniform: 3.0732095556649255 Unigram: 2.2537482628248133
2022-03-08 13:57:39 | INFO | fairseq.trainer | begin training epoch 40
2022-03-08 13:57:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:57:59 | INFO | train_inner | epoch 040:      4 / 49 loss=7.308, ppl=158.49, wps=12820.5, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=1900, lr=0.000237553, gnorm=0.991, loss_scale=32, train_wall=453, gb_free=8.3, wall=9737
2022-03-08 14:01:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:01:45 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.711 | ppl 419.01 | wps 18915.8 | wpb 510.9 | bsz 1 | num_updates 1945 | best_loss 8.682
2022-03-08 14:01:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1945 updates
2022-03-08 14:01:45 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-08 14:01:45 | INFO | train | epoch 040 | loss 7.191 | ppl 146.14 | wps 12928.7 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 1945 | lr 0.000243176 | gnorm 1.04 | loss_scale 64 | train_wall 220 | gb_free 8.3 | wall 9964
KL Stats: Epoch 40 Divergences: Uniform: 3.1137815741044625 Unigram: 2.2892318093611355
2022-03-08 14:01:45 | INFO | fairseq.trainer | begin training epoch 41
2022-03-08 14:01:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:02:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 14:05:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:05:51 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.761 | ppl 433.89 | wps 18964.1 | wpb 510.9 | bsz 1 | num_updates 1993 | best_loss 8.682
2022-03-08 14:05:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1993 updates
2022-03-08 14:05:51 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-08 14:05:51 | INFO | train | epoch 041 | loss 7.098 | ppl 136.99 | wps 12680.9 | ups 0.2 | wpb 64844.1 | bsz 126.7 | num_updates 1993 | lr 0.000249175 | gnorm 0.946 | loss_scale 32 | train_wall 219 | gb_free 8.3 | wall 10209
KL Stats: Epoch 41 Divergences: Uniform: 3.158192333755383 Unigram: 2.334355634697251
2022-03-08 14:05:51 | INFO | fairseq.trainer | begin training epoch 42
2022-03-08 14:05:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:06:24 | INFO | train_inner | epoch 042:      7 / 49 loss=7.135, ppl=140.51, wps=12828.4, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=2000, lr=0.00025005, gnorm=1, loss_scale=32, train_wall=453, gb_free=8.3, wall=10243
2022-03-08 14:09:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:09:56 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.756 | ppl 432.35 | wps 18898.9 | wpb 510.9 | bsz 1 | num_updates 2042 | best_loss 8.682
2022-03-08 14:09:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2042 updates
2022-03-08 14:09:56 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-08 14:09:56 | INFO | train | epoch 042 | loss 7.024 | ppl 130.13 | wps 12943.6 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 2042 | lr 0.000255299 | gnorm 1.073 | loss_scale 32 | train_wall 219 | gb_free 8.3 | wall 10455
KL Stats: Epoch 42 Divergences: Uniform: 3.1902344750446683 Unigram: 2.3715480944664966
2022-03-08 14:09:56 | INFO | fairseq.trainer | begin training epoch 43
2022-03-08 14:09:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:13:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 14:13:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:14:02 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.837 | ppl 457.27 | wps 18962.3 | wpb 510.9 | bsz 1 | num_updates 2090 | best_loss 8.682
2022-03-08 14:14:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2090 updates
2022-03-08 14:14:02 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-08 14:14:02 | INFO | train | epoch 043 | loss 6.935 | ppl 122.39 | wps 12672.8 | ups 0.2 | wpb 64844.1 | bsz 126.7 | num_updates 2090 | lr 0.000261298 | gnorm 1.023 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 10700
KL Stats: Epoch 43 Divergences: Uniform: 3.2321190759132112 Unigram: 2.4109423469493803
2022-03-08 14:14:02 | INFO | fairseq.trainer | begin training epoch 44
2022-03-08 14:14:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:14:50 | INFO | train_inner | epoch 044:     10 / 49 loss=6.964, ppl=124.88, wps=12830.7, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=2100, lr=0.000262548, gnorm=1.063, loss_scale=32, train_wall=453, gb_free=8.3, wall=10748
2022-03-08 14:17:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:18:07 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.826 | ppl 453.67 | wps 18913.6 | wpb 510.9 | bsz 1 | num_updates 2139 | best_loss 8.682
2022-03-08 14:18:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 2139 updates
2022-03-08 14:18:07 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-08 14:18:07 | INFO | train | epoch 044 | loss 6.857 | ppl 115.9 | wps 12942 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 2139 | lr 0.000267422 | gnorm 1.035 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 10946
KL Stats: Epoch 44 Divergences: Uniform: 3.263852091705918 Unigram: 2.453126253753275
2022-03-08 14:18:07 | INFO | fairseq.trainer | begin training epoch 45
2022-03-08 14:18:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:20:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 14:22:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:22:13 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.848 | ppl 460.94 | wps 18886 | wpb 510.9 | bsz 1 | num_updates 2187 | best_loss 8.682
2022-03-08 14:22:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 2187 updates
2022-03-08 14:22:13 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-08 14:22:13 | INFO | train | epoch 045 | loss 6.777 | ppl 109.66 | wps 12671.4 | ups 0.2 | wpb 64844.1 | bsz 126.7 | num_updates 2187 | lr 0.00027342 | gnorm 1.073 | loss_scale 16 | train_wall 220 | gb_free 8.3 | wall 11191
KL Stats: Epoch 45 Divergences: Uniform: 3.303478062711679 Unigram: 2.4958940277472577
2022-03-08 14:22:13 | INFO | fairseq.trainer | begin training epoch 46
2022-03-08 14:22:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:23:16 | INFO | train_inner | epoch 046:     13 / 49 loss=6.796, ppl=111.11, wps=12828.1, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=2200, lr=0.000275045, gnorm=1.026, loss_scale=16, train_wall=453, gb_free=8.3, wall=11254
2022-03-08 14:26:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:26:18 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.828 | ppl 454.41 | wps 18771.2 | wpb 510.9 | bsz 1 | num_updates 2236 | best_loss 8.682
2022-03-08 14:26:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2236 updates
2022-03-08 14:26:18 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-08 14:26:18 | INFO | train | epoch 046 | loss 6.701 | ppl 104.01 | wps 12943 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 2236 | lr 0.000279544 | gnorm 1.091 | loss_scale 16 | train_wall 219 | gb_free 8.3 | wall 11437
KL Stats: Epoch 46 Divergences: Uniform: 3.346139276718354 Unigram: 2.537831018675194
2022-03-08 14:26:18 | INFO | fairseq.trainer | begin training epoch 47
2022-03-08 14:26:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:30:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:30:24 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.908 | ppl 480.53 | wps 18978.2 | wpb 510.9 | bsz 1 | num_updates 2285 | best_loss 8.682
2022-03-08 14:30:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 2285 updates
2022-03-08 14:30:24 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-08 14:30:24 | INFO | train | epoch 047 | loss 6.613 | ppl 97.86 | wps 12948.7 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 2285 | lr 0.000285668 | gnorm 1.064 | loss_scale 16 | train_wall 220 | gb_free 8.3 | wall 11682
KL Stats: Epoch 47 Divergences: Uniform: 3.390327458327028 Unigram: 2.5809414587743493
2022-03-08 14:30:24 | INFO | fairseq.trainer | begin training epoch 48
2022-03-08 14:30:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:31:36 | INFO | train_inner | epoch 048:     15 / 49 loss=6.634, ppl=99.33, wps=12957.8, ups=0.2, wpb=64867.4, bsz=126.7, num_updates=2300, lr=0.000287543, gnorm=1.113, loss_scale=32, train_wall=448, gb_free=8.3, wall=11755
2022-03-08 14:34:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:34:30 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.897 | ppl 476.78 | wps 18927 | wpb 510.9 | bsz 1 | num_updates 2334 | best_loss 8.682
2022-03-08 14:34:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 2334 updates
2022-03-08 14:34:30 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-08 14:34:30 | INFO | train | epoch 048 | loss 6.534 | ppl 92.65 | wps 12936.2 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 2334 | lr 0.000291792 | gnorm 1.099 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 11928
KL Stats: Epoch 48 Divergences: Uniform: 3.422029863821448 Unigram: 2.6193235731311644
2022-03-08 14:34:30 | INFO | fairseq.trainer | begin training epoch 49
2022-03-08 14:34:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:38:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:38:35 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.913 | ppl 482.07 | wps 18948.8 | wpb 510.9 | bsz 1 | num_updates 2383 | best_loss 8.682
2022-03-08 14:38:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 2383 updates
2022-03-08 14:38:35 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-08 14:38:35 | INFO | train | epoch 049 | loss 6.458 | ppl 87.9 | wps 12939.6 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 2383 | lr 0.000297915 | gnorm 1.138 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 12174
KL Stats: Epoch 49 Divergences: Uniform: 3.464933370623968 Unigram: 2.6662842694958337
2022-03-08 14:38:35 | INFO | fairseq.trainer | begin training epoch 50
2022-03-08 14:38:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:39:57 | INFO | train_inner | epoch 050:     17 / 49 loss=6.468, ppl=88.55, wps=12951.5, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=2400, lr=0.00030004, gnorm=1.102, loss_scale=32, train_wall=448, gb_free=8.3, wall=12256
2022-03-08 14:41:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 14:42:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:42:41 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.952 | ppl 495.23 | wps 18911.6 | wpb 510.9 | bsz 1 | num_updates 2431 | best_loss 8.682
2022-03-08 14:42:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 2431 updates
2022-03-08 14:42:41 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-08 14:42:41 | INFO | train | epoch 050 | loss 6.371 | ppl 82.76 | wps 12677.9 | ups 0.2 | wpb 64844.1 | bsz 126.7 | num_updates 2431 | lr 0.000303914 | gnorm 1.087 | loss_scale 32 | train_wall 220 | gb_free 8.3 | wall 12419
KL Stats: Epoch 50 Divergences: Uniform: 3.5033474554398962 Unigram: 2.71566855075078
2022-03-08 14:42:41 | INFO | fairseq.trainer | begin training epoch 51
2022-03-08 14:42:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:45:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 14:46:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:46:46 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 8.987 | ppl 507.32 | wps 18910.2 | wpb 510.9 | bsz 1 | num_updates 2479 | best_loss 8.682
2022-03-08 14:46:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 2479 updates
2022-03-08 14:46:46 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-08 14:46:46 | INFO | train | epoch 051 | loss 6.297 | ppl 78.61 | wps 12674.9 | ups 0.2 | wpb 64844.1 | bsz 126.7 | num_updates 2479 | lr 0.000309913 | gnorm 1.157 | loss_scale 16 | train_wall 219 | gb_free 8.3 | wall 12665
KL Stats: Epoch 51 Divergences: Uniform: 3.5447037769268985 Unigram: 2.7526826513763183
2022-03-08 14:46:46 | INFO | fairseq.trainer | begin training epoch 52
2022-03-08 14:46:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:48:28 | INFO | train_inner | epoch 052:     21 / 49 loss=6.3, ppl=78.77, wps=12709.8, ups=0.2, wpb=64876.2, bsz=126.7, num_updates=2500, lr=0.000312538, gnorm=1.138, loss_scale=16, train_wall=457, gb_free=8.3, wall=12766
2022-03-08 14:50:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:50:52 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.052 | ppl 530.67 | wps 18964.3 | wpb 510.9 | bsz 1 | num_updates 2528 | best_loss 8.682
2022-03-08 14:50:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 2528 updates
2022-03-08 14:50:52 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-08 14:50:52 | INFO | train | epoch 052 | loss 6.223 | ppl 74.72 | wps 12942.9 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 2528 | lr 0.000316037 | gnorm 1.236 | loss_scale 16 | train_wall 220 | gb_free 8.3 | wall 12910
KL Stats: Epoch 52 Divergences: Uniform: 3.595861252688876 Unigram: 2.8010784519057492
2022-03-08 14:50:52 | INFO | fairseq.trainer | begin training epoch 53
2022-03-08 14:50:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:54:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:54:58 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.056 | ppl 532.43 | wps 18923.7 | wpb 510.9 | bsz 1 | num_updates 2577 | best_loss 8.682
2022-03-08 14:54:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 2577 updates
2022-03-08 14:54:58 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-08 14:54:58 | INFO | train | epoch 053 | loss 6.14 | ppl 70.51 | wps 12930.2 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 2577 | lr 0.000322161 | gnorm 1.117 | loss_scale 16 | train_wall 220 | gb_free 8.3 | wall 13156
KL Stats: Epoch 53 Divergences: Uniform: 3.6419848330845124 Unigram: 2.846698824242203
2022-03-08 14:54:58 | INFO | fairseq.trainer | begin training epoch 54
2022-03-08 14:54:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:56:49 | INFO | train_inner | epoch 054:     23 / 49 loss=6.146, ppl=70.79, wps=12949.2, ups=0.2, wpb=64867.4, bsz=126.7, num_updates=2600, lr=0.000325035, gnorm=1.192, loss_scale=32, train_wall=448, gb_free=8.3, wall=13267
2022-03-08 14:58:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 14:58:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:59:03 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.126 | ppl 558.79 | wps 18912.1 | wpb 510.9 | bsz 1 | num_updates 2625 | best_loss 8.682
2022-03-08 14:59:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 2625 updates
2022-03-08 14:59:03 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-08 14:59:03 | INFO | train | epoch 054 | loss 6.07 | ppl 67.19 | wps 12678.1 | ups 0.2 | wpb 64844.1 | bsz 126.7 | num_updates 2625 | lr 0.000328159 | gnorm 1.318 | loss_scale 16 | train_wall 219 | gb_free 8.3 | wall 13401
KL Stats: Epoch 54 Divergences: Uniform: 3.6896866246316216 Unigram: 2.8976623588270156
2022-03-08 14:59:03 | INFO | fairseq.trainer | begin training epoch 55
2022-03-08 14:59:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:02:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:03:08 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.12 | ppl 556.57 | wps 18947.1 | wpb 510.9 | bsz 1 | num_updates 2674 | best_loss 8.682
2022-03-08 15:03:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 2674 updates
2022-03-08 15:03:08 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-08 15:03:08 | INFO | train | epoch 055 | loss 5.983 | ppl 63.25 | wps 12954.5 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 2674 | lr 0.000334283 | gnorm 1.13 | loss_scale 16 | train_wall 219 | gb_free 8.3 | wall 13647
KL Stats: Epoch 55 Divergences: Uniform: 3.7164400982513413 Unigram: 2.9483108615951075
2022-03-08 15:03:08 | INFO | fairseq.trainer | begin training epoch 56
2022-03-08 15:03:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:05:14 | INFO | train_inner | epoch 056:     26 / 49 loss=5.986, ppl=63.38, wps=12836.9, ups=0.2, wpb=64876.2, bsz=126.7, num_updates=2700, lr=0.000337533, gnorm=1.218, loss_scale=16, train_wall=452, gb_free=8.3, wall=13772
2022-03-08 15:07:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:07:14 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.158 | ppl 571.22 | wps 18897.2 | wpb 510.9 | bsz 1 | num_updates 2723 | best_loss 8.682
2022-03-08 15:07:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 2723 updates
2022-03-08 15:07:14 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-08 15:07:14 | INFO | train | epoch 056 | loss 5.917 | ppl 60.4 | wps 12936.9 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 2723 | lr 0.000340407 | gnorm 1.317 | loss_scale 16 | train_wall 220 | gb_free 8.3 | wall 13892
KL Stats: Epoch 56 Divergences: Uniform: 3.7716912160617784 Unigram: 2.9891180347171105
2022-03-08 15:07:14 | INFO | fairseq.trainer | begin training epoch 57
2022-03-08 15:07:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:09:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 15:11:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:11:20 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.169 | ppl 575.73 | wps 18918.4 | wpb 510.9 | bsz 1 | num_updates 2771 | best_loss 8.682
2022-03-08 15:11:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 2771 updates
2022-03-08 15:11:20 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-08 15:11:20 | INFO | train | epoch 057 | loss 5.826 | ppl 56.71 | wps 12671 | ups 0.2 | wpb 64844.1 | bsz 126.7 | num_updates 2771 | lr 0.000346406 | gnorm 1.102 | loss_scale 16 | train_wall 220 | gb_free 8.3 | wall 14138
KL Stats: Epoch 57 Divergences: Uniform: 3.8174939952214166 Unigram: 3.0365811380017376
2022-03-08 15:11:20 | INFO | fairseq.trainer | begin training epoch 58
2022-03-08 15:11:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:13:40 | INFO | train_inner | epoch 058:     29 / 49 loss=5.84, ppl=57.26, wps=12831.4, ups=0.2, wpb=64867.4, bsz=126.7, num_updates=2800, lr=0.00035003, gnorm=1.306, loss_scale=16, train_wall=453, gb_free=8.3, wall=14278
2022-03-08 15:15:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:15:25 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.248 | ppl 607.92 | wps 18923.8 | wpb 510.9 | bsz 1 | num_updates 2820 | best_loss 8.682
2022-03-08 15:15:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 2820 updates
2022-03-08 15:15:25 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-08 15:15:25 | INFO | train | epoch 058 | loss 5.774 | ppl 54.72 | wps 12951.9 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 2820 | lr 0.00035253 | gnorm 1.346 | loss_scale 16 | train_wall 219 | gb_free 8.3 | wall 14383
KL Stats: Epoch 58 Divergences: Uniform: 3.8654679255212128 Unigram: 3.086023846300021
2022-03-08 15:15:25 | INFO | fairseq.trainer | begin training epoch 59
2022-03-08 15:15:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:19:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:19:31 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.259 | ppl 612.87 | wps 18879.1 | wpb 510.9 | bsz 1 | num_updates 2869 | best_loss 8.682
2022-03-08 15:19:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 2869 updates
2022-03-08 15:19:31 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-08 15:19:31 | INFO | train | epoch 059 | loss 5.682 | ppl 51.35 | wps 12938.5 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 2869 | lr 0.000358653 | gnorm 1.241 | loss_scale 16 | train_wall 220 | gb_free 8.3 | wall 14629
KL Stats: Epoch 59 Divergences: Uniform: 3.90158870465266 Unigram: 3.1371697522538127
2022-03-08 15:19:31 | INFO | fairseq.trainer | begin training epoch 60
2022-03-08 15:19:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:22:01 | INFO | train_inner | epoch 060:     31 / 49 loss=5.67, ppl=50.9, wps=12948.2, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=2900, lr=0.000362528, gnorm=1.196, loss_scale=32, train_wall=448, gb_free=8.3, wall=14779
2022-03-08 15:22:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 15:23:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:23:37 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.309 | ppl 634.38 | wps 18721.7 | wpb 510.9 | bsz 1 | num_updates 2917 | best_loss 8.682
2022-03-08 15:23:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 2917 updates
2022-03-08 15:23:37 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-08 15:23:37 | INFO | train | epoch 060 | loss 5.611 | ppl 48.87 | wps 12623.9 | ups 0.19 | wpb 64844.1 | bsz 126.7 | num_updates 2917 | lr 0.000364652 | gnorm 1.325 | loss_scale 16 | train_wall 220 | gb_free 8.3 | wall 14876
KL Stats: Epoch 60 Divergences: Uniform: 3.952660383389682 Unigram: 3.1870911709190937
2022-03-08 15:23:37 | INFO | fairseq.trainer | begin training epoch 61
2022-03-08 15:23:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:27:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:27:45 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.321 | ppl 639.47 | wps 18663.5 | wpb 510.9 | bsz 1 | num_updates 2966 | best_loss 8.682
2022-03-08 15:27:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 2966 updates
2022-03-08 15:27:45 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-08 15:27:45 | INFO | train | epoch 061 | loss 5.536 | ppl 46.4 | wps 12828.5 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 2966 | lr 0.000370776 | gnorm 1.242 | loss_scale 16 | train_wall 221 | gb_free 8.3 | wall 15123
KL Stats: Epoch 61 Divergences: Uniform: 4.0003439075011515 Unigram: 3.23870155695081
2022-03-08 15:27:45 | INFO | fairseq.trainer | begin training epoch 62
2022-03-08 15:27:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:30:31 | INFO | train_inner | epoch 062:     34 / 49 loss=5.525, ppl=46.05, wps=12719.3, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=3000, lr=0.000375025, gnorm=1.289, loss_scale=16, train_wall=456, gb_free=8.3, wall=15289
2022-03-08 15:31:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:31:53 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.391 | ppl 671.2 | wps 18642.7 | wpb 510.9 | bsz 1 | num_updates 3015 | best_loss 8.682
2022-03-08 15:31:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 3015 updates
2022-03-08 15:31:53 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-08 15:31:53 | INFO | train | epoch 062 | loss 5.461 | ppl 44.04 | wps 12824.1 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 3015 | lr 0.0003769 | gnorm 1.282 | loss_scale 16 | train_wall 221 | gb_free 8.3 | wall 15371
KL Stats: Epoch 62 Divergences: Uniform: 4.051201897778514 Unigram: 3.294675147745808
2022-03-08 15:31:53 | INFO | fairseq.trainer | begin training epoch 63
2022-03-08 15:31:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:35:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:36:01 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.421 | ppl 685.54 | wps 18772.3 | wpb 510.9 | bsz 1 | num_updates 3064 | best_loss 8.682
2022-03-08 15:36:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 3064 updates
2022-03-08 15:36:01 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-08 15:36:01 | INFO | train | epoch 063 | loss 5.392 | ppl 42 | wps 12828.3 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 3064 | lr 0.000383023 | gnorm 1.344 | loss_scale 32 | train_wall 221 | gb_free 8.3 | wall 15619
KL Stats: Epoch 63 Divergences: Uniform: 4.094693934214839 Unigram: 3.3367814794538666
2022-03-08 15:36:01 | INFO | fairseq.trainer | begin training epoch 64
2022-03-08 15:36:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:38:56 | INFO | train_inner | epoch 064:     36 / 49 loss=5.376, ppl=41.54, wps=12848, ups=0.2, wpb=64876.2, bsz=126.7, num_updates=3100, lr=0.000387523, gnorm=1.346, loss_scale=32, train_wall=451, gb_free=8.3, wall=15794
2022-03-08 15:39:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 15:39:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:40:08 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.488 | ppl 718.11 | wps 18661.5 | wpb 510.9 | bsz 1 | num_updates 3112 | best_loss 8.682
2022-03-08 15:40:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 3112 updates
2022-03-08 15:40:08 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-08 15:40:08 | INFO | train | epoch 064 | loss 5.319 | ppl 39.92 | wps 12580.6 | ups 0.19 | wpb 64844.1 | bsz 126.7 | num_updates 3112 | lr 0.000389022 | gnorm 1.39 | loss_scale 16 | train_wall 221 | gb_free 8.3 | wall 15866
KL Stats: Epoch 64 Divergences: Uniform: 4.137692395270451 Unigram: 3.3961697022862998
2022-03-08 15:40:08 | INFO | fairseq.trainer | begin training epoch 65
2022-03-08 15:40:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:44:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:44:16 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.535 | ppl 741.93 | wps 18692.4 | wpb 510.9 | bsz 1 | num_updates 3161 | best_loss 8.682
2022-03-08 15:44:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 3161 updates
2022-03-08 15:44:16 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-08 15:44:16 | INFO | train | epoch 065 | loss 5.241 | ppl 37.81 | wps 12837.1 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 3161 | lr 0.000395146 | gnorm 1.296 | loss_scale 16 | train_wall 221 | gb_free 8.3 | wall 16114
KL Stats: Epoch 65 Divergences: Uniform: 4.1921275654339585 Unigram: 3.447426982086188
2022-03-08 15:44:16 | INFO | fairseq.trainer | begin training epoch 66
2022-03-08 15:44:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:47:25 | INFO | train_inner | epoch 066:     39 / 49 loss=5.225, ppl=37.4, wps=12727.6, ups=0.2, wpb=64867.4, bsz=126.7, num_updates=3200, lr=0.00040002, gnorm=1.362, loss_scale=16, train_wall=456, gb_free=8.3, wall=16304
2022-03-08 15:48:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:48:23 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.57 | ppl 760.25 | wps 18720.3 | wpb 510.9 | bsz 1 | num_updates 3210 | best_loss 8.682
2022-03-08 15:48:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 3210 updates
2022-03-08 15:48:23 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-08 15:48:23 | INFO | train | epoch 066 | loss 5.179 | ppl 36.22 | wps 12840.2 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 3210 | lr 0.00040127 | gnorm 1.428 | loss_scale 16 | train_wall 221 | gb_free 8.3 | wall 16361
KL Stats: Epoch 66 Divergences: Uniform: 4.237255584557725 Unigram: 3.4991842069675565
2022-03-08 15:48:23 | INFO | fairseq.trainer | begin training epoch 67
2022-03-08 15:48:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:50:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 15:52:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:52:31 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.629 | ppl 791.72 | wps 18649.1 | wpb 510.9 | bsz 1 | num_updates 3258 | best_loss 8.682
2022-03-08 15:52:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 3258 updates
2022-03-08 15:52:31 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-08 15:52:31 | INFO | train | epoch 067 | loss 5.098 | ppl 34.24 | wps 12565.9 | ups 0.19 | wpb 64844.1 | bsz 126.7 | num_updates 3258 | lr 0.000407269 | gnorm 1.319 | loss_scale 16 | train_wall 221 | gb_free 8.3 | wall 16609
KL Stats: Epoch 67 Divergences: Uniform: 4.271493350418612 Unigram: 3.552950798861708
2022-03-08 15:52:31 | INFO | fairseq.trainer | begin training epoch 68
2022-03-08 15:52:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:55:55 | INFO | train_inner | epoch 068:     42 / 49 loss=5.08, ppl=33.82, wps=12723.3, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=3300, lr=0.000412518, gnorm=1.35, loss_scale=16, train_wall=456, gb_free=8.3, wall=16813
2022-03-08 15:56:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:56:38 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.635 | ppl 794.84 | wps 18736.9 | wpb 510.9 | bsz 1 | num_updates 3307 | best_loss 8.682
2022-03-08 15:56:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 3307 updates
2022-03-08 15:56:38 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-08 15:56:38 | INFO | train | epoch 068 | loss 5.031 | ppl 32.69 | wps 12835.5 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 3307 | lr 0.000413392 | gnorm 1.337 | loss_scale 16 | train_wall 221 | gb_free 8.3 | wall 16857
KL Stats: Epoch 68 Divergences: Uniform: 4.341150480934815 Unigram: 3.6071423937802067
2022-03-08 15:56:38 | INFO | fairseq.trainer | begin training epoch 69
2022-03-08 15:56:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:00:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:00:46 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.686 | ppl 823.58 | wps 18702.2 | wpb 510.9 | bsz 1 | num_updates 3356 | best_loss 8.682
2022-03-08 16:00:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 3356 updates
2022-03-08 16:00:46 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-08 16:00:46 | INFO | train | epoch 069 | loss 4.97 | ppl 31.34 | wps 12843.8 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 3356 | lr 0.000419516 | gnorm 1.411 | loss_scale 16 | train_wall 221 | gb_free 8.3 | wall 17104
KL Stats: Epoch 69 Divergences: Uniform: 4.385051634462421 Unigram: 3.6572586744299316
2022-03-08 16:00:46 | INFO | fairseq.trainer | begin training epoch 70
2022-03-08 16:00:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:01:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 16:04:25 | INFO | train_inner | epoch 070:     45 / 49 loss=4.943, ppl=30.75, wps=12733.2, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=3400, lr=0.000425015, gnorm=1.418, loss_scale=16, train_wall=455, gb_free=8.3, wall=17323
2022-03-08 16:04:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:04:53 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.747 | ppl 859.39 | wps 18667.4 | wpb 510.9 | bsz 1 | num_updates 3404 | best_loss 8.682
2022-03-08 16:04:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 3404 updates
2022-03-08 16:04:53 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-08 16:04:53 | INFO | train | epoch 070 | loss 4.897 | ppl 29.8 | wps 12574.7 | ups 0.19 | wpb 64844.1 | bsz 126.7 | num_updates 3404 | lr 0.000425515 | gnorm 1.417 | loss_scale 16 | train_wall 221 | gb_free 8.3 | wall 17352
KL Stats: Epoch 70 Divergences: Uniform: 4.432192627720625 Unigram: 3.7080296286140246
2022-03-08 16:04:53 | INFO | fairseq.trainer | begin training epoch 71
2022-03-08 16:04:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:08:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:09:01 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.777 | ppl 877.2 | wps 18659.2 | wpb 510.9 | bsz 1 | num_updates 3453 | best_loss 8.682
2022-03-08 16:09:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 3453 updates
2022-03-08 16:09:01 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-08 16:09:01 | INFO | train | epoch 071 | loss 4.832 | ppl 28.48 | wps 12842.3 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 3453 | lr 0.000431639 | gnorm 1.394 | loss_scale 16 | train_wall 221 | gb_free 8.3 | wall 17599
KL Stats: Epoch 71 Divergences: Uniform: 4.4925971572578955 Unigram: 3.7629450079092805
2022-03-08 16:09:01 | INFO | fairseq.trainer | begin training epoch 72
2022-03-08 16:09:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:12:49 | INFO | train_inner | epoch 072:     47 / 49 loss=4.804, ppl=27.93, wps=12853, ups=0.2, wpb=64871.8, bsz=126.7, num_updates=3500, lr=0.000437513, gnorm=1.409, loss_scale=32, train_wall=451, gb_free=8.3, wall=17828
2022-03-08 16:12:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:13:08 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.826 | ppl 907.41 | wps 18675.1 | wpb 510.9 | bsz 1 | num_updates 3502 | best_loss 8.682
2022-03-08 16:13:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 3502 updates
2022-03-08 16:13:08 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-08 16:13:08 | INFO | train | epoch 072 | loss 4.767 | ppl 27.23 | wps 12838.9 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 3502 | lr 0.000437762 | gnorm 1.44 | loss_scale 32 | train_wall 221 | gb_free 8.3 | wall 17847
KL Stats: Epoch 72 Divergences: Uniform: 4.5341968795408345 Unigram: 3.8083513238397235
2022-03-08 16:13:08 | INFO | fairseq.trainer | begin training epoch 73
2022-03-08 16:13:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:15:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 16:17:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:17:16 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.88 | ppl 942.32 | wps 18735.4 | wpb 510.9 | bsz 1 | num_updates 3550 | best_loss 8.682
2022-03-08 16:17:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 3550 updates
2022-03-08 16:17:16 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-08 16:17:16 | INFO | train | epoch 073 | loss 4.703 | ppl 26.05 | wps 12573 | ups 0.19 | wpb 64844.1 | bsz 126.7 | num_updates 3550 | lr 0.000443761 | gnorm 1.405 | loss_scale 16 | train_wall 221 | gb_free 8.3 | wall 18094
KL Stats: Epoch 73 Divergences: Uniform: 4.5920648558101815 Unigram: 3.8615345136454455
2022-03-08 16:17:16 | INFO | fairseq.trainer | begin training epoch 74
2022-03-08 16:17:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:21:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:21:24 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.954 | ppl 991.81 | wps 18686.2 | wpb 510.9 | bsz 1 | num_updates 3599 | best_loss 8.682
2022-03-08 16:21:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 3599 updates
2022-03-08 16:21:24 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-08 16:21:24 | INFO | train | epoch 074 | loss 4.629 | ppl 24.74 | wps 12833.2 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 3599 | lr 0.000449885 | gnorm 1.342 | loss_scale 16 | train_wall 221 | gb_free 8.3 | wall 18342
KL Stats: Epoch 74 Divergences: Uniform: 4.628181695646871 Unigram: 3.9186968917300713
2022-03-08 16:21:24 | INFO | fairseq.trainer | begin training epoch 75
2022-03-08 16:21:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:21:28 | INFO | train_inner | epoch 075:      1 / 49 loss=4.667, ppl=25.4, wps=12431.3, ups=0.19, wpb=64544.1, bsz=126.1, num_updates=3600, lr=0.00045001, gnorm=1.374, loss_scale=16, train_wall=453, gb_free=8.3, wall=18347
2022-03-08 16:25:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:25:31 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.971 | ppl 1003.74 | wps 18700 | wpb 510.9 | bsz 1 | num_updates 3648 | best_loss 8.682
2022-03-08 16:25:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 3648 updates
2022-03-08 16:25:31 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-08 16:25:31 | INFO | train | epoch 075 | loss 4.566 | ppl 23.68 | wps 12830.6 | ups 0.2 | wpb 64858.2 | bsz 126.7 | num_updates 3648 | lr 0.000456009 | gnorm 1.365 | loss_scale 16 | train_wall 221 | gb_free 8.3 | wall 18590
KL Stats: Epoch 75 Divergences: Uniform: 4.676241524314931 Unigram: 3.9683585367529384
2022-03-08 16:25:31 | INFO | fairseq.trainer | begin training epoch 76
2022-03-08 16:25:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:26:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
