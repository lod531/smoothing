Sender: LSF System <lsfadmin@eu-g3-072>
Subject: Job 206858932: <w103_fp16_size_0.25_jelinek_0.09_0.01_0.9_#1> in cluster <euler> Exited

Job <w103_fp16_size_0.25_jelinek_0.09_0.01_0.9_#1> was submitted from host <eu-login-36> by user <andriusb> in cluster <euler> at Wed Mar  2 08:31:57 2022
Job was executed on host(s) <eu-g3-072>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Wed Mar  2 08:47:22 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Mar  2 08:47:22 2022
Terminated at Thu Mar  3 09:44:41 2022
Results reported at Thu Mar  3 09:44:41 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.09, 0.01, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321671 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   89793.41 sec.
    Max Memory :                                 8475 MB
    Average Memory :                             3090.11 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11525.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   89839 sec.
    Turnaround time :                            90764 sec.

The output (if any) follows:

2022-03-02 08:47:29 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321671, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321671, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.09, 0.01, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-02 08:47:30 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-02 08:47:34 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
Calculating frequency stats:
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 678/450337 [00:00<01:06, 6765.84it/s]  0%|          | 1355/450337 [00:00<01:17, 5830.33it/s]  0%|          | 1947/450337 [00:00<01:20, 5552.82it/s]  1%|          | 2507/450337 [00:00<01:20, 5534.03it/s]  1%|          | 3209/450337 [00:00<01:14, 6026.19it/s]  1%|          | 3816/450337 [00:00<01:14, 5975.11it/s]  1%|          | 4520/450337 [00:00<01:10, 6310.58it/s]  1%|          | 5235/450337 [00:00<01:07, 6564.12it/s]  1%|▏         | 5916/450337 [00:00<01:06, 6637.60it/s]  1%|▏         | 6582/450337 [00:01<01:13, 6018.34it/s]  2%|▏         | 7196/450337 [00:01<01:13, 6024.14it/s]  2%|▏         | 7807/450337 [00:01<01:13, 6002.03it/s]  2%|▏         | 8413/450337 [00:01<01:15, 5856.51it/s]  2%|▏         | 9071/450337 [00:01<01:12, 6059.51it/s]  2%|▏         | 9681/450337 [00:01<01:14, 5912.29it/s]  2%|▏         | 10310/450337 [00:01<01:13, 6018.88it/s]  2%|▏         | 10915/450337 [00:01<01:14, 5878.02it/s]  3%|▎         | 11507/450337 [00:01<01:14, 5889.81it/s]  3%|▎         | 12146/450337 [00:02<01:12, 6029.49it/s]  3%|▎         | 12751/450337 [00:02<01:14, 5843.53it/s]  3%|▎         | 13350/450337 [00:02<01:14, 5883.47it/s]  3%|▎         | 13994/450337 [00:02<01:12, 6044.73it/s]  3%|▎         | 14600/450337 [00:02<01:13, 5948.44it/s]  3%|▎         | 15282/450337 [00:02<01:10, 6201.46it/s]  4%|▎         | 15904/450337 [00:02<01:13, 5871.90it/s]  4%|▎         | 16496/450337 [00:02<01:15, 5775.96it/s]  4%|▍         | 17102/450337 [00:02<01:13, 5855.69it/s]  4%|▍         | 17691/450337 [00:02<01:14, 5791.23it/s]  4%|▍         | 18290/450337 [00:03<01:13, 5847.90it/s]  4%|▍         | 18994/450337 [00:03<01:09, 6193.42it/s]  4%|▍         | 19652/450337 [00:03<01:08, 6307.01it/s]  5%|▍         | 20285/450337 [00:03<01:12, 5910.17it/s]  5%|▍         | 20899/450337 [00:03<01:11, 5967.06it/s]  5%|▍         | 21501/450337 [00:03<01:16, 5624.25it/s]  5%|▍         | 22127/450337 [00:03<01:13, 5791.96it/s]  5%|▌         | 22802/450337 [00:03<01:10, 6064.46it/s]  5%|▌         | 23414/450337 [00:03<01:10, 6062.63it/s]  5%|▌         | 24154/450337 [00:04<01:06, 6449.11it/s]  6%|▌         | 24860/450337 [00:04<01:04, 6625.66it/s]  6%|▌         | 25526/450337 [00:04<01:04, 6566.61it/s]  6%|▌         | 26185/450337 [00:04<01:07, 6304.77it/s]  6%|▌         | 26819/450337 [00:04<01:12, 5880.37it/s]  6%|▌         | 27415/450337 [00:04<01:13, 5728.42it/s]  6%|▌         | 28002/450337 [00:04<01:13, 5765.09it/s]  6%|▋         | 28732/450337 [00:04<01:08, 6190.68it/s]  7%|▋         | 29356/450337 [00:04<01:11, 5895.70it/s]  7%|▋         | 30024/450337 [00:04<01:08, 6111.43it/s]  7%|▋         | 30641/450337 [00:05<01:12, 5766.11it/s]  7%|▋         | 31225/450337 [00:05<01:14, 5620.92it/s]  7%|▋         | 31842/450337 [00:05<01:12, 5769.45it/s]  7%|▋         | 32424/450337 [00:05<01:13, 5673.73it/s]  7%|▋         | 32995/450337 [00:05<01:13, 5647.37it/s]  7%|▋         | 33562/450337 [00:05<01:14, 5613.32it/s]  8%|▊         | 34152/450337 [00:05<01:13, 5693.22it/s]  8%|▊         | 34855/450337 [00:05<01:08, 6083.02it/s]  8%|▊         | 35466/450337 [00:05<01:11, 5804.92it/s]  8%|▊         | 36051/450337 [00:06<01:11, 5810.62it/s]  8%|▊         | 36652/450337 [00:06<01:10, 5864.69it/s]  8%|▊         | 37241/450337 [00:06<01:13, 5630.27it/s]  8%|▊         | 37808/450337 [00:06<01:14, 5507.88it/s]  9%|▊         | 38362/450337 [00:06<01:15, 5482.09it/s]  9%|▊         | 38962/450337 [00:06<01:13, 5628.92it/s]  9%|▉         | 39527/450337 [00:06<01:13, 5578.72it/s]  9%|▉         | 40172/450337 [00:06<01:10, 5832.06it/s]  9%|▉         | 40863/450337 [00:06<01:06, 6143.09it/s]  9%|▉         | 41479/450337 [00:06<01:08, 5966.76it/s]  9%|▉         | 42078/450337 [00:07<01:12, 5593.02it/s]  9%|▉         | 42643/450337 [00:07<01:12, 5588.84it/s] 10%|▉         | 43206/450337 [00:07<01:13, 5553.55it/s] 10%|▉         | 43872/450337 [00:07<01:09, 5868.73it/s] 10%|▉         | 44462/450337 [00:07<01:09, 5863.11it/s] 10%|█         | 45056/450337 [00:07<01:08, 5880.53it/s] 10%|█         | 45692/450337 [00:07<01:07, 6019.13it/s] 10%|█         | 46392/450337 [00:07<01:04, 6301.97it/s] 10%|█         | 47024/450337 [00:07<01:05, 6182.02it/s] 11%|█         | 47957/450337 [00:08<00:56, 7102.41it/s] 11%|█         | 48670/450337 [00:08<00:57, 6965.85it/s] 11%|█         | 49369/450337 [00:08<00:57, 6945.27it/s] 11%|█         | 50066/450337 [00:08<01:00, 6599.56it/s] 11%|█▏        | 50731/450337 [00:08<01:04, 6159.55it/s] 11%|█▏        | 51355/450337 [00:08<01:05, 6079.81it/s] 12%|█▏        | 52074/450337 [00:08<01:02, 6379.30it/s] 12%|█▏        | 52817/450337 [00:08<00:59, 6677.51it/s] 12%|█▏        | 53491/450337 [00:08<01:03, 6286.99it/s] 12%|█▏        | 54128/450337 [00:09<01:04, 6188.62it/s] 12%|█▏        | 54752/450337 [00:09<01:07, 5868.79it/s] 12%|█▏        | 55345/450337 [00:09<01:07, 5872.46it/s] 12%|█▏        | 56042/450337 [00:09<01:03, 6180.42it/s] 13%|█▎        | 56665/450337 [00:09<01:06, 5961.04it/s] 13%|█▎        | 57266/450337 [00:09<01:11, 5520.83it/s] 13%|█▎        | 57888/450337 [00:09<01:08, 5709.97it/s] 13%|█▎        | 58614/450337 [00:09<01:03, 6140.75it/s] 13%|█▎        | 59237/450337 [00:09<01:05, 5941.11it/s] 13%|█▎        | 59838/450337 [00:09<01:06, 5830.95it/s] 13%|█▎        | 60431/450337 [00:10<01:06, 5851.18it/s] 14%|█▎        | 61187/450337 [00:10<01:01, 6341.99it/s] 14%|█▎        | 61826/450337 [00:10<01:02, 6175.62it/s] 14%|█▍        | 62455/450337 [00:10<01:02, 6205.29it/s] 14%|█▍        | 63079/450337 [00:10<01:06, 5813.36it/s] 14%|█▍        | 63696/450337 [00:10<01:05, 5911.17it/s] 14%|█▍        | 64293/450337 [00:10<01:06, 5822.42it/s] 14%|█▍        | 64932/450337 [00:10<01:04, 5983.62it/s] 15%|█▍        | 65534/450337 [00:10<01:06, 5796.14it/s] 15%|█▍        | 66146/450337 [00:11<01:05, 5880.09it/s] 15%|█▍        | 66893/450337 [00:11<01:00, 6336.00it/s] 15%|█▍        | 67530/450337 [00:11<01:05, 5838.59it/s] 15%|█▌        | 68124/450337 [00:11<01:07, 5701.18it/s] 15%|█▌        | 68894/450337 [00:11<01:01, 6251.67it/s] 15%|█▌        | 69528/450337 [00:11<01:02, 6117.63it/s] 16%|█▌        | 70164/450337 [00:11<01:01, 6179.02it/s] 16%|█▌        | 70787/450337 [00:11<01:03, 5987.39it/s] 16%|█▌        | 71427/450337 [00:11<01:02, 6103.65it/s] 16%|█▌        | 72045/450337 [00:12<01:01, 6123.53it/s] 16%|█▌        | 72702/450337 [00:12<01:00, 6247.12it/s] 16%|█▋        | 73441/450337 [00:12<00:57, 6572.59it/s] 16%|█▋        | 74215/450337 [00:12<00:54, 6902.60it/s] 17%|█▋        | 74907/450337 [00:12<00:56, 6672.67it/s] 17%|█▋        | 75577/450337 [00:12<00:59, 6246.50it/s] 17%|█▋        | 76209/450337 [00:12<00:59, 6261.61it/s] 17%|█▋        | 76840/450337 [00:12<00:59, 6240.38it/s] 17%|█▋        | 77489/450337 [00:12<00:59, 6304.75it/s] 17%|█▋        | 78122/450337 [00:12<01:00, 6151.86it/s] 17%|█▋        | 78740/450337 [00:13<01:03, 5810.12it/s] 18%|█▊        | 79326/450337 [00:13<01:07, 5486.10it/s] 18%|█▊        | 79920/450337 [00:13<01:06, 5608.39it/s] 18%|█▊        | 80495/450337 [00:13<01:05, 5647.23it/s] 18%|█▊        | 81168/450337 [00:13<01:01, 5956.98it/s] 18%|█▊        | 81772/450337 [00:13<01:01, 5978.42it/s] 18%|█▊        | 82376/450337 [00:13<01:01, 5994.67it/s] 18%|█▊        | 83010/450337 [00:13<01:00, 6090.24it/s] 19%|█▊        | 83679/450337 [00:13<00:58, 6263.10it/s] 19%|█▊        | 84307/450337 [00:14<01:00, 6052.04it/s] 19%|█▉        | 84949/450337 [00:14<00:59, 6153.69it/s] 19%|█▉        | 85567/450337 [00:14<00:59, 6122.75it/s] 19%|█▉        | 86181/450337 [00:14<01:00, 6002.95it/s] 19%|█▉        | 86839/450337 [00:14<00:58, 6170.25it/s] 19%|█▉        | 87525/450337 [00:14<00:56, 6365.91it/s] 20%|█▉        | 88188/450337 [00:14<00:56, 6440.31it/s] 20%|█▉        | 88834/450337 [00:14<01:01, 5852.74it/s] 20%|█▉        | 89430/450337 [00:14<01:02, 5755.58it/s] 20%|█▉        | 90013/450337 [00:14<01:05, 5521.17it/s] 20%|██        | 90586/450337 [00:15<01:04, 5578.14it/s] 20%|██        | 91259/450337 [00:15<01:00, 5903.23it/s] 20%|██        | 91855/450337 [00:15<01:00, 5895.41it/s] 21%|██        | 92529/450337 [00:15<00:58, 6139.14it/s] 21%|██        | 93147/450337 [00:15<01:00, 5919.90it/s] 21%|██        | 93746/450337 [00:15<01:00, 5931.67it/s] 21%|██        | 94342/450337 [00:15<01:01, 5777.51it/s] 21%|██        | 95039/450337 [00:15<00:58, 6114.88it/s] 21%|██        | 95654/450337 [00:15<01:00, 5847.00it/s] 21%|██▏       | 96331/450337 [00:16<00:57, 6106.61it/s] 22%|██▏       | 97120/450337 [00:16<00:53, 6612.28it/s] 22%|██▏       | 97786/450337 [00:16<00:56, 6223.61it/s] 22%|██▏       | 98416/450337 [00:16<00:56, 6223.37it/s] 22%|██▏       | 99044/450337 [00:16<00:57, 6136.52it/s] 22%|██▏       | 99662/450337 [00:16<00:59, 5889.07it/s] 22%|██▏       | 100255/450337 [00:16<01:00, 5798.57it/s] 22%|██▏       | 100856/450337 [00:16<00:59, 5858.12it/s] 23%|██▎       | 101444/450337 [00:16<01:00, 5799.91it/s] 23%|██▎       | 102134/450337 [00:16<00:56, 6116.13it/s] 23%|██▎       | 102748/450337 [00:17<00:57, 6048.90it/s] 23%|██▎       | 103355/450337 [00:17<00:58, 5904.99it/s] 23%|██▎       | 103947/450337 [00:17<01:00, 5766.08it/s] 23%|██▎       | 104575/450337 [00:17<00:58, 5912.65it/s] 23%|██▎       | 105168/450337 [00:17<00:59, 5833.10it/s] 24%|██▎       | 105853/450337 [00:17<00:56, 6128.21it/s] 24%|██▎       | 106468/450337 [00:17<00:58, 5866.90it/s] 24%|██▍       | 107058/450337 [00:17<00:58, 5847.24it/s] 24%|██▍       | 107645/450337 [00:17<00:58, 5823.14it/s] 24%|██▍       | 108229/450337 [00:18<00:59, 5769.42it/s] 24%|██▍       | 108807/450337 [00:18<01:01, 5535.76it/s] 24%|██▍       | 109406/450337 [00:18<01:00, 5658.10it/s] 24%|██▍       | 110017/450337 [00:18<00:58, 5787.50it/s] 25%|██▍       | 110648/450337 [00:18<00:57, 5934.10it/s] 25%|██▍       | 111289/450337 [00:18<00:55, 6073.94it/s] 25%|██▍       | 111898/450337 [00:18<00:57, 5931.64it/s] 25%|██▍       | 112547/450337 [00:18<00:55, 6092.66it/s] 25%|██▌       | 113158/450337 [00:18<00:56, 5945.65it/s] 25%|██▌       | 113755/450337 [00:18<00:58, 5759.40it/s] 25%|██▌       | 114405/450337 [00:19<00:56, 5966.22it/s] 26%|██▌       | 115010/450337 [00:19<00:55, 5990.01it/s] 26%|██▌       | 115611/450337 [00:19<01:00, 5536.43it/s] 26%|██▌       | 116227/450337 [00:19<00:58, 5705.55it/s] 26%|██▌       | 116804/450337 [00:19<00:59, 5624.33it/s] 26%|██▌       | 117371/450337 [00:19<00:59, 5632.48it/s] 26%|██▌       | 117965/450337 [00:19<00:58, 5716.92it/s] 26%|██▋       | 118612/450337 [00:19<00:55, 5933.35it/s] 26%|██▋       | 119208/450337 [00:19<00:56, 5810.42it/s] 27%|██▋       | 119792/450337 [00:20<00:57, 5762.86it/s] 27%|██▋       | 120565/450337 [00:20<00:52, 6330.53it/s] 27%|██▋       | 121201/450337 [00:20<00:53, 6173.73it/s] 27%|██▋       | 121821/450337 [00:20<00:56, 5853.11it/s] 27%|██▋       | 122441/450337 [00:20<00:55, 5950.43it/s] 27%|██▋       | 123040/450337 [00:20<00:55, 5879.83it/s] 27%|██▋       | 123631/450337 [00:20<00:55, 5834.40it/s] 28%|██▊       | 124225/450337 [00:20<00:55, 5861.58it/s] 28%|██▊       | 124813/450337 [00:20<00:56, 5767.49it/s] 28%|██▊       | 125411/450337 [00:20<00:55, 5828.88it/s] 28%|██▊       | 126138/450337 [00:21<00:51, 6246.52it/s] 28%|██▊       | 126767/450337 [00:21<00:51, 6257.02it/s] 28%|██▊       | 127394/450337 [00:21<00:54, 5973.12it/s] 28%|██▊       | 128042/450337 [00:21<00:52, 6113.56it/s] 29%|██▊       | 128657/450337 [00:21<00:55, 5830.77it/s] 29%|██▊       | 129282/450337 [00:21<00:54, 5945.00it/s] 29%|██▉       | 129881/450337 [00:21<00:56, 5662.49it/s] 29%|██▉       | 130489/450337 [00:21<00:55, 5778.36it/s] 29%|██▉       | 131071/450337 [00:21<00:56, 5662.00it/s] 29%|██▉       | 131641/450337 [00:22<00:59, 5333.05it/s] 29%|██▉       | 132180/450337 [00:22<00:59, 5341.62it/s] 29%|██▉       | 132843/450337 [00:22<00:55, 5704.44it/s] 30%|██▉       | 133418/450337 [00:22<00:55, 5717.43it/s] 30%|██▉       | 134101/450337 [00:22<00:52, 6036.34it/s] 30%|██▉       | 134805/450337 [00:22<00:49, 6330.95it/s] 30%|███       | 135534/450337 [00:22<00:47, 6611.52it/s] 30%|███       | 136198/450337 [00:22<00:48, 6459.56it/s] 30%|███       | 136915/450337 [00:22<00:47, 6664.77it/s] 31%|███       | 137596/450337 [00:22<00:46, 6702.53it/s] 31%|███       | 138268/450337 [00:23<00:49, 6245.64it/s] 31%|███       | 138919/450337 [00:23<00:49, 6318.73it/s] 31%|███       | 139557/450337 [00:23<00:49, 6272.94it/s] 31%|███       | 140188/450337 [00:23<00:50, 6112.58it/s] 31%|███▏      | 140803/450337 [00:23<00:50, 6086.50it/s] 31%|███▏      | 141414/450337 [00:23<00:52, 5900.29it/s] 32%|███▏      | 142009/450337 [00:23<00:52, 5912.81it/s] 32%|███▏      | 142602/450337 [00:23<00:52, 5846.32it/s] 32%|███▏      | 143188/450337 [00:23<00:54, 5681.49it/s] 32%|███▏      | 143972/450337 [00:24<00:48, 6297.46it/s] 32%|███▏      | 144606/450337 [00:24<00:48, 6286.67it/s] 32%|███▏      | 145280/450337 [00:24<00:47, 6419.15it/s] 32%|███▏      | 145941/450337 [00:24<00:47, 6474.68it/s] 33%|███▎      | 146591/450337 [00:24<00:49, 6185.79it/s] 33%|███▎      | 147214/450337 [00:24<00:50, 5963.89it/s] 33%|███▎      | 147814/450337 [00:24<00:54, 5578.80it/s] 33%|███▎      | 148406/450337 [00:24<00:53, 5671.36it/s] 33%|███▎      | 149070/450337 [00:24<00:50, 5942.54it/s] 33%|███▎      | 149670/450337 [00:24<00:52, 5722.70it/s] 33%|███▎      | 150248/450337 [00:25<00:53, 5627.65it/s] 33%|███▎      | 150842/450337 [00:25<00:52, 5715.64it/s] 34%|███▎      | 151442/450337 [00:25<00:51, 5793.78it/s] 34%|███▍      | 152024/450337 [00:25<00:52, 5719.62it/s] 34%|███▍      | 152598/450337 [00:25<00:52, 5686.48it/s] 34%|███▍      | 153327/450337 [00:25<00:48, 6151.31it/s] 34%|███▍      | 153945/450337 [00:25<00:51, 5788.45it/s] 34%|███▍      | 154530/450337 [00:25<00:51, 5764.41it/s] 34%|███▍      | 155158/450337 [00:25<00:49, 5911.61it/s] 35%|███▍      | 155788/450337 [00:26<00:48, 6023.61it/s] 35%|███▍      | 156423/450337 [00:26<00:48, 6118.46it/s] 35%|███▍      | 157048/450337 [00:26<00:47, 6156.40it/s] 35%|███▌      | 157692/450337 [00:26<00:46, 6237.17it/s] 35%|███▌      | 158403/450337 [00:26<00:45, 6484.63it/s] 35%|███▌      | 159053/450337 [00:26<00:48, 6006.95it/s] 35%|███▌      | 159662/450337 [00:26<00:48, 6004.70it/s] 36%|███▌      | 160295/450337 [00:26<00:47, 6088.17it/s] 36%|███▌      | 160969/450337 [00:26<00:46, 6277.26it/s] 36%|███▌      | 161601/450337 [00:26<00:47, 6066.33it/s] 36%|███▌      | 162223/450337 [00:27<00:47, 6108.18it/s] 36%|███▌      | 162859/450337 [00:27<00:46, 6179.30it/s] 36%|███▋      | 163480/450337 [00:27<00:46, 6104.05it/s] 36%|███▋      | 164092/450337 [00:27<00:49, 5814.69it/s] 37%|███▋      | 164739/450337 [00:27<00:47, 5998.50it/s] 37%|███▋      | 165378/450337 [00:27<00:46, 6105.78it/s] 37%|███▋      | 166019/450337 [00:27<00:45, 6190.19it/s] 37%|███▋      | 166687/450337 [00:27<00:44, 6333.84it/s] 37%|███▋      | 167323/450337 [00:27<00:45, 6262.76it/s] 37%|███▋      | 167951/450337 [00:28<00:47, 5958.16it/s] 37%|███▋      | 168583/450337 [00:28<00:46, 6058.79it/s] 38%|███▊      | 169195/450337 [00:28<00:46, 6076.46it/s] 38%|███▊      | 169805/450337 [00:28<00:48, 5798.81it/s] 38%|███▊      | 170389/450337 [00:28<00:49, 5628.40it/s] 38%|███▊      | 170984/450337 [00:28<00:48, 5716.62it/s] 38%|███▊      | 171631/450337 [00:28<00:46, 5931.98it/s] 38%|███▊      | 172227/450337 [00:28<00:48, 5685.13it/s] 38%|███▊      | 172857/450337 [00:28<00:47, 5855.29it/s] 39%|███▊      | 173450/450337 [00:28<00:47, 5874.93it/s] 39%|███▊      | 174057/450337 [00:29<00:46, 5930.82it/s] 39%|███▉      | 174735/450337 [00:29<00:44, 6180.34it/s] 39%|███▉      | 175355/450337 [00:29<00:46, 5927.53it/s] 39%|███▉      | 175951/450337 [00:29<00:47, 5811.64it/s] 39%|███▉      | 176535/450337 [00:29<00:48, 5663.07it/s] 39%|███▉      | 177127/450337 [00:29<00:47, 5724.19it/s] 39%|███▉      | 177702/450337 [00:29<00:51, 5252.87it/s] 40%|███▉      | 178345/450337 [00:29<00:48, 5573.63it/s] 40%|███▉      | 178964/450337 [00:29<00:47, 5745.18it/s] 40%|███▉      | 179584/450337 [00:30<00:46, 5872.32it/s] 40%|████      | 180177/450337 [00:30<00:47, 5675.24it/s] 40%|████      | 180816/450337 [00:30<00:45, 5872.25it/s] 40%|████      | 181408/450337 [00:30<00:47, 5706.73it/s] 40%|████      | 181983/450337 [00:30<00:48, 5561.93it/s] 41%|████      | 182557/450337 [00:30<00:47, 5612.15it/s] 41%|████      | 183152/450337 [00:30<00:46, 5706.32it/s] 41%|████      | 183725/450337 [00:30<00:49, 5357.10it/s] 41%|████      | 184266/450337 [00:30<00:52, 5106.64it/s] 41%|████      | 184898/450337 [00:31<00:48, 5440.10it/s] 41%|████      | 185449/450337 [00:31<00:49, 5341.94it/s] 41%|████▏     | 186008/450337 [00:31<00:48, 5408.34it/s] 41%|████▏     | 186555/450337 [00:31<00:48, 5421.19it/s] 42%|████▏     | 187204/450337 [00:31<00:45, 5730.48it/s] 42%|████▏     | 187793/450337 [00:31<00:45, 5777.17it/s] 42%|████▏     | 188412/450337 [00:31<00:44, 5893.21it/s] 42%|████▏     | 189028/450337 [00:31<00:43, 5970.44it/s] 42%|████▏     | 189659/450337 [00:31<00:42, 6064.33it/s] 42%|████▏     | 190267/450337 [00:31<00:43, 5977.40it/s] 42%|████▏     | 190880/450337 [00:32<00:43, 6021.37it/s] 43%|████▎     | 191483/450337 [00:32<00:44, 5856.82it/s] 43%|████▎     | 192140/450337 [00:32<00:42, 6064.29it/s] 43%|████▎     | 192811/450337 [00:32<00:41, 6247.91it/s] 43%|████▎     | 193438/450337 [00:32<00:41, 6132.75it/s] 43%|████▎     | 194076/450337 [00:32<00:41, 6200.82it/s] 43%|████▎     | 194698/450337 [00:32<00:42, 5948.95it/s] 43%|████▎     | 195338/450337 [00:32<00:41, 6078.60it/s] 44%|████▎     | 195969/450337 [00:32<00:41, 6137.13it/s] 44%|████▎     | 196585/450337 [00:32<00:43, 5840.38it/s] 44%|████▍     | 197453/450337 [00:33<00:38, 6646.62it/s] 44%|████▍     | 198125/450337 [00:33<00:39, 6416.23it/s] 44%|████▍     | 198773/450337 [00:33<00:40, 6185.72it/s] 44%|████▍     | 199397/450337 [00:33<00:40, 6172.11it/s] 44%|████▍     | 200018/450337 [00:33<00:42, 5879.60it/s] 45%|████▍     | 200611/450337 [00:33<00:45, 5527.48it/s] 45%|████▍     | 201254/450337 [00:33<00:43, 5765.98it/s] 45%|████▍     | 201837/450337 [00:33<00:43, 5752.78it/s] 45%|████▍     | 202417/450337 [00:33<00:43, 5763.39it/s] 45%|████▌     | 203041/450337 [00:34<00:41, 5898.33it/s] 45%|████▌     | 203713/450337 [00:34<00:40, 6137.75it/s] 45%|████▌     | 204330/450337 [00:34<00:41, 5937.67it/s] 46%|████▌     | 204927/450337 [00:34<00:42, 5806.22it/s] 46%|████▌     | 205526/450337 [00:34<00:41, 5855.61it/s] 46%|████▌     | 206114/450337 [00:34<00:41, 5818.88it/s] 46%|████▌     | 206744/450337 [00:34<00:40, 5953.85it/s] 46%|████▌     | 207341/450337 [00:34<00:42, 5751.80it/s] 46%|████▌     | 207934/450337 [00:34<00:41, 5803.08it/s] 46%|████▋     | 208516/450337 [00:34<00:42, 5714.15it/s] 46%|████▋     | 209089/450337 [00:35<00:44, 5409.57it/s] 47%|████▋     | 209757/450337 [00:35<00:41, 5765.62it/s] 47%|████▋     | 210442/450337 [00:35<00:39, 6077.02it/s] 47%|████▋     | 211055/450337 [00:35<00:39, 6012.49it/s] 47%|████▋     | 211660/450337 [00:35<00:40, 5926.63it/s] 47%|████▋     | 212290/450337 [00:35<00:39, 6031.36it/s] 47%|████▋     | 212981/450337 [00:35<00:37, 6281.68it/s] 47%|████▋     | 213612/450337 [00:35<00:39, 5922.08it/s] 48%|████▊     | 214210/450337 [00:35<00:40, 5799.38it/s] 48%|████▊     | 214794/450337 [00:36<00:43, 5423.29it/s] 48%|████▊     | 215380/450337 [00:36<00:42, 5536.58it/s] 48%|████▊     | 215947/450337 [00:36<00:42, 5570.39it/s] 48%|████▊     | 216508/450337 [00:36<00:43, 5430.14it/s] 48%|████▊     | 217077/450337 [00:36<00:42, 5500.75it/s] 48%|████▊     | 217675/450337 [00:36<00:41, 5631.17it/s] 48%|████▊     | 218330/450337 [00:36<00:39, 5899.23it/s] 49%|████▊     | 218923/450337 [00:36<00:39, 5895.79it/s] 49%|████▊     | 219536/450337 [00:36<00:38, 5959.38it/s] 49%|████▉     | 220134/450337 [00:36<00:38, 5927.24it/s] 49%|████▉     | 220728/450337 [00:37<00:39, 5813.45it/s] 49%|████▉     | 221363/450337 [00:37<00:38, 5967.48it/s] 49%|████▉     | 221961/450337 [00:37<00:39, 5793.80it/s] 49%|████▉     | 222542/450337 [00:37<00:41, 5555.19it/s] 50%|████▉     | 223150/450337 [00:37<00:39, 5702.37it/s] 50%|████▉     | 223723/450337 [00:37<00:41, 5419.79it/s] 50%|████▉     | 224421/450337 [00:37<00:38, 5852.93it/s] 50%|████▉     | 225048/450337 [00:37<00:37, 5966.13it/s] 50%|█████     | 225715/450337 [00:37<00:36, 6167.33it/s] 50%|█████     | 226417/450337 [00:38<00:34, 6416.98it/s] 50%|█████     | 227062/450337 [00:38<00:36, 6145.32it/s] 51%|█████     | 227681/450337 [00:38<00:37, 5967.81it/s] 51%|█████     | 228282/450337 [00:38<00:37, 5930.21it/s] 51%|█████     | 228878/450337 [00:38<00:38, 5703.35it/s] 51%|█████     | 229452/450337 [00:38<00:39, 5597.76it/s] 51%|█████     | 230014/450337 [00:38<00:40, 5471.70it/s] 51%|█████     | 230674/450337 [00:38<00:37, 5789.56it/s] 51%|█████▏    | 231256/450337 [00:38<00:38, 5737.94it/s] 51%|█████▏    | 231832/450337 [00:39<00:39, 5572.03it/s] 52%|█████▏    | 232413/450337 [00:39<00:38, 5637.80it/s] 52%|█████▏    | 233125/450337 [00:39<00:35, 6066.40it/s] 52%|█████▏    | 233813/450337 [00:39<00:34, 6286.28it/s] 52%|█████▏    | 234493/450337 [00:39<00:33, 6434.62it/s] 52%|█████▏    | 235139/450337 [00:39<00:34, 6274.13it/s] 52%|█████▏    | 235775/450337 [00:39<00:34, 6291.69it/s] 52%|█████▏    | 236406/450337 [00:39<00:37, 5709.82it/s] 53%|█████▎    | 236988/450337 [00:39<00:37, 5697.64it/s] 53%|█████▎    | 237602/450337 [00:39<00:36, 5817.94it/s] 53%|█████▎    | 238190/450337 [00:40<00:37, 5653.65it/s] 53%|█████▎    | 238923/450337 [00:40<00:34, 6121.86it/s] 53%|█████▎    | 239541/450337 [00:40<00:34, 6063.31it/s] 53%|█████▎    | 240152/450337 [00:40<00:36, 5809.00it/s] 54%|█████▎    | 240942/450337 [00:40<00:32, 6393.46it/s] 54%|█████▎    | 241588/450337 [00:40<00:33, 6320.59it/s] 54%|█████▍    | 242225/450337 [00:40<00:32, 6334.70it/s] 54%|█████▍    | 242862/450337 [00:40<00:33, 6216.47it/s] 54%|█████▍    | 243487/450337 [00:40<00:33, 6181.82it/s] 54%|█████▍    | 244107/450337 [00:41<00:33, 6072.29it/s] 54%|█████▍    | 244716/450337 [00:41<00:34, 6027.31it/s] 54%|█████▍    | 245320/450337 [00:41<00:34, 6027.60it/s] 55%|█████▍    | 245986/450337 [00:41<00:32, 6212.61it/s] 55%|█████▍    | 246697/450337 [00:41<00:31, 6477.63it/s] 55%|█████▍    | 247359/450337 [00:41<00:31, 6509.43it/s] 55%|█████▌    | 248011/450337 [00:41<00:32, 6279.68it/s] 55%|█████▌    | 248642/450337 [00:41<00:32, 6192.57it/s] 55%|█████▌    | 249344/450337 [00:41<00:31, 6427.02it/s] 56%|█████▌    | 249997/450337 [00:41<00:31, 6451.51it/s] 56%|█████▌    | 250644/450337 [00:42<00:31, 6301.71it/s] 56%|█████▌    | 251279/450337 [00:42<00:31, 6315.16it/s] 56%|█████▌    | 251912/450337 [00:42<00:31, 6278.82it/s] 56%|█████▌    | 252541/450337 [00:42<00:31, 6182.50it/s] 56%|█████▌    | 253160/450337 [00:42<00:33, 5881.76it/s] 56%|█████▋    | 253852/450337 [00:42<00:31, 6176.81it/s] 57%|█████▋    | 254474/450337 [00:42<00:33, 5895.87it/s] 57%|█████▋    | 255099/450337 [00:42<00:32, 5995.24it/s] 57%|█████▋    | 255703/450337 [00:42<00:32, 5986.94it/s] 57%|█████▋    | 256353/450337 [00:42<00:31, 6130.14it/s] 57%|█████▋    | 256993/450337 [00:43<00:31, 6209.10it/s] 57%|█████▋    | 257682/450337 [00:43<00:30, 6404.67it/s] 57%|█████▋    | 258325/450337 [00:43<00:33, 5717.71it/s] 57%|█████▋    | 258912/450337 [00:43<00:33, 5685.92it/s] 58%|█████▊    | 259491/450337 [00:43<00:34, 5584.63it/s] 58%|█████▊    | 260143/450337 [00:43<00:32, 5843.67it/s] 58%|█████▊    | 260869/450337 [00:43<00:30, 6244.13it/s] 58%|█████▊    | 261500/450337 [00:43<00:30, 6093.05it/s] 58%|█████▊    | 262115/450337 [00:43<00:32, 5838.15it/s] 58%|█████▊    | 262704/450337 [00:44<00:32, 5747.32it/s] 58%|█████▊    | 263283/450337 [00:44<00:32, 5679.53it/s] 59%|█████▊    | 263912/450337 [00:44<00:31, 5847.82it/s] 59%|█████▊    | 264500/450337 [00:44<00:32, 5720.63it/s] 59%|█████▉    | 265131/450337 [00:44<00:31, 5889.82it/s] 59%|█████▉    | 265723/450337 [00:44<00:32, 5741.58it/s] 59%|█████▉    | 266300/450337 [00:44<00:32, 5685.07it/s] 59%|█████▉    | 266870/450337 [00:44<00:32, 5625.43it/s] 59%|█████▉    | 267470/450337 [00:44<00:31, 5732.76it/s] 60%|█████▉    | 268045/450337 [00:45<00:31, 5731.31it/s] 60%|█████▉    | 268683/450337 [00:45<00:30, 5922.33it/s] 60%|█████▉    | 269336/450337 [00:45<00:29, 6097.41it/s] 60%|█████▉    | 269947/450337 [00:45<00:31, 5747.00it/s] 60%|██████    | 270593/450337 [00:45<00:30, 5946.93it/s] 60%|██████    | 271192/450337 [00:45<00:30, 5913.57it/s] 60%|██████    | 271787/450337 [00:45<00:30, 5914.75it/s] 61%|██████    | 272512/450337 [00:45<00:28, 6303.54it/s] 61%|██████    | 273210/450337 [00:45<00:27, 6500.69it/s] 61%|██████    | 273862/450337 [00:45<00:27, 6455.10it/s] 61%|██████    | 274509/450337 [00:46<00:28, 6158.02it/s] 61%|██████    | 275129/450337 [00:46<00:29, 5887.33it/s] 61%|██████    | 275722/450337 [00:46<00:30, 5772.13it/s] 61%|██████▏   | 276303/450337 [00:46<00:30, 5681.26it/s] 61%|██████▏   | 276873/450337 [00:46<00:30, 5639.68it/s] 62%|██████▏   | 277447/450337 [00:46<00:30, 5668.27it/s] 62%|██████▏   | 278093/450337 [00:46<00:29, 5897.37it/s] 62%|██████▏   | 278741/450337 [00:46<00:28, 6067.67it/s] 62%|██████▏   | 279350/450337 [00:46<00:29, 5836.17it/s] 62%|██████▏   | 279947/450337 [00:47<00:29, 5873.67it/s] 62%|██████▏   | 280577/450337 [00:47<00:28, 5991.83it/s] 62%|██████▏   | 281178/450337 [00:47<00:29, 5818.18it/s] 63%|██████▎   | 281818/450337 [00:47<00:28, 5982.05it/s] 63%|██████▎   | 282419/450337 [00:47<00:29, 5710.73it/s] 63%|██████▎   | 283068/450337 [00:47<00:28, 5932.50it/s] 63%|██████▎   | 283698/450337 [00:47<00:27, 6028.21it/s] 63%|██████▎   | 284304/450337 [00:47<00:27, 5953.67it/s] 63%|██████▎   | 284922/450337 [00:47<00:27, 6016.30it/s] 63%|██████▎   | 285526/450337 [00:47<00:27, 5910.41it/s] 64%|██████▎   | 286153/450337 [00:48<00:27, 6013.50it/s] 64%|██████▎   | 286756/450337 [00:48<00:27, 5893.86it/s] 64%|██████▍   | 287347/450337 [00:48<00:28, 5761.65it/s] 64%|██████▍   | 288042/450337 [00:48<00:26, 6104.81it/s] 64%|██████▍   | 288665/450337 [00:48<00:26, 6135.75it/s] 64%|██████▍   | 289281/450337 [00:48<00:28, 5711.79it/s] 64%|██████▍   | 289859/450337 [00:48<00:29, 5440.44it/s] 64%|██████▍   | 290410/450337 [00:48<00:29, 5429.08it/s] 65%|██████▍   | 291023/450337 [00:48<00:28, 5616.28it/s] 65%|██████▍   | 291662/450337 [00:49<00:27, 5833.03it/s] 65%|██████▍   | 292250/450337 [00:49<00:27, 5686.59it/s] 65%|██████▌   | 292845/450337 [00:49<00:27, 5754.45it/s] 65%|██████▌   | 293427/450337 [00:49<00:27, 5773.33it/s] 65%|██████▌   | 294007/450337 [00:49<00:27, 5742.90it/s] 65%|██████▌   | 294677/450337 [00:49<00:25, 6020.18it/s] 66%|██████▌   | 295362/450337 [00:49<00:24, 6262.58it/s] 66%|██████▌   | 295990/450337 [00:49<00:25, 5996.09it/s] 66%|██████▌   | 296595/450337 [00:49<00:25, 6008.25it/s] 66%|██████▌   | 297212/450337 [00:49<00:25, 6047.78it/s] 66%|██████▌   | 297872/450337 [00:50<00:24, 6203.14it/s] 66%|██████▋   | 298494/450337 [00:50<00:25, 6016.53it/s] 66%|██████▋   | 299098/450337 [00:50<00:25, 5922.80it/s] 67%|██████▋   | 299815/450337 [00:50<00:23, 6282.77it/s] 67%|██████▋   | 300446/450337 [00:50<00:24, 6112.61it/s] 67%|██████▋   | 301060/450337 [00:50<00:25, 5946.19it/s] 67%|██████▋   | 301657/450337 [00:50<00:25, 5849.98it/s] 67%|██████▋   | 302244/450337 [00:50<00:25, 5738.82it/s] 67%|██████▋   | 302896/450337 [00:50<00:24, 5957.51it/s] 67%|██████▋   | 303532/450337 [00:50<00:24, 6071.08it/s] 68%|██████▊   | 304154/450337 [00:51<00:23, 6110.51it/s] 68%|██████▊   | 304767/450337 [00:51<00:24, 5988.81it/s] 68%|██████▊   | 305368/450337 [00:51<00:24, 5969.95it/s] 68%|██████▊   | 305966/450337 [00:51<00:24, 5861.52it/s] 68%|██████▊   | 306553/450337 [00:51<00:25, 5718.56it/s] 68%|██████▊   | 307126/450337 [00:51<00:25, 5698.41it/s] 68%|██████▊   | 307791/450337 [00:51<00:23, 5975.10it/s] 68%|██████▊   | 308390/450337 [00:51<00:26, 5355.23it/s] 69%|██████▊   | 309161/450337 [00:51<00:23, 5998.01it/s] 69%|██████▉   | 309908/450337 [00:52<00:21, 6410.66it/s] 69%|██████▉   | 310563/450337 [00:52<00:22, 6266.55it/s] 69%|██████▉   | 311200/450337 [00:52<00:22, 6067.30it/s] 69%|██████▉   | 311827/450337 [00:52<00:22, 6123.88it/s] 69%|██████▉   | 312445/450337 [00:52<00:22, 6062.32it/s] 70%|██████▉   | 313097/450337 [00:52<00:22, 6193.32it/s] 70%|██████▉   | 313800/450337 [00:52<00:21, 6429.34it/s] 70%|██████▉   | 314446/450337 [00:52<00:21, 6432.31it/s] 70%|██████▉   | 315092/450337 [00:52<00:23, 5855.44it/s] 70%|███████   | 315733/450337 [00:53<00:22, 6004.22it/s] 70%|███████   | 316343/450337 [00:53<00:23, 5717.29it/s] 70%|███████   | 316968/450337 [00:53<00:22, 5864.82it/s] 71%|███████   | 317562/450337 [00:53<00:23, 5683.52it/s] 71%|███████   | 318136/450337 [00:53<00:23, 5603.27it/s] 71%|███████   | 318707/450337 [00:53<00:23, 5631.44it/s] 71%|███████   | 319373/450337 [00:53<00:22, 5925.58it/s] 71%|███████   | 319969/450337 [00:53<00:22, 5799.09it/s] 71%|███████   | 320552/450337 [00:53<00:23, 5615.69it/s] 71%|███████▏  | 321327/450337 [00:53<00:20, 6223.26it/s] 71%|███████▏  | 321955/450337 [00:54<00:21, 5889.60it/s] 72%|███████▏  | 322551/450337 [00:54<00:22, 5737.55it/s] 72%|███████▏  | 323170/450337 [00:54<00:21, 5863.82it/s] 72%|███████▏  | 323761/450337 [00:54<00:21, 5777.39it/s] 72%|███████▏  | 324366/450337 [00:54<00:21, 5851.51it/s] 72%|███████▏  | 324964/450337 [00:54<00:21, 5884.70it/s] 72%|███████▏  | 325555/450337 [00:54<00:23, 5423.37it/s] 72%|███████▏  | 326132/450337 [00:54<00:22, 5516.84it/s] 73%|███████▎  | 326845/450337 [00:54<00:20, 5974.00it/s] 73%|███████▎  | 327469/450337 [00:55<00:20, 6050.34it/s] 73%|███████▎  | 328275/450337 [00:55<00:18, 6628.12it/s] 73%|███████▎  | 328943/450337 [00:55<00:18, 6504.15it/s] 73%|███████▎  | 329598/450337 [00:55<00:19, 6081.18it/s] 73%|███████▎  | 330214/450337 [00:55<00:20, 5741.31it/s] 73%|███████▎  | 330796/450337 [00:55<00:20, 5730.53it/s] 74%|███████▎  | 331375/450337 [00:55<00:20, 5725.67it/s] 74%|███████▎  | 331957/450337 [00:55<00:20, 5749.34it/s] 74%|███████▍  | 332602/450337 [00:55<00:19, 5951.59it/s] 74%|███████▍  | 333200/450337 [00:56<00:20, 5844.53it/s] 74%|███████▍  | 333838/450337 [00:56<00:19, 5998.56it/s] 74%|███████▍  | 334440/450337 [00:56<00:19, 5919.64it/s] 74%|███████▍  | 335055/450337 [00:56<00:19, 5985.31it/s] 75%|███████▍  | 335655/450337 [00:56<00:19, 5987.45it/s] 75%|███████▍  | 336282/450337 [00:56<00:18, 6069.12it/s] 75%|███████▍  | 336890/450337 [00:56<00:18, 6008.81it/s] 75%|███████▍  | 337565/450337 [00:56<00:18, 6225.19it/s] 75%|███████▌  | 338189/450337 [00:56<00:18, 6206.00it/s] 75%|███████▌  | 338856/450337 [00:56<00:17, 6343.75it/s] 75%|███████▌  | 339491/450337 [00:57<00:17, 6205.49it/s] 76%|███████▌  | 340113/450337 [00:57<00:19, 5575.93it/s] 76%|███████▌  | 340749/450337 [00:57<00:18, 5787.50it/s] 76%|███████▌  | 341356/450337 [00:57<00:18, 5852.02it/s] 76%|███████▌  | 341949/450337 [00:57<00:19, 5676.40it/s] 76%|███████▌  | 342570/450337 [00:57<00:18, 5820.95it/s] 76%|███████▌  | 343158/450337 [00:57<00:18, 5755.83it/s] 76%|███████▋  | 343868/450337 [00:57<00:17, 6135.09it/s] 76%|███████▋  | 344486/450337 [00:57<00:18, 5711.61it/s] 77%|███████▋  | 345181/450337 [00:58<00:17, 6052.93it/s] 77%|███████▋  | 345795/450337 [00:58<00:18, 5545.60it/s] 77%|███████▋  | 346473/450337 [00:58<00:17, 5879.60it/s] 77%|███████▋  | 347074/450337 [00:58<00:17, 5843.42it/s] 77%|███████▋  | 347667/450337 [00:58<00:17, 5848.39it/s] 77%|███████▋  | 348258/450337 [00:58<00:17, 5773.30it/s] 77%|███████▋  | 348865/450337 [00:58<00:17, 5856.43it/s] 78%|███████▊  | 349497/450337 [00:58<00:16, 5991.17it/s] 78%|███████▊  | 350311/450337 [00:58<00:15, 6621.50it/s] 78%|███████▊  | 350977/450337 [00:58<00:15, 6248.02it/s] 78%|███████▊  | 351609/450337 [00:59<00:16, 6163.04it/s] 78%|███████▊  | 352230/450337 [00:59<00:17, 5669.12it/s] 78%|███████▊  | 352807/450337 [00:59<00:17, 5530.74it/s] 78%|███████▊  | 353456/450337 [00:59<00:16, 5791.57it/s] 79%|███████▊  | 354042/450337 [00:59<00:17, 5531.60it/s] 79%|███████▉  | 354785/450337 [00:59<00:15, 6056.52it/s] 79%|███████▉  | 355400/450337 [00:59<00:16, 5741.47it/s] 79%|███████▉  | 356063/450337 [00:59<00:15, 5987.00it/s] 79%|███████▉  | 356670/450337 [00:59<00:15, 5872.01it/s] 79%|███████▉  | 357263/450337 [01:00<00:16, 5508.08it/s] 79%|███████▉  | 357888/450337 [01:00<00:16, 5711.24it/s] 80%|███████▉  | 358466/450337 [01:00<00:16, 5707.40it/s] 80%|███████▉  | 359066/450337 [01:00<00:15, 5789.58it/s] 80%|███████▉  | 359649/450337 [01:00<00:15, 5794.50it/s] 80%|███████▉  | 360267/450337 [01:00<00:15, 5900.71it/s] 80%|████████  | 360900/450337 [01:00<00:14, 6021.98it/s] 80%|████████  | 361518/450337 [01:00<00:14, 6066.94it/s] 80%|████████  | 362265/450337 [01:00<00:13, 6476.91it/s] 81%|████████  | 362914/450337 [01:00<00:13, 6386.24it/s] 81%|████████  | 363554/450337 [01:01<00:13, 6348.67it/s] 81%|████████  | 364190/450337 [01:01<00:13, 6301.87it/s] 81%|████████  | 364859/450337 [01:01<00:13, 6414.23it/s] 81%|████████  | 365514/450337 [01:01<00:13, 6447.05it/s] 81%|████████▏ | 366187/450337 [01:01<00:12, 6525.60it/s] 81%|████████▏ | 366840/450337 [01:01<00:13, 6061.47it/s] 82%|████████▏ | 367478/450337 [01:01<00:13, 6146.38it/s] 82%|████████▏ | 368098/450337 [01:01<00:14, 5612.69it/s] 82%|████████▏ | 368780/450337 [01:01<00:13, 5935.62it/s] 82%|████████▏ | 369386/450337 [01:02<00:13, 5969.02it/s] 82%|████████▏ | 370016/450337 [01:02<00:13, 6057.65it/s] 82%|████████▏ | 370628/450337 [01:02<00:13, 5875.76it/s] 82%|████████▏ | 371221/450337 [01:02<00:13, 5858.92it/s] 83%|████████▎ | 371811/450337 [01:02<00:13, 5780.03it/s] 83%|████████▎ | 372392/450337 [01:02<00:13, 5715.89it/s] 83%|████████▎ | 372982/450337 [01:02<00:13, 5765.10it/s] 83%|████████▎ | 373612/450337 [01:02<00:12, 5913.62it/s] 83%|████████▎ | 374257/450337 [01:02<00:12, 6064.51it/s] 83%|████████▎ | 374922/450337 [01:02<00:12, 6228.25it/s] 83%|████████▎ | 375546/450337 [01:03<00:12, 5791.41it/s] 84%|████████▎ | 376134/450337 [01:03<00:12, 5813.93it/s] 84%|████████▎ | 376721/450337 [01:03<00:13, 5552.17it/s] 84%|████████▍ | 377319/450337 [01:03<00:12, 5668.97it/s] 84%|████████▍ | 377930/450337 [01:03<00:12, 5793.77it/s] 84%|████████▍ | 378513/450337 [01:03<00:12, 5741.63it/s] 84%|████████▍ | 379151/450337 [01:03<00:12, 5925.39it/s] 84%|████████▍ | 379796/450337 [01:03<00:11, 6075.51it/s] 84%|████████▍ | 380406/450337 [01:03<00:11, 6063.49it/s] 85%|████████▍ | 381131/450337 [01:04<00:10, 6411.79it/s] 85%|████████▍ | 381774/450337 [01:04<00:11, 6155.95it/s] 85%|████████▍ | 382393/450337 [01:04<00:11, 5776.51it/s] 85%|████████▌ | 382977/450337 [01:04<00:11, 5699.21it/s] 85%|████████▌ | 383559/450337 [01:04<00:11, 5731.57it/s] 85%|████████▌ | 384227/450337 [01:04<00:11, 6001.59it/s] 85%|████████▌ | 384859/450337 [01:04<00:10, 6087.13it/s] 86%|████████▌ | 385471/450337 [01:04<00:10, 5938.53it/s] 86%|████████▌ | 386068/450337 [01:04<00:10, 5898.36it/s] 86%|████████▌ | 386660/450337 [01:04<00:10, 5872.36it/s] 86%|████████▌ | 387427/450337 [01:05<00:09, 6397.94it/s] 86%|████████▌ | 388069/450337 [01:05<00:09, 6297.65it/s] 86%|████████▋ | 388701/450337 [01:05<00:10, 6066.31it/s] 86%|████████▋ | 389348/450337 [01:05<00:09, 6180.29it/s] 87%|████████▋ | 389969/450337 [01:05<00:09, 6160.86it/s] 87%|████████▋ | 390587/450337 [01:05<00:09, 6066.00it/s] 87%|████████▋ | 391195/450337 [01:05<00:09, 6047.69it/s] 87%|████████▋ | 391801/450337 [01:05<00:10, 5776.72it/s] 87%|████████▋ | 392382/450337 [01:05<00:10, 5734.18it/s] 87%|████████▋ | 392958/450337 [01:06<00:10, 5706.02it/s] 87%|████████▋ | 393530/450337 [01:06<00:10, 5540.77it/s] 88%|████████▊ | 394223/450337 [01:06<00:09, 5935.14it/s] 88%|████████▊ | 394820/450337 [01:06<00:09, 5644.77it/s] 88%|████████▊ | 395431/450337 [01:06<00:09, 5771.54it/s] 88%|████████▊ | 396025/450337 [01:06<00:09, 5816.39it/s] 88%|████████▊ | 396610/450337 [01:06<00:09, 5554.65it/s] 88%|████████▊ | 397170/450337 [01:06<00:09, 5428.02it/s] 88%|████████▊ | 397775/450337 [01:06<00:09, 5590.10it/s] 88%|████████▊ | 398446/450337 [01:07<00:08, 5910.91it/s] 89%|████████▊ | 399112/450337 [01:07<00:08, 6128.09it/s] 89%|████████▉ | 399825/450337 [01:07<00:07, 6416.02it/s] 89%|████████▉ | 400470/450337 [01:07<00:08, 6224.26it/s] 89%|████████▉ | 401096/450337 [01:07<00:08, 6107.06it/s] 89%|████████▉ | 401709/450337 [01:07<00:08, 5859.68it/s] 89%|████████▉ | 402298/450337 [01:07<00:08, 5472.70it/s] 89%|████████▉ | 402852/450337 [01:07<00:08, 5487.96it/s] 90%|████████▉ | 403452/450337 [01:07<00:08, 5625.81it/s] 90%|████████▉ | 404019/450337 [01:07<00:08, 5604.77it/s] 90%|████████▉ | 404678/450337 [01:08<00:07, 5889.42it/s] 90%|████████▉ | 405270/450337 [01:08<00:07, 5674.22it/s] 90%|█████████ | 405841/450337 [01:08<00:07, 5655.95it/s] 90%|█████████ | 406409/450337 [01:08<00:07, 5547.72it/s] 90%|█████████ | 407011/450337 [01:08<00:07, 5680.06it/s] 91%|█████████ | 407581/450337 [01:08<00:07, 5623.07it/s] 91%|█████████ | 408159/450337 [01:08<00:07, 5668.75it/s] 91%|█████████ | 408803/450337 [01:08<00:07, 5895.25it/s] 91%|█████████ | 409414/450337 [01:08<00:06, 5958.14it/s] 91%|█████████ | 410011/450337 [01:09<00:06, 5768.66it/s] 91%|█████████ | 410647/450337 [01:09<00:06, 5940.58it/s] 91%|█████████▏| 411243/450337 [01:09<00:06, 5865.92it/s] 91%|█████████▏| 411831/450337 [01:09<00:06, 5827.44it/s] 92%|█████████▏| 412415/450337 [01:09<00:07, 5301.51it/s] 92%|█████████▏| 412955/450337 [01:09<00:07, 5257.35it/s] 92%|█████████▏| 413548/450337 [01:09<00:06, 5440.96it/s] 92%|█████████▏| 414098/450337 [01:09<00:06, 5197.98it/s] 92%|█████████▏| 414625/450337 [01:09<00:06, 5215.05it/s] 92%|█████████▏| 415186/450337 [01:09<00:06, 5326.17it/s] 92%|█████████▏| 415800/450337 [01:10<00:06, 5550.18it/s] 92%|█████████▏| 416358/450337 [01:10<00:06, 5305.20it/s] 93%|█████████▎| 416976/450337 [01:10<00:06, 5551.11it/s] 93%|█████████▎| 417566/450337 [01:10<00:05, 5647.32it/s] 93%|█████████▎| 418225/450337 [01:10<00:05, 5912.00it/s] 93%|█████████▎| 418820/450337 [01:10<00:05, 5834.45it/s] 93%|█████████▎| 419426/450337 [01:10<00:05, 5892.90it/s] 93%|█████████▎| 420030/450337 [01:10<00:05, 5935.85it/s] 93%|█████████▎| 420625/450337 [01:10<00:05, 5923.79it/s] 94%|█████████▎| 421219/450337 [01:11<00:05, 5542.71it/s] 94%|█████████▎| 421786/450337 [01:11<00:05, 5577.34it/s] 94%|█████████▍| 422387/450337 [01:11<00:04, 5702.02it/s] 94%|█████████▍| 423065/450337 [01:11<00:04, 6012.17it/s] 94%|█████████▍| 423670/450337 [01:11<00:04, 5867.98it/s] 94%|█████████▍| 424269/450337 [01:11<00:04, 5901.55it/s] 94%|█████████▍| 424862/450337 [01:11<00:04, 5750.16it/s] 94%|█████████▍| 425440/450337 [01:11<00:04, 5683.24it/s] 95%|█████████▍| 426086/450337 [01:11<00:04, 5905.48it/s] 95%|█████████▍| 426813/450337 [01:11<00:03, 6300.42it/s] 95%|█████████▍| 427446/450337 [01:12<00:03, 6172.75it/s] 95%|█████████▌| 428066/450337 [01:12<00:03, 5968.40it/s] 95%|█████████▌| 428814/450337 [01:12<00:03, 6396.52it/s] 95%|█████████▌| 429458/450337 [01:12<00:03, 6260.48it/s] 96%|█████████▌| 430087/450337 [01:12<00:03, 6219.31it/s] 96%|█████████▌| 430711/450337 [01:12<00:03, 6015.49it/s] 96%|█████████▌| 431315/450337 [01:12<00:03, 5602.20it/s] 96%|█████████▌| 431882/450337 [01:12<00:03, 5511.58it/s] 96%|█████████▌| 432699/450337 [01:12<00:02, 6249.90it/s] 96%|█████████▌| 433333/450337 [01:13<00:02, 6185.59it/s] 96%|█████████▋| 433958/450337 [01:13<00:02, 6164.03it/s] 97%|█████████▋| 434579/450337 [01:13<00:02, 5722.42it/s] 97%|█████████▋| 435160/450337 [01:13<00:02, 5710.35it/s] 97%|█████████▋| 435839/450337 [01:13<00:02, 6013.64it/s] 97%|█████████▋| 436447/450337 [01:13<00:02, 5740.44it/s] 97%|█████████▋| 437037/450337 [01:13<00:02, 5782.73it/s] 97%|█████████▋| 437660/450337 [01:13<00:02, 5902.61it/s] 97%|█████████▋| 438254/450337 [01:13<00:02, 5859.48it/s] 97%|█████████▋| 438843/450337 [01:13<00:01, 5781.60it/s] 98%|█████████▊| 439423/450337 [01:14<00:01, 5765.14it/s] 98%|█████████▊| 440001/450337 [01:14<00:01, 5530.52it/s] 98%|█████████▊| 440580/450337 [01:14<00:01, 5601.08it/s] 98%|█████████▊| 441208/450337 [01:14<00:01, 5796.62it/s] 98%|█████████▊| 441843/450337 [01:14<00:01, 5955.48it/s] 98%|█████████▊| 442441/450337 [01:14<00:01, 5905.09it/s] 98%|█████████▊| 443033/450337 [01:14<00:01, 5876.62it/s] 99%|█████████▊| 443691/450337 [01:14<00:01, 6083.59it/s] 99%|█████████▊| 444301/450337 [01:14<00:01, 6015.16it/s] 99%|█████████▉| 444904/450337 [01:15<00:00, 5525.21it/s] 99%|█████████▉| 445502/450337 [01:15<00:00, 5648.91it/s] 99%|█████████▉| 446074/450337 [01:15<00:00, 5474.13it/s] 99%|█████████▉| 446627/450337 [01:15<00:00, 5450.12it/s] 99%|█████████▉| 447268/450337 [01:15<00:00, 5720.36it/s] 99%|█████████▉| 447844/450337 [01:15<00:00, 5687.28it/s]100%|█████████▉| 448441/450337 [01:15<00:00, 5765.06it/s]100%|█████████▉| 449032/450337 [01:15<00:00, 5805.00it/s]100%|█████████▉| 449614/450337 [01:15<00:00, 5622.08it/s]100%|█████████▉| 450237/450337 [01:15<00:00, 5796.81it/s]100%|██████████| 450337/450337 [01:15<00:00, 5927.60it/s]

gathering stats for n=1
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 1900/450337 [00:00<00:23, 18999.02it/s]  1%|          | 3961/450337 [00:00<00:22, 19942.31it/s]  1%|▏         | 6149/450337 [00:00<00:21, 20826.45it/s]  2%|▏         | 8232/450337 [00:00<00:22, 19904.89it/s]  2%|▏         | 10286/450337 [00:00<00:21, 20111.66it/s]  3%|▎         | 12302/450337 [00:00<00:22, 19782.00it/s]  3%|▎         | 14284/450337 [00:00<00:22, 19775.84it/s]  4%|▎         | 16264/450337 [00:00<00:22, 19404.52it/s]  4%|▍         | 18207/450337 [00:00<00:22, 19287.90it/s]  5%|▍         | 20287/450337 [00:01<00:21, 19739.18it/s]  5%|▍         | 22304/450337 [00:01<00:21, 19862.06it/s]  5%|▌         | 24578/450337 [00:01<00:20, 20726.27it/s]  6%|▌         | 26653/450337 [00:01<00:20, 20574.46it/s]  6%|▋         | 28713/450337 [00:01<00:20, 20498.72it/s]  7%|▋         | 30764/450337 [00:01<00:21, 19918.70it/s]  7%|▋         | 32760/450337 [00:01<00:21, 19812.56it/s]  8%|▊         | 34822/450337 [00:01<00:20, 20049.46it/s]  8%|▊         | 36830/450337 [00:01<00:20, 19815.48it/s]  9%|▊         | 38814/450337 [00:01<00:21, 19517.33it/s]  9%|▉         | 40891/450337 [00:02<00:20, 19878.79it/s] 10%|▉         | 42882/450337 [00:02<00:20, 19424.19it/s] 10%|▉         | 44939/450337 [00:02<00:20, 19751.65it/s] 10%|█         | 47043/450337 [00:02<00:20, 20125.36it/s] 11%|█         | 49568/450337 [00:02<00:18, 21636.84it/s] 11%|█▏        | 51736/450337 [00:02<00:18, 20983.99it/s] 12%|█▏        | 53947/450337 [00:02<00:18, 21308.34it/s] 12%|█▏        | 56084/450337 [00:02<00:18, 20859.21it/s] 13%|█▎        | 58176/450337 [00:02<00:19, 20201.60it/s] 13%|█▎        | 60206/450337 [00:02<00:19, 20224.51it/s] 14%|█▍        | 62373/450337 [00:03<00:18, 20644.07it/s] 14%|█▍        | 64443/450337 [00:03<00:18, 20318.90it/s] 15%|█▍        | 66529/450337 [00:03<00:18, 20475.92it/s] 15%|█▌        | 68580/450337 [00:03<00:18, 20306.01it/s] 16%|█▌        | 70683/450337 [00:03<00:18, 20515.50it/s] 16%|█▌        | 72773/450337 [00:03<00:18, 20625.02it/s] 17%|█▋        | 74986/450337 [00:03<00:17, 21065.11it/s] 17%|█▋        | 77094/450337 [00:03<00:18, 20396.99it/s] 18%|█▊        | 79139/450337 [00:03<00:18, 19630.76it/s] 18%|█▊        | 81131/450337 [00:04<00:18, 19709.41it/s] 18%|█▊        | 83273/450337 [00:04<00:18, 20197.71it/s] 19%|█▉        | 85299/450337 [00:04<00:18, 20066.00it/s] 19%|█▉        | 87466/450337 [00:04<00:17, 20534.89it/s] 20%|█▉        | 89524/450337 [00:04<00:18, 20038.09it/s] 20%|██        | 91533/450337 [00:04<00:17, 20025.87it/s] 21%|██        | 93603/450337 [00:04<00:17, 20222.11it/s] 21%|██        | 95628/450337 [00:04<00:17, 20059.71it/s] 22%|██▏       | 97830/450337 [00:04<00:17, 20637.45it/s] 22%|██▏       | 99897/450337 [00:04<00:17, 20359.20it/s] 23%|██▎       | 101936/450337 [00:05<00:17, 20331.31it/s] 23%|██▎       | 103971/450337 [00:05<00:17, 20032.72it/s] 24%|██▎       | 106020/450337 [00:05<00:17, 20160.37it/s] 24%|██▍       | 108038/450337 [00:05<00:17, 20074.97it/s] 24%|██▍       | 110047/450337 [00:05<00:17, 19714.39it/s] 25%|██▍       | 112110/450337 [00:05<00:16, 19980.55it/s] 25%|██▌       | 114113/450337 [00:05<00:16, 19991.66it/s] 26%|██▌       | 116114/450337 [00:05<00:16, 19666.59it/s] 26%|██▌       | 118083/450337 [00:05<00:17, 19495.60it/s] 27%|██▋       | 120221/450337 [00:05<00:16, 20047.98it/s] 27%|██▋       | 122228/450337 [00:06<00:16, 19912.62it/s] 28%|██▊       | 124221/450337 [00:06<00:16, 19836.75it/s] 28%|██▊       | 126307/450337 [00:06<00:16, 20128.88it/s] 28%|██▊       | 128325/450337 [00:06<00:15, 20142.74it/s] 29%|██▉       | 130341/450337 [00:06<00:15, 20011.35it/s] 29%|██▉       | 132343/450337 [00:06<00:16, 19619.12it/s] 30%|██▉       | 134307/450337 [00:06<00:16, 19592.02it/s] 30%|███       | 136382/450337 [00:06<00:15, 19925.40it/s] 31%|███       | 138482/450337 [00:06<00:15, 20242.25it/s] 31%|███       | 140508/450337 [00:06<00:15, 19994.03it/s] 32%|███▏      | 142509/450337 [00:07<00:15, 19959.08it/s] 32%|███▏      | 144623/450337 [00:07<00:15, 20307.09it/s] 33%|███▎      | 146713/450337 [00:07<00:14, 20483.23it/s] 33%|███▎      | 148763/450337 [00:07<00:14, 20110.78it/s] 33%|███▎      | 150777/450337 [00:07<00:15, 19935.18it/s] 34%|███▍      | 152772/450337 [00:07<00:15, 19787.31it/s] 34%|███▍      | 154752/450337 [00:07<00:14, 19754.03it/s] 35%|███▍      | 156871/450337 [00:07<00:14, 20177.41it/s] 35%|███▌      | 158973/450337 [00:07<00:14, 20417.87it/s] 36%|███▌      | 161090/450337 [00:07<00:14, 20641.60it/s] 36%|███▌      | 163155/450337 [00:08<00:13, 20580.86it/s] 37%|███▋      | 165214/450337 [00:08<00:13, 20405.47it/s] 37%|███▋      | 167355/450337 [00:08<00:13, 20699.39it/s] 38%|███▊      | 169426/450337 [00:08<00:13, 20389.33it/s] 38%|███▊      | 171467/450337 [00:08<00:14, 19881.99it/s] 39%|███▊      | 173459/450337 [00:08<00:13, 19853.02it/s] 39%|███▉      | 175447/450337 [00:08<00:13, 19704.97it/s] 39%|███▉      | 177419/450337 [00:08<00:14, 19288.95it/s] 40%|███▉      | 179363/450337 [00:08<00:14, 19332.52it/s] 40%|████      | 181317/450337 [00:09<00:13, 19383.80it/s] 41%|████      | 183257/450337 [00:09<00:13, 19258.21it/s] 41%|████      | 185184/450337 [00:09<00:13, 19053.78it/s] 42%|████▏     | 187104/450337 [00:09<00:13, 19096.00it/s] 42%|████▏     | 189181/450337 [00:09<00:13, 19586.12it/s] 42%|████▏     | 191209/450337 [00:09<00:13, 19791.52it/s] 43%|████▎     | 193190/450337 [00:09<00:13, 19752.42it/s] 43%|████▎     | 195166/450337 [00:09<00:12, 19664.77it/s] 44%|████▍     | 197317/450337 [00:09<00:12, 20207.14it/s] 44%|████▍     | 199339/450337 [00:09<00:12, 19880.25it/s] 45%|████▍     | 201329/450337 [00:10<00:12, 19448.98it/s] 45%|████▌     | 203413/450337 [00:10<00:12, 19852.08it/s] 46%|████▌     | 205402/450337 [00:10<00:12, 19848.68it/s] 46%|████▌     | 207389/450337 [00:10<00:12, 19573.18it/s] 46%|████▋     | 209349/450337 [00:10<00:12, 19349.85it/s] 47%|████▋     | 211429/450337 [00:10<00:12, 19774.48it/s] 47%|████▋     | 213497/450337 [00:10<00:11, 20041.48it/s] 48%|████▊     | 215503/450337 [00:10<00:12, 19501.96it/s] 48%|████▊     | 217472/450337 [00:10<00:11, 19554.13it/s] 49%|████▉     | 219555/450337 [00:10<00:11, 19927.02it/s] 49%|████▉     | 221564/450337 [00:11<00:11, 19968.75it/s] 50%|████▉     | 223563/450337 [00:11<00:11, 19224.63it/s] 50%|█████     | 225830/450337 [00:11<00:11, 20221.91it/s] 51%|█████     | 227861/450337 [00:11<00:11, 20075.58it/s] 51%|█████     | 229875/450337 [00:11<00:11, 19467.96it/s] 51%|█████▏    | 231847/450337 [00:11<00:11, 19534.83it/s] 52%|█████▏    | 234016/450337 [00:11<00:10, 20156.88it/s] 52%|█████▏    | 236127/450337 [00:11<00:10, 20431.08it/s] 53%|█████▎    | 238175/450337 [00:11<00:10, 20004.07it/s] 53%|█████▎    | 240224/450337 [00:12<00:10, 20143.52it/s] 54%|█████▍    | 242483/450337 [00:12<00:09, 20860.30it/s] 54%|█████▍    | 244573/450337 [00:12<00:09, 20788.80it/s] 55%|█████▍    | 246766/450337 [00:12<00:09, 21127.04it/s] 55%|█████▌    | 248881/450337 [00:12<00:09, 21059.20it/s] 56%|█████▌    | 250999/450337 [00:12<00:09, 21085.70it/s] 56%|█████▌    | 253109/450337 [00:12<00:09, 20637.53it/s] 57%|█████▋    | 255176/450337 [00:12<00:09, 20243.62it/s] 57%|█████▋    | 257204/450337 [00:12<00:09, 20224.78it/s] 58%|█████▊    | 259229/450337 [00:12<00:09, 19317.92it/s] 58%|█████▊    | 261335/450337 [00:13<00:09, 19813.70it/s] 58%|█████▊    | 263325/450337 [00:13<00:09, 19638.33it/s] 59%|█████▉    | 265295/450337 [00:13<00:09, 19634.54it/s] 59%|█████▉    | 267263/450337 [00:13<00:09, 19606.55it/s] 60%|█████▉    | 269306/450337 [00:13<00:09, 19847.91it/s] 60%|██████    | 271294/450337 [00:13<00:09, 19738.38it/s] 61%|██████    | 273550/450337 [00:13<00:08, 20573.87it/s] 61%|██████    | 275610/450337 [00:13<00:08, 19958.24it/s] 62%|██████▏   | 277611/450337 [00:13<00:08, 19688.45it/s] 62%|██████▏   | 279693/450337 [00:13<00:08, 20015.45it/s] 63%|██████▎   | 281699/450337 [00:14<00:08, 19979.00it/s] 63%|██████▎   | 283700/450337 [00:14<00:08, 19960.98it/s] 63%|██████▎   | 285733/450337 [00:14<00:08, 20067.42it/s] 64%|██████▍   | 287742/450337 [00:14<00:08, 19850.35it/s] 64%|██████▍   | 289751/450337 [00:14<00:08, 19918.04it/s] 65%|██████▍   | 291744/450337 [00:14<00:07, 19915.97it/s] 65%|██████▌   | 293743/450337 [00:14<00:07, 19929.94it/s] 66%|██████▌   | 295839/450337 [00:14<00:07, 20235.26it/s] 66%|██████▌   | 297903/450337 [00:14<00:07, 20355.33it/s] 67%|██████▋   | 299968/450337 [00:14<00:07, 20436.19it/s] 67%|██████▋   | 302012/450337 [00:15<00:07, 19838.64it/s] 68%|██████▊   | 304179/450337 [00:15<00:07, 20373.54it/s] 68%|██████▊   | 306221/450337 [00:15<00:07, 20201.05it/s] 68%|██████▊   | 308244/450337 [00:15<00:07, 19860.83it/s] 69%|██████▉   | 310528/450337 [00:15<00:06, 20731.59it/s] 69%|██████▉   | 312606/450337 [00:15<00:06, 20563.43it/s] 70%|██████▉   | 314666/450337 [00:15<00:06, 20449.37it/s] 70%|███████   | 316713/450337 [00:15<00:06, 19623.74it/s] 71%|███████   | 318683/450337 [00:15<00:06, 19086.68it/s] 71%|███████   | 320599/450337 [00:16<00:06, 18992.13it/s] 72%|███████▏  | 322661/450337 [00:16<00:06, 19459.19it/s] 72%|███████▏  | 324705/450337 [00:16<00:06, 19740.38it/s] 73%|███████▎  | 326684/450337 [00:16<00:06, 19540.20it/s] 73%|███████▎  | 329013/450337 [00:16<00:05, 20640.97it/s] 74%|███████▎  | 331082/450337 [00:16<00:05, 20075.66it/s] 74%|███████▍  | 333096/450337 [00:16<00:05, 19861.62it/s] 74%|███████▍  | 335166/450337 [00:16<00:05, 20104.65it/s] 75%|███████▍  | 337248/450337 [00:16<00:05, 20309.90it/s] 75%|███████▌  | 339390/450337 [00:16<00:05, 20637.51it/s] 76%|███████▌  | 341457/450337 [00:17<00:05, 19995.52it/s] 76%|███████▋  | 343514/450337 [00:17<00:05, 20159.43it/s] 77%|███████▋  | 345535/450337 [00:17<00:05, 20140.19it/s] 77%|███████▋  | 347553/450337 [00:17<00:05, 20059.79it/s] 78%|███████▊  | 349562/450337 [00:17<00:05, 20004.16it/s] 78%|███████▊  | 351713/450337 [00:17<00:04, 20448.74it/s] 79%|███████▊  | 353760/450337 [00:17<00:04, 19388.75it/s] 79%|███████▉  | 355739/450337 [00:17<00:04, 19502.92it/s] 79%|███████▉  | 357699/450337 [00:17<00:04, 19459.07it/s] 80%|███████▉  | 359690/450337 [00:17<00:04, 19589.88it/s] 80%|████████  | 361936/450337 [00:18<00:04, 20428.68it/s] 81%|████████  | 364025/450337 [00:18<00:04, 20560.47it/s] 81%|████████▏ | 366209/450337 [00:18<00:04, 20933.94it/s] 82%|████████▏ | 368306/450337 [00:18<00:04, 20096.03it/s] 82%|████████▏ | 370369/450337 [00:18<00:03, 20248.88it/s] 83%|████████▎ | 372401/450337 [00:18<00:03, 19877.67it/s] 83%|████████▎ | 374408/450337 [00:18<00:03, 19921.31it/s] 84%|████████▎ | 376405/450337 [00:18<00:03, 19284.53it/s] 84%|████████▍ | 378340/450337 [00:18<00:03, 19081.83it/s] 84%|████████▍ | 380307/450337 [00:19<00:03, 19244.16it/s] 85%|████████▍ | 382258/450337 [00:19<00:03, 19319.47it/s] 85%|████████▌ | 384218/450337 [00:19<00:03, 19399.65it/s] 86%|████████▌ | 386183/450337 [00:19<00:03, 19463.12it/s] 86%|████████▌ | 388331/450337 [00:19<00:03, 20062.35it/s] 87%|████████▋ | 390339/450337 [00:19<00:02, 20024.24it/s] 87%|████████▋ | 392343/450337 [00:19<00:02, 19498.25it/s] 88%|████████▊ | 394297/450337 [00:19<00:02, 19462.29it/s] 88%|████████▊ | 396258/450337 [00:19<00:02, 19493.19it/s] 88%|████████▊ | 398210/450337 [00:19<00:02, 19495.06it/s] 89%|████████▉ | 400377/450337 [00:20<00:02, 20136.15it/s] 89%|████████▉ | 402393/450337 [00:20<00:02, 19503.82it/s] 90%|████████▉ | 404349/450337 [00:20<00:02, 19488.38it/s] 90%|█████████ | 406315/450337 [00:20<00:02, 19535.24it/s] 91%|█████████ | 408272/450337 [00:20<00:02, 19346.31it/s] 91%|█████████ | 410245/450337 [00:20<00:02, 19455.39it/s] 92%|█████████▏| 412193/450337 [00:20<00:01, 19447.38it/s] 92%|█████████▏| 414139/450337 [00:20<00:01, 18757.82it/s] 92%|█████████▏| 416021/450337 [00:20<00:01, 18725.73it/s] 93%|█████████▎| 418033/450337 [00:20<00:01, 19132.54it/s] 93%|█████████▎| 419962/450337 [00:21<00:01, 19177.80it/s] 94%|█████████▎| 421883/450337 [00:21<00:01, 18893.50it/s] 94%|█████████▍| 423927/450337 [00:21<00:01, 19344.04it/s] 95%|█████████▍| 425865/450337 [00:21<00:01, 19338.05it/s] 95%|█████████▌| 427932/450337 [00:21<00:01, 19731.87it/s] 96%|█████████▌| 430096/450337 [00:21<00:00, 20295.37it/s] 96%|█████████▌| 432128/450337 [00:21<00:00, 19476.72it/s] 96%|█████████▋| 434180/450337 [00:21<00:00, 19775.78it/s] 97%|█████████▋| 436165/450337 [00:21<00:00, 19277.38it/s] 97%|█████████▋| 438100/450337 [00:21<00:00, 19157.80it/s] 98%|█████████▊| 440021/450337 [00:22<00:00, 18824.77it/s] 98%|█████████▊| 442050/450337 [00:22<00:00, 19243.59it/s] 99%|█████████▊| 444110/450337 [00:22<00:00, 19636.96it/s] 99%|█████████▉| 446078/450337 [00:22<00:00, 19148.87it/s] 99%|█████████▉| 448083/450337 [00:22<00:00, 19406.58it/s]100%|█████████▉| 450139/450337 [00:22<00:00, 19740.44it/s]100%|██████████| 450337/450337 [00:22<00:00, 19927.52it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 21.09it/s]2022-03-02 08:49:21 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-02 08:49:21 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-02 08:49:21 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-02 08:49:21 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-02 08:49:21 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-02 08:49:21 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-02 08:49:21 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-02 08:49:21 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-02 08:49:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-02 08:49:21 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-02 08:49:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-02 08:49:21 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-02 08:49:21 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-02 08:49:21 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_last.pt
2022-03-02 08:49:21 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_last.pt
2022-03-02 08:49:21 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-02 08:49:21 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-02 08:49:21 | INFO | fairseq.trainer | begin training epoch 1
2022-03-02 08:49:21 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-02 08:49:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-02 08:49:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-02 08:49:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 08:49:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 08:49:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-02 08:57:23 | INFO | train_inner | epoch 001:    105 / 393 loss=17.095, ppl=140019, wps=14691.6, ups=0.22, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.109, loss_scale=4, train_wall=476, gb_free=10.1, wall=482
2022-03-02 09:04:49 | INFO | train_inner | epoch 001:    205 / 393 loss=14.853, ppl=29601, wps=14684.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.402, loss_scale=4, train_wall=441, gb_free=10.1, wall=928
2022-03-02 09:12:15 | INFO | train_inner | epoch 001:    305 / 393 loss=12.96, ppl=7967.47, wps=14693.6, ups=0.22, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.943, loss_scale=4, train_wall=441, gb_free=10.1, wall=1374
2022-03-02 09:18:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 09:18:52 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.303 | ppl 2525.88 | wps 34065.5 | wpb 2034.1 | bsz 4 | num_updates 388
2022-03-02 09:18:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-02 09:18:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 09:18:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 09:18:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 1 @ 388 updates, score 11.303) (writing took 4.636136878281832 seconds)
2022-03-02 09:18:57 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-02 09:18:57 | INFO | train | epoch 001 | loss 14.227 | ppl 19173.4 | wps 14596.8 | ups 0.22 | wpb 65460.6 | bsz 127.9 | num_updates 388 | lr 4.85903e-05 | gnorm 1.539 | loss_scale 4 | train_wall 1745 | gb_free 10.1 | wall 1776
2022-03-02 09:18:57 | INFO | fairseq.trainer | begin training epoch 2
2022-03-02 09:18:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 09:19:51 | INFO | train_inner | epoch 002:     12 / 393 loss=11.638, ppl=3187.97, wps=14332.3, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.572, loss_scale=4, train_wall=439, gb_free=10.1, wall=1829
2022-03-02 09:27:16 | INFO | train_inner | epoch 002:    112 / 393 loss=11.108, ppl=2206.75, wps=14699.2, ups=0.22, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.475, loss_scale=4, train_wall=441, gb_free=10.1, wall=2275
2022-03-02 09:34:43 | INFO | train_inner | epoch 002:    212 / 393 loss=10.817, ppl=1803.54, wps=14687.5, ups=0.22, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.52, loss_scale=8, train_wall=441, gb_free=10.1, wall=2722
2022-03-02 09:42:09 | INFO | train_inner | epoch 002:    312 / 393 loss=10.572, ppl=1521.86, wps=14689.8, ups=0.22, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.601, loss_scale=8, train_wall=441, gb_free=10.1, wall=3168
2022-03-02 09:48:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 09:48:14 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.237 | ppl 1206.95 | wps 34040 | wpb 2034.1 | bsz 4 | num_updates 781 | best_loss 10.237
2022-03-02 09:48:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 781 updates
2022-03-02 09:48:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 09:48:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 09:48:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 2 @ 781 updates, score 10.237) (writing took 4.850970342755318 seconds)
2022-03-02 09:48:19 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-02 09:48:19 | INFO | train | epoch 002 | loss 10.754 | ppl 1727.44 | wps 14597.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 0.551 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 3538
2022-03-02 09:48:19 | INFO | fairseq.trainer | begin training epoch 3
2022-03-02 09:48:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 09:49:44 | INFO | train_inner | epoch 003:     19 / 393 loss=10.357, ppl=1311.84, wps=14325, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.664, loss_scale=8, train_wall=439, gb_free=10.1, wall=3623
2022-03-02 09:57:10 | INFO | train_inner | epoch 003:    119 / 393 loss=10.15, ppl=1136.18, wps=14690.4, ups=0.22, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.729, loss_scale=8, train_wall=441, gb_free=10.1, wall=4069
2022-03-02 10:04:37 | INFO | train_inner | epoch 003:    219 / 393 loss=9.979, ppl=1009.11, wps=14687.8, ups=0.22, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.82, loss_scale=8, train_wall=441, gb_free=10.1, wall=4515
2022-03-02 10:12:03 | INFO | train_inner | epoch 003:    319 / 393 loss=9.823, ppl=905.87, wps=14688, ups=0.22, wpb=65535.4, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.812, loss_scale=16, train_wall=441, gb_free=10.1, wall=4962
2022-03-02 10:17:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 10:17:37 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.573 | ppl 761.56 | wps 33923.3 | wpb 2034.1 | bsz 4 | num_updates 1174 | best_loss 9.573
2022-03-02 10:17:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1174 updates
2022-03-02 10:17:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 10:17:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 10:17:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 3 @ 1174 updates, score 9.573) (writing took 4.743927125819027 seconds)
2022-03-02 10:17:42 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-02 10:17:42 | INFO | train | epoch 003 | loss 9.944 | ppl 984.84 | wps 14593.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1174 | lr 0.000146821 | gnorm 0.792 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 5301
2022-03-02 10:17:42 | INFO | fairseq.trainer | begin training epoch 4
2022-03-02 10:17:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 10:19:38 | INFO | train_inner | epoch 004:     26 / 393 loss=9.672, ppl=815.69, wps=14323.9, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.826, loss_scale=16, train_wall=439, gb_free=10.1, wall=5417
2022-03-02 10:27:04 | INFO | train_inner | epoch 004:    126 / 393 loss=9.524, ppl=736.18, wps=14685.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.813, loss_scale=16, train_wall=441, gb_free=10.1, wall=5863
2022-03-02 10:34:31 | INFO | train_inner | epoch 004:    226 / 393 loss=9.412, ppl=681.06, wps=14687.8, ups=0.22, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.843, loss_scale=16, train_wall=441, gb_free=10.1, wall=6310
2022-03-02 10:41:57 | INFO | train_inner | epoch 004:    326 / 393 loss=9.307, ppl=633.51, wps=14687.2, ups=0.22, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.845, loss_scale=16, train_wall=441, gb_free=10.1, wall=6756
2022-03-02 10:46:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 10:47:00 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.109 | ppl 552.08 | wps 34146.7 | wpb 2034.1 | bsz 4 | num_updates 1567 | best_loss 9.109
2022-03-02 10:47:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1567 updates
2022-03-02 10:47:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 10:47:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 10:47:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 4 @ 1567 updates, score 9.109) (writing took 4.4280713107436895 seconds)
2022-03-02 10:47:05 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-02 10:47:05 | INFO | train | epoch 004 | loss 9.391 | ppl 671.27 | wps 14596.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1567 | lr 0.000195936 | gnorm 0.836 | loss_scale 32 | train_wall 1732 | gb_free 10.1 | wall 7064
2022-03-02 10:47:05 | INFO | fairseq.trainer | begin training epoch 5
2022-03-02 10:47:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 10:49:32 | INFO | train_inner | epoch 005:     33 / 393 loss=9.175, ppl=577.84, wps=14332.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.847, loss_scale=32, train_wall=439, gb_free=10.1, wall=7211
2022-03-02 10:56:58 | INFO | train_inner | epoch 005:    133 / 393 loss=9.057, ppl=532.74, wps=14683.5, ups=0.22, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.819, loss_scale=32, train_wall=441, gb_free=10.1, wall=7657
2022-03-02 11:03:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 11:04:29 | INFO | train_inner | epoch 005:    234 / 393 loss=8.976, ppl=503.49, wps=14532.4, ups=0.22, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.839, loss_scale=16, train_wall=446, gb_free=10.1, wall=8108
2022-03-02 11:11:56 | INFO | train_inner | epoch 005:    334 / 393 loss=8.879, ppl=470.68, wps=14678.2, ups=0.22, wpb=65530.2, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.798, loss_scale=16, train_wall=442, gb_free=10.1, wall=8555
2022-03-02 11:16:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 11:16:23 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.754 | ppl 431.83 | wps 34086.8 | wpb 2034.1 | bsz 4 | num_updates 1959 | best_loss 8.754
2022-03-02 11:16:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1959 updates
2022-03-02 11:16:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 11:16:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 11:16:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 5 @ 1959 updates, score 8.754) (writing took 4.409236785955727 seconds)
2022-03-02 11:16:28 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-02 11:16:28 | INFO | train | epoch 005 | loss 8.958 | ppl 497.46 | wps 14552.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 1959 | lr 0.000244926 | gnorm 0.82 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 8827
2022-03-02 11:16:28 | INFO | fairseq.trainer | begin training epoch 6
2022-03-02 11:16:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 11:19:31 | INFO | train_inner | epoch 006:     41 / 393 loss=8.77, ppl=436.66, wps=14335.8, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.792, loss_scale=16, train_wall=439, gb_free=10.1, wall=9010
2022-03-02 11:26:57 | INFO | train_inner | epoch 006:    141 / 393 loss=8.67, ppl=407.39, wps=14677.8, ups=0.22, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.783, loss_scale=16, train_wall=442, gb_free=10.1, wall=9456
2022-03-02 11:34:24 | INFO | train_inner | epoch 006:    241 / 393 loss=8.607, ppl=389.96, wps=14688.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.772, loss_scale=16, train_wall=441, gb_free=10.1, wall=9903
2022-03-02 11:41:50 | INFO | train_inner | epoch 006:    341 / 393 loss=8.54, ppl=372.3, wps=14689.4, ups=0.22, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.78, loss_scale=32, train_wall=441, gb_free=10.1, wall=10349
2022-03-02 11:45:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 11:45:46 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.473 | ppl 355.29 | wps 33676.7 | wpb 2034.1 | bsz 4 | num_updates 2352 | best_loss 8.473
2022-03-02 11:45:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2352 updates
2022-03-02 11:45:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 11:45:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 11:45:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 6 @ 2352 updates, score 8.473) (writing took 4.729650497436523 seconds)
2022-03-02 11:45:51 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-02 11:45:51 | INFO | train | epoch 006 | loss 8.601 | ppl 388.17 | wps 14590.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 2352 | lr 0.000294041 | gnorm 0.773 | loss_scale 32 | train_wall 1733 | gb_free 10.1 | wall 10590
2022-03-02 11:45:51 | INFO | fairseq.trainer | begin training epoch 7
2022-03-02 11:45:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 11:49:26 | INFO | train_inner | epoch 007:     48 / 393 loss=8.428, ppl=344.35, wps=14311.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.75, loss_scale=32, train_wall=440, gb_free=10.1, wall=10805
2022-03-02 11:56:52 | INFO | train_inner | epoch 007:    148 / 393 loss=8.339, ppl=323.85, wps=14686.4, ups=0.22, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.743, loss_scale=32, train_wall=441, gb_free=10.1, wall=11251
2022-03-02 12:04:18 | INFO | train_inner | epoch 007:    248 / 393 loss=8.304, ppl=315.99, wps=14690.2, ups=0.22, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.728, loss_scale=32, train_wall=441, gb_free=10.1, wall=11697
2022-03-02 12:10:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 12:11:49 | INFO | train_inner | epoch 007:    349 / 393 loss=8.251, ppl=304.55, wps=14532.5, ups=0.22, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.74, loss_scale=16, train_wall=446, gb_free=10.1, wall=12148
2022-03-02 12:15:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 12:15:10 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.263 | ppl 307.13 | wps 34159.2 | wpb 2034.1 | bsz 4 | num_updates 2744 | best_loss 8.263
2022-03-02 12:15:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2744 updates
2022-03-02 12:15:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 12:15:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 12:15:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 7 @ 2744 updates, score 8.263) (writing took 4.770553143694997 seconds)
2022-03-02 12:15:15 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-02 12:15:15 | INFO | train | epoch 007 | loss 8.298 | ppl 314.84 | wps 14552.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 2744 | lr 0.000343031 | gnorm 0.737 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 12353
2022-03-02 12:15:15 | INFO | fairseq.trainer | begin training epoch 8
2022-03-02 12:15:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 12:19:24 | INFO | train_inner | epoch 008:     56 / 393 loss=8.15, ppl=284.07, wps=14331.4, ups=0.22, wpb=65238.4, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.72, loss_scale=16, train_wall=439, gb_free=10.1, wall=12603
2022-03-02 12:26:50 | INFO | train_inner | epoch 008:    156 / 393 loss=8.065, ppl=267.74, wps=14691.7, ups=0.22, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.699, loss_scale=16, train_wall=441, gb_free=10.1, wall=13049
2022-03-02 12:34:16 | INFO | train_inner | epoch 008:    256 / 393 loss=8.044, ppl=263.92, wps=14687.9, ups=0.22, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.694, loss_scale=16, train_wall=441, gb_free=10.1, wall=13495
2022-03-02 12:41:43 | INFO | train_inner | epoch 008:    356 / 393 loss=8.015, ppl=258.67, wps=14682.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.686, loss_scale=16, train_wall=441, gb_free=10.1, wall=13942
2022-03-02 12:44:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 12:44:32 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.086 | ppl 271.71 | wps 34133 | wpb 2034.1 | bsz 4 | num_updates 3137 | best_loss 8.086
2022-03-02 12:44:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3137 updates
2022-03-02 12:44:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 12:44:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 12:44:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 8 @ 3137 updates, score 8.086) (writing took 4.660842058248818 seconds)
2022-03-02 12:44:37 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-02 12:44:37 | INFO | train | epoch 008 | loss 8.046 | ppl 264.26 | wps 14596.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 3137 | lr 0.000392147 | gnorm 0.7 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 14116
2022-03-02 12:44:37 | INFO | fairseq.trainer | begin training epoch 9
2022-03-02 12:44:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 12:49:18 | INFO | train_inner | epoch 009:     63 / 393 loss=7.908, ppl=240.12, wps=14329, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.688, loss_scale=32, train_wall=439, gb_free=10.1, wall=14397
2022-03-02 12:56:44 | INFO | train_inner | epoch 009:    163 / 393 loss=7.852, ppl=230.98, wps=14692.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.674, loss_scale=32, train_wall=441, gb_free=10.1, wall=14843
2022-03-02 12:56:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 13:04:15 | INFO | train_inner | epoch 009:    264 / 393 loss=7.837, ppl=228.68, wps=14551.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.673, loss_scale=16, train_wall=445, gb_free=10.1, wall=15293
2022-03-02 13:11:41 | INFO | train_inner | epoch 009:    364 / 393 loss=7.812, ppl=224.67, wps=14694.7, ups=0.22, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.652, loss_scale=16, train_wall=441, gb_free=10.1, wall=15739
2022-03-02 13:13:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 13:13:54 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.958 | ppl 248.71 | wps 34074.9 | wpb 2034.1 | bsz 4 | num_updates 3529 | best_loss 7.958
2022-03-02 13:13:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3529 updates
2022-03-02 13:13:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 13:13:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 13:13:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 9 @ 3529 updates, score 7.958) (writing took 4.5470996564254165 seconds)
2022-03-02 13:13:59 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-02 13:13:59 | INFO | train | epoch 009 | loss 7.834 | ppl 228.22 | wps 14564.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3529 | lr 0.000441137 | gnorm 0.661 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 15878
2022-03-02 13:13:59 | INFO | fairseq.trainer | begin training epoch 10
2022-03-02 13:13:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 13:19:16 | INFO | train_inner | epoch 010:     71 / 393 loss=7.696, ppl=207.41, wps=14334, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.634, loss_scale=16, train_wall=439, gb_free=10.1, wall=16195
2022-03-02 13:26:42 | INFO | train_inner | epoch 010:    171 / 393 loss=7.657, ppl=201.77, wps=14684.8, ups=0.22, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.641, loss_scale=16, train_wall=441, gb_free=10.1, wall=16641
2022-03-02 13:34:08 | INFO | train_inner | epoch 010:    271 / 393 loss=7.663, ppl=202.68, wps=14681.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.637, loss_scale=16, train_wall=441, gb_free=10.1, wall=17087
2022-03-02 13:41:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 13:41:39 | INFO | train_inner | epoch 010:    372 / 393 loss=7.653, ppl=201.21, wps=14542.8, ups=0.22, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.616, loss_scale=16, train_wall=446, gb_free=10.1, wall=17538
2022-03-02 13:43:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 13:43:17 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.875 | ppl 234.76 | wps 34097.9 | wpb 2034.1 | bsz 4 | num_updates 3921 | best_loss 7.875
2022-03-02 13:43:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3921 updates
2022-03-02 13:43:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 13:43:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 13:43:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 10 @ 3921 updates, score 7.875) (writing took 4.766206522472203 seconds)
2022-03-02 13:43:22 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-02 13:43:22 | INFO | train | epoch 010 | loss 7.656 | ppl 201.67 | wps 14554.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3921 | lr 0.000490127 | gnorm 0.638 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 17641
2022-03-02 13:43:22 | INFO | fairseq.trainer | begin training epoch 11
2022-03-02 13:43:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 13:49:15 | INFO | train_inner | epoch 011:     79 / 393 loss=7.516, ppl=183.04, wps=14315.3, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.638, loss_scale=16, train_wall=440, gb_free=10.1, wall=17994
2022-03-02 13:56:41 | INFO | train_inner | epoch 011:    179 / 393 loss=7.498, ppl=180.73, wps=14690.9, ups=0.22, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.606, loss_scale=16, train_wall=441, gb_free=10.1, wall=18440
2022-03-02 14:04:07 | INFO | train_inner | epoch 011:    279 / 393 loss=7.5, ppl=181.02, wps=14688.3, ups=0.22, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.578, loss_scale=16, train_wall=441, gb_free=10.1, wall=18886
2022-03-02 14:11:33 | INFO | train_inner | epoch 011:    379 / 393 loss=7.509, ppl=182.17, wps=14693, ups=0.22, wpb=65530.2, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.577, loss_scale=16, train_wall=441, gb_free=10.1, wall=19332
2022-03-02 14:12:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 14:12:40 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.756 | ppl 216.14 | wps 33773.6 | wpb 2034.1 | bsz 4 | num_updates 4314 | best_loss 7.756
2022-03-02 14:12:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4314 updates
2022-03-02 14:12:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 14:12:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 14:12:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 11 @ 4314 updates, score 7.756) (writing took 4.787195095792413 seconds)
2022-03-02 14:12:45 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-02 14:12:45 | INFO | train | epoch 011 | loss 7.498 | ppl 180.75 | wps 14591.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 4314 | lr 0.00048146 | gnorm 0.593 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 19404
2022-03-02 14:12:45 | INFO | fairseq.trainer | begin training epoch 12
2022-03-02 14:12:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 14:19:09 | INFO | train_inner | epoch 012:     86 / 393 loss=7.352, ppl=163.4, wps=14305.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.578, loss_scale=16, train_wall=440, gb_free=10.1, wall=19788
2022-03-02 14:19:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 14:26:40 | INFO | train_inner | epoch 012:    187 / 393 loss=7.346, ppl=162.74, wps=14547.2, ups=0.22, wpb=65530.2, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.564, loss_scale=16, train_wall=446, gb_free=10.1, wall=20239
2022-03-02 14:34:06 | INFO | train_inner | epoch 012:    287 / 393 loss=7.359, ppl=164.15, wps=14689.1, ups=0.22, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.543, loss_scale=16, train_wall=441, gb_free=10.1, wall=20685
2022-03-02 14:41:32 | INFO | train_inner | epoch 012:    387 / 393 loss=7.353, ppl=163.43, wps=14690.2, ups=0.22, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.543, loss_scale=16, train_wall=441, gb_free=10.1, wall=21131
2022-03-02 14:41:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 14:42:03 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.698 | ppl 207.7 | wps 33924.2 | wpb 2034.1 | bsz 4 | num_updates 4706 | best_loss 7.698
2022-03-02 14:42:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4706 updates
2022-03-02 14:42:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 14:42:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 14:42:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 12 @ 4706 updates, score 7.698) (writing took 4.733018023893237 seconds)
2022-03-02 14:42:08 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-02 14:42:08 | INFO | train | epoch 012 | loss 7.348 | ppl 162.91 | wps 14556.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 4706 | lr 0.000460971 | gnorm 0.557 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 21167
2022-03-02 14:42:08 | INFO | fairseq.trainer | begin training epoch 13
2022-03-02 14:42:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 14:49:07 | INFO | train_inner | epoch 013:     94 / 393 loss=7.205, ppl=147.57, wps=14332.4, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.559, loss_scale=16, train_wall=439, gb_free=10.1, wall=21586
2022-03-02 14:56:33 | INFO | train_inner | epoch 013:    194 / 393 loss=7.216, ppl=148.71, wps=14696.3, ups=0.22, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.534, loss_scale=16, train_wall=441, gb_free=10.1, wall=22032
2022-03-02 14:59:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 15:04:04 | INFO | train_inner | epoch 013:    295 / 393 loss=7.235, ppl=150.65, wps=14538.3, ups=0.22, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.535, loss_scale=16, train_wall=446, gb_free=10.1, wall=22483
2022-03-02 15:11:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 15:11:25 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.662 | ppl 202.55 | wps 34196.4 | wpb 2034.1 | bsz 4 | num_updates 5098 | best_loss 7.662
2022-03-02 15:11:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5098 updates
2022-03-02 15:11:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 15:11:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 15:11:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 13 @ 5098 updates, score 7.662) (writing took 4.5006398959085345 seconds)
2022-03-02 15:11:30 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-02 15:11:30 | INFO | train | epoch 013 | loss 7.224 | ppl 149.48 | wps 14563.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5098 | lr 0.000442894 | gnorm 0.54 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 22929
2022-03-02 15:11:30 | INFO | fairseq.trainer | begin training epoch 14
2022-03-02 15:11:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 15:11:39 | INFO | train_inner | epoch 014:      2 / 393 loss=7.244, ppl=151.58, wps=14339.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.532, loss_scale=16, train_wall=439, gb_free=10.1, wall=22938
2022-03-02 15:19:05 | INFO | train_inner | epoch 014:    102 / 393 loss=7.072, ppl=134.56, wps=14704.9, ups=0.22, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.539, loss_scale=16, train_wall=441, gb_free=10.1, wall=23384
2022-03-02 15:26:31 | INFO | train_inner | epoch 014:    202 / 393 loss=7.116, ppl=138.75, wps=14688.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.537, loss_scale=16, train_wall=441, gb_free=10.1, wall=23830
2022-03-02 15:33:57 | INFO | train_inner | epoch 014:    302 / 393 loss=7.143, ppl=141.29, wps=14695.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.528, loss_scale=16, train_wall=441, gb_free=10.1, wall=24276
2022-03-02 15:38:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 15:40:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 15:40:47 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.631 | ppl 198.24 | wps 34142.9 | wpb 2034.1 | bsz 4 | num_updates 5490 | best_loss 7.631
2022-03-02 15:40:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5490 updates
2022-03-02 15:40:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 15:40:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 15:40:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 14 @ 5490 updates, score 7.631) (writing took 4.827393224462867 seconds)
2022-03-02 15:40:52 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-02 15:40:52 | INFO | train | epoch 014 | loss 7.118 | ppl 138.86 | wps 14564.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5490 | lr 0.00042679 | gnorm 0.532 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 24691
2022-03-02 15:40:52 | INFO | fairseq.trainer | begin training epoch 15
2022-03-02 15:40:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 15:41:37 | INFO | train_inner | epoch 015:     10 / 393 loss=7.125, ppl=139.6, wps=14187.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.521, loss_scale=16, train_wall=444, gb_free=10.1, wall=24736
2022-03-02 15:49:03 | INFO | train_inner | epoch 015:    110 / 393 loss=6.983, ppl=126.51, wps=14683.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.524, loss_scale=16, train_wall=441, gb_free=10.1, wall=25182
2022-03-02 15:56:29 | INFO | train_inner | epoch 015:    210 / 393 loss=7.02, ppl=129.77, wps=14696.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.529, loss_scale=16, train_wall=441, gb_free=10.1, wall=25628
2022-03-02 16:03:55 | INFO | train_inner | epoch 015:    310 / 393 loss=7.048, ppl=132.35, wps=14691.1, ups=0.22, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.506, loss_scale=16, train_wall=441, gb_free=10.1, wall=26074
2022-03-02 16:10:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 16:10:09 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.609 | ppl 195.19 | wps 34139.4 | wpb 2034.1 | bsz 4 | num_updates 5883 | best_loss 7.609
2022-03-02 16:10:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5883 updates
2022-03-02 16:10:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 16:10:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 16:10:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 15 @ 5883 updates, score 7.609) (writing took 4.609421371482313 seconds)
2022-03-02 16:10:14 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-02 16:10:14 | INFO | train | epoch 015 | loss 7.025 | ppl 130.25 | wps 14598.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 5883 | lr 0.000412288 | gnorm 0.518 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 26453
2022-03-02 16:10:14 | INFO | fairseq.trainer | begin training epoch 16
2022-03-02 16:10:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 16:11:30 | INFO | train_inner | epoch 016:     17 / 393 loss=7.032, ppl=130.9, wps=14337.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.524, loss_scale=16, train_wall=439, gb_free=10.1, wall=26529
2022-03-02 16:17:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 16:19:01 | INFO | train_inner | epoch 016:    118 / 393 loss=6.897, ppl=119.16, wps=14544, ups=0.22, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.521, loss_scale=16, train_wall=446, gb_free=10.1, wall=26980
2022-03-02 16:26:27 | INFO | train_inner | epoch 016:    218 / 393 loss=6.939, ppl=122.67, wps=14690.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.533, loss_scale=16, train_wall=441, gb_free=10.1, wall=27426
2022-03-02 16:33:52 | INFO | train_inner | epoch 016:    318 / 393 loss=6.972, ppl=125.51, wps=14701.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.522, loss_scale=16, train_wall=441, gb_free=10.1, wall=27871
2022-03-02 16:39:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 16:39:31 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.596 | ppl 193.52 | wps 34015 | wpb 2034.1 | bsz 4 | num_updates 6275 | best_loss 7.596
2022-03-02 16:39:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6275 updates
2022-03-02 16:39:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 16:39:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 16:39:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 16 @ 6275 updates, score 7.596) (writing took 4.569247934967279 seconds)
2022-03-02 16:39:36 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-02 16:39:36 | INFO | train | epoch 016 | loss 6.944 | ppl 123.09 | wps 14564.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6275 | lr 0.000399202 | gnorm 0.525 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 28215
2022-03-02 16:39:36 | INFO | fairseq.trainer | begin training epoch 17
2022-03-02 16:39:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 16:41:28 | INFO | train_inner | epoch 017:     25 / 393 loss=6.939, ppl=122.67, wps=14338.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.504, loss_scale=16, train_wall=439, gb_free=10.1, wall=28326
2022-03-02 16:48:53 | INFO | train_inner | epoch 017:    125 / 393 loss=6.832, ppl=113.94, wps=14701.1, ups=0.22, wpb=65530.2, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.532, loss_scale=16, train_wall=441, gb_free=10.1, wall=28772
2022-03-02 16:56:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 16:56:24 | INFO | train_inner | epoch 017:    226 / 393 loss=6.872, ppl=117.1, wps=14549.2, ups=0.22, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.524, loss_scale=16, train_wall=445, gb_free=10.1, wall=29223
2022-03-02 17:03:50 | INFO | train_inner | epoch 017:    326 / 393 loss=6.892, ppl=118.79, wps=14687.7, ups=0.22, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.504, loss_scale=16, train_wall=441, gb_free=10.1, wall=29669
2022-03-02 17:08:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 17:08:53 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.589 | ppl 192.6 | wps 34052.5 | wpb 2034.1 | bsz 4 | num_updates 6667 | best_loss 7.589
2022-03-02 17:08:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6667 updates
2022-03-02 17:08:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 17:08:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 17:08:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 17 @ 6667 updates, score 7.589) (writing took 4.42154121119529 seconds)
2022-03-02 17:08:58 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-02 17:08:58 | INFO | train | epoch 017 | loss 6.87 | ppl 116.99 | wps 14567 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6667 | lr 0.000387289 | gnorm 0.52 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 29977
2022-03-02 17:08:58 | INFO | fairseq.trainer | begin training epoch 18
2022-03-02 17:08:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 17:11:25 | INFO | train_inner | epoch 018:     33 / 393 loss=6.863, ppl=116.4, wps=14344.9, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.527, loss_scale=16, train_wall=439, gb_free=10.1, wall=30124
2022-03-02 17:18:51 | INFO | train_inner | epoch 018:    133 / 393 loss=6.755, ppl=107.98, wps=14691.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.519, loss_scale=16, train_wall=441, gb_free=10.1, wall=30570
2022-03-02 17:26:17 | INFO | train_inner | epoch 018:    233 / 393 loss=6.801, ppl=111.53, wps=14693, ups=0.22, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.54, loss_scale=16, train_wall=441, gb_free=10.1, wall=31016
2022-03-02 17:33:43 | INFO | train_inner | epoch 018:    333 / 393 loss=6.843, ppl=114.78, wps=14694.5, ups=0.22, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.51, loss_scale=16, train_wall=441, gb_free=10.1, wall=31462
2022-03-02 17:34:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 17:38:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 17:38:15 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.586 | ppl 192.19 | wps 34020.9 | wpb 2034.1 | bsz 4 | num_updates 7059 | best_loss 7.586
2022-03-02 17:38:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7059 updates
2022-03-02 17:38:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 17:38:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt
2022-03-02 17:38:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.09_0.01_0.9_#1/checkpoint_best.pt (epoch 18 @ 7059 updates, score 7.586) (writing took 4.825163139030337 seconds)
2022-03-02 17:38:20 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-02 17:38:20 | INFO | train | epoch 018 | loss 6.804 | ppl 111.76 | wps 14563.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7059 | lr 0.000376382 | gnorm 0.525 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 31739
2022-03-02 17:38:20 | INFO | fairseq.trainer | begin training epoch 19
2022-03-02 17:38:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 17:41:23 | INFO | train_inner | epoch 019:     41 / 393 loss=6.786, ppl=110.32, wps=14195, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.524, loss_scale=16, train_wall=443, gb_free=10.1, wall=31921
2022-03-02 17:48:48 | INFO | train_inner | epoch 019:    141 / 393 loss=6.71, ppl=104.67, wps=14697.9, ups=0.22, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.532, loss_scale=16, train_wall=441, gb_free=10.1, wall=32367
2022-03-02 17:56:14 | INFO | train_inner | epoch 019:    241 / 393 loss=6.744, ppl=107.17, wps=14702.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.534, loss_scale=16, train_wall=441, gb_free=10.1, wall=32813
2022-03-02 18:03:40 | INFO | train_inner | epoch 019:    341 / 393 loss=6.777, ppl=109.65, wps=14694.8, ups=0.22, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.55, loss_scale=16, train_wall=441, gb_free=10.1, wall=33259
2022-03-02 18:07:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 18:07:36 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.588 | ppl 192.47 | wps 34184.3 | wpb 2034.1 | bsz 4 | num_updates 7452 | best_loss 7.586
2022-03-02 18:07:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7452 updates
2022-03-02 18:07:36 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-02 18:07:36 | INFO | train | epoch 019 | loss 6.744 | ppl 107.18 | wps 14644.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 7452 | lr 0.000366322 | gnorm 0.531 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 33495
2022-03-02 18:07:36 | INFO | fairseq.trainer | begin training epoch 20
2022-03-02 18:07:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 18:11:10 | INFO | train_inner | epoch 020:     48 / 393 loss=6.711, ppl=104.79, wps=14490, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.525, loss_scale=16, train_wall=439, gb_free=10.1, wall=33709
2022-03-02 18:14:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 18:18:41 | INFO | train_inner | epoch 020:    149 / 393 loss=6.652, ppl=100.56, wps=14551.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.525, loss_scale=16, train_wall=445, gb_free=10.1, wall=34160
2022-03-02 18:26:07 | INFO | train_inner | epoch 020:    249 / 393 loss=6.692, ppl=103.39, wps=14697.3, ups=0.22, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.542, loss_scale=16, train_wall=441, gb_free=10.1, wall=34606
2022-03-02 18:33:33 | INFO | train_inner | epoch 020:    349 / 393 loss=6.731, ppl=106.21, wps=14689.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.522, loss_scale=16, train_wall=441, gb_free=10.1, wall=35052
2022-03-02 18:36:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 18:36:54 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.594 | ppl 193.25 | wps 34098.8 | wpb 2034.1 | bsz 4 | num_updates 7844 | best_loss 7.586
2022-03-02 18:36:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7844 updates
2022-03-02 18:36:54 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-02 18:36:54 | INFO | train | epoch 020 | loss 6.689 | ppl 103.16 | wps 14603.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7844 | lr 0.000357052 | gnorm 0.534 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 35253
2022-03-02 18:36:54 | INFO | fairseq.trainer | begin training epoch 21
2022-03-02 18:36:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 18:41:03 | INFO | train_inner | epoch 021:     56 / 393 loss=6.65, ppl=100.41, wps=14478.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.544, loss_scale=16, train_wall=439, gb_free=10.1, wall=35502
2022-03-02 18:48:30 | INFO | train_inner | epoch 021:    156 / 393 loss=6.604, ppl=97.29, wps=14690.8, ups=0.22, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.538, loss_scale=16, train_wall=441, gb_free=10.1, wall=35948
2022-03-02 18:52:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 18:56:00 | INFO | train_inner | epoch 021:    257 / 393 loss=6.651, ppl=100.5, wps=14552.9, ups=0.22, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.534, loss_scale=16, train_wall=445, gb_free=10.1, wall=36399
2022-03-02 19:03:26 | INFO | train_inner | epoch 021:    357 / 393 loss=6.663, ppl=101.33, wps=14703.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.528, loss_scale=16, train_wall=441, gb_free=10.1, wall=36844
2022-03-02 19:06:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 19:06:11 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.608 | ppl 195.15 | wps 34151.7 | wpb 2034.1 | bsz 4 | num_updates 8236 | best_loss 7.586
2022-03-02 19:06:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8236 updates
2022-03-02 19:06:11 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-02 19:06:11 | INFO | train | epoch 021 | loss 6.637 | ppl 99.55 | wps 14605.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8236 | lr 0.000348451 | gnorm 0.534 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 37009
2022-03-02 19:06:11 | INFO | fairseq.trainer | begin training epoch 22
2022-03-02 19:06:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 19:10:56 | INFO | train_inner | epoch 022:     64 / 393 loss=6.592, ppl=96.48, wps=14486.8, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.551, loss_scale=16, train_wall=439, gb_free=10.1, wall=37295
2022-03-02 19:18:22 | INFO | train_inner | epoch 022:    164 / 393 loss=6.56, ppl=94.35, wps=14709.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.541, loss_scale=16, train_wall=441, gb_free=10.1, wall=37740
2022-03-02 19:25:47 | INFO | train_inner | epoch 022:    264 / 393 loss=6.599, ppl=96.93, wps=14703.7, ups=0.22, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.531, loss_scale=16, train_wall=441, gb_free=10.1, wall=38186
2022-03-02 19:30:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 19:33:17 | INFO | train_inner | epoch 022:    365 / 393 loss=6.639, ppl=99.64, wps=14566.4, ups=0.22, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.551, loss_scale=16, train_wall=445, gb_free=10.1, wall=38636
2022-03-02 19:35:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 19:35:26 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.62 | ppl 196.7 | wps 34201.3 | wpb 2034.1 | bsz 4 | num_updates 8628 | best_loss 7.586
2022-03-02 19:35:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8628 updates
2022-03-02 19:35:26 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-02 19:35:26 | INFO | train | epoch 022 | loss 6.59 | ppl 96.33 | wps 14615.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8628 | lr 0.000340443 | gnorm 0.545 | loss_scale 16 | train_wall 1730 | gb_free 10.1 | wall 38765
2022-03-02 19:35:26 | INFO | fairseq.trainer | begin training epoch 23
2022-03-02 19:35:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 19:40:47 | INFO | train_inner | epoch 023:     72 / 393 loss=6.523, ppl=91.94, wps=14495, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.547, loss_scale=16, train_wall=439, gb_free=10.1, wall=39086
2022-03-02 19:48:14 | INFO | train_inner | epoch 023:    172 / 393 loss=6.514, ppl=91.41, wps=14685.4, ups=0.22, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.55, loss_scale=16, train_wall=441, gb_free=10.1, wall=39532
2022-03-02 19:55:39 | INFO | train_inner | epoch 023:    272 / 393 loss=6.565, ppl=94.7, wps=14697.1, ups=0.22, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.564, loss_scale=16, train_wall=441, gb_free=10.1, wall=39978
2022-03-02 20:03:05 | INFO | train_inner | epoch 023:    372 / 393 loss=6.595, ppl=96.66, wps=14699.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.54, loss_scale=16, train_wall=441, gb_free=10.1, wall=40424
2022-03-02 20:04:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 20:04:43 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.611 | ppl 195.52 | wps 33921.2 | wpb 2034.1 | bsz 4 | num_updates 9021 | best_loss 7.586
2022-03-02 20:04:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9021 updates
2022-03-02 20:04:44 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-02 20:04:44 | INFO | train | epoch 023 | loss 6.545 | ppl 93.39 | wps 14640.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9021 | lr 0.000332945 | gnorm 0.548 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 40522
2022-03-02 20:04:44 | INFO | fairseq.trainer | begin training epoch 24
2022-03-02 20:04:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 20:09:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 20:10:41 | INFO | train_inner | epoch 024:     80 / 393 loss=6.471, ppl=88.7, wps=14325.8, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.553, loss_scale=16, train_wall=444, gb_free=10.1, wall=40880
2022-03-02 20:18:07 | INFO | train_inner | epoch 024:    180 / 393 loss=6.475, ppl=88.94, wps=14686.6, ups=0.22, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.558, loss_scale=16, train_wall=441, gb_free=10.1, wall=41326
2022-03-02 20:25:33 | INFO | train_inner | epoch 024:    280 / 393 loss=6.524, ppl=92, wps=14699.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.547, loss_scale=16, train_wall=441, gb_free=10.1, wall=41772
2022-03-02 20:32:59 | INFO | train_inner | epoch 024:    380 / 393 loss=6.554, ppl=93.95, wps=14686.1, ups=0.22, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.548, loss_scale=16, train_wall=441, gb_free=10.1, wall=42218
2022-03-02 20:33:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 20:34:01 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.645 | ppl 200.09 | wps 33859.5 | wpb 2034.1 | bsz 4 | num_updates 9413 | best_loss 7.586
2022-03-02 20:34:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9413 updates
2022-03-02 20:34:02 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-02 20:34:02 | INFO | train | epoch 024 | loss 6.503 | ppl 90.69 | wps 14596.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 9413 | lr 0.000325939 | gnorm 0.554 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 42280
2022-03-02 20:34:02 | INFO | fairseq.trainer | begin training epoch 25
2022-03-02 20:34:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 20:40:29 | INFO | train_inner | epoch 025:     87 / 393 loss=6.42, ppl=85.63, wps=14486.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.574, loss_scale=16, train_wall=439, gb_free=10.1, wall=42668
2022-03-02 20:47:55 | INFO | train_inner | epoch 025:    187 / 393 loss=6.441, ppl=86.87, wps=14697.5, ups=0.22, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.549, loss_scale=32, train_wall=441, gb_free=10.1, wall=43114
2022-03-02 20:48:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 20:55:26 | INFO | train_inner | epoch 025:    288 / 393 loss=6.481, ppl=89.3, wps=14545.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.55, loss_scale=16, train_wall=446, gb_free=10.1, wall=43565
2022-03-02 21:02:52 | INFO | train_inner | epoch 025:    388 / 393 loss=6.527, ppl=92.21, wps=14684.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.549, loss_scale=16, train_wall=441, gb_free=10.1, wall=44011
2022-03-02 21:03:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 21:03:19 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.644 | ppl 200.01 | wps 34030.1 | wpb 2034.1 | bsz 4 | num_updates 9805 | best_loss 7.586
2022-03-02 21:03:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9805 updates
2022-03-02 21:03:19 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-02 21:03:19 | INFO | train | epoch 025 | loss 6.464 | ppl 88.28 | wps 14601.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 9805 | lr 0.000319357 | gnorm 0.556 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 44038
2022-03-02 21:03:19 | INFO | fairseq.trainer | begin training epoch 26
2022-03-02 21:03:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 21:10:23 | INFO | train_inner | epoch 026:     95 / 393 loss=6.37, ppl=82.72, wps=14480.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.565, loss_scale=16, train_wall=439, gb_free=10.1, wall=44462
2022-03-02 21:17:49 | INFO | train_inner | epoch 026:    195 / 393 loss=6.411, ppl=85.1, wps=14698.2, ups=0.22, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.558, loss_scale=16, train_wall=441, gb_free=10.1, wall=44908
2022-03-02 21:25:14 | INFO | train_inner | epoch 026:    295 / 393 loss=6.444, ppl=87.08, wps=14705.9, ups=0.22, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.564, loss_scale=16, train_wall=441, gb_free=10.1, wall=45353
2022-03-02 21:27:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 21:32:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 21:32:36 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.66 | ppl 202.28 | wps 34064.5 | wpb 2034.1 | bsz 4 | num_updates 10197 | best_loss 7.586
2022-03-02 21:32:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10197 updates
2022-03-02 21:32:36 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-02 21:32:36 | INFO | train | epoch 026 | loss 6.427 | ppl 86.07 | wps 14606.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10197 | lr 0.000313158 | gnorm 0.563 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 45795
2022-03-02 21:32:36 | INFO | fairseq.trainer | begin training epoch 27
2022-03-02 21:32:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 21:32:49 | INFO | train_inner | epoch 027:      3 / 393 loss=6.485, ppl=89.55, wps=14341.7, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.565, loss_scale=16, train_wall=444, gb_free=10.1, wall=45808
2022-03-02 21:37:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 21:40:19 | INFO | train_inner | epoch 027:    104 / 393 loss=6.329, ppl=80.37, wps=14558.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.558, loss_scale=8, train_wall=445, gb_free=10.1, wall=46258
2022-03-02 21:47:45 | INFO | train_inner | epoch 027:    204 / 393 loss=6.377, ppl=83.1, wps=14711, ups=0.22, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.573, loss_scale=8, train_wall=441, gb_free=10.1, wall=46704
2022-03-02 21:55:11 | INFO | train_inner | epoch 027:    304 / 393 loss=6.421, ppl=85.68, wps=14696.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.561, loss_scale=8, train_wall=441, gb_free=10.1, wall=47150
2022-03-02 22:01:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 22:01:52 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.671 | ppl 203.84 | wps 34166.3 | wpb 2034.1 | bsz 4 | num_updates 10589 | best_loss 7.586
2022-03-02 22:01:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10589 updates
2022-03-02 22:01:52 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-02 22:01:52 | INFO | train | epoch 027 | loss 6.393 | ppl 84.06 | wps 14609.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10589 | lr 0.000307307 | gnorm 0.567 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 47551
2022-03-02 22:01:52 | INFO | fairseq.trainer | begin training epoch 28
2022-03-02 22:01:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 22:02:41 | INFO | train_inner | epoch 028:     11 / 393 loss=6.437, ppl=86.65, wps=14482.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.575, loss_scale=8, train_wall=439, gb_free=10.1, wall=47600
2022-03-02 22:10:07 | INFO | train_inner | epoch 028:    111 / 393 loss=6.295, ppl=78.51, wps=14699.7, ups=0.22, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.566, loss_scale=8, train_wall=441, gb_free=10.1, wall=48046
2022-03-02 22:17:33 | INFO | train_inner | epoch 028:    211 / 393 loss=6.344, ppl=81.23, wps=14702.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.577, loss_scale=16, train_wall=441, gb_free=10.1, wall=48492
2022-03-02 22:19:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 22:25:03 | INFO | train_inner | epoch 028:    312 / 393 loss=6.391, ppl=83.91, wps=14562.4, ups=0.22, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.586, loss_scale=8, train_wall=445, gb_free=10.1, wall=48942
2022-03-02 22:31:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 22:31:08 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.688 | ppl 206.2 | wps 34035.8 | wpb 2034.1 | bsz 4 | num_updates 10981 | best_loss 7.586
2022-03-02 22:31:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10981 updates
2022-03-02 22:31:08 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-02 22:31:08 | INFO | train | epoch 028 | loss 6.361 | ppl 82.18 | wps 14610.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10981 | lr 0.000301772 | gnorm 0.576 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 49307
2022-03-02 22:31:08 | INFO | fairseq.trainer | begin training epoch 29
2022-03-02 22:31:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 22:32:33 | INFO | train_inner | epoch 029:     19 / 393 loss=6.403, ppl=84.62, wps=14487.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.583, loss_scale=8, train_wall=439, gb_free=10.1, wall=49392
2022-03-02 22:40:00 | INFO | train_inner | epoch 029:    119 / 393 loss=6.274, ppl=77.39, wps=14679.2, ups=0.22, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.584, loss_scale=8, train_wall=442, gb_free=10.1, wall=49839
2022-03-02 22:47:26 | INFO | train_inner | epoch 029:    219 / 393 loss=6.319, ppl=79.85, wps=14690.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.593, loss_scale=8, train_wall=441, gb_free=10.1, wall=50285
2022-03-02 22:54:52 | INFO | train_inner | epoch 029:    319 / 393 loss=6.366, ppl=82.51, wps=14698.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.576, loss_scale=8, train_wall=441, gb_free=10.1, wall=50731
2022-03-02 23:00:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 23:00:26 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.709 | ppl 209.24 | wps 34035.7 | wpb 2034.1 | bsz 4 | num_updates 11374 | best_loss 7.586
2022-03-02 23:00:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11374 updates
2022-03-02 23:00:26 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-02 23:00:26 | INFO | train | epoch 029 | loss 6.329 | ppl 80.41 | wps 14635.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 11374 | lr 0.000296513 | gnorm 0.586 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 51065
2022-03-02 23:00:26 | INFO | fairseq.trainer | begin training epoch 30
2022-03-02 23:00:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 23:02:22 | INFO | train_inner | epoch 030:     26 / 393 loss=6.345, ppl=81.32, wps=14477, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.58, loss_scale=16, train_wall=439, gb_free=10.1, wall=51181
2022-03-02 23:09:48 | INFO | train_inner | epoch 030:    126 / 393 loss=6.254, ppl=76.3, wps=14697.5, ups=0.22, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.586, loss_scale=16, train_wall=441, gb_free=10.1, wall=51627
2022-03-02 23:17:14 | INFO | train_inner | epoch 030:    226 / 393 loss=6.286, ppl=78.04, wps=14702.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.579, loss_scale=16, train_wall=441, gb_free=10.1, wall=52073
2022-03-02 23:23:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 23:24:44 | INFO | train_inner | epoch 030:    327 / 393 loss=6.337, ppl=80.83, wps=14549.5, ups=0.22, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.596, loss_scale=8, train_wall=445, gb_free=10.1, wall=52523
2022-03-02 23:29:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 23:29:43 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.723 | ppl 211.32 | wps 34061.9 | wpb 2034.1 | bsz 4 | num_updates 11766 | best_loss 7.586
2022-03-02 23:29:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11766 updates
2022-03-02 23:29:43 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-02 23:29:43 | INFO | train | epoch 030 | loss 6.3 | ppl 78.78 | wps 14606.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 11766 | lr 0.000291532 | gnorm 0.588 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 52822
2022-03-02 23:29:43 | INFO | fairseq.trainer | begin training epoch 31
2022-03-02 23:29:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 23:32:15 | INFO | train_inner | epoch 031:     34 / 393 loss=6.302, ppl=78.92, wps=14492.4, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.596, loss_scale=8, train_wall=439, gb_free=10.1, wall=52973
2022-03-02 23:39:41 | INFO | train_inner | epoch 031:    134 / 393 loss=6.224, ppl=74.73, wps=14694.4, ups=0.22, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.599, loss_scale=8, train_wall=441, gb_free=10.1, wall=53419
2022-03-02 23:47:06 | INFO | train_inner | epoch 031:    234 / 393 loss=6.275, ppl=77.41, wps=14706.6, ups=0.22, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.586, loss_scale=8, train_wall=441, gb_free=10.1, wall=53865
2022-03-02 23:54:32 | INFO | train_inner | epoch 031:    334 / 393 loss=6.31, ppl=79.37, wps=14707, ups=0.22, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.604, loss_scale=8, train_wall=441, gb_free=10.1, wall=54311
2022-03-02 23:58:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 23:58:59 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.735 | ppl 213.05 | wps 34023.1 | wpb 2034.1 | bsz 4 | num_updates 12159 | best_loss 7.586
2022-03-02 23:58:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12159 updates
2022-03-02 23:58:59 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-02 23:58:59 | INFO | train | epoch 031 | loss 6.272 | ppl 77.28 | wps 14648.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 12159 | lr 0.000286781 | gnorm 0.597 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 54578
2022-03-02 23:58:59 | INFO | fairseq.trainer | begin training epoch 32
2022-03-02 23:58:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 00:02:02 | INFO | train_inner | epoch 032:     41 / 393 loss=6.259, ppl=76.59, wps=14495.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.601, loss_scale=16, train_wall=439, gb_free=10.1, wall=54761
2022-03-03 00:02:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 00:09:32 | INFO | train_inner | epoch 032:    142 / 393 loss=6.194, ppl=73.23, wps=14561.8, ups=0.22, wpb=65530.2, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.587, loss_scale=8, train_wall=445, gb_free=10.1, wall=55211
2022-03-03 00:16:58 | INFO | train_inner | epoch 032:    242 / 393 loss=6.253, ppl=76.27, wps=14703.9, ups=0.22, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.602, loss_scale=8, train_wall=441, gb_free=10.1, wall=55657
2022-03-03 00:24:24 | INFO | train_inner | epoch 032:    342 / 393 loss=6.284, ppl=77.93, wps=14687.7, ups=0.22, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.584, loss_scale=8, train_wall=441, gb_free=10.1, wall=56103
2022-03-03 00:28:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 00:28:16 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.755 | ppl 216.01 | wps 34166.5 | wpb 2034.1 | bsz 4 | num_updates 12551 | best_loss 7.586
2022-03-03 00:28:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12551 updates
2022-03-03 00:28:16 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-03 00:28:16 | INFO | train | epoch 032 | loss 6.245 | ppl 75.86 | wps 14609.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12551 | lr 0.000282267 | gnorm 0.592 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 56335
2022-03-03 00:28:16 | INFO | fairseq.trainer | begin training epoch 33
2022-03-03 00:28:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 00:31:54 | INFO | train_inner | epoch 033:     49 / 393 loss=6.238, ppl=75.5, wps=14492.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.599, loss_scale=8, train_wall=439, gb_free=10.1, wall=56553
2022-03-03 00:39:20 | INFO | train_inner | epoch 033:    149 / 393 loss=6.183, ppl=72.67, wps=14711.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.606, loss_scale=8, train_wall=441, gb_free=10.1, wall=56998
2022-03-03 00:41:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 00:46:50 | INFO | train_inner | epoch 033:    250 / 393 loss=6.217, ppl=74.41, wps=14557, ups=0.22, wpb=65530.9, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.605, loss_scale=8, train_wall=445, gb_free=10.1, wall=57449
2022-03-03 00:54:15 | INFO | train_inner | epoch 033:    350 / 393 loss=6.263, ppl=76.78, wps=14703.2, ups=0.22, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.609, loss_scale=8, train_wall=441, gb_free=10.1, wall=57894
2022-03-03 00:57:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 00:57:32 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.754 | ppl 215.81 | wps 34065.3 | wpb 2034.1 | bsz 4 | num_updates 12943 | best_loss 7.586
2022-03-03 00:57:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12943 updates
2022-03-03 00:57:32 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-03 00:57:32 | INFO | train | epoch 033 | loss 6.221 | ppl 74.58 | wps 14614.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12943 | lr 0.00027796 | gnorm 0.608 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 58090
2022-03-03 00:57:32 | INFO | fairseq.trainer | begin training epoch 34
2022-03-03 00:57:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 01:01:46 | INFO | train_inner | epoch 034:     57 / 393 loss=6.196, ppl=73.33, wps=14496.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.621, loss_scale=8, train_wall=439, gb_free=10.1, wall=58344
2022-03-03 01:09:11 | INFO | train_inner | epoch 034:    157 / 393 loss=6.15, ppl=71.03, wps=14698.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.6, loss_scale=8, train_wall=441, gb_free=10.1, wall=58790
2022-03-03 01:16:37 | INFO | train_inner | epoch 034:    257 / 393 loss=6.206, ppl=73.83, wps=14705.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.602, loss_scale=8, train_wall=441, gb_free=10.1, wall=59236
2022-03-03 01:24:03 | INFO | train_inner | epoch 034:    357 / 393 loss=6.248, ppl=75.98, wps=14702.3, ups=0.22, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.621, loss_scale=16, train_wall=441, gb_free=10.1, wall=59682
2022-03-03 01:26:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 01:26:48 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.776 | ppl 219.24 | wps 34046.5 | wpb 2034.1 | bsz 4 | num_updates 13336 | best_loss 7.586
2022-03-03 01:26:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13336 updates
2022-03-03 01:26:48 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-03 01:26:48 | INFO | train | epoch 034 | loss 6.197 | ppl 73.34 | wps 14648.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 13336 | lr 0.000273834 | gnorm 0.609 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 59847
2022-03-03 01:26:48 | INFO | fairseq.trainer | begin training epoch 35
2022-03-03 01:26:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 01:27:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 01:31:38 | INFO | train_inner | epoch 035:     65 / 393 loss=6.168, ppl=71.89, wps=14344.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.609, loss_scale=8, train_wall=443, gb_free=10.1, wall=60137
2022-03-03 01:39:03 | INFO | train_inner | epoch 035:    165 / 393 loss=6.134, ppl=70.23, wps=14701.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.604, loss_scale=8, train_wall=441, gb_free=10.1, wall=60582
2022-03-03 01:46:29 | INFO | train_inner | epoch 035:    265 / 393 loss=6.188, ppl=72.9, wps=14702.2, ups=0.22, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.626, loss_scale=8, train_wall=441, gb_free=10.1, wall=61028
2022-03-03 01:53:55 | INFO | train_inner | epoch 035:    365 / 393 loss=6.221, ppl=74.62, wps=14697.1, ups=0.22, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.622, loss_scale=8, train_wall=441, gb_free=10.1, wall=61474
2022-03-03 01:55:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 01:56:04 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.78 | ppl 219.73 | wps 34101.3 | wpb 2034.1 | bsz 4 | num_updates 13728 | best_loss 7.586
2022-03-03 01:56:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13728 updates
2022-03-03 01:56:04 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-03 01:56:04 | INFO | train | epoch 035 | loss 6.173 | ppl 72.16 | wps 14608.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 13728 | lr 0.000269896 | gnorm 0.617 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 61603
2022-03-03 01:56:04 | INFO | fairseq.trainer | begin training epoch 36
2022-03-03 01:56:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 02:01:26 | INFO | train_inner | epoch 036:     72 / 393 loss=6.129, ppl=69.96, wps=14483.3, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.63, loss_scale=8, train_wall=439, gb_free=10.1, wall=61924
2022-03-03 02:06:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 02:08:56 | INFO | train_inner | epoch 036:    173 / 393 loss=6.124, ppl=69.75, wps=14549.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.612, loss_scale=8, train_wall=445, gb_free=10.1, wall=62375
2022-03-03 02:16:21 | INFO | train_inner | epoch 036:    273 / 393 loss=6.164, ppl=71.7, wps=14709.3, ups=0.22, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.633, loss_scale=8, train_wall=441, gb_free=10.1, wall=62820
2022-03-03 02:23:47 | INFO | train_inner | epoch 036:    373 / 393 loss=6.202, ppl=73.59, wps=14715.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.622, loss_scale=8, train_wall=441, gb_free=10.1, wall=63266
2022-03-03 02:25:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 02:25:20 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.811 | ppl 224.57 | wps 34059.1 | wpb 2034.1 | bsz 4 | num_updates 14120 | best_loss 7.586
2022-03-03 02:25:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14120 updates
2022-03-03 02:25:20 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-03 02:25:20 | INFO | train | epoch 036 | loss 6.151 | ppl 71.08 | wps 14612.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14120 | lr 0.000266123 | gnorm 0.625 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 63359
2022-03-03 02:25:20 | INFO | fairseq.trainer | begin training epoch 37
2022-03-03 02:25:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 02:31:17 | INFO | train_inner | epoch 037:     80 / 393 loss=6.083, ppl=67.8, wps=14487.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.626, loss_scale=8, train_wall=439, gb_free=10.1, wall=63716
2022-03-03 02:38:43 | INFO | train_inner | epoch 037:    180 / 393 loss=6.105, ppl=68.82, wps=14708.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.638, loss_scale=8, train_wall=441, gb_free=10.1, wall=64162
2022-03-03 02:46:08 | INFO | train_inner | epoch 037:    280 / 393 loss=6.149, ppl=70.99, wps=14703.2, ups=0.22, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.638, loss_scale=16, train_wall=441, gb_free=10.1, wall=64607
2022-03-03 02:46:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 02:53:39 | INFO | train_inner | epoch 037:    381 / 393 loss=6.197, ppl=73.34, wps=14556.4, ups=0.22, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.638, loss_scale=8, train_wall=445, gb_free=10.1, wall=65058
2022-03-03 02:54:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 02:54:37 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.812 | ppl 224.7 | wps 34070.3 | wpb 2034.1 | bsz 4 | num_updates 14512 | best_loss 7.586
2022-03-03 02:54:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14512 updates
2022-03-03 02:54:37 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-03 02:54:37 | INFO | train | epoch 037 | loss 6.131 | ppl 70.06 | wps 14611.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14512 | lr 0.000262504 | gnorm 0.635 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 65116
2022-03-03 02:54:37 | INFO | fairseq.trainer | begin training epoch 38
2022-03-03 02:54:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 03:01:09 | INFO | train_inner | epoch 038:     88 / 393 loss=6.06, ppl=66.73, wps=14490.4, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.613, loss_scale=8, train_wall=439, gb_free=10.1, wall=65508
2022-03-03 03:08:35 | INFO | train_inner | epoch 038:    188 / 393 loss=6.083, ppl=67.79, wps=14696.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.639, loss_scale=8, train_wall=441, gb_free=10.1, wall=65954
2022-03-03 03:16:01 | INFO | train_inner | epoch 038:    288 / 393 loss=6.131, ppl=70.1, wps=14697.1, ups=0.22, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.639, loss_scale=8, train_wall=441, gb_free=10.1, wall=66400
2022-03-03 03:23:27 | INFO | train_inner | epoch 038:    388 / 393 loss=6.176, ppl=72.32, wps=14697, ups=0.22, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.65, loss_scale=8, train_wall=441, gb_free=10.1, wall=66846
2022-03-03 03:23:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 03:23:53 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.83 | ppl 227.47 | wps 33958.3 | wpb 2034.1 | bsz 4 | num_updates 14905 | best_loss 7.586
2022-03-03 03:23:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14905 updates
2022-03-03 03:23:54 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-03 03:23:54 | INFO | train | epoch 038 | loss 6.111 | ppl 69.14 | wps 14643.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 14905 | lr 0.00025902 | gnorm 0.635 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 66872
2022-03-03 03:23:54 | INFO | fairseq.trainer | begin training epoch 39
2022-03-03 03:23:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 03:30:58 | INFO | train_inner | epoch 039:     95 / 393 loss=6.023, ppl=65.05, wps=14464, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.612, loss_scale=16, train_wall=440, gb_free=10.1, wall=67297
2022-03-03 03:31:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 03:38:28 | INFO | train_inner | epoch 039:    196 / 393 loss=6.076, ppl=67.45, wps=14565.2, ups=0.22, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.634, loss_scale=8, train_wall=445, gb_free=10.1, wall=67747
2022-03-03 03:45:53 | INFO | train_inner | epoch 039:    296 / 393 loss=6.122, ppl=69.67, wps=14714.8, ups=0.22, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.638, loss_scale=8, train_wall=440, gb_free=10.1, wall=68192
2022-03-03 03:53:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 03:53:10 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.832 | ppl 227.88 | wps 34169.8 | wpb 2034.1 | bsz 4 | num_updates 15297 | best_loss 7.586
2022-03-03 03:53:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15297 updates
2022-03-03 03:53:10 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-03 03:53:10 | INFO | train | epoch 039 | loss 6.092 | ppl 68.21 | wps 14609.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 15297 | lr 0.00025568 | gnorm 0.633 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 68629
2022-03-03 03:53:10 | INFO | fairseq.trainer | begin training epoch 40
2022-03-03 03:53:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 03:53:23 | INFO | train_inner | epoch 040:      3 / 393 loss=6.148, ppl=70.92, wps=14492.1, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.652, loss_scale=8, train_wall=439, gb_free=10.1, wall=68642
2022-03-03 04:00:49 | INFO | train_inner | epoch 040:    103 / 393 loss=6.004, ppl=64.19, wps=14706.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.629, loss_scale=8, train_wall=441, gb_free=10.1, wall=69088
2022-03-03 04:08:15 | INFO | train_inner | epoch 040:    203 / 393 loss=6.053, ppl=66.39, wps=14708.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.64, loss_scale=8, train_wall=441, gb_free=10.1, wall=69533
2022-03-03 04:11:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 04:15:45 | INFO | train_inner | epoch 040:    304 / 393 loss=6.102, ppl=68.67, wps=14558.7, ups=0.22, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.66, loss_scale=8, train_wall=445, gb_free=10.1, wall=69984
2022-03-03 04:22:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 04:22:26 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.863 | ppl 232.83 | wps 33976.3 | wpb 2034.1 | bsz 4 | num_updates 15689 | best_loss 7.586
2022-03-03 04:22:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15689 updates
2022-03-03 04:22:26 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-03 04:22:26 | INFO | train | epoch 040 | loss 6.074 | ppl 67.37 | wps 14612.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 15689 | lr 0.000252466 | gnorm 0.645 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 70385
2022-03-03 04:22:26 | INFO | fairseq.trainer | begin training epoch 41
2022-03-03 04:22:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 04:23:15 | INFO | train_inner | epoch 041:     11 / 393 loss=6.125, ppl=69.81, wps=14484, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.654, loss_scale=8, train_wall=439, gb_free=10.1, wall=70434
2022-03-03 04:30:41 | INFO | train_inner | epoch 041:    111 / 393 loss=5.988, ppl=63.48, wps=14699.2, ups=0.22, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.661, loss_scale=8, train_wall=441, gb_free=10.1, wall=70880
2022-03-03 04:38:07 | INFO | train_inner | epoch 041:    211 / 393 loss=6.046, ppl=66.06, wps=14695.7, ups=0.22, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.646, loss_scale=8, train_wall=441, gb_free=10.1, wall=71326
2022-03-03 04:45:33 | INFO | train_inner | epoch 041:    311 / 393 loss=6.084, ppl=67.82, wps=14701.7, ups=0.22, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.661, loss_scale=8, train_wall=441, gb_free=10.1, wall=71772
2022-03-03 04:51:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 04:51:43 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.859 | ppl 232.11 | wps 34084.3 | wpb 2034.1 | bsz 4 | num_updates 16082 | best_loss 7.586
2022-03-03 04:51:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16082 updates
2022-03-03 04:51:43 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-03 04:51:43 | INFO | train | epoch 041 | loss 6.056 | ppl 66.55 | wps 14643.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 16082 | lr 0.000249362 | gnorm 0.655 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 72142
2022-03-03 04:51:43 | INFO | fairseq.trainer | begin training epoch 42
2022-03-03 04:51:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 04:53:03 | INFO | train_inner | epoch 042:     18 / 393 loss=6.104, ppl=68.78, wps=14482.2, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.648, loss_scale=16, train_wall=439, gb_free=10.1, wall=72222
2022-03-03 04:55:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 05:00:34 | INFO | train_inner | epoch 042:    119 / 393 loss=5.984, ppl=63.31, wps=14552.6, ups=0.22, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.638, loss_scale=8, train_wall=445, gb_free=10.1, wall=72673
2022-03-03 05:07:59 | INFO | train_inner | epoch 042:    219 / 393 loss=6.03, ppl=65.35, wps=14704.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.645, loss_scale=8, train_wall=441, gb_free=10.1, wall=73118
2022-03-03 05:15:25 | INFO | train_inner | epoch 042:    319 / 393 loss=6.073, ppl=67.31, wps=14701.6, ups=0.22, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.662, loss_scale=8, train_wall=441, gb_free=10.1, wall=73564
2022-03-03 05:20:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 05:21:00 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.874 | ppl 234.52 | wps 34040.9 | wpb 2034.1 | bsz 4 | num_updates 16474 | best_loss 7.586
2022-03-03 05:21:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16474 updates
2022-03-03 05:21:00 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-03 05:21:00 | INFO | train | epoch 042 | loss 6.04 | ppl 65.81 | wps 14607.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16474 | lr 0.000246377 | gnorm 0.647 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 73898
2022-03-03 05:21:00 | INFO | fairseq.trainer | begin training epoch 43
2022-03-03 05:21:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 05:22:56 | INFO | train_inner | epoch 043:     26 / 393 loss=6.058, ppl=66.65, wps=14483.4, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.641, loss_scale=8, train_wall=439, gb_free=10.1, wall=74014
2022-03-03 05:30:21 | INFO | train_inner | epoch 043:    126 / 393 loss=5.969, ppl=62.64, wps=14711.8, ups=0.22, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.654, loss_scale=8, train_wall=441, gb_free=10.1, wall=74460
2022-03-03 05:36:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 05:37:51 | INFO | train_inner | epoch 043:    227 / 393 loss=6.015, ppl=64.68, wps=14566.2, ups=0.22, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.646, loss_scale=8, train_wall=445, gb_free=10.1, wall=74910
2022-03-03 05:45:17 | INFO | train_inner | epoch 043:    327 / 393 loss=6.068, ppl=67.08, wps=14703, ups=0.22, wpb=65530.2, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.66, loss_scale=8, train_wall=441, gb_free=10.1, wall=75356
2022-03-03 05:50:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 05:50:15 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.921 | ppl 242.31 | wps 34170.5 | wpb 2034.1 | bsz 4 | num_updates 16866 | best_loss 7.586
2022-03-03 05:50:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16866 updates
2022-03-03 05:50:15 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-03 05:50:15 | INFO | train | epoch 043 | loss 6.024 | ppl 65.09 | wps 14615.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16866 | lr 0.000243497 | gnorm 0.655 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 75654
2022-03-03 05:50:15 | INFO | fairseq.trainer | begin training epoch 44
2022-03-03 05:50:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 05:52:47 | INFO | train_inner | epoch 044:     34 / 393 loss=6.033, ppl=65.48, wps=14491.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.666, loss_scale=8, train_wall=439, gb_free=10.1, wall=75806
2022-03-03 06:00:13 | INFO | train_inner | epoch 044:    134 / 393 loss=5.952, ppl=61.89, wps=14701.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.663, loss_scale=8, train_wall=441, gb_free=10.1, wall=76252
2022-03-03 06:07:38 | INFO | train_inner | epoch 044:    234 / 393 loss=6.006, ppl=64.26, wps=14704.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.665, loss_scale=8, train_wall=441, gb_free=10.1, wall=76697
2022-03-03 06:15:04 | INFO | train_inner | epoch 044:    334 / 393 loss=6.048, ppl=66.15, wps=14695.8, ups=0.22, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.662, loss_scale=16, train_wall=441, gb_free=10.1, wall=77143
2022-03-03 06:15:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 06:19:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 06:19:32 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.902 | ppl 239.24 | wps 33943.9 | wpb 2034.1 | bsz 4 | num_updates 17258 | best_loss 7.586
2022-03-03 06:19:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17258 updates
2022-03-03 06:19:32 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-03 06:19:32 | INFO | train | epoch 044 | loss 6.008 | ppl 64.37 | wps 14608.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 17258 | lr 0.000240716 | gnorm 0.667 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 77411
2022-03-03 06:19:32 | INFO | fairseq.trainer | begin training epoch 45
2022-03-03 06:19:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 06:22:39 | INFO | train_inner | epoch 045:     42 / 393 loss=6.017, ppl=64.76, wps=14343.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.681, loss_scale=8, train_wall=443, gb_free=10.1, wall=77598
2022-03-03 06:30:05 | INFO | train_inner | epoch 045:    142 / 393 loss=5.952, ppl=61.91, wps=14705.7, ups=0.22, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.655, loss_scale=8, train_wall=441, gb_free=10.1, wall=78044
2022-03-03 06:37:31 | INFO | train_inner | epoch 045:    242 / 393 loss=5.991, ppl=63.61, wps=14700.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.666, loss_scale=8, train_wall=441, gb_free=10.1, wall=78490
2022-03-03 06:44:56 | INFO | train_inner | epoch 045:    342 / 393 loss=6.04, ppl=65.81, wps=14701.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.667, loss_scale=8, train_wall=441, gb_free=10.1, wall=78935
2022-03-03 06:48:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 06:48:48 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.917 | ppl 241.62 | wps 34052.8 | wpb 2034.1 | bsz 4 | num_updates 17651 | best_loss 7.586
2022-03-03 06:48:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17651 updates
2022-03-03 06:48:48 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-03 06:48:48 | INFO | train | epoch 045 | loss 5.995 | ppl 63.77 | wps 14648.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 17651 | lr 0.000238021 | gnorm 0.665 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 79167
2022-03-03 06:48:48 | INFO | fairseq.trainer | begin training epoch 46
2022-03-03 06:48:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 06:52:27 | INFO | train_inner | epoch 046:     49 / 393 loss=5.989, ppl=63.51, wps=14490.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.671, loss_scale=8, train_wall=439, gb_free=10.1, wall=79386
2022-03-03 06:53:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 06:59:57 | INFO | train_inner | epoch 046:    150 / 393 loss=5.932, ppl=61.04, wps=14558.4, ups=0.22, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.683, loss_scale=8, train_wall=445, gb_free=10.1, wall=79836
2022-03-03 07:07:23 | INFO | train_inner | epoch 046:    250 / 393 loss=5.982, ppl=63.19, wps=14703.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.673, loss_scale=8, train_wall=441, gb_free=10.1, wall=80281
2022-03-03 07:14:48 | INFO | train_inner | epoch 046:    350 / 393 loss=6.031, ppl=65.41, wps=14709.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.662, loss_scale=8, train_wall=441, gb_free=10.1, wall=80727
2022-03-03 07:17:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 07:18:04 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.912 | ppl 240.85 | wps 33956.5 | wpb 2034.1 | bsz 4 | num_updates 18043 | best_loss 7.586
2022-03-03 07:18:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18043 updates
2022-03-03 07:18:04 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-03 07:18:04 | INFO | train | epoch 046 | loss 5.98 | ppl 63.13 | wps 14612.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18043 | lr 0.000235421 | gnorm 0.672 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 80923
2022-03-03 07:18:04 | INFO | fairseq.trainer | begin training epoch 47
2022-03-03 07:18:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 07:22:18 | INFO | train_inner | epoch 047:     57 / 393 loss=5.959, ppl=62.22, wps=14496.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.673, loss_scale=8, train_wall=439, gb_free=10.1, wall=81177
2022-03-03 07:29:44 | INFO | train_inner | epoch 047:    157 / 393 loss=5.922, ppl=60.64, wps=14705.6, ups=0.22, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.68, loss_scale=8, train_wall=441, gb_free=10.1, wall=81623
2022-03-03 07:31:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 07:37:14 | INFO | train_inner | epoch 047:    258 / 393 loss=5.975, ppl=62.89, wps=14560.2, ups=0.22, wpb=65530.2, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.672, loss_scale=8, train_wall=445, gb_free=10.1, wall=82073
2022-03-03 07:44:40 | INFO | train_inner | epoch 047:    358 / 393 loss=6.022, ppl=64.97, wps=14698.3, ups=0.22, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.684, loss_scale=8, train_wall=441, gb_free=10.1, wall=82519
2022-03-03 07:47:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 07:47:20 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.915 | ppl 241.27 | wps 34111.8 | wpb 2034.1 | bsz 4 | num_updates 18435 | best_loss 7.586
2022-03-03 07:47:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18435 updates
2022-03-03 07:47:20 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-03 07:47:20 | INFO | train | epoch 047 | loss 5.967 | ppl 62.56 | wps 14611.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18435 | lr 0.000232905 | gnorm 0.678 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 82679
2022-03-03 07:47:20 | INFO | fairseq.trainer | begin training epoch 48
2022-03-03 07:47:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 07:52:10 | INFO | train_inner | epoch 048:     65 / 393 loss=5.935, ppl=61.16, wps=14484.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.686, loss_scale=8, train_wall=439, gb_free=10.1, wall=82969
2022-03-03 07:59:36 | INFO | train_inner | epoch 048:    165 / 393 loss=5.921, ppl=60.61, wps=14710.7, ups=0.22, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.665, loss_scale=8, train_wall=441, gb_free=10.1, wall=83415
2022-03-03 08:07:01 | INFO | train_inner | epoch 048:    265 / 393 loss=5.963, ppl=62.37, wps=14700.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.678, loss_scale=8, train_wall=441, gb_free=10.1, wall=83860
2022-03-03 08:10:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 08:14:32 | INFO | train_inner | epoch 048:    366 / 393 loss=6.013, ppl=64.56, wps=14548.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.697, loss_scale=8, train_wall=445, gb_free=10.1, wall=84311
2022-03-03 08:16:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 08:16:37 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.944 | ppl 246.2 | wps 34144.9 | wpb 2034.1 | bsz 4 | num_updates 18827 | best_loss 7.586
2022-03-03 08:16:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18827 updates
2022-03-03 08:16:37 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-03 08:16:37 | INFO | train | epoch 048 | loss 5.954 | ppl 62.01 | wps 14610.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18827 | lr 0.000230467 | gnorm 0.682 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 84436
2022-03-03 08:16:37 | INFO | fairseq.trainer | begin training epoch 49
2022-03-03 08:16:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 08:22:02 | INFO | train_inner | epoch 049:     73 / 393 loss=5.91, ppl=60.13, wps=14492, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.671, loss_scale=8, train_wall=439, gb_free=10.1, wall=84761
2022-03-03 08:29:28 | INFO | train_inner | epoch 049:    173 / 393 loss=5.91, ppl=60.15, wps=14698.9, ups=0.22, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.7, loss_scale=8, train_wall=441, gb_free=10.1, wall=85207
2022-03-03 08:36:54 | INFO | train_inner | epoch 049:    273 / 393 loss=5.956, ppl=62.06, wps=14694.5, ups=0.22, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.69, loss_scale=8, train_wall=441, gb_free=10.1, wall=85653
2022-03-03 08:44:20 | INFO | train_inner | epoch 049:    373 / 393 loss=5.999, ppl=63.96, wps=14694.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.685, loss_scale=8, train_wall=441, gb_free=10.1, wall=86099
2022-03-03 08:45:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 08:45:54 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.958 | ppl 248.62 | wps 33880.6 | wpb 2034.1 | bsz 4 | num_updates 19220 | best_loss 7.586
2022-03-03 08:45:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19220 updates
2022-03-03 08:45:54 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-03 08:45:54 | INFO | train | epoch 049 | loss 5.942 | ppl 61.5 | wps 14640.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 19220 | lr 0.000228099 | gnorm 0.687 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 86193
2022-03-03 08:45:54 | INFO | fairseq.trainer | begin training epoch 50
2022-03-03 08:45:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 08:51:51 | INFO | train_inner | epoch 050:     80 / 393 loss=5.89, ppl=59.29, wps=14472, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=19300, lr=0.000227626, gnorm=0.688, loss_scale=16, train_wall=439, gb_free=10.1, wall=86550
2022-03-03 08:53:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 08:59:21 | INFO | train_inner | epoch 050:    181 / 393 loss=5.891, ppl=59.33, wps=14547.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.685, loss_scale=8, train_wall=445, gb_free=10.1, wall=87000
2022-03-03 09:06:47 | INFO | train_inner | epoch 050:    281 / 393 loss=5.949, ppl=61.77, wps=14699.2, ups=0.22, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.689, loss_scale=8, train_wall=441, gb_free=10.1, wall=87446
2022-03-03 09:14:13 | INFO | train_inner | epoch 050:    381 / 393 loss=6, ppl=64, wps=14698.8, ups=0.22, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.695, loss_scale=8, train_wall=441, gb_free=10.1, wall=87892
2022-03-03 09:15:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 09:15:11 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.941 | ppl 245.77 | wps 33972.2 | wpb 2034.1 | bsz 4 | num_updates 19612 | best_loss 7.586
2022-03-03 09:15:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19612 updates
2022-03-03 09:15:11 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-03 09:15:11 | INFO | train | epoch 050 | loss 5.93 | ppl 60.97 | wps 14603.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 19612 | lr 0.000225808 | gnorm 0.69 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 87950
2022-03-03 09:15:11 | INFO | fairseq.trainer | begin training epoch 51
2022-03-03 09:15:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 09:21:44 | INFO | train_inner | epoch 051:     88 / 393 loss=5.864, ppl=58.26, wps=14478.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.689, loss_scale=8, train_wall=439, gb_free=10.1, wall=88343
2022-03-03 09:29:09 | INFO | train_inner | epoch 051:    188 / 393 loss=5.892, ppl=59.4, wps=14702.8, ups=0.22, wpb=65530.2, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.686, loss_scale=8, train_wall=441, gb_free=10.1, wall=88788
2022-03-03 09:33:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 09:36:39 | INFO | train_inner | epoch 051:    289 / 393 loss=5.945, ppl=61.62, wps=14566.2, ups=0.22, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.702, loss_scale=8, train_wall=445, gb_free=10.1, wall=89238
2022-03-03 09:44:05 | INFO | train_inner | epoch 051:    389 / 393 loss=5.98, ppl=63.13, wps=14694.1, ups=0.22, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.706, loss_scale=8, train_wall=441, gb_free=10.1, wall=89684
2022-03-03 09:44:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 09:44:28 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.968 | ppl 250.4 | wps 34129.8 | wpb 2034.1 | bsz 4 | num_updates 20004 | best_loss 7.586
2022-03-03 09:44:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20004 updates
2022-03-03 09:44:28 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-03 09:44:28 | INFO | train | epoch 051 | loss 5.919 | ppl 60.49 | wps 14608.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20004 | lr 0.000223584 | gnorm 0.696 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 89707
2022-03-03 09:44:28 | INFO | fairseq.trainer | begin training epoch 52
2022-03-03 09:44:28 | INFO | fairseq_cli.train | Start iterating over samples
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
