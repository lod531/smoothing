Sender: LSF System <lsfadmin@eu-g3-062>
Subject: Job 207131843: <w103_size_0.125_fp16_label_smoothing_0.01_#1> in cluster <euler> Exited

Job <w103_size_0.125_fp16_label_smoothing_0.01_#1> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:19:34 2022
Job was executed on host(s) <eu-g3-062>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 09:19:41 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 09:19:41 2022
Terminated at Sat Mar  5 11:25:21 2022
Results reported at Sat Mar  5 11:25:21 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.01 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   93791.32 sec.
    Max Memory :                                 6741 MB
    Average Memory :                             3448.68 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               13259.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   93940 sec.
    Turnaround time :                            93947 sec.

The output (if any) follows:

2022-03-04 09:19:48 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.01, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 09:19:48 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-04 09:19:51 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-04 09:19:51 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 09:19:51 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 09:19:51 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-04 09:19:51 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-04 09:19:51 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 09:19:51 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-04 09:19:53 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 09:19:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:19:53 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-04 09:19:53 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:19:53 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 09:19:53 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 09:19:53 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 09:19:53 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 09:19:53 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 09:19:54 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-04 09:19:54 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 09:19:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:20:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 09:20:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:20:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 09:20:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:22:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 09:25:40 | INFO | train_inner | epoch 001:    105 / 196 loss=16.37, nll_loss=16.35, ppl=83513.8, wps=20854.9, ups=0.32, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.513, loss_scale=4, train_wall=322, gb_free=19.9, wall=346
2022-03-04 09:30:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:30:27 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.063 | nll_loss 13.008 | ppl 8237.28 | wps 41305.6 | wpb 510.9 | bsz 1 | num_updates 191
2022-03-04 09:30:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-04 09:30:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 09:30:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 09:30:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.063) (writing took 6.76974489307031 seconds)
2022-03-04 09:30:34 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 09:30:34 | INFO | train | epoch 001 | loss 15.277 | nll_loss 15.246 | ppl 38866.2 | wps 20530.6 | ups 0.31 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.578 | loss_scale 8 | train_wall 584 | gb_free 19.9 | wall 641
2022-03-04 09:30:34 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 09:30:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:31:02 | INFO | train_inner | epoch 002:      9 / 196 loss=13.982, nll_loss=13.938, ppl=15694.5, wps=20257.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.526, loss_scale=8, train_wall=288, gb_free=19.9, wall=669
2022-03-04 09:36:14 | INFO | train_inner | epoch 002:    109 / 196 loss=12.03, nll_loss=11.962, ppl=3990.31, wps=21018.8, ups=0.32, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.951, loss_scale=16, train_wall=289, gb_free=19.9, wall=981
2022-03-04 09:40:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:40:49 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.413 | nll_loss 10.316 | ppl 1274.81 | wps 41096.4 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 10.413
2022-03-04 09:40:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-04 09:40:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 09:40:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 09:40:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 2 @ 387 updates, score 10.413) (writing took 6.79407501895912 seconds)
2022-03-04 09:40:56 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 09:40:56 | INFO | train | epoch 002 | loss 11.515 | nll_loss 11.439 | ppl 2775.7 | wps 20624.2 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.811 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 1263
2022-03-04 09:40:56 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 09:40:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:41:37 | INFO | train_inner | epoch 003:     13 / 196 loss=10.71, nll_loss=10.62, ppl=1574.25, wps=20267.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.584, loss_scale=16, train_wall=288, gb_free=19.9, wall=1303
2022-03-04 09:46:48 | INFO | train_inner | epoch 003:    113 / 196 loss=10.13, nll_loss=10.025, ppl=1041.63, wps=21055.2, ups=0.32, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.501, loss_scale=32, train_wall=289, gb_free=19.9, wall=1614
2022-03-04 09:50:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:51:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:51:11 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.689 | nll_loss 9.573 | ppl 761.52 | wps 41647.8 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 9.689
2022-03-04 09:51:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 582 updates
2022-03-04 09:51:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 09:51:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 09:51:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 3 @ 582 updates, score 9.689) (writing took 6.8185687970835716 seconds)
2022-03-04 09:51:17 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 09:51:17 | INFO | train | epoch 003 | loss 10.019 | nll_loss 9.911 | ppl 962.83 | wps 20539.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 582 | lr 7.28355e-05 | gnorm 0.537 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 1884
2022-03-04 09:51:18 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 09:51:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:52:14 | INFO | train_inner | epoch 004:     18 / 196 loss=9.795, nll_loss=9.681, ppl=820.95, wps=20069.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.613, loss_scale=32, train_wall=291, gb_free=19.9, wall=1940
2022-03-04 09:57:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:57:27 | INFO | train_inner | epoch 004:    119 / 196 loss=9.503, nll_loss=9.383, ppl=667.87, wps=20877.5, ups=0.32, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.625, loss_scale=32, train_wall=291, gb_free=19.9, wall=2254
2022-03-04 10:01:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:01:31 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.206 | nll_loss 9.081 | ppl 541.75 | wps 41418.2 | wpb 510.9 | bsz 1 | num_updates 777 | best_loss 9.206
2022-03-04 10:01:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 777 updates
2022-03-04 10:01:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:01:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:01:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 4 @ 777 updates, score 9.206) (writing took 6.850057055940852 seconds)
2022-03-04 10:01:38 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 10:01:38 | INFO | train | epoch 004 | loss 9.439 | nll_loss 9.319 | ppl 638.53 | wps 20563.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 777 | lr 9.72056e-05 | gnorm 0.682 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 2505
2022-03-04 10:01:38 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 10:01:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:02:50 | INFO | train_inner | epoch 005:     23 / 196 loss=9.273, nll_loss=9.149, ppl=567.51, wps=20283.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.767, loss_scale=32, train_wall=288, gb_free=19.9, wall=2576
2022-03-04 10:04:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:08:04 | INFO | train_inner | epoch 005:    124 / 196 loss=9.019, nll_loss=8.889, ppl=474.24, wps=20875.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.853, loss_scale=32, train_wall=291, gb_free=19.9, wall=2890
2022-03-04 10:11:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:11:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:11:53 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.8 | nll_loss 8.666 | ppl 406.22 | wps 41619.4 | wpb 510.9 | bsz 1 | num_updates 971 | best_loss 8.8
2022-03-04 10:11:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 971 updates
2022-03-04 10:11:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:11:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:11:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 5 @ 971 updates, score 8.8) (writing took 6.862698230193928 seconds)
2022-03-04 10:11:59 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 10:11:59 | INFO | train | epoch 005 | loss 8.977 | nll_loss 8.847 | ppl 460.59 | wps 20435.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 971 | lr 0.000121451 | gnorm 0.853 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 3126
2022-03-04 10:11:59 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 10:11:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:13:30 | INFO | train_inner | epoch 006:     29 / 196 loss=8.819, nll_loss=8.686, ppl=411.94, wps=20046.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.904, loss_scale=32, train_wall=291, gb_free=19.9, wall=3216
2022-03-04 10:18:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:18:44 | INFO | train_inner | epoch 006:    130 / 196 loss=8.617, nll_loss=8.48, ppl=357.11, wps=20875.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.903, loss_scale=32, train_wall=291, gb_free=19.9, wall=3530
2022-03-04 10:22:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:22:13 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.461 | nll_loss 8.32 | ppl 319.6 | wps 41319.2 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 8.461
2022-03-04 10:22:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1166 updates
2022-03-04 10:22:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:22:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:22:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 6 @ 1166 updates, score 8.461) (writing took 7.120063564972952 seconds)
2022-03-04 10:22:20 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 10:22:20 | INFO | train | epoch 006 | loss 8.591 | nll_loss 8.454 | ppl 350.68 | wps 20555.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1166 | lr 0.000145821 | gnorm 0.93 | loss_scale 32 | train_wall 564 | gb_free 19.9 | wall 3747
2022-03-04 10:22:20 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 10:22:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:24:06 | INFO | train_inner | epoch 007:     34 / 196 loss=8.453, nll_loss=8.313, ppl=318.11, wps=20269.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.952, loss_scale=32, train_wall=288, gb_free=19.9, wall=3853
2022-03-04 10:25:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:29:20 | INFO | train_inner | epoch 007:    135 / 196 loss=8.288, nll_loss=8.145, ppl=283.11, wps=20869.7, ups=0.32, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.974, loss_scale=32, train_wall=291, gb_free=19.9, wall=4167
2022-03-04 10:32:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:32:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:32:34 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.2 | nll_loss 8.055 | ppl 265.87 | wps 41379.6 | wpb 510.9 | bsz 1 | num_updates 1360 | best_loss 8.2
2022-03-04 10:32:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1360 updates
2022-03-04 10:32:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:32:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:32:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 7 @ 1360 updates, score 8.2) (writing took 7.158035199856386 seconds)
2022-03-04 10:32:42 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 10:32:42 | INFO | train | epoch 007 | loss 8.268 | nll_loss 8.125 | ppl 279.12 | wps 20437.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1360 | lr 0.000170066 | gnorm 0.974 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 4368
2022-03-04 10:32:42 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 10:32:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:34:46 | INFO | train_inner | epoch 008:     40 / 196 loss=8.128, nll_loss=7.982, ppl=252.9, wps=20049.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.931, loss_scale=32, train_wall=291, gb_free=19.9, wall=4493
2022-03-04 10:39:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:40:00 | INFO | train_inner | epoch 008:    141 / 196 loss=7.987, nll_loss=7.838, ppl=228.84, wps=20857.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.983, loss_scale=32, train_wall=292, gb_free=19.9, wall=4807
2022-03-04 10:42:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:42:56 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.957 | nll_loss 7.805 | ppl 223.6 | wps 41138.4 | wpb 510.9 | bsz 1 | num_updates 1555 | best_loss 7.957
2022-03-04 10:42:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1555 updates
2022-03-04 10:42:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:42:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:43:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 8 @ 1555 updates, score 7.957) (writing took 6.883647774811834 seconds)
2022-03-04 10:43:03 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 10:43:03 | INFO | train | epoch 008 | loss 7.977 | nll_loss 7.828 | ppl 227.25 | wps 20534.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1555 | lr 0.000194436 | gnorm 0.953 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 4990
2022-03-04 10:43:03 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 10:43:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:45:23 | INFO | train_inner | epoch 009:     45 / 196 loss=7.839, nll_loss=7.687, ppl=206.13, wps=20252.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.95, loss_scale=32, train_wall=288, gb_free=19.9, wall=5130
2022-03-04 10:46:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:50:38 | INFO | train_inner | epoch 009:    146 / 196 loss=7.705, nll_loss=7.551, ppl=187.5, wps=20795.2, ups=0.32, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.983, loss_scale=32, train_wall=292, gb_free=19.9, wall=5445
2022-03-04 10:53:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:53:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:53:18 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.753 | nll_loss 7.596 | ppl 193.53 | wps 41472.5 | wpb 510.9 | bsz 1 | num_updates 1749 | best_loss 7.753
2022-03-04 10:53:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1749 updates
2022-03-04 10:53:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:53:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 10:53:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 9 @ 1749 updates, score 7.753) (writing took 7.182139551034197 seconds)
2022-03-04 10:53:25 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 10:53:25 | INFO | train | epoch 009 | loss 7.705 | nll_loss 7.551 | ppl 187.59 | wps 20398.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1749 | lr 0.000218681 | gnorm 0.97 | loss_scale 32 | train_wall 566 | gb_free 19.9 | wall 5612
2022-03-04 10:53:25 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 10:53:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:56:04 | INFO | train_inner | epoch 010:     51 / 196 loss=7.577, nll_loss=7.421, ppl=171.33, wps=20043.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.94, loss_scale=32, train_wall=291, gb_free=19.9, wall=5771
2022-03-04 11:01:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:01:19 | INFO | train_inner | epoch 010:    152 / 196 loss=7.453, nll_loss=7.295, ppl=156.99, wps=20847.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.957, loss_scale=32, train_wall=292, gb_free=19.9, wall=6085
2022-03-04 11:03:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:03:40 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.576 | nll_loss 7.414 | ppl 170.58 | wps 41204.5 | wpb 510.9 | bsz 1 | num_updates 1944 | best_loss 7.576
2022-03-04 11:03:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1944 updates
2022-03-04 11:03:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:03:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:03:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 10 @ 1944 updates, score 7.576) (writing took 7.32947396999225 seconds)
2022-03-04 11:03:48 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 11:03:48 | INFO | train | epoch 010 | loss 7.452 | nll_loss 7.294 | ppl 156.89 | wps 20513.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1944 | lr 0.000243051 | gnorm 0.94 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 6234
2022-03-04 11:03:48 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 11:03:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:06:42 | INFO | train_inner | epoch 011:     56 / 196 loss=7.312, nll_loss=7.151, ppl=142.11, wps=20228.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.938, loss_scale=32, train_wall=288, gb_free=19.9, wall=6408
2022-03-04 11:08:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:11:56 | INFO | train_inner | epoch 011:    157 / 196 loss=7.211, nll_loss=7.048, ppl=132.3, wps=20855.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.938, loss_scale=32, train_wall=292, gb_free=19.9, wall=6723
2022-03-04 11:13:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:14:02 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.424 | nll_loss 7.258 | ppl 153.06 | wps 41107.3 | wpb 510.9 | bsz 1 | num_updates 2139 | best_loss 7.424
2022-03-04 11:14:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2139 updates
2022-03-04 11:14:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:14:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:14:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 11 @ 2139 updates, score 7.424) (writing took 6.663555509177968 seconds)
2022-03-04 11:14:09 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 11:14:09 | INFO | train | epoch 011 | loss 7.214 | nll_loss 7.051 | ppl 132.63 | wps 20546.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2139 | lr 0.000267422 | gnorm 0.935 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 6855
2022-03-04 11:14:09 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 11:14:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:15:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:17:22 | INFO | train_inner | epoch 012:     62 / 196 loss=7.083, nll_loss=6.918, ppl=120.95, wps=20067.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.926, loss_scale=32, train_wall=291, gb_free=19.9, wall=7048
2022-03-04 11:21:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:22:36 | INFO | train_inner | epoch 012:    163 / 196 loss=6.984, nll_loss=6.817, ppl=112.77, wps=20857.2, ups=0.32, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.915, loss_scale=16, train_wall=291, gb_free=19.9, wall=7363
2022-03-04 11:24:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:24:24 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.308 | nll_loss 7.138 | ppl 140.86 | wps 41185.1 | wpb 510.9 | bsz 1 | num_updates 2333 | best_loss 7.308
2022-03-04 11:24:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2333 updates
2022-03-04 11:24:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:24:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:24:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 12 @ 2333 updates, score 7.308) (writing took 6.5972790280357 seconds)
2022-03-04 11:24:30 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 11:24:30 | INFO | train | epoch 012 | loss 6.994 | nll_loss 6.827 | ppl 113.52 | wps 20427.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 2333 | lr 0.000291667 | gnorm 0.914 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 7477
2022-03-04 11:24:30 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 11:24:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:27:59 | INFO | train_inner | epoch 013:     67 / 196 loss=6.853, nll_loss=6.683, ppl=102.76, wps=20226.2, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.906, loss_scale=32, train_wall=288, gb_free=19.9, wall=7686
2022-03-04 11:33:10 | INFO | train_inner | epoch 013:    167 / 196 loss=6.795, nll_loss=6.624, ppl=98.61, wps=21056.7, ups=0.32, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.904, loss_scale=32, train_wall=289, gb_free=19.9, wall=7997
2022-03-04 11:34:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:34:45 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.189 | nll_loss 7.018 | ppl 129.57 | wps 41525.7 | wpb 510.9 | bsz 1 | num_updates 2529 | best_loss 7.189
2022-03-04 11:34:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2529 updates
2022-03-04 11:34:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:34:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:34:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 13 @ 2529 updates, score 7.189) (writing took 6.759207906899974 seconds)
2022-03-04 11:34:52 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 11:34:52 | INFO | train | epoch 013 | loss 6.792 | nll_loss 6.621 | ppl 98.41 | wps 20636.3 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 2529 | lr 0.000316162 | gnorm 0.901 | loss_scale 64 | train_wall 565 | gb_free 19.9 | wall 8099
2022-03-04 11:34:52 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 11:34:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:35:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:38:36 | INFO | train_inner | epoch 014:     72 / 196 loss=6.653, nll_loss=6.479, ppl=89.21, wps=20057.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.884, loss_scale=32, train_wall=291, gb_free=19.9, wall=8323
2022-03-04 11:39:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:43:50 | INFO | train_inner | epoch 014:    173 / 196 loss=6.596, nll_loss=6.421, ppl=85.7, wps=20868.9, ups=0.32, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.876, loss_scale=16, train_wall=291, gb_free=19.9, wall=8637
2022-03-04 11:45:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:45:06 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.108 | nll_loss 6.935 | ppl 122.33 | wps 41435.2 | wpb 510.9 | bsz 1 | num_updates 2723 | best_loss 7.108
2022-03-04 11:45:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2723 updates
2022-03-04 11:45:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:45:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:45:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 14 @ 2723 updates, score 7.108) (writing took 6.819275507936254 seconds)
2022-03-04 11:45:13 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 11:45:13 | INFO | train | epoch 014 | loss 6.601 | nll_loss 6.427 | ppl 86.03 | wps 20432.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 2723 | lr 0.000340407 | gnorm 0.888 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 8720
2022-03-04 11:45:13 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 11:45:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:47:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:49:16 | INFO | train_inner | epoch 015:     78 / 196 loss=6.462, nll_loss=6.285, ppl=77.98, wps=20067.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.879, loss_scale=16, train_wall=291, gb_free=19.9, wall=8963
2022-03-04 11:54:27 | INFO | train_inner | epoch 015:    178 / 196 loss=6.433, nll_loss=6.255, ppl=76.35, wps=21066.6, ups=0.32, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.879, loss_scale=32, train_wall=289, gb_free=19.9, wall=9274
2022-03-04 11:54:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:55:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:55:28 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.04 | nll_loss 6.866 | ppl 116.68 | wps 41017.4 | wpb 510.9 | bsz 1 | num_updates 2917 | best_loss 7.04
2022-03-04 11:55:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2917 updates
2022-03-04 11:55:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:55:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 11:55:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 15 @ 2917 updates, score 7.04) (writing took 7.257038264069706 seconds)
2022-03-04 11:55:35 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 11:55:35 | INFO | train | epoch 015 | loss 6.43 | nll_loss 6.252 | ppl 76.2 | wps 20420.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 2917 | lr 0.000364652 | gnorm 0.884 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 9342
2022-03-04 11:55:35 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 11:55:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:59:53 | INFO | train_inner | epoch 016:     83 / 196 loss=6.294, nll_loss=6.113, ppl=69.22, wps=20033.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.875, loss_scale=16, train_wall=291, gb_free=19.9, wall=9600
2022-03-04 12:05:05 | INFO | train_inner | epoch 016:    183 / 196 loss=6.273, nll_loss=6.091, ppl=68.19, wps=21030.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.858, loss_scale=32, train_wall=289, gb_free=19.9, wall=9912
2022-03-04 12:05:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:05:50 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.013 | nll_loss 6.839 | ppl 114.45 | wps 40963.9 | wpb 510.9 | bsz 1 | num_updates 3113 | best_loss 7.013
2022-03-04 12:05:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3113 updates
2022-03-04 12:05:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:05:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:05:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 16 @ 3113 updates, score 7.013) (writing took 6.838269515894353 seconds)
2022-03-04 12:05:57 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 12:05:57 | INFO | train | epoch 016 | loss 6.268 | nll_loss 6.086 | ppl 67.95 | wps 20626.9 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 3113 | lr 0.000389147 | gnorm 0.859 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 9964
2022-03-04 12:05:57 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 12:05:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:08:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:10:31 | INFO | train_inner | epoch 017:     88 / 196 loss=6.119, nll_loss=5.935, ppl=61.17, wps=20055, ups=0.31, wpb=65367, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.858, loss_scale=32, train_wall=291, gb_free=19.9, wall=10238
2022-03-04 12:10:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:15:45 | INFO | train_inner | epoch 017:    189 / 196 loss=6.126, nll_loss=5.941, ppl=61.45, wps=20854.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.858, loss_scale=16, train_wall=292, gb_free=19.9, wall=10552
2022-03-04 12:16:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:16:12 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.973 | nll_loss 6.797 | ppl 111.19 | wps 41368.3 | wpb 510.9 | bsz 1 | num_updates 3307 | best_loss 6.973
2022-03-04 12:16:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3307 updates
2022-03-04 12:16:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:16:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:16:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 17 @ 3307 updates, score 6.973) (writing took 6.802940184948966 seconds)
2022-03-04 12:16:18 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 12:16:18 | INFO | train | epoch 017 | loss 6.115 | nll_loss 5.93 | ppl 60.98 | wps 20431.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 3307 | lr 0.000413392 | gnorm 0.864 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 10585
2022-03-04 12:16:18 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 12:16:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:21:08 | INFO | train_inner | epoch 018:     93 / 196 loss=5.972, nll_loss=5.784, ppl=55.12, wps=20253.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.875, loss_scale=32, train_wall=288, gb_free=19.9, wall=10875
2022-03-04 12:24:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:26:22 | INFO | train_inner | epoch 018:    194 / 196 loss=5.986, nll_loss=5.798, ppl=55.66, wps=20875.4, ups=0.32, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.854, loss_scale=16, train_wall=291, gb_free=19.9, wall=11189
2022-03-04 12:26:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:26:33 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.009 | nll_loss 6.831 | ppl 113.86 | wps 41231.4 | wpb 510.9 | bsz 1 | num_updates 3502 | best_loss 6.973
2022-03-04 12:26:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3502 updates
2022-03-04 12:26:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:26:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:26:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 18 @ 3502 updates, score 7.009) (writing took 3.190939428983256 seconds)
2022-03-04 12:26:36 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 12:26:36 | INFO | train | epoch 018 | loss 5.972 | nll_loss 5.785 | ppl 55.12 | wps 20668.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 3502 | lr 0.000437762 | gnorm 0.862 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 11203
2022-03-04 12:26:36 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 12:26:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:31:41 | INFO | train_inner | epoch 019:     98 / 196 loss=5.816, nll_loss=5.625, ppl=49.36, wps=20487.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.868, loss_scale=32, train_wall=288, gb_free=19.9, wall=11508
2022-03-04 12:36:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:36:51 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.963 | nll_loss 6.784 | ppl 110.19 | wps 41227.3 | wpb 510.9 | bsz 1 | num_updates 3698 | best_loss 6.963
2022-03-04 12:36:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3698 updates
2022-03-04 12:36:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:36:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt
2022-03-04 12:36:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_best.pt (epoch 19 @ 3698 updates, score 6.963) (writing took 6.9314004520419985 seconds)
2022-03-04 12:36:58 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 12:36:58 | INFO | train | epoch 019 | loss 5.839 | nll_loss 5.648 | ppl 50.16 | wps 20616.9 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 3698 | lr 0.000462258 | gnorm 0.861 | loss_scale 32 | train_wall 566 | gb_free 19.9 | wall 11825
2022-03-04 12:36:58 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 12:36:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:37:04 | INFO | train_inner | epoch 020:      2 / 196 loss=5.861, nll_loss=5.671, ppl=50.95, wps=20207.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.862, loss_scale=32, train_wall=289, gb_free=19.9, wall=11831
2022-03-04 12:38:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:42:20 | INFO | train_inner | epoch 020:    103 / 196 loss=5.68, nll_loss=5.486, ppl=44.83, wps=20784.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.855, loss_scale=32, train_wall=292, gb_free=19.9, wall=12146
2022-03-04 12:43:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:47:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:47:14 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.974 | nll_loss 6.79 | ppl 110.63 | wps 41098 | wpb 510.9 | bsz 1 | num_updates 3892 | best_loss 6.963
2022-03-04 12:47:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3892 updates
2022-03-04 12:47:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:47:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:47:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 20 @ 3892 updates, score 6.974) (writing took 3.10575590794906 seconds)
2022-03-04 12:47:17 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 12:47:17 | INFO | train | epoch 020 | loss 5.709 | nll_loss 5.516 | ppl 45.76 | wps 20510.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 3892 | lr 0.000486503 | gnorm 0.868 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 12444
2022-03-04 12:47:17 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 12:47:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:47:42 | INFO | train_inner | epoch 021:      8 / 196 loss=5.723, nll_loss=5.53, ppl=46.21, wps=20272.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=3900, lr=0.000487503, gnorm=0.882, loss_scale=16, train_wall=291, gb_free=19.9, wall=12469
2022-03-04 12:52:54 | INFO | train_inner | epoch 021:    108 / 196 loss=5.558, nll_loss=5.361, ppl=41.11, wps=21020.2, ups=0.32, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.849, loss_scale=32, train_wall=289, gb_free=19.9, wall=12781
2022-03-04 12:52:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:57:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:57:33 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.008 | nll_loss 6.826 | ppl 113.49 | wps 40969.4 | wpb 510.9 | bsz 1 | num_updates 4087 | best_loss 6.963
2022-03-04 12:57:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4087 updates
2022-03-04 12:57:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:57:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 12:57:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 21 @ 4087 updates, score 7.008) (writing took 3.2351989010348916 seconds)
2022-03-04 12:57:36 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 12:57:36 | INFO | train | epoch 021 | loss 5.587 | nll_loss 5.391 | ppl 41.96 | wps 20616.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 4087 | lr 0.00049465 | gnorm 0.854 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 13063
2022-03-04 12:57:36 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 12:57:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:58:17 | INFO | train_inner | epoch 022:     13 / 196 loss=5.597, nll_loss=5.401, ppl=42.25, wps=20246.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.85, loss_scale=16, train_wall=291, gb_free=19.9, wall=13103
2022-03-04 13:00:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:03:32 | INFO | train_inner | epoch 022:    114 / 196 loss=5.433, nll_loss=5.234, ppl=37.63, wps=20812.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.852, loss_scale=16, train_wall=292, gb_free=19.9, wall=13418
2022-03-04 13:07:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:07:52 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.009 | nll_loss 6.824 | ppl 113.34 | wps 41079.6 | wpb 510.9 | bsz 1 | num_updates 4282 | best_loss 6.963
2022-03-04 13:07:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4282 updates
2022-03-04 13:07:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:07:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:07:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 22 @ 4282 updates, score 7.009) (writing took 3.1061457011383027 seconds)
2022-03-04 13:07:55 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 13:07:55 | INFO | train | epoch 022 | loss 5.456 | nll_loss 5.257 | ppl 38.23 | wps 20627.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 4282 | lr 0.000483255 | gnorm 0.839 | loss_scale 32 | train_wall 566 | gb_free 19.9 | wall 13681
2022-03-04 13:07:55 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 13:07:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:08:51 | INFO | train_inner | epoch 023:     18 / 196 loss=5.448, nll_loss=5.249, ppl=38.03, wps=20464.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=4300, lr=0.000482243, gnorm=0.819, loss_scale=32, train_wall=288, gb_free=19.9, wall=13738
2022-03-04 13:14:03 | INFO | train_inner | epoch 023:    118 / 196 loss=5.307, nll_loss=5.105, ppl=34.42, wps=21016.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.814, loss_scale=64, train_wall=289, gb_free=19.9, wall=14049
2022-03-04 13:14:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:17:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:18:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:18:11 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.036 | nll_loss 6.85 | ppl 115.4 | wps 40352.4 | wpb 510.9 | bsz 1 | num_updates 4476 | best_loss 6.963
2022-03-04 13:18:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4476 updates
2022-03-04 13:18:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:18:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:18:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 23 @ 4476 updates, score 7.036) (writing took 3.6064814371056855 seconds)
2022-03-04 13:18:14 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 13:18:14 | INFO | train | epoch 023 | loss 5.328 | nll_loss 5.126 | ppl 34.92 | wps 20494.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 4476 | lr 0.000472667 | gnorm 0.815 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 14301
2022-03-04 13:18:14 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 13:18:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:19:29 | INFO | train_inner | epoch 024:     24 / 196 loss=5.311, nll_loss=5.109, ppl=34.51, wps=20024.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.805, loss_scale=16, train_wall=294, gb_free=19.9, wall=14376
2022-03-04 13:24:41 | INFO | train_inner | epoch 024:    124 / 196 loss=5.199, nll_loss=4.994, ppl=31.87, wps=20994.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.829, loss_scale=32, train_wall=289, gb_free=19.9, wall=14688
2022-03-04 13:27:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:28:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:28:31 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.078 | nll_loss 6.893 | ppl 118.85 | wps 40929.6 | wpb 510.9 | bsz 1 | num_updates 4671 | best_loss 6.963
2022-03-04 13:28:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4671 updates
2022-03-04 13:28:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:28:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:28:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 24 @ 4671 updates, score 7.078) (writing took 3.1172367979306728 seconds)
2022-03-04 13:28:34 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 13:28:34 | INFO | train | epoch 024 | loss 5.211 | nll_loss 5.006 | ppl 32.14 | wps 20605.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4671 | lr 0.000462695 | gnorm 0.807 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 14920
2022-03-04 13:28:34 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 13:28:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:30:04 | INFO | train_inner | epoch 025:     29 / 196 loss=5.194, nll_loss=4.989, ppl=31.75, wps=20251.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.816, loss_scale=16, train_wall=291, gb_free=19.9, wall=15011
2022-03-04 13:35:16 | INFO | train_inner | epoch 025:    129 / 196 loss=5.089, nll_loss=4.881, ppl=29.47, wps=21051.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.794, loss_scale=32, train_wall=289, gb_free=19.9, wall=15322
2022-03-04 13:38:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:38:49 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.119 | nll_loss 6.932 | ppl 122.07 | wps 41460.1 | wpb 510.9 | bsz 1 | num_updates 4867 | best_loss 6.963
2022-03-04 13:38:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4867 updates
2022-03-04 13:38:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:38:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:38:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 25 @ 4867 updates, score 7.119) (writing took 3.0552922699134797 seconds)
2022-03-04 13:38:52 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 13:38:52 | INFO | train | epoch 025 | loss 5.101 | nll_loss 4.894 | ppl 29.73 | wps 20752.7 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 4867 | lr 0.000453283 | gnorm 0.805 | loss_scale 32 | train_wall 566 | gb_free 19.9 | wall 15539
2022-03-04 13:38:52 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 13:38:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:40:35 | INFO | train_inner | epoch 026:     33 / 196 loss=5.077, nll_loss=4.869, ppl=29.22, wps=20474, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.812, loss_scale=32, train_wall=288, gb_free=19.9, wall=15641
2022-03-04 13:41:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:43:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:45:52 | INFO | train_inner | epoch 026:    135 / 196 loss=4.986, nll_loss=4.776, ppl=27.4, wps=20639.5, ups=0.31, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.801, loss_scale=16, train_wall=294, gb_free=19.9, wall=15959
2022-03-04 13:49:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:49:07 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.158 | nll_loss 6.966 | ppl 125 | wps 41302.4 | wpb 510.9 | bsz 1 | num_updates 5061 | best_loss 6.963
2022-03-04 13:49:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5061 updates
2022-03-04 13:49:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:49:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:49:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 26 @ 5061 updates, score 7.158) (writing took 3.1829974979627877 seconds)
2022-03-04 13:49:10 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 13:49:10 | INFO | train | epoch 026 | loss 4.995 | nll_loss 4.786 | ppl 27.58 | wps 20548.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 5061 | lr 0.00044451 | gnorm 0.811 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 16156
2022-03-04 13:49:10 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 13:49:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:51:11 | INFO | train_inner | epoch 027:     39 / 196 loss=4.959, nll_loss=4.748, ppl=26.87, wps=20496.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.814, loss_scale=32, train_wall=288, gb_free=19.9, wall=16278
2022-03-04 13:56:23 | INFO | train_inner | epoch 027:    139 / 196 loss=4.898, nll_loss=4.686, ppl=25.74, wps=21035.4, ups=0.32, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.802, loss_scale=32, train_wall=289, gb_free=19.9, wall=16589
2022-03-04 13:56:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:58:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:59:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:59:25 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.211 | nll_loss 7.022 | ppl 129.98 | wps 41560.5 | wpb 510.9 | bsz 1 | num_updates 5255 | best_loss 6.963
2022-03-04 13:59:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5255 updates
2022-03-04 13:59:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:59:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 13:59:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 27 @ 5255 updates, score 7.211) (writing took 3.060098022921011 seconds)
2022-03-04 13:59:28 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 13:59:28 | INFO | train | epoch 027 | loss 4.899 | nll_loss 4.687 | ppl 25.75 | wps 20547.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 5255 | lr 0.000436228 | gnorm 0.807 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 16774
2022-03-04 13:59:28 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 13:59:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:01:48 | INFO | train_inner | epoch 028:     45 / 196 loss=4.851, nll_loss=4.638, ppl=24.9, wps=20114.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.806, loss_scale=16, train_wall=294, gb_free=19.9, wall=16914
2022-03-04 14:06:59 | INFO | train_inner | epoch 028:    145 / 196 loss=4.813, nll_loss=4.598, ppl=24.22, wps=21076.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.805, loss_scale=32, train_wall=288, gb_free=19.9, wall=17225
2022-03-04 14:09:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:09:42 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.212 | nll_loss 7.024 | ppl 130.13 | wps 41281 | wpb 510.9 | bsz 1 | num_updates 5451 | best_loss 6.963
2022-03-04 14:09:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5451 updates
2022-03-04 14:09:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:09:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:09:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 28 @ 5451 updates, score 7.212) (writing took 3.0940239899791777 seconds)
2022-03-04 14:09:45 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 14:09:45 | INFO | train | epoch 028 | loss 4.807 | nll_loss 4.592 | ppl 24.13 | wps 20783.2 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 5451 | lr 0.000428314 | gnorm 0.805 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 17392
2022-03-04 14:09:45 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 14:09:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:11:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:12:21 | INFO | train_inner | epoch 029:     50 / 196 loss=4.753, nll_loss=4.538, ppl=23.22, wps=20296.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.808, loss_scale=32, train_wall=291, gb_free=19.9, wall=17547
2022-03-04 14:17:31 | INFO | train_inner | epoch 029:    150 / 196 loss=4.727, nll_loss=4.51, ppl=22.78, wps=21086, ups=0.32, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.828, loss_scale=32, train_wall=288, gb_free=19.9, wall=17858
2022-03-04 14:18:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:19:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:19:59 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.282 | nll_loss 7.086 | ppl 135.9 | wps 41283.5 | wpb 510.9 | bsz 1 | num_updates 5645 | best_loss 6.963
2022-03-04 14:19:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5645 updates
2022-03-04 14:19:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:20:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:20:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 29 @ 5645 updates, score 7.282) (writing took 3.1788054688367993 seconds)
2022-03-04 14:20:02 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 14:20:02 | INFO | train | epoch 029 | loss 4.718 | nll_loss 4.501 | ppl 22.65 | wps 20566.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 5645 | lr 0.000420889 | gnorm 0.817 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 18009
2022-03-04 14:20:02 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 14:20:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:22:53 | INFO | train_inner | epoch 030:     55 / 196 loss=4.666, nll_loss=4.448, ppl=21.82, wps=20314.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.799, loss_scale=32, train_wall=290, gb_free=19.9, wall=18180
2022-03-04 14:25:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:28:07 | INFO | train_inner | epoch 030:    156 / 196 loss=4.646, nll_loss=4.427, ppl=21.5, wps=20880.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.817, loss_scale=32, train_wall=291, gb_free=19.9, wall=18494
2022-03-04 14:30:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:30:16 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.315 | nll_loss 7.125 | ppl 139.54 | wps 41259.1 | wpb 510.9 | bsz 1 | num_updates 5840 | best_loss 6.963
2022-03-04 14:30:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5840 updates
2022-03-04 14:30:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:30:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:30:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 30 @ 5840 updates, score 7.315) (writing took 3.0555309162009507 seconds)
2022-03-04 14:30:19 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 14:30:19 | INFO | train | epoch 030 | loss 4.634 | nll_loss 4.415 | ppl 21.33 | wps 20688.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 5840 | lr 0.000413803 | gnorm 0.808 | loss_scale 32 | train_wall 564 | gb_free 19.9 | wall 18626
2022-03-04 14:30:19 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 14:30:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:31:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:33:29 | INFO | train_inner | epoch 031:     61 / 196 loss=4.571, nll_loss=4.351, ppl=20.4, wps=20311.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.82, loss_scale=16, train_wall=291, gb_free=19.9, wall=18816
2022-03-04 14:38:40 | INFO | train_inner | epoch 031:    161 / 196 loss=4.576, nll_loss=4.355, ppl=20.46, wps=21052.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.838, loss_scale=32, train_wall=289, gb_free=19.9, wall=19127
2022-03-04 14:40:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:40:34 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.358 | nll_loss 7.162 | ppl 143.24 | wps 41193.7 | wpb 510.9 | bsz 1 | num_updates 6035 | best_loss 6.963
2022-03-04 14:40:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6035 updates
2022-03-04 14:40:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:40:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:40:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 31 @ 6035 updates, score 7.358) (writing took 3.480992436874658 seconds)
2022-03-04 14:40:37 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 14:40:37 | INFO | train | epoch 031 | loss 4.556 | nll_loss 4.334 | ppl 20.17 | wps 20647.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 6035 | lr 0.000407063 | gnorm 0.833 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 19244
2022-03-04 14:40:37 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 14:40:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:44:00 | INFO | train_inner | epoch 032:     65 / 196 loss=4.485, nll_loss=4.262, ppl=19.18, wps=20459.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.835, loss_scale=32, train_wall=288, gb_free=19.9, wall=19446
2022-03-04 14:44:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:49:14 | INFO | train_inner | epoch 032:    166 / 196 loss=4.501, nll_loss=4.278, ppl=19.4, wps=20849.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.832, loss_scale=32, train_wall=292, gb_free=19.9, wall=19761
2022-03-04 14:50:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:50:52 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.434 | nll_loss 7.235 | ppl 150.68 | wps 41037 | wpb 510.9 | bsz 1 | num_updates 6230 | best_loss 6.963
2022-03-04 14:50:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6230 updates
2022-03-04 14:50:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:50:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 14:50:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 32 @ 6230 updates, score 7.434) (writing took 3.1572714999783784 seconds)
2022-03-04 14:50:55 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 14:50:55 | INFO | train | epoch 032 | loss 4.48 | nll_loss 4.257 | ppl 19.12 | wps 20654.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 6230 | lr 0.000400642 | gnorm 0.839 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 19862
2022-03-04 14:50:55 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 14:50:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:51:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:53:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:54:39 | INFO | train_inner | epoch 033:     72 / 196 loss=4.405, nll_loss=4.18, ppl=18.13, wps=20091.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.839, loss_scale=16, train_wall=294, gb_free=19.9, wall=20086
2022-03-04 14:59:50 | INFO | train_inner | epoch 033:    172 / 196 loss=4.435, nll_loss=4.21, ppl=18.51, wps=21080.9, ups=0.32, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.846, loss_scale=16, train_wall=288, gb_free=19.9, wall=20397
2022-03-04 15:01:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:01:09 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.456 | nll_loss 7.261 | ppl 153.41 | wps 41372.1 | wpb 510.9 | bsz 1 | num_updates 6424 | best_loss 6.963
2022-03-04 15:01:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6424 updates
2022-03-04 15:01:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:01:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:01:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 33 @ 6424 updates, score 7.456) (writing took 3.0512634480837733 seconds)
2022-03-04 15:01:12 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 15:01:12 | INFO | train | epoch 033 | loss 4.408 | nll_loss 4.182 | ppl 18.15 | wps 20567.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 6424 | lr 0.000394546 | gnorm 0.845 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 20479
2022-03-04 15:01:12 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 15:01:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:02:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:05:12 | INFO | train_inner | epoch 034:     77 / 196 loss=4.332, nll_loss=4.105, ppl=17.21, wps=20309.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.835, loss_scale=16, train_wall=291, gb_free=19.9, wall=20719
2022-03-04 15:10:23 | INFO | train_inner | epoch 034:    177 / 196 loss=4.369, nll_loss=4.142, ppl=17.66, wps=21078.9, ups=0.32, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.838, loss_scale=32, train_wall=288, gb_free=19.9, wall=21030
2022-03-04 15:11:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:11:27 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.473 | nll_loss 7.277 | ppl 155.13 | wps 41446 | wpb 510.9 | bsz 1 | num_updates 6619 | best_loss 6.963
2022-03-04 15:11:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6619 updates
2022-03-04 15:11:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:11:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:11:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 34 @ 6619 updates, score 7.473) (writing took 3.1361990799196064 seconds)
2022-03-04 15:11:30 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 15:11:30 | INFO | train | epoch 034 | loss 4.34 | nll_loss 4.113 | ppl 17.3 | wps 20671.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 6619 | lr 0.00038869 | gnorm 0.83 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 21096
2022-03-04 15:11:30 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 15:11:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:14:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:15:45 | INFO | train_inner | epoch 035:     82 / 196 loss=4.251, nll_loss=4.022, ppl=16.24, wps=20269.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.857, loss_scale=16, train_wall=291, gb_free=19.9, wall=21352
2022-03-04 15:20:57 | INFO | train_inner | epoch 035:    182 / 196 loss=4.308, nll_loss=4.079, ppl=16.9, wps=21055.4, ups=0.32, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.856, loss_scale=16, train_wall=289, gb_free=19.9, wall=21663
2022-03-04 15:21:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:21:45 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.553 | nll_loss 7.356 | ppl 163.78 | wps 41280.6 | wpb 510.9 | bsz 1 | num_updates 6814 | best_loss 6.963
2022-03-04 15:21:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6814 updates
2022-03-04 15:21:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:21:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:21:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 35 @ 6814 updates, score 7.553) (writing took 3.114868963835761 seconds)
2022-03-04 15:21:48 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 15:21:48 | INFO | train | epoch 035 | loss 4.274 | nll_loss 4.045 | ppl 16.5 | wps 20649.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 6814 | lr 0.000383088 | gnorm 0.859 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 21715
2022-03-04 15:21:48 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 15:21:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:24:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:26:18 | INFO | train_inner | epoch 036:     87 / 196 loss=4.184, nll_loss=3.953, ppl=15.48, wps=20320.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.867, loss_scale=16, train_wall=291, gb_free=19.9, wall=21985
2022-03-04 15:31:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:31:32 | INFO | train_inner | epoch 036:    188 / 196 loss=4.257, nll_loss=4.026, ppl=16.29, wps=20882.3, ups=0.32, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.857, loss_scale=16, train_wall=291, gb_free=19.9, wall=22299
2022-03-04 15:31:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:32:02 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.59 | nll_loss 7.393 | ppl 168.08 | wps 41277.6 | wpb 510.9 | bsz 1 | num_updates 7008 | best_loss 6.963
2022-03-04 15:32:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7008 updates
2022-03-04 15:32:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:32:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:32:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 36 @ 7008 updates, score 7.59) (writing took 3.2273708668071777 seconds)
2022-03-04 15:32:05 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 15:32:05 | INFO | train | epoch 036 | loss 4.211 | nll_loss 3.98 | ppl 15.78 | wps 20577.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7008 | lr 0.000377749 | gnorm 0.863 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 22332
2022-03-04 15:32:05 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 15:32:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:36:51 | INFO | train_inner | epoch 037:     92 / 196 loss=4.114, nll_loss=3.88, ppl=14.73, wps=20502.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.859, loss_scale=16, train_wall=288, gb_free=19.9, wall=22618
2022-03-04 15:40:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:42:05 | INFO | train_inner | epoch 037:    193 / 196 loss=4.197, nll_loss=3.965, ppl=15.62, wps=20882.2, ups=0.32, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.873, loss_scale=16, train_wall=291, gb_free=19.9, wall=22931
2022-03-04 15:42:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:42:19 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.635 | nll_loss 7.438 | ppl 173.37 | wps 41292.8 | wpb 510.9 | bsz 1 | num_updates 7203 | best_loss 6.963
2022-03-04 15:42:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7203 updates
2022-03-04 15:42:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:42:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:42:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 37 @ 7203 updates, score 7.635) (writing took 3.220395991113037 seconds)
2022-03-04 15:42:22 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 15:42:22 | INFO | train | epoch 037 | loss 4.152 | nll_loss 3.919 | ppl 15.13 | wps 20682.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 7203 | lr 0.0003726 | gnorm 0.865 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 22949
2022-03-04 15:42:22 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 15:42:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:47:24 | INFO | train_inner | epoch 038:     97 / 196 loss=4.049, nll_loss=3.813, ppl=14.06, wps=20508.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.861, loss_scale=32, train_wall=288, gb_free=19.9, wall=23250
2022-03-04 15:51:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:52:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:52:36 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.688 | nll_loss 7.492 | ppl 180 | wps 41297.4 | wpb 510.9 | bsz 1 | num_updates 7398 | best_loss 6.963
2022-03-04 15:52:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7398 updates
2022-03-04 15:52:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:52:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 15:52:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 38 @ 7398 updates, score 7.688) (writing took 3.1840148598421365 seconds)
2022-03-04 15:52:39 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 15:52:39 | INFO | train | epoch 038 | loss 4.094 | nll_loss 3.86 | ppl 14.52 | wps 20670.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 7398 | lr 0.000367657 | gnorm 0.867 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 23566
2022-03-04 15:52:39 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 15:52:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:52:46 | INFO | train_inner | epoch 039:      2 / 196 loss=4.141, nll_loss=3.907, ppl=15, wps=20290.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.873, loss_scale=16, train_wall=291, gb_free=19.9, wall=23572
2022-03-04 15:57:57 | INFO | train_inner | epoch 039:    102 / 196 loss=3.988, nll_loss=3.751, ppl=13.46, wps=21068.5, ups=0.32, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.878, loss_scale=32, train_wall=289, gb_free=19.9, wall=23883
2022-03-04 15:58:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:02:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:02:53 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.725 | nll_loss 7.528 | ppl 184.51 | wps 41056.2 | wpb 510.9 | bsz 1 | num_updates 7593 | best_loss 6.963
2022-03-04 16:02:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7593 updates
2022-03-04 16:02:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:02:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:02:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 39 @ 7593 updates, score 7.725) (writing took 3.153495794860646 seconds)
2022-03-04 16:02:57 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 16:02:57 | INFO | train | epoch 039 | loss 4.039 | nll_loss 3.802 | ppl 13.95 | wps 20679.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 7593 | lr 0.000362905 | gnorm 0.875 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 24183
2022-03-04 16:02:57 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 16:02:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:03:18 | INFO | train_inner | epoch 040:      7 / 196 loss=4.081, nll_loss=3.845, ppl=14.37, wps=20324.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7600, lr=0.000362738, gnorm=0.882, loss_scale=16, train_wall=290, gb_free=19.9, wall=24205
2022-03-04 16:05:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:08:32 | INFO | train_inner | epoch 040:    108 / 196 loss=3.947, nll_loss=3.708, ppl=13.07, wps=20867.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.88, loss_scale=16, train_wall=291, gb_free=19.9, wall=24519
2022-03-04 16:13:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:13:11 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.789 | nll_loss 7.59 | ppl 192.73 | wps 41388.7 | wpb 510.9 | bsz 1 | num_updates 7788 | best_loss 6.963
2022-03-04 16:13:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7788 updates
2022-03-04 16:13:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:13:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:13:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 40 @ 7788 updates, score 7.789) (writing took 3.200948331039399 seconds)
2022-03-04 16:13:14 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 16:13:14 | INFO | train | epoch 040 | loss 3.985 | nll_loss 3.747 | ppl 13.43 | wps 20673.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 7788 | lr 0.000358333 | gnorm 0.892 | loss_scale 32 | train_wall 565 | gb_free 19.9 | wall 24800
2022-03-04 16:13:14 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 16:13:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:13:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:13:54 | INFO | train_inner | epoch 041:     13 / 196 loss=4.012, nll_loss=3.774, ppl=13.68, wps=20306.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.901, loss_scale=16, train_wall=291, gb_free=19.9, wall=24841
2022-03-04 16:19:05 | INFO | train_inner | epoch 041:    113 / 196 loss=3.894, nll_loss=3.654, ppl=12.59, wps=21080.8, ups=0.32, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.877, loss_scale=16, train_wall=288, gb_free=19.9, wall=25152
2022-03-04 16:20:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:23:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:23:28 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.834 | nll_loss 7.635 | ppl 198.82 | wps 41090.3 | wpb 510.9 | bsz 1 | num_updates 7982 | best_loss 6.963
2022-03-04 16:23:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 7982 updates
2022-03-04 16:23:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:23:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:23:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 41 @ 7982 updates, score 7.834) (writing took 3.177227940876037 seconds)
2022-03-04 16:23:31 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 16:23:31 | INFO | train | epoch 041 | loss 3.935 | nll_loss 3.695 | ppl 12.95 | wps 20573.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7982 | lr 0.000353952 | gnorm 0.895 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 25418
2022-03-04 16:23:31 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 16:23:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:24:27 | INFO | train_inner | epoch 042:     18 / 196 loss=3.961, nll_loss=3.721, ppl=13.19, wps=20309.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=8000, lr=0.000353553, gnorm=0.91, loss_scale=16, train_wall=291, gb_free=19.9, wall=25474
2022-03-04 16:27:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:29:41 | INFO | train_inner | epoch 042:    119 / 196 loss=3.855, nll_loss=3.614, ppl=12.24, wps=20857.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.897, loss_scale=16, train_wall=291, gb_free=19.9, wall=25788
2022-03-04 16:33:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:33:46 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.906 | nll_loss 7.706 | ppl 208.86 | wps 41119.2 | wpb 510.9 | bsz 1 | num_updates 8177 | best_loss 6.963
2022-03-04 16:33:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8177 updates
2022-03-04 16:33:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:33:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:33:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 42 @ 8177 updates, score 7.906) (writing took 3.2017019109334797 seconds)
2022-03-04 16:33:49 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 16:33:49 | INFO | train | epoch 042 | loss 3.886 | nll_loss 3.645 | ppl 12.51 | wps 20641.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 8177 | lr 0.000349706 | gnorm 0.903 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 26036
2022-03-04 16:33:49 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 16:33:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:34:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:35:04 | INFO | train_inner | epoch 043:     24 / 196 loss=3.9, nll_loss=3.659, ppl=12.63, wps=20256.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.905, loss_scale=16, train_wall=291, gb_free=19.9, wall=26111
2022-03-04 16:40:15 | INFO | train_inner | epoch 043:    124 / 196 loss=3.818, nll_loss=3.575, ppl=11.91, wps=21079, ups=0.32, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.906, loss_scale=16, train_wall=288, gb_free=19.9, wall=26421
2022-03-04 16:40:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:43:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:44:03 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.968 | nll_loss 7.768 | ppl 217.92 | wps 41571.8 | wpb 510.9 | bsz 1 | num_updates 8371 | best_loss 6.963
2022-03-04 16:44:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8371 updates
2022-03-04 16:44:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:44:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:44:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 43 @ 8371 updates, score 7.968) (writing took 3.2045944610144943 seconds)
2022-03-04 16:44:06 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 16:44:06 | INFO | train | epoch 043 | loss 3.839 | nll_loss 3.597 | ppl 12.1 | wps 20573.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 8371 | lr 0.00034563 | gnorm 0.903 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 26653
2022-03-04 16:44:06 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 16:44:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:45:37 | INFO | train_inner | epoch 044:     29 / 196 loss=3.842, nll_loss=3.599, ppl=12.12, wps=20302.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=8400, lr=0.000345033, gnorm=0.902, loss_scale=16, train_wall=291, gb_free=19.9, wall=26743
2022-03-04 16:47:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:50:51 | INFO | train_inner | epoch 044:    130 / 196 loss=3.78, nll_loss=3.535, ppl=11.6, wps=20870.6, ups=0.32, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.933, loss_scale=16, train_wall=291, gb_free=19.9, wall=27057
2022-03-04 16:54:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:54:21 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.009 | nll_loss 7.809 | ppl 224.33 | wps 41335.9 | wpb 510.9 | bsz 1 | num_updates 8566 | best_loss 6.963
2022-03-04 16:54:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8566 updates
2022-03-04 16:54:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:54:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 16:54:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 44 @ 8566 updates, score 8.009) (writing took 3.2019905380439013 seconds)
2022-03-04 16:54:24 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 16:54:24 | INFO | train | epoch 044 | loss 3.795 | nll_loss 3.551 | ppl 11.72 | wps 20672.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 8566 | lr 0.000341673 | gnorm 0.928 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 27270
2022-03-04 16:54:24 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 16:54:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:56:10 | INFO | train_inner | epoch 045:     34 / 196 loss=3.799, nll_loss=3.554, ppl=11.75, wps=20493.4, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.921, loss_scale=32, train_wall=288, gb_free=19.9, wall=27376
2022-03-04 16:57:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:01:24 | INFO | train_inner | epoch 045:    135 / 196 loss=3.741, nll_loss=3.495, ppl=11.28, wps=20882, ups=0.32, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.906, loss_scale=16, train_wall=291, gb_free=19.9, wall=27690
2022-03-04 17:04:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:04:38 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.067 | nll_loss 7.867 | ppl 233.41 | wps 41312.8 | wpb 510.9 | bsz 1 | num_updates 8761 | best_loss 6.963
2022-03-04 17:04:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8761 updates
2022-03-04 17:04:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:04:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:04:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 45 @ 8761 updates, score 8.067) (writing took 3.068064710823819 seconds)
2022-03-04 17:04:41 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 17:04:41 | INFO | train | epoch 045 | loss 3.751 | nll_loss 3.506 | ppl 11.36 | wps 20685.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 8761 | lr 0.000337849 | gnorm 0.911 | loss_scale 32 | train_wall 564 | gb_free 19.9 | wall 27887
2022-03-04 17:04:41 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 17:04:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:04:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:06:45 | INFO | train_inner | epoch 046:     40 / 196 loss=3.737, nll_loss=3.491, ppl=11.24, wps=20326.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=8800, lr=0.0003371, gnorm=0.915, loss_scale=16, train_wall=290, gb_free=19.9, wall=28012
2022-03-04 17:11:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:12:00 | INFO | train_inner | epoch 046:    141 / 196 loss=3.706, nll_loss=3.459, ppl=10.99, wps=20839.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.938, loss_scale=16, train_wall=292, gb_free=19.9, wall=28326
2022-03-04 17:14:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:14:55 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.114 | nll_loss 7.915 | ppl 241.35 | wps 41164.7 | wpb 510.9 | bsz 1 | num_updates 8955 | best_loss 6.963
2022-03-04 17:14:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8955 updates
2022-03-04 17:14:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:14:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:14:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 46 @ 8955 updates, score 8.114) (writing took 3.256282336078584 seconds)
2022-03-04 17:14:59 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 17:14:59 | INFO | train | epoch 046 | loss 3.71 | nll_loss 3.463 | ppl 11.03 | wps 20547.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 8955 | lr 0.00033417 | gnorm 0.927 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 28505
2022-03-04 17:14:59 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 17:14:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:17:19 | INFO | train_inner | epoch 047:     45 / 196 loss=3.691, nll_loss=3.444, ppl=10.88, wps=20491.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.922, loss_scale=16, train_wall=288, gb_free=19.9, wall=28645
2022-03-04 17:18:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:22:33 | INFO | train_inner | epoch 047:    146 / 196 loss=3.669, nll_loss=3.421, ppl=10.71, wps=20866.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.941, loss_scale=16, train_wall=291, gb_free=19.9, wall=28959
2022-03-04 17:25:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:25:13 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.136 | nll_loss 7.935 | ppl 244.76 | wps 41301.1 | wpb 510.9 | bsz 1 | num_updates 9150 | best_loss 6.963
2022-03-04 17:25:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 9150 updates
2022-03-04 17:25:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:25:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:25:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 47 @ 9150 updates, score 8.136) (writing took 3.1178710779640824 seconds)
2022-03-04 17:25:16 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 17:25:16 | INFO | train | epoch 047 | loss 3.668 | nll_loss 3.419 | ppl 10.7 | wps 20682.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 9150 | lr 0.00033059 | gnorm 0.932 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 29122
2022-03-04 17:25:16 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 17:25:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:25:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:27:54 | INFO | train_inner | epoch 048:     51 / 196 loss=3.651, nll_loss=3.402, ppl=10.57, wps=20314.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=9200, lr=0.00032969, gnorm=0.936, loss_scale=16, train_wall=291, gb_free=19.9, wall=29281
2022-03-04 17:32:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:33:09 | INFO | train_inner | epoch 048:    152 / 196 loss=3.634, nll_loss=3.384, ppl=10.44, wps=20851.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.945, loss_scale=16, train_wall=292, gb_free=19.9, wall=29595
2022-03-04 17:35:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:35:30 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.195 | nll_loss 7.991 | ppl 254.45 | wps 41241.1 | wpb 510.9 | bsz 1 | num_updates 9344 | best_loss 6.963
2022-03-04 17:35:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 9344 updates
2022-03-04 17:35:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:35:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:35:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 48 @ 9344 updates, score 8.195) (writing took 3.187560662860051 seconds)
2022-03-04 17:35:33 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 17:35:33 | INFO | train | epoch 048 | loss 3.63 | nll_loss 3.381 | ppl 10.42 | wps 20562.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 9344 | lr 0.00032714 | gnorm 0.947 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 29740
2022-03-04 17:35:33 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 17:35:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:38:28 | INFO | train_inner | epoch 049:     56 / 196 loss=3.598, nll_loss=3.348, ppl=10.18, wps=20500.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.942, loss_scale=16, train_wall=288, gb_free=19.9, wall=29914
2022-03-04 17:40:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:43:42 | INFO | train_inner | epoch 049:    157 / 196 loss=3.604, nll_loss=3.354, ppl=10.22, wps=20869.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.939, loss_scale=16, train_wall=291, gb_free=19.9, wall=30228
2022-03-04 17:45:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:45:47 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.255 | nll_loss 8.054 | ppl 265.72 | wps 41450.1 | wpb 510.9 | bsz 1 | num_updates 9539 | best_loss 6.963
2022-03-04 17:45:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 9539 updates
2022-03-04 17:45:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:45:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:45:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 49 @ 9539 updates, score 8.255) (writing took 3.1576168488245457 seconds)
2022-03-04 17:45:51 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 17:45:51 | INFO | train | epoch 049 | loss 3.592 | nll_loss 3.341 | ppl 10.13 | wps 20672.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 9539 | lr 0.000323779 | gnorm 0.937 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 30357
2022-03-04 17:45:51 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 17:45:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:47:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:49:03 | INFO | train_inner | epoch 050:     62 / 196 loss=3.557, nll_loss=3.305, ppl=9.88, wps=20315, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=9600, lr=0.000322749, gnorm=0.938, loss_scale=16, train_wall=291, gb_free=19.9, wall=30550
2022-03-04 17:53:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:54:18 | INFO | train_inner | epoch 050:    163 / 196 loss=3.576, nll_loss=3.324, ppl=10.02, wps=20810, ups=0.32, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.963, loss_scale=16, train_wall=292, gb_free=19.9, wall=30865
2022-03-04 17:56:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:56:05 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.3 | nll_loss 8.099 | ppl 274.25 | wps 41191.2 | wpb 510.9 | bsz 1 | num_updates 9733 | best_loss 6.963
2022-03-04 17:56:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 9733 updates
2022-03-04 17:56:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:56:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 17:56:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 50 @ 9733 updates, score 8.3) (writing took 3.163306765956804 seconds)
2022-03-04 17:56:09 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 17:56:09 | INFO | train | epoch 050 | loss 3.556 | nll_loss 3.304 | ppl 9.88 | wps 20543.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 9733 | lr 0.000320536 | gnorm 0.952 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 30975
2022-03-04 17:56:09 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 17:56:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:59:37 | INFO | train_inner | epoch 051:     67 / 196 loss=3.511, nll_loss=3.258, ppl=9.57, wps=20499.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.959, loss_scale=16, train_wall=288, gb_free=19.9, wall=31184
2022-03-04 18:00:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:04:51 | INFO | train_inner | epoch 051:    168 / 196 loss=3.54, nll_loss=3.287, ppl=9.76, wps=20850.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.947, loss_scale=16, train_wall=292, gb_free=19.9, wall=31498
2022-03-04 18:06:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:06:23 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 8.322 | nll_loss 8.12 | ppl 278.26 | wps 41219.4 | wpb 510.9 | bsz 1 | num_updates 9928 | best_loss 6.963
2022-03-04 18:06:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 9928 updates
2022-03-04 18:06:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:06:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:06:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 51 @ 9928 updates, score 8.322) (writing took 3.2122027589939535 seconds)
2022-03-04 18:06:26 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 18:06:26 | INFO | train | epoch 051 | loss 3.521 | nll_loss 3.268 | ppl 9.63 | wps 20660.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 9928 | lr 0.000317372 | gnorm 0.952 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 31593
2022-03-04 18:06:26 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 18:06:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:07:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:10:14 | INFO | train_inner | epoch 052:     73 / 196 loss=3.477, nll_loss=3.223, ppl=9.34, wps=20292.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10000, lr=0.000316228, gnorm=0.948, loss_scale=16, train_wall=291, gb_free=19.9, wall=31820
2022-03-04 18:14:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:15:28 | INFO | train_inner | epoch 052:    174 / 196 loss=3.513, nll_loss=3.26, ppl=9.58, wps=20861, ups=0.32, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.975, loss_scale=16, train_wall=291, gb_free=19.9, wall=32134
2022-03-04 18:16:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:16:41 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 8.406 | nll_loss 8.204 | ppl 294.87 | wps 41239.9 | wpb 510.9 | bsz 1 | num_updates 10122 | best_loss 6.963
2022-03-04 18:16:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 10122 updates
2022-03-04 18:16:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:16:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:16:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 52 @ 10122 updates, score 8.406) (writing took 3.0862442401703447 seconds)
2022-03-04 18:16:44 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 18:16:44 | INFO | train | epoch 052 | loss 3.487 | nll_loss 3.233 | ppl 9.4 | wps 20560.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 10122 | lr 0.000314316 | gnorm 0.962 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 32210
2022-03-04 18:16:44 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 18:16:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:20:47 | INFO | train_inner | epoch 053:     78 / 196 loss=3.427, nll_loss=3.172, ppl=9.01, wps=20497.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.966, loss_scale=16, train_wall=288, gb_free=19.9, wall=32453
2022-03-04 18:21:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:26:00 | INFO | train_inner | epoch 053:    179 / 196 loss=3.495, nll_loss=3.24, ppl=9.45, wps=20877.5, ups=0.32, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.976, loss_scale=16, train_wall=291, gb_free=19.9, wall=32767
2022-03-04 18:26:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:26:58 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 8.426 | nll_loss 8.224 | ppl 298.99 | wps 41094.4 | wpb 510.9 | bsz 1 | num_updates 10317 | best_loss 6.963
2022-03-04 18:26:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 10317 updates
2022-03-04 18:26:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:27:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:27:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 53 @ 10317 updates, score 8.426) (writing took 3.2463329760357738 seconds)
2022-03-04 18:27:01 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 18:27:01 | INFO | train | epoch 053 | loss 3.455 | nll_loss 3.2 | ppl 9.19 | wps 20674.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 10317 | lr 0.000311332 | gnorm 0.978 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 32828
2022-03-04 18:27:01 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 18:27:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:28:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:31:23 | INFO | train_inner | epoch 054:     84 / 196 loss=3.394, nll_loss=3.138, ppl=8.8, wps=20265.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=10400, lr=0.000310087, gnorm=0.976, loss_scale=16, train_wall=291, gb_free=19.9, wall=33090
2022-03-04 18:34:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:35:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:36:40 | INFO | train_inner | epoch 054:    186 / 196 loss=3.457, nll_loss=3.202, ppl=9.2, wps=20646.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.973, loss_scale=8, train_wall=294, gb_free=19.9, wall=33407
2022-03-04 18:37:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:37:16 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 8.508 | nll_loss 8.305 | ppl 316.16 | wps 41504.9 | wpb 510.9 | bsz 1 | num_updates 10510 | best_loss 6.963
2022-03-04 18:37:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 10510 updates
2022-03-04 18:37:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:37:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:37:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 54 @ 10510 updates, score 8.508) (writing took 3.3771539949811995 seconds)
2022-03-04 18:37:19 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 18:37:19 | INFO | train | epoch 054 | loss 3.42 | nll_loss 3.164 | ppl 8.96 | wps 20430.5 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 10510 | lr 0.00030846 | gnorm 0.973 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 33446
2022-03-04 18:37:19 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 18:37:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:41:59 | INFO | train_inner | epoch 055:     90 / 196 loss=3.355, nll_loss=3.098, ppl=8.56, wps=20544.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.977, loss_scale=8, train_wall=287, gb_free=19.9, wall=33725
2022-03-04 18:47:09 | INFO | train_inner | epoch 055:    190 / 196 loss=3.435, nll_loss=3.179, ppl=9.06, wps=21084.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.983, loss_scale=16, train_wall=288, gb_free=19.9, wall=34036
2022-03-04 18:47:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:47:33 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 8.534 | nll_loss 8.332 | ppl 322.26 | wps 41388.4 | wpb 510.9 | bsz 1 | num_updates 10706 | best_loss 6.963
2022-03-04 18:47:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 10706 updates
2022-03-04 18:47:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:47:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:47:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 55 @ 10706 updates, score 8.534) (writing took 3.344287262065336 seconds)
2022-03-04 18:47:36 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 18:47:36 | INFO | train | epoch 055 | loss 3.393 | nll_loss 3.136 | ppl 8.79 | wps 20805.9 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 10706 | lr 0.000305623 | gnorm 0.98 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 34063
2022-03-04 18:47:36 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 18:47:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:49:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:52:30 | INFO | train_inner | epoch 056:     95 / 196 loss=3.318, nll_loss=3.06, ppl=8.34, wps=20363.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=10800, lr=0.00030429, gnorm=0.963, loss_scale=16, train_wall=290, gb_free=19.9, wall=34357
2022-03-04 18:55:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:57:43 | INFO | train_inner | epoch 056:    196 / 196 loss=3.41, nll_loss=3.153, ppl=8.89, wps=20929.2, ups=0.32, wpb=65363.4, bsz=127.7, num_updates=10900, lr=0.000302891, gnorm=1.005, loss_scale=16, train_wall=290, gb_free=19.9, wall=34669
2022-03-04 18:57:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:57:48 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 8.604 | nll_loss 8.402 | ppl 338.22 | wps 41344.2 | wpb 510.9 | bsz 1 | num_updates 10900 | best_loss 6.963
2022-03-04 18:57:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 10900 updates
2022-03-04 18:57:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:57:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 18:57:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 56 @ 10900 updates, score 8.604) (writing took 3.3578276559710503 seconds)
2022-03-04 18:57:51 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 18:57:51 | INFO | train | epoch 056 | loss 3.36 | nll_loss 3.102 | ppl 8.59 | wps 20628.6 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 10900 | lr 0.000302891 | gnorm 0.984 | loss_scale 16 | train_wall 563 | gb_free 19.9 | wall 34678
2022-03-04 18:57:51 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 18:57:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:02:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:03:05 | INFO | train_inner | epoch 057:    101 / 196 loss=3.285, nll_loss=3.026, ppl=8.14, wps=20354.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.975, loss_scale=16, train_wall=291, gb_free=19.9, wall=34991
2022-03-04 19:07:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:08:04 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 8.628 | nll_loss 8.426 | ppl 343.86 | wps 41434.5 | wpb 510.9 | bsz 1 | num_updates 11095 | best_loss 6.963
2022-03-04 19:08:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 11095 updates
2022-03-04 19:08:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:08:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:08:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 57 @ 11095 updates, score 8.628) (writing took 3.2637294630985707 seconds)
2022-03-04 19:08:07 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 19:08:07 | INFO | train | epoch 057 | loss 3.333 | nll_loss 3.074 | ppl 8.42 | wps 20734.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 11095 | lr 0.000300218 | gnorm 0.981 | loss_scale 16 | train_wall 563 | gb_free 19.9 | wall 35294
2022-03-04 19:08:07 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 19:08:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:08:23 | INFO | train_inner | epoch 058:      5 / 196 loss=3.373, nll_loss=3.115, ppl=8.66, wps=20555.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=11100, lr=0.00030015, gnorm=0.986, loss_scale=16, train_wall=287, gb_free=19.9, wall=35309
2022-03-04 19:09:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:13:38 | INFO | train_inner | epoch 058:    106 / 196 loss=3.265, nll_loss=3.004, ppl=8.02, wps=20808.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.969, loss_scale=16, train_wall=292, gb_free=19.9, wall=35624
2022-03-04 19:15:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:18:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:18:20 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 8.674 | nll_loss 8.47 | ppl 354.49 | wps 41561.3 | wpb 510.9 | bsz 1 | num_updates 11289 | best_loss 6.963
2022-03-04 19:18:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 11289 updates
2022-03-04 19:18:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:18:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:18:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 58 @ 11289 updates, score 8.674) (writing took 3.1300447860267013 seconds)
2022-03-04 19:18:24 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 19:18:24 | INFO | train | epoch 058 | loss 3.305 | nll_loss 3.045 | ppl 8.25 | wps 20590.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 11289 | lr 0.000297627 | gnorm 0.98 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 35910
2022-03-04 19:18:24 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 19:18:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:18:58 | INFO | train_inner | epoch 059:     11 / 196 loss=3.339, nll_loss=3.08, ppl=8.46, wps=20419.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=11300, lr=0.000297482, gnorm=0.992, loss_scale=16, train_wall=289, gb_free=19.9, wall=35944
2022-03-04 19:22:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:24:10 | INFO | train_inner | epoch 059:    112 / 196 loss=3.244, nll_loss=2.983, ppl=7.9, wps=20979.6, ups=0.32, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.99, loss_scale=16, train_wall=290, gb_free=19.9, wall=36257
2022-03-04 19:28:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:28:34 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 8.709 | nll_loss 8.507 | ppl 363.75 | wps 41567.5 | wpb 510.9 | bsz 1 | num_updates 11484 | best_loss 6.963
2022-03-04 19:28:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 11484 updates
2022-03-04 19:28:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:28:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:28:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 59 @ 11484 updates, score 8.709) (writing took 3.1712081499863416 seconds)
2022-03-04 19:28:38 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 19:28:38 | INFO | train | epoch 059 | loss 3.278 | nll_loss 3.018 | ppl 8.1 | wps 20784.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 11484 | lr 0.000295089 | gnorm 1 | loss_scale 16 | train_wall 562 | gb_free 19.9 | wall 36524
2022-03-04 19:28:38 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 19:28:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:29:27 | INFO | train_inner | epoch 060:     16 / 196 loss=3.303, nll_loss=3.043, ppl=8.24, wps=20614.9, ups=0.32, wpb=65359.9, bsz=127.7, num_updates=11500, lr=0.000294884, gnorm=1.01, loss_scale=16, train_wall=286, gb_free=19.9, wall=36574
2022-03-04 19:29:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:34:40 | INFO | train_inner | epoch 060:    117 / 196 loss=3.223, nll_loss=2.961, ppl=7.79, wps=20960.6, ups=0.32, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=1.016, loss_scale=16, train_wall=290, gb_free=19.9, wall=36886
2022-03-04 19:36:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:38:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:38:49 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 8.752 | nll_loss 8.549 | ppl 374.61 | wps 41603 | wpb 510.9 | bsz 1 | num_updates 11678 | best_loss 6.963
2022-03-04 19:38:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 11678 updates
2022-03-04 19:38:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:38:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:38:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 60 @ 11678 updates, score 8.752) (writing took 3.1776763978414237 seconds)
2022-03-04 19:38:52 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 19:38:52 | INFO | train | epoch 060 | loss 3.251 | nll_loss 2.989 | ppl 7.94 | wps 20662.3 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 11678 | lr 0.000292628 | gnorm 1.018 | loss_scale 8 | train_wall 563 | gb_free 19.9 | wall 37139
2022-03-04 19:38:52 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 19:38:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:40:00 | INFO | train_inner | epoch 061:     22 / 196 loss=3.267, nll_loss=3.006, ppl=8.03, wps=20396.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=11700, lr=0.000292353, gnorm=1.019, loss_scale=8, train_wall=290, gb_free=19.9, wall=37207
2022-03-04 19:45:10 | INFO | train_inner | epoch 061:    122 / 196 loss=3.203, nll_loss=2.94, ppl=7.67, wps=21149.8, ups=0.32, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=1.01, loss_scale=16, train_wall=288, gb_free=19.9, wall=37517
2022-03-04 19:48:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:49:04 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 8.798 | nll_loss 8.594 | ppl 386.29 | wps 41108.5 | wpb 510.9 | bsz 1 | num_updates 11874 | best_loss 6.963
2022-03-04 19:49:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 11874 updates
2022-03-04 19:49:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:49:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:49:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 61 @ 11874 updates, score 8.798) (writing took 3.148060705047101 seconds)
2022-03-04 19:49:07 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 19:49:07 | INFO | train | epoch 061 | loss 3.226 | nll_loss 2.964 | ppl 7.8 | wps 20855.1 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 11874 | lr 0.000290203 | gnorm 1.006 | loss_scale 16 | train_wall 563 | gb_free 19.9 | wall 37754
2022-03-04 19:49:07 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 19:49:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:49:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:50:31 | INFO | train_inner | epoch 062:     27 / 196 loss=3.24, nll_loss=2.978, ppl=7.88, wps=20379.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=11900, lr=0.000289886, gnorm=0.999, loss_scale=16, train_wall=290, gb_free=19.9, wall=37838
2022-03-04 19:55:41 | INFO | train_inner | epoch 062:    127 / 196 loss=3.181, nll_loss=2.917, ppl=7.55, wps=21141.8, ups=0.32, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=1.011, loss_scale=16, train_wall=288, gb_free=19.9, wall=38148
2022-03-04 19:56:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:59:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:59:19 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 8.843 | nll_loss 8.638 | ppl 398.49 | wps 41403 | wpb 510.9 | bsz 1 | num_updates 12068 | best_loss 6.963
2022-03-04 19:59:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 12068 updates
2022-03-04 19:59:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:59:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 19:59:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 62 @ 12068 updates, score 8.843) (writing took 3.0979367040563375 seconds)
2022-03-04 19:59:22 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 19:59:22 | INFO | train | epoch 062 | loss 3.2 | nll_loss 2.937 | ppl 7.66 | wps 20636.7 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 12068 | lr 0.000287861 | gnorm 1.01 | loss_scale 16 | train_wall 563 | gb_free 19.9 | wall 38369
2022-03-04 19:59:22 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 19:59:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:01:02 | INFO | train_inner | epoch 063:     32 / 196 loss=3.206, nll_loss=2.943, ppl=7.69, wps=20337, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=12100, lr=0.00028748, gnorm=1.016, loss_scale=16, train_wall=290, gb_free=19.9, wall=38469
2022-03-04 20:03:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:06:15 | INFO | train_inner | epoch 063:    133 / 196 loss=3.165, nll_loss=2.901, ppl=7.47, wps=20949.3, ups=0.32, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=1.013, loss_scale=16, train_wall=290, gb_free=19.9, wall=38782
2022-03-04 20:09:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:09:35 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 8.86 | nll_loss 8.657 | ppl 403.71 | wps 41464.7 | wpb 510.9 | bsz 1 | num_updates 12263 | best_loss 6.963
2022-03-04 20:09:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 12263 updates
2022-03-04 20:09:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:09:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:09:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 63 @ 12263 updates, score 8.86) (writing took 3.0821353930514306 seconds)
2022-03-04 20:09:38 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 20:09:38 | INFO | train | epoch 063 | loss 3.177 | nll_loss 2.913 | ppl 7.53 | wps 20735.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 12263 | lr 0.000285563 | gnorm 1.014 | loss_scale 16 | train_wall 563 | gb_free 19.9 | wall 38985
2022-03-04 20:09:38 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 20:09:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:09:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:11:36 | INFO | train_inner | epoch 064:     38 / 196 loss=3.177, nll_loss=2.913, ppl=7.53, wps=20392.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=12300, lr=0.000285133, gnorm=1.009, loss_scale=16, train_wall=290, gb_free=19.9, wall=39102
2022-03-04 20:16:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:16:48 | INFO | train_inner | epoch 064:    139 / 196 loss=3.143, nll_loss=2.878, ppl=7.35, wps=20966.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=12400, lr=0.000283981, gnorm=1.015, loss_scale=16, train_wall=290, gb_free=19.9, wall=39415
2022-03-04 20:19:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:19:49 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 8.899 | nll_loss 8.697 | ppl 414.99 | wps 41614.2 | wpb 510.9 | bsz 1 | num_updates 12457 | best_loss 6.963
2022-03-04 20:19:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 12457 updates
2022-03-04 20:19:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:19:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:19:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 64 @ 12457 updates, score 8.899) (writing took 3.115200900938362 seconds)
2022-03-04 20:19:52 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 20:19:52 | INFO | train | epoch 064 | loss 3.152 | nll_loss 2.887 | ppl 7.4 | wps 20666.3 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 12457 | lr 0.00028333 | gnorm 1.016 | loss_scale 16 | train_wall 563 | gb_free 19.9 | wall 39599
2022-03-04 20:19:52 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 20:19:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:22:06 | INFO | train_inner | epoch 065:     43 / 196 loss=3.151, nll_loss=2.887, ppl=7.39, wps=20599.1, ups=0.32, wpb=65367, bsz=127.7, num_updates=12500, lr=0.000282843, gnorm=1.027, loss_scale=16, train_wall=287, gb_free=19.9, wall=39732
2022-03-04 20:23:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:27:18 | INFO | train_inner | epoch 065:    144 / 196 loss=3.127, nll_loss=2.861, ppl=7.27, wps=20963.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=12600, lr=0.000281718, gnorm=1.027, loss_scale=16, train_wall=290, gb_free=19.9, wall=40045
2022-03-04 20:29:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:29:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:30:04 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 8.952 | nll_loss 8.748 | ppl 429.97 | wps 41758.9 | wpb 510.9 | bsz 1 | num_updates 12651 | best_loss 6.963
2022-03-04 20:30:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 12651 updates
2022-03-04 20:30:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:30:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:30:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 65 @ 12651 updates, score 8.952) (writing took 3.136321038007736 seconds)
2022-03-04 20:30:07 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 20:30:07 | INFO | train | epoch 065 | loss 3.129 | nll_loss 2.863 | ppl 7.28 | wps 20692.8 | ups 0.32 | wpb 65534.1 | bsz 128 | num_updates 12651 | lr 0.00028115 | gnorm 1.028 | loss_scale 16 | train_wall 563 | gb_free 19.9 | wall 40213
2022-03-04 20:30:07 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 20:30:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:32:38 | INFO | train_inner | epoch 066:     49 / 196 loss=3.117, nll_loss=2.851, ppl=7.21, wps=20472.7, ups=0.31, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=1.026, loss_scale=16, train_wall=289, gb_free=19.9, wall=40365
2022-03-04 20:36:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:37:51 | INFO | train_inner | epoch 066:    150 / 196 loss=3.106, nll_loss=2.84, ppl=7.16, wps=20987.3, ups=0.32, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=1.023, loss_scale=16, train_wall=290, gb_free=19.9, wall=40677
2022-03-04 20:40:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:40:17 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.02 | nll_loss 8.818 | ppl 451.3 | wps 41748.4 | wpb 510.9 | bsz 1 | num_updates 12846 | best_loss 6.963
2022-03-04 20:40:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 12846 updates
2022-03-04 20:40:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:40:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:40:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 66 @ 12846 updates, score 9.02) (writing took 3.1099169459193945 seconds)
2022-03-04 20:40:20 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 20:40:20 | INFO | train | epoch 066 | loss 3.107 | nll_loss 2.841 | ppl 7.16 | wps 20795.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 12846 | lr 0.000279008 | gnorm 1.026 | loss_scale 16 | train_wall 562 | gb_free 19.9 | wall 40827
2022-03-04 20:40:20 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 20:40:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:43:08 | INFO | train_inner | epoch 067:     54 / 196 loss=3.09, nll_loss=2.824, ppl=7.08, wps=20616.3, ups=0.32, wpb=65363.4, bsz=127.7, num_updates=12900, lr=0.000278423, gnorm=1.032, loss_scale=16, train_wall=286, gb_free=19.9, wall=40994
2022-03-04 20:43:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:45:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:48:24 | INFO | train_inner | epoch 067:    156 / 196 loss=3.096, nll_loss=2.83, ppl=7.11, wps=20734.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=13000, lr=0.00027735, gnorm=1.027, loss_scale=8, train_wall=293, gb_free=19.9, wall=41310
2022-03-04 20:50:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:50:33 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.036 | nll_loss 8.835 | ppl 456.74 | wps 40520.5 | wpb 510.9 | bsz 1 | num_updates 13040 | best_loss 6.963
2022-03-04 20:50:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 13040 updates
2022-03-04 20:50:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:50:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 20:50:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 67 @ 13040 updates, score 9.036) (writing took 3.12710321508348 seconds)
2022-03-04 20:50:36 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 20:50:36 | INFO | train | epoch 067 | loss 3.084 | nll_loss 2.818 | ppl 7.05 | wps 20627.2 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 13040 | lr 0.000276924 | gnorm 1.03 | loss_scale 8 | train_wall 563 | gb_free 19.9 | wall 41443
2022-03-04 20:50:36 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 20:50:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:53:43 | INFO | train_inner | epoch 068:     60 / 196 loss=3.065, nll_loss=2.798, ppl=6.95, wps=20468.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=13100, lr=0.000276289, gnorm=1.034, loss_scale=16, train_wall=288, gb_free=19.9, wall=41630
2022-03-04 20:58:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:58:57 | INFO | train_inner | epoch 068:    161 / 196 loss=3.075, nll_loss=2.808, ppl=7, wps=20873.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=1.052, loss_scale=16, train_wall=291, gb_free=19.9, wall=41944
2022-03-04 21:00:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:00:50 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.088 | nll_loss 8.885 | ppl 472.73 | wps 40796.7 | wpb 510.9 | bsz 1 | num_updates 13235 | best_loss 6.963
2022-03-04 21:00:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 13235 updates
2022-03-04 21:00:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:00:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:00:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 68 @ 13235 updates, score 9.088) (writing took 3.1110882749781013 seconds)
2022-03-04 21:00:54 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 21:00:54 | INFO | train | epoch 068 | loss 3.065 | nll_loss 2.797 | ppl 6.95 | wps 20662.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 13235 | lr 0.000274877 | gnorm 1.046 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 42060
2022-03-04 21:00:54 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 21:00:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:04:16 | INFO | train_inner | epoch 069:     65 / 196 loss=3.035, nll_loss=2.767, ppl=6.8, wps=20492.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=13300, lr=0.000274204, gnorm=1.04, loss_scale=16, train_wall=288, gb_free=19.9, wall=42263
2022-03-04 21:05:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:06:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:09:33 | INFO | train_inner | epoch 069:    167 / 196 loss=3.06, nll_loss=2.792, ppl=6.93, wps=20664.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=13400, lr=0.000273179, gnorm=1.055, loss_scale=8, train_wall=294, gb_free=19.9, wall=42580
2022-03-04 21:11:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:11:08 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.115 | nll_loss 8.911 | ppl 481.38 | wps 40736.3 | wpb 510.9 | bsz 1 | num_updates 13429 | best_loss 6.963
2022-03-04 21:11:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 13429 updates
2022-03-04 21:11:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:11:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:11:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 69 @ 13429 updates, score 9.115) (writing took 3.5480826860293746 seconds)
2022-03-04 21:11:11 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 21:11:11 | INFO | train | epoch 069 | loss 3.043 | nll_loss 2.775 | ppl 6.84 | wps 20549.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 13429 | lr 0.000272884 | gnorm 1.045 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 42678
2022-03-04 21:11:11 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 21:11:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:14:52 | INFO | train_inner | epoch 070:     71 / 196 loss=3.01, nll_loss=2.741, ppl=6.69, wps=20467.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=1.03, loss_scale=16, train_wall=288, gb_free=19.9, wall=42899
2022-03-04 21:19:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:20:07 | INFO | train_inner | epoch 070:    172 / 196 loss=3.05, nll_loss=2.781, ppl=6.87, wps=20859.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=13600, lr=0.000271163, gnorm=1.048, loss_scale=16, train_wall=292, gb_free=19.9, wall=43213
2022-03-04 21:21:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:21:25 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.159 | nll_loss 8.956 | ppl 496.45 | wps 41546.5 | wpb 510.9 | bsz 1 | num_updates 13624 | best_loss 6.963
2022-03-04 21:21:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 13624 updates
2022-03-04 21:21:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:21:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:21:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 70 @ 13624 updates, score 9.159) (writing took 3.0889333949889988 seconds)
2022-03-04 21:21:29 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 21:21:29 | INFO | train | epoch 070 | loss 3.024 | nll_loss 2.755 | ppl 6.75 | wps 20681.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 13624 | lr 0.000270924 | gnorm 1.038 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 43295
2022-03-04 21:21:29 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 21:21:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:25:24 | INFO | train_inner | epoch 071:     76 / 196 loss=2.982, nll_loss=2.712, ppl=6.55, wps=20611.6, ups=0.32, wpb=65363.4, bsz=127.7, num_updates=13700, lr=0.000270172, gnorm=1.036, loss_scale=16, train_wall=287, gb_free=19.9, wall=43530
2022-03-04 21:26:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:30:36 | INFO | train_inner | epoch 071:    177 / 196 loss=3.031, nll_loss=2.762, ppl=6.78, wps=20981.3, ups=0.32, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=1.053, loss_scale=16, train_wall=290, gb_free=19.9, wall=43843
2022-03-04 21:31:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:31:39 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.185 | nll_loss 8.982 | ppl 505.55 | wps 41614.9 | wpb 510.9 | bsz 1 | num_updates 13819 | best_loss 6.963
2022-03-04 21:31:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 13819 updates
2022-03-04 21:31:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:31:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:31:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 71 @ 13819 updates, score 9.185) (writing took 3.0325276302173734 seconds)
2022-03-04 21:31:42 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 21:31:42 | INFO | train | epoch 071 | loss 3.005 | nll_loss 2.736 | ppl 6.66 | wps 20787.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 13819 | lr 0.000269006 | gnorm 1.047 | loss_scale 16 | train_wall 562 | gb_free 19.9 | wall 43909
2022-03-04 21:31:43 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 21:31:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:33:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:35:56 | INFO | train_inner | epoch 072:     82 / 196 loss=2.962, nll_loss=2.692, ppl=6.46, wps=20412.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=1.037, loss_scale=16, train_wall=289, gb_free=19.9, wall=44163
2022-03-04 21:40:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:41:09 | INFO | train_inner | epoch 072:    183 / 196 loss=3.015, nll_loss=2.745, ppl=6.71, wps=20967.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=14000, lr=0.000267261, gnorm=1.062, loss_scale=16, train_wall=290, gb_free=19.9, wall=44475
2022-03-04 21:41:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:41:54 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.218 | nll_loss 9.013 | ppl 516.76 | wps 40635.8 | wpb 510.9 | bsz 1 | num_updates 14013 | best_loss 6.963
2022-03-04 21:41:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 14013 updates
2022-03-04 21:41:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:41:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:41:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 72 @ 14013 updates, score 9.218) (writing took 3.051221681991592 seconds)
2022-03-04 21:41:57 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 21:41:57 | INFO | train | epoch 072 | loss 2.984 | nll_loss 2.714 | ppl 6.56 | wps 20647.4 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 14013 | lr 0.000267137 | gnorm 1.049 | loss_scale 16 | train_wall 563 | gb_free 19.9 | wall 44524
2022-03-04 21:41:57 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 21:41:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:46:28 | INFO | train_inner | epoch 073:     87 / 196 loss=2.939, nll_loss=2.667, ppl=6.35, wps=20466.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=14100, lr=0.000266312, gnorm=1.059, loss_scale=16, train_wall=288, gb_free=19.9, wall=44795
2022-03-04 21:46:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:50:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:51:46 | INFO | train_inner | epoch 073:    189 / 196 loss=2.998, nll_loss=2.729, ppl=6.63, wps=20644.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=14200, lr=0.000265372, gnorm=1.069, loss_scale=8, train_wall=295, gb_free=19.9, wall=45112
2022-03-04 21:52:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:52:12 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.24 | nll_loss 9.039 | ppl 526.14 | wps 40715.5 | wpb 510.9 | bsz 1 | num_updates 14207 | best_loss 6.963
2022-03-04 21:52:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 14207 updates
2022-03-04 21:52:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:52:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 21:52:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 73 @ 14207 updates, score 9.24) (writing took 3.03775001806207 seconds)
2022-03-04 21:52:15 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 21:52:15 | INFO | train | epoch 073 | loss 2.965 | nll_loss 2.694 | ppl 6.47 | wps 20555.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14207 | lr 0.000265307 | gnorm 1.065 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 45142
2022-03-04 21:52:15 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 21:52:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:57:05 | INFO | train_inner | epoch 074:     93 / 196 loss=2.911, nll_loss=2.639, ppl=6.23, wps=20476.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=1.049, loss_scale=8, train_wall=288, gb_free=19.9, wall=45431
2022-03-04 21:59:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:02:19 | INFO | train_inner | epoch 074:    194 / 196 loss=2.991, nll_loss=2.721, ppl=6.59, wps=20838.8, ups=0.32, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=1.058, loss_scale=8, train_wall=292, gb_free=19.9, wall=45746
2022-03-04 22:02:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:02:30 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.277 | nll_loss 9.073 | ppl 538.57 | wps 40634.2 | wpb 510.9 | bsz 1 | num_updates 14402 | best_loss 6.963
2022-03-04 22:02:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 14402 updates
2022-03-04 22:02:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:02:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:02:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 74 @ 14402 updates, score 9.277) (writing took 3.1069025499746203 seconds)
2022-03-04 22:02:33 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 22:02:33 | INFO | train | epoch 074 | loss 2.948 | nll_loss 2.677 | ppl 6.39 | wps 20642.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 14402 | lr 0.000263505 | gnorm 1.053 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 45760
2022-03-04 22:02:33 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 22:02:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:07:39 | INFO | train_inner | epoch 075:     98 / 196 loss=2.878, nll_loss=2.605, ppl=6.08, wps=20477.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14500, lr=0.000262613, gnorm=1.051, loss_scale=16, train_wall=288, gb_free=19.9, wall=46065
2022-03-04 22:10:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:12:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:12:47 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.306 | nll_loss 9.104 | ppl 550.19 | wps 41435.9 | wpb 510.9 | bsz 1 | num_updates 14597 | best_loss 6.963
2022-03-04 22:12:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 14597 updates
2022-03-04 22:12:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:12:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:12:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 75 @ 14597 updates, score 9.306) (writing took 3.020156014012173 seconds)
2022-03-04 22:12:50 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-04 22:12:50 | INFO | train | epoch 075 | loss 2.93 | nll_loss 2.658 | ppl 6.31 | wps 20680.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 14597 | lr 0.000261739 | gnorm 1.057 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 46377
2022-03-04 22:12:51 | INFO | fairseq.trainer | begin training epoch 76
2022-03-04 22:12:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:13:00 | INFO | train_inner | epoch 076:      3 / 196 loss=2.981, nll_loss=2.711, ppl=6.55, wps=20342.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=14600, lr=0.000261712, gnorm=1.063, loss_scale=8, train_wall=290, gb_free=19.9, wall=46386
2022-03-04 22:18:10 | INFO | train_inner | epoch 076:    103 / 196 loss=2.877, nll_loss=2.604, ppl=6.08, wps=21137.1, ups=0.32, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=1.051, loss_scale=16, train_wall=288, gb_free=19.9, wall=46697
2022-03-04 22:22:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:23:04 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 9.366 | nll_loss 9.165 | ppl 574.22 | wps 40391.2 | wpb 510.9 | bsz 1 | num_updates 14793 | best_loss 6.963
2022-03-04 22:23:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 14793 updates
2022-03-04 22:23:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:23:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:23:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 76 @ 14793 updates, score 9.366) (writing took 3.0741357188671827 seconds)
2022-03-04 22:23:08 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-04 22:23:08 | INFO | train | epoch 076 | loss 2.914 | nll_loss 2.642 | ppl 6.24 | wps 20786.8 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 14793 | lr 0.000259999 | gnorm 1.073 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 46994
2022-03-04 22:23:08 | INFO | fairseq.trainer | begin training epoch 77
2022-03-04 22:23:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:23:30 | INFO | train_inner | epoch 077:      7 / 196 loss=2.946, nll_loss=2.674, ppl=6.38, wps=20450.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14800, lr=0.000259938, gnorm=1.098, loss_scale=16, train_wall=288, gb_free=19.9, wall=47016
2022-03-04 22:24:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:28:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:28:48 | INFO | train_inner | epoch 077:    109 / 196 loss=2.857, nll_loss=2.584, ppl=5.99, wps=20600.6, ups=0.31, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=1.074, loss_scale=8, train_wall=295, gb_free=19.9, wall=47334
2022-03-04 22:33:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:33:23 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 9.42 | nll_loss 9.216 | ppl 594.89 | wps 40715.2 | wpb 510.9 | bsz 1 | num_updates 14987 | best_loss 6.963
2022-03-04 22:33:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 14987 updates
2022-03-04 22:33:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:33:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:33:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 77 @ 14987 updates, score 9.42) (writing took 3.0612338799983263 seconds)
2022-03-04 22:33:26 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-04 22:33:26 | INFO | train | epoch 077 | loss 2.894 | nll_loss 2.621 | ppl 6.15 | wps 20518.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14987 | lr 0.000258311 | gnorm 1.084 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 47613
2022-03-04 22:33:26 | INFO | fairseq.trainer | begin training epoch 78
2022-03-04 22:33:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:34:07 | INFO | train_inner | epoch 078:     13 / 196 loss=2.921, nll_loss=2.649, ppl=6.27, wps=20473.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=1.084, loss_scale=8, train_wall=288, gb_free=19.9, wall=47654
2022-03-04 22:39:18 | INFO | train_inner | epoch 078:    113 / 196 loss=2.844, nll_loss=2.57, ppl=5.94, wps=21046.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=15100, lr=0.000257343, gnorm=1.069, loss_scale=16, train_wall=289, gb_free=19.9, wall=47965
2022-03-04 22:41:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:43:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:43:41 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 9.423 | nll_loss 9.22 | ppl 596.48 | wps 40426.6 | wpb 510.9 | bsz 1 | num_updates 15182 | best_loss 6.963
2022-03-04 22:43:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 15182 updates
2022-03-04 22:43:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:43:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:43:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 78 @ 15182 updates, score 9.423) (writing took 3.0298714109230787 seconds)
2022-03-04 22:43:44 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-04 22:43:44 | INFO | train | epoch 078 | loss 2.878 | nll_loss 2.605 | ppl 6.08 | wps 20654.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 15182 | lr 0.000256647 | gnorm 1.068 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 48231
2022-03-04 22:43:44 | INFO | fairseq.trainer | begin training epoch 79
2022-03-04 22:43:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:44:40 | INFO | train_inner | epoch 079:     18 / 196 loss=2.91, nll_loss=2.638, ppl=6.22, wps=20297.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15200, lr=0.000256495, gnorm=1.079, loss_scale=16, train_wall=291, gb_free=19.9, wall=48287
2022-03-04 22:48:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:49:55 | INFO | train_inner | epoch 079:    119 / 196 loss=2.833, nll_loss=2.558, ppl=5.89, wps=20853.7, ups=0.32, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=1.069, loss_scale=16, train_wall=292, gb_free=19.9, wall=48601
2022-03-04 22:53:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:53:59 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 9.463 | nll_loss 9.259 | ppl 612.75 | wps 40691.6 | wpb 510.9 | bsz 1 | num_updates 15377 | best_loss 6.963
2022-03-04 22:53:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 15377 updates
2022-03-04 22:53:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:54:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 22:54:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 79 @ 15377 updates, score 9.463) (writing took 3.0485238200053573 seconds)
2022-03-04 22:54:02 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-04 22:54:02 | INFO | train | epoch 079 | loss 2.863 | nll_loss 2.589 | ppl 6.02 | wps 20664.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 15377 | lr 0.000255014 | gnorm 1.073 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 48848
2022-03-04 22:54:02 | INFO | fairseq.trainer | begin training epoch 80
2022-03-04 22:54:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:55:13 | INFO | train_inner | epoch 080:     23 / 196 loss=2.887, nll_loss=2.614, ppl=6.12, wps=20517.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=1.064, loss_scale=16, train_wall=288, gb_free=19.9, wall=48920
2022-03-04 22:55:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:58:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:00:28 | INFO | train_inner | epoch 080:    125 / 196 loss=2.828, nll_loss=2.553, ppl=5.87, wps=20792.7, ups=0.32, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=1.061, loss_scale=8, train_wall=293, gb_free=19.9, wall=49235
2022-03-04 23:04:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:04:14 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 9.482 | nll_loss 9.281 | ppl 622.11 | wps 39608.1 | wpb 510.9 | bsz 1 | num_updates 15571 | best_loss 6.963
2022-03-04 23:04:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 15571 updates
2022-03-04 23:04:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:04:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:04:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 80 @ 15571 updates, score 9.482) (writing took 3.0900478071998805 seconds)
2022-03-04 23:04:17 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-04 23:04:17 | INFO | train | epoch 080 | loss 2.847 | nll_loss 2.573 | ppl 5.95 | wps 20636.3 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 15571 | lr 0.000253421 | gnorm 1.063 | loss_scale 8 | train_wall 563 | gb_free 19.9 | wall 49464
2022-03-04 23:04:17 | INFO | fairseq.trainer | begin training epoch 81
2022-03-04 23:04:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:05:48 | INFO | train_inner | epoch 081:     29 / 196 loss=2.856, nll_loss=2.582, ppl=5.99, wps=20478.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15600, lr=0.000253185, gnorm=1.073, loss_scale=16, train_wall=288, gb_free=19.9, wall=49554
2022-03-04 23:09:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:11:02 | INFO | train_inner | epoch 081:    130 / 196 loss=2.813, nll_loss=2.538, ppl=5.81, wps=20828.2, ups=0.32, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=1.073, loss_scale=8, train_wall=292, gb_free=19.9, wall=49869
2022-03-04 23:14:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:14:32 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 9.538 | nll_loss 9.336 | ppl 646.24 | wps 40509.4 | wpb 510.9 | bsz 1 | num_updates 15766 | best_loss 6.963
2022-03-04 23:14:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 15766 updates
2022-03-04 23:14:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:14:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:14:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 81 @ 15766 updates, score 9.538) (writing took 3.0753316371701658 seconds)
2022-03-04 23:14:35 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-04 23:14:35 | INFO | train | epoch 081 | loss 2.831 | nll_loss 2.557 | ppl 5.88 | wps 20641.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 15766 | lr 0.000251848 | gnorm 1.075 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 50082
2022-03-04 23:14:35 | INFO | fairseq.trainer | begin training epoch 82
2022-03-04 23:14:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:16:21 | INFO | train_inner | epoch 082:     34 / 196 loss=2.838, nll_loss=2.564, ppl=5.91, wps=20484.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=1.074, loss_scale=16, train_wall=288, gb_free=19.9, wall=50188
2022-03-04 23:16:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:21:34 | INFO | train_inner | epoch 082:    135 / 196 loss=2.809, nll_loss=2.534, ppl=5.79, wps=20934.2, ups=0.32, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=1.081, loss_scale=8, train_wall=291, gb_free=19.9, wall=50501
2022-03-04 23:24:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:24:49 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.546 | nll_loss 9.344 | ppl 649.99 | wps 40525.2 | wpb 510.9 | bsz 1 | num_updates 15961 | best_loss 6.963
2022-03-04 23:24:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 15961 updates
2022-03-04 23:24:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:24:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:24:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 82 @ 15961 updates, score 9.546) (writing took 3.0863270449917763 seconds)
2022-03-04 23:24:52 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-04 23:24:52 | INFO | train | epoch 082 | loss 2.815 | nll_loss 2.54 | ppl 5.82 | wps 20702.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 15961 | lr 0.000250305 | gnorm 1.088 | loss_scale 16 | train_wall 564 | gb_free 19.9 | wall 50698
2022-03-04 23:24:52 | INFO | fairseq.trainer | begin training epoch 83
2022-03-04 23:24:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:26:53 | INFO | train_inner | epoch 083:     39 / 196 loss=2.814, nll_loss=2.539, ppl=5.81, wps=20488.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=16000, lr=0.00025, gnorm=1.098, loss_scale=16, train_wall=288, gb_free=19.9, wall=50820
2022-03-04 23:27:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:32:06 | INFO | train_inner | epoch 083:    140 / 196 loss=2.796, nll_loss=2.52, ppl=5.74, wps=20929, ups=0.32, wpb=65532.4, bsz=128, num_updates=16100, lr=0.000249222, gnorm=1.098, loss_scale=8, train_wall=291, gb_free=19.9, wall=51133
2022-03-04 23:35:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:35:06 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 9.582 | nll_loss 9.381 | ppl 666.78 | wps 40645.4 | wpb 510.9 | bsz 1 | num_updates 16156 | best_loss 6.963
2022-03-04 23:35:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 16156 updates
2022-03-04 23:35:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:35:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:35:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 83 @ 16156 updates, score 9.582) (writing took 3.07319342601113 seconds)
2022-03-04 23:35:09 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-04 23:35:09 | INFO | train | epoch 083 | loss 2.801 | nll_loss 2.526 | ppl 5.76 | wps 20691.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 16156 | lr 0.00024879 | gnorm 1.095 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 51315
2022-03-04 23:35:09 | INFO | fairseq.trainer | begin training epoch 84
2022-03-04 23:35:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:37:26 | INFO | train_inner | epoch 084:     44 / 196 loss=2.797, nll_loss=2.521, ppl=5.74, wps=20480.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=1.086, loss_scale=16, train_wall=288, gb_free=19.9, wall=51452
2022-03-04 23:39:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:42:40 | INFO | train_inner | epoch 084:    145 / 196 loss=2.784, nll_loss=2.508, ppl=5.69, wps=20846.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=16300, lr=0.000247689, gnorm=1.086, loss_scale=8, train_wall=292, gb_free=19.9, wall=51767
2022-03-04 23:45:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:45:23 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 9.617 | nll_loss 9.414 | ppl 682.35 | wps 40749.4 | wpb 510.9 | bsz 1 | num_updates 16351 | best_loss 6.963
2022-03-04 23:45:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 16351 updates
2022-03-04 23:45:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:45:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:45:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 84 @ 16351 updates, score 9.617) (writing took 3.0931987438816577 seconds)
2022-03-04 23:45:26 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-04 23:45:26 | INFO | train | epoch 084 | loss 2.785 | nll_loss 2.509 | ppl 5.69 | wps 20657.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 16351 | lr 0.000247302 | gnorm 1.081 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 51933
2022-03-04 23:45:26 | INFO | fairseq.trainer | begin training epoch 85
2022-03-04 23:45:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:47:59 | INFO | train_inner | epoch 085:     49 / 196 loss=2.781, nll_loss=2.506, ppl=5.68, wps=20487.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=16400, lr=0.000246932, gnorm=1.079, loss_scale=16, train_wall=288, gb_free=19.9, wall=52086
2022-03-04 23:49:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:53:14 | INFO | train_inner | epoch 085:    150 / 196 loss=2.775, nll_loss=2.499, ppl=5.65, wps=20830.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=16500, lr=0.000246183, gnorm=1.113, loss_scale=8, train_wall=292, gb_free=19.9, wall=52400
2022-03-04 23:55:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:55:42 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 9.66 | nll_loss 9.459 | ppl 703.77 | wps 40606.3 | wpb 510.9 | bsz 1 | num_updates 16546 | best_loss 6.963
2022-03-04 23:55:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 16546 updates
2022-03-04 23:55:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:55:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-04 23:55:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 85 @ 16546 updates, score 9.66) (writing took 3.1072930691298097 seconds)
2022-03-04 23:55:45 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-04 23:55:45 | INFO | train | epoch 085 | loss 2.773 | nll_loss 2.496 | ppl 5.64 | wps 20638.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 16546 | lr 0.000245841 | gnorm 1.1 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 52551
2022-03-04 23:55:45 | INFO | fairseq.trainer | begin training epoch 86
2022-03-04 23:55:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:58:33 | INFO | train_inner | epoch 086:     54 / 196 loss=2.754, nll_loss=2.478, ppl=5.57, wps=20446.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=1.09, loss_scale=16, train_wall=289, gb_free=19.9, wall=52720
2022-03-05 00:00:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:03:48 | INFO | train_inner | epoch 086:    155 / 196 loss=2.768, nll_loss=2.492, ppl=5.62, wps=20812.1, ups=0.32, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=1.093, loss_scale=8, train_wall=292, gb_free=19.9, wall=53035
2022-03-05 00:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:06:01 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 9.681 | nll_loss 9.478 | ppl 713.29 | wps 40809.1 | wpb 510.9 | bsz 1 | num_updates 16741 | best_loss 6.963
2022-03-05 00:06:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 16741 updates
2022-03-05 00:06:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:06:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:06:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 86 @ 16741 updates, score 9.681) (writing took 3.0442493609152734 seconds)
2022-03-05 00:06:04 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-05 00:06:04 | INFO | train | epoch 086 | loss 2.757 | nll_loss 2.481 | ppl 5.58 | wps 20619 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 16741 | lr 0.000244405 | gnorm 1.095 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 53170
2022-03-05 00:06:04 | INFO | fairseq.trainer | begin training epoch 87
2022-03-05 00:06:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:09:08 | INFO | train_inner | epoch 087:     59 / 196 loss=2.742, nll_loss=2.465, ppl=5.52, wps=20467, ups=0.31, wpb=65367, bsz=127.7, num_updates=16800, lr=0.000243975, gnorm=1.095, loss_scale=16, train_wall=288, gb_free=19.9, wall=53354
2022-03-05 00:11:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:14:22 | INFO | train_inner | epoch 087:    160 / 196 loss=2.751, nll_loss=2.474, ppl=5.56, wps=20842.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=16900, lr=0.000243252, gnorm=1.104, loss_scale=8, train_wall=292, gb_free=19.9, wall=53669
2022-03-05 00:16:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:16:19 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 9.685 | nll_loss 9.483 | ppl 715.6 | wps 40551.8 | wpb 510.9 | bsz 1 | num_updates 16936 | best_loss 6.963
2022-03-05 00:16:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 16936 updates
2022-03-05 00:16:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:16:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:16:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 87 @ 16936 updates, score 9.685) (writing took 3.1000433249864727 seconds)
2022-03-05 00:16:22 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-05 00:16:22 | INFO | train | epoch 087 | loss 2.743 | nll_loss 2.466 | ppl 5.53 | wps 20648 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 16936 | lr 0.000242993 | gnorm 1.097 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 53788
2022-03-05 00:16:22 | INFO | fairseq.trainer | begin training epoch 88
2022-03-05 00:16:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:19:43 | INFO | train_inner | epoch 088:     64 / 196 loss=2.721, nll_loss=2.443, ppl=5.44, wps=20387.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=1.091, loss_scale=16, train_wall=289, gb_free=19.9, wall=53989
2022-03-05 00:23:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:24:56 | INFO | train_inner | epoch 088:    165 / 196 loss=2.747, nll_loss=2.469, ppl=5.54, wps=20939.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=1.109, loss_scale=8, train_wall=291, gb_free=19.9, wall=54302
2022-03-05 00:26:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:26:37 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 9.786 | nll_loss 9.584 | ppl 767.24 | wps 40820.5 | wpb 510.9 | bsz 1 | num_updates 17131 | best_loss 6.963
2022-03-05 00:26:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 17131 updates
2022-03-05 00:26:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:26:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:26:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 88 @ 17131 updates, score 9.786) (writing took 2.993531476939097 seconds)
2022-03-05 00:26:40 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-05 00:26:40 | INFO | train | epoch 088 | loss 2.73 | nll_loss 2.453 | ppl 5.47 | wps 20657.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 17131 | lr 0.000241607 | gnorm 1.101 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 54406
2022-03-05 00:26:40 | INFO | fairseq.trainer | begin training epoch 89
2022-03-05 00:26:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:30:14 | INFO | train_inner | epoch 089:     69 / 196 loss=2.704, nll_loss=2.426, ppl=5.37, wps=20558.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=17200, lr=0.000241121, gnorm=1.093, loss_scale=16, train_wall=287, gb_free=19.9, wall=54620
2022-03-05 00:30:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:35:27 | INFO | train_inner | epoch 089:    170 / 196 loss=2.739, nll_loss=2.462, ppl=5.51, wps=20890.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=17300, lr=0.000240424, gnorm=1.116, loss_scale=8, train_wall=291, gb_free=19.9, wall=54934
2022-03-05 00:36:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:36:53 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 9.769 | nll_loss 9.567 | ppl 758.4 | wps 40683.4 | wpb 510.9 | bsz 1 | num_updates 17326 | best_loss 6.963
2022-03-05 00:36:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 17326 updates
2022-03-05 00:36:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:36:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:36:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 89 @ 17326 updates, score 9.769) (writing took 3.0257261330261827 seconds)
2022-03-05 00:36:56 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-05 00:36:56 | INFO | train | epoch 089 | loss 2.717 | nll_loss 2.439 | ppl 5.42 | wps 20711.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 17326 | lr 0.000240243 | gnorm 1.101 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 55022
2022-03-05 00:36:56 | INFO | fairseq.trainer | begin training epoch 90
2022-03-05 00:36:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:40:46 | INFO | train_inner | epoch 090:     74 / 196 loss=2.68, nll_loss=2.402, ppl=5.28, wps=20488.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=1.097, loss_scale=16, train_wall=288, gb_free=19.9, wall=55253
2022-03-05 00:41:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:46:00 | INFO | train_inner | epoch 090:    175 / 196 loss=2.732, nll_loss=2.455, ppl=5.48, wps=20859.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=1.108, loss_scale=8, train_wall=292, gb_free=19.9, wall=55567
2022-03-05 00:47:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:47:10 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 9.817 | nll_loss 9.617 | ppl 785.41 | wps 40716 | wpb 510.9 | bsz 1 | num_updates 17521 | best_loss 6.963
2022-03-05 00:47:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 17521 updates
2022-03-05 00:47:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:47:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:47:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 90 @ 17521 updates, score 9.817) (writing took 3.0399160841479897 seconds)
2022-03-05 00:47:14 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-05 00:47:14 | INFO | train | epoch 090 | loss 2.704 | nll_loss 2.426 | ppl 5.37 | wps 20661.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 17521 | lr 0.000238902 | gnorm 1.108 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 55640
2022-03-05 00:47:14 | INFO | fairseq.trainer | begin training epoch 91
2022-03-05 00:47:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:51:20 | INFO | train_inner | epoch 091:     79 / 196 loss=2.676, nll_loss=2.397, ppl=5.27, wps=20481, ups=0.31, wpb=65367, bsz=127.7, num_updates=17600, lr=0.000238366, gnorm=1.109, loss_scale=16, train_wall=288, gb_free=19.9, wall=55886
2022-03-05 00:55:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:56:34 | INFO | train_inner | epoch 091:    180 / 196 loss=2.714, nll_loss=2.436, ppl=5.41, wps=20814.6, ups=0.32, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=1.099, loss_scale=16, train_wall=292, gb_free=19.9, wall=56201
2022-03-05 00:56:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:57:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:57:29 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 9.814 | nll_loss 9.613 | ppl 782.82 | wps 40610.3 | wpb 510.9 | bsz 1 | num_updates 17715 | best_loss 6.963
2022-03-05 00:57:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 17715 updates
2022-03-05 00:57:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:57:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 00:57:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 91 @ 17715 updates, score 9.814) (writing took 3.050125590991229 seconds)
2022-03-05 00:57:32 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-05 00:57:32 | INFO | train | epoch 091 | loss 2.69 | nll_loss 2.411 | ppl 5.32 | wps 20526.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 17715 | lr 0.000237591 | gnorm 1.102 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 56259
2022-03-05 00:57:32 | INFO | fairseq.trainer | begin training epoch 92
2022-03-05 00:57:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:01:57 | INFO | train_inner | epoch 092:     85 / 196 loss=2.65, nll_loss=2.371, ppl=5.17, wps=20266.5, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=1.119, loss_scale=8, train_wall=291, gb_free=19.9, wall=56524
2022-03-05 01:07:09 | INFO | train_inner | epoch 092:    185 / 196 loss=2.711, nll_loss=2.433, ppl=5.4, wps=21018.7, ups=0.32, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=1.133, loss_scale=16, train_wall=289, gb_free=19.9, wall=56835
2022-03-05 01:07:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:07:48 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 9.84 | nll_loss 9.639 | ppl 797.27 | wps 40287.6 | wpb 510.9 | bsz 1 | num_updates 17911 | best_loss 6.963
2022-03-05 01:07:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 17911 updates
2022-03-05 01:07:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:07:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:07:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 92 @ 17911 updates, score 9.84) (writing took 3.0427476388867944 seconds)
2022-03-05 01:07:51 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-05 01:07:51 | INFO | train | epoch 092 | loss 2.68 | nll_loss 2.401 | ppl 5.28 | wps 20730.9 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 17911 | lr 0.000236287 | gnorm 1.126 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 56877
2022-03-05 01:07:51 | INFO | fairseq.trainer | begin training epoch 93
2022-03-05 01:07:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:09:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:12:32 | INFO | train_inner | epoch 093:     90 / 196 loss=2.636, nll_loss=2.356, ppl=5.12, wps=20239.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18000, lr=0.000235702, gnorm=1.111, loss_scale=8, train_wall=292, gb_free=19.9, wall=57158
2022-03-05 01:17:43 | INFO | train_inner | epoch 093:    190 / 196 loss=2.701, nll_loss=2.422, ppl=5.36, wps=21071.9, ups=0.32, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=1.13, loss_scale=16, train_wall=289, gb_free=19.9, wall=57469
2022-03-05 01:18:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:18:06 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 9.854 | nll_loss 9.652 | ppl 804.28 | wps 40725.3 | wpb 510.9 | bsz 1 | num_updates 18106 | best_loss 6.963
2022-03-05 01:18:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 18106 updates
2022-03-05 01:18:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:18:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:18:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 93 @ 18106 updates, score 9.854) (writing took 2.9976467948872596 seconds)
2022-03-05 01:18:09 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-05 01:18:09 | INFO | train | epoch 093 | loss 2.665 | nll_loss 2.386 | ppl 5.23 | wps 20644.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 18106 | lr 0.000235011 | gnorm 1.118 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 57496
2022-03-05 01:18:09 | INFO | fairseq.trainer | begin training epoch 94
2022-03-05 01:18:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:20:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:23:05 | INFO | train_inner | epoch 094:     95 / 196 loss=2.618, nll_loss=2.337, ppl=5.05, wps=20301.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=1.104, loss_scale=8, train_wall=291, gb_free=19.9, wall=57791
2022-03-05 01:28:16 | INFO | train_inner | epoch 094:    195 / 196 loss=2.694, nll_loss=2.416, ppl=5.34, wps=21069.5, ups=0.32, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=1.111, loss_scale=16, train_wall=289, gb_free=19.9, wall=58102
2022-03-05 01:28:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:28:24 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 9.911 | nll_loss 9.71 | ppl 837.62 | wps 40627 | wpb 510.9 | bsz 1 | num_updates 18301 | best_loss 6.963
2022-03-05 01:28:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 18301 updates
2022-03-05 01:28:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:28:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:28:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 94 @ 18301 updates, score 9.911) (writing took 2.9808242609724402 seconds)
2022-03-05 01:28:27 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-05 01:28:27 | INFO | train | epoch 094 | loss 2.654 | nll_loss 2.375 | ppl 5.19 | wps 20668.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 18301 | lr 0.000233756 | gnorm 1.108 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 58113
2022-03-05 01:28:27 | INFO | fairseq.trainer | begin training epoch 95
2022-03-05 01:28:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:33:35 | INFO | train_inner | epoch 095:     99 / 196 loss=2.599, nll_loss=2.318, ppl=4.99, wps=20494.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18400, lr=0.000233126, gnorm=1.099, loss_scale=16, train_wall=288, gb_free=19.9, wall=58421
2022-03-05 01:34:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:34:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:38:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:38:41 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 9.95 | nll_loss 9.749 | ppl 860.51 | wps 40954.4 | wpb 510.9 | bsz 1 | num_updates 18495 | best_loss 6.963
2022-03-05 01:38:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 18495 updates
2022-03-05 01:38:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:38:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:38:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 95 @ 18495 updates, score 9.95) (writing took 3.0131587728392333 seconds)
2022-03-05 01:38:44 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-05 01:38:44 | INFO | train | epoch 095 | loss 2.644 | nll_loss 2.364 | ppl 5.15 | wps 20555.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 18495 | lr 0.000232527 | gnorm 1.113 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 58731
2022-03-05 01:38:44 | INFO | fairseq.trainer | begin training epoch 96
2022-03-05 01:38:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:39:00 | INFO | train_inner | epoch 096:      5 / 196 loss=2.684, nll_loss=2.405, ppl=5.3, wps=20100.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=18500, lr=0.000232495, gnorm=1.129, loss_scale=8, train_wall=294, gb_free=19.9, wall=58747
2022-03-05 01:41:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:44:14 | INFO | train_inner | epoch 096:    106 / 196 loss=2.593, nll_loss=2.312, ppl=4.96, wps=20850.6, ups=0.32, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=1.117, loss_scale=8, train_wall=292, gb_free=19.9, wall=59061
2022-03-05 01:48:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:48:59 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 9.973 | nll_loss 9.77 | ppl 873.15 | wps 40509.4 | wpb 510.9 | bsz 1 | num_updates 18690 | best_loss 6.963
2022-03-05 01:48:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 18690 updates
2022-03-05 01:48:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:49:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:49:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 96 @ 18690 updates, score 9.973) (writing took 3.0263766839634627 seconds)
2022-03-05 01:49:02 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-05 01:49:02 | INFO | train | epoch 096 | loss 2.631 | nll_loss 2.351 | ppl 5.1 | wps 20656.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 18690 | lr 0.000231311 | gnorm 1.122 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 59349
2022-03-05 01:49:02 | INFO | fairseq.trainer | begin training epoch 97
2022-03-05 01:49:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:49:33 | INFO | train_inner | epoch 097:     10 / 196 loss=2.663, nll_loss=2.384, ppl=5.22, wps=20485.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=1.127, loss_scale=16, train_wall=288, gb_free=19.9, wall=59380
2022-03-05 01:49:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:54:48 | INFO | train_inner | epoch 097:    111 / 196 loss=2.587, nll_loss=2.306, ppl=4.94, wps=20844.7, ups=0.32, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=1.103, loss_scale=8, train_wall=292, gb_free=19.9, wall=59694
2022-03-05 01:57:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:59:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:59:17 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 9.98 | nll_loss 9.778 | ppl 877.84 | wps 40814.1 | wpb 510.9 | bsz 1 | num_updates 18884 | best_loss 6.963
2022-03-05 01:59:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 18884 updates
2022-03-05 01:59:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:59:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 01:59:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 97 @ 18884 updates, score 9.98) (writing took 3.0165778379887342 seconds)
2022-03-05 01:59:20 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-05 01:59:20 | INFO | train | epoch 097 | loss 2.619 | nll_loss 2.338 | ppl 5.06 | wps 20551.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 18884 | lr 0.000230119 | gnorm 1.115 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 59966
2022-03-05 01:59:20 | INFO | fairseq.trainer | begin training epoch 98
2022-03-05 01:59:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:00:10 | INFO | train_inner | epoch 098:     16 / 196 loss=2.647, nll_loss=2.367, ppl=5.16, wps=20294.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18900, lr=0.000230022, gnorm=1.13, loss_scale=8, train_wall=291, gb_free=19.9, wall=60016
2022-03-05 02:05:21 | INFO | train_inner | epoch 098:    116 / 196 loss=2.58, nll_loss=2.299, ppl=4.92, wps=21025.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=19000, lr=0.000229416, gnorm=1.121, loss_scale=16, train_wall=289, gb_free=19.9, wall=60328
2022-03-05 02:06:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:09:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:09:35 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 9.999 | nll_loss 9.796 | ppl 889.06 | wps 40524.9 | wpb 510.9 | bsz 1 | num_updates 19079 | best_loss 6.963
2022-03-05 02:09:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 19079 updates
2022-03-05 02:09:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:09:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:09:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 98 @ 19079 updates, score 9.999) (writing took 2.979590089060366 seconds)
2022-03-05 02:09:38 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-05 02:09:38 | INFO | train | epoch 098 | loss 2.609 | nll_loss 2.328 | ppl 5.02 | wps 20648.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 19079 | lr 0.00022894 | gnorm 1.129 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 60585
2022-03-05 02:09:38 | INFO | fairseq.trainer | begin training epoch 99
2022-03-05 02:09:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:10:43 | INFO | train_inner | epoch 099:     21 / 196 loss=2.631, nll_loss=2.351, ppl=5.1, wps=20302.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=1.122, loss_scale=8, train_wall=291, gb_free=19.9, wall=60650
2022-03-05 02:13:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:15:58 | INFO | train_inner | epoch 099:    122 / 196 loss=2.583, nll_loss=2.301, ppl=4.93, wps=20859.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=19200, lr=0.000228218, gnorm=1.12, loss_scale=8, train_wall=292, gb_free=19.9, wall=60964
2022-03-05 02:19:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:19:52 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 10.047 | nll_loss 9.847 | ppl 920.68 | wps 40828.1 | wpb 510.9 | bsz 1 | num_updates 19274 | best_loss 6.963
2022-03-05 02:19:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 19274 updates
2022-03-05 02:19:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:19:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:19:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 99 @ 19274 updates, score 10.047) (writing took 2.9753503219690174 seconds)
2022-03-05 02:19:55 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-05 02:19:55 | INFO | train | epoch 099 | loss 2.598 | nll_loss 2.317 | ppl 4.98 | wps 20667.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 19274 | lr 0.000227779 | gnorm 1.123 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 61202
2022-03-05 02:19:55 | INFO | fairseq.trainer | begin training epoch 100
2022-03-05 02:19:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:21:16 | INFO | train_inner | epoch 100:     26 / 196 loss=2.607, nll_loss=2.326, ppl=5.01, wps=20500.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=19300, lr=0.000227626, gnorm=1.132, loss_scale=16, train_wall=288, gb_free=19.9, wall=61283
2022-03-05 02:21:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:26:31 | INFO | train_inner | epoch 100:    127 / 196 loss=2.575, nll_loss=2.293, ppl=4.9, wps=20855.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=19400, lr=0.000227038, gnorm=1.15, loss_scale=8, train_wall=292, gb_free=19.9, wall=61597
2022-03-05 02:30:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:30:10 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 10.031 | nll_loss 9.831 | ppl 910.64 | wps 40344.2 | wpb 510.9 | bsz 1 | num_updates 19469 | best_loss 6.963
2022-03-05 02:30:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 19469 updates
2022-03-05 02:30:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:30:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:30:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 100 @ 19469 updates, score 10.031) (writing took 3.01855832291767 seconds)
2022-03-05 02:30:13 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-05 02:30:13 | INFO | train | epoch 100 | loss 2.587 | nll_loss 2.306 | ppl 4.95 | wps 20666.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 19469 | lr 0.000226636 | gnorm 1.137 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 61820
2022-03-05 02:30:13 | INFO | fairseq.trainer | begin training epoch 101
2022-03-05 02:30:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:31:50 | INFO | train_inner | epoch 101:     31 / 196 loss=2.594, nll_loss=2.312, ppl=4.97, wps=20488.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=1.123, loss_scale=16, train_wall=288, gb_free=19.9, wall=61916
2022-03-05 02:34:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:37:04 | INFO | train_inner | epoch 101:    132 / 196 loss=2.57, nll_loss=2.287, ppl=4.88, wps=20827.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=19600, lr=0.000225877, gnorm=1.135, loss_scale=16, train_wall=292, gb_free=19.9, wall=62231
2022-03-05 02:39:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:40:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:40:28 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 10.066 | nll_loss 9.865 | ppl 932.69 | wps 40508.8 | wpb 510.9 | bsz 1 | num_updates 19663 | best_loss 6.963
2022-03-05 02:40:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 19663 updates
2022-03-05 02:40:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:40:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:40:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 101 @ 19663 updates, score 10.066) (writing took 3.0720109110698104 seconds)
2022-03-05 02:40:31 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-05 02:40:31 | INFO | train | epoch 101 | loss 2.577 | nll_loss 2.295 | ppl 4.91 | wps 20535.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19663 | lr 0.000225515 | gnorm 1.131 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 62438
2022-03-05 02:40:31 | INFO | fairseq.trainer | begin training epoch 102
2022-03-05 02:40:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:42:26 | INFO | train_inner | epoch 102:     37 / 196 loss=2.58, nll_loss=2.299, ppl=4.92, wps=20289.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=19700, lr=0.000225303, gnorm=1.124, loss_scale=8, train_wall=291, gb_free=19.9, wall=62553
2022-03-05 02:45:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:47:41 | INFO | train_inner | epoch 102:    138 / 196 loss=2.556, nll_loss=2.274, ppl=4.84, wps=20843.7, ups=0.32, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=1.124, loss_scale=8, train_wall=292, gb_free=19.9, wall=62868
2022-03-05 02:50:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:50:46 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 10.116 | nll_loss 9.915 | ppl 965.11 | wps 40517 | wpb 510.9 | bsz 1 | num_updates 19858 | best_loss 6.963
2022-03-05 02:50:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 19858 updates
2022-03-05 02:50:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:50:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 02:50:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 102 @ 19858 updates, score 10.116) (writing took 3.1045933400746435 seconds)
2022-03-05 02:50:49 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-05 02:50:49 | INFO | train | epoch 102 | loss 2.566 | nll_loss 2.284 | ppl 4.87 | wps 20652.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 19858 | lr 0.000224405 | gnorm 1.12 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 63056
2022-03-05 02:50:49 | INFO | fairseq.trainer | begin training epoch 103
2022-03-05 02:50:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:52:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:53:03 | INFO | train_inner | epoch 103:     43 / 196 loss=2.572, nll_loss=2.29, ppl=4.89, wps=20290.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=1.126, loss_scale=8, train_wall=291, gb_free=19.9, wall=63190
2022-03-05 02:58:14 | INFO | train_inner | epoch 103:    143 / 196 loss=2.554, nll_loss=2.271, ppl=4.83, wps=21067.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=20000, lr=0.000223607, gnorm=1.13, loss_scale=8, train_wall=289, gb_free=19.9, wall=63501
2022-03-05 03:00:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:00:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:01:03 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 10.126 | nll_loss 9.924 | ppl 971.26 | wps 40509.6 | wpb 510.9 | bsz 1 | num_updates 20052 | best_loss 6.963
2022-03-05 03:01:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 20052 updates
2022-03-05 03:01:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:01:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:01:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 103 @ 20052 updates, score 10.126) (writing took 2.9971108459867537 seconds)
2022-03-05 03:01:06 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-05 03:01:06 | INFO | train | epoch 103 | loss 2.556 | nll_loss 2.274 | ppl 4.84 | wps 20568.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 20052 | lr 0.000223317 | gnorm 1.138 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 63673
2022-03-05 03:01:07 | INFO | fairseq.trainer | begin training epoch 104
2022-03-05 03:01:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:03:36 | INFO | train_inner | epoch 104:     48 / 196 loss=2.548, nll_loss=2.266, ppl=4.81, wps=20317.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=20100, lr=0.00022305, gnorm=1.145, loss_scale=8, train_wall=291, gb_free=19.9, wall=63822
2022-03-05 03:07:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:08:50 | INFO | train_inner | epoch 104:    149 / 196 loss=2.552, nll_loss=2.269, ppl=4.82, wps=20865.8, ups=0.32, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=1.136, loss_scale=8, train_wall=292, gb_free=19.9, wall=64137
2022-03-05 03:11:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:11:21 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 10.151 | nll_loss 9.95 | ppl 988.94 | wps 40703.2 | wpb 510.9 | bsz 1 | num_updates 20247 | best_loss 6.963
2022-03-05 03:11:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 20247 updates
2022-03-05 03:11:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:11:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:11:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 104 @ 20247 updates, score 10.151) (writing took 3.0233959769830108 seconds)
2022-03-05 03:11:24 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-05 03:11:24 | INFO | train | epoch 104 | loss 2.547 | nll_loss 2.265 | ppl 4.8 | wps 20674.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 20247 | lr 0.000222239 | gnorm 1.14 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 64290
2022-03-05 03:11:24 | INFO | fairseq.trainer | begin training epoch 105
2022-03-05 03:11:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:14:09 | INFO | train_inner | epoch 105:     53 / 196 loss=2.532, nll_loss=2.249, ppl=4.76, wps=20497.3, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=1.141, loss_scale=8, train_wall=288, gb_free=19.9, wall=64455
2022-03-05 03:16:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:19:23 | INFO | train_inner | epoch 105:    154 / 196 loss=2.545, nll_loss=2.263, ppl=4.8, wps=20859.3, ups=0.32, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=1.146, loss_scale=8, train_wall=292, gb_free=19.9, wall=64770
2022-03-05 03:21:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:21:39 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 10.175 | nll_loss 9.975 | ppl 1006.56 | wps 40433.4 | wpb 510.9 | bsz 1 | num_updates 20442 | best_loss 6.963
2022-03-05 03:21:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 20442 updates
2022-03-05 03:21:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:21:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:21:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 105 @ 20442 updates, score 10.175) (writing took 2.957646880997345 seconds)
2022-03-05 03:21:41 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-05 03:21:41 | INFO | train | epoch 105 | loss 2.535 | nll_loss 2.253 | ppl 4.77 | wps 20661 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 20442 | lr 0.000221176 | gnorm 1.138 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 64908
2022-03-05 03:21:42 | INFO | fairseq.trainer | begin training epoch 106
2022-03-05 03:21:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:24:42 | INFO | train_inner | epoch 106:     58 / 196 loss=2.525, nll_loss=2.242, ppl=4.73, wps=20467.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=20500, lr=0.000220863, gnorm=1.134, loss_scale=16, train_wall=288, gb_free=19.9, wall=65089
2022-03-05 03:27:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:29:57 | INFO | train_inner | epoch 106:    159 / 196 loss=2.531, nll_loss=2.248, ppl=4.75, wps=20833.1, ups=0.32, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=1.148, loss_scale=8, train_wall=292, gb_free=19.9, wall=65404
2022-03-05 03:31:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:31:57 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 10.199 | nll_loss 9.999 | ppl 1023.55 | wps 40452.5 | wpb 510.9 | bsz 1 | num_updates 20637 | best_loss 6.963
2022-03-05 03:31:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 20637 updates
2022-03-05 03:31:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:32:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:32:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 106 @ 20637 updates, score 10.199) (writing took 2.9901495748199522 seconds)
2022-03-05 03:32:00 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-05 03:32:00 | INFO | train | epoch 106 | loss 2.527 | nll_loss 2.244 | ppl 4.74 | wps 20639.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 20637 | lr 0.000220129 | gnorm 1.147 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 65526
2022-03-05 03:32:00 | INFO | fairseq.trainer | begin training epoch 107
2022-03-05 03:32:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:35:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:35:18 | INFO | train_inner | epoch 107:     64 / 196 loss=2.511, nll_loss=2.227, ppl=4.68, wps=20362, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=1.154, loss_scale=8, train_wall=290, gb_free=19.9, wall=65725
2022-03-05 03:40:29 | INFO | train_inner | epoch 107:    164 / 196 loss=2.532, nll_loss=2.249, ppl=4.75, wps=21039.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=20800, lr=0.000219265, gnorm=1.139, loss_scale=8, train_wall=289, gb_free=19.9, wall=66036
2022-03-05 03:42:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:42:14 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 10.26 | nll_loss 10.057 | ppl 1065.56 | wps 40771.2 | wpb 510.9 | bsz 1 | num_updates 20832 | best_loss 6.963
2022-03-05 03:42:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 20832 updates
2022-03-05 03:42:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:42:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:42:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 107 @ 20832 updates, score 10.26) (writing took 2.9775569720659405 seconds)
2022-03-05 03:42:17 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-05 03:42:17 | INFO | train | epoch 107 | loss 2.518 | nll_loss 2.234 | ppl 4.71 | wps 20693.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 20832 | lr 0.000219096 | gnorm 1.146 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 66143
2022-03-05 03:42:17 | INFO | fairseq.trainer | begin training epoch 108
2022-03-05 03:42:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:45:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:45:51 | INFO | train_inner | epoch 108:     69 / 196 loss=2.495, nll_loss=2.211, ppl=4.63, wps=20309.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=20900, lr=0.000218739, gnorm=1.131, loss_scale=8, train_wall=291, gb_free=19.9, wall=66358
2022-03-05 03:51:02 | INFO | train_inner | epoch 108:    169 / 196 loss=2.529, nll_loss=2.246, ppl=4.74, wps=21074, ups=0.32, wpb=65532.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=1.148, loss_scale=8, train_wall=289, gb_free=19.9, wall=66669
2022-03-05 03:52:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:52:31 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 10.236 | nll_loss 10.035 | ppl 1049.08 | wps 40651.5 | wpb 510.9 | bsz 1 | num_updates 21027 | best_loss 6.963
2022-03-05 03:52:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 21027 updates
2022-03-05 03:52:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:52:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 03:52:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 108 @ 21027 updates, score 10.236) (writing took 3.020078263944015 seconds)
2022-03-05 03:52:34 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-05 03:52:34 | INFO | train | epoch 108 | loss 2.508 | nll_loss 2.224 | ppl 4.67 | wps 20670.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 21027 | lr 0.000218078 | gnorm 1.136 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 66761
2022-03-05 03:52:34 | INFO | fairseq.trainer | begin training epoch 109
2022-03-05 03:52:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:56:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:56:24 | INFO | train_inner | epoch 109:     74 / 196 loss=2.479, nll_loss=2.194, ppl=4.58, wps=20300.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=1.124, loss_scale=8, train_wall=291, gb_free=19.9, wall=66991
2022-03-05 04:01:35 | INFO | train_inner | epoch 109:    174 / 196 loss=2.525, nll_loss=2.242, ppl=4.73, wps=21068, ups=0.32, wpb=65532.4, bsz=128, num_updates=21200, lr=0.000217186, gnorm=1.162, loss_scale=8, train_wall=289, gb_free=19.9, wall=67302
2022-03-05 04:02:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:02:48 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 10.258 | nll_loss 10.058 | ppl 1066.25 | wps 40761.6 | wpb 510.9 | bsz 1 | num_updates 21222 | best_loss 6.963
2022-03-05 04:02:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 21222 updates
2022-03-05 04:02:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:02:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:02:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 109 @ 21222 updates, score 10.258) (writing took 3.0307080650236458 seconds)
2022-03-05 04:02:51 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-05 04:02:51 | INFO | train | epoch 109 | loss 2.499 | nll_loss 2.215 | ppl 4.64 | wps 20671.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 21222 | lr 0.000217074 | gnorm 1.144 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 67378
2022-03-05 04:02:51 | INFO | fairseq.trainer | begin training epoch 110
2022-03-05 04:02:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:04:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:06:57 | INFO | train_inner | epoch 110:     79 / 196 loss=2.466, nll_loss=2.181, ppl=4.54, wps=20304.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=21300, lr=0.000216676, gnorm=1.16, loss_scale=8, train_wall=291, gb_free=19.9, wall=67624
2022-03-05 04:12:08 | INFO | train_inner | epoch 110:    179 / 196 loss=2.518, nll_loss=2.234, ppl=4.71, wps=21057.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=21400, lr=0.000216169, gnorm=1.141, loss_scale=16, train_wall=289, gb_free=19.9, wall=67935
2022-03-05 04:12:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:13:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:13:06 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 10.307 | nll_loss 10.106 | ppl 1101.74 | wps 40685.2 | wpb 510.9 | bsz 1 | num_updates 21416 | best_loss 6.963
2022-03-05 04:13:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 21416 updates
2022-03-05 04:13:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:13:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:13:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 110 @ 21416 updates, score 10.307) (writing took 3.048970723990351 seconds)
2022-03-05 04:13:09 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-05 04:13:09 | INFO | train | epoch 110 | loss 2.49 | nll_loss 2.206 | ppl 4.61 | wps 20555.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 21416 | lr 0.000216088 | gnorm 1.153 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 67996
2022-03-05 04:13:09 | INFO | fairseq.trainer | begin training epoch 111
2022-03-05 04:13:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:17:31 | INFO | train_inner | epoch 111:     84 / 196 loss=2.459, nll_loss=2.174, ppl=4.51, wps=20259.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=1.138, loss_scale=8, train_wall=291, gb_free=19.9, wall=68258
2022-03-05 04:22:43 | INFO | train_inner | epoch 111:    184 / 196 loss=2.506, nll_loss=2.222, ppl=4.67, wps=21037.9, ups=0.32, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=1.134, loss_scale=16, train_wall=289, gb_free=19.9, wall=68569
2022-03-05 04:23:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:23:25 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 10.316 | nll_loss 10.115 | ppl 1108.84 | wps 40420.6 | wpb 510.9 | bsz 1 | num_updates 21612 | best_loss 6.963
2022-03-05 04:23:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 21612 updates
2022-03-05 04:23:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:23:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:23:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 111 @ 21612 updates, score 10.316) (writing took 3.0259415039326996 seconds)
2022-03-05 04:23:28 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-05 04:23:28 | INFO | train | epoch 111 | loss 2.482 | nll_loss 2.197 | ppl 4.59 | wps 20736.5 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 21612 | lr 0.000215106 | gnorm 1.136 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 68614
2022-03-05 04:23:28 | INFO | fairseq.trainer | begin training epoch 112
2022-03-05 04:23:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:23:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:28:05 | INFO | train_inner | epoch 112:     89 / 196 loss=2.445, nll_loss=2.16, ppl=4.47, wps=20285.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21700, lr=0.000214669, gnorm=1.141, loss_scale=8, train_wall=291, gb_free=19.9, wall=68891
2022-03-05 04:30:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:33:19 | INFO | train_inner | epoch 112:    190 / 196 loss=2.5, nll_loss=2.216, ppl=4.65, wps=20847.3, ups=0.32, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=1.147, loss_scale=8, train_wall=292, gb_free=19.9, wall=69206
2022-03-05 04:33:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:33:42 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 10.335 | nll_loss 10.134 | ppl 1124.01 | wps 40746.1 | wpb 510.9 | bsz 1 | num_updates 21806 | best_loss 6.963
2022-03-05 04:33:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 21806 updates
2022-03-05 04:33:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:33:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:33:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 112 @ 21806 updates, score 10.335) (writing took 3.038301110966131 seconds)
2022-03-05 04:33:45 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-05 04:33:45 | INFO | train | epoch 112 | loss 2.47 | nll_loss 2.186 | ppl 4.55 | wps 20549.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 21806 | lr 0.000214147 | gnorm 1.142 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 69232
2022-03-05 04:33:46 | INFO | fairseq.trainer | begin training epoch 113
2022-03-05 04:33:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:38:38 | INFO | train_inner | epoch 113:     94 / 196 loss=2.435, nll_loss=2.149, ppl=4.43, wps=20480.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21900, lr=0.000213687, gnorm=1.139, loss_scale=16, train_wall=288, gb_free=19.9, wall=69525
2022-03-05 04:39:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:43:53 | INFO | train_inner | epoch 113:    195 / 196 loss=2.501, nll_loss=2.218, ppl=4.65, wps=20843.7, ups=0.32, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=1.174, loss_scale=8, train_wall=292, gb_free=19.9, wall=69839
2022-03-05 04:43:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:44:00 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 10.367 | nll_loss 10.166 | ppl 1149.14 | wps 40672.1 | wpb 510.9 | bsz 1 | num_updates 22001 | best_loss 6.963
2022-03-05 04:44:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 22001 updates
2022-03-05 04:44:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:44:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:44:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 113 @ 22001 updates, score 10.367) (writing took 3.007390320999548 seconds)
2022-03-05 04:44:03 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-05 04:44:03 | INFO | train | epoch 113 | loss 2.465 | nll_loss 2.18 | ppl 4.53 | wps 20651.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22001 | lr 0.000213196 | gnorm 1.157 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 69850
2022-03-05 04:44:04 | INFO | fairseq.trainer | begin training epoch 114
2022-03-05 04:44:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:46:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:49:15 | INFO | train_inner | epoch 114:    100 / 196 loss=2.418, nll_loss=2.132, ppl=4.38, wps=20297.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=22100, lr=0.000212718, gnorm=1.163, loss_scale=8, train_wall=291, gb_free=19.9, wall=70161
2022-03-05 04:53:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:54:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:54:18 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 10.383 | nll_loss 10.182 | ppl 1161.76 | wps 40456.8 | wpb 510.9 | bsz 1 | num_updates 22195 | best_loss 6.963
2022-03-05 04:54:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 22195 updates
2022-03-05 04:54:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:54:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 04:54:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 114 @ 22195 updates, score 10.383) (writing took 3.0126355171669275 seconds)
2022-03-05 04:54:21 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-05 04:54:21 | INFO | train | epoch 114 | loss 2.456 | nll_loss 2.17 | ppl 4.5 | wps 20558.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22195 | lr 0.000212262 | gnorm 1.159 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 70468
2022-03-05 04:54:21 | INFO | fairseq.trainer | begin training epoch 115
2022-03-05 04:54:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:54:37 | INFO | train_inner | epoch 115:      5 / 196 loss=2.487, nll_loss=2.202, ppl=4.6, wps=20300.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22200, lr=0.000212238, gnorm=1.155, loss_scale=8, train_wall=291, gb_free=19.9, wall=70483
2022-03-05 04:59:48 | INFO | train_inner | epoch 115:    105 / 196 loss=2.411, nll_loss=2.125, ppl=4.36, wps=21063.8, ups=0.32, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=1.145, loss_scale=8, train_wall=289, gb_free=19.9, wall=70794
2022-03-05 05:03:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:04:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:04:36 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 10.38 | nll_loss 10.18 | ppl 1160.28 | wps 40330 | wpb 510.9 | bsz 1 | num_updates 22390 | best_loss 6.963
2022-03-05 05:04:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 22390 updates
2022-03-05 05:04:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:04:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:04:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 115 @ 22390 updates, score 10.38) (writing took 3.041559058940038 seconds)
2022-03-05 05:04:39 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-05 05:04:39 | INFO | train | epoch 115 | loss 2.447 | nll_loss 2.162 | ppl 4.47 | wps 20646.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22390 | lr 0.000211336 | gnorm 1.155 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 71086
2022-03-05 05:04:39 | INFO | fairseq.trainer | begin training epoch 116
2022-03-05 05:04:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:05:10 | INFO | train_inner | epoch 116:     10 / 196 loss=2.484, nll_loss=2.2, ppl=4.59, wps=20261.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22400, lr=0.000211289, gnorm=1.166, loss_scale=8, train_wall=291, gb_free=19.9, wall=71117
2022-03-05 05:10:22 | INFO | train_inner | epoch 116:    110 / 196 loss=2.411, nll_loss=2.124, ppl=4.36, wps=21024.4, ups=0.32, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=1.145, loss_scale=8, train_wall=289, gb_free=19.9, wall=71429
2022-03-05 05:14:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:14:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:14:55 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 10.41 | nll_loss 10.211 | ppl 1185.35 | wps 40519.4 | wpb 510.9 | bsz 1 | num_updates 22585 | best_loss 6.963
2022-03-05 05:14:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 22585 updates
2022-03-05 05:14:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:14:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:14:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 116 @ 22585 updates, score 10.41) (writing took 3.01854707300663 seconds)
2022-03-05 05:14:58 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-05 05:14:58 | INFO | train | epoch 116 | loss 2.439 | nll_loss 2.153 | ppl 4.45 | wps 20628.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22585 | lr 0.000210421 | gnorm 1.154 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 71704
2022-03-05 05:14:58 | INFO | fairseq.trainer | begin training epoch 117
2022-03-05 05:14:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:15:45 | INFO | train_inner | epoch 117:     15 / 196 loss=2.454, nll_loss=2.169, ppl=4.5, wps=20266.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22600, lr=0.000210352, gnorm=1.156, loss_scale=8, train_wall=291, gb_free=19.9, wall=71751
2022-03-05 05:20:56 | INFO | train_inner | epoch 117:    115 / 196 loss=2.409, nll_loss=2.123, ppl=4.36, wps=21031.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=22700, lr=0.000209888, gnorm=1.161, loss_scale=8, train_wall=289, gb_free=19.9, wall=72063
2022-03-05 05:22:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:25:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:25:13 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 10.407 | nll_loss 10.206 | ppl 1181.04 | wps 39907.9 | wpb 510.9 | bsz 1 | num_updates 22780 | best_loss 6.963
2022-03-05 05:25:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 22780 updates
2022-03-05 05:25:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:25:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:25:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 117 @ 22780 updates, score 10.407) (writing took 3.0415656769182533 seconds)
2022-03-05 05:25:16 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-05 05:25:16 | INFO | train | epoch 117 | loss 2.431 | nll_loss 2.145 | ppl 4.42 | wps 20644 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22780 | lr 0.000209519 | gnorm 1.162 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 72323
2022-03-05 05:25:16 | INFO | fairseq.trainer | begin training epoch 118
2022-03-05 05:25:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:26:18 | INFO | train_inner | epoch 118:     20 / 196 loss=2.452, nll_loss=2.166, ppl=4.49, wps=20306.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22800, lr=0.000209427, gnorm=1.165, loss_scale=8, train_wall=291, gb_free=19.9, wall=72385
2022-03-05 05:31:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:31:31 | INFO | train_inner | epoch 118:    121 / 196 loss=2.405, nll_loss=2.118, ppl=4.34, wps=20940.5, ups=0.32, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=1.16, loss_scale=8, train_wall=291, gb_free=19.9, wall=72698
2022-03-05 05:35:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:35:28 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 10.458 | nll_loss 10.258 | ppl 1224.16 | wps 41319.5 | wpb 510.9 | bsz 1 | num_updates 22975 | best_loss 6.963
2022-03-05 05:35:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 22975 updates
2022-03-05 05:35:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:35:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:35:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 118 @ 22975 updates, score 10.458) (writing took 3.0218077870085835 seconds)
2022-03-05 05:35:31 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-05 05:35:31 | INFO | train | epoch 118 | loss 2.423 | nll_loss 2.137 | ppl 4.4 | wps 20743.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22975 | lr 0.000208628 | gnorm 1.167 | loss_scale 8 | train_wall 563 | gb_free 19.9 | wall 72938
2022-03-05 05:35:31 | INFO | fairseq.trainer | begin training epoch 119
2022-03-05 05:35:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:36:49 | INFO | train_inner | epoch 119:     25 / 196 loss=2.435, nll_loss=2.149, ppl=4.44, wps=20569.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=23000, lr=0.000208514, gnorm=1.17, loss_scale=8, train_wall=287, gb_free=19.9, wall=73016
2022-03-05 05:38:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:42:02 | INFO | train_inner | epoch 119:    126 / 196 loss=2.401, nll_loss=2.114, ppl=4.33, wps=20918.2, ups=0.32, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=1.155, loss_scale=8, train_wall=291, gb_free=19.9, wall=73329
2022-03-05 05:44:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:45:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:45:44 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 10.472 | nll_loss 10.273 | ppl 1237.01 | wps 41456.6 | wpb 510.9 | bsz 1 | num_updates 23169 | best_loss 6.963
2022-03-05 05:45:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 23169 updates
2022-03-05 05:45:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:45:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:45:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 119 @ 23169 updates, score 10.472) (writing took 3.017746496014297 seconds)
2022-03-05 05:45:47 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-05 05:45:47 | INFO | train | epoch 119 | loss 2.415 | nll_loss 2.129 | ppl 4.37 | wps 20625.5 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 23169 | lr 0.000207753 | gnorm 1.166 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 73554
2022-03-05 05:45:47 | INFO | fairseq.trainer | begin training epoch 120
2022-03-05 05:45:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:47:23 | INFO | train_inner | epoch 120:     31 / 196 loss=2.427, nll_loss=2.141, ppl=4.41, wps=20369.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23200, lr=0.000207614, gnorm=1.188, loss_scale=8, train_wall=290, gb_free=19.9, wall=73650
2022-03-05 05:52:34 | INFO | train_inner | epoch 120:    131 / 196 loss=2.398, nll_loss=2.111, ppl=4.32, wps=21073.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=23300, lr=0.000207168, gnorm=1.154, loss_scale=16, train_wall=289, gb_free=19.9, wall=73961
2022-03-05 05:54:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:55:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:56:01 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 10.505 | nll_loss 10.306 | ppl 1266.37 | wps 40442.8 | wpb 510.9 | bsz 1 | num_updates 23364 | best_loss 6.963
2022-03-05 05:56:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 23364 updates
2022-03-05 05:56:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:56:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 05:56:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 120 @ 23364 updates, score 10.505) (writing took 3.001332242973149 seconds)
2022-03-05 05:56:04 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-05 05:56:04 | INFO | train | epoch 120 | loss 2.408 | nll_loss 2.121 | ppl 4.35 | wps 20665.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 23364 | lr 0.000206884 | gnorm 1.162 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 74171
2022-03-05 05:56:05 | INFO | fairseq.trainer | begin training epoch 121
2022-03-05 05:56:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:57:57 | INFO | train_inner | epoch 121:     36 / 196 loss=2.413, nll_loss=2.126, ppl=4.37, wps=20252.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23400, lr=0.000206725, gnorm=1.169, loss_scale=8, train_wall=292, gb_free=19.9, wall=74283
2022-03-05 06:03:08 | INFO | train_inner | epoch 121:    136 / 196 loss=2.386, nll_loss=2.099, ppl=4.28, wps=21027.8, ups=0.32, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=1.176, loss_scale=16, train_wall=289, gb_free=19.9, wall=74595
2022-03-05 06:03:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:06:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:06:20 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 10.51 | nll_loss 10.311 | ppl 1269.91 | wps 40392.5 | wpb 510.9 | bsz 1 | num_updates 23559 | best_loss 6.963
2022-03-05 06:06:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 23559 updates
2022-03-05 06:06:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:06:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:06:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 121 @ 23559 updates, score 10.51) (writing took 3.436841869028285 seconds)
2022-03-05 06:06:24 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-05 06:06:24 | INFO | train | epoch 121 | loss 2.399 | nll_loss 2.112 | ppl 4.32 | wps 20611.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 23559 | lr 0.000206026 | gnorm 1.172 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 74790
2022-03-05 06:06:24 | INFO | fairseq.trainer | begin training epoch 122
2022-03-05 06:06:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:08:31 | INFO | train_inner | epoch 122:     41 / 196 loss=2.4, nll_loss=2.114, ppl=4.33, wps=20275.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=23600, lr=0.000205847, gnorm=1.151, loss_scale=8, train_wall=291, gb_free=19.9, wall=74918
2022-03-05 06:10:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:13:44 | INFO | train_inner | epoch 122:    142 / 196 loss=2.394, nll_loss=2.107, ppl=4.31, wps=20918.8, ups=0.32, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=1.166, loss_scale=8, train_wall=291, gb_free=19.9, wall=75231
2022-03-05 06:16:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:16:36 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 10.521 | nll_loss 10.323 | ppl 1280.96 | wps 41120 | wpb 510.9 | bsz 1 | num_updates 23754 | best_loss 6.963
2022-03-05 06:16:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 23754 updates
2022-03-05 06:16:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:16:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:16:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 122 @ 23754 updates, score 10.521) (writing took 3.0135256748180836 seconds)
2022-03-05 06:16:39 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-05 06:16:39 | INFO | train | epoch 122 | loss 2.392 | nll_loss 2.105 | ppl 4.3 | wps 20730.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 23754 | lr 0.000205178 | gnorm 1.16 | loss_scale 8 | train_wall 563 | gb_free 19.9 | wall 75406
2022-03-05 06:16:39 | INFO | fairseq.trainer | begin training epoch 123
2022-03-05 06:16:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:18:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:19:05 | INFO | train_inner | epoch 123:     47 / 196 loss=2.391, nll_loss=2.104, ppl=4.3, wps=20360.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23800, lr=0.00020498, gnorm=1.175, loss_scale=8, train_wall=290, gb_free=19.9, wall=75552
2022-03-05 06:24:16 | INFO | train_inner | epoch 123:    147 / 196 loss=2.389, nll_loss=2.102, ppl=4.29, wps=21061.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=23900, lr=0.000204551, gnorm=1.164, loss_scale=8, train_wall=289, gb_free=19.9, wall=75863
2022-03-05 06:25:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:26:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:26:54 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 10.544 | nll_loss 10.344 | ppl 1299.7 | wps 40541.1 | wpb 510.9 | bsz 1 | num_updates 23948 | best_loss 6.963
2022-03-05 06:26:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 23948 updates
2022-03-05 06:26:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:26:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:26:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 123 @ 23948 updates, score 10.544) (writing took 3.022630423074588 seconds)
2022-03-05 06:26:57 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-05 06:26:57 | INFO | train | epoch 123 | loss 2.386 | nll_loss 2.099 | ppl 4.28 | wps 20568.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23948 | lr 0.000204346 | gnorm 1.177 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 76023
2022-03-05 06:26:57 | INFO | fairseq.trainer | begin training epoch 124
2022-03-05 06:26:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:29:39 | INFO | train_inner | epoch 124:     52 / 196 loss=2.371, nll_loss=2.084, ppl=4.24, wps=20275.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=24000, lr=0.000204124, gnorm=1.179, loss_scale=8, train_wall=291, gb_free=19.9, wall=76185
2022-03-05 06:34:50 | INFO | train_inner | epoch 124:    152 / 196 loss=2.387, nll_loss=2.1, ppl=4.29, wps=21040.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=24100, lr=0.0002037, gnorm=1.161, loss_scale=16, train_wall=289, gb_free=19.9, wall=76497
2022-03-05 06:35:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:37:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:37:12 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 10.588 | nll_loss 10.389 | ppl 1340.83 | wps 40831.6 | wpb 510.9 | bsz 1 | num_updates 24143 | best_loss 6.963
2022-03-05 06:37:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 24143 updates
2022-03-05 06:37:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:37:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:37:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 124 @ 24143 updates, score 10.588) (writing took 3.0081676251720637 seconds)
2022-03-05 06:37:15 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-05 06:37:15 | INFO | train | epoch 124 | loss 2.378 | nll_loss 2.09 | ppl 4.26 | wps 20643.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 24143 | lr 0.000203519 | gnorm 1.163 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 76641
2022-03-05 06:37:15 | INFO | fairseq.trainer | begin training epoch 125
2022-03-05 06:37:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:40:12 | INFO | train_inner | epoch 125:     57 / 196 loss=2.364, nll_loss=2.077, ppl=4.22, wps=20298.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=24200, lr=0.000203279, gnorm=1.159, loss_scale=8, train_wall=291, gb_free=19.9, wall=76819
2022-03-05 06:42:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:45:27 | INFO | train_inner | epoch 125:    158 / 196 loss=2.38, nll_loss=2.093, ppl=4.27, wps=20842.1, ups=0.32, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=1.171, loss_scale=8, train_wall=292, gb_free=19.9, wall=77133
2022-03-05 06:47:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:47:30 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 10.595 | nll_loss 10.396 | ppl 1346.98 | wps 40306.8 | wpb 510.9 | bsz 1 | num_updates 24338 | best_loss 6.963
2022-03-05 06:47:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 24338 updates
2022-03-05 06:47:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:47:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:47:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 125 @ 24338 updates, score 10.595) (writing took 3.1614481802098453 seconds)
2022-03-05 06:47:33 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-05 06:47:33 | INFO | train | epoch 125 | loss 2.371 | nll_loss 2.084 | ppl 4.24 | wps 20648 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 24338 | lr 0.000202702 | gnorm 1.166 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 77260
2022-03-05 06:47:33 | INFO | fairseq.trainer | begin training epoch 126
2022-03-05 06:47:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:50:46 | INFO | train_inner | epoch 126:     62 / 196 loss=2.353, nll_loss=2.065, ppl=4.18, wps=20456.5, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=24400, lr=0.000202444, gnorm=1.163, loss_scale=16, train_wall=288, gb_free=19.9, wall=77453
2022-03-05 06:54:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:56:01 | INFO | train_inner | epoch 126:    163 / 196 loss=2.38, nll_loss=2.092, ppl=4.26, wps=20840.3, ups=0.32, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=1.174, loss_scale=8, train_wall=292, gb_free=19.9, wall=77767
2022-03-05 06:57:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:57:48 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 10.611 | nll_loss 10.414 | ppl 1363.94 | wps 40445.9 | wpb 510.9 | bsz 1 | num_updates 24533 | best_loss 6.963
2022-03-05 06:57:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 24533 updates
2022-03-05 06:57:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:57:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 06:57:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 126 @ 24533 updates, score 10.611) (writing took 3.0485690268687904 seconds)
2022-03-05 06:57:51 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-05 06:57:51 | INFO | train | epoch 126 | loss 2.364 | nll_loss 2.076 | ppl 4.22 | wps 20643.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 24533 | lr 0.000201895 | gnorm 1.168 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 77878
2022-03-05 06:57:51 | INFO | fairseq.trainer | begin training epoch 127
2022-03-05 06:57:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:01:20 | INFO | train_inner | epoch 127:     67 / 196 loss=2.349, nll_loss=2.061, ppl=4.17, wps=20488.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=24600, lr=0.000201619, gnorm=1.168, loss_scale=16, train_wall=288, gb_free=19.9, wall=78086
2022-03-05 07:06:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:06:34 | INFO | train_inner | epoch 127:    168 / 196 loss=2.372, nll_loss=2.084, ppl=4.24, wps=20860.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=24700, lr=0.000201211, gnorm=1.159, loss_scale=8, train_wall=292, gb_free=19.9, wall=78400
2022-03-05 07:08:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:08:06 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 10.632 | nll_loss 10.434 | ppl 1383.43 | wps 40704 | wpb 510.9 | bsz 1 | num_updates 24728 | best_loss 6.963
2022-03-05 07:08:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 24728 updates
2022-03-05 07:08:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:08:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:08:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 127 @ 24728 updates, score 10.632) (writing took 6.926776736974716 seconds)
2022-03-05 07:08:12 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-05 07:08:12 | INFO | train | epoch 127 | loss 2.357 | nll_loss 2.069 | ppl 4.19 | wps 20540.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24728 | lr 0.000201097 | gnorm 1.164 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 78499
2022-03-05 07:08:12 | INFO | fairseq.trainer | begin training epoch 128
2022-03-05 07:08:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:11:56 | INFO | train_inner | epoch 128:     72 / 196 loss=2.334, nll_loss=2.046, ppl=4.13, wps=20307.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=24800, lr=0.000200805, gnorm=1.173, loss_scale=8, train_wall=287, gb_free=19.9, wall=78722
2022-03-05 07:15:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:17:09 | INFO | train_inner | epoch 128:    173 / 196 loss=2.373, nll_loss=2.085, ppl=4.24, wps=20935.8, ups=0.32, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=1.188, loss_scale=8, train_wall=291, gb_free=19.9, wall=79035
2022-03-05 07:18:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:18:25 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 10.622 | nll_loss 10.425 | ppl 1375.2 | wps 41098.2 | wpb 510.9 | bsz 1 | num_updates 24923 | best_loss 6.963
2022-03-05 07:18:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 24923 updates
2022-03-05 07:18:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:18:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:18:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 128 @ 24923 updates, score 10.622) (writing took 2.98882076703012 seconds)
2022-03-05 07:18:28 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-05 07:18:28 | INFO | train | epoch 128 | loss 2.351 | nll_loss 2.063 | ppl 4.18 | wps 20747.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 24923 | lr 0.000200309 | gnorm 1.18 | loss_scale 8 | train_wall 563 | gb_free 19.9 | wall 79114
2022-03-05 07:18:28 | INFO | fairseq.trainer | begin training epoch 129
2022-03-05 07:18:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:22:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:22:29 | INFO | train_inner | epoch 129:     78 / 196 loss=2.318, nll_loss=2.029, ppl=4.08, wps=20387.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=25000, lr=0.0002, gnorm=1.171, loss_scale=8, train_wall=290, gb_free=19.9, wall=79356
2022-03-05 07:27:39 | INFO | train_inner | epoch 129:    178 / 196 loss=2.37, nll_loss=2.082, ppl=4.23, wps=21153, ups=0.32, wpb=65532.4, bsz=128, num_updates=25100, lr=0.000199601, gnorm=1.186, loss_scale=8, train_wall=288, gb_free=19.9, wall=79666
2022-03-05 07:28:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:28:39 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 10.657 | nll_loss 10.459 | ppl 1408.01 | wps 41316.9 | wpb 510.9 | bsz 1 | num_updates 25118 | best_loss 6.963
2022-03-05 07:28:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 25118 updates
2022-03-05 07:28:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:28:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:28:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 129 @ 25118 updates, score 10.657) (writing took 2.9963121148757637 seconds)
2022-03-05 07:28:43 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-05 07:28:43 | INFO | train | epoch 129 | loss 2.343 | nll_loss 2.055 | ppl 4.15 | wps 20754.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 25118 | lr 0.00019953 | gnorm 1.175 | loss_scale 8 | train_wall 563 | gb_free 19.9 | wall 79729
2022-03-05 07:28:43 | INFO | fairseq.trainer | begin training epoch 130
2022-03-05 07:28:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:29:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:33:01 | INFO | train_inner | epoch 130:     83 / 196 loss=2.313, nll_loss=2.023, ppl=4.07, wps=20292, ups=0.31, wpb=65367, bsz=127.7, num_updates=25200, lr=0.000199205, gnorm=1.16, loss_scale=8, train_wall=291, gb_free=19.9, wall=79988
2022-03-05 07:36:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:38:16 | INFO | train_inner | epoch 130:    184 / 196 loss=2.366, nll_loss=2.078, ppl=4.22, wps=20820.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=25300, lr=0.000198811, gnorm=1.18, loss_scale=8, train_wall=292, gb_free=19.9, wall=80303
2022-03-05 07:38:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:38:58 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 10.68 | nll_loss 10.482 | ppl 1430.27 | wps 40603.9 | wpb 510.9 | bsz 1 | num_updates 25312 | best_loss 6.963
2022-03-05 07:38:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 25312 updates
2022-03-05 07:38:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:39:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:39:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 130 @ 25312 updates, score 10.68) (writing took 3.0053744160104543 seconds)
2022-03-05 07:39:01 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-05 07:39:01 | INFO | train | epoch 130 | loss 2.337 | nll_loss 2.048 | ppl 4.14 | wps 20523.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25312 | lr 0.000198764 | gnorm 1.172 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 80348
2022-03-05 07:39:01 | INFO | fairseq.trainer | begin training epoch 131
2022-03-05 07:39:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:43:36 | INFO | train_inner | epoch 131:     88 / 196 loss=2.303, nll_loss=2.013, ppl=4.04, wps=20458, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25400, lr=0.000198419, gnorm=1.176, loss_scale=16, train_wall=288, gb_free=19.9, wall=80622
2022-03-05 07:46:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:48:50 | INFO | train_inner | epoch 131:    189 / 196 loss=2.364, nll_loss=2.076, ppl=4.22, wps=20827.3, ups=0.32, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=1.179, loss_scale=8, train_wall=292, gb_free=19.9, wall=80937
2022-03-05 07:49:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:49:17 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 10.693 | nll_loss 10.495 | ppl 1443.22 | wps 39928.4 | wpb 510.9 | bsz 1 | num_updates 25507 | best_loss 6.963
2022-03-05 07:49:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 25507 updates
2022-03-05 07:49:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:49:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:49:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 131 @ 25507 updates, score 10.693) (writing took 2.9796808778773993 seconds)
2022-03-05 07:49:20 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-05 07:49:20 | INFO | train | epoch 131 | loss 2.33 | nll_loss 2.042 | ppl 4.12 | wps 20630.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 25507 | lr 0.000198002 | gnorm 1.177 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 80966
2022-03-05 07:49:20 | INFO | fairseq.trainer | begin training epoch 132
2022-03-05 07:49:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:54:09 | INFO | train_inner | epoch 132:     93 / 196 loss=2.292, nll_loss=2.002, ppl=4.01, wps=20483.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25600, lr=0.000197642, gnorm=1.172, loss_scale=16, train_wall=288, gb_free=19.9, wall=81256
2022-03-05 07:54:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:59:24 | INFO | train_inner | epoch 132:    194 / 196 loss=2.357, nll_loss=2.069, ppl=4.2, wps=20858.7, ups=0.32, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=1.178, loss_scale=8, train_wall=292, gb_free=19.9, wall=81570
2022-03-05 07:59:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:59:34 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 10.694 | nll_loss 10.497 | ppl 1445.13 | wps 40560.8 | wpb 510.9 | bsz 1 | num_updates 25702 | best_loss 6.963
2022-03-05 07:59:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 25702 updates
2022-03-05 07:59:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:59:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 07:59:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 132 @ 25702 updates, score 10.694) (writing took 2.9686563990544528 seconds)
2022-03-05 07:59:37 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-05 07:59:37 | INFO | train | epoch 132 | loss 2.323 | nll_loss 2.034 | ppl 4.1 | wps 20663.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 25702 | lr 0.00019725 | gnorm 1.174 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 81584
2022-03-05 07:59:37 | INFO | fairseq.trainer | begin training epoch 133
2022-03-05 07:59:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:01:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:04:46 | INFO | train_inner | epoch 133:     99 / 196 loss=2.284, nll_loss=1.994, ppl=3.98, wps=20294.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25800, lr=0.000196875, gnorm=1.19, loss_scale=8, train_wall=291, gb_free=19.9, wall=81892
2022-03-05 08:09:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:09:52 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 10.72 | nll_loss 10.521 | ppl 1469.5 | wps 40826.2 | wpb 510.9 | bsz 1 | num_updates 25897 | best_loss 6.963
2022-03-05 08:09:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 25897 updates
2022-03-05 08:09:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:09:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:09:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 133 @ 25897 updates, score 10.72) (writing took 2.9780346190091223 seconds)
2022-03-05 08:09:55 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-05 08:09:55 | INFO | train | epoch 133 | loss 2.318 | nll_loss 2.029 | ppl 4.08 | wps 20660.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 25897 | lr 0.000196506 | gnorm 1.184 | loss_scale 16 | train_wall 565 | gb_free 19.9 | wall 82202
2022-03-05 08:09:55 | INFO | fairseq.trainer | begin training epoch 134
2022-03-05 08:09:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:10:04 | INFO | train_inner | epoch 134:      3 / 196 loss=2.35, nll_loss=2.062, ppl=4.18, wps=20497.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=25900, lr=0.000196494, gnorm=1.178, loss_scale=16, train_wall=288, gb_free=19.9, wall=82211
2022-03-05 08:10:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:15:19 | INFO | train_inner | epoch 134:    104 / 196 loss=2.281, nll_loss=1.991, ppl=3.97, wps=20844.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=26000, lr=0.000196116, gnorm=1.182, loss_scale=8, train_wall=292, gb_free=19.9, wall=82525
2022-03-05 08:19:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:20:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:20:10 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 10.735 | nll_loss 10.537 | ppl 1486.08 | wps 40640.1 | wpb 510.9 | bsz 1 | num_updates 26091 | best_loss 6.963
2022-03-05 08:20:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 26091 updates
2022-03-05 08:20:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:20:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:20:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 134 @ 26091 updates, score 10.735) (writing took 2.9913489639293402 seconds)
2022-03-05 08:20:13 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-05 08:20:13 | INFO | train | epoch 134 | loss 2.31 | nll_loss 2.021 | ppl 4.06 | wps 20551.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26091 | lr 0.000195774 | gnorm 1.189 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 82819
2022-03-05 08:20:13 | INFO | fairseq.trainer | begin training epoch 135
2022-03-05 08:20:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:20:41 | INFO | train_inner | epoch 135:      9 / 196 loss=2.334, nll_loss=2.046, ppl=4.13, wps=20296.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=26100, lr=0.00019574, gnorm=1.194, loss_scale=8, train_wall=291, gb_free=19.9, wall=82848
2022-03-05 08:25:52 | INFO | train_inner | epoch 135:    109 / 196 loss=2.276, nll_loss=1.986, ppl=3.96, wps=21043.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=26200, lr=0.000195366, gnorm=1.181, loss_scale=16, train_wall=289, gb_free=19.9, wall=83159
2022-03-05 08:26:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:30:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:30:28 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 10.747 | nll_loss 10.548 | ppl 1497.6 | wps 40715.4 | wpb 510.9 | bsz 1 | num_updates 26286 | best_loss 6.963
2022-03-05 08:30:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 26286 updates
2022-03-05 08:30:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:30:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:30:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 135 @ 26286 updates, score 10.747) (writing took 3.0028163220267743 seconds)
2022-03-05 08:30:31 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-05 08:30:31 | INFO | train | epoch 135 | loss 2.305 | nll_loss 2.016 | ppl 4.04 | wps 20641.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 26286 | lr 0.000195046 | gnorm 1.186 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 83438
2022-03-05 08:30:31 | INFO | fairseq.trainer | begin training epoch 136
2022-03-05 08:30:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:31:15 | INFO | train_inner | epoch 136:     14 / 196 loss=2.329, nll_loss=2.04, ppl=4.11, wps=20267.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=26300, lr=0.000194994, gnorm=1.195, loss_scale=8, train_wall=291, gb_free=19.9, wall=83481
2022-03-05 08:33:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:36:30 | INFO | train_inner | epoch 136:    115 / 196 loss=2.275, nll_loss=1.985, ppl=3.96, wps=20796.3, ups=0.32, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=1.176, loss_scale=8, train_wall=292, gb_free=19.9, wall=83797
2022-03-05 08:40:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:40:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:40:48 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 10.772 | nll_loss 10.575 | ppl 1525.84 | wps 40444.2 | wpb 510.9 | bsz 1 | num_updates 26480 | best_loss 6.963
2022-03-05 08:40:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 26480 updates
2022-03-05 08:40:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:40:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:40:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 136 @ 26480 updates, score 10.772) (writing took 2.9812537799589336 seconds)
2022-03-05 08:40:51 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-05 08:40:51 | INFO | train | epoch 136 | loss 2.299 | nll_loss 2.009 | ppl 4.02 | wps 20498.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26480 | lr 0.000194331 | gnorm 1.186 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 84057
2022-03-05 08:40:51 | INFO | fairseq.trainer | begin training epoch 137
2022-03-05 08:40:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:41:53 | INFO | train_inner | epoch 137:     20 / 196 loss=2.319, nll_loss=2.03, ppl=4.08, wps=20245.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=26500, lr=0.000194257, gnorm=1.196, loss_scale=8, train_wall=292, gb_free=19.9, wall=84119
2022-03-05 08:47:05 | INFO | train_inner | epoch 137:    120 / 196 loss=2.276, nll_loss=1.986, ppl=3.96, wps=21020, ups=0.32, wpb=65532.4, bsz=128, num_updates=26600, lr=0.000193892, gnorm=1.172, loss_scale=8, train_wall=289, gb_free=19.9, wall=84431
2022-03-05 08:48:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:51:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:51:06 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 10.758 | nll_loss 10.56 | ppl 1509.18 | wps 40484.5 | wpb 510.9 | bsz 1 | num_updates 26675 | best_loss 6.963
2022-03-05 08:51:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 26675 updates
2022-03-05 08:51:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:51:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 08:51:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 137 @ 26675 updates, score 10.758) (writing took 2.981889930088073 seconds)
2022-03-05 08:51:09 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-05 08:51:09 | INFO | train | epoch 137 | loss 2.293 | nll_loss 2.003 | ppl 4.01 | wps 20631.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 26675 | lr 0.000193619 | gnorm 1.176 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 84676
2022-03-05 08:51:09 | INFO | fairseq.trainer | begin training epoch 138
2022-03-05 08:51:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:52:27 | INFO | train_inner | epoch 138:     25 / 196 loss=2.303, nll_loss=2.014, ppl=4.04, wps=20263.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=26700, lr=0.000193528, gnorm=1.17, loss_scale=8, train_wall=291, gb_free=19.9, wall=84754
2022-03-05 08:55:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:57:42 | INFO | train_inner | epoch 138:    126 / 196 loss=2.264, nll_loss=1.973, ppl=3.93, wps=20799.6, ups=0.32, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=1.169, loss_scale=8, train_wall=292, gb_free=19.9, wall=85069
2022-03-05 09:01:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:01:25 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 10.792 | nll_loss 10.592 | ppl 1543.82 | wps 40544.8 | wpb 510.9 | bsz 1 | num_updates 26870 | best_loss 6.963
2022-03-05 09:01:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 26870 updates
2022-03-05 09:01:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:01:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:01:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 138 @ 26870 updates, score 10.792) (writing took 2.999959903070703 seconds)
2022-03-05 09:01:28 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-05 09:01:28 | INFO | train | epoch 138 | loss 2.287 | nll_loss 1.997 | ppl 3.99 | wps 20613.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 26870 | lr 0.000192915 | gnorm 1.171 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 85295
2022-03-05 09:01:28 | INFO | fairseq.trainer | begin training epoch 139
2022-03-05 09:01:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:03:02 | INFO | train_inner | epoch 139:     30 / 196 loss=2.309, nll_loss=2.019, ppl=4.05, wps=20459, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=26900, lr=0.000192807, gnorm=1.176, loss_scale=16, train_wall=288, gb_free=19.9, wall=85388
2022-03-05 09:05:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:08:17 | INFO | train_inner | epoch 139:    131 / 196 loss=2.27, nll_loss=1.979, ppl=3.94, wps=20800.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=27000, lr=0.00019245, gnorm=1.176, loss_scale=8, train_wall=292, gb_free=19.9, wall=85703
2022-03-05 09:11:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:11:44 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 10.818 | nll_loss 10.621 | ppl 1575.18 | wps 40408.1 | wpb 510.9 | bsz 1 | num_updates 27065 | best_loss 6.963
2022-03-05 09:11:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 27065 updates
2022-03-05 09:11:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:11:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:11:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 139 @ 27065 updates, score 10.818) (writing took 3.0287603510078043 seconds)
2022-03-05 09:11:47 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-05 09:11:47 | INFO | train | epoch 139 | loss 2.282 | nll_loss 1.991 | ppl 3.98 | wps 20618 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 27065 | lr 0.000192219 | gnorm 1.179 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 85914
2022-03-05 09:11:47 | INFO | fairseq.trainer | begin training epoch 140
2022-03-05 09:11:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:12:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:13:40 | INFO | train_inner | epoch 140:     36 / 196 loss=2.287, nll_loss=1.997, ppl=3.99, wps=20246.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=27100, lr=0.000192095, gnorm=1.177, loss_scale=8, train_wall=292, gb_free=19.9, wall=86026
2022-03-05 09:18:52 | INFO | train_inner | epoch 140:    136 / 196 loss=2.273, nll_loss=1.983, ppl=3.95, wps=21004, ups=0.32, wpb=65532.4, bsz=128, num_updates=27200, lr=0.000191741, gnorm=1.174, loss_scale=8, train_wall=289, gb_free=19.9, wall=86338
2022-03-05 09:19:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:21:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:22:03 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 10.834 | nll_loss 10.636 | ppl 1591.71 | wps 40177.1 | wpb 510.9 | bsz 1 | num_updates 27259 | best_loss 6.963
2022-03-05 09:22:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 27259 updates
2022-03-05 09:22:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:22:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:22:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 140 @ 27259 updates, score 10.834) (writing took 2.99839479313232 seconds)
2022-03-05 09:22:06 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-05 09:22:06 | INFO | train | epoch 140 | loss 2.275 | nll_loss 1.984 | ppl 3.96 | wps 20502.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27259 | lr 0.000191534 | gnorm 1.173 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 86533
2022-03-05 09:22:07 | INFO | fairseq.trainer | begin training epoch 141
2022-03-05 09:22:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:24:14 | INFO | train_inner | epoch 141:     41 / 196 loss=2.273, nll_loss=1.983, ppl=3.95, wps=20249.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=27300, lr=0.00019139, gnorm=1.18, loss_scale=8, train_wall=291, gb_free=19.9, wall=86661
2022-03-05 09:29:26 | INFO | train_inner | epoch 141:    141 / 196 loss=2.267, nll_loss=1.976, ppl=3.93, wps=21007.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=27400, lr=0.00019104, gnorm=1.191, loss_scale=16, train_wall=289, gb_free=19.9, wall=86973
2022-03-05 09:31:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:32:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:32:22 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 10.864 | nll_loss 10.665 | ppl 1623.6 | wps 40432.9 | wpb 510.9 | bsz 1 | num_updates 27454 | best_loss 6.963
2022-03-05 09:32:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 27454 updates
2022-03-05 09:32:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:32:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:32:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 141 @ 27454 updates, score 10.864) (writing took 2.983324547065422 seconds)
2022-03-05 09:32:25 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-05 09:32:25 | INFO | train | epoch 141 | loss 2.271 | nll_loss 1.98 | ppl 3.95 | wps 20617.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 27454 | lr 0.000190852 | gnorm 1.19 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 87152
2022-03-05 09:32:26 | INFO | fairseq.trainer | begin training epoch 142
2022-03-05 09:32:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:34:49 | INFO | train_inner | epoch 142:     46 / 196 loss=2.264, nll_loss=1.973, ppl=3.93, wps=20270, ups=0.31, wpb=65367, bsz=127.7, num_updates=27500, lr=0.000190693, gnorm=1.197, loss_scale=8, train_wall=291, gb_free=19.9, wall=87296
2022-03-05 09:40:01 | INFO | train_inner | epoch 142:    146 / 196 loss=2.272, nll_loss=1.981, ppl=3.95, wps=21030, ups=0.32, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=1.189, loss_scale=16, train_wall=289, gb_free=19.9, wall=87607
2022-03-05 09:40:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:42:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:42:42 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 10.857 | nll_loss 10.658 | ppl 1616.25 | wps 41432.6 | wpb 510.9 | bsz 1 | num_updates 27649 | best_loss 6.963
2022-03-05 09:42:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 27649 updates
2022-03-05 09:42:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:42:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:42:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 142 @ 27649 updates, score 10.857) (writing took 2.9686339690815657 seconds)
2022-03-05 09:42:45 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-05 09:42:45 | INFO | train | epoch 142 | loss 2.265 | nll_loss 1.974 | ppl 3.93 | wps 20593.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27649 | lr 0.000190178 | gnorm 1.189 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 87772
2022-03-05 09:42:45 | INFO | fairseq.trainer | begin training epoch 143
2022-03-05 09:42:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:45:24 | INFO | train_inner | epoch 143:     51 / 196 loss=2.26, nll_loss=1.969, ppl=3.92, wps=20226.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=27700, lr=0.000190003, gnorm=1.19, loss_scale=8, train_wall=292, gb_free=19.9, wall=87930
2022-03-05 09:48:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:50:38 | INFO | train_inner | epoch 143:    152 / 196 loss=2.258, nll_loss=1.967, ppl=3.91, wps=20872.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=27800, lr=0.000189661, gnorm=1.197, loss_scale=8, train_wall=291, gb_free=19.9, wall=88244
2022-03-05 09:52:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:52:59 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 10.884 | nll_loss 10.686 | ppl 1647.62 | wps 41232.4 | wpb 510.9 | bsz 1 | num_updates 27844 | best_loss 6.963
2022-03-05 09:52:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 27844 updates
2022-03-05 09:52:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:53:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 09:53:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 143 @ 27844 updates, score 10.884) (writing took 2.973620400065556 seconds)
2022-03-05 09:53:02 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-05 09:53:02 | INFO | train | epoch 143 | loss 2.258 | nll_loss 1.967 | ppl 3.91 | wps 20687.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 27844 | lr 0.000189511 | gnorm 1.193 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 88389
2022-03-05 09:53:02 | INFO | fairseq.trainer | begin training epoch 144
2022-03-05 09:53:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:55:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:55:59 | INFO | train_inner | epoch 144:     57 / 196 loss=2.249, nll_loss=1.958, ppl=3.89, wps=20312.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=27900, lr=0.000189321, gnorm=1.178, loss_scale=8, train_wall=291, gb_free=19.9, wall=88566
2022-03-05 10:01:10 | INFO | train_inner | epoch 144:    157 / 196 loss=2.263, nll_loss=1.972, ppl=3.92, wps=21093.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=28000, lr=0.000188982, gnorm=1.17, loss_scale=8, train_wall=288, gb_free=19.9, wall=88877
2022-03-05 10:02:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:03:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:03:16 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 10.888 | nll_loss 10.692 | ppl 1653.86 | wps 40114.9 | wpb 510.9 | bsz 1 | num_updates 28038 | best_loss 6.963
2022-03-05 10:03:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 28038 updates
2022-03-05 10:03:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:03:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:03:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 144 @ 28038 updates, score 10.888) (writing took 3.011280076112598 seconds)
2022-03-05 10:03:19 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-05 10:03:19 | INFO | train | epoch 144 | loss 2.251 | nll_loss 1.96 | ppl 3.89 | wps 20570.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28038 | lr 0.000188854 | gnorm 1.179 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 89006
2022-03-05 10:03:19 | INFO | fairseq.trainer | begin training epoch 145
2022-03-05 10:03:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:06:33 | INFO | train_inner | epoch 145:     62 / 196 loss=2.237, nll_loss=1.946, ppl=3.85, wps=20237.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=28100, lr=0.000188646, gnorm=1.197, loss_scale=8, train_wall=291, gb_free=19.9, wall=89200
2022-03-05 10:10:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:11:49 | INFO | train_inner | epoch 145:    163 / 196 loss=2.26, nll_loss=1.97, ppl=3.92, wps=20764.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=28200, lr=0.000188311, gnorm=1.184, loss_scale=8, train_wall=293, gb_free=19.9, wall=89515
2022-03-05 10:13:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:13:37 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 10.941 | nll_loss 10.742 | ppl 1712.21 | wps 40286.1 | wpb 510.9 | bsz 1 | num_updates 28233 | best_loss 6.963
2022-03-05 10:13:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 28233 updates
2022-03-05 10:13:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:13:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:13:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 145 @ 28233 updates, score 10.941) (writing took 2.9927646999713033 seconds)
2022-03-05 10:13:40 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-05 10:13:40 | INFO | train | epoch 145 | loss 2.248 | nll_loss 1.957 | ppl 3.88 | wps 20577.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28233 | lr 0.000188201 | gnorm 1.193 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 89626
2022-03-05 10:13:40 | INFO | fairseq.trainer | begin training epoch 146
2022-03-05 10:13:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:17:09 | INFO | train_inner | epoch 146:     67 / 196 loss=2.227, nll_loss=1.935, ppl=3.82, wps=20426.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28300, lr=0.000187978, gnorm=1.188, loss_scale=8, train_wall=289, gb_free=19.9, wall=89835
2022-03-05 10:18:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:22:24 | INFO | train_inner | epoch 146:    168 / 196 loss=2.261, nll_loss=1.97, ppl=3.92, wps=20790.9, ups=0.32, wpb=65536, bsz=128, num_updates=28400, lr=0.000187647, gnorm=1.197, loss_scale=8, train_wall=292, gb_free=19.9, wall=90151
2022-03-05 10:23:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:23:56 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 10.929 | nll_loss 10.731 | ppl 1699.78 | wps 40176.4 | wpb 510.9 | bsz 1 | num_updates 28428 | best_loss 6.963
2022-03-05 10:23:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 28428 updates
2022-03-05 10:23:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:23:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:23:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 146 @ 28428 updates, score 10.929) (writing took 3.0778544808272272 seconds)
2022-03-05 10:23:59 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-05 10:23:59 | INFO | train | epoch 146 | loss 2.242 | nll_loss 1.951 | ppl 3.87 | wps 20593.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28428 | lr 0.000187554 | gnorm 1.186 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 90246
2022-03-05 10:23:59 | INFO | fairseq.trainer | begin training epoch 147
2022-03-05 10:23:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:25:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:27:47 | INFO | train_inner | epoch 147:     73 / 196 loss=2.223, nll_loss=1.932, ppl=3.81, wps=20219.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=28500, lr=0.000187317, gnorm=1.189, loss_scale=8, train_wall=292, gb_free=19.9, wall=90474
2022-03-05 10:32:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:33:02 | INFO | train_inner | epoch 147:    174 / 196 loss=2.252, nll_loss=1.961, ppl=3.89, wps=20814.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=28600, lr=0.000186989, gnorm=1.193, loss_scale=8, train_wall=292, gb_free=19.9, wall=90789
2022-03-05 10:34:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:34:15 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 10.92 | nll_loss 10.722 | ppl 1689.13 | wps 40403.2 | wpb 510.9 | bsz 1 | num_updates 28622 | best_loss 6.963
2022-03-05 10:34:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 28622 updates
2022-03-05 10:34:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:34:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:34:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 147 @ 28622 updates, score 10.92) (writing took 3.0307910558767617 seconds)
2022-03-05 10:34:18 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-05 10:34:18 | INFO | train | epoch 147 | loss 2.236 | nll_loss 1.944 | ppl 3.85 | wps 20511.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28622 | lr 0.000186918 | gnorm 1.189 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 90865
2022-03-05 10:34:18 | INFO | fairseq.trainer | begin training epoch 148
2022-03-05 10:34:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:38:20 | INFO | train_inner | epoch 148:     78 / 196 loss=2.217, nll_loss=1.925, ppl=3.8, wps=20533.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=28700, lr=0.000186663, gnorm=1.178, loss_scale=8, train_wall=287, gb_free=19.9, wall=91107
2022-03-05 10:39:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:43:34 | INFO | train_inner | epoch 148:    179 / 196 loss=2.251, nll_loss=1.96, ppl=3.89, wps=20883, ups=0.32, wpb=65532.4, bsz=128, num_updates=28800, lr=0.000186339, gnorm=1.201, loss_scale=8, train_wall=291, gb_free=19.9, wall=91421
2022-03-05 10:44:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:44:32 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 10.916 | nll_loss 10.719 | ppl 1685.28 | wps 40448.1 | wpb 510.9 | bsz 1 | num_updates 28817 | best_loss 6.963
2022-03-05 10:44:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 28817 updates
2022-03-05 10:44:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:44:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:44:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 148 @ 28817 updates, score 10.916) (writing took 3.0486643188633025 seconds)
2022-03-05 10:44:35 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-05 10:44:35 | INFO | train | epoch 148 | loss 2.232 | nll_loss 1.94 | ppl 3.84 | wps 20698.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 28817 | lr 0.000186284 | gnorm 1.197 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 91481
2022-03-05 10:44:35 | INFO | fairseq.trainer | begin training epoch 149
2022-03-05 10:44:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:45:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:48:56 | INFO | train_inner | epoch 149:     84 / 196 loss=2.199, nll_loss=1.907, ppl=3.75, wps=20330.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28900, lr=0.000186016, gnorm=1.203, loss_scale=8, train_wall=290, gb_free=19.9, wall=91742
2022-03-05 10:53:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:54:09 | INFO | train_inner | epoch 149:    185 / 196 loss=2.255, nll_loss=1.964, ppl=3.9, wps=20935.2, ups=0.32, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=1.194, loss_scale=8, train_wall=291, gb_free=19.9, wall=92055
2022-03-05 10:54:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:54:47 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 10.955 | nll_loss 10.76 | ppl 1733.59 | wps 41315.4 | wpb 510.9 | bsz 1 | num_updates 29011 | best_loss 6.963
2022-03-05 10:54:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 29011 updates
2022-03-05 10:54:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:54:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 10:54:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 149 @ 29011 updates, score 10.955) (writing took 2.979063814971596 seconds)
2022-03-05 10:54:50 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-05 10:54:50 | INFO | train | epoch 149 | loss 2.225 | nll_loss 1.933 | ppl 3.82 | wps 20625 | ups 0.32 | wpb 65447.1 | bsz 127.8 | num_updates 29011 | lr 0.00018566 | gnorm 1.195 | loss_scale 8 | train_wall 564 | gb_free 19.9 | wall 92097
2022-03-05 10:54:50 | INFO | fairseq.trainer | begin training epoch 150
2022-03-05 10:54:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:59:27 | INFO | train_inner | epoch 150:     89 / 196 loss=2.197, nll_loss=1.904, ppl=3.74, wps=20540.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=29100, lr=0.000185376, gnorm=1.193, loss_scale=8, train_wall=288, gb_free=19.9, wall=92374
2022-03-05 11:01:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:04:43 | INFO | train_inner | epoch 150:    190 / 196 loss=2.25, nll_loss=1.959, ppl=3.89, wps=20758.3, ups=0.32, wpb=65536, bsz=128, num_updates=29200, lr=0.000185058, gnorm=1.22, loss_scale=8, train_wall=293, gb_free=19.9, wall=92689
2022-03-05 11:05:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:05:06 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 10.954 | nll_loss 10.757 | ppl 1730.13 | wps 40231.3 | wpb 510.9 | bsz 1 | num_updates 29206 | best_loss 6.963
2022-03-05 11:05:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 29206 updates
2022-03-05 11:05:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:05:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:05:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 150 @ 29206 updates, score 10.954) (writing took 2.9788220929913223 seconds)
2022-03-05 11:05:09 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-05 11:05:09 | INFO | train | epoch 150 | loss 2.221 | nll_loss 1.93 | ppl 3.81 | wps 20627.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 29206 | lr 0.000185039 | gnorm 1.206 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 92716
2022-03-05 11:05:09 | INFO | fairseq.trainer | begin training epoch 151
2022-03-05 11:05:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:10:03 | INFO | train_inner | epoch 151:     94 / 196 loss=2.184, nll_loss=1.891, ppl=3.71, wps=20414.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=29300, lr=0.000184742, gnorm=1.185, loss_scale=16, train_wall=289, gb_free=19.9, wall=93010
2022-03-05 11:10:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:15:16 | INFO | train_inner | epoch 151:    195 / 196 loss=2.251, nll_loss=1.96, ppl=3.89, wps=20903.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=29400, lr=0.000184428, gnorm=1.198, loss_scale=8, train_wall=291, gb_free=19.9, wall=93323
2022-03-05 11:15:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:15:24 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 10.998 | nll_loss 10.803 | ppl 1786.03 | wps 41255 | wpb 510.9 | bsz 1 | num_updates 29401 | best_loss 6.963
2022-03-05 11:15:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 29401 updates
2022-03-05 11:15:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:15:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt
2022-03-05 11:15:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.01_#1/checkpoint_last.pt (epoch 151 @ 29401 updates, score 10.998) (writing took 2.935759407002479 seconds)
2022-03-05 11:15:27 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-05 11:15:27 | INFO | train | epoch 151 | loss 2.217 | nll_loss 1.925 | ppl 3.8 | wps 20656 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 29401 | lr 0.000184425 | gnorm 1.191 | loss_scale 8 | train_wall 565 | gb_free 19.9 | wall 93334
2022-03-05 11:15:27 | INFO | fairseq.trainer | begin training epoch 152
2022-03-05 11:15:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:17:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:20:37 | INFO | train_inner | epoch 152:    100 / 196 loss=2.178, nll_loss=1.884, ppl=3.69, wps=20376, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=29500, lr=0.000184115, gnorm=1.203, loss_scale=8, train_wall=290, gb_free=19.9, wall=93644
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 79, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 424, in forward
    x = self.activation_dropout_module(x)
KeyboardInterrupt
