Sender: LSF System <lsfadmin@eu-g3-072>
Subject: Job 207014198: <w103_fp16_size_0.25_jelinek_0.02_0.08_0.9_#1> in cluster <euler> Exited

Job <w103_fp16_size_0.25_jelinek_0.02_0.08_0.9_#1> was submitted from host <eu-login-19> by user <andriusb> in cluster <euler> at Thu Mar  3 10:48:00 2022
Job was executed on host(s) <eu-g3-072>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Thu Mar  3 10:48:10 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Mar  3 10:48:10 2022
Terminated at Fri Mar  4 13:21:39 2022
Results reported at Fri Mar  4 13:21:39 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.02, 0.08, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321671 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   95544.71 sec.
    Max Memory :                                 8476 MB
    Average Memory :                             3076.10 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11524.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   95608 sec.
    Turnaround time :                            95619 sec.

The output (if any) follows:

2022-03-03 10:48:23 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321671, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321671, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.02, 0.08, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-03 10:48:24 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-03 10:48:28 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
Calculating frequency stats:
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 694/450337 [00:00<01:04, 6919.04it/s]  0%|          | 1386/450337 [00:00<01:10, 6391.36it/s]  0%|          | 2028/450337 [00:00<01:14, 6015.29it/s]  1%|          | 2698/450337 [00:00<01:11, 6264.37it/s]  1%|          | 3398/450337 [00:00<01:08, 6518.34it/s]  1%|          | 4087/450337 [00:00<01:07, 6637.53it/s]  1%|          | 4867/450337 [00:00<01:03, 7004.70it/s]  1%|          | 5584/450337 [00:00<01:03, 7053.94it/s]  1%|▏         | 6292/450337 [00:00<01:02, 7056.68it/s]  2%|▏         | 6999/450337 [00:01<01:07, 6581.40it/s]  2%|▏         | 7664/450337 [00:01<01:07, 6574.12it/s]  2%|▏         | 8327/450337 [00:01<01:07, 6525.91it/s]  2%|▏         | 8983/450337 [00:01<01:08, 6397.94it/s]  2%|▏         | 9630/450337 [00:01<01:08, 6418.44it/s]  2%|▏         | 10319/450337 [00:01<01:07, 6553.52it/s]  2%|▏         | 10976/450337 [00:01<01:07, 6478.69it/s]  3%|▎         | 11626/450337 [00:01<01:08, 6360.11it/s]  3%|▎         | 12304/450337 [00:01<01:07, 6479.60it/s]  3%|▎         | 12984/450337 [00:01<01:06, 6571.02it/s]  3%|▎         | 13703/450337 [00:02<01:04, 6752.73it/s]  3%|▎         | 14387/450337 [00:02<01:04, 6768.22it/s]  3%|▎         | 15066/450337 [00:02<01:04, 6772.56it/s]  3%|▎         | 15744/450337 [00:02<01:06, 6549.66it/s]  4%|▎         | 16401/450337 [00:02<01:08, 6330.88it/s]  4%|▍         | 17087/450337 [00:02<01:06, 6480.25it/s]  4%|▍         | 17738/450337 [00:02<01:07, 6410.33it/s]  4%|▍         | 18394/450337 [00:02<01:06, 6447.37it/s]  4%|▍         | 19133/450337 [00:02<01:04, 6720.45it/s]  4%|▍         | 19850/450337 [00:03<01:02, 6851.96it/s]  5%|▍         | 20537/450337 [00:03<01:04, 6625.97it/s]  5%|▍         | 21202/450337 [00:03<01:05, 6580.29it/s]  5%|▍         | 21862/450337 [00:03<01:06, 6421.29it/s]  5%|▌         | 22538/450337 [00:03<01:05, 6513.37it/s]  5%|▌         | 23218/450337 [00:03<01:04, 6594.94it/s]  5%|▌         | 24016/450337 [00:03<01:00, 7001.49it/s]  6%|▌         | 24798/450337 [00:03<00:58, 7238.16it/s]  6%|▌         | 25524/450337 [00:03<00:59, 7120.81it/s]  6%|▌         | 26238/450337 [00:03<01:02, 6779.00it/s]  6%|▌         | 26920/450337 [00:04<01:05, 6449.41it/s]  6%|▌         | 27570/450337 [00:04<01:07, 6243.06it/s]  6%|▋         | 28265/450337 [00:04<01:05, 6436.47it/s]  6%|▋         | 28956/450337 [00:04<01:04, 6565.79it/s]  7%|▋         | 29617/450337 [00:04<01:04, 6530.21it/s]  7%|▋         | 30278/450337 [00:04<01:04, 6552.30it/s]  7%|▋         | 30935/450337 [00:04<01:06, 6264.70it/s]  7%|▋         | 31584/450337 [00:04<01:06, 6324.00it/s]  7%|▋         | 32220/450337 [00:04<01:09, 6043.79it/s]  7%|▋         | 32860/450337 [00:05<01:07, 6139.97it/s]  7%|▋         | 33478/450337 [00:05<01:09, 6041.31it/s]  8%|▊         | 34139/450337 [00:05<01:07, 6201.18it/s]  8%|▊         | 34892/450337 [00:05<01:03, 6582.11it/s]  8%|▊         | 35553/450337 [00:05<01:05, 6343.26it/s]  8%|▊         | 36215/450337 [00:05<01:04, 6421.92it/s]  8%|▊         | 36860/450337 [00:05<01:04, 6376.87it/s]  8%|▊         | 37500/450337 [00:05<01:07, 6112.88it/s]  8%|▊         | 38116/450337 [00:05<01:07, 6118.27it/s]  9%|▊         | 38768/450337 [00:05<01:06, 6230.81it/s]  9%|▊         | 39394/450337 [00:06<01:06, 6179.90it/s]  9%|▉         | 40061/450337 [00:06<01:04, 6321.84it/s]  9%|▉         | 40821/450337 [00:06<01:01, 6694.37it/s]  9%|▉         | 41492/450337 [00:06<01:02, 6552.52it/s]  9%|▉         | 42149/450337 [00:06<01:06, 6170.69it/s]  9%|▉         | 42772/450337 [00:06<01:06, 6144.79it/s] 10%|▉         | 43390/450337 [00:06<01:06, 6135.48it/s] 10%|▉         | 44081/450337 [00:06<01:03, 6355.40it/s] 10%|▉         | 44778/450337 [00:06<01:02, 6534.38it/s] 10%|█         | 45434/450337 [00:06<01:02, 6517.71it/s] 10%|█         | 46179/450337 [00:07<00:59, 6792.47it/s] 10%|█         | 46860/450337 [00:07<01:00, 6645.32it/s] 11%|█         | 47908/450337 [00:07<00:51, 7768.82it/s] 11%|█         | 48689/450337 [00:07<00:52, 7596.56it/s] 11%|█         | 49472/450337 [00:07<00:52, 7658.64it/s] 11%|█         | 50241/450337 [00:07<00:56, 7084.46it/s] 11%|█▏        | 50960/450337 [00:07<00:58, 6809.71it/s] 11%|█▏        | 51649/450337 [00:07<00:58, 6765.97it/s] 12%|█▏        | 52516/450337 [00:07<00:54, 7300.70it/s] 12%|█▏        | 53254/450337 [00:08<00:55, 7091.71it/s] 12%|█▏        | 53969/450337 [00:08<00:57, 6848.83it/s] 12%|█▏        | 54659/450337 [00:08<01:01, 6389.09it/s] 12%|█▏        | 55309/450337 [00:08<01:01, 6414.94it/s] 12%|█▏        | 56084/450337 [00:08<00:58, 6787.84it/s] 13%|█▎        | 56770/450337 [00:08<01:00, 6490.94it/s] 13%|█▎        | 57426/450337 [00:08<01:03, 6183.94it/s] 13%|█▎        | 58097/450337 [00:08<01:02, 6325.42it/s] 13%|█▎        | 58872/450337 [00:08<00:58, 6727.24it/s] 13%|█▎        | 59552/450337 [00:09<01:00, 6422.95it/s] 13%|█▎        | 60201/450337 [00:09<01:00, 6405.55it/s] 14%|█▎        | 60977/450337 [00:09<00:57, 6787.27it/s] 14%|█▎        | 61675/450337 [00:09<00:56, 6837.91it/s] 14%|█▍        | 62363/450337 [00:09<00:57, 6711.72it/s] 14%|█▍        | 63038/450337 [00:09<00:58, 6577.37it/s] 14%|█▍        | 63716/450337 [00:09<00:58, 6635.01it/s] 14%|█▍        | 64382/450337 [00:09<00:59, 6504.62it/s] 14%|█▍        | 65035/450337 [00:09<00:59, 6510.00it/s] 15%|█▍        | 65688/450337 [00:09<00:59, 6417.03it/s] 15%|█▍        | 66360/450337 [00:10<00:59, 6502.80it/s] 15%|█▍        | 67051/450337 [00:10<00:57, 6620.43it/s] 15%|█▌        | 67714/450337 [00:10<01:00, 6295.34it/s] 15%|█▌        | 68368/450337 [00:10<01:00, 6363.84it/s] 15%|█▌        | 69093/450337 [00:10<00:57, 6621.01it/s] 16%|█▌        | 69831/450337 [00:10<00:55, 6837.10it/s] 16%|█▌        | 70518/450337 [00:10<00:55, 6838.20it/s] 16%|█▌        | 71204/450337 [00:10<00:57, 6592.79it/s] 16%|█▌        | 71873/450337 [00:10<00:57, 6617.56it/s] 16%|█▌        | 72563/450337 [00:11<00:56, 6696.63it/s] 16%|█▋        | 73416/450337 [00:11<00:52, 7236.01it/s] 16%|█▋        | 74241/450337 [00:11<00:49, 7533.70it/s] 17%|█▋        | 74997/450337 [00:11<00:51, 7286.05it/s] 17%|█▋        | 75729/450337 [00:11<00:54, 6875.49it/s] 17%|█▋        | 76459/450337 [00:11<00:53, 6992.29it/s] 17%|█▋        | 77164/450337 [00:11<00:54, 6790.37it/s] 17%|█▋        | 77891/450337 [00:11<00:53, 6926.42it/s] 17%|█▋        | 78588/450337 [00:11<00:56, 6546.68it/s] 18%|█▊        | 79249/450337 [00:12<01:00, 6157.39it/s] 18%|█▊        | 79888/450337 [00:12<00:59, 6217.56it/s] 18%|█▊        | 80529/450337 [00:12<00:58, 6270.35it/s] 18%|█▊        | 81284/450337 [00:12<00:55, 6633.18it/s] 18%|█▊        | 81953/450337 [00:12<00:56, 6545.59it/s] 18%|█▊        | 82636/450337 [00:12<00:55, 6625.33it/s] 19%|█▊        | 83374/450337 [00:12<00:53, 6841.72it/s] 19%|█▊        | 84072/450337 [00:12<00:53, 6881.13it/s] 19%|█▉        | 84775/450337 [00:12<00:52, 6922.24it/s] 19%|█▉        | 85469/450337 [00:12<00:55, 6593.04it/s] 19%|█▉        | 86133/450337 [00:13<00:55, 6562.22it/s] 19%|█▉        | 86847/450337 [00:13<00:54, 6726.56it/s] 19%|█▉        | 87585/450337 [00:13<00:52, 6917.56it/s] 20%|█▉        | 88283/450337 [00:13<00:52, 6932.01it/s] 20%|█▉        | 88978/450337 [00:13<00:56, 6366.22it/s] 20%|█▉        | 89625/450337 [00:13<00:56, 6378.51it/s] 20%|██        | 90270/450337 [00:13<00:57, 6237.23it/s] 20%|██        | 90899/450337 [00:13<00:58, 6165.39it/s] 20%|██        | 91649/450337 [00:13<00:54, 6545.15it/s] 21%|██        | 92349/450337 [00:13<00:53, 6671.80it/s] 21%|██        | 93020/450337 [00:14<00:54, 6610.95it/s] 21%|██        | 93699/450337 [00:14<00:53, 6654.27it/s] 21%|██        | 94367/450337 [00:14<00:54, 6475.85it/s] 21%|██        | 95097/450337 [00:14<00:52, 6705.48it/s] 21%|██▏       | 95770/450337 [00:14<00:54, 6524.78it/s] 21%|██▏       | 96590/450337 [00:14<00:50, 7008.48it/s] 22%|██▏       | 97348/450337 [00:14<00:49, 7175.34it/s] 22%|██▏       | 98069/450337 [00:14<00:52, 6723.37it/s] 22%|██▏       | 98749/450337 [00:14<00:54, 6444.38it/s] 22%|██▏       | 99400/450337 [00:15<00:55, 6309.36it/s] 22%|██▏       | 100050/450337 [00:15<00:55, 6358.38it/s] 22%|██▏       | 100690/450337 [00:15<00:56, 6195.81it/s] 23%|██▎       | 101382/450337 [00:15<00:54, 6398.46it/s] 23%|██▎       | 102107/450337 [00:15<00:52, 6640.91it/s] 23%|██▎       | 102775/450337 [00:15<00:53, 6531.44it/s] 23%|██▎       | 103431/450337 [00:15<00:54, 6401.12it/s] 23%|██▎       | 104073/450337 [00:15<00:56, 6180.16it/s] 23%|██▎       | 104773/450337 [00:15<00:53, 6409.19it/s] 23%|██▎       | 105417/450337 [00:16<00:54, 6370.94it/s] 24%|██▎       | 106076/450337 [00:16<00:53, 6430.18it/s] 24%|██▎       | 106721/450337 [00:16<00:53, 6373.55it/s] 24%|██▍       | 107360/450337 [00:16<00:55, 6229.58it/s] 24%|██▍       | 108072/450337 [00:16<00:52, 6481.24it/s] 24%|██▍       | 108722/450337 [00:16<00:56, 6076.19it/s] 24%|██▍       | 109371/450337 [00:16<00:55, 6190.71it/s] 24%|██▍       | 110040/450337 [00:16<00:53, 6327.40it/s] 25%|██▍       | 110764/450337 [00:16<00:51, 6584.92it/s] 25%|██▍       | 111478/450337 [00:16<00:50, 6741.70it/s] 25%|██▍       | 112155/450337 [00:17<00:51, 6581.41it/s] 25%|██▌       | 112864/450337 [00:17<00:50, 6721.43it/s] 25%|██▌       | 113539/450337 [00:17<00:52, 6373.92it/s] 25%|██▌       | 114247/450337 [00:17<00:51, 6574.41it/s] 26%|██▌       | 114909/450337 [00:17<00:51, 6526.61it/s] 26%|██▌       | 115565/450337 [00:17<00:54, 6164.65it/s] 26%|██▌       | 116221/450337 [00:17<00:53, 6271.60it/s] 26%|██▌       | 116853/450337 [00:17<00:53, 6186.52it/s] 26%|██▌       | 117487/450337 [00:17<00:53, 6229.14it/s] 26%|██▌       | 118124/450337 [00:17<00:52, 6269.44it/s] 26%|██▋       | 118818/450337 [00:18<00:51, 6465.62it/s] 27%|██▋       | 119467/450337 [00:18<00:51, 6379.75it/s] 27%|██▋       | 120257/450337 [00:18<00:48, 6819.02it/s] 27%|██▋       | 120941/450337 [00:18<00:49, 6674.86it/s] 27%|██▋       | 121611/450337 [00:18<00:51, 6440.11it/s] 27%|██▋       | 122265/450337 [00:18<00:50, 6468.35it/s] 27%|██▋       | 122914/450337 [00:18<00:51, 6390.55it/s] 27%|██▋       | 123555/450337 [00:18<00:51, 6372.11it/s] 28%|██▊       | 124194/450337 [00:18<00:51, 6334.40it/s] 28%|██▊       | 124829/450337 [00:19<00:51, 6301.98it/s] 28%|██▊       | 125470/450337 [00:19<00:51, 6333.28it/s] 28%|██▊       | 126186/450337 [00:19<00:49, 6577.31it/s] 28%|██▊       | 126926/450337 [00:19<00:47, 6821.25it/s] 28%|██▊       | 127609/450337 [00:19<00:49, 6517.81it/s] 28%|██▊       | 128268/450337 [00:19<00:49, 6531.03it/s] 29%|██▊       | 128924/450337 [00:19<00:49, 6471.97it/s] 29%|██▉       | 129623/450337 [00:19<00:48, 6623.13it/s] 29%|██▉       | 130287/450337 [00:19<00:51, 6273.93it/s] 29%|██▉       | 130919/450337 [00:19<00:50, 6269.87it/s] 29%|██▉       | 131550/450337 [00:20<00:51, 6231.16it/s] 29%|██▉       | 132176/450337 [00:20<00:53, 5935.83it/s] 30%|██▉       | 132910/450337 [00:20<00:50, 6324.36it/s] 30%|██▉       | 133548/450337 [00:20<00:50, 6323.02it/s] 30%|██▉       | 134291/450337 [00:20<00:47, 6638.73it/s] 30%|██▉       | 135061/450337 [00:20<00:45, 6949.60it/s] 30%|███       | 135793/450337 [00:20<00:44, 7053.77it/s] 30%|███       | 136501/450337 [00:20<00:44, 7003.33it/s] 30%|███       | 137321/450337 [00:20<00:42, 7356.54it/s] 31%|███       | 138059/450337 [00:21<00:44, 7012.09it/s] 31%|███       | 138765/450337 [00:21<00:44, 6987.17it/s] 31%|███       | 139467/450337 [00:21<00:45, 6770.30it/s] 31%|███       | 140148/450337 [00:21<00:46, 6705.02it/s] 31%|███▏      | 140821/450337 [00:21<00:46, 6708.23it/s] 31%|███▏      | 141494/450337 [00:21<00:47, 6511.53it/s] 32%|███▏      | 142147/450337 [00:21<00:48, 6315.45it/s] 32%|███▏      | 142804/450337 [00:21<00:48, 6382.76it/s] 32%|███▏      | 143588/450337 [00:21<00:45, 6797.77it/s] 32%|███▏      | 144271/450337 [00:21<00:45, 6714.74it/s] 32%|███▏      | 145075/450337 [00:22<00:43, 7093.89it/s] 32%|███▏      | 145787/450337 [00:22<00:43, 7015.02it/s] 33%|███▎      | 146491/450337 [00:22<00:44, 6761.42it/s] 33%|███▎      | 147170/450337 [00:22<00:45, 6609.34it/s] 33%|███▎      | 147834/450337 [00:22<00:48, 6239.42it/s] 33%|███▎      | 148495/450337 [00:22<00:47, 6340.50it/s] 33%|███▎      | 149196/450337 [00:22<00:46, 6522.73it/s] 33%|███▎      | 149852/450337 [00:22<00:47, 6370.13it/s] 33%|███▎      | 150492/450337 [00:22<00:47, 6272.50it/s] 34%|███▎      | 151122/450337 [00:23<00:47, 6251.05it/s] 34%|███▎      | 151762/450337 [00:23<00:47, 6291.49it/s] 34%|███▍      | 152393/450337 [00:23<00:47, 6249.15it/s] 34%|███▍      | 153092/450337 [00:23<00:45, 6465.05it/s] 34%|███▍      | 153740/450337 [00:23<00:45, 6464.99it/s] 34%|███▍      | 154388/450337 [00:23<00:46, 6354.98it/s] 34%|███▍      | 155053/450337 [00:23<00:45, 6435.26it/s] 35%|███▍      | 155759/450337 [00:23<00:44, 6618.19it/s] 35%|███▍      | 156453/450337 [00:23<00:43, 6710.49it/s] 35%|███▍      | 157151/450337 [00:23<00:43, 6783.14it/s] 35%|███▌      | 157869/450337 [00:24<00:42, 6894.95it/s] 35%|███▌      | 158559/450337 [00:24<00:42, 6843.21it/s] 35%|███▌      | 159244/450337 [00:24<00:44, 6486.19it/s] 36%|███▌      | 160001/450337 [00:24<00:42, 6796.30it/s] 36%|███▌      | 160685/450337 [00:24<00:43, 6625.17it/s] 36%|███▌      | 161388/450337 [00:24<00:42, 6740.46it/s] 36%|███▌      | 162065/450337 [00:24<00:43, 6624.60it/s] 36%|███▌      | 162760/450337 [00:24<00:42, 6714.78it/s] 36%|███▋      | 163434/450337 [00:24<00:43, 6662.15it/s] 36%|███▋      | 164102/450337 [00:24<00:45, 6341.71it/s] 37%|███▋      | 164836/450337 [00:25<00:43, 6625.39it/s] 37%|███▋      | 165596/450337 [00:25<00:41, 6906.77it/s] 37%|███▋      | 166291/450337 [00:25<00:42, 6741.85it/s] 37%|███▋      | 167056/450337 [00:25<00:40, 7004.33it/s] 37%|███▋      | 167760/450337 [00:25<00:41, 6836.29it/s] 37%|███▋      | 168447/450337 [00:25<00:43, 6505.90it/s] 38%|███▊      | 169166/450337 [00:25<00:41, 6695.02it/s] 38%|███▊      | 169840/450337 [00:25<00:43, 6452.19it/s] 38%|███▊      | 170490/450337 [00:25<00:44, 6319.01it/s] 38%|███▊      | 171125/450337 [00:26<00:45, 6162.04it/s] 38%|███▊      | 171805/450337 [00:26<00:43, 6341.51it/s] 38%|███▊      | 172442/450337 [00:26<00:44, 6264.62it/s] 38%|███▊      | 173089/450337 [00:26<00:43, 6318.75it/s] 39%|███▊      | 173750/450337 [00:26<00:43, 6401.47it/s] 39%|███▊      | 174392/450337 [00:26<00:43, 6273.83it/s] 39%|███▉      | 175143/450337 [00:26<00:41, 6631.47it/s] 39%|███▉      | 175809/450337 [00:26<00:42, 6433.69it/s] 39%|███▉      | 176455/450337 [00:26<00:42, 6401.36it/s] 39%|███▉      | 177097/450337 [00:26<00:43, 6246.88it/s] 39%|███▉      | 177724/450337 [00:27<00:47, 5748.11it/s] 40%|███▉      | 178409/450337 [00:27<00:44, 6047.87it/s] 40%|███▉      | 179112/450337 [00:27<00:42, 6321.49it/s] 40%|███▉      | 179752/450337 [00:27<00:43, 6261.36it/s] 40%|████      | 180410/450337 [00:27<00:42, 6348.17it/s] 40%|████      | 181049/450337 [00:27<00:43, 6229.78it/s] 40%|████      | 181675/450337 [00:27<00:43, 6139.87it/s] 40%|████      | 182310/450337 [00:27<00:43, 6199.82it/s] 41%|████      | 182990/450337 [00:27<00:41, 6371.73it/s] 41%|████      | 183629/450337 [00:28<00:42, 6215.78it/s] 41%|████      | 184253/450337 [00:28<00:45, 5842.12it/s] 41%|████      | 184941/450337 [00:28<00:43, 6129.94it/s] 41%|████      | 185560/450337 [00:28<00:44, 5921.55it/s] 41%|████▏     | 186212/450337 [00:28<00:43, 6086.97it/s] 41%|████▏     | 186826/450337 [00:28<00:43, 6098.89it/s] 42%|████▏     | 187529/450337 [00:28<00:41, 6364.82it/s] 42%|████▏     | 188169/450337 [00:28<00:41, 6300.49it/s] 42%|████▏     | 188906/450337 [00:28<00:39, 6608.59it/s] 42%|████▏     | 189569/450337 [00:28<00:39, 6538.76it/s] 42%|████▏     | 190232/450337 [00:29<00:39, 6565.46it/s] 42%|████▏     | 190899/450337 [00:29<00:39, 6592.68it/s] 43%|████▎     | 191560/450337 [00:29<00:39, 6500.08it/s] 43%|████▎     | 192263/450337 [00:29<00:38, 6653.23it/s] 43%|████▎     | 192930/450337 [00:29<00:38, 6626.83it/s] 43%|████▎     | 193594/450337 [00:29<00:38, 6600.84it/s] 43%|████▎     | 194258/450337 [00:29<00:38, 6609.38it/s] 43%|████▎     | 194920/450337 [00:29<00:39, 6525.96it/s] 43%|████▎     | 195585/450337 [00:29<00:38, 6556.32it/s] 44%|████▎     | 196241/450337 [00:30<00:39, 6410.54it/s] 44%|████▎     | 196932/450337 [00:30<00:38, 6556.52it/s] 44%|████▍     | 197855/450337 [00:30<00:34, 7343.31it/s] 44%|████▍     | 198592/450337 [00:30<00:36, 6832.83it/s] 44%|████▍     | 199284/450337 [00:30<00:36, 6804.08it/s] 44%|████▍     | 199970/450337 [00:30<00:38, 6581.62it/s] 45%|████▍     | 200633/450337 [00:30<00:39, 6242.83it/s] 45%|████▍     | 201314/450337 [00:30<00:38, 6394.40it/s] 45%|████▍     | 201959/450337 [00:30<00:38, 6375.39it/s] 45%|████▍     | 202613/450337 [00:30<00:38, 6418.16it/s] 45%|████▌     | 203315/450337 [00:31<00:37, 6592.55it/s] 45%|████▌     | 204027/450337 [00:31<00:36, 6740.39it/s] 45%|████▌     | 204704/450337 [00:31<00:38, 6454.13it/s] 46%|████▌     | 205354/450337 [00:31<00:38, 6424.75it/s] 46%|████▌     | 205999/450337 [00:31<00:38, 6323.61it/s] 46%|████▌     | 206673/450337 [00:31<00:37, 6443.76it/s] 46%|████▌     | 207320/450337 [00:31<00:38, 6241.98it/s] 46%|████▌     | 207988/450337 [00:31<00:38, 6367.53it/s] 46%|████▋     | 208627/450337 [00:31<00:38, 6207.80it/s] 46%|████▋     | 209250/450337 [00:32<00:39, 6118.88it/s] 47%|████▋     | 209941/450337 [00:32<00:37, 6342.85it/s] 47%|████▋     | 210625/450337 [00:32<00:36, 6484.26it/s] 47%|████▋     | 211297/450337 [00:32<00:36, 6551.95it/s] 47%|████▋     | 211954/450337 [00:32<00:36, 6534.14it/s] 47%|████▋     | 212704/450337 [00:32<00:34, 6817.00it/s] 47%|████▋     | 213387/450337 [00:32<00:35, 6671.82it/s] 48%|████▊     | 214056/450337 [00:32<00:36, 6394.49it/s] 48%|████▊     | 214699/450337 [00:32<00:37, 6232.22it/s] 48%|████▊     | 215325/450337 [00:32<00:38, 6131.90it/s] 48%|████▊     | 215946/450337 [00:33<00:38, 6148.34it/s] 48%|████▊     | 216562/450337 [00:33<00:38, 6043.93it/s] 48%|████▊     | 217219/450337 [00:33<00:37, 6190.06it/s] 48%|████▊     | 217933/450337 [00:33<00:35, 6458.50it/s] 49%|████▊     | 218607/450337 [00:33<00:35, 6537.11it/s] 49%|████▊     | 219315/450337 [00:33<00:34, 6690.52it/s] 49%|████▉     | 219986/450337 [00:33<00:35, 6490.06it/s] 49%|████▉     | 220637/450337 [00:33<00:36, 6295.96it/s] 49%|████▉     | 221340/450337 [00:33<00:35, 6504.06it/s] 49%|████▉     | 221993/450337 [00:34<00:36, 6259.64it/s] 49%|████▉     | 222623/450337 [00:34<00:37, 6138.40it/s] 50%|████▉     | 223254/450337 [00:34<00:36, 6180.81it/s] 50%|████▉     | 223874/450337 [00:34<00:37, 6041.76it/s] 50%|████▉     | 224623/450337 [00:34<00:35, 6445.06it/s] 50%|█████     | 225324/450337 [00:34<00:34, 6603.67it/s] 50%|█████     | 226137/450337 [00:34<00:31, 7050.88it/s] 50%|█████     | 226845/450337 [00:34<00:33, 6670.05it/s] 51%|█████     | 227518/450337 [00:34<00:33, 6562.71it/s] 51%|█████     | 228179/450337 [00:34<00:34, 6466.44it/s] 51%|█████     | 228829/450337 [00:35<00:35, 6239.91it/s] 51%|█████     | 229456/450337 [00:35<00:36, 6126.36it/s] 51%|█████     | 230071/450337 [00:35<00:36, 6022.46it/s] 51%|█████     | 230762/450337 [00:35<00:35, 6271.02it/s] 51%|█████▏    | 231392/450337 [00:35<00:35, 6247.66it/s] 52%|█████▏    | 232019/450337 [00:35<00:35, 6102.59it/s] 52%|█████▏    | 232751/450337 [00:35<00:33, 6453.84it/s] 52%|█████▏    | 233506/450337 [00:35<00:32, 6772.85it/s] 52%|█████▏    | 234222/450337 [00:35<00:31, 6884.55it/s] 52%|█████▏    | 234913/450337 [00:36<00:31, 6877.88it/s] 52%|█████▏    | 235609/450337 [00:36<00:31, 6894.74it/s] 52%|█████▏    | 236300/450337 [00:36<00:32, 6664.65it/s] 53%|█████▎    | 236969/450337 [00:36<00:33, 6457.30it/s] 53%|█████▎    | 237646/450337 [00:36<00:32, 6544.96it/s] 53%|█████▎    | 238303/450337 [00:36<00:32, 6512.60it/s] 53%|█████▎    | 239019/450337 [00:36<00:31, 6701.03it/s] 53%|█████▎    | 239691/450337 [00:36<00:31, 6622.57it/s] 53%|█████▎    | 240355/450337 [00:36<00:32, 6485.04it/s] 54%|█████▎    | 241226/450337 [00:36<00:29, 7128.32it/s] 54%|█████▎    | 241942/450337 [00:37<00:29, 7023.65it/s] 54%|█████▍    | 242647/450337 [00:37<00:30, 6825.94it/s] 54%|█████▍    | 243332/450337 [00:37<00:30, 6693.59it/s] 54%|█████▍    | 244004/450337 [00:37<00:30, 6665.07it/s] 54%|█████▍    | 244684/450337 [00:37<00:30, 6702.34it/s] 54%|█████▍    | 245356/450337 [00:37<00:30, 6674.64it/s] 55%|█████▍    | 246100/450337 [00:37<00:29, 6893.04it/s] 55%|█████▍    | 246926/450337 [00:37<00:27, 7291.68it/s] 55%|█████▍    | 247657/450337 [00:37<00:28, 7004.59it/s] 55%|█████▌    | 248361/450337 [00:37<00:29, 6872.94it/s] 55%|█████▌    | 249089/450337 [00:38<00:28, 6986.51it/s] 55%|█████▌    | 249827/450337 [00:38<00:28, 7093.30it/s] 56%|█████▌    | 250539/450337 [00:38<00:29, 6882.36it/s] 56%|█████▌    | 251230/450337 [00:38<00:28, 6868.72it/s] 56%|█████▌    | 251919/450337 [00:38<00:29, 6826.81it/s] 56%|█████▌    | 252603/450337 [00:38<00:28, 6829.57it/s] 56%|█████▌    | 253287/450337 [00:38<00:30, 6464.76it/s] 56%|█████▋    | 254088/450337 [00:38<00:28, 6901.59it/s] 57%|█████▋    | 254784/450337 [00:38<00:29, 6624.17it/s] 57%|█████▋    | 255452/450337 [00:39<00:29, 6589.62it/s] 57%|█████▋    | 256179/450337 [00:39<00:28, 6782.17it/s] 57%|█████▋    | 256895/450337 [00:39<00:28, 6883.08it/s] 57%|█████▋    | 257586/450337 [00:39<00:28, 6881.81it/s] 57%|█████▋    | 258277/450337 [00:39<00:29, 6412.34it/s] 57%|█████▋    | 258926/450337 [00:39<00:30, 6218.66it/s] 58%|█████▊    | 259554/450337 [00:39<00:30, 6157.58it/s] 58%|█████▊    | 260251/450337 [00:39<00:29, 6384.35it/s] 58%|█████▊    | 261043/450337 [00:39<00:27, 6815.21it/s] 58%|█████▊    | 261729/450337 [00:39<00:28, 6649.24it/s] 58%|█████▊    | 262398/450337 [00:40<00:29, 6332.18it/s] 58%|█████▊    | 263036/450337 [00:40<00:29, 6321.76it/s] 59%|█████▊    | 263707/450337 [00:40<00:29, 6421.69it/s] 59%|█████▊    | 264352/450337 [00:40<00:30, 6153.71it/s] 59%|█████▉    | 265075/450337 [00:40<00:28, 6451.67it/s] 59%|█████▉    | 265725/450337 [00:40<00:29, 6296.66it/s] 59%|█████▉    | 266358/450337 [00:40<00:29, 6285.97it/s] 59%|█████▉    | 266989/450337 [00:40<00:29, 6161.89it/s] 59%|█████▉    | 267622/450337 [00:40<00:29, 6209.83it/s] 60%|█████▉    | 268245/450337 [00:41<00:29, 6199.61it/s] 60%|█████▉    | 268962/450337 [00:41<00:27, 6482.57it/s] 60%|█████▉    | 269612/450337 [00:41<00:28, 6442.46it/s] 60%|██████    | 270316/450337 [00:41<00:27, 6616.63it/s] 60%|██████    | 270979/450337 [00:41<00:27, 6488.05it/s] 60%|██████    | 271629/450337 [00:41<00:27, 6410.06it/s] 61%|██████    | 272459/450337 [00:41<00:25, 6962.04it/s] 61%|██████    | 273191/450337 [00:41<00:25, 7067.18it/s] 61%|██████    | 273900/450337 [00:41<00:25, 6974.00it/s] 61%|██████    | 274599/450337 [00:41<00:26, 6754.67it/s] 61%|██████    | 275277/450337 [00:42<00:27, 6371.02it/s] 61%|██████▏   | 275950/450337 [00:42<00:26, 6470.82it/s] 61%|██████▏   | 276602/450337 [00:42<00:28, 6135.60it/s] 62%|██████▏   | 277271/450337 [00:42<00:27, 6289.50it/s] 62%|██████▏   | 277940/450337 [00:42<00:26, 6401.91it/s] 62%|██████▏   | 278674/450337 [00:42<00:25, 6671.00it/s] 62%|██████▏   | 279345/450337 [00:42<00:26, 6347.87it/s] 62%|██████▏   | 280001/450337 [00:42<00:26, 6407.00it/s] 62%|██████▏   | 280647/450337 [00:42<00:26, 6420.84it/s] 62%|██████▏   | 281308/450337 [00:43<00:26, 6475.20it/s] 63%|██████▎   | 281958/450337 [00:43<00:26, 6400.33it/s] 63%|██████▎   | 282600/450337 [00:43<00:26, 6261.18it/s] 63%|██████▎   | 283302/450337 [00:43<00:25, 6480.65it/s] 63%|██████▎   | 283988/450337 [00:43<00:25, 6588.75it/s] 63%|██████▎   | 284649/450337 [00:43<00:25, 6526.70it/s] 63%|██████▎   | 285305/450337 [00:43<00:25, 6517.38it/s] 64%|██████▎   | 286014/450337 [00:43<00:24, 6682.74it/s] 64%|██████▎   | 286684/450337 [00:43<00:25, 6400.95it/s] 64%|██████▍   | 287327/450337 [00:43<00:25, 6315.24it/s] 64%|██████▍   | 288096/450337 [00:44<00:24, 6706.62it/s] 64%|██████▍   | 288770/450337 [00:44<00:24, 6610.86it/s] 64%|██████▍   | 289434/450337 [00:44<00:24, 6570.72it/s] 64%|██████▍   | 290093/450337 [00:44<00:25, 6248.46it/s] 65%|██████▍   | 290776/450337 [00:44<00:24, 6404.89it/s] 65%|██████▍   | 291420/450337 [00:44<00:25, 6259.22it/s] 65%|██████▍   | 292049/450337 [00:44<00:25, 6223.64it/s] 65%|██████▌   | 292778/450337 [00:44<00:24, 6531.56it/s] 65%|██████▌   | 293434/450337 [00:44<00:24, 6296.14it/s] 65%|██████▌   | 294071/450337 [00:45<00:24, 6314.01it/s] 65%|██████▌   | 294768/450337 [00:45<00:23, 6503.57it/s] 66%|██████▌   | 295465/450337 [00:45<00:23, 6638.16it/s] 66%|██████▌   | 296153/450337 [00:45<00:22, 6707.98it/s] 66%|██████▌   | 296826/450337 [00:45<00:23, 6552.60it/s] 66%|██████▌   | 297483/450337 [00:45<00:23, 6464.16it/s] 66%|██████▌   | 298214/450337 [00:45<00:22, 6710.66it/s] 66%|██████▋   | 298887/450337 [00:45<00:23, 6536.66it/s] 67%|██████▋   | 299587/450337 [00:45<00:22, 6667.13it/s] 67%|██████▋   | 300256/450337 [00:45<00:22, 6589.95it/s] 67%|██████▋   | 300917/450337 [00:46<00:23, 6419.09it/s] 67%|██████▋   | 301561/450337 [00:46<00:23, 6415.40it/s] 67%|██████▋   | 302204/450337 [00:46<00:23, 6247.38it/s] 67%|██████▋   | 302917/450337 [00:46<00:22, 6501.79it/s] 67%|██████▋   | 303653/450337 [00:46<00:21, 6751.74it/s] 68%|██████▊   | 304331/450337 [00:46<00:21, 6656.41it/s] 68%|██████▊   | 304999/450337 [00:46<00:22, 6578.41it/s] 68%|██████▊   | 305658/450337 [00:46<00:22, 6343.72it/s] 68%|██████▊   | 306359/450337 [00:46<00:22, 6524.24it/s] 68%|██████▊   | 307014/450337 [00:47<00:22, 6367.06it/s] 68%|██████▊   | 307757/450337 [00:47<00:21, 6671.07it/s] 68%|██████▊   | 308427/450337 [00:47<00:23, 6026.10it/s] 69%|██████▊   | 309274/450337 [00:47<00:21, 6690.34it/s] 69%|██████▉   | 310052/450337 [00:47<00:20, 6994.10it/s] 69%|██████▉   | 310765/450337 [00:47<00:20, 6757.58it/s] 69%|██████▉   | 311451/450337 [00:47<00:20, 6634.70it/s] 69%|██████▉   | 312122/450337 [00:47<00:20, 6593.92it/s] 69%|██████▉   | 312787/450337 [00:47<00:20, 6570.81it/s] 70%|██████▉   | 313479/450337 [00:47<00:20, 6666.52it/s] 70%|██████▉   | 314228/450337 [00:48<00:19, 6895.40it/s] 70%|██████▉   | 314920/450337 [00:48<00:20, 6536.07it/s] 70%|███████   | 315579/450337 [00:48<00:20, 6432.58it/s] 70%|███████   | 316240/450337 [00:48<00:20, 6482.93it/s] 70%|███████   | 316891/450337 [00:48<00:20, 6383.51it/s] 71%|███████   | 317532/450337 [00:48<00:21, 6265.44it/s] 71%|███████   | 318161/450337 [00:48<00:21, 6146.70it/s] 71%|███████   | 318802/450337 [00:48<00:21, 6222.12it/s] 71%|███████   | 319503/450337 [00:48<00:20, 6450.74it/s] 71%|███████   | 320150/450337 [00:49<00:20, 6304.00it/s] 71%|███████   | 320833/450337 [00:49<00:20, 6455.56it/s] 71%|███████▏  | 321576/450337 [00:49<00:19, 6735.07it/s] 72%|███████▏  | 322252/450337 [00:49<00:20, 6287.18it/s] 72%|███████▏  | 322888/450337 [00:49<00:20, 6293.27it/s] 72%|███████▏  | 323551/450337 [00:49<00:19, 6387.85it/s] 72%|███████▏  | 324235/450337 [00:49<00:19, 6515.44it/s] 72%|███████▏  | 324890/450337 [00:49<00:19, 6519.59it/s] 72%|███████▏  | 325545/450337 [00:49<00:20, 6016.65it/s] 72%|███████▏  | 326197/450337 [00:49<00:20, 6157.26it/s] 73%|███████▎  | 326945/450337 [00:50<00:18, 6526.10it/s] 73%|███████▎  | 327673/450337 [00:50<00:18, 6741.62it/s] 73%|███████▎  | 328462/450337 [00:50<00:17, 7069.70it/s] 73%|███████▎  | 329174/450337 [00:50<00:18, 6727.95it/s] 73%|███████▎  | 329854/450337 [00:50<00:18, 6551.61it/s] 73%|███████▎  | 330514/450337 [00:50<00:19, 6121.35it/s] 74%|███████▎  | 331208/450337 [00:50<00:18, 6329.11it/s] 74%|███████▎  | 331848/450337 [00:50<00:19, 6175.41it/s] 74%|███████▍  | 332558/450337 [00:50<00:18, 6434.42it/s] 74%|███████▍  | 333207/450337 [00:51<00:18, 6258.77it/s] 74%|███████▍  | 333905/450337 [00:51<00:18, 6459.85it/s] 74%|███████▍  | 334555/450337 [00:51<00:18, 6390.03it/s] 74%|███████▍  | 335222/450337 [00:51<00:17, 6468.21it/s] 75%|███████▍  | 335893/450337 [00:51<00:17, 6537.59it/s] 75%|███████▍  | 336572/450337 [00:51<00:17, 6610.62it/s] 75%|███████▍  | 337235/450337 [00:51<00:17, 6613.15it/s] 75%|███████▌  | 337973/450337 [00:51<00:16, 6834.90it/s] 75%|███████▌  | 338658/450337 [00:51<00:16, 6714.21it/s] 75%|███████▌  | 339371/450337 [00:51<00:16, 6827.26it/s] 76%|███████▌  | 340055/450337 [00:52<00:17, 6146.80it/s] 76%|███████▌  | 340744/450337 [00:52<00:17, 6349.53it/s] 76%|███████▌  | 341415/450337 [00:52<00:16, 6442.45it/s] 76%|███████▌  | 342070/450337 [00:52<00:16, 6468.64it/s] 76%|███████▌  | 342741/450337 [00:52<00:16, 6537.09it/s] 76%|███████▋  | 343408/450337 [00:52<00:16, 6570.80it/s] 76%|███████▋  | 344169/450337 [00:52<00:15, 6865.95it/s] 77%|███████▋  | 344859/450337 [00:52<00:15, 6634.12it/s] 77%|███████▋  | 345526/450337 [00:52<00:16, 6519.58it/s] 77%|███████▋  | 346181/450337 [00:53<00:16, 6199.57it/s] 77%|███████▋  | 346912/450337 [00:53<00:15, 6507.47it/s] 77%|███████▋  | 347574/450337 [00:53<00:15, 6533.43it/s] 77%|███████▋  | 348231/450337 [00:53<00:16, 6349.87it/s] 77%|███████▋  | 348894/450337 [00:53<00:15, 6421.49it/s] 78%|███████▊  | 349567/450337 [00:53<00:15, 6509.01it/s] 78%|███████▊  | 350441/450337 [00:53<00:13, 7159.17it/s] 78%|███████▊  | 351160/450337 [00:53<00:14, 6754.91it/s] 78%|███████▊  | 351842/450337 [00:53<00:14, 6708.45it/s] 78%|███████▊  | 352518/450337 [00:54<00:16, 6068.06it/s] 78%|███████▊  | 353193/450337 [00:54<00:15, 6243.98it/s] 79%|███████▊  | 353829/450337 [00:54<00:15, 6150.69it/s] 79%|███████▊  | 354616/450337 [00:54<00:14, 6626.89it/s] 79%|███████▉  | 355288/450337 [00:54<00:15, 6256.67it/s] 79%|███████▉  | 355992/450337 [00:54<00:14, 6470.46it/s] 79%|███████▉  | 356648/450337 [00:54<00:14, 6412.89it/s] 79%|███████▉  | 357295/450337 [00:54<00:15, 5983.15it/s] 79%|███████▉  | 357996/450337 [00:54<00:14, 6263.67it/s] 80%|███████▉  | 358641/450337 [00:54<00:14, 6311.13it/s] 80%|███████▉  | 359287/450337 [00:55<00:14, 6352.69it/s] 80%|███████▉  | 359927/450337 [00:55<00:14, 6343.74it/s] 80%|████████  | 360674/450337 [00:55<00:13, 6672.67it/s] 80%|████████  | 361365/450337 [00:55<00:13, 6739.90it/s] 80%|████████  | 362175/450337 [00:55<00:12, 7137.78it/s] 81%|████████  | 362891/450337 [00:55<00:12, 6882.04it/s] 81%|████████  | 363583/450337 [00:55<00:12, 6849.46it/s] 81%|████████  | 364297/450337 [00:55<00:12, 6932.99it/s] 81%|████████  | 364995/450337 [00:55<00:12, 6940.78it/s] 81%|████████  | 365696/450337 [00:56<00:12, 6954.07it/s] 81%|████████▏ | 366393/450337 [00:56<00:12, 6710.48it/s] 82%|████████▏ | 367067/450337 [00:56<00:12, 6667.14it/s] 82%|████████▏ | 367736/450337 [00:56<00:12, 6373.20it/s] 82%|████████▏ | 368377/450337 [00:56<00:13, 6299.02it/s] 82%|████████▏ | 369069/450337 [00:56<00:12, 6471.19it/s] 82%|████████▏ | 369754/450337 [00:56<00:12, 6570.53it/s] 82%|████████▏ | 370413/450337 [00:56<00:12, 6474.92it/s] 82%|████████▏ | 371062/450337 [00:56<00:12, 6446.93it/s] 83%|████████▎ | 371708/450337 [00:56<00:12, 6363.21it/s] 83%|████████▎ | 372346/450337 [00:57<00:12, 6234.43it/s] 83%|████████▎ | 373042/450337 [00:57<00:11, 6444.58it/s] 83%|████████▎ | 373688/450337 [00:57<00:11, 6397.66it/s] 83%|████████▎ | 374419/450337 [00:57<00:11, 6664.87it/s] 83%|████████▎ | 375087/450337 [00:57<00:11, 6628.08it/s] 83%|████████▎ | 375751/450337 [00:57<00:11, 6308.29it/s] 84%|████████▎ | 376397/450337 [00:57<00:11, 6346.23it/s] 84%|████████▎ | 377035/450337 [00:57<00:12, 6091.72it/s] 84%|████████▍ | 377699/450337 [00:57<00:11, 6246.17it/s] 84%|████████▍ | 378408/450337 [00:57<00:11, 6485.67it/s] 84%|████████▍ | 379115/450337 [00:58<00:10, 6655.49it/s] 84%|████████▍ | 379804/450337 [00:58<00:10, 6722.25it/s] 84%|████████▍ | 380479/450337 [00:58<00:10, 6625.22it/s] 85%|████████▍ | 381265/450337 [00:58<00:09, 6987.73it/s] 85%|████████▍ | 381966/450337 [00:58<00:10, 6731.07it/s] 85%|████████▍ | 382643/450337 [00:58<00:10, 6362.49it/s] 85%|████████▌ | 383285/450337 [00:58<00:10, 6291.71it/s] 85%|████████▌ | 383918/450337 [00:58<00:10, 6260.72it/s] 85%|████████▌ | 384709/450337 [00:58<00:09, 6733.26it/s] 86%|████████▌ | 385387/450337 [00:59<00:09, 6680.07it/s] 86%|████████▌ | 386058/450337 [00:59<00:10, 6356.02it/s] 86%|████████▌ | 386700/450337 [00:59<00:09, 6369.03it/s] 86%|████████▌ | 387507/450337 [00:59<00:09, 6858.64it/s] 86%|████████▌ | 388198/450337 [00:59<00:09, 6773.47it/s] 86%|████████▋ | 388879/450337 [00:59<00:09, 6555.88it/s] 87%|████████▋ | 389585/450337 [00:59<00:09, 6697.09it/s] 87%|████████▋ | 390304/450337 [00:59<00:08, 6836.90it/s] 87%|████████▋ | 390991/450337 [00:59<00:08, 6681.26it/s] 87%|████████▋ | 391662/450337 [01:00<00:09, 6279.62it/s] 87%|████████▋ | 392296/450337 [01:00<00:09, 6233.90it/s] 87%|████████▋ | 392924/450337 [01:00<00:09, 6226.84it/s] 87%|████████▋ | 393550/450337 [01:00<00:09, 6063.02it/s] 88%|████████▊ | 394294/450337 [01:00<00:08, 6453.86it/s] 88%|████████▊ | 394943/450337 [01:00<00:08, 6337.02it/s] 88%|████████▊ | 395656/450337 [01:00<00:08, 6563.68it/s] 88%|████████▊ | 396316/450337 [01:00<00:08, 6408.60it/s] 88%|████████▊ | 396960/450337 [01:00<00:08, 6254.16it/s] 88%|████████▊ | 397588/450337 [01:00<00:08, 6089.49it/s] 88%|████████▊ | 398325/450337 [01:01<00:08, 6451.79it/s] 89%|████████▊ | 399051/450337 [01:01<00:07, 6679.96it/s] 89%|████████▉ | 399820/450337 [01:01<00:07, 6965.81it/s] 89%|████████▉ | 400520/450337 [01:01<00:07, 6791.96it/s] 89%|████████▉ | 401202/450337 [01:01<00:07, 6638.11it/s] 89%|████████▉ | 401869/450337 [01:01<00:07, 6310.40it/s] 89%|████████▉ | 402505/450337 [01:01<00:08, 5910.39it/s] 90%|████████▉ | 403175/450337 [01:01<00:07, 6122.83it/s] 90%|████████▉ | 403837/450337 [01:01<00:07, 6255.85it/s] 90%|████████▉ | 404488/450337 [01:02<00:07, 6324.57it/s] 90%|████████▉ | 405125/450337 [01:02<00:07, 6147.25it/s] 90%|█████████ | 405764/450337 [01:02<00:07, 6214.11it/s] 90%|█████████ | 406389/450337 [01:02<00:07, 6211.73it/s] 90%|█████████ | 407020/450337 [01:02<00:06, 6235.27it/s] 91%|█████████ | 407645/450337 [01:02<00:07, 6079.48it/s] 91%|█████████ | 408363/450337 [01:02<00:06, 6389.84it/s] 91%|█████████ | 409078/450337 [01:02<00:06, 6609.65it/s] 91%|█████████ | 409741/450337 [01:02<00:06, 6400.91it/s] 91%|█████████ | 410384/450337 [01:02<00:06, 6331.21it/s] 91%|█████████▏| 411049/450337 [01:03<00:06, 6420.61it/s] 91%|█████████▏| 411693/450337 [01:03<00:06, 6379.00it/s] 92%|█████████▏| 412332/450337 [01:03<00:06, 6007.65it/s] 92%|█████████▏| 412938/450337 [01:03<00:06, 5863.32it/s] 92%|█████████▏| 413593/450337 [01:03<00:06, 6050.37it/s] 92%|█████████▏| 414202/450337 [01:03<00:06, 5904.76it/s] 92%|█████████▏| 414796/450337 [01:03<00:06, 5779.39it/s] 92%|█████████▏| 415447/450337 [01:03<00:05, 5986.73it/s] 92%|█████████▏| 416073/450337 [01:03<00:05, 6065.66it/s] 93%|█████████▎| 416682/450337 [01:04<00:05, 6021.33it/s] 93%|█████████▎| 417325/450337 [01:04<00:05, 6137.64it/s] 93%|█████████▎| 418078/450337 [01:04<00:04, 6545.22it/s] 93%|█████████▎| 418735/450337 [01:04<00:05, 6225.18it/s] 93%|█████████▎| 419423/450337 [01:04<00:04, 6412.46it/s] 93%|█████████▎| 420074/450337 [01:04<00:04, 6430.84it/s] 93%|█████████▎| 420730/450337 [01:04<00:04, 6465.01it/s] 94%|█████████▎| 421379/450337 [01:04<00:04, 6106.02it/s] 94%|█████████▎| 422004/450337 [01:04<00:04, 6146.80it/s] 94%|█████████▍| 422749/450337 [01:04<00:04, 6522.65it/s] 94%|█████████▍| 423406/450337 [01:05<00:04, 6400.25it/s] 94%|█████████▍| 424050/450337 [01:05<00:04, 6343.27it/s] 94%|█████████▍| 424687/450337 [01:05<00:04, 6274.19it/s] 94%|█████████▍| 425316/450337 [01:05<00:04, 6235.45it/s] 95%|█████████▍| 426027/450337 [01:05<00:03, 6490.21it/s] 95%|█████████▍| 426808/450337 [01:05<00:03, 6874.25it/s] 95%|█████████▍| 427498/450337 [01:05<00:03, 6700.42it/s] 95%|█████████▌| 428170/450337 [01:05<00:03, 6611.46it/s] 95%|█████████▌| 428873/450337 [01:05<00:03, 6724.76it/s] 95%|█████████▌| 429603/450337 [01:05<00:03, 6891.47it/s] 96%|█████████▌| 430294/450337 [01:06<00:02, 6860.15it/s] 96%|█████████▌| 430981/450337 [01:06<00:02, 6520.13it/s] 96%|█████████▌| 431637/450337 [01:06<00:02, 6295.27it/s] 96%|█████████▌| 432305/450337 [01:06<00:02, 6402.91it/s] 96%|█████████▌| 433200/450337 [01:06<00:02, 7125.60it/s] 96%|█████████▋| 433918/450337 [01:06<00:02, 6814.76it/s] 97%|█████████▋| 434606/450337 [01:06<00:02, 6348.49it/s] 97%|█████████▋| 435250/450337 [01:06<00:02, 6257.69it/s] 97%|█████████▋| 435944/450337 [01:06<00:02, 6445.46it/s] 97%|█████████▋| 436595/450337 [01:07<00:02, 6351.12it/s] 97%|█████████▋| 437311/450337 [01:07<00:01, 6570.29it/s] 97%|█████████▋| 437972/450337 [01:07<00:01, 6485.11it/s] 97%|█████████▋| 438624/450337 [01:07<00:01, 6413.68it/s] 98%|█████████▊| 439268/450337 [01:07<00:01, 6228.09it/s] 98%|█████████▊| 439893/450337 [01:07<00:01, 6154.59it/s] 98%|█████████▊| 440514/450337 [01:07<00:01, 6170.38it/s] 98%|█████████▊| 441132/450337 [01:07<00:01, 6151.53it/s] 98%|█████████▊| 441843/450337 [01:07<00:01, 6431.36it/s] 98%|█████████▊| 442488/450337 [01:08<00:01, 6403.46it/s] 98%|█████████▊| 443171/450337 [01:08<00:01, 6526.16it/s] 99%|█████████▊| 443903/450337 [01:08<00:00, 6755.96it/s] 99%|█████████▊| 444580/450337 [01:08<00:00, 6312.90it/s] 99%|█████████▉| 445218/450337 [01:08<00:00, 6060.43it/s] 99%|█████████▉| 445862/450337 [01:08<00:00, 6157.25it/s] 99%|█████████▉| 446483/450337 [01:08<00:00, 6103.98it/s] 99%|█████████▉| 447141/450337 [01:08<00:00, 6239.98it/s] 99%|█████████▉| 447845/450337 [01:08<00:00, 6470.42it/s]100%|█████████▉| 448532/450337 [01:08<00:00, 6584.79it/s]100%|█████████▉| 449193/450337 [01:09<00:00, 6590.23it/s]100%|█████████▉| 449854/450337 [01:09<00:00, 6258.60it/s]100%|██████████| 450337/450337 [01:09<00:00, 6505.58it/s]

gathering stats for n=1
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 1891/450337 [00:00<00:23, 18888.85it/s]  1%|          | 3933/450337 [00:00<00:22, 19782.41it/s]  1%|▏         | 6157/450337 [00:00<00:21, 20894.68it/s]  2%|▏         | 8247/450337 [00:00<00:22, 19955.65it/s]  2%|▏         | 10285/450337 [00:00<00:21, 20103.31it/s]  3%|▎         | 12300/450337 [00:00<00:22, 19844.46it/s]  3%|▎         | 14358/450337 [00:00<00:21, 20077.25it/s]  4%|▎         | 16369/450337 [00:00<00:21, 19808.58it/s]  4%|▍         | 18366/450337 [00:00<00:21, 19852.28it/s]  5%|▍         | 20489/450337 [00:01<00:21, 20268.74it/s]  5%|▌         | 22518/450337 [00:01<00:21, 20139.83it/s]  6%|▌         | 24807/450337 [00:01<00:20, 20962.24it/s]  6%|▌         | 26906/450337 [00:01<00:20, 20414.69it/s]  6%|▋         | 28952/450337 [00:01<00:20, 20345.07it/s]  7%|▋         | 30990/450337 [00:01<00:20, 19970.13it/s]  7%|▋         | 32990/450337 [00:01<00:21, 19720.56it/s]  8%|▊         | 35004/450337 [00:01<00:20, 19838.69it/s]  8%|▊         | 36990/450337 [00:01<00:21, 19570.92it/s]  9%|▊         | 38949/450337 [00:01<00:21, 19431.67it/s]  9%|▉         | 40998/450337 [00:02<00:20, 19740.93it/s] 10%|▉         | 42974/450337 [00:02<00:21, 19292.99it/s] 10%|▉         | 45003/450337 [00:02<00:20, 19583.03it/s] 10%|█         | 47091/450337 [00:02<00:20, 19958.82it/s] 11%|█         | 49575/450337 [00:02<00:18, 21403.28it/s] 11%|█▏        | 51720/450337 [00:02<00:19, 20726.71it/s] 12%|█▏        | 53896/450337 [00:02<00:18, 21027.21it/s] 12%|█▏        | 56005/450337 [00:02<00:19, 20641.83it/s] 13%|█▎        | 58075/450337 [00:02<00:19, 20069.20it/s] 13%|█▎        | 60113/450337 [00:02<00:19, 20158.55it/s] 14%|█▍        | 62262/450337 [00:03<00:18, 20546.52it/s] 14%|█▍        | 64321/450337 [00:03<00:19, 20308.49it/s] 15%|█▍        | 66355/450337 [00:03<00:18, 20227.30it/s] 15%|█▌        | 68380/450337 [00:03<00:19, 19967.75it/s] 16%|█▌        | 70575/450337 [00:03<00:18, 20547.51it/s] 16%|█▌        | 72633/450337 [00:03<00:18, 20525.29it/s] 17%|█▋        | 74992/450337 [00:03<00:17, 21433.02it/s] 17%|█▋        | 77138/450337 [00:03<00:17, 21002.78it/s] 18%|█▊        | 79242/450337 [00:03<00:18, 20331.27it/s] 18%|█▊        | 81319/450337 [00:04<00:18, 20457.01it/s] 19%|█▊        | 83374/450337 [00:04<00:17, 20482.08it/s] 19%|█▉        | 85426/450337 [00:04<00:17, 20372.83it/s] 19%|█▉        | 87560/450337 [00:04<00:17, 20650.67it/s] 20%|█▉        | 89628/450337 [00:04<00:17, 20155.42it/s] 20%|██        | 91648/450337 [00:04<00:17, 20046.24it/s] 21%|██        | 93722/450337 [00:04<00:17, 20249.40it/s] 21%|██▏       | 95750/450337 [00:04<00:17, 20130.78it/s] 22%|██▏       | 97993/450337 [00:04<00:16, 20809.55it/s] 22%|██▏       | 100077/450337 [00:04<00:17, 20211.29it/s] 23%|██▎       | 102122/450337 [00:05<00:17, 20280.41it/s] 23%|██▎       | 104154/450337 [00:05<00:17, 19729.86it/s] 24%|██▎       | 106186/450337 [00:05<00:17, 19897.51it/s] 24%|██▍       | 108180/450337 [00:05<00:17, 19714.84it/s] 24%|██▍       | 110155/450337 [00:05<00:17, 19530.67it/s] 25%|██▍       | 112257/450337 [00:05<00:16, 19966.25it/s] 25%|██▌       | 114257/450337 [00:05<00:16, 19939.59it/s] 26%|██▌       | 116253/450337 [00:05<00:17, 19586.50it/s] 26%|██▋       | 118214/450337 [00:05<00:17, 19496.06it/s] 27%|██▋       | 120329/450337 [00:05<00:16, 19976.83it/s] 27%|██▋       | 122329/450337 [00:06<00:16, 19778.95it/s] 28%|██▊       | 124309/450337 [00:06<00:16, 19639.22it/s] 28%|██▊       | 126373/450337 [00:06<00:16, 19930.41it/s] 29%|██▊       | 128368/450337 [00:06<00:16, 19889.79it/s] 29%|██▉       | 130358/450337 [00:06<00:16, 19824.73it/s] 29%|██▉       | 132342/450337 [00:06<00:16, 19561.45it/s] 30%|██▉       | 134397/450337 [00:06<00:15, 19848.00it/s] 30%|███       | 136663/450337 [00:06<00:15, 20681.74it/s] 31%|███       | 138780/450337 [00:06<00:14, 20820.47it/s] 31%|███▏      | 140864/450337 [00:06<00:14, 20642.20it/s] 32%|███▏      | 142930/450337 [00:07<00:15, 20084.21it/s] 32%|███▏      | 145188/450337 [00:07<00:14, 20805.64it/s] 33%|███▎      | 147274/450337 [00:07<00:14, 20460.92it/s] 33%|███▎      | 149324/450337 [00:07<00:14, 20095.62it/s] 34%|███▎      | 151337/450337 [00:07<00:15, 19783.22it/s] 34%|███▍      | 153394/450337 [00:07<00:14, 20008.35it/s] 35%|███▍      | 155398/450337 [00:07<00:14, 19773.97it/s] 35%|███▍      | 157526/450337 [00:07<00:14, 20207.03it/s] 35%|███▌      | 159550/450337 [00:07<00:14, 20039.47it/s] 36%|███▌      | 161619/450337 [00:08<00:14, 20229.35it/s] 36%|███▋      | 163650/450337 [00:08<00:14, 20252.21it/s] 37%|███▋      | 165785/450337 [00:08<00:13, 20574.19it/s] 37%|███▋      | 167844/450337 [00:08<00:13, 20316.05it/s] 38%|███▊      | 169877/450337 [00:08<00:13, 20055.84it/s] 38%|███▊      | 171884/450337 [00:08<00:14, 19809.08it/s] 39%|███▊      | 173873/450337 [00:08<00:13, 19829.10it/s] 39%|███▉      | 175857/450337 [00:08<00:14, 19565.14it/s] 39%|███▉      | 177815/450337 [00:08<00:14, 18960.99it/s] 40%|███▉      | 179834/450337 [00:08<00:14, 19313.90it/s] 40%|████      | 181770/450337 [00:09<00:13, 19278.04it/s] 41%|████      | 183701/450337 [00:09<00:13, 19122.99it/s] 41%|████      | 185616/450337 [00:09<00:14, 18717.43it/s] 42%|████▏     | 187561/450337 [00:09<00:13, 18930.09it/s] 42%|████▏     | 189611/450337 [00:09<00:13, 19386.52it/s] 43%|████▎     | 191594/450337 [00:09<00:13, 19514.69it/s] 43%|████▎     | 193642/450337 [00:09<00:12, 19799.70it/s] 43%|████▎     | 195689/450337 [00:09<00:12, 19997.88it/s] 44%|████▍     | 197905/450337 [00:09<00:12, 20642.43it/s] 44%|████▍     | 199971/450337 [00:09<00:12, 20018.96it/s] 45%|████▍     | 201978/450337 [00:10<00:12, 19631.86it/s] 45%|████▌     | 204085/450337 [00:10<00:12, 20047.34it/s] 46%|████▌     | 206095/450337 [00:10<00:12, 19567.34it/s] 46%|████▌     | 208070/450337 [00:10<00:12, 19609.40it/s] 47%|████▋     | 210035/450337 [00:10<00:12, 19455.86it/s] 47%|████▋     | 212033/450337 [00:10<00:12, 19609.34it/s] 48%|████▊     | 214030/450337 [00:10<00:11, 19715.43it/s] 48%|████▊     | 216004/450337 [00:10<00:12, 19418.06it/s] 48%|████▊     | 217980/450337 [00:10<00:11, 19517.28it/s] 49%|████▉     | 220016/450337 [00:10<00:11, 19758.88it/s] 49%|████▉     | 221994/450337 [00:11<00:11, 19541.42it/s] 50%|████▉     | 223950/450337 [00:11<00:11, 19163.88it/s] 50%|█████     | 226225/450337 [00:11<00:11, 20210.13it/s] 51%|█████     | 228250/450337 [00:11<00:11, 19790.20it/s] 51%|█████     | 230234/450337 [00:11<00:11, 19256.62it/s] 52%|█████▏    | 232165/450337 [00:11<00:11, 19206.82it/s] 52%|█████▏    | 234389/450337 [00:11<00:10, 20089.61it/s] 52%|█████▏    | 236403/450337 [00:11<00:10, 20011.55it/s] 53%|█████▎    | 238457/450337 [00:11<00:10, 20159.94it/s] 53%|█████▎    | 240476/450337 [00:12<00:10, 20102.77it/s] 54%|█████▍    | 242563/450337 [00:12<00:10, 20320.98it/s] 54%|█████▍    | 244597/450337 [00:12<00:10, 20300.03it/s] 55%|█████▍    | 246745/450337 [00:12<00:09, 20649.17it/s] 55%|█████▌    | 248815/450337 [00:12<00:09, 20661.99it/s] 56%|█████▌    | 250952/450337 [00:12<00:09, 20873.19it/s] 56%|█████▌    | 253040/450337 [00:12<00:09, 20495.52it/s] 57%|█████▋    | 255113/450337 [00:12<00:09, 20564.43it/s] 57%|█████▋    | 257221/450337 [00:12<00:09, 20715.71it/s] 58%|█████▊    | 259294/450337 [00:12<00:09, 19909.09it/s] 58%|█████▊    | 261368/450337 [00:13<00:09, 20148.96it/s] 58%|█████▊    | 263389/450337 [00:13<00:09, 19872.51it/s] 59%|█████▉    | 265381/450337 [00:13<00:09, 19755.53it/s] 59%|█████▉    | 267360/450337 [00:13<00:09, 19654.03it/s] 60%|█████▉    | 269373/450337 [00:13<00:09, 19792.93it/s] 60%|██████    | 271354/450337 [00:13<00:09, 19698.81it/s] 61%|██████    | 273619/450337 [00:13<00:08, 20567.97it/s] 61%|██████    | 275678/450337 [00:13<00:08, 20004.91it/s] 62%|██████▏   | 277683/450337 [00:13<00:08, 19534.59it/s] 62%|██████▏   | 279714/450337 [00:13<00:08, 19757.66it/s] 63%|██████▎   | 281694/450337 [00:14<00:08, 19718.43it/s] 63%|██████▎   | 283690/450337 [00:14<00:08, 19783.79it/s] 63%|██████▎   | 285671/450337 [00:14<00:08, 19724.98it/s] 64%|██████▍   | 287645/450337 [00:14<00:08, 19574.12it/s] 64%|██████▍   | 289625/450337 [00:14<00:08, 19638.66it/s] 65%|██████▍   | 291602/450337 [00:14<00:08, 19675.97it/s] 65%|██████▌   | 293571/450337 [00:14<00:08, 19577.53it/s] 66%|██████▌   | 295623/450337 [00:14<00:07, 19857.29it/s] 66%|██████▌   | 297614/450337 [00:14<00:07, 19867.43it/s] 67%|██████▋   | 299630/450337 [00:14<00:07, 19953.20it/s] 67%|██████▋   | 301626/450337 [00:15<00:07, 19611.70it/s] 67%|██████▋   | 303674/450337 [00:15<00:07, 19867.03it/s] 68%|██████▊   | 305663/450337 [00:15<00:07, 19512.81it/s] 68%|██████▊   | 307717/450337 [00:15<00:07, 19812.72it/s] 69%|██████▉   | 309864/450337 [00:15<00:06, 20302.34it/s] 69%|██████▉   | 311897/450337 [00:15<00:06, 19948.58it/s] 70%|██████▉   | 314076/450337 [00:15<00:06, 20486.37it/s] 70%|███████   | 316128/450337 [00:15<00:06, 19934.67it/s] 71%|███████   | 318127/450337 [00:15<00:06, 19286.37it/s] 71%|███████   | 320080/450337 [00:16<00:06, 19350.80it/s] 72%|███████▏  | 322053/450337 [00:16<00:06, 19458.27it/s] 72%|███████▏  | 324003/450337 [00:16<00:06, 19237.11it/s] 72%|███████▏  | 325930/450337 [00:16<00:06, 18924.03it/s] 73%|███████▎  | 328228/450337 [00:16<00:06, 20095.09it/s] 73%|███████▎  | 330243/450337 [00:16<00:06, 19564.85it/s] 74%|███████▍  | 332206/450337 [00:16<00:06, 19502.90it/s] 74%|███████▍  | 334184/450337 [00:16<00:05, 19576.19it/s] 75%|███████▍  | 336252/450337 [00:16<00:05, 19897.06it/s] 75%|███████▌  | 338286/450337 [00:16<00:05, 20027.20it/s] 76%|███████▌  | 340291/450337 [00:17<00:05, 19573.89it/s] 76%|███████▌  | 342335/450337 [00:17<00:05, 19825.18it/s] 76%|███████▋  | 344353/450337 [00:17<00:05, 19929.62it/s] 77%|███████▋  | 346349/450337 [00:17<00:05, 19742.59it/s] 77%|███████▋  | 348326/450337 [00:17<00:05, 19734.48it/s] 78%|███████▊  | 350584/450337 [00:17<00:04, 20578.11it/s] 78%|███████▊  | 352644/450337 [00:17<00:04, 19677.35it/s] 79%|███████▉  | 354727/450337 [00:17<00:04, 20005.64it/s] 79%|███████▉  | 356736/450337 [00:17<00:04, 19613.03it/s] 80%|███████▉  | 358704/450337 [00:17<00:04, 19304.31it/s] 80%|████████  | 360755/450337 [00:18<00:04, 19651.31it/s] 81%|████████  | 362929/450337 [00:18<00:04, 20261.74it/s] 81%|████████  | 365090/450337 [00:18<00:04, 20658.61it/s] 82%|████████▏ | 367160/450337 [00:18<00:04, 20630.11it/s] 82%|████████▏ | 369226/450337 [00:18<00:04, 20208.18it/s] 82%|████████▏ | 371251/450337 [00:18<00:03, 19957.23it/s] 83%|████████▎ | 373250/450337 [00:18<00:03, 19943.47it/s] 83%|████████▎ | 375299/450337 [00:18<00:03, 20098.63it/s] 84%|████████▍ | 377311/450337 [00:18<00:03, 19497.19it/s] 84%|████████▍ | 379369/450337 [00:19<00:03, 19811.83it/s] 85%|████████▍ | 381512/450337 [00:19<00:03, 20281.42it/s] 85%|████████▌ | 383544/450337 [00:19<00:03, 19613.64it/s] 86%|████████▌ | 385677/450337 [00:19<00:03, 20107.08it/s] 86%|████████▌ | 387785/450337 [00:19<00:03, 20391.20it/s] 87%|████████▋ | 389830/450337 [00:19<00:02, 20257.99it/s] 87%|████████▋ | 391860/450337 [00:19<00:02, 19826.45it/s] 87%|████████▋ | 393847/450337 [00:19<00:02, 19729.38it/s] 88%|████████▊ | 395859/450337 [00:19<00:02, 19838.03it/s] 88%|████████▊ | 397845/450337 [00:19<00:02, 19527.47it/s] 89%|████████▉ | 400032/450337 [00:20<00:02, 20205.05it/s] 89%|████████▉ | 402056/450337 [00:20<00:02, 19561.01it/s] 90%|████████▉ | 404018/450337 [00:20<00:02, 19313.78it/s] 90%|█████████ | 405986/450337 [00:20<00:02, 19419.30it/s] 91%|█████████ | 407932/450337 [00:20<00:02, 19211.50it/s] 91%|█████████ | 409916/450337 [00:20<00:02, 19394.82it/s] 91%|█████████▏| 411912/450337 [00:20<00:01, 19560.62it/s] 92%|█████████▏| 413870/450337 [00:20<00:01, 18969.03it/s] 92%|█████████▏| 415772/450337 [00:20<00:01, 18850.07it/s] 93%|█████████▎| 417661/450337 [00:20<00:01, 18834.81it/s] 93%|█████████▎| 419628/450337 [00:21<00:01, 19078.39it/s] 94%|█████████▎| 421538/450337 [00:21<00:01, 18993.28it/s] 94%|█████████▍| 423577/450337 [00:21<00:01, 19401.41it/s] 94%|█████████▍| 425519/450337 [00:21<00:01, 19218.78it/s] 95%|█████████▍| 427682/450337 [00:21<00:01, 19929.41it/s] 95%|█████████▌| 429733/450337 [00:21<00:01, 20095.05it/s] 96%|█████████▌| 431744/450337 [00:21<00:00, 19620.62it/s] 96%|█████████▋| 433974/450337 [00:21<00:00, 20403.41it/s] 97%|█████████▋| 436019/450337 [00:21<00:00, 19806.33it/s] 97%|█████████▋| 438047/450337 [00:21<00:00, 19942.25it/s] 98%|█████████▊| 440046/450337 [00:22<00:00, 19451.05it/s] 98%|█████████▊| 442074/450337 [00:22<00:00, 19689.13it/s] 99%|█████████▊| 444156/450337 [00:22<00:00, 20019.76it/s] 99%|█████████▉| 446162/450337 [00:22<00:00, 19278.72it/s]100%|█████████▉| 448134/450337 [00:22<00:00, 19404.18it/s]100%|█████████▉| 450118/450337 [00:22<00:00, 19530.85it/s]100%|██████████| 450337/450337 [00:22<00:00, 19904.61it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 22.74it/s]2022-03-03 10:50:08 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-03 10:50:08 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-03 10:50:08 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-03 10:50:08 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-03 10:50:08 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-03 10:50:08 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-03 10:50:08 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-03 10:50:09 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-03 10:50:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 10:50:09 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-03 10:50:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 10:50:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-03 10:50:09 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-03 10:50:09 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_last.pt
2022-03-03 10:50:09 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_last.pt
2022-03-03 10:50:09 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-03 10:50:09 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-03 10:50:09 | INFO | fairseq.trainer | begin training epoch 1
2022-03-03 10:50:09 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-03 10:50:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-03 10:50:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 10:50:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 10:50:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 10:50:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 10:58:10 | INFO | train_inner | epoch 001:    105 / 393 loss=16.959, ppl=127362, wps=14711.1, ups=0.22, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.354, loss_scale=4, train_wall=476, gb_free=10.1, wall=481
2022-03-03 11:05:36 | INFO | train_inner | epoch 001:    205 / 393 loss=14.558, ppl=24119, wps=14708.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.529, loss_scale=4, train_wall=441, gb_free=10.1, wall=927
2022-03-03 11:13:01 | INFO | train_inner | epoch 001:    305 / 393 loss=12.507, ppl=5822.02, wps=14708.8, ups=0.22, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.037, loss_scale=4, train_wall=441, gb_free=10.1, wall=1373
2022-03-03 11:19:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 11:19:38 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.67 | ppl 1629.71 | wps 34075.3 | wpb 2034.1 | bsz 4 | num_updates 388
2022-03-03 11:19:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-03 11:19:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 11:19:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 11:19:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 1 @ 388 updates, score 10.67) (writing took 4.916184236295521 seconds)
2022-03-03 11:19:43 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-03 11:19:43 | INFO | train | epoch 001 | loss 13.865 | ppl 14923.8 | wps 14613.6 | ups 0.22 | wpb 65460.6 | bsz 127.9 | num_updates 388 | lr 4.85903e-05 | gnorm 1.669 | loss_scale 4 | train_wall 1743 | gb_free 10.1 | wall 1774
2022-03-03 11:19:43 | INFO | fairseq.trainer | begin training epoch 2
2022-03-03 11:19:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 11:20:36 | INFO | train_inner | epoch 002:     12 / 393 loss=11.044, ppl=2111.11, wps=14343.9, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.611, loss_scale=4, train_wall=439, gb_free=10.1, wall=1827
2022-03-03 11:28:01 | INFO | train_inner | epoch 002:    112 / 393 loss=10.464, ppl=1412.89, wps=14717.2, ups=0.22, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.474, loss_scale=4, train_wall=440, gb_free=10.1, wall=2273
2022-03-03 11:35:27 | INFO | train_inner | epoch 002:    212 / 393 loss=10.18, ppl=1160.31, wps=14716.9, ups=0.22, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.512, loss_scale=8, train_wall=440, gb_free=10.1, wall=2718
2022-03-03 11:42:52 | INFO | train_inner | epoch 002:    312 / 393 loss=9.944, ppl=985.23, wps=14713.4, ups=0.22, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.571, loss_scale=8, train_wall=441, gb_free=10.1, wall=3163
2022-03-03 11:48:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 11:48:57 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.625 | ppl 789.86 | wps 34156.1 | wpb 2034.1 | bsz 4 | num_updates 781 | best_loss 9.625
2022-03-03 11:48:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 781 updates
2022-03-03 11:48:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 11:49:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 11:49:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 2 @ 781 updates, score 9.625) (writing took 4.90249044355005 seconds)
2022-03-03 11:49:02 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-03 11:49:02 | INFO | train | epoch 002 | loss 10.122 | ppl 1114.33 | wps 14619.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 0.536 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 3534
2022-03-03 11:49:02 | INFO | fairseq.trainer | begin training epoch 3
2022-03-03 11:49:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 11:50:27 | INFO | train_inner | epoch 003:     19 / 393 loss=9.736, ppl=852.78, wps=14342.6, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.637, loss_scale=8, train_wall=439, gb_free=10.1, wall=3618
2022-03-03 11:57:52 | INFO | train_inner | epoch 003:    119 / 393 loss=9.53, ppl=739.19, wps=14716.3, ups=0.22, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.725, loss_scale=8, train_wall=440, gb_free=10.1, wall=4064
2022-03-03 12:05:18 | INFO | train_inner | epoch 003:    219 / 393 loss=9.362, ppl=658.19, wps=14706.6, ups=0.22, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.792, loss_scale=8, train_wall=441, gb_free=10.1, wall=4509
2022-03-03 12:12:43 | INFO | train_inner | epoch 003:    319 / 393 loss=9.212, ppl=592.97, wps=14710.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.83, loss_scale=16, train_wall=441, gb_free=10.1, wall=4955
2022-03-03 12:18:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:18:17 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.976 | ppl 503.45 | wps 34249.4 | wpb 2034.1 | bsz 4 | num_updates 1174 | best_loss 8.976
2022-03-03 12:18:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1174 updates
2022-03-03 12:18:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 12:18:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 12:18:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 3 @ 1174 updates, score 8.976) (writing took 4.957240544259548 seconds)
2022-03-03 12:18:22 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-03 12:18:22 | INFO | train | epoch 003 | loss 9.329 | ppl 642.97 | wps 14616.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1174 | lr 0.000146821 | gnorm 0.783 | loss_scale 16 | train_wall 1729 | gb_free 10.1 | wall 5294
2022-03-03 12:18:22 | INFO | fairseq.trainer | begin training epoch 4
2022-03-03 12:18:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:20:18 | INFO | train_inner | epoch 004:     26 / 393 loss=9.062, ppl=534.36, wps=14346.1, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.798, loss_scale=16, train_wall=439, gb_free=10.1, wall=5410
2022-03-03 12:27:44 | INFO | train_inner | epoch 004:    126 / 393 loss=8.913, ppl=481.91, wps=14713, ups=0.22, wpb=65530.2, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.791, loss_scale=16, train_wall=441, gb_free=10.1, wall=5855
2022-03-03 12:35:09 | INFO | train_inner | epoch 004:    226 / 393 loss=8.802, ppl=446.44, wps=14712.6, ups=0.22, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.808, loss_scale=16, train_wall=441, gb_free=10.1, wall=6300
2022-03-03 12:42:35 | INFO | train_inner | epoch 004:    326 / 393 loss=8.701, ppl=416.09, wps=14706.5, ups=0.22, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.829, loss_scale=16, train_wall=441, gb_free=10.1, wall=6746
2022-03-03 12:47:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:47:38 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.527 | ppl 368.85 | wps 34084.1 | wpb 2034.1 | bsz 4 | num_updates 1567 | best_loss 8.527
2022-03-03 12:47:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1567 updates
2022-03-03 12:47:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 12:47:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 12:47:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 4 @ 1567 updates, score 8.527) (writing took 4.51771677006036 seconds)
2022-03-03 12:47:42 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-03 12:47:42 | INFO | train | epoch 004 | loss 8.782 | ppl 440.23 | wps 14618.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1567 | lr 0.000195936 | gnorm 0.808 | loss_scale 32 | train_wall 1730 | gb_free 10.1 | wall 7054
2022-03-03 12:47:42 | INFO | fairseq.trainer | begin training epoch 5
2022-03-03 12:47:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:50:09 | INFO | train_inner | epoch 005:     33 / 393 loss=8.568, ppl=379.56, wps=14352.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.812, loss_scale=32, train_wall=439, gb_free=10.1, wall=7201
2022-03-03 12:57:35 | INFO | train_inner | epoch 005:    133 / 393 loss=8.449, ppl=349.53, wps=14703.3, ups=0.22, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.813, loss_scale=32, train_wall=441, gb_free=10.1, wall=7646
2022-03-03 12:59:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 13:05:05 | INFO | train_inner | epoch 005:    234 / 393 loss=8.369, ppl=330.51, wps=14568.4, ups=0.22, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.826, loss_scale=16, train_wall=445, gb_free=10.1, wall=8096
2022-03-03 13:12:30 | INFO | train_inner | epoch 005:    334 / 393 loss=8.275, ppl=309.65, wps=14713.5, ups=0.22, wpb=65530.2, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.781, loss_scale=16, train_wall=440, gb_free=10.1, wall=8542
2022-03-03 13:16:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 13:16:57 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.169 | ppl 287.75 | wps 34179.3 | wpb 2034.1 | bsz 4 | num_updates 1959 | best_loss 8.169
2022-03-03 13:16:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1959 updates
2022-03-03 13:16:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 13:17:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 13:17:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 5 @ 1959 updates, score 8.169) (writing took 5.0057556219398975 seconds)
2022-03-03 13:17:02 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-03 13:17:02 | INFO | train | epoch 005 | loss 8.352 | ppl 326.81 | wps 14577.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 1959 | lr 0.000244926 | gnorm 0.806 | loss_scale 16 | train_wall 1730 | gb_free 10.1 | wall 8814
2022-03-03 13:17:03 | INFO | fairseq.trainer | begin training epoch 6
2022-03-03 13:17:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 13:20:05 | INFO | train_inner | epoch 006:     41 / 393 loss=8.166, ppl=287.17, wps=14341.1, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.775, loss_scale=16, train_wall=439, gb_free=10.1, wall=8997
2022-03-03 13:27:31 | INFO | train_inner | epoch 006:    141 / 393 loss=8.064, ppl=267.53, wps=14703, ups=0.22, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.779, loss_scale=16, train_wall=441, gb_free=10.1, wall=9442
2022-03-03 13:34:56 | INFO | train_inner | epoch 006:    241 / 393 loss=8.001, ppl=256.24, wps=14712, ups=0.22, wpb=65535.4, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.759, loss_scale=16, train_wall=441, gb_free=10.1, wall=9888
2022-03-03 13:42:22 | INFO | train_inner | epoch 006:    341 / 393 loss=7.937, ppl=245.11, wps=14708.6, ups=0.22, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.771, loss_scale=32, train_wall=441, gb_free=10.1, wall=10333
2022-03-03 13:46:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 13:46:18 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.902 | ppl 239.19 | wps 34086.8 | wpb 2034.1 | bsz 4 | num_updates 2352 | best_loss 7.902
2022-03-03 13:46:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2352 updates
2022-03-03 13:46:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 13:46:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 13:46:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 6 @ 2352 updates, score 7.902) (writing took 5.010617284104228 seconds)
2022-03-03 13:46:23 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-03 13:46:23 | INFO | train | epoch 006 | loss 7.996 | ppl 255.21 | wps 14611.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 2352 | lr 0.000294041 | gnorm 0.764 | loss_scale 32 | train_wall 1730 | gb_free 10.1 | wall 10575
2022-03-03 13:46:23 | INFO | fairseq.trainer | begin training epoch 7
2022-03-03 13:46:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 13:49:57 | INFO | train_inner | epoch 007:     48 / 393 loss=7.824, ppl=226.6, wps=14334.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.737, loss_scale=32, train_wall=439, gb_free=10.1, wall=10789
2022-03-03 13:51:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 13:57:27 | INFO | train_inner | epoch 007:    149 / 393 loss=7.733, ppl=212.78, wps=14567.5, ups=0.22, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.732, loss_scale=16, train_wall=445, gb_free=10.1, wall=11238
2022-03-03 14:04:52 | INFO | train_inner | epoch 007:    249 / 393 loss=7.699, ppl=207.79, wps=14718.5, ups=0.22, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.729, loss_scale=16, train_wall=440, gb_free=10.1, wall=11684
2022-03-03 14:12:18 | INFO | train_inner | epoch 007:    349 / 393 loss=7.645, ppl=200.19, wps=14716.2, ups=0.22, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.716, loss_scale=16, train_wall=440, gb_free=10.1, wall=12129
2022-03-03 14:15:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:15:38 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.683 | ppl 205.53 | wps 34145 | wpb 2034.1 | bsz 4 | num_updates 2744 | best_loss 7.683
2022-03-03 14:15:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2744 updates
2022-03-03 14:15:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 14:15:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 14:15:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 7 @ 2744 updates, score 7.683) (writing took 4.994839219376445 seconds)
2022-03-03 14:15:43 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-03 14:15:43 | INFO | train | epoch 007 | loss 7.694 | ppl 207.04 | wps 14581.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 2744 | lr 0.000343031 | gnorm 0.728 | loss_scale 16 | train_wall 1729 | gb_free 10.1 | wall 12334
2022-03-03 14:15:43 | INFO | fairseq.trainer | begin training epoch 8
2022-03-03 14:15:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:19:52 | INFO | train_inner | epoch 008:     56 / 393 loss=7.546, ppl=186.9, wps=14344.1, ups=0.22, wpb=65238.4, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.701, loss_scale=16, train_wall=438, gb_free=10.1, wall=12584
2022-03-03 14:27:18 | INFO | train_inner | epoch 008:    156 / 393 loss=7.459, ppl=175.93, wps=14715.1, ups=0.22, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.709, loss_scale=16, train_wall=440, gb_free=10.1, wall=13029
2022-03-03 14:34:43 | INFO | train_inner | epoch 008:    256 / 393 loss=7.439, ppl=173.53, wps=14706, ups=0.22, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.696, loss_scale=32, train_wall=441, gb_free=10.1, wall=13475
2022-03-03 14:35:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 14:42:13 | INFO | train_inner | epoch 008:    357 / 393 loss=7.414, ppl=170.59, wps=14564.5, ups=0.22, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.697, loss_scale=16, train_wall=445, gb_free=10.1, wall=13925
2022-03-03 14:44:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:44:58 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.515 | ppl 182.96 | wps 34083 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 7.515
2022-03-03 14:44:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3136 updates
2022-03-03 14:44:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 14:45:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 14:45:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 8 @ 3136 updates, score 7.515) (writing took 4.5609175357967615 seconds)
2022-03-03 14:45:03 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-03 14:45:03 | INFO | train | epoch 008 | loss 7.442 | ppl 173.93 | wps 14581.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 3136 | lr 0.000392022 | gnorm 0.698 | loss_scale 16 | train_wall 1730 | gb_free 10.1 | wall 14094
2022-03-03 14:45:03 | INFO | fairseq.trainer | begin training epoch 9
2022-03-03 14:45:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:49:48 | INFO | train_inner | epoch 009:     64 / 393 loss=7.307, ppl=158.33, wps=14352.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.694, loss_scale=16, train_wall=439, gb_free=10.1, wall=14379
2022-03-03 14:57:13 | INFO | train_inner | epoch 009:    164 / 393 loss=7.248, ppl=151.97, wps=14711.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.65, loss_scale=16, train_wall=441, gb_free=10.1, wall=14825
2022-03-03 15:04:39 | INFO | train_inner | epoch 009:    264 / 393 loss=7.235, ppl=150.64, wps=14712.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.67, loss_scale=16, train_wall=441, gb_free=10.1, wall=15270
2022-03-03 15:12:04 | INFO | train_inner | epoch 009:    364 / 393 loss=7.21, ppl=148.07, wps=14715.4, ups=0.22, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.664, loss_scale=16, train_wall=440, gb_free=10.1, wall=15716
2022-03-03 15:14:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 15:14:18 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.386 | ppl 167.24 | wps 34156.4 | wpb 2034.1 | bsz 4 | num_updates 3529 | best_loss 7.386
2022-03-03 15:14:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3529 updates
2022-03-03 15:14:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 15:14:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 15:14:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 9 @ 3529 updates, score 7.386) (writing took 4.967676987871528 seconds)
2022-03-03 15:14:23 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-03 15:14:23 | INFO | train | epoch 009 | loss 7.233 | ppl 150.4 | wps 14616.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 3529 | lr 0.000441137 | gnorm 0.668 | loss_scale 32 | train_wall 1729 | gb_free 10.1 | wall 15854
2022-03-03 15:14:23 | INFO | fairseq.trainer | begin training epoch 10
2022-03-03 15:14:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 15:15:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 15:19:44 | INFO | train_inner | epoch 010:     72 / 393 loss=7.097, ppl=136.89, wps=14200.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.667, loss_scale=16, train_wall=443, gb_free=10.1, wall=16175
2022-03-03 15:27:09 | INFO | train_inner | epoch 010:    172 / 393 loss=7.053, ppl=132.78, wps=14707.9, ups=0.22, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.641, loss_scale=16, train_wall=441, gb_free=10.1, wall=16621
2022-03-03 15:34:35 | INFO | train_inner | epoch 010:    272 / 393 loss=7.065, ppl=133.88, wps=14712.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.67, loss_scale=16, train_wall=441, gb_free=10.1, wall=17066
2022-03-03 15:42:00 | INFO | train_inner | epoch 010:    372 / 393 loss=7.053, ppl=132.76, wps=14716.4, ups=0.22, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.624, loss_scale=16, train_wall=440, gb_free=10.1, wall=17511
2022-03-03 15:43:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 15:43:38 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.297 | ppl 157.28 | wps 34144.5 | wpb 2034.1 | bsz 4 | num_updates 3921 | best_loss 7.297
2022-03-03 15:43:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3921 updates
2022-03-03 15:43:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 15:43:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 15:43:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 10 @ 3921 updates, score 7.297) (writing took 4.99960717651993 seconds)
2022-03-03 15:43:43 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-03 15:43:43 | INFO | train | epoch 010 | loss 7.055 | ppl 132.99 | wps 14579.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3921 | lr 0.000490127 | gnorm 0.651 | loss_scale 16 | train_wall 1729 | gb_free 10.1 | wall 17614
2022-03-03 15:43:43 | INFO | fairseq.trainer | begin training epoch 11
2022-03-03 15:43:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 15:49:35 | INFO | train_inner | epoch 011:     79 / 393 loss=6.915, ppl=120.7, wps=14342.9, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.653, loss_scale=16, train_wall=439, gb_free=10.1, wall=17966
2022-03-03 15:53:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 15:57:05 | INFO | train_inner | epoch 011:    180 / 393 loss=6.897, ppl=119.22, wps=14567.8, ups=0.22, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.637, loss_scale=16, train_wall=445, gb_free=10.1, wall=18416
2022-03-03 16:04:31 | INFO | train_inner | epoch 011:    280 / 393 loss=6.898, ppl=119.23, wps=14703.4, ups=0.22, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.59, loss_scale=16, train_wall=441, gb_free=10.1, wall=18862
2022-03-03 16:11:56 | INFO | train_inner | epoch 011:    380 / 393 loss=6.909, ppl=120.15, wps=14711.8, ups=0.22, wpb=65530.2, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.61, loss_scale=16, train_wall=441, gb_free=10.1, wall=19307
2022-03-03 16:12:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:12:58 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.209 | ppl 147.94 | wps 34173.4 | wpb 2034.1 | bsz 4 | num_updates 4313 | best_loss 7.209
2022-03-03 16:12:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4313 updates
2022-03-03 16:12:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 16:13:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 16:13:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 11 @ 4313 updates, score 7.209) (writing took 4.941375387832522 seconds)
2022-03-03 16:13:03 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-03 16:13:03 | INFO | train | epoch 011 | loss 6.897 | ppl 119.16 | wps 14578.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 4313 | lr 0.000481515 | gnorm 0.617 | loss_scale 16 | train_wall 1730 | gb_free 10.1 | wall 19375
2022-03-03 16:13:03 | INFO | fairseq.trainer | begin training epoch 12
2022-03-03 16:13:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:19:31 | INFO | train_inner | epoch 012:     87 / 393 loss=6.751, ppl=107.69, wps=14345.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.595, loss_scale=16, train_wall=439, gb_free=10.1, wall=19762
2022-03-03 16:26:56 | INFO | train_inner | epoch 012:    187 / 393 loss=6.746, ppl=107.34, wps=14715.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.574, loss_scale=16, train_wall=440, gb_free=10.1, wall=20208
2022-03-03 16:34:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 16:34:26 | INFO | train_inner | epoch 012:    288 / 393 loss=6.759, ppl=108.32, wps=14560.3, ups=0.22, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.579, loss_scale=16, train_wall=445, gb_free=10.1, wall=20658
2022-03-03 16:41:52 | INFO | train_inner | epoch 012:    388 / 393 loss=6.753, ppl=107.85, wps=14708.5, ups=0.22, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.573, loss_scale=16, train_wall=441, gb_free=10.1, wall=21103
2022-03-03 16:42:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:42:19 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.147 | ppl 141.78 | wps 34183.9 | wpb 2034.1 | bsz 4 | num_updates 4705 | best_loss 7.147
2022-03-03 16:42:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4705 updates
2022-03-03 16:42:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 16:42:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 16:42:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 12 @ 4705 updates, score 7.147) (writing took 4.472674074582756 seconds)
2022-03-03 16:42:23 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-03 16:42:23 | INFO | train | epoch 012 | loss 6.748 | ppl 107.49 | wps 14582 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 4705 | lr 0.00046102 | gnorm 0.581 | loss_scale 16 | train_wall 1730 | gb_free 10.1 | wall 21134
2022-03-03 16:42:23 | INFO | fairseq.trainer | begin training epoch 13
2022-03-03 16:42:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:49:27 | INFO | train_inner | epoch 013:     95 / 393 loss=6.605, ppl=97.31, wps=14349.8, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.582, loss_scale=16, train_wall=439, gb_free=10.1, wall=21558
2022-03-03 16:56:52 | INFO | train_inner | epoch 013:    195 / 393 loss=6.618, ppl=98.21, wps=14709, ups=0.22, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.555, loss_scale=16, train_wall=441, gb_free=10.1, wall=22003
2022-03-03 17:04:17 | INFO | train_inner | epoch 013:    295 / 393 loss=6.631, ppl=99.14, wps=14714.4, ups=0.22, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.546, loss_scale=16, train_wall=441, gb_free=10.1, wall=22449
2022-03-03 17:11:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:11:38 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.105 | ppl 137.66 | wps 34189.5 | wpb 2034.1 | bsz 4 | num_updates 5098 | best_loss 7.105
2022-03-03 17:11:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5098 updates
2022-03-03 17:11:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 17:11:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 17:11:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 13 @ 5098 updates, score 7.105) (writing took 4.912916619330645 seconds)
2022-03-03 17:11:43 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-03 17:11:43 | INFO | train | epoch 013 | loss 6.624 | ppl 98.6 | wps 14614.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 5098 | lr 0.000442894 | gnorm 0.561 | loss_scale 16 | train_wall 1730 | gb_free 10.1 | wall 22895
2022-03-03 17:11:43 | INFO | fairseq.trainer | begin training epoch 14
2022-03-03 17:11:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 17:11:52 | INFO | train_inner | epoch 014:      2 / 393 loss=6.645, ppl=100.06, wps=14342.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.567, loss_scale=16, train_wall=439, gb_free=10.1, wall=22904
2022-03-03 17:13:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 17:19:22 | INFO | train_inner | epoch 014:    103 / 393 loss=6.472, ppl=88.79, wps=14573.3, ups=0.22, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.556, loss_scale=16, train_wall=445, gb_free=10.1, wall=23353
2022-03-03 17:26:48 | INFO | train_inner | epoch 014:    203 / 393 loss=6.516, ppl=91.55, wps=14709, ups=0.22, wpb=65530.9, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.555, loss_scale=16, train_wall=441, gb_free=10.1, wall=23799
2022-03-03 17:34:13 | INFO | train_inner | epoch 014:    303 / 393 loss=6.539, ppl=93.02, wps=14717.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.548, loss_scale=16, train_wall=440, gb_free=10.1, wall=24244
2022-03-03 17:40:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:40:58 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.07 | ppl 134.38 | wps 34062.2 | wpb 2034.1 | bsz 4 | num_updates 5490 | best_loss 7.07
2022-03-03 17:40:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5490 updates
2022-03-03 17:40:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 17:41:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 17:41:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 14 @ 5490 updates, score 7.07) (writing took 4.947471493855119 seconds)
2022-03-03 17:41:03 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-03 17:41:03 | INFO | train | epoch 014 | loss 6.517 | ppl 91.58 | wps 14580.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5490 | lr 0.00042679 | gnorm 0.555 | loss_scale 16 | train_wall 1729 | gb_free 10.1 | wall 24655
2022-03-03 17:41:03 | INFO | fairseq.trainer | begin training epoch 15
2022-03-03 17:41:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 17:41:48 | INFO | train_inner | epoch 015:     10 / 393 loss=6.526, ppl=92.13, wps=14339.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.56, loss_scale=16, train_wall=439, gb_free=10.1, wall=24699
2022-03-03 17:49:13 | INFO | train_inner | epoch 015:    110 / 393 loss=6.382, ppl=83.41, wps=14713.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.522, loss_scale=16, train_wall=441, gb_free=10.1, wall=25145
2022-03-03 17:51:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 17:56:43 | INFO | train_inner | epoch 015:    211 / 393 loss=6.421, ppl=85.69, wps=14568, ups=0.22, wpb=65530.9, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.571, loss_scale=16, train_wall=445, gb_free=10.1, wall=25595
2022-03-03 18:04:09 | INFO | train_inner | epoch 015:    311 / 393 loss=6.447, ppl=87.25, wps=14713.7, ups=0.22, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.546, loss_scale=16, train_wall=441, gb_free=10.1, wall=26040
2022-03-03 18:10:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:10:18 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.053 | ppl 132.76 | wps 34140.6 | wpb 2034.1 | bsz 4 | num_updates 5882 | best_loss 7.053
2022-03-03 18:10:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5882 updates
2022-03-03 18:10:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 18:10:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 18:10:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 15 @ 5882 updates, score 7.053) (writing took 5.145173515193164 seconds)
2022-03-03 18:10:23 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-03 18:10:23 | INFO | train | epoch 015 | loss 6.424 | ppl 85.89 | wps 14578.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5882 | lr 0.000412323 | gnorm 0.546 | loss_scale 16 | train_wall 1729 | gb_free 10.1 | wall 26415
2022-03-03 18:10:23 | INFO | fairseq.trainer | begin training epoch 16
2022-03-03 18:10:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:11:44 | INFO | train_inner | epoch 016:     18 / 393 loss=6.429, ppl=86.18, wps=14335.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.545, loss_scale=16, train_wall=439, gb_free=10.1, wall=26495
2022-03-03 18:19:09 | INFO | train_inner | epoch 016:    118 / 393 loss=6.298, ppl=78.66, wps=14710.9, ups=0.22, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.546, loss_scale=16, train_wall=441, gb_free=10.1, wall=26941
2022-03-03 18:26:35 | INFO | train_inner | epoch 016:    218 / 393 loss=6.339, ppl=80.94, wps=14710.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.552, loss_scale=16, train_wall=441, gb_free=10.1, wall=27386
2022-03-03 18:30:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 18:34:05 | INFO | train_inner | epoch 016:    319 / 393 loss=6.37, ppl=82.7, wps=14561, ups=0.22, wpb=65535.4, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.557, loss_scale=16, train_wall=445, gb_free=10.1, wall=27836
2022-03-03 18:39:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:39:39 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.049 | ppl 132.38 | wps 34192.9 | wpb 2034.1 | bsz 4 | num_updates 6274 | best_loss 7.049
2022-03-03 18:39:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6274 updates
2022-03-03 18:39:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 18:39:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 18:39:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 16 @ 6274 updates, score 7.049) (writing took 4.579084767028689 seconds)
2022-03-03 18:39:43 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-03 18:39:43 | INFO | train | epoch 016 | loss 6.343 | ppl 81.16 | wps 14580.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6274 | lr 0.000399234 | gnorm 0.547 | loss_scale 16 | train_wall 1730 | gb_free 10.1 | wall 28175
2022-03-03 18:39:43 | INFO | fairseq.trainer | begin training epoch 17
2022-03-03 18:39:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:41:39 | INFO | train_inner | epoch 017:     26 / 393 loss=6.337, ppl=80.81, wps=14353.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.539, loss_scale=16, train_wall=439, gb_free=10.1, wall=28291
2022-03-03 18:49:05 | INFO | train_inner | epoch 017:    126 / 393 loss=6.233, ppl=75.21, wps=14712.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.552, loss_scale=16, train_wall=441, gb_free=10.1, wall=28736
2022-03-03 18:56:30 | INFO | train_inner | epoch 017:    226 / 393 loss=6.272, ppl=77.28, wps=14714.5, ups=0.22, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.568, loss_scale=16, train_wall=440, gb_free=10.1, wall=29181
2022-03-03 19:03:56 | INFO | train_inner | epoch 017:    326 / 393 loss=6.292, ppl=78.36, wps=14714.3, ups=0.22, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.531, loss_scale=16, train_wall=440, gb_free=10.1, wall=29627
2022-03-03 19:08:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:08:58 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.038 | ppl 131.44 | wps 34083.6 | wpb 2034.1 | bsz 4 | num_updates 6667 | best_loss 7.038
2022-03-03 19:08:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6667 updates
2022-03-03 19:08:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 19:09:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt
2022-03-03 19:09:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#1/checkpoint_best.pt (epoch 17 @ 6667 updates, score 7.038) (writing took 4.938853902742267 seconds)
2022-03-03 19:09:03 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-03 19:09:03 | INFO | train | epoch 017 | loss 6.27 | ppl 77.18 | wps 14618.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 6667 | lr 0.000387289 | gnorm 0.55 | loss_scale 32 | train_wall 1729 | gb_free 10.1 | wall 29935
2022-03-03 19:09:03 | INFO | fairseq.trainer | begin training epoch 18
2022-03-03 19:09:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 19:10:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 19:11:35 | INFO | train_inner | epoch 018:     34 / 393 loss=6.26, ppl=76.63, wps=14206.2, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.556, loss_scale=16, train_wall=443, gb_free=10.1, wall=30086
2022-03-03 19:19:00 | INFO | train_inner | epoch 018:    134 / 393 loss=6.156, ppl=71.32, wps=14709.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.539, loss_scale=16, train_wall=441, gb_free=10.1, wall=30532
2022-03-03 19:26:26 | INFO | train_inner | epoch 018:    234 / 393 loss=6.201, ppl=73.56, wps=14714, ups=0.22, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.549, loss_scale=16, train_wall=440, gb_free=10.1, wall=30977
2022-03-03 19:33:51 | INFO | train_inner | epoch 018:    334 / 393 loss=6.242, ppl=75.69, wps=14714.3, ups=0.22, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.558, loss_scale=16, train_wall=440, gb_free=10.1, wall=31422
2022-03-03 19:38:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:38:18 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.042 | ppl 131.75 | wps 34136.5 | wpb 2034.1 | bsz 4 | num_updates 7059 | best_loss 7.038
2022-03-03 19:38:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7059 updates
2022-03-03 19:38:18 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-03 19:38:18 | INFO | train | epoch 018 | loss 6.204 | ppl 73.7 | wps 14621.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7059 | lr 0.000376382 | gnorm 0.554 | loss_scale 16 | train_wall 1729 | gb_free 10.1 | wall 31690
2022-03-03 19:38:18 | INFO | fairseq.trainer | begin training epoch 19
2022-03-03 19:38:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 19:41:21 | INFO | train_inner | epoch 019:     41 / 393 loss=6.184, ppl=72.71, wps=14503.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.551, loss_scale=16, train_wall=439, gb_free=10.1, wall=31872
2022-03-03 19:48:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 19:48:51 | INFO | train_inner | epoch 019:    142 / 393 loss=6.111, ppl=69.13, wps=14569.4, ups=0.22, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.556, loss_scale=16, train_wall=445, gb_free=10.1, wall=32322
2022-03-03 19:56:16 | INFO | train_inner | epoch 019:    242 / 393 loss=6.142, ppl=70.64, wps=14707.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.561, loss_scale=16, train_wall=441, gb_free=10.1, wall=32768
2022-03-03 20:03:42 | INFO | train_inner | epoch 019:    342 / 393 loss=6.176, ppl=72.31, wps=14712.7, ups=0.22, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.583, loss_scale=16, train_wall=441, gb_free=10.1, wall=33213
2022-03-03 20:07:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:07:33 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.04 | ppl 131.57 | wps 34162.6 | wpb 2034.1 | bsz 4 | num_updates 7451 | best_loss 7.038
2022-03-03 20:07:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7451 updates
2022-03-03 20:07:33 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-03 20:07:33 | INFO | train | epoch 019 | loss 6.144 | ppl 70.71 | wps 14621.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7451 | lr 0.000366347 | gnorm 0.563 | loss_scale 16 | train_wall 1729 | gb_free 10.1 | wall 33445
2022-03-03 20:07:33 | INFO | fairseq.trainer | begin training epoch 20
2022-03-03 20:07:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:11:12 | INFO | train_inner | epoch 020:     49 / 393 loss=6.111, ppl=69.1, wps=14503.1, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.554, loss_scale=16, train_wall=439, gb_free=10.1, wall=33663
2022-03-03 20:13:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:18:41 | INFO | train_inner | epoch 020:    150 / 393 loss=6.055, ppl=66.49, wps=14574.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.566, loss_scale=8, train_wall=445, gb_free=10.1, wall=34113
2022-03-03 20:26:07 | INFO | train_inner | epoch 020:    250 / 393 loss=6.091, ppl=68.17, wps=14715.6, ups=0.22, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.575, loss_scale=8, train_wall=440, gb_free=10.1, wall=34558
2022-03-03 20:33:32 | INFO | train_inner | epoch 020:    350 / 393 loss=6.13, ppl=70.02, wps=14710.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.569, loss_scale=8, train_wall=441, gb_free=10.1, wall=35004
2022-03-03 20:36:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:36:48 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.054 | ppl 132.87 | wps 34050.1 | wpb 2034.1 | bsz 4 | num_updates 7843 | best_loss 7.038
2022-03-03 20:36:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7843 updates
2022-03-03 20:36:48 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-03 20:36:48 | INFO | train | epoch 020 | loss 6.089 | ppl 68.05 | wps 14622 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7843 | lr 0.000357075 | gnorm 0.562 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 35200
2022-03-03 20:36:48 | INFO | fairseq.trainer | begin training epoch 21
2022-03-03 20:36:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:41:02 | INFO | train_inner | epoch 021:     57 / 393 loss=6.048, ppl=66.17, wps=14493.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.557, loss_scale=8, train_wall=439, gb_free=10.1, wall=35454
2022-03-03 20:48:28 | INFO | train_inner | epoch 021:    157 / 393 loss=6.008, ppl=64.36, wps=14718.2, ups=0.22, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.567, loss_scale=8, train_wall=440, gb_free=10.1, wall=35899
2022-03-03 20:55:53 | INFO | train_inner | epoch 021:    257 / 393 loss=6.051, ppl=66.31, wps=14713.2, ups=0.22, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.572, loss_scale=16, train_wall=441, gb_free=10.1, wall=36344
2022-03-03 20:57:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 21:03:23 | INFO | train_inner | epoch 021:    358 / 393 loss=6.063, ppl=66.85, wps=14571.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.563, loss_scale=8, train_wall=445, gb_free=10.1, wall=36794
2022-03-03 21:05:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:06:03 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.058 | ppl 133.29 | wps 34167.6 | wpb 2034.1 | bsz 4 | num_updates 8235 | best_loss 7.038
2022-03-03 21:06:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8235 updates
2022-03-03 21:06:03 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-03 21:06:03 | INFO | train | epoch 021 | loss 6.038 | ppl 65.72 | wps 14623.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8235 | lr 0.000348472 | gnorm 0.575 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 36955
2022-03-03 21:06:03 | INFO | fairseq.trainer | begin training epoch 22
2022-03-03 21:06:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:10:53 | INFO | train_inner | epoch 022:     65 / 393 loss=5.992, ppl=63.65, wps=14493.7, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.588, loss_scale=8, train_wall=439, gb_free=10.1, wall=37244
2022-03-03 21:18:18 | INFO | train_inner | epoch 022:    165 / 393 loss=5.963, ppl=62.38, wps=14712.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.576, loss_scale=8, train_wall=441, gb_free=10.1, wall=37690
2022-03-03 21:25:44 | INFO | train_inner | epoch 022:    265 / 393 loss=6.001, ppl=64.04, wps=14714.5, ups=0.22, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.57, loss_scale=8, train_wall=440, gb_free=10.1, wall=38135
2022-03-03 21:33:09 | INFO | train_inner | epoch 022:    365 / 393 loss=6.036, ppl=65.6, wps=14709.7, ups=0.22, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.573, loss_scale=8, train_wall=441, gb_free=10.1, wall=38581
2022-03-03 21:35:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:35:18 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.078 | ppl 135.09 | wps 34193.1 | wpb 2034.1 | bsz 4 | num_updates 8628 | best_loss 7.038
2022-03-03 21:35:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8628 updates
2022-03-03 21:35:18 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-03 21:35:18 | INFO | train | epoch 022 | loss 5.991 | ppl 63.58 | wps 14656.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 8628 | lr 0.000340443 | gnorm 0.568 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 38710
2022-03-03 21:35:18 | INFO | fairseq.trainer | begin training epoch 23
2022-03-03 21:35:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:40:39 | INFO | train_inner | epoch 023:     72 / 393 loss=5.924, ppl=60.73, wps=14495.8, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.567, loss_scale=16, train_wall=439, gb_free=10.1, wall=39031
2022-03-03 21:48:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 21:48:09 | INFO | train_inner | epoch 023:    173 / 393 loss=5.917, ppl=60.41, wps=14565.4, ups=0.22, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.58, loss_scale=8, train_wall=445, gb_free=10.1, wall=39481
2022-03-03 21:55:35 | INFO | train_inner | epoch 023:    273 / 393 loss=5.964, ppl=62.43, wps=14717.1, ups=0.22, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.586, loss_scale=8, train_wall=440, gb_free=10.1, wall=39926
2022-03-03 22:03:00 | INFO | train_inner | epoch 023:    373 / 393 loss=5.995, ppl=63.78, wps=14715.8, ups=0.22, wpb=65530.2, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.585, loss_scale=8, train_wall=440, gb_free=10.1, wall=40371
2022-03-03 22:04:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:04:33 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.081 | ppl 135.39 | wps 34174.9 | wpb 2034.1 | bsz 4 | num_updates 9020 | best_loss 7.038
2022-03-03 22:04:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9020 updates
2022-03-03 22:04:34 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-03 22:04:34 | INFO | train | epoch 023 | loss 5.946 | ppl 61.65 | wps 14621.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 9020 | lr 0.000332964 | gnorm 0.582 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 40465
2022-03-03 22:04:34 | INFO | fairseq.trainer | begin training epoch 24
2022-03-03 22:04:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:10:30 | INFO | train_inner | epoch 024:     80 / 393 loss=5.872, ppl=58.56, wps=14501.8, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.58, loss_scale=8, train_wall=439, gb_free=10.1, wall=40821
2022-03-03 22:17:55 | INFO | train_inner | epoch 024:    180 / 393 loss=5.877, ppl=58.79, wps=14718.6, ups=0.22, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.579, loss_scale=8, train_wall=440, gb_free=10.1, wall=41266
2022-03-03 22:25:21 | INFO | train_inner | epoch 024:    280 / 393 loss=5.926, ppl=60.8, wps=14711.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.586, loss_scale=8, train_wall=441, gb_free=10.1, wall=41712
2022-03-03 22:28:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 22:32:51 | INFO | train_inner | epoch 024:    381 / 393 loss=5.953, ppl=61.96, wps=14564.6, ups=0.22, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.583, loss_scale=8, train_wall=445, gb_free=10.1, wall=42162
2022-03-03 22:33:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:33:49 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.098 | ppl 136.99 | wps 34128.8 | wpb 2034.1 | bsz 4 | num_updates 9412 | best_loss 7.038
2022-03-03 22:33:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9412 updates
2022-03-03 22:33:49 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-03 22:33:49 | INFO | train | epoch 024 | loss 5.904 | ppl 59.9 | wps 14621.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 9412 | lr 0.000325956 | gnorm 0.584 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 42220
2022-03-03 22:33:49 | INFO | fairseq.trainer | begin training epoch 25
2022-03-03 22:33:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:40:21 | INFO | train_inner | epoch 025:     88 / 393 loss=5.821, ppl=56.54, wps=14497.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.616, loss_scale=8, train_wall=439, gb_free=10.1, wall=42612
2022-03-03 22:47:46 | INFO | train_inner | epoch 025:    188 / 393 loss=5.844, ppl=57.44, wps=14706.8, ups=0.22, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.571, loss_scale=8, train_wall=441, gb_free=10.1, wall=43058
2022-03-03 22:55:11 | INFO | train_inner | epoch 025:    288 / 393 loss=5.884, ppl=59.07, wps=14720.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.594, loss_scale=8, train_wall=440, gb_free=10.1, wall=43503
2022-03-03 23:02:37 | INFO | train_inner | epoch 025:    388 / 393 loss=5.925, ppl=60.78, wps=14717.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.575, loss_scale=8, train_wall=440, gb_free=10.1, wall=43948
2022-03-03 23:02:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:03:03 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.1 | ppl 137.14 | wps 34145 | wpb 2034.1 | bsz 4 | num_updates 9805 | best_loss 7.038
2022-03-03 23:03:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9805 updates
2022-03-03 23:03:03 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-03 23:03:03 | INFO | train | epoch 025 | loss 5.866 | ppl 58.33 | wps 14660.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9805 | lr 0.000319357 | gnorm 0.59 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 43975
2022-03-03 23:03:03 | INFO | fairseq.trainer | begin training epoch 26
2022-03-03 23:03:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:10:07 | INFO | train_inner | epoch 026:     95 / 393 loss=5.774, ppl=54.71, wps=14504.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.575, loss_scale=16, train_wall=439, gb_free=10.1, wall=44398
2022-03-03 23:11:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 23:17:36 | INFO | train_inner | epoch 026:    196 / 393 loss=5.815, ppl=56.32, wps=14570.4, ups=0.22, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.595, loss_scale=8, train_wall=445, gb_free=10.1, wall=44848
2022-03-03 23:25:02 | INFO | train_inner | epoch 026:    296 / 393 loss=5.846, ppl=57.5, wps=14715.6, ups=0.22, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.623, loss_scale=8, train_wall=440, gb_free=10.1, wall=45293
2022-03-03 23:32:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:32:18 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.123 | ppl 139.43 | wps 34110.1 | wpb 2034.1 | bsz 4 | num_updates 10197 | best_loss 7.038
2022-03-03 23:32:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10197 updates
2022-03-03 23:32:18 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-03 23:32:18 | INFO | train | epoch 026 | loss 5.829 | ppl 56.84 | wps 14624.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10197 | lr 0.000313158 | gnorm 0.587 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 45729
2022-03-03 23:32:18 | INFO | fairseq.trainer | begin training epoch 27
2022-03-03 23:32:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:32:31 | INFO | train_inner | epoch 027:      3 / 393 loss=5.881, ppl=58.95, wps=14506.1, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.556, loss_scale=8, train_wall=438, gb_free=10.1, wall=45743
2022-03-03 23:39:57 | INFO | train_inner | epoch 027:    103 / 393 loss=5.733, ppl=53.19, wps=14714, ups=0.22, wpb=65530.9, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.589, loss_scale=8, train_wall=440, gb_free=10.1, wall=46188
2022-03-03 23:47:22 | INFO | train_inner | epoch 027:    203 / 393 loss=5.782, ppl=55.03, wps=14721.3, ups=0.22, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.616, loss_scale=8, train_wall=440, gb_free=10.1, wall=46633
2022-03-03 23:51:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 23:54:52 | INFO | train_inner | epoch 027:    304 / 393 loss=5.822, ppl=56.59, wps=14573, ups=0.22, wpb=65535.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.594, loss_scale=8, train_wall=445, gb_free=10.1, wall=47083
2022-03-04 00:01:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:01:32 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.122 | ppl 139.28 | wps 34142.5 | wpb 2034.1 | bsz 4 | num_updates 10589 | best_loss 7.038
2022-03-04 00:01:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10589 updates
2022-03-04 00:01:32 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 00:01:32 | INFO | train | epoch 027 | loss 5.795 | ppl 55.54 | wps 14626.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10589 | lr 0.000307307 | gnorm 0.597 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 47484
2022-03-04 00:01:32 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 00:01:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:02:22 | INFO | train_inner | epoch 028:     11 / 393 loss=5.834, ppl=57.06, wps=14506.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.587, loss_scale=8, train_wall=438, gb_free=10.1, wall=47533
2022-03-04 00:09:47 | INFO | train_inner | epoch 028:    111 / 393 loss=5.701, ppl=52.02, wps=14716.7, ups=0.22, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.612, loss_scale=8, train_wall=440, gb_free=10.1, wall=47978
2022-03-04 00:17:12 | INFO | train_inner | epoch 028:    211 / 393 loss=5.747, ppl=53.71, wps=14716.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.596, loss_scale=8, train_wall=440, gb_free=10.1, wall=48423
2022-03-04 00:24:38 | INFO | train_inner | epoch 028:    311 / 393 loss=5.79, ppl=55.34, wps=14713.1, ups=0.22, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.591, loss_scale=8, train_wall=441, gb_free=10.1, wall=48869
2022-03-04 00:30:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:30:47 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.15 | ppl 142 | wps 34153.3 | wpb 2034.1 | bsz 4 | num_updates 10982 | best_loss 7.038
2022-03-04 00:30:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10982 updates
2022-03-04 00:30:47 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 00:30:47 | INFO | train | epoch 028 | loss 5.763 | ppl 54.31 | wps 14661.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 10982 | lr 0.000301758 | gnorm 0.606 | loss_scale 16 | train_wall 1729 | gb_free 10.1 | wall 49239
2022-03-04 00:30:47 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 00:30:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:31:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 00:32:12 | INFO | train_inner | epoch 029:     19 / 393 loss=5.805, ppl=55.9, wps=14361.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.633, loss_scale=8, train_wall=443, gb_free=10.1, wall=49323
2022-03-04 00:39:37 | INFO | train_inner | epoch 029:    119 / 393 loss=5.68, ppl=51.27, wps=14713.7, ups=0.22, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.598, loss_scale=8, train_wall=440, gb_free=10.1, wall=49769
2022-03-04 00:47:03 | INFO | train_inner | epoch 029:    219 / 393 loss=5.722, ppl=52.79, wps=14712.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.596, loss_scale=8, train_wall=440, gb_free=10.1, wall=50214
2022-03-04 00:54:28 | INFO | train_inner | epoch 029:    319 / 393 loss=5.767, ppl=54.47, wps=14715.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.634, loss_scale=8, train_wall=440, gb_free=10.1, wall=50659
2022-03-04 00:59:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:00:02 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.168 | ppl 143.79 | wps 34170.2 | wpb 2034.1 | bsz 4 | num_updates 11374 | best_loss 7.038
2022-03-04 01:00:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11374 updates
2022-03-04 01:00:02 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 01:00:02 | INFO | train | epoch 029 | loss 5.732 | ppl 53.14 | wps 14622.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 11374 | lr 0.000296513 | gnorm 0.609 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 50993
2022-03-04 01:00:02 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 01:00:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:01:58 | INFO | train_inner | epoch 030:     26 / 393 loss=5.744, ppl=53.61, wps=14502.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.599, loss_scale=8, train_wall=439, gb_free=10.1, wall=51109
2022-03-04 01:09:23 | INFO | train_inner | epoch 030:    126 / 393 loss=5.659, ppl=50.53, wps=14712.7, ups=0.22, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.591, loss_scale=16, train_wall=441, gb_free=10.1, wall=51555
2022-03-04 01:09:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 01:16:53 | INFO | train_inner | epoch 030:    227 / 393 loss=5.689, ppl=51.59, wps=14570.7, ups=0.22, wpb=65530.2, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.624, loss_scale=8, train_wall=445, gb_free=10.1, wall=52004
2022-03-04 01:24:18 | INFO | train_inner | epoch 030:    327 / 393 loss=5.737, ppl=53.33, wps=14716.1, ups=0.22, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.609, loss_scale=8, train_wall=440, gb_free=10.1, wall=52450
2022-03-04 01:29:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:29:17 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.178 | ppl 144.82 | wps 34122.4 | wpb 2034.1 | bsz 4 | num_updates 11766 | best_loss 7.038
2022-03-04 01:29:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11766 updates
2022-03-04 01:29:17 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 01:29:17 | INFO | train | epoch 030 | loss 5.702 | ppl 52.06 | wps 14623.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 11766 | lr 0.000291532 | gnorm 0.606 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 52748
2022-03-04 01:29:17 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 01:29:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:31:48 | INFO | train_inner | epoch 031:     34 / 393 loss=5.703, ppl=52.11, wps=14503.4, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.605, loss_scale=8, train_wall=438, gb_free=10.1, wall=52900
2022-03-04 01:39:14 | INFO | train_inner | epoch 031:    134 / 393 loss=5.629, ppl=49.5, wps=14715.2, ups=0.22, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.616, loss_scale=8, train_wall=440, gb_free=10.1, wall=53345
2022-03-04 01:46:39 | INFO | train_inner | epoch 031:    234 / 393 loss=5.677, ppl=51.16, wps=14717.7, ups=0.22, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.609, loss_scale=8, train_wall=440, gb_free=10.1, wall=53790
2022-03-04 01:47:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 01:54:09 | INFO | train_inner | epoch 031:    335 / 393 loss=5.711, ppl=52.39, wps=14568.8, ups=0.22, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.621, loss_scale=8, train_wall=445, gb_free=10.1, wall=54240
2022-03-04 01:58:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:58:31 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.192 | ppl 146.26 | wps 34224.4 | wpb 2034.1 | bsz 4 | num_updates 12158 | best_loss 7.038
2022-03-04 01:58:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12158 updates
2022-03-04 01:58:32 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 01:58:32 | INFO | train | epoch 031 | loss 5.674 | ppl 51.06 | wps 14624 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12158 | lr 0.000286793 | gnorm 0.617 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 54503
2022-03-04 01:58:32 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 01:58:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:01:39 | INFO | train_inner | epoch 032:     42 / 393 loss=5.659, ppl=50.52, wps=14504.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.624, loss_scale=8, train_wall=438, gb_free=10.1, wall=54690
2022-03-04 02:09:04 | INFO | train_inner | epoch 032:    142 / 393 loss=5.601, ppl=48.55, wps=14717.5, ups=0.22, wpb=65530.2, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.597, loss_scale=8, train_wall=440, gb_free=10.1, wall=55135
2022-03-04 02:16:29 | INFO | train_inner | epoch 032:    242 / 393 loss=5.656, ppl=50.41, wps=14715.2, ups=0.22, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.617, loss_scale=8, train_wall=440, gb_free=10.1, wall=55581
2022-03-04 02:23:55 | INFO | train_inner | epoch 032:    342 / 393 loss=5.685, ppl=51.46, wps=14716.1, ups=0.22, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.615, loss_scale=8, train_wall=440, gb_free=10.1, wall=56026
2022-03-04 02:26:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 02:26:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 02:27:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:27:46 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.199 | ppl 146.95 | wps 34149 | wpb 2034.1 | bsz 4 | num_updates 12549 | best_loss 7.038
2022-03-04 02:27:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12549 updates
2022-03-04 02:27:46 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 02:27:46 | INFO | train | epoch 032 | loss 5.648 | ppl 50.14 | wps 14587.5 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 12549 | lr 0.00028229 | gnorm 0.618 | loss_scale 4 | train_wall 1729 | gb_free 10.1 | wall 56257
2022-03-04 02:27:46 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 02:27:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:31:33 | INFO | train_inner | epoch 033:     51 / 393 loss=5.636, ppl=49.72, wps=14224.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.629, loss_scale=4, train_wall=447, gb_free=10.1, wall=56485
2022-03-04 02:38:58 | INFO | train_inner | epoch 033:    151 / 393 loss=5.589, ppl=48.13, wps=14723.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.624, loss_scale=4, train_wall=440, gb_free=10.1, wall=56930
2022-03-04 02:46:24 | INFO | train_inner | epoch 033:    251 / 393 loss=5.622, ppl=49.24, wps=14718.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.64, loss_scale=4, train_wall=440, gb_free=10.1, wall=57375
2022-03-04 02:53:49 | INFO | train_inner | epoch 033:    351 / 393 loss=5.662, ppl=50.65, wps=14716.3, ups=0.22, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.617, loss_scale=4, train_wall=440, gb_free=10.1, wall=57820
2022-03-04 02:56:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:57:01 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.214 | ppl 148.49 | wps 34171.2 | wpb 2034.1 | bsz 4 | num_updates 12942 | best_loss 7.038
2022-03-04 02:57:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12942 updates
2022-03-04 02:57:01 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 02:57:01 | INFO | train | epoch 033 | loss 5.623 | ppl 49.28 | wps 14663.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 12942 | lr 0.000277971 | gnorm 0.618 | loss_scale 4 | train_wall 1729 | gb_free 10.1 | wall 58012
2022-03-04 02:57:01 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 02:57:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:01:19 | INFO | train_inner | epoch 034:     58 / 393 loss=5.598, ppl=48.43, wps=14506.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.616, loss_scale=4, train_wall=438, gb_free=10.1, wall=58270
2022-03-04 03:08:44 | INFO | train_inner | epoch 034:    158 / 393 loss=5.557, ppl=47.08, wps=14717.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.618, loss_scale=8, train_wall=440, gb_free=10.1, wall=58715
2022-03-04 03:16:09 | INFO | train_inner | epoch 034:    258 / 393 loss=5.61, ppl=48.83, wps=14721.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.621, loss_scale=8, train_wall=440, gb_free=10.1, wall=59161
2022-03-04 03:23:34 | INFO | train_inner | epoch 034:    358 / 393 loss=5.645, ppl=50.06, wps=14721.5, ups=0.22, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.648, loss_scale=8, train_wall=440, gb_free=10.1, wall=59606
2022-03-04 03:26:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:26:15 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.236 | ppl 150.76 | wps 34261.4 | wpb 2034.1 | bsz 4 | num_updates 13335 | best_loss 7.038
2022-03-04 03:26:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13335 updates
2022-03-04 03:26:15 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 03:26:15 | INFO | train | epoch 034 | loss 5.599 | ppl 48.46 | wps 14665.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 13335 | lr 0.000273844 | gnorm 0.626 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 59766
2022-03-04 03:26:15 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 03:26:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:31:04 | INFO | train_inner | epoch 035:     65 / 393 loss=5.565, ppl=47.35, wps=14499.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.612, loss_scale=8, train_wall=439, gb_free=10.1, wall=60056
2022-03-04 03:38:30 | INFO | train_inner | epoch 035:    165 / 393 loss=5.54, ppl=46.54, wps=14720.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.623, loss_scale=8, train_wall=440, gb_free=10.1, wall=60501
2022-03-04 03:45:55 | INFO | train_inner | epoch 035:    265 / 393 loss=5.59, ppl=48.17, wps=14712.9, ups=0.22, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.618, loss_scale=16, train_wall=441, gb_free=10.1, wall=60946
2022-03-04 03:45:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 03:53:25 | INFO | train_inner | epoch 035:    366 / 393 loss=5.622, ppl=49.24, wps=14569.9, ups=0.22, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.651, loss_scale=8, train_wall=445, gb_free=10.1, wall=61396
2022-03-04 03:55:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:55:29 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.25 | ppl 152.26 | wps 34122.1 | wpb 2034.1 | bsz 4 | num_updates 13727 | best_loss 7.038
2022-03-04 03:55:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13727 updates
2022-03-04 03:55:29 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 03:55:29 | INFO | train | epoch 035 | loss 5.575 | ppl 47.68 | wps 14624.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 13727 | lr 0.000269906 | gnorm 0.629 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 61521
2022-03-04 03:55:29 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 03:55:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:00:55 | INFO | train_inner | epoch 036:     73 / 393 loss=5.53, ppl=46.21, wps=14509.4, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.661, loss_scale=8, train_wall=438, gb_free=10.1, wall=61846
2022-03-04 04:08:20 | INFO | train_inner | epoch 036:    173 / 393 loss=5.528, ppl=46.15, wps=14715.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.621, loss_scale=8, train_wall=440, gb_free=10.1, wall=62291
2022-03-04 04:15:45 | INFO | train_inner | epoch 036:    273 / 393 loss=5.567, ppl=47.39, wps=14719.7, ups=0.22, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.647, loss_scale=8, train_wall=440, gb_free=10.1, wall=62736
2022-03-04 04:23:10 | INFO | train_inner | epoch 036:    373 / 393 loss=5.6, ppl=48.5, wps=14713.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.62, loss_scale=8, train_wall=441, gb_free=10.1, wall=63182
2022-03-04 04:24:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:24:44 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.269 | ppl 154.23 | wps 33982.5 | wpb 2034.1 | bsz 4 | num_updates 14120 | best_loss 7.038
2022-03-04 04:24:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14120 updates
2022-03-04 04:24:44 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 04:24:44 | INFO | train | epoch 036 | loss 5.553 | ppl 46.96 | wps 14661.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 14120 | lr 0.000266123 | gnorm 0.637 | loss_scale 16 | train_wall 1729 | gb_free 10.1 | wall 63276
2022-03-04 04:24:44 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 04:24:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:24:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 04:30:45 | INFO | train_inner | epoch 037:     81 / 393 loss=5.488, ppl=44.88, wps=14359.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.63, loss_scale=8, train_wall=443, gb_free=10.1, wall=63636
2022-03-04 04:38:10 | INFO | train_inner | epoch 037:    181 / 393 loss=5.507, ppl=45.48, wps=14718, ups=0.22, wpb=65530.2, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.639, loss_scale=8, train_wall=440, gb_free=10.1, wall=64082
2022-03-04 04:45:35 | INFO | train_inner | epoch 037:    281 / 393 loss=5.551, ppl=46.88, wps=14718, ups=0.22, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.655, loss_scale=8, train_wall=440, gb_free=10.1, wall=64527
2022-03-04 04:53:01 | INFO | train_inner | epoch 037:    381 / 393 loss=5.594, ppl=48.3, wps=14717.9, ups=0.22, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.642, loss_scale=8, train_wall=440, gb_free=10.1, wall=64972
2022-03-04 04:53:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:53:59 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.265 | ppl 153.83 | wps 34131.3 | wpb 2034.1 | bsz 4 | num_updates 14512 | best_loss 7.038
2022-03-04 04:53:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14512 updates
2022-03-04 04:53:59 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 04:53:59 | INFO | train | epoch 037 | loss 5.532 | ppl 46.27 | wps 14626.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14512 | lr 0.000262504 | gnorm 0.643 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 65030
2022-03-04 04:53:59 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 04:53:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:00:31 | INFO | train_inner | epoch 038:     88 / 393 loss=5.465, ppl=44.18, wps=14504.1, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.624, loss_scale=8, train_wall=438, gb_free=10.1, wall=65422
2022-03-04 05:06:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 05:06:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 05:08:05 | INFO | train_inner | epoch 038:    190 / 393 loss=5.485, ppl=44.8, wps=14431.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.646, loss_scale=4, train_wall=449, gb_free=10.1, wall=65876
2022-03-04 05:15:30 | INFO | train_inner | epoch 038:    290 / 393 loss=5.532, ppl=46.26, wps=14717.6, ups=0.22, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.641, loss_scale=4, train_wall=440, gb_free=10.1, wall=66321
2022-03-04 05:22:55 | INFO | train_inner | epoch 038:    390 / 393 loss=5.572, ppl=47.57, wps=14718.6, ups=0.22, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.654, loss_scale=4, train_wall=440, gb_free=10.1, wall=66767
2022-03-04 05:23:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:23:13 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.293 | ppl 156.79 | wps 34296.5 | wpb 2034.1 | bsz 4 | num_updates 14903 | best_loss 7.038
2022-03-04 05:23:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14903 updates
2022-03-04 05:23:13 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 05:23:13 | INFO | train | epoch 038 | loss 5.512 | ppl 45.63 | wps 14589 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 14903 | lr 0.000259038 | gnorm 0.64 | loss_scale 4 | train_wall 1729 | gb_free 10.1 | wall 66784
2022-03-04 05:23:13 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 05:23:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:30:25 | INFO | train_inner | epoch 039:     97 / 393 loss=5.426, ppl=42.98, wps=14505.1, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.619, loss_scale=4, train_wall=438, gb_free=10.1, wall=67216
2022-03-04 05:37:50 | INFO | train_inner | epoch 039:    197 / 393 loss=5.479, ppl=44.6, wps=14722.1, ups=0.22, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.663, loss_scale=4, train_wall=440, gb_free=10.1, wall=67662
2022-03-04 05:45:15 | INFO | train_inner | epoch 039:    297 / 393 loss=5.523, ppl=45.97, wps=14720.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.655, loss_scale=8, train_wall=440, gb_free=10.1, wall=68107
2022-03-04 05:52:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:52:27 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.309 | ppl 158.61 | wps 34073.8 | wpb 2034.1 | bsz 4 | num_updates 15296 | best_loss 7.038
2022-03-04 05:52:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15296 updates
2022-03-04 05:52:27 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 05:52:27 | INFO | train | epoch 039 | loss 5.493 | ppl 45.03 | wps 14663.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 15296 | lr 0.000255688 | gnorm 0.646 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 68539
2022-03-04 05:52:27 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 05:52:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:52:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 05:52:50 | INFO | train_inner | epoch 040:      5 / 393 loss=5.541, ppl=46.57, wps=14357.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.65, loss_scale=4, train_wall=443, gb_free=10.1, wall=68561
2022-03-04 06:00:15 | INFO | train_inner | epoch 040:    105 / 393 loss=5.409, ppl=42.48, wps=14723, ups=0.22, wpb=65530.9, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.641, loss_scale=4, train_wall=440, gb_free=10.1, wall=69006
2022-03-04 06:07:40 | INFO | train_inner | epoch 040:    205 / 393 loss=5.456, ppl=43.88, wps=14718.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.651, loss_scale=4, train_wall=440, gb_free=10.1, wall=69452
2022-03-04 06:15:06 | INFO | train_inner | epoch 040:    305 / 393 loss=5.501, ppl=45.29, wps=14711.9, ups=0.22, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.649, loss_scale=4, train_wall=441, gb_free=10.1, wall=69897
2022-03-04 06:21:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:21:42 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.324 | ppl 160.2 | wps 33959.9 | wpb 2034.1 | bsz 4 | num_updates 15688 | best_loss 7.038
2022-03-04 06:21:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15688 updates
2022-03-04 06:21:42 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 06:21:42 | INFO | train | epoch 040 | loss 5.474 | ppl 44.44 | wps 14624.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 15688 | lr 0.000252474 | gnorm 0.654 | loss_scale 4 | train_wall 1729 | gb_free 10.1 | wall 70293
2022-03-04 06:21:42 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 06:21:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:22:36 | INFO | train_inner | epoch 041:     12 / 393 loss=5.52, ppl=45.89, wps=14500.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.672, loss_scale=4, train_wall=439, gb_free=10.1, wall=70347
2022-03-04 06:30:01 | INFO | train_inner | epoch 041:    112 / 393 loss=5.391, ppl=41.97, wps=14716.6, ups=0.22, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.63, loss_scale=4, train_wall=440, gb_free=10.1, wall=70792
2022-03-04 06:37:26 | INFO | train_inner | epoch 041:    212 / 393 loss=5.447, ppl=43.62, wps=14720, ups=0.22, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.639, loss_scale=8, train_wall=440, gb_free=10.1, wall=71237
2022-03-04 06:44:51 | INFO | train_inner | epoch 041:    312 / 393 loss=5.484, ppl=44.75, wps=14719.8, ups=0.22, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.681, loss_scale=8, train_wall=440, gb_free=10.1, wall=71683
2022-03-04 06:50:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:50:56 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.317 | ppl 159.43 | wps 34118.8 | wpb 2034.1 | bsz 4 | num_updates 16081 | best_loss 7.038
2022-03-04 06:50:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16081 updates
2022-03-04 06:50:56 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 06:50:56 | INFO | train | epoch 041 | loss 5.456 | ppl 43.9 | wps 14663.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 16081 | lr 0.00024937 | gnorm 0.649 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 72048
2022-03-04 06:50:56 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 06:50:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:52:21 | INFO | train_inner | epoch 042:     19 / 393 loss=5.497, ppl=45.16, wps=14505.5, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.64, loss_scale=8, train_wall=438, gb_free=10.1, wall=72132
2022-03-04 06:56:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 06:59:51 | INFO | train_inner | epoch 042:    120 / 393 loss=5.39, ppl=41.92, wps=14564.7, ups=0.22, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.673, loss_scale=4, train_wall=445, gb_free=10.1, wall=72582
2022-03-04 07:07:16 | INFO | train_inner | epoch 042:    220 / 393 loss=5.43, ppl=43.12, wps=14718.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.656, loss_scale=4, train_wall=440, gb_free=10.1, wall=73028
2022-03-04 07:14:41 | INFO | train_inner | epoch 042:    320 / 393 loss=5.47, ppl=44.31, wps=14721.9, ups=0.22, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.673, loss_scale=4, train_wall=440, gb_free=10.1, wall=73473
2022-03-04 07:20:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:20:11 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.329 | ppl 160.74 | wps 34116.9 | wpb 2034.1 | bsz 4 | num_updates 16473 | best_loss 7.038
2022-03-04 07:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16473 updates
2022-03-04 07:20:11 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 07:20:11 | INFO | train | epoch 042 | loss 5.439 | ppl 43.38 | wps 14625.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16473 | lr 0.000246385 | gnorm 0.664 | loss_scale 4 | train_wall 1729 | gb_free 10.1 | wall 73802
2022-03-04 07:20:11 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 07:20:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:22:11 | INFO | train_inner | epoch 043:     27 / 393 loss=5.451, ppl=43.75, wps=14504.1, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.669, loss_scale=4, train_wall=438, gb_free=10.1, wall=73923
2022-03-04 07:29:37 | INFO | train_inner | epoch 043:    127 / 393 loss=5.372, ppl=41.41, wps=14720.1, ups=0.22, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.664, loss_scale=4, train_wall=440, gb_free=10.1, wall=74368
2022-03-04 07:36:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 07:37:06 | INFO | train_inner | epoch 043:    228 / 393 loss=5.416, ppl=42.69, wps=14574.1, ups=0.22, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.682, loss_scale=4, train_wall=445, gb_free=10.1, wall=74818
2022-03-04 07:44:31 | INFO | train_inner | epoch 043:    328 / 393 loss=5.462, ppl=44.08, wps=14720, ups=0.22, wpb=65530.2, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.651, loss_scale=4, train_wall=440, gb_free=10.1, wall=75263
2022-03-04 07:49:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:49:25 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.358 | ppl 164.02 | wps 34123.6 | wpb 2034.1 | bsz 4 | num_updates 16865 | best_loss 7.038
2022-03-04 07:49:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16865 updates
2022-03-04 07:49:25 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 07:49:25 | INFO | train | epoch 043 | loss 5.422 | ppl 42.88 | wps 14628.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16865 | lr 0.000243504 | gnorm 0.665 | loss_scale 4 | train_wall 1728 | gb_free 10.1 | wall 75557
2022-03-04 07:49:25 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 07:49:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:52:01 | INFO | train_inner | epoch 044:     35 / 393 loss=5.426, ppl=43, wps=14510.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.662, loss_scale=4, train_wall=438, gb_free=10.1, wall=75712
2022-03-04 07:59:26 | INFO | train_inner | epoch 044:    135 / 393 loss=5.353, ppl=40.87, wps=14722.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.671, loss_scale=4, train_wall=440, gb_free=10.1, wall=76158
2022-03-04 08:06:51 | INFO | train_inner | epoch 044:    235 / 393 loss=5.405, ppl=42.36, wps=14723.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.657, loss_scale=4, train_wall=440, gb_free=10.1, wall=76603
2022-03-04 08:14:17 | INFO | train_inner | epoch 044:    335 / 393 loss=5.444, ppl=43.52, wps=14713.5, ups=0.22, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.682, loss_scale=4, train_wall=441, gb_free=10.1, wall=77048
2022-03-04 08:16:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 08:18:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:18:39 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.366 | ppl 164.95 | wps 34238 | wpb 2034.1 | bsz 4 | num_updates 17257 | best_loss 7.038
2022-03-04 08:18:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17257 updates
2022-03-04 08:18:39 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 08:18:39 | INFO | train | epoch 044 | loss 5.406 | ppl 42.39 | wps 14627.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 17257 | lr 0.000240723 | gnorm 0.672 | loss_scale 4 | train_wall 1729 | gb_free 10.1 | wall 77311
2022-03-04 08:18:39 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 08:18:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:21:51 | INFO | train_inner | epoch 045:     43 / 393 loss=5.412, ppl=42.58, wps=14362.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.672, loss_scale=4, train_wall=443, gb_free=10.1, wall=77502
2022-03-04 08:29:16 | INFO | train_inner | epoch 045:    143 / 393 loss=5.35, ppl=40.78, wps=14718.6, ups=0.22, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.661, loss_scale=4, train_wall=440, gb_free=10.1, wall=77948
2022-03-04 08:36:42 | INFO | train_inner | epoch 045:    243 / 393 loss=5.391, ppl=41.96, wps=14718.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.696, loss_scale=4, train_wall=440, gb_free=10.1, wall=78393
2022-03-04 08:44:07 | INFO | train_inner | epoch 045:    343 / 393 loss=5.433, ppl=43.2, wps=14723.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.686, loss_scale=4, train_wall=440, gb_free=10.1, wall=78838
2022-03-04 08:47:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:47:54 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.365 | ppl 164.9 | wps 34162.6 | wpb 2034.1 | bsz 4 | num_updates 17650 | best_loss 7.038
2022-03-04 08:47:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17650 updates
2022-03-04 08:47:54 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 08:47:54 | INFO | train | epoch 045 | loss 5.391 | ppl 41.96 | wps 14664.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 17650 | lr 0.000238028 | gnorm 0.674 | loss_scale 4 | train_wall 1729 | gb_free 10.1 | wall 79065
2022-03-04 08:47:54 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 08:47:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:51:36 | INFO | train_inner | epoch 046:     50 / 393 loss=5.384, ppl=41.75, wps=14505.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.67, loss_scale=4, train_wall=438, gb_free=10.1, wall=79288
2022-03-04 08:55:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 08:59:06 | INFO | train_inner | epoch 046:    151 / 393 loss=5.332, ppl=40.29, wps=14575.7, ups=0.22, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.684, loss_scale=4, train_wall=445, gb_free=10.1, wall=79737
2022-03-04 09:06:31 | INFO | train_inner | epoch 046:    251 / 393 loss=5.377, ppl=41.55, wps=14718.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.679, loss_scale=4, train_wall=440, gb_free=10.1, wall=80183
2022-03-04 09:13:56 | INFO | train_inner | epoch 046:    351 / 393 loss=5.424, ppl=42.93, wps=14725.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.696, loss_scale=4, train_wall=440, gb_free=10.1, wall=80628
2022-03-04 09:17:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:17:08 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.384 | ppl 167.09 | wps 34122.7 | wpb 2034.1 | bsz 4 | num_updates 18042 | best_loss 7.038
2022-03-04 09:17:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18042 updates
2022-03-04 09:17:08 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 09:17:08 | INFO | train | epoch 046 | loss 5.376 | ppl 41.53 | wps 14629.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18042 | lr 0.000235428 | gnorm 0.687 | loss_scale 4 | train_wall 1728 | gb_free 10.1 | wall 80819
2022-03-04 09:17:08 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 09:17:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:21:26 | INFO | train_inner | epoch 047:     58 / 393 loss=5.354, ppl=40.89, wps=14507.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.679, loss_scale=4, train_wall=438, gb_free=10.1, wall=81077
2022-03-04 09:28:51 | INFO | train_inner | epoch 047:    158 / 393 loss=5.321, ppl=39.96, wps=14724.6, ups=0.22, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.679, loss_scale=4, train_wall=440, gb_free=10.1, wall=81523
2022-03-04 09:36:16 | INFO | train_inner | epoch 047:    258 / 393 loss=5.37, ppl=41.36, wps=14723, ups=0.22, wpb=65530.2, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.66, loss_scale=8, train_wall=440, gb_free=10.1, wall=81968
2022-03-04 09:43:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 09:43:46 | INFO | train_inner | epoch 047:    359 / 393 loss=5.412, ppl=42.59, wps=14571.3, ups=0.22, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.675, loss_scale=4, train_wall=445, gb_free=10.1, wall=82417
2022-03-04 09:46:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:46:22 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.396 | ppl 168.47 | wps 33981.3 | wpb 2034.1 | bsz 4 | num_updates 18434 | best_loss 7.038
2022-03-04 09:46:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18434 updates
2022-03-04 09:46:22 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 09:46:22 | INFO | train | epoch 047 | loss 5.362 | ppl 41.12 | wps 14629.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18434 | lr 0.000232911 | gnorm 0.674 | loss_scale 4 | train_wall 1728 | gb_free 10.1 | wall 82573
2022-03-04 09:46:22 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 09:46:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:51:16 | INFO | train_inner | epoch 048:     66 / 393 loss=5.329, ppl=40.2, wps=14508, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.707, loss_scale=4, train_wall=438, gb_free=10.1, wall=82867
2022-03-04 09:58:41 | INFO | train_inner | epoch 048:    166 / 393 loss=5.317, ppl=39.87, wps=14718.3, ups=0.22, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.669, loss_scale=4, train_wall=440, gb_free=10.1, wall=83312
2022-03-04 10:06:06 | INFO | train_inner | epoch 048:    266 / 393 loss=5.359, ppl=41.03, wps=14720.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.676, loss_scale=4, train_wall=440, gb_free=10.1, wall=83758
2022-03-04 10:13:32 | INFO | train_inner | epoch 048:    366 / 393 loss=5.402, ppl=42.28, wps=14714.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.692, loss_scale=4, train_wall=440, gb_free=10.1, wall=84203
2022-03-04 10:15:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:15:36 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.408 | ppl 169.87 | wps 34095.6 | wpb 2034.1 | bsz 4 | num_updates 18827 | best_loss 7.038
2022-03-04 10:15:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18827 updates
2022-03-04 10:15:36 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 10:15:36 | INFO | train | epoch 048 | loss 5.349 | ppl 40.75 | wps 14663.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 18827 | lr 0.000230467 | gnorm 0.683 | loss_scale 4 | train_wall 1729 | gb_free 10.1 | wall 84328
2022-03-04 10:15:36 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 10:15:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:21:02 | INFO | train_inner | epoch 049:     73 / 393 loss=5.303, ppl=39.48, wps=14501.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.678, loss_scale=4, train_wall=439, gb_free=10.1, wall=84653
2022-03-04 10:24:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 10:28:31 | INFO | train_inner | epoch 049:    174 / 393 loss=5.304, ppl=39.51, wps=14573, ups=0.22, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.686, loss_scale=4, train_wall=445, gb_free=10.1, wall=85103
2022-03-04 10:35:57 | INFO | train_inner | epoch 049:    274 / 393 loss=5.348, ppl=40.72, wps=14719.5, ups=0.22, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.681, loss_scale=4, train_wall=440, gb_free=10.1, wall=85548
2022-03-04 10:43:22 | INFO | train_inner | epoch 049:    374 / 393 loss=5.389, ppl=41.89, wps=14721.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.696, loss_scale=4, train_wall=440, gb_free=10.1, wall=85993
2022-03-04 10:44:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:44:51 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.42 | ppl 171.3 | wps 33959.3 | wpb 2034.1 | bsz 4 | num_updates 19219 | best_loss 7.038
2022-03-04 10:44:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19219 updates
2022-03-04 10:44:51 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 10:44:51 | INFO | train | epoch 049 | loss 5.334 | ppl 40.35 | wps 14626.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 19219 | lr 0.000228105 | gnorm 0.687 | loss_scale 4 | train_wall 1729 | gb_free 10.1 | wall 86082
2022-03-04 10:44:51 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 10:44:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:50:51 | INFO | train_inner | epoch 050:     81 / 393 loss=5.282, ppl=38.92, wps=14508, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=19300, lr=0.000227626, gnorm=0.669, loss_scale=4, train_wall=438, gb_free=10.1, wall=86443
2022-03-04 10:58:17 | INFO | train_inner | epoch 050:    181 / 393 loss=5.287, ppl=39.04, wps=14720.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.689, loss_scale=4, train_wall=440, gb_free=10.1, wall=86888
2022-03-04 11:04:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 11:05:46 | INFO | train_inner | epoch 050:    282 / 393 loss=5.342, ppl=40.57, wps=14574.3, ups=0.22, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.711, loss_scale=4, train_wall=445, gb_free=10.1, wall=87338
2022-03-04 11:13:11 | INFO | train_inner | epoch 050:    382 / 393 loss=5.385, ppl=41.79, wps=14720.2, ups=0.22, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.695, loss_scale=4, train_wall=440, gb_free=10.1, wall=87783
2022-03-04 11:13:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:14:05 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.43 | ppl 172.39 | wps 34059.2 | wpb 2034.1 | bsz 4 | num_updates 19611 | best_loss 7.038
2022-03-04 11:14:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19611 updates
2022-03-04 11:14:05 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 11:14:05 | INFO | train | epoch 050 | loss 5.322 | ppl 40.01 | wps 14628.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 19611 | lr 0.000225814 | gnorm 0.69 | loss_scale 4 | train_wall 1728 | gb_free 10.1 | wall 87836
2022-03-04 11:14:05 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 11:14:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:20:41 | INFO | train_inner | epoch 051:     89 / 393 loss=5.258, ppl=38.26, wps=14506.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.68, loss_scale=4, train_wall=438, gb_free=10.1, wall=88233
2022-03-04 11:28:06 | INFO | train_inner | epoch 051:    189 / 393 loss=5.286, ppl=39.03, wps=14719, ups=0.22, wpb=65530.2, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.674, loss_scale=4, train_wall=440, gb_free=10.1, wall=88678
2022-03-04 11:35:32 | INFO | train_inner | epoch 051:    289 / 393 loss=5.338, ppl=40.46, wps=14712.5, ups=0.22, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.711, loss_scale=4, train_wall=440, gb_free=10.1, wall=89123
2022-03-04 11:42:57 | INFO | train_inner | epoch 051:    389 / 393 loss=5.365, ppl=41.22, wps=14721.7, ups=0.22, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.695, loss_scale=8, train_wall=440, gb_free=10.1, wall=89568
2022-03-04 11:43:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:43:19 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.441 | ppl 173.81 | wps 34217.9 | wpb 2034.1 | bsz 4 | num_updates 20004 | best_loss 7.038
2022-03-04 11:43:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20004 updates
2022-03-04 11:43:19 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 11:43:19 | INFO | train | epoch 051 | loss 5.31 | ppl 39.67 | wps 14663.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 20004 | lr 0.000223584 | gnorm 0.69 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 89591
2022-03-04 11:43:19 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 11:43:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:50:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 11:50:31 | INFO | train_inner | epoch 052:     97 / 393 loss=5.238, ppl=37.75, wps=14365, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=20100, lr=0.00022305, gnorm=0.677, loss_scale=4, train_wall=443, gb_free=10.1, wall=90023
2022-03-04 11:57:56 | INFO | train_inner | epoch 052:    197 / 393 loss=5.281, ppl=38.89, wps=14721.3, ups=0.22, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.671, loss_scale=4, train_wall=440, gb_free=10.1, wall=90468
2022-03-04 12:05:22 | INFO | train_inner | epoch 052:    297 / 393 loss=5.315, ppl=39.82, wps=14718, ups=0.22, wpb=65530.9, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.699, loss_scale=4, train_wall=440, gb_free=10.1, wall=90913
2022-03-04 12:12:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:12:34 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.448 | ppl 174.65 | wps 34131.5 | wpb 2034.1 | bsz 4 | num_updates 20396 | best_loss 7.038
2022-03-04 12:12:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20396 updates
2022-03-04 12:12:34 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 12:12:34 | INFO | train | epoch 052 | loss 5.298 | ppl 39.34 | wps 14626.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20396 | lr 0.000221425 | gnorm 0.688 | loss_scale 4 | train_wall 1729 | gb_free 10.1 | wall 91345
2022-03-04 12:12:34 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 12:12:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:12:52 | INFO | train_inner | epoch 053:      4 / 393 loss=5.357, ppl=40.98, wps=14503.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.705, loss_scale=4, train_wall=439, gb_free=10.1, wall=91363
2022-03-04 12:20:17 | INFO | train_inner | epoch 053:    104 / 393 loss=5.221, ppl=37.3, wps=14716.7, ups=0.22, wpb=65530.2, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.706, loss_scale=4, train_wall=440, gb_free=10.1, wall=91808
2022-03-04 12:27:42 | INFO | train_inner | epoch 053:    204 / 393 loss=5.271, ppl=38.62, wps=14716.9, ups=0.22, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.698, loss_scale=4, train_wall=440, gb_free=10.1, wall=92254
2022-03-04 12:30:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 12:35:12 | INFO | train_inner | epoch 053:    305 / 393 loss=5.313, ppl=39.75, wps=14573.7, ups=0.22, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.695, loss_scale=4, train_wall=445, gb_free=10.1, wall=92703
2022-03-04 12:41:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:41:48 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.46 | ppl 176.04 | wps 34140.8 | wpb 2034.1 | bsz 4 | num_updates 20788 | best_loss 7.038
2022-03-04 12:41:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20788 updates
2022-03-04 12:41:48 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 12:41:48 | INFO | train | epoch 053 | loss 5.286 | ppl 39.02 | wps 14625.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20788 | lr 0.000219328 | gnorm 0.697 | loss_scale 4 | train_wall 1729 | gb_free 10.1 | wall 93100
2022-03-04 12:41:48 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 12:41:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:42:42 | INFO | train_inner | epoch 054:     12 / 393 loss=5.331, ppl=40.25, wps=14504.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=20800, lr=0.000219265, gnorm=0.689, loss_scale=4, train_wall=439, gb_free=10.1, wall=93153
2022-03-04 12:50:07 | INFO | train_inner | epoch 054:    112 / 393 loss=5.214, ppl=37.12, wps=14719.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.699, loss_scale=4, train_wall=440, gb_free=10.1, wall=93598
2022-03-04 12:57:32 | INFO | train_inner | epoch 054:    212 / 393 loss=5.26, ppl=38.33, wps=14711.5, ups=0.22, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.685, loss_scale=4, train_wall=441, gb_free=10.1, wall=94044
2022-03-04 13:04:58 | INFO | train_inner | epoch 054:    312 / 393 loss=5.303, ppl=39.48, wps=14721.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.687, loss_scale=4, train_wall=440, gb_free=10.1, wall=94489
2022-03-04 13:10:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:11:03 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.443 | ppl 174.04 | wps 34178.4 | wpb 2034.1 | bsz 4 | num_updates 21181 | best_loss 7.038
2022-03-04 13:11:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21181 updates
2022-03-04 13:11:03 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 13:11:03 | INFO | train | epoch 054 | loss 5.276 | ppl 38.73 | wps 14663.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 21181 | lr 0.000217284 | gnorm 0.694 | loss_scale 8 | train_wall 1729 | gb_free 10.1 | wall 94854
2022-03-04 13:11:03 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 13:11:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:12:27 | INFO | train_inner | epoch 055:     19 / 393 loss=5.317, ppl=39.87, wps=14504.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.706, loss_scale=8, train_wall=438, gb_free=10.1, wall=94939
2022-03-04 13:19:53 | INFO | train_inner | epoch 055:    119 / 393 loss=5.211, ppl=37.03, wps=14716.1, ups=0.22, wpb=65530.2, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.689, loss_scale=8, train_wall=440, gb_free=10.1, wall=95384
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
