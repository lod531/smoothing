Sender: LSF System <lsfadmin@eu-g3-072>
Subject: Job 207346224: <w103_fp16_size_0.03125_jelinek_0.04_0.04_0.92_#1> in cluster <euler> Exited

Job <w103_fp16_size_0.03125_jelinek_0.04_0.04_0.92_#1> was submitted from host <eu-login-10> by user <andriusb> in cluster <euler> at Sun Mar  6 13:28:14 2022
Job was executed on host(s) <eu-g3-072>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Sun Mar  6 15:59:28 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Sun Mar  6 15:59:28 2022
Terminated at Sun Mar  6 20:38:52 2022
Results reported at Sun Mar  6 20:38:52 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.03125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.04, 0.04, 0.92)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --no-epoch-checkpoints --no-last-checkpoints --seed 66575621 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   16744.29 sec.
    Max Memory :                                 6627 MB
    Average Memory :                             3880.76 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               13373.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   16764 sec.
    Turnaround time :                            25838 sec.

The output (if any) follows:

2022-03-06 15:59:40 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575621, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.03125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575621, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.04, 0.04, 0.92)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-06 15:59:40 | INFO | fairseq.tasks.language_modeling | dictionary: 96056 types
2022-03-06 15:59:42 | INFO | fairseq.data.data_utils | loaded 56,292 examples from: data-bin/wikitext-103-raw-size-0.03125/train
Calculating frequency stats:
  0%|          | 0/56292 [00:00<?, ?it/s]  1%|          | 686/56292 [00:00<00:08, 6833.26it/s]  2%|▏         | 1370/56292 [00:00<00:09, 5976.10it/s]  4%|▎         | 1975/56292 [00:00<00:09, 5684.42it/s]  5%|▍         | 2561/56292 [00:00<00:09, 5744.50it/s]  6%|▌         | 3238/56292 [00:00<00:08, 6092.67it/s]  7%|▋         | 3874/56292 [00:00<00:08, 6180.00it/s]  8%|▊         | 4577/56292 [00:00<00:08, 6447.03it/s]  9%|▉         | 5276/56292 [00:00<00:07, 6615.77it/s] 11%|█         | 5981/56292 [00:00<00:07, 6749.61it/s] 12%|█▏        | 6658/56292 [00:01<00:08, 6127.85it/s] 13%|█▎        | 7283/56292 [00:01<00:07, 6142.32it/s] 14%|█▍        | 7906/56292 [00:01<00:07, 6080.59it/s] 15%|█▌        | 8520/56292 [00:01<00:08, 5832.50it/s] 16%|█▋        | 9172/56292 [00:01<00:07, 6026.71it/s] 17%|█▋        | 9780/56292 [00:01<00:07, 6006.49it/s] 18%|█▊        | 10408/56292 [00:01<00:07, 6084.54it/s] 20%|█▉        | 11020/56292 [00:01<00:07, 6056.90it/s] 21%|██        | 11628/56292 [00:01<00:07, 5879.89it/s] 22%|██▏       | 12264/56292 [00:02<00:07, 6011.66it/s] 23%|██▎       | 12883/56292 [00:02<00:07, 6057.76it/s] 24%|██▍       | 13567/56292 [00:02<00:06, 6286.23it/s] 25%|██▌       | 14198/56292 [00:02<00:06, 6151.24it/s] 26%|██▋       | 14843/56292 [00:02<00:06, 6238.23it/s] 27%|██▋       | 15469/56292 [00:02<00:06, 6078.58it/s] 29%|██▊       | 16079/56292 [00:02<00:06, 5934.65it/s] 30%|██▉       | 16674/56292 [00:02<00:06, 5863.44it/s] 31%|███       | 17279/56292 [00:02<00:06, 5914.11it/s] 32%|███▏      | 17907/56292 [00:02<00:06, 6012.14it/s] 33%|███▎      | 18510/56292 [00:03<00:06, 5866.35it/s] 34%|███▍      | 19242/56292 [00:03<00:05, 6279.65it/s] 35%|███▌      | 19911/56292 [00:03<00:05, 6394.52it/s] 37%|███▋      | 20553/56292 [00:03<00:05, 6132.31it/s] 38%|███▊      | 21178/56292 [00:03<00:05, 6162.96it/s] 39%|███▊      | 21797/56292 [00:03<00:05, 5956.88it/s] 40%|███▉      | 22434/56292 [00:03<00:05, 6071.56it/s] 41%|████      | 23077/56292 [00:03<00:05, 6169.26it/s] 42%|████▏     | 23778/56292 [00:03<00:05, 6408.12it/s] 44%|████▎     | 24539/56292 [00:03<00:04, 6762.04it/s] 45%|████▍     | 25241/56292 [00:04<00:04, 6835.43it/s] 46%|████▌     | 25926/56292 [00:04<00:04, 6502.19it/s] 47%|████▋     | 26581/56292 [00:04<00:04, 6178.20it/s] 48%|████▊     | 27205/56292 [00:04<00:04, 5862.17it/s] 49%|████▉     | 27797/56292 [00:04<00:04, 5857.06it/s] 51%|█████     | 28514/56292 [00:04<00:04, 6226.17it/s] 52%|█████▏    | 29147/56292 [00:04<00:04, 6251.45it/s] 53%|█████▎    | 29776/56292 [00:04<00:04, 6070.15it/s] 54%|█████▍    | 30387/56292 [00:04<00:04, 6051.21it/s] 55%|█████▌    | 30995/56292 [00:05<00:04, 5684.71it/s] 56%|█████▌    | 31625/56292 [00:05<00:04, 5852.99it/s] 57%|█████▋    | 32216/56292 [00:05<00:04, 5606.70it/s] 58%|█████▊    | 32837/56292 [00:05<00:04, 5775.13it/s] 59%|█████▉    | 33420/56292 [00:05<00:04, 5656.65it/s] 60%|██████    | 34046/56292 [00:05<00:03, 5825.91it/s] 62%|██████▏   | 34724/56292 [00:05<00:03, 6100.93it/s] 63%|██████▎   | 35338/56292 [00:05<00:03, 5931.62it/s] 64%|██████▍   | 35935/56292 [00:05<00:03, 5861.46it/s] 65%|██████▍   | 36583/56292 [00:06<00:03, 6039.72it/s] 66%|██████▌   | 37190/56292 [00:06<00:03, 5804.80it/s] 67%|██████▋   | 37774/56292 [00:06<00:03, 5672.64it/s] 68%|██████▊   | 38344/56292 [00:06<00:03, 5638.47it/s] 69%|██████▉   | 38967/56292 [00:06<00:02, 5808.53it/s] 70%|███████   | 39550/56292 [00:06<00:02, 5738.63it/s] 71%|███████▏  | 40198/56292 [00:06<00:02, 5951.59it/s] 73%|███████▎  | 40879/56292 [00:06<00:02, 6202.79it/s] 74%|███████▎  | 41501/56292 [00:06<00:02, 6062.22it/s] 75%|███████▍  | 42109/56292 [00:06<00:02, 5707.77it/s] 76%|███████▌  | 42685/56292 [00:07<00:02, 5672.58it/s] 77%|███████▋  | 43256/56292 [00:07<00:02, 5613.24it/s] 78%|███████▊  | 43924/56292 [00:07<00:02, 5915.87it/s] 79%|███████▉  | 44563/56292 [00:07<00:01, 6053.21it/s] 80%|████████  | 45171/56292 [00:07<00:01, 5970.92it/s] 81%|████████▏ | 45770/56292 [00:07<00:01, 5929.78it/s] 83%|████████▎ | 46486/56292 [00:07<00:01, 6290.01it/s] 84%|████████▎ | 47117/56292 [00:07<00:01, 6263.55it/s] 85%|████████▌ | 48070/56292 [00:07<00:01, 7225.74it/s] 87%|████████▋ | 48795/56292 [00:08<00:01, 7002.05it/s] 88%|████████▊ | 49552/56292 [00:08<00:00, 7161.26it/s] 89%|████████▉ | 50271/56292 [00:08<00:00, 6507.93it/s] 90%|█████████ | 50935/56292 [00:08<00:00, 6285.78it/s] 92%|█████████▏| 51573/56292 [00:08<00:00, 6182.33it/s] 93%|█████████▎| 52449/56292 [00:08<00:00, 6890.57it/s] 94%|█████████▍| 53148/56292 [00:08<00:00, 6555.31it/s] 96%|█████████▌| 53813/56292 [00:08<00:00, 6467.90it/s] 97%|█████████▋| 54466/56292 [00:08<00:00, 5987.83it/s] 98%|█████████▊| 55087/56292 [00:09<00:00, 6044.58it/s] 99%|█████████▉| 55770/56292 [00:09<00:00, 6261.76it/s]100%|██████████| 56292/56292 [00:09<00:00, 6116.19it/s]

gathering stats for n=1
  0%|          | 0/56292 [00:00<?, ?it/s]  3%|▎         | 1909/56292 [00:00<00:02, 19074.78it/s]  7%|▋         | 3973/56292 [00:00<00:02, 19989.57it/s] 11%|█         | 6208/56292 [00:00<00:02, 21064.41it/s] 15%|█▍        | 8315/56292 [00:00<00:02, 20111.84it/s] 18%|█▊        | 10333/56292 [00:00<00:02, 20114.53it/s] 22%|██▏       | 12349/56292 [00:00<00:02, 19965.77it/s] 26%|██▌       | 14406/56292 [00:00<00:02, 20154.14it/s] 29%|██▉       | 16424/56292 [00:00<00:01, 19984.72it/s] 33%|███▎      | 18425/56292 [00:00<00:01, 19992.02it/s] 36%|███▋      | 20433/56292 [00:01<00:01, 20016.04it/s] 40%|███▉      | 22436/56292 [00:01<00:01, 19966.70it/s] 44%|████▍     | 24763/56292 [00:01<00:01, 20962.36it/s] 48%|████▊     | 26861/56292 [00:01<00:01, 20288.32it/s] 51%|█████▏    | 28895/56292 [00:01<00:01, 20268.89it/s] 55%|█████▍    | 30926/56292 [00:01<00:01, 19843.00it/s] 58%|█████▊    | 32915/56292 [00:01<00:01, 19505.94it/s] 62%|██████▏   | 34951/56292 [00:01<00:01, 19753.15it/s] 66%|██████▌   | 36930/56292 [00:01<00:00, 19555.06it/s] 69%|██████▉   | 38888/56292 [00:01<00:00, 19307.88it/s] 73%|███████▎  | 40956/56292 [00:02<00:00, 19703.57it/s] 76%|███████▋  | 42929/56292 [00:02<00:00, 19231.87it/s] 80%|███████▉  | 44964/56292 [00:02<00:00, 19556.74it/s] 84%|████████▎ | 47043/56292 [00:02<00:00, 19912.90it/s] 88%|████████▊ | 49548/56292 [00:02<00:00, 21426.76it/s] 92%|█████████▏| 51696/56292 [00:02<00:00, 20749.31it/s] 96%|█████████▌| 53866/56292 [00:02<00:00, 21014.51it/s] 99%|█████████▉| 55974/56292 [00:02<00:00, 20544.24it/s]100%|██████████| 56292/56292 [00:02<00:00, 20125.91it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 217.60it/s]2022-03-06 16:00:01 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(96056, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=96056, bias=False)
  )
)
2022-03-06 16:00:01 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-06 16:00:01 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-06 16:00:01 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-06 16:00:01 | INFO | fairseq_cli.train | num. shared model params: 68,094,976 (num. trained: 68,094,976)
2022-03-06 16:00:01 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-06 16:00:01 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.03125/valid
2022-03-06 16:00:01 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-06 16:00:01 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 16:00:01 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-06 16:00:01 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 16:00:01 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-06 16:00:01 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-06 16:00:01 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_last.pt
2022-03-06 16:00:01 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_last.pt
2022-03-06 16:00:01 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-06 16:00:01 | INFO | fairseq.data.data_utils | loaded 56,292 examples from: data-bin/wikitext-103-raw-size-0.03125/train
2022-03-06 16:00:01 | INFO | fairseq.trainer | begin training epoch 1
2022-03-06 16:00:01 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-06 16:00:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-06 16:00:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 16:00:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:00:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 16:00:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-06 16:02:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:02:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 15.585 | ppl 49138.4 | wps 40119.9 | wpb 510.9 | bsz 1 | num_updates 44
2022-03-06 16:02:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 44 updates
2022-03-06 16:02:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:02:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:02:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 1 @ 44 updates, score 15.585) (writing took 1.8120749490335584 seconds)
2022-03-06 16:02:44 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-06 16:02:44 | INFO | train | epoch 001 | loss 16.783 | ppl 112736 | wps 21813.7 | ups 0.34 | wpb 64791.3 | bsz 126.5 | num_updates 44 | lr 5.5989e-06 | gnorm 5.236 | loss_scale 4 | train_wall 144 | gb_free 21.5 | wall 163
2022-03-06 16:02:44 | INFO | fairseq.trainer | begin training epoch 2
2022-03-06 16:02:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:05:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:05:06 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 14.022 | ppl 16636.6 | wps 40064.7 | wpb 510.9 | bsz 1 | num_updates 93 | best_loss 14.022
2022-03-06 16:05:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 93 updates
2022-03-06 16:05:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:05:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:05:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 2 @ 93 updates, score 14.022) (writing took 1.7398632355034351 seconds)
2022-03-06 16:05:08 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-06 16:05:08 | INFO | train | epoch 002 | loss 14.729 | ppl 27149.5 | wps 21972.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 93 | lr 1.17227e-05 | gnorm 2.273 | loss_scale 4 | train_wall 127 | gb_free 21.5 | wall 307
2022-03-06 16:05:08 | INFO | fairseq.trainer | begin training epoch 3
2022-03-06 16:05:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:05:28 | INFO | train_inner | epoch 003:      7 / 49 loss=15.584, ppl=49114.8, wps=21975.6, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=100, lr=1.25975e-05, gnorm=3.527, loss_scale=4, train_wall=289, gb_free=21.5, wall=327
2022-03-06 16:07:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:07:31 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 13.376 | ppl 10629.2 | wps 40071.2 | wpb 510.9 | bsz 1 | num_updates 142 | best_loss 13.376
2022-03-06 16:07:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 142 updates
2022-03-06 16:07:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:07:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:07:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 3 @ 142 updates, score 13.376) (writing took 1.809219497255981 seconds)
2022-03-06 16:07:33 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-06 16:07:33 | INFO | train | epoch 003 | loss 13.784 | ppl 14105 | wps 21972.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 142 | lr 1.78465e-05 | gnorm 1.471 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 452
2022-03-06 16:07:33 | INFO | fairseq.trainer | begin training epoch 4
2022-03-06 16:07:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:09:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:09:56 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 12.581 | ppl 6126.55 | wps 40098.4 | wpb 510.9 | bsz 1 | num_updates 191 | best_loss 12.581
2022-03-06 16:09:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 191 updates
2022-03-06 16:09:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:09:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:09:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 4 @ 191 updates, score 12.581) (writing took 1.7548551531508565 seconds)
2022-03-06 16:09:58 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-06 16:09:58 | INFO | train | epoch 004 | loss 13.068 | ppl 8590.08 | wps 21955 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 191 | lr 2.39702e-05 | gnorm 1.289 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 597
2022-03-06 16:09:58 | INFO | fairseq.trainer | begin training epoch 5
2022-03-06 16:09:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:10:23 | INFO | train_inner | epoch 005:      9 / 49 loss=13.301, ppl=10095.5, wps=21984.8, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=200, lr=2.5095e-05, gnorm=1.344, loss_scale=8, train_wall=258, gb_free=21.5, wall=622
2022-03-06 16:12:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:12:21 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.779 | ppl 3513.36 | wps 40022.6 | wpb 510.9 | bsz 1 | num_updates 240 | best_loss 11.779
2022-03-06 16:12:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 240 updates
2022-03-06 16:12:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:12:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:12:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 5 @ 240 updates, score 11.779) (writing took 1.8153179306536913 seconds)
2022-03-06 16:12:22 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-06 16:12:22 | INFO | train | epoch 005 | loss 12.206 | ppl 4724.9 | wps 21938.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 240 | lr 3.0094e-05 | gnorm 1.001 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 742
2022-03-06 16:12:22 | INFO | fairseq.trainer | begin training epoch 6
2022-03-06 16:12:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:14:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:14:45 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.177 | ppl 2314.53 | wps 40108.8 | wpb 510.9 | bsz 1 | num_updates 289 | best_loss 11.177
2022-03-06 16:14:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 289 updates
2022-03-06 16:14:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:14:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:14:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 6 @ 289 updates, score 11.177) (writing took 1.7204253487288952 seconds)
2022-03-06 16:14:47 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-06 16:14:47 | INFO | train | epoch 006 | loss 11.46 | ppl 2817.62 | wps 21965.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 289 | lr 3.62178e-05 | gnorm 0.739 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 886
2022-03-06 16:14:47 | INFO | fairseq.trainer | begin training epoch 7
2022-03-06 16:14:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:15:18 | INFO | train_inner | epoch 007:     11 / 49 loss=11.688, ppl=3298.5, wps=21976.6, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=300, lr=3.75925e-05, gnorm=0.824, loss_scale=16, train_wall=258, gb_free=21.5, wall=918
2022-03-06 16:17:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:17:10 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.814 | ppl 1800.75 | wps 39992 | wpb 510.9 | bsz 1 | num_updates 338 | best_loss 10.814
2022-03-06 16:17:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 338 updates
2022-03-06 16:17:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:17:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:17:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 7 @ 338 updates, score 10.814) (writing took 1.8401486864313483 seconds)
2022-03-06 16:17:12 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-06 16:17:12 | INFO | train | epoch 007 | loss 10.94 | ppl 1964.41 | wps 21943.5 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 338 | lr 4.23416e-05 | gnorm 0.546 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 1031
2022-03-06 16:17:12 | INFO | fairseq.trainer | begin training epoch 8
2022-03-06 16:17:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:19:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:19:35 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.598 | ppl 1549.47 | wps 40093.2 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 10.598
2022-03-06 16:19:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 387 updates
2022-03-06 16:19:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:19:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:19:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 8 @ 387 updates, score 10.598) (writing took 1.9855303866788745 seconds)
2022-03-06 16:19:37 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-06 16:19:37 | INFO | train | epoch 008 | loss 10.638 | ppl 1593.01 | wps 21919.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 387 | lr 4.84653e-05 | gnorm 0.471 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 1176
2022-03-06 16:19:37 | INFO | fairseq.trainer | begin training epoch 9
2022-03-06 16:19:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:20:14 | INFO | train_inner | epoch 009:     13 / 49 loss=10.718, ppl=1684.09, wps=21956.5, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=400, lr=5.009e-05, gnorm=0.485, loss_scale=32, train_wall=258, gb_free=21.5, wall=1213
2022-03-06 16:21:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:22:00 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.444 | ppl 1393.28 | wps 39935.5 | wpb 510.9 | bsz 1 | num_updates 436 | best_loss 10.444
2022-03-06 16:22:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 436 updates
2022-03-06 16:22:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:22:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:22:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 9 @ 436 updates, score 10.444) (writing took 1.8627622444182634 seconds)
2022-03-06 16:22:02 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-06 16:22:02 | INFO | train | epoch 009 | loss 10.447 | ppl 1396.3 | wps 21939.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 436 | lr 5.45891e-05 | gnorm 0.435 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 1321
2022-03-06 16:22:02 | INFO | fairseq.trainer | begin training epoch 10
2022-03-06 16:22:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:24:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:24:25 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.302 | ppl 1262.58 | wps 40146.8 | wpb 510.9 | bsz 1 | num_updates 485 | best_loss 10.302
2022-03-06 16:24:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 485 updates
2022-03-06 16:24:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:24:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:24:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 10 @ 485 updates, score 10.302) (writing took 1.9533183099702 seconds)
2022-03-06 16:24:27 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-06 16:24:27 | INFO | train | epoch 010 | loss 10.292 | ppl 1253.58 | wps 21921.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 485 | lr 6.07129e-05 | gnorm 0.484 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 1466
2022-03-06 16:24:27 | INFO | fairseq.trainer | begin training epoch 11
2022-03-06 16:24:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:25:09 | INFO | train_inner | epoch 011:     15 / 49 loss=10.323, ppl=1280.84, wps=21948.4, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=500, lr=6.25875e-05, gnorm=0.458, loss_scale=32, train_wall=258, gb_free=21.5, wall=1509
2022-03-06 16:25:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 16:26:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:26:50 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.171 | ppl 1152.62 | wps 39952 | wpb 510.9 | bsz 1 | num_updates 533 | best_loss 10.171
2022-03-06 16:26:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 533 updates
2022-03-06 16:26:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:26:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:26:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 11 @ 533 updates, score 10.171) (writing took 2.0234713684767485 seconds)
2022-03-06 16:26:52 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-06 16:26:52 | INFO | train | epoch 011 | loss 10.142 | ppl 1130.24 | wps 21465.8 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 533 | lr 6.67117e-05 | gnorm 0.515 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 1611
2022-03-06 16:26:52 | INFO | fairseq.trainer | begin training epoch 12
2022-03-06 16:26:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:29:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:29:15 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.038 | ppl 1051.48 | wps 40136.5 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 10.038
2022-03-06 16:29:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 582 updates
2022-03-06 16:29:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:29:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:29:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 12 @ 582 updates, score 10.038) (writing took 1.9066410763189197 seconds)
2022-03-06 16:29:17 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-06 16:29:17 | INFO | train | epoch 012 | loss 9.999 | ppl 1023.24 | wps 21932.5 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 582 | lr 7.28355e-05 | gnorm 0.576 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 1756
2022-03-06 16:29:17 | INFO | fairseq.trainer | begin training epoch 13
2022-03-06 16:29:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:30:08 | INFO | train_inner | epoch 013:     18 / 49 loss=10.019, ppl=1037.63, wps=21748.1, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=600, lr=7.5085e-05, gnorm=0.56, loss_scale=32, train_wall=261, gb_free=21.5, wall=1807
2022-03-06 16:31:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:31:40 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 9.922 | ppl 970.03 | wps 40063.2 | wpb 510.9 | bsz 1 | num_updates 631 | best_loss 9.922
2022-03-06 16:31:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 631 updates
2022-03-06 16:31:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:31:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:31:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 13 @ 631 updates, score 9.922) (writing took 1.954653169028461 seconds)
2022-03-06 16:31:41 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-06 16:31:41 | INFO | train | epoch 013 | loss 9.857 | ppl 927.62 | wps 21951.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 631 | lr 7.89592e-05 | gnorm 0.588 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 1901
2022-03-06 16:31:41 | INFO | fairseq.trainer | begin training epoch 14
2022-03-06 16:31:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:32:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 16:33:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:34:04 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 9.808 | ppl 896.14 | wps 40328.4 | wpb 510.9 | bsz 1 | num_updates 679 | best_loss 9.808
2022-03-06 16:34:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 679 updates
2022-03-06 16:34:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:34:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:34:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 14 @ 679 updates, score 9.808) (writing took 1.9830349376425147 seconds)
2022-03-06 16:34:06 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-06 16:34:06 | INFO | train | epoch 014 | loss 9.725 | ppl 846.05 | wps 21484.2 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 679 | lr 8.4958e-05 | gnorm 0.615 | loss_scale 32 | train_wall 126 | gb_free 21.5 | wall 2046
2022-03-06 16:34:06 | INFO | fairseq.trainer | begin training epoch 15
2022-03-06 16:34:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:35:06 | INFO | train_inner | epoch 015:     21 / 49 loss=9.74, ppl=854.84, wps=21755.7, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=700, lr=8.75825e-05, gnorm=0.656, loss_scale=32, train_wall=261, gb_free=21.5, wall=2105
2022-03-06 16:36:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:36:29 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 9.712 | ppl 838.73 | wps 40271.7 | wpb 510.9 | bsz 1 | num_updates 728 | best_loss 9.712
2022-03-06 16:36:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 728 updates
2022-03-06 16:36:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:36:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:36:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 15 @ 728 updates, score 9.712) (writing took 1.982723449356854 seconds)
2022-03-06 16:36:31 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-06 16:36:31 | INFO | train | epoch 015 | loss 9.601 | ppl 776.43 | wps 21939.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 728 | lr 9.10818e-05 | gnorm 0.704 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 2190
2022-03-06 16:36:31 | INFO | fairseq.trainer | begin training epoch 16
2022-03-06 16:36:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:38:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:38:54 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.613 | ppl 782.81 | wps 40173.3 | wpb 510.9 | bsz 1 | num_updates 777 | best_loss 9.613
2022-03-06 16:38:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 777 updates
2022-03-06 16:38:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:38:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:38:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 16 @ 777 updates, score 9.613) (writing took 2.015757475979626 seconds)
2022-03-06 16:38:56 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-06 16:38:56 | INFO | train | epoch 016 | loss 9.477 | ppl 712.48 | wps 21921.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 777 | lr 9.72056e-05 | gnorm 0.704 | loss_scale 64 | train_wall 127 | gb_free 21.5 | wall 2335
2022-03-06 16:38:56 | INFO | fairseq.trainer | begin training epoch 17
2022-03-06 16:38:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:39:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 16:40:04 | INFO | train_inner | epoch 017:     24 / 49 loss=9.483, ppl=715.65, wps=21743.2, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=800, lr=0.00010008, gnorm=0.706, loss_scale=32, train_wall=261, gb_free=21.5, wall=2403
2022-03-06 16:41:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:41:19 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 9.546 | ppl 747.7 | wps 40311.9 | wpb 510.9 | bsz 1 | num_updates 825 | best_loss 9.546
2022-03-06 16:41:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 825 updates
2022-03-06 16:41:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:41:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:41:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 17 @ 825 updates, score 9.546) (writing took 1.8975299960002303 seconds)
2022-03-06 16:41:21 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-06 16:41:21 | INFO | train | epoch 017 | loss 9.36 | ppl 657.1 | wps 21488.8 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 825 | lr 0.000103204 | gnorm 0.799 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 2480
2022-03-06 16:41:21 | INFO | fairseq.trainer | begin training epoch 18
2022-03-06 16:41:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:43:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:43:44 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 9.459 | ppl 704.02 | wps 40098.5 | wpb 510.9 | bsz 1 | num_updates 874 | best_loss 9.459
2022-03-06 16:43:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 874 updates
2022-03-06 16:43:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:43:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:43:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 18 @ 874 updates, score 9.459) (writing took 2.005459927022457 seconds)
2022-03-06 16:43:46 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-06 16:43:46 | INFO | train | epoch 018 | loss 9.245 | ppl 606.9 | wps 21900.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 874 | lr 0.000109328 | gnorm 0.811 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 2625
2022-03-06 16:43:46 | INFO | fairseq.trainer | begin training epoch 19
2022-03-06 16:43:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:45:00 | INFO | train_inner | epoch 019:     26 / 49 loss=9.243, ppl=606.04, wps=21939.9, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=900, lr=0.000112578, gnorm=0.817, loss_scale=32, train_wall=259, gb_free=21.5, wall=2699
2022-03-06 16:46:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:46:09 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.4 | ppl 675.53 | wps 40057.7 | wpb 510.9 | bsz 1 | num_updates 923 | best_loss 9.4
2022-03-06 16:46:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 923 updates
2022-03-06 16:46:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:46:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:46:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 19 @ 923 updates, score 9.4) (writing took 2.1062967218458652 seconds)
2022-03-06 16:46:11 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-06 16:46:11 | INFO | train | epoch 019 | loss 9.135 | ppl 562.17 | wps 21897.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 923 | lr 0.000115452 | gnorm 0.826 | loss_scale 64 | train_wall 127 | gb_free 21.5 | wall 2770
2022-03-06 16:46:11 | INFO | fairseq.trainer | begin training epoch 20
2022-03-06 16:46:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:46:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 16:48:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:48:34 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.337 | ppl 646.77 | wps 40078.9 | wpb 510.9 | bsz 1 | num_updates 971 | best_loss 9.337
2022-03-06 16:48:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 971 updates
2022-03-06 16:48:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:48:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:48:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 20 @ 971 updates, score 9.337) (writing took 1.9903350621461868 seconds)
2022-03-06 16:48:36 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-06 16:48:36 | INFO | train | epoch 020 | loss 9.027 | ppl 521.52 | wps 21464.9 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 971 | lr 0.000121451 | gnorm 0.802 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 2915
2022-03-06 16:48:36 | INFO | fairseq.trainer | begin training epoch 21
2022-03-06 16:48:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:49:59 | INFO | train_inner | epoch 021:     29 / 49 loss=9.019, ppl=518.93, wps=21719.5, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=1000, lr=0.000125075, gnorm=0.827, loss_scale=32, train_wall=261, gb_free=21.5, wall=2998
2022-03-06 16:50:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:50:59 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.261 | ppl 613.67 | wps 40003.5 | wpb 510.9 | bsz 1 | num_updates 1020 | best_loss 9.261
2022-03-06 16:50:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1020 updates
2022-03-06 16:50:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:51:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:51:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 21 @ 1020 updates, score 9.261) (writing took 2.007469962351024 seconds)
2022-03-06 16:51:01 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-06 16:51:01 | INFO | train | epoch 021 | loss 8.927 | ppl 486.71 | wps 21909.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1020 | lr 0.000127575 | gnorm 0.878 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 3060
2022-03-06 16:51:01 | INFO | fairseq.trainer | begin training epoch 22
2022-03-06 16:51:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:53:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 16:53:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:53:24 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.211 | ppl 592.84 | wps 39928.4 | wpb 510.9 | bsz 1 | num_updates 1068 | best_loss 9.211
2022-03-06 16:53:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1068 updates
2022-03-06 16:53:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:53:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:53:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 22 @ 1068 updates, score 9.211) (writing took 1.9204597314819694 seconds)
2022-03-06 16:53:26 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-06 16:53:26 | INFO | train | epoch 022 | loss 8.826 | ppl 453.74 | wps 21460.7 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1068 | lr 0.000133573 | gnorm 0.85 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 3206
2022-03-06 16:53:26 | INFO | fairseq.trainer | begin training epoch 23
2022-03-06 16:53:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:54:57 | INFO | train_inner | epoch 023:     32 / 49 loss=8.816, ppl=450.67, wps=21736.8, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=1100, lr=0.000137573, gnorm=0.856, loss_scale=32, train_wall=261, gb_free=21.5, wall=3296
2022-03-06 16:55:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:55:49 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.163 | ppl 573.39 | wps 40148.3 | wpb 510.9 | bsz 1 | num_updates 1117 | best_loss 9.163
2022-03-06 16:55:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1117 updates
2022-03-06 16:55:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:55:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:55:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 23 @ 1117 updates, score 9.163) (writing took 1.8930290276184678 seconds)
2022-03-06 16:55:51 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-06 16:55:51 | INFO | train | epoch 023 | loss 8.73 | ppl 424.62 | wps 21949.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1117 | lr 0.000139697 | gnorm 0.847 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 3350
2022-03-06 16:55:51 | INFO | fairseq.trainer | begin training epoch 24
2022-03-06 16:55:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:58:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:58:14 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.107 | ppl 551.54 | wps 40010 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 9.107
2022-03-06 16:58:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1166 updates
2022-03-06 16:58:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:58:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 16:58:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 24 @ 1166 updates, score 9.107) (writing took 1.9286019932478666 seconds)
2022-03-06 16:58:16 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-06 16:58:16 | INFO | train | epoch 024 | loss 8.636 | ppl 397.95 | wps 21922.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1166 | lr 0.000145821 | gnorm 0.901 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 3495
2022-03-06 16:58:16 | INFO | fairseq.trainer | begin training epoch 25
2022-03-06 16:58:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:59:53 | INFO | train_inner | epoch 025:     34 / 49 loss=8.617, ppl=392.62, wps=21952.8, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=1200, lr=0.00015007, gnorm=0.845, loss_scale=64, train_wall=258, gb_free=21.5, wall=3592
2022-03-06 17:00:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:00:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:00:39 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.056 | ppl 532.38 | wps 40198.8 | wpb 510.9 | bsz 1 | num_updates 1214 | best_loss 9.056
2022-03-06 17:00:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1214 updates
2022-03-06 17:00:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:00:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:00:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 25 @ 1214 updates, score 9.056) (writing took 1.9296800447627902 seconds)
2022-03-06 17:00:41 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-06 17:00:41 | INFO | train | epoch 025 | loss 8.54 | ppl 372.12 | wps 21476.2 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1214 | lr 0.00015182 | gnorm 0.802 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 3640
2022-03-06 17:00:41 | INFO | fairseq.trainer | begin training epoch 26
2022-03-06 17:00:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:02:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:03:04 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9 | ppl 512.18 | wps 40086.7 | wpb 510.9 | bsz 1 | num_updates 1263 | best_loss 9
2022-03-06 17:03:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1263 updates
2022-03-06 17:03:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:03:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:03:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 26 @ 1263 updates, score 9.0) (writing took 1.9701926978304982 seconds)
2022-03-06 17:03:06 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-06 17:03:06 | INFO | train | epoch 026 | loss 8.452 | ppl 350.14 | wps 21927 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1263 | lr 0.000157943 | gnorm 0.888 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 3785
2022-03-06 17:03:06 | INFO | fairseq.trainer | begin training epoch 27
2022-03-06 17:03:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:04:51 | INFO | train_inner | epoch 027:     37 / 49 loss=8.43, ppl=344.94, wps=21741.8, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=1300, lr=0.000162568, gnorm=0.898, loss_scale=32, train_wall=261, gb_free=21.5, wall=3890
2022-03-06 17:05:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:05:29 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.961 | ppl 498.27 | wps 40130.9 | wpb 510.9 | bsz 1 | num_updates 1312 | best_loss 8.961
2022-03-06 17:05:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 1312 updates
2022-03-06 17:05:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:05:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:05:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 27 @ 1312 updates, score 8.961) (writing took 1.9273870065808296 seconds)
2022-03-06 17:05:31 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-06 17:05:31 | INFO | train | epoch 027 | loss 8.357 | ppl 327.98 | wps 21926.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1312 | lr 0.000164067 | gnorm 0.893 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 3930
2022-03-06 17:05:31 | INFO | fairseq.trainer | begin training epoch 28
2022-03-06 17:05:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:07:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:07:54 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.927 | ppl 486.69 | wps 40084.9 | wpb 510.9 | bsz 1 | num_updates 1361 | best_loss 8.927
2022-03-06 17:07:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 1361 updates
2022-03-06 17:07:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:07:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:07:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 28 @ 1361 updates, score 8.927) (writing took 1.958280953578651 seconds)
2022-03-06 17:07:56 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-06 17:07:56 | INFO | train | epoch 028 | loss 8.265 | ppl 307.52 | wps 21927.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1361 | lr 0.000170191 | gnorm 0.882 | loss_scale 64 | train_wall 127 | gb_free 21.5 | wall 4075
2022-03-06 17:07:56 | INFO | fairseq.trainer | begin training epoch 29
2022-03-06 17:07:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:08:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:09:49 | INFO | train_inner | epoch 029:     40 / 49 loss=8.237, ppl=301.72, wps=21742.7, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=1400, lr=0.000175065, gnorm=0.883, loss_scale=32, train_wall=261, gb_free=21.5, wall=4188
2022-03-06 17:10:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:10:19 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.9 | ppl 477.57 | wps 39993.9 | wpb 510.9 | bsz 1 | num_updates 1409 | best_loss 8.9
2022-03-06 17:10:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 1409 updates
2022-03-06 17:10:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:10:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:10:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 29 @ 1409 updates, score 8.9) (writing took 1.9839030299335718 seconds)
2022-03-06 17:10:21 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-06 17:10:21 | INFO | train | epoch 029 | loss 8.171 | ppl 288.23 | wps 21467.4 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1409 | lr 0.00017619 | gnorm 0.904 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 4220
2022-03-06 17:10:21 | INFO | fairseq.trainer | begin training epoch 30
2022-03-06 17:10:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:12:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:12:44 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.866 | ppl 466.69 | wps 39868.4 | wpb 510.9 | bsz 1 | num_updates 1458 | best_loss 8.866
2022-03-06 17:12:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 1458 updates
2022-03-06 17:12:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:12:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:12:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 30 @ 1458 updates, score 8.866) (writing took 1.9138984121382236 seconds)
2022-03-06 17:12:46 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-06 17:12:46 | INFO | train | epoch 030 | loss 8.081 | ppl 270.73 | wps 21915.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1458 | lr 0.000182314 | gnorm 0.914 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 4365
2022-03-06 17:12:46 | INFO | fairseq.trainer | begin training epoch 31
2022-03-06 17:12:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:14:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:14:48 | INFO | train_inner | epoch 031:     43 / 49 loss=8.05, ppl=265.04, wps=21740.8, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=1500, lr=0.000187563, gnorm=0.896, loss_scale=32, train_wall=261, gb_free=21.5, wall=4487
2022-03-06 17:15:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:15:09 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.844 | ppl 459.58 | wps 39869.6 | wpb 510.9 | bsz 1 | num_updates 1506 | best_loss 8.844
2022-03-06 17:15:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 1506 updates
2022-03-06 17:15:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:15:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:15:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 31 @ 1506 updates, score 8.844) (writing took 1.9985785726457834 seconds)
2022-03-06 17:15:11 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-06 17:15:11 | INFO | train | epoch 031 | loss 7.989 | ppl 254.02 | wps 21475.1 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1506 | lr 0.000188312 | gnorm 0.886 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 4510
2022-03-06 17:15:11 | INFO | fairseq.trainer | begin training epoch 32
2022-03-06 17:15:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:17:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:17:34 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.813 | ppl 449.85 | wps 39894 | wpb 510.9 | bsz 1 | num_updates 1555 | best_loss 8.813
2022-03-06 17:17:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 1555 updates
2022-03-06 17:17:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:17:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:17:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 32 @ 1555 updates, score 8.813) (writing took 1.90634233225137 seconds)
2022-03-06 17:17:36 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-06 17:17:36 | INFO | train | epoch 032 | loss 7.898 | ppl 238.57 | wps 21917.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1555 | lr 0.000194436 | gnorm 0.93 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 4655
2022-03-06 17:17:36 | INFO | fairseq.trainer | begin training epoch 33
2022-03-06 17:17:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:19:43 | INFO | train_inner | epoch 033:     45 / 49 loss=7.86, ppl=232.28, wps=21939.9, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=1600, lr=0.00020006, gnorm=0.924, loss_scale=32, train_wall=258, gb_free=21.5, wall=4782
2022-03-06 17:19:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:19:59 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.804 | ppl 446.97 | wps 39823.8 | wpb 510.9 | bsz 1 | num_updates 1604 | best_loss 8.804
2022-03-06 17:19:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 1604 updates
2022-03-06 17:19:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:20:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:20:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 33 @ 1604 updates, score 8.804) (writing took 1.93481972720474 seconds)
2022-03-06 17:20:01 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-06 17:20:01 | INFO | train | epoch 033 | loss 7.806 | ppl 223.75 | wps 21923.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1604 | lr 0.00020056 | gnorm 0.911 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 4800
2022-03-06 17:20:01 | INFO | fairseq.trainer | begin training epoch 34
2022-03-06 17:20:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:22:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:22:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:22:24 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.795 | ppl 444.23 | wps 40006.9 | wpb 510.9 | bsz 1 | num_updates 1652 | best_loss 8.795
2022-03-06 17:22:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 1652 updates
2022-03-06 17:22:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:22:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:22:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 34 @ 1652 updates, score 8.795) (writing took 1.9680122369900346 seconds)
2022-03-06 17:22:26 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-06 17:22:26 | INFO | train | epoch 034 | loss 7.715 | ppl 210.18 | wps 21471.3 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1652 | lr 0.000206559 | gnorm 0.946 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 4945
2022-03-06 17:22:26 | INFO | fairseq.trainer | begin training epoch 35
2022-03-06 17:22:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:24:42 | INFO | train_inner | epoch 035:     48 / 49 loss=7.677, ppl=204.61, wps=21736.9, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=1700, lr=0.000212558, gnorm=0.943, loss_scale=32, train_wall=261, gb_free=21.5, wall=5081
2022-03-06 17:24:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:24:49 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.749 | ppl 430.3 | wps 40031.6 | wpb 510.9 | bsz 1 | num_updates 1701 | best_loss 8.749
2022-03-06 17:24:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 1701 updates
2022-03-06 17:24:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:24:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:24:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 35 @ 1701 updates, score 8.749) (writing took 1.902132778428495 seconds)
2022-03-06 17:24:51 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-06 17:24:51 | INFO | train | epoch 035 | loss 7.629 | ppl 197.93 | wps 21933.5 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1701 | lr 0.000212682 | gnorm 0.948 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 5090
2022-03-06 17:24:51 | INFO | fairseq.trainer | begin training epoch 36
2022-03-06 17:24:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:27:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:27:14 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.742 | ppl 428.17 | wps 39843.5 | wpb 510.9 | bsz 1 | num_updates 1750 | best_loss 8.742
2022-03-06 17:27:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 1750 updates
2022-03-06 17:27:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:27:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:27:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 36 @ 1750 updates, score 8.742) (writing took 1.942563533782959 seconds)
2022-03-06 17:27:16 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-06 17:27:16 | INFO | train | epoch 036 | loss 7.541 | ppl 186.3 | wps 21911.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1750 | lr 0.000218806 | gnorm 0.984 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 5235
2022-03-06 17:27:16 | INFO | fairseq.trainer | begin training epoch 37
2022-03-06 17:27:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:28:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:29:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:29:39 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.736 | ppl 426.37 | wps 40169.5 | wpb 510.9 | bsz 1 | num_updates 1798 | best_loss 8.736
2022-03-06 17:29:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 1798 updates
2022-03-06 17:29:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:29:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:29:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 37 @ 1798 updates, score 8.736) (writing took 1.9112260723486543 seconds)
2022-03-06 17:29:41 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-06 17:29:41 | INFO | train | epoch 037 | loss 7.453 | ppl 175.18 | wps 21470.5 | ups 0.33 | wpb 64844.1 | bsz 126.7 | num_updates 1798 | lr 0.000224805 | gnorm 0.964 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 5380
2022-03-06 17:29:41 | INFO | fairseq.trainer | begin training epoch 38
2022-03-06 17:29:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:29:46 | INFO | train_inner | epoch 038:      2 / 49 loss=7.496, ppl=180.49, wps=21186.9, ups=0.33, wpb=64544.1, bsz=126.1, num_updates=1800, lr=0.000225055, gnorm=0.97, loss_scale=32, train_wall=260, gb_free=21.5, wall=5386
2022-03-06 17:31:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:32:04 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.715 | ppl 420.31 | wps 40062.5 | wpb 510.9 | bsz 1 | num_updates 1847 | best_loss 8.715
2022-03-06 17:32:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 1847 updates
2022-03-06 17:32:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:32:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt
2022-03-06 17:32:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.03125-jelinek_0.04_0.04_0.92_#1/checkpoint_best.pt (epoch 38 @ 1847 updates, score 8.715) (writing took 1.8873844454064965 seconds)
2022-03-06 17:32:06 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-06 17:32:06 | INFO | train | epoch 038 | loss 7.367 | ppl 165.1 | wps 21932 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1847 | lr 0.000230929 | gnorm 0.93 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 5525
2022-03-06 17:32:06 | INFO | fairseq.trainer | begin training epoch 39
2022-03-06 17:32:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:34:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:34:29 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.733 | ppl 425.57 | wps 40073 | wpb 510.9 | bsz 1 | num_updates 1896 | best_loss 8.715
2022-03-06 17:34:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 1896 updates
2022-03-06 17:34:29 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-06 17:34:29 | INFO | train | epoch 039 | loss 7.281 | ppl 155.55 | wps 22204.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1896 | lr 0.000237053 | gnorm 0.936 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 5668
2022-03-06 17:34:29 | INFO | fairseq.trainer | begin training epoch 40
2022-03-06 17:34:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:34:40 | INFO | train_inner | epoch 040:      4 / 49 loss=7.319, ppl=159.69, wps=22089.8, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=1900, lr=0.000237553, gnorm=0.945, loss_scale=32, train_wall=259, gb_free=21.5, wall=5679
2022-03-06 17:35:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:36:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:36:52 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.74 | ppl 427.49 | wps 40053.1 | wpb 510.9 | bsz 1 | num_updates 1944 | best_loss 8.715
2022-03-06 17:36:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 1944 updates
2022-03-06 17:36:52 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-06 17:36:52 | INFO | train | epoch 040 | loss 7.197 | ppl 146.68 | wps 21755.2 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 1944 | lr 0.000243051 | gnorm 0.994 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 5811
2022-03-06 17:36:52 | INFO | fairseq.trainer | begin training epoch 41
2022-03-06 17:36:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:39:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:39:15 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.757 | ppl 432.75 | wps 39956.8 | wpb 510.9 | bsz 1 | num_updates 1993 | best_loss 8.715
2022-03-06 17:39:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 1993 updates
2022-03-06 17:39:15 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-06 17:39:15 | INFO | train | epoch 041 | loss 7.113 | ppl 138.47 | wps 22215.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 1993 | lr 0.000249175 | gnorm 0.988 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 5954
2022-03-06 17:39:15 | INFO | fairseq.trainer | begin training epoch 42
2022-03-06 17:39:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:39:35 | INFO | train_inner | epoch 042:      7 / 49 loss=7.142, ppl=141.23, wps=22020.3, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=2000, lr=0.00025005, gnorm=0.982, loss_scale=32, train_wall=261, gb_free=21.5, wall=5974
2022-03-06 17:41:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:41:38 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.769 | ppl 436.13 | wps 39951 | wpb 510.9 | bsz 1 | num_updates 2042 | best_loss 8.715
2022-03-06 17:41:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 2042 updates
2022-03-06 17:41:38 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-06 17:41:38 | INFO | train | epoch 042 | loss 7.03 | ppl 130.73 | wps 22220.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2042 | lr 0.000255299 | gnorm 0.981 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 6097
2022-03-06 17:41:38 | INFO | fairseq.trainer | begin training epoch 43
2022-03-06 17:41:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:42:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:42:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:43:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:44:01 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.786 | ppl 441.28 | wps 39851.9 | wpb 510.9 | bsz 1 | num_updates 2089 | best_loss 8.715
2022-03-06 17:44:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 2089 updates
2022-03-06 17:44:01 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-06 17:44:01 | INFO | train | epoch 043 | loss 6.953 | ppl 123.9 | wps 21289.7 | ups 0.33 | wpb 64829.4 | bsz 126.6 | num_updates 2089 | lr 0.000261173 | gnorm 1.043 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 6240
2022-03-06 17:44:01 | INFO | fairseq.trainer | begin training epoch 44
2022-03-06 17:44:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:44:32 | INFO | train_inner | epoch 044:     11 / 49 loss=6.977, ppl=125.97, wps=21806.4, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=2100, lr=0.000262548, gnorm=1.009, loss_scale=16, train_wall=264, gb_free=21.5, wall=6271
2022-03-06 17:46:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:46:24 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.813 | ppl 449.62 | wps 40015.5 | wpb 510.9 | bsz 1 | num_updates 2138 | best_loss 8.715
2022-03-06 17:46:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 2138 updates
2022-03-06 17:46:24 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-06 17:46:24 | INFO | train | epoch 044 | loss 6.869 | ppl 116.89 | wps 22226.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2138 | lr 0.000267297 | gnorm 1.018 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 6383
2022-03-06 17:46:24 | INFO | fairseq.trainer | begin training epoch 45
2022-03-06 17:46:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:48:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:48:47 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.842 | ppl 458.81 | wps 39914 | wpb 510.9 | bsz 1 | num_updates 2187 | best_loss 8.715
2022-03-06 17:48:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 2187 updates
2022-03-06 17:48:47 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-06 17:48:47 | INFO | train | epoch 045 | loss 6.795 | ppl 111.03 | wps 22217.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2187 | lr 0.00027342 | gnorm 1.091 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 6526
2022-03-06 17:48:47 | INFO | fairseq.trainer | begin training epoch 46
2022-03-06 17:48:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:49:24 | INFO | train_inner | epoch 046:     13 / 49 loss=6.811, ppl=112.28, wps=22240.2, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=2200, lr=0.000275045, gnorm=1.041, loss_scale=32, train_wall=258, gb_free=21.5, wall=6563
2022-03-06 17:51:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:51:10 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.861 | ppl 465.1 | wps 39922.1 | wpb 510.9 | bsz 1 | num_updates 2236 | best_loss 8.715
2022-03-06 17:51:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 2236 updates
2022-03-06 17:51:10 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-06 17:51:10 | INFO | train | epoch 046 | loss 6.703 | ppl 104.15 | wps 22218.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2236 | lr 0.000279544 | gnorm 0.965 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 6669
2022-03-06 17:51:10 | INFO | fairseq.trainer | begin training epoch 47
2022-03-06 17:51:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:53:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:53:33 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.919 | ppl 484.08 | wps 39893 | wpb 510.9 | bsz 1 | num_updates 2285 | best_loss 8.715
2022-03-06 17:53:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 2285 updates
2022-03-06 17:53:33 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-06 17:53:33 | INFO | train | epoch 047 | loss 6.625 | ppl 98.7 | wps 22233 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2285 | lr 0.000285668 | gnorm 1.069 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 6812
2022-03-06 17:53:33 | INFO | fairseq.trainer | begin training epoch 48
2022-03-06 17:53:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:54:15 | INFO | train_inner | epoch 048:     15 / 49 loss=6.637, ppl=99.52, wps=22244.5, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=2300, lr=0.000287543, gnorm=1.04, loss_scale=32, train_wall=258, gb_free=21.5, wall=6855
2022-03-06 17:55:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:55:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:55:56 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.958 | ppl 497.19 | wps 40015.5 | wpb 510.9 | bsz 1 | num_updates 2333 | best_loss 8.715
2022-03-06 17:55:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 2333 updates
2022-03-06 17:55:56 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-06 17:55:56 | INFO | train | epoch 048 | loss 6.542 | ppl 93.18 | wps 21775.8 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 2333 | lr 0.000291667 | gnorm 1.064 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 6955
2022-03-06 17:55:56 | INFO | fairseq.trainer | begin training epoch 49
2022-03-06 17:55:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:58:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:58:19 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.985 | ppl 506.75 | wps 40011.9 | wpb 510.9 | bsz 1 | num_updates 2382 | best_loss 8.715
2022-03-06 17:58:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 2382 updates
2022-03-06 17:58:19 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-06 17:58:19 | INFO | train | epoch 049 | loss 6.465 | ppl 88.33 | wps 22236.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2382 | lr 0.00029779 | gnorm 1.093 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 7098
2022-03-06 17:58:19 | INFO | fairseq.trainer | begin training epoch 50
2022-03-06 17:58:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:59:10 | INFO | train_inner | epoch 050:     18 / 49 loss=6.477, ppl=89.06, wps=22031, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=2400, lr=0.00030004, gnorm=1.106, loss_scale=32, train_wall=261, gb_free=21.5, wall=7149
2022-03-06 18:00:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:00:42 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.049 | ppl 529.87 | wps 40047.4 | wpb 510.9 | bsz 1 | num_updates 2431 | best_loss 8.715
2022-03-06 18:00:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 2431 updates
2022-03-06 18:00:42 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-06 18:00:42 | INFO | train | epoch 050 | loss 6.386 | ppl 83.63 | wps 22202.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2431 | lr 0.000303914 | gnorm 1.092 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 7241
2022-03-06 18:00:42 | INFO | fairseq.trainer | begin training epoch 51
2022-03-06 18:00:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:01:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:02:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:03:05 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.068 | ppl 536.54 | wps 40124.7 | wpb 510.9 | bsz 1 | num_updates 2479 | best_loss 8.715
2022-03-06 18:03:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 2479 updates
2022-03-06 18:03:05 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-06 18:03:05 | INFO | train | epoch 051 | loss 6.31 | ppl 79.35 | wps 21788.3 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 2479 | lr 0.000309913 | gnorm 1.144 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 7384
2022-03-06 18:03:05 | INFO | fairseq.trainer | begin training epoch 52
2022-03-06 18:03:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:04:04 | INFO | train_inner | epoch 052:     21 / 49 loss=6.314, ppl=79.56, wps=22033.5, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=2500, lr=0.000312538, gnorm=1.087, loss_scale=16, train_wall=261, gb_free=21.5, wall=7444
2022-03-06 18:05:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:05:28 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.089 | ppl 544.64 | wps 40121.9 | wpb 510.9 | bsz 1 | num_updates 2528 | best_loss 8.715
2022-03-06 18:05:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 2528 updates
2022-03-06 18:05:28 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-06 18:05:28 | INFO | train | epoch 052 | loss 6.227 | ppl 74.9 | wps 22216.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2528 | lr 0.000316037 | gnorm 1.103 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 7527
2022-03-06 18:05:28 | INFO | fairseq.trainer | begin training epoch 53
2022-03-06 18:05:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:07:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:07:51 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.115 | ppl 554.3 | wps 40091.3 | wpb 510.9 | bsz 1 | num_updates 2577 | best_loss 8.715
2022-03-06 18:07:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 2577 updates
2022-03-06 18:07:51 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-06 18:07:51 | INFO | train | epoch 053 | loss 6.153 | ppl 71.15 | wps 22227.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2577 | lr 0.000322161 | gnorm 1.176 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 7670
2022-03-06 18:07:51 | INFO | fairseq.trainer | begin training epoch 54
2022-03-06 18:07:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:08:56 | INFO | train_inner | epoch 054:     23 / 49 loss=6.154, ppl=71.21, wps=22242.6, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=2600, lr=0.000325035, gnorm=1.15, loss_scale=32, train_wall=258, gb_free=21.5, wall=7735
2022-03-06 18:10:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:10:14 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.145 | ppl 566.18 | wps 40269.8 | wpb 510.9 | bsz 1 | num_updates 2626 | best_loss 8.715
2022-03-06 18:10:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 2626 updates
2022-03-06 18:10:14 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-06 18:10:14 | INFO | train | epoch 054 | loss 6.075 | ppl 67.4 | wps 22231.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2626 | lr 0.000328284 | gnorm 1.187 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 7813
2022-03-06 18:10:14 | INFO | fairseq.trainer | begin training epoch 55
2022-03-06 18:10:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:11:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:12:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:12:37 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.212 | ppl 593.05 | wps 40117.3 | wpb 510.9 | bsz 1 | num_updates 2674 | best_loss 8.715
2022-03-06 18:12:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 2674 updates
2022-03-06 18:12:37 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-06 18:12:37 | INFO | train | epoch 055 | loss 6.014 | ppl 64.63 | wps 21752.2 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 2674 | lr 0.000334283 | gnorm 1.336 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 7956
2022-03-06 18:12:37 | INFO | fairseq.trainer | begin training epoch 56
2022-03-06 18:12:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:13:51 | INFO | train_inner | epoch 056:     26 / 49 loss=6.001, ppl=64.04, wps=22026.2, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=2700, lr=0.000337533, gnorm=1.198, loss_scale=16, train_wall=261, gb_free=21.5, wall=8030
2022-03-06 18:14:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:15:00 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.26 | ppl 612.9 | wps 39588.1 | wpb 510.9 | bsz 1 | num_updates 2723 | best_loss 8.715
2022-03-06 18:15:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 2723 updates
2022-03-06 18:15:00 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-06 18:15:00 | INFO | train | epoch 056 | loss 5.917 | ppl 60.41 | wps 22219.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2723 | lr 0.000340407 | gnorm 1.071 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 8099
2022-03-06 18:15:00 | INFO | fairseq.trainer | begin training epoch 57
2022-03-06 18:15:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:17:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:17:23 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.259 | ppl 612.75 | wps 40078.4 | wpb 510.9 | bsz 1 | num_updates 2772 | best_loss 8.715
2022-03-06 18:17:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 2772 updates
2022-03-06 18:17:23 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-06 18:17:23 | INFO | train | epoch 057 | loss 5.841 | ppl 57.33 | wps 22229 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2772 | lr 0.000346531 | gnorm 1.13 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 8242
2022-03-06 18:17:23 | INFO | fairseq.trainer | begin training epoch 58
2022-03-06 18:17:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:18:42 | INFO | train_inner | epoch 058:     28 / 49 loss=5.843, ppl=57.39, wps=22237.3, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=2800, lr=0.00035003, gnorm=1.21, loss_scale=32, train_wall=258, gb_free=21.5, wall=8321
2022-03-06 18:19:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:19:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:19:46 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.298 | ppl 629.63 | wps 40084.5 | wpb 510.9 | bsz 1 | num_updates 2820 | best_loss 8.715
2022-03-06 18:19:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 2820 updates
2022-03-06 18:19:46 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-06 18:19:46 | INFO | train | epoch 058 | loss 5.775 | ppl 54.77 | wps 21764.4 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 2820 | lr 0.00035253 | gnorm 1.332 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 8385
2022-03-06 18:19:46 | INFO | fairseq.trainer | begin training epoch 59
2022-03-06 18:19:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:22:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:22:09 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.326 | ppl 641.91 | wps 39593.7 | wpb 510.9 | bsz 1 | num_updates 2869 | best_loss 8.715
2022-03-06 18:22:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 2869 updates
2022-03-06 18:22:09 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-06 18:22:09 | INFO | train | epoch 059 | loss 5.698 | ppl 51.9 | wps 22197.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2869 | lr 0.000358653 | gnorm 1.195 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 8528
2022-03-06 18:22:09 | INFO | fairseq.trainer | begin training epoch 60
2022-03-06 18:22:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:23:37 | INFO | train_inner | epoch 060:     31 / 49 loss=5.688, ppl=51.56, wps=22019.6, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=2900, lr=0.000362528, gnorm=1.217, loss_scale=16, train_wall=261, gb_free=21.5, wall=8616
2022-03-06 18:24:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:24:32 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.387 | ppl 669.74 | wps 39926.3 | wpb 510.9 | bsz 1 | num_updates 2918 | best_loss 8.715
2022-03-06 18:24:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 2918 updates
2022-03-06 18:24:32 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-06 18:24:32 | INFO | train | epoch 060 | loss 5.613 | ppl 48.93 | wps 22218.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2918 | lr 0.000364777 | gnorm 1.154 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 8671
2022-03-06 18:24:32 | INFO | fairseq.trainer | begin training epoch 61
2022-03-06 18:24:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:26:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:26:55 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.406 | ppl 678.35 | wps 40052.1 | wpb 510.9 | bsz 1 | num_updates 2967 | best_loss 8.715
2022-03-06 18:26:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 2967 updates
2022-03-06 18:26:55 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-06 18:26:55 | INFO | train | epoch 061 | loss 5.545 | ppl 46.69 | wps 22209.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 2967 | lr 0.000370901 | gnorm 1.273 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 8814
2022-03-06 18:26:55 | INFO | fairseq.trainer | begin training epoch 62
2022-03-06 18:26:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:27:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:28:32 | INFO | train_inner | epoch 062:     34 / 49 loss=5.534, ppl=46.33, wps=22017.5, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=3000, lr=0.000375025, gnorm=1.303, loss_scale=16, train_wall=261, gb_free=21.5, wall=8911
2022-03-06 18:29:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:29:18 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.505 | ppl 726.7 | wps 39888.8 | wpb 510.9 | bsz 1 | num_updates 3015 | best_loss 8.715
2022-03-06 18:29:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 3015 updates
2022-03-06 18:29:18 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-06 18:29:18 | INFO | train | epoch 062 | loss 5.474 | ppl 44.44 | wps 21768.1 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 3015 | lr 0.0003769 | gnorm 1.302 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 8957
2022-03-06 18:29:18 | INFO | fairseq.trainer | begin training epoch 63
2022-03-06 18:29:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:31:41 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.531 | ppl 739.9 | wps 40095.3 | wpb 510.9 | bsz 1 | num_updates 3064 | best_loss 8.715
2022-03-06 18:31:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 3064 updates
2022-03-06 18:31:41 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-06 18:31:41 | INFO | train | epoch 063 | loss 5.394 | ppl 42.05 | wps 22230 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3064 | lr 0.000383023 | gnorm 1.24 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 9100
2022-03-06 18:31:41 | INFO | fairseq.trainer | begin training epoch 64
2022-03-06 18:31:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:33:23 | INFO | train_inner | epoch 064:     36 / 49 loss=5.381, ppl=41.68, wps=22246.1, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=3100, lr=0.000387523, gnorm=1.271, loss_scale=16, train_wall=258, gb_free=21.5, wall=9202
2022-03-06 18:33:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:34:04 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.578 | ppl 764.05 | wps 39813.8 | wpb 510.9 | bsz 1 | num_updates 3113 | best_loss 8.715
2022-03-06 18:34:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 3113 updates
2022-03-06 18:34:04 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-06 18:34:04 | INFO | train | epoch 064 | loss 5.328 | ppl 40.16 | wps 22216 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3113 | lr 0.000389147 | gnorm 1.326 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 9243
2022-03-06 18:34:04 | INFO | fairseq.trainer | begin training epoch 65
2022-03-06 18:34:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:34:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:36:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:36:27 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.627 | ppl 790.48 | wps 39941.5 | wpb 510.9 | bsz 1 | num_updates 3161 | best_loss 8.715
2022-03-06 18:36:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 3161 updates
2022-03-06 18:36:27 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-06 18:36:27 | INFO | train | epoch 065 | loss 5.244 | ppl 37.9 | wps 21769.3 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 3161 | lr 0.000395146 | gnorm 1.256 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 9386
2022-03-06 18:36:27 | INFO | fairseq.trainer | begin training epoch 66
2022-03-06 18:36:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:38:18 | INFO | train_inner | epoch 066:     39 / 49 loss=5.229, ppl=37.51, wps=22021.1, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=3200, lr=0.00040002, gnorm=1.279, loss_scale=16, train_wall=261, gb_free=21.5, wall=9497
2022-03-06 18:38:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:38:50 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.645 | ppl 800.9 | wps 40004.5 | wpb 510.9 | bsz 1 | num_updates 3210 | best_loss 8.715
2022-03-06 18:38:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 3210 updates
2022-03-06 18:38:50 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-06 18:38:50 | INFO | train | epoch 066 | loss 5.181 | ppl 36.28 | wps 22216.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3210 | lr 0.00040127 | gnorm 1.37 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 9529
2022-03-06 18:38:50 | INFO | fairseq.trainer | begin training epoch 67
2022-03-06 18:38:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:40:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:41:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:41:13 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.684 | ppl 822.82 | wps 40053.9 | wpb 510.9 | bsz 1 | num_updates 3258 | best_loss 8.715
2022-03-06 18:41:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 3258 updates
2022-03-06 18:41:13 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-06 18:41:13 | INFO | train | epoch 067 | loss 5.113 | ppl 34.61 | wps 21785.2 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 3258 | lr 0.000407269 | gnorm 1.391 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 9672
2022-03-06 18:41:13 | INFO | fairseq.trainer | begin training epoch 68
2022-03-06 18:41:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:43:12 | INFO | train_inner | epoch 068:     42 / 49 loss=5.085, ppl=33.93, wps=22036.7, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=3300, lr=0.000412518, gnorm=1.276, loss_scale=8, train_wall=261, gb_free=21.5, wall=9791
2022-03-06 18:43:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:43:36 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.761 | ppl 867.53 | wps 39780.6 | wpb 510.9 | bsz 1 | num_updates 3307 | best_loss 8.715
2022-03-06 18:43:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 3307 updates
2022-03-06 18:43:36 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-06 18:43:36 | INFO | train | epoch 068 | loss 5.033 | ppl 32.74 | wps 22225.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3307 | lr 0.000413392 | gnorm 1.205 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 9815
2022-03-06 18:43:36 | INFO | fairseq.trainer | begin training epoch 69
2022-03-06 18:43:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:45:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:45:59 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.863 | ppl 931.4 | wps 40014 | wpb 510.9 | bsz 1 | num_updates 3356 | best_loss 8.715
2022-03-06 18:45:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 3356 updates
2022-03-06 18:45:59 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-06 18:45:59 | INFO | train | epoch 069 | loss 4.973 | ppl 31.4 | wps 22214.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3356 | lr 0.000419516 | gnorm 1.347 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 9958
2022-03-06 18:45:59 | INFO | fairseq.trainer | begin training epoch 70
2022-03-06 18:45:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:48:04 | INFO | train_inner | epoch 070:     44 / 49 loss=4.949, ppl=30.89, wps=22239.4, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=3400, lr=0.000425015, gnorm=1.368, loss_scale=16, train_wall=258, gb_free=21.5, wall=10083
2022-03-06 18:48:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:48:22 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.827 | ppl 908 | wps 40061 | wpb 510.9 | bsz 1 | num_updates 3405 | best_loss 8.715
2022-03-06 18:48:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 3405 updates
2022-03-06 18:48:22 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-06 18:48:22 | INFO | train | epoch 070 | loss 4.906 | ppl 29.99 | wps 22235.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3405 | lr 0.00042564 | gnorm 1.318 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 10101
2022-03-06 18:48:22 | INFO | fairseq.trainer | begin training epoch 71
2022-03-06 18:48:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:50:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:50:45 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.904 | ppl 957.78 | wps 40080.5 | wpb 510.9 | bsz 1 | num_updates 3454 | best_loss 8.715
2022-03-06 18:50:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 3454 updates
2022-03-06 18:50:45 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-06 18:50:45 | INFO | train | epoch 071 | loss 4.834 | ppl 28.51 | wps 22220.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3454 | lr 0.000431764 | gnorm 1.419 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 10244
2022-03-06 18:50:45 | INFO | fairseq.trainer | begin training epoch 72
2022-03-06 18:50:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:52:55 | INFO | train_inner | epoch 072:     46 / 49 loss=4.811, ppl=28.07, wps=22245.6, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=3500, lr=0.000437513, gnorm=1.409, loss_scale=16, train_wall=258, gb_free=21.5, wall=10375
2022-03-06 18:53:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:53:08 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.94 | ppl 982.01 | wps 39090.1 | wpb 510.9 | bsz 1 | num_updates 3503 | best_loss 8.715
2022-03-06 18:53:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 3503 updates
2022-03-06 18:53:08 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-06 18:53:08 | INFO | train | epoch 072 | loss 4.772 | ppl 27.33 | wps 22213.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3503 | lr 0.000437887 | gnorm 1.411 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 10387
2022-03-06 18:53:08 | INFO | fairseq.trainer | begin training epoch 73
2022-03-06 18:53:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:54:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:55:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:55:31 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 10.024 | ppl 1041.22 | wps 39929.4 | wpb 510.9 | bsz 1 | num_updates 3551 | best_loss 8.715
2022-03-06 18:55:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 3551 updates
2022-03-06 18:55:31 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-06 18:55:31 | INFO | train | epoch 073 | loss 4.689 | ppl 25.8 | wps 21773.1 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 3551 | lr 0.000443886 | gnorm 1.259 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 10530
2022-03-06 18:55:31 | INFO | fairseq.trainer | begin training epoch 74
2022-03-06 18:55:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:57:49 | INFO | train_inner | epoch 074:     49 / 49 loss=4.668, ppl=25.43, wps=22016.5, ups=0.34, wpb=64544.1, bsz=126.1, num_updates=3600, lr=0.00045001, gnorm=1.332, loss_scale=16, train_wall=260, gb_free=21.5, wall=10668
2022-03-06 18:57:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:57:54 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 10.034 | ppl 1048.39 | wps 39941.5 | wpb 510.9 | bsz 1 | num_updates 3600 | best_loss 8.715
2022-03-06 18:57:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 3600 updates
2022-03-06 18:57:54 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-06 18:57:54 | INFO | train | epoch 074 | loss 4.642 | ppl 24.97 | wps 22224.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3600 | lr 0.00045001 | gnorm 1.409 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 10673
2022-03-06 18:57:54 | INFO | fairseq.trainer | begin training epoch 75
2022-03-06 18:57:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:59:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:00:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:00:17 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 10.126 | ppl 1117.62 | wps 40169.5 | wpb 510.9 | bsz 1 | num_updates 3648 | best_loss 8.715
2022-03-06 19:00:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 3648 updates
2022-03-06 19:00:17 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-06 19:00:17 | INFO | train | epoch 075 | loss 4.568 | ppl 23.71 | wps 21788.2 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 3648 | lr 0.000456009 | gnorm 1.342 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 10816
2022-03-06 19:00:17 | INFO | fairseq.trainer | begin training epoch 76
2022-03-06 19:00:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:02:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:02:40 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 10.17 | ppl 1152.41 | wps 40076.9 | wpb 510.9 | bsz 1 | num_updates 3697 | best_loss 8.715
2022-03-06 19:02:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 3697 updates
2022-03-06 19:02:40 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-06 19:02:40 | INFO | train | epoch 076 | loss 4.512 | ppl 22.82 | wps 22219.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3697 | lr 0.000462133 | gnorm 1.414 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 10959
2022-03-06 19:02:40 | INFO | fairseq.trainer | begin training epoch 77
2022-03-06 19:02:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:02:49 | INFO | train_inner | epoch 077:      3 / 49 loss=4.535, ppl=23.18, wps=21626.3, ups=0.33, wpb=64871.8, bsz=126.7, num_updates=3700, lr=0.000462508, gnorm=1.375, loss_scale=8, train_wall=261, gb_free=21.5, wall=10968
2022-03-06 19:04:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:05:03 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 10.197 | ppl 1173.61 | wps 40021.3 | wpb 510.9 | bsz 1 | num_updates 3746 | best_loss 8.715
2022-03-06 19:05:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 3746 updates
2022-03-06 19:05:03 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-06 19:05:03 | INFO | train | epoch 077 | loss 4.438 | ppl 21.68 | wps 22230.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3746 | lr 0.000468256 | gnorm 1.329 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 11102
2022-03-06 19:05:03 | INFO | fairseq.trainer | begin training epoch 78
2022-03-06 19:05:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:07:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:07:26 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 10.245 | ppl 1213.41 | wps 39929.2 | wpb 510.9 | bsz 1 | num_updates 3795 | best_loss 8.715
2022-03-06 19:07:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 3795 updates
2022-03-06 19:07:26 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-06 19:07:26 | INFO | train | epoch 078 | loss 4.382 | ppl 20.85 | wps 22222.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3795 | lr 0.00047438 | gnorm 1.368 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 11245
2022-03-06 19:07:26 | INFO | fairseq.trainer | begin training epoch 79
2022-03-06 19:07:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:07:40 | INFO | train_inner | epoch 079:      5 / 49 loss=4.403, ppl=21.16, wps=22239.9, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=3800, lr=0.000475005, gnorm=1.351, loss_scale=16, train_wall=258, gb_free=21.5, wall=11259
2022-03-06 19:09:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:09:49 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 10.31 | ppl 1269.1 | wps 40164.9 | wpb 510.9 | bsz 1 | num_updates 3844 | best_loss 8.715
2022-03-06 19:09:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 3844 updates
2022-03-06 19:09:49 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-06 19:09:49 | INFO | train | epoch 079 | loss 4.333 | ppl 20.16 | wps 22229.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3844 | lr 0.000480504 | gnorm 1.498 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 11388
2022-03-06 19:09:49 | INFO | fairseq.trainer | begin training epoch 80
2022-03-06 19:09:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:12:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:12:12 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 10.366 | ppl 1319.69 | wps 40018.3 | wpb 510.9 | bsz 1 | num_updates 3893 | best_loss 8.715
2022-03-06 19:12:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 3893 updates
2022-03-06 19:12:12 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-06 19:12:12 | INFO | train | epoch 080 | loss 4.257 | ppl 19.12 | wps 22198.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3893 | lr 0.000486628 | gnorm 1.232 | loss_scale 32 | train_wall 127 | gb_free 21.5 | wall 11531
2022-03-06 19:12:12 | INFO | fairseq.trainer | begin training epoch 81
2022-03-06 19:12:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:12:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:12:35 | INFO | train_inner | epoch 081:      8 / 49 loss=4.287, ppl=19.52, wps=22022.1, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=3900, lr=0.000487503, gnorm=1.363, loss_scale=16, train_wall=261, gb_free=21.5, wall=11554
2022-03-06 19:13:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:14:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:14:35 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 10.405 | ppl 1356.08 | wps 40257.9 | wpb 510.9 | bsz 1 | num_updates 3940 | best_loss 8.715
2022-03-06 19:14:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 3940 updates
2022-03-06 19:14:35 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-06 19:14:35 | INFO | train | epoch 081 | loss 4.214 | ppl 18.55 | wps 21318.4 | ups 0.33 | wpb 64829.4 | bsz 126.6 | num_updates 3940 | lr 0.000492502 | gnorm 1.439 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 11674
2022-03-06 19:14:35 | INFO | fairseq.trainer | begin training epoch 82
2022-03-06 19:14:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:16:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:16:58 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 10.479 | ppl 1427.3 | wps 40044.1 | wpb 510.9 | bsz 1 | num_updates 3989 | best_loss 8.715
2022-03-06 19:16:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 3989 updates
2022-03-06 19:16:58 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-06 19:16:58 | INFO | train | epoch 082 | loss 4.152 | ppl 17.78 | wps 22218.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 3989 | lr 0.000498625 | gnorm 1.387 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 11817
2022-03-06 19:16:58 | INFO | fairseq.trainer | begin training epoch 83
2022-03-06 19:16:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:17:29 | INFO | train_inner | epoch 083:     11 / 49 loss=4.169, ppl=17.99, wps=22031, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4000, lr=0.0005, gnorm=1.392, loss_scale=8, train_wall=261, gb_free=21.5, wall=11848
2022-03-06 19:19:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:19:21 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 10.542 | ppl 1490.55 | wps 39863.3 | wpb 510.9 | bsz 1 | num_updates 4038 | best_loss 8.715
2022-03-06 19:19:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 4038 updates
2022-03-06 19:19:21 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-06 19:19:21 | INFO | train | epoch 083 | loss 4.096 | ppl 17.11 | wps 22233.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4038 | lr 0.000497642 | gnorm 1.348 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 11960
2022-03-06 19:19:21 | INFO | fairseq.trainer | begin training epoch 84
2022-03-06 19:19:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:21:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:21:44 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 10.58 | ppl 1530.8 | wps 39876.7 | wpb 510.9 | bsz 1 | num_updates 4087 | best_loss 8.715
2022-03-06 19:21:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 4087 updates
2022-03-06 19:21:44 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-06 19:21:44 | INFO | train | epoch 084 | loss 4.031 | ppl 16.35 | wps 22213.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4087 | lr 0.00049465 | gnorm 1.316 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 12103
2022-03-06 19:21:44 | INFO | fairseq.trainer | begin training epoch 85
2022-03-06 19:21:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:22:21 | INFO | train_inner | epoch 085:     13 / 49 loss=4.048, ppl=16.54, wps=22241.4, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4100, lr=0.000493865, gnorm=1.341, loss_scale=16, train_wall=258, gb_free=21.5, wall=12140
2022-03-06 19:24:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:24:07 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 10.627 | ppl 1581.03 | wps 40161.8 | wpb 510.9 | bsz 1 | num_updates 4136 | best_loss 8.715
2022-03-06 19:24:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 4136 updates
2022-03-06 19:24:07 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-06 19:24:07 | INFO | train | epoch 085 | loss 3.972 | ppl 15.7 | wps 22221.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4136 | lr 0.000491711 | gnorm 1.316 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 12246
2022-03-06 19:24:07 | INFO | fairseq.trainer | begin training epoch 86
2022-03-06 19:24:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:25:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:26:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:26:30 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 10.642 | ppl 1598.11 | wps 40095.7 | wpb 510.9 | bsz 1 | num_updates 4184 | best_loss 8.715
2022-03-06 19:26:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 4184 updates
2022-03-06 19:26:30 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-06 19:26:30 | INFO | train | epoch 086 | loss 3.914 | ppl 15.07 | wps 21784.9 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 4184 | lr 0.000488882 | gnorm 1.278 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 12389
2022-03-06 19:26:30 | INFO | fairseq.trainer | begin training epoch 87
2022-03-06 19:26:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:27:15 | INFO | train_inner | epoch 087:     16 / 49 loss=3.926, ppl=15.2, wps=22032.4, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=4200, lr=0.00048795, gnorm=1.296, loss_scale=16, train_wall=261, gb_free=21.5, wall=12435
2022-03-06 19:28:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:28:53 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 10.706 | ppl 1670.13 | wps 39289.4 | wpb 510.9 | bsz 1 | num_updates 4233 | best_loss 8.715
2022-03-06 19:28:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 4233 updates
2022-03-06 19:28:53 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-06 19:28:53 | INFO | train | epoch 087 | loss 3.864 | ppl 14.56 | wps 22201.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4233 | lr 0.000486044 | gnorm 1.29 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 12532
2022-03-06 19:28:53 | INFO | fairseq.trainer | begin training epoch 88
2022-03-06 19:28:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:31:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:31:16 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 10.793 | ppl 1774.61 | wps 40117.8 | wpb 510.9 | bsz 1 | num_updates 4282 | best_loss 8.715
2022-03-06 19:31:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 4282 updates
2022-03-06 19:31:16 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-06 19:31:16 | INFO | train | epoch 088 | loss 3.81 | ppl 14.03 | wps 22223.5 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4282 | lr 0.000483255 | gnorm 1.276 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 12675
2022-03-06 19:31:16 | INFO | fairseq.trainer | begin training epoch 89
2022-03-06 19:31:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:32:07 | INFO | train_inner | epoch 089:     18 / 49 loss=3.818, ppl=14.11, wps=22236.8, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=4300, lr=0.000482243, gnorm=1.278, loss_scale=32, train_wall=258, gb_free=21.5, wall=12726
2022-03-06 19:32:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:33:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:33:39 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 10.809 | ppl 1794.12 | wps 40076.4 | wpb 510.9 | bsz 1 | num_updates 4330 | best_loss 8.715
2022-03-06 19:33:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 4330 updates
2022-03-06 19:33:39 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-06 19:33:39 | INFO | train | epoch 089 | loss 3.754 | ppl 13.49 | wps 21774.4 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 4330 | lr 0.000480569 | gnorm 1.206 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 12818
2022-03-06 19:33:39 | INFO | fairseq.trainer | begin training epoch 90
2022-03-06 19:33:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:34:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:35:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:36:02 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 10.898 | ppl 1907.79 | wps 39299.1 | wpb 510.9 | bsz 1 | num_updates 4378 | best_loss 8.715
2022-03-06 19:36:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 4378 updates
2022-03-06 19:36:02 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-06 19:36:02 | INFO | train | epoch 090 | loss 3.713 | ppl 13.11 | wps 21752.4 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 4378 | lr 0.000477928 | gnorm 1.279 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 12961
2022-03-06 19:36:02 | INFO | fairseq.trainer | begin training epoch 91
2022-03-06 19:36:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:37:04 | INFO | train_inner | epoch 091:     22 / 49 loss=3.715, ppl=13.13, wps=21816.2, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=4400, lr=0.000476731, gnorm=1.223, loss_scale=8, train_wall=264, gb_free=21.5, wall=13024
2022-03-06 19:38:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:38:25 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 10.924 | ppl 1942.62 | wps 40115.1 | wpb 510.9 | bsz 1 | num_updates 4427 | best_loss 8.715
2022-03-06 19:38:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 4427 updates
2022-03-06 19:38:25 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-06 19:38:25 | INFO | train | epoch 091 | loss 3.663 | ppl 12.67 | wps 22240 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4427 | lr 0.000475275 | gnorm 1.207 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 13104
2022-03-06 19:38:25 | INFO | fairseq.trainer | begin training epoch 92
2022-03-06 19:38:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:40:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:40:48 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 10.929 | ppl 1949.3 | wps 40085.9 | wpb 510.9 | bsz 1 | num_updates 4476 | best_loss 8.715
2022-03-06 19:40:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 4476 updates
2022-03-06 19:40:48 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-06 19:40:48 | INFO | train | epoch 092 | loss 3.617 | ppl 12.27 | wps 22233.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4476 | lr 0.000472667 | gnorm 1.238 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 13247
2022-03-06 19:40:48 | INFO | fairseq.trainer | begin training epoch 93
2022-03-06 19:40:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:41:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:41:59 | INFO | train_inner | epoch 093:     25 / 49 loss=3.618, ppl=12.27, wps=22034.3, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4500, lr=0.000471405, gnorm=1.229, loss_scale=8, train_wall=261, gb_free=21.5, wall=13318
2022-03-06 19:43:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:43:11 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 10.976 | ppl 2014.39 | wps 40077.4 | wpb 510.9 | bsz 1 | num_updates 4524 | best_loss 8.715
2022-03-06 19:43:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 4524 updates
2022-03-06 19:43:11 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-06 19:43:11 | INFO | train | epoch 093 | loss 3.572 | ppl 11.89 | wps 21771.4 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 4524 | lr 0.000470152 | gnorm 1.191 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 13390
2022-03-06 19:43:11 | INFO | fairseq.trainer | begin training epoch 94
2022-03-06 19:43:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:45:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:45:34 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 11.052 | ppl 2123.73 | wps 40121.5 | wpb 510.9 | bsz 1 | num_updates 4573 | best_loss 8.715
2022-03-06 19:45:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 4573 updates
2022-03-06 19:45:34 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-06 19:45:34 | INFO | train | epoch 094 | loss 3.53 | ppl 11.55 | wps 22238.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4573 | lr 0.000467627 | gnorm 1.191 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 13533
2022-03-06 19:45:34 | INFO | fairseq.trainer | begin training epoch 95
2022-03-06 19:45:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:46:50 | INFO | train_inner | epoch 095:     27 / 49 loss=3.528, ppl=11.54, wps=22257.3, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4600, lr=0.000466252, gnorm=1.174, loss_scale=8, train_wall=258, gb_free=21.5, wall=13609
2022-03-06 19:47:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:47:57 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 11.128 | ppl 2237.97 | wps 40095.1 | wpb 510.9 | bsz 1 | num_updates 4622 | best_loss 8.715
2022-03-06 19:47:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 4622 updates
2022-03-06 19:47:57 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-06 19:47:57 | INFO | train | epoch 095 | loss 3.486 | ppl 11.2 | wps 22242.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4622 | lr 0.000465141 | gnorm 1.165 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 13676
2022-03-06 19:47:57 | INFO | fairseq.trainer | begin training epoch 96
2022-03-06 19:47:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:48:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:50:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:50:20 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 11.15 | ppl 2273.13 | wps 40009.9 | wpb 510.9 | bsz 1 | num_updates 4670 | best_loss 8.715
2022-03-06 19:50:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 4670 updates
2022-03-06 19:50:20 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-06 19:50:20 | INFO | train | epoch 096 | loss 3.451 | ppl 10.93 | wps 21789.2 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 4670 | lr 0.000462745 | gnorm 1.211 | loss_scale 8 | train_wall 126 | gb_free 21.5 | wall 13819
2022-03-06 19:50:20 | INFO | fairseq.trainer | begin training epoch 97
2022-03-06 19:50:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:51:45 | INFO | train_inner | epoch 097:     30 / 49 loss=3.445, ppl=10.89, wps=22043.7, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4700, lr=0.000461266, gnorm=1.193, loss_scale=8, train_wall=261, gb_free=21.5, wall=13904
2022-03-06 19:52:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:52:42 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 11.146 | ppl 2265.91 | wps 40206.7 | wpb 510.9 | bsz 1 | num_updates 4719 | best_loss 8.715
2022-03-06 19:52:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 4719 updates
2022-03-06 19:52:43 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-06 19:52:43 | INFO | train | epoch 097 | loss 3.413 | ppl 10.65 | wps 22231.2 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4719 | lr 0.000460336 | gnorm 1.164 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 13962
2022-03-06 19:52:43 | INFO | fairseq.trainer | begin training epoch 98
2022-03-06 19:52:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:55:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:55:06 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 11.189 | ppl 2335.32 | wps 40068.7 | wpb 510.9 | bsz 1 | num_updates 4768 | best_loss 8.715
2022-03-06 19:55:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 4768 updates
2022-03-06 19:55:06 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-06 19:55:06 | INFO | train | epoch 098 | loss 3.375 | ppl 10.38 | wps 22213.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4768 | lr 0.000457965 | gnorm 1.169 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 14105
2022-03-06 19:55:06 | INFO | fairseq.trainer | begin training epoch 99
2022-03-06 19:55:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:56:36 | INFO | train_inner | epoch 099:     32 / 49 loss=3.371, ppl=10.35, wps=22242.7, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4800, lr=0.000456435, gnorm=1.139, loss_scale=16, train_wall=258, gb_free=21.5, wall=14195
2022-03-06 19:57:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:57:29 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 11.242 | ppl 2422.43 | wps 40092.1 | wpb 510.9 | bsz 1 | num_updates 4817 | best_loss 8.715
2022-03-06 19:57:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 4817 updates
2022-03-06 19:57:29 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-06 19:57:29 | INFO | train | epoch 099 | loss 3.333 | ppl 10.08 | wps 22232 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4817 | lr 0.000455629 | gnorm 1.068 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 14248
2022-03-06 19:57:29 | INFO | fairseq.trainer | begin training epoch 100
2022-03-06 19:57:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:58:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:59:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:59:52 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 11.269 | ppl 2467.71 | wps 40233.2 | wpb 510.9 | bsz 1 | num_updates 4865 | best_loss 8.715
2022-03-06 19:59:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 4865 updates
2022-03-06 19:59:52 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-06 19:59:52 | INFO | train | epoch 100 | loss 3.305 | ppl 9.88 | wps 21767.7 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 4865 | lr 0.000453376 | gnorm 1.153 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 14391
2022-03-06 19:59:52 | INFO | fairseq.trainer | begin training epoch 101
2022-03-06 19:59:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:01:31 | INFO | train_inner | epoch 101:     35 / 49 loss=3.297, ppl=9.83, wps=22032.9, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=4900, lr=0.000451754, gnorm=1.119, loss_scale=8, train_wall=261, gb_free=21.5, wall=14490
2022-03-06 20:02:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:02:14 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 11.286 | ppl 2496.42 | wps 40041.2 | wpb 510.9 | bsz 1 | num_updates 4914 | best_loss 8.715
2022-03-06 20:02:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 4914 updates
2022-03-06 20:02:14 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-06 20:02:14 | INFO | train | epoch 101 | loss 3.269 | ppl 9.64 | wps 22232.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4914 | lr 0.00045111 | gnorm 1.087 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 14534
2022-03-06 20:02:14 | INFO | fairseq.trainer | begin training epoch 102
2022-03-06 20:02:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:04:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:04:37 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 11.34 | ppl 2591.84 | wps 40089.5 | wpb 510.9 | bsz 1 | num_updates 4963 | best_loss 8.715
2022-03-06 20:04:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 4963 updates
2022-03-06 20:04:37 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-06 20:04:37 | INFO | train | epoch 102 | loss 3.238 | ppl 9.43 | wps 22233.5 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 4963 | lr 0.000448878 | gnorm 1.111 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 14677
2022-03-06 20:04:37 | INFO | fairseq.trainer | begin training epoch 103
2022-03-06 20:04:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:05:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:06:25 | INFO | train_inner | epoch 103:     38 / 49 loss=3.232, ppl=9.4, wps=22030.8, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=5000, lr=0.000447214, gnorm=1.1, loss_scale=8, train_wall=261, gb_free=21.5, wall=14784
2022-03-06 20:06:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:07:00 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 11.396 | ppl 2694.2 | wps 40089.6 | wpb 510.9 | bsz 1 | num_updates 5011 | best_loss 8.715
2022-03-06 20:07:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 5011 updates
2022-03-06 20:07:00 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-06 20:07:00 | INFO | train | epoch 103 | loss 3.208 | ppl 9.24 | wps 21763 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 5011 | lr 0.000446722 | gnorm 1.098 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 14820
2022-03-06 20:07:00 | INFO | fairseq.trainer | begin training epoch 104
2022-03-06 20:07:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:09:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:09:23 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 11.398 | ppl 2699.54 | wps 40229.8 | wpb 510.9 | bsz 1 | num_updates 5060 | best_loss 8.715
2022-03-06 20:09:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 5060 updates
2022-03-06 20:09:23 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-06 20:09:23 | INFO | train | epoch 104 | loss 3.179 | ppl 9.06 | wps 22227.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5060 | lr 0.000444554 | gnorm 1.078 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 14963
2022-03-06 20:09:23 | INFO | fairseq.trainer | begin training epoch 105
2022-03-06 20:09:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:11:17 | INFO | train_inner | epoch 105:     40 / 49 loss=3.172, ppl=9.01, wps=22245.5, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=5100, lr=0.000442807, gnorm=1.083, loss_scale=8, train_wall=258, gb_free=21.5, wall=15076
2022-03-06 20:11:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:11:46 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 11.431 | ppl 2761.86 | wps 40126 | wpb 510.9 | bsz 1 | num_updates 5109 | best_loss 8.715
2022-03-06 20:11:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 5109 updates
2022-03-06 20:11:46 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-06 20:11:46 | INFO | train | epoch 105 | loss 3.152 | ppl 8.89 | wps 22228.9 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5109 | lr 0.000442417 | gnorm 1.083 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 15106
2022-03-06 20:11:46 | INFO | fairseq.trainer | begin training epoch 106
2022-03-06 20:11:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:12:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:14:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:14:09 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 11.435 | ppl 2769.11 | wps 40140 | wpb 510.9 | bsz 1 | num_updates 5157 | best_loss 8.715
2022-03-06 20:14:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 5157 updates
2022-03-06 20:14:09 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-06 20:14:09 | INFO | train | epoch 106 | loss 3.122 | ppl 8.71 | wps 21775.8 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 5157 | lr 0.000440353 | gnorm 1.068 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 15248
2022-03-06 20:14:09 | INFO | fairseq.trainer | begin training epoch 107
2022-03-06 20:14:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:16:11 | INFO | train_inner | epoch 107:     43 / 49 loss=3.114, ppl=8.66, wps=22036.4, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=5200, lr=0.000438529, gnorm=1.055, loss_scale=8, train_wall=261, gb_free=21.5, wall=15370
2022-03-06 20:16:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:16:32 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 11.483 | ppl 2863.34 | wps 40067 | wpb 510.9 | bsz 1 | num_updates 5206 | best_loss 8.715
2022-03-06 20:16:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 5206 updates
2022-03-06 20:16:32 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-06 20:16:32 | INFO | train | epoch 107 | loss 3.094 | ppl 8.54 | wps 22229.6 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5206 | lr 0.000438276 | gnorm 1.035 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 15391
2022-03-06 20:16:32 | INFO | fairseq.trainer | begin training epoch 108
2022-03-06 20:16:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:18:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:18:55 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 11.504 | ppl 2904.52 | wps 39996.9 | wpb 510.9 | bsz 1 | num_updates 5255 | best_loss 8.715
2022-03-06 20:18:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 5255 updates
2022-03-06 20:18:55 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-06 20:18:55 | INFO | train | epoch 108 | loss 3.072 | ppl 8.41 | wps 22234.4 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5255 | lr 0.000436228 | gnorm 1.065 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 15534
2022-03-06 20:18:55 | INFO | fairseq.trainer | begin training epoch 109
2022-03-06 20:18:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:21:03 | INFO | train_inner | epoch 109:     45 / 49 loss=3.061, ppl=8.35, wps=22248.7, ups=0.34, wpb=64876.2, bsz=126.7, num_updates=5300, lr=0.000434372, gnorm=1.039, loss_scale=16, train_wall=258, gb_free=21.5, wall=15662
2022-03-06 20:21:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:21:18 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 11.529 | ppl 2955.32 | wps 40129.4 | wpb 510.9 | bsz 1 | num_updates 5304 | best_loss 8.715
2022-03-06 20:21:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 5304 updates
2022-03-06 20:21:18 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-06 20:21:18 | INFO | train | epoch 109 | loss 3.043 | ppl 8.24 | wps 22228.5 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5304 | lr 0.000434208 | gnorm 1.009 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 15677
2022-03-06 20:21:18 | INFO | fairseq.trainer | begin training epoch 110
2022-03-06 20:21:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:21:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:23:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:23:41 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 11.553 | ppl 3005.02 | wps 40133.4 | wpb 510.9 | bsz 1 | num_updates 5352 | best_loss 8.715
2022-03-06 20:23:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 5352 updates
2022-03-06 20:23:41 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-06 20:23:41 | INFO | train | epoch 110 | loss 3.019 | ppl 8.11 | wps 21776.4 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 5352 | lr 0.000432257 | gnorm 1.033 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 15820
2022-03-06 20:23:41 | INFO | fairseq.trainer | begin training epoch 111
2022-03-06 20:23:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:25:57 | INFO | train_inner | epoch 111:     48 / 49 loss=3.011, ppl=8.06, wps=22034.9, ups=0.34, wpb=64867.4, bsz=126.7, num_updates=5400, lr=0.000430331, gnorm=1.016, loss_scale=8, train_wall=261, gb_free=21.5, wall=15956
2022-03-06 20:25:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:26:04 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 11.594 | ppl 3090.6 | wps 40000.8 | wpb 510.9 | bsz 1 | num_updates 5401 | best_loss 8.715
2022-03-06 20:26:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 5401 updates
2022-03-06 20:26:04 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-06 20:26:04 | INFO | train | epoch 111 | loss 2.998 | ppl 7.99 | wps 22226.1 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5401 | lr 0.000430292 | gnorm 1.003 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 15963
2022-03-06 20:26:04 | INFO | fairseq.trainer | begin training epoch 112
2022-03-06 20:26:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:28:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:28:27 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 11.599 | ppl 3103.11 | wps 39877.2 | wpb 510.9 | bsz 1 | num_updates 5450 | best_loss 8.715
2022-03-06 20:28:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 5450 updates
2022-03-06 20:28:27 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-06 20:28:27 | INFO | train | epoch 112 | loss 2.978 | ppl 7.88 | wps 22230.3 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5450 | lr 0.000428353 | gnorm 1.009 | loss_scale 16 | train_wall 127 | gb_free 21.5 | wall 16106
2022-03-06 20:28:27 | INFO | fairseq.trainer | begin training epoch 113
2022-03-06 20:28:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:29:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:30:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:30:50 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 11.627 | ppl 3162.52 | wps 40130.2 | wpb 510.9 | bsz 1 | num_updates 5498 | best_loss 8.715
2022-03-06 20:30:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 5498 updates
2022-03-06 20:30:50 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-06 20:30:50 | INFO | train | epoch 113 | loss 2.952 | ppl 7.74 | wps 21770.4 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 5498 | lr 0.000426479 | gnorm 0.975 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 16249
2022-03-06 20:30:50 | INFO | fairseq.trainer | begin training epoch 114
2022-03-06 20:30:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:30:56 | INFO | train_inner | epoch 114:      2 / 49 loss=2.964, ppl=7.8, wps=21613.6, ups=0.33, wpb=64544.1, bsz=126.1, num_updates=5500, lr=0.000426401, gnorm=0.994, loss_scale=8, train_wall=260, gb_free=21.5, wall=16255
2022-03-06 20:33:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:33:13 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 11.651 | ppl 3216.54 | wps 39967.8 | wpb 510.9 | bsz 1 | num_updates 5547 | best_loss 8.715
2022-03-06 20:33:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 5547 updates
2022-03-06 20:33:13 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-06 20:33:13 | INFO | train | epoch 114 | loss 2.932 | ppl 7.63 | wps 22228.8 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5547 | lr 0.000424591 | gnorm 0.98 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 16392
2022-03-06 20:33:13 | INFO | fairseq.trainer | begin training epoch 115
2022-03-06 20:33:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:35:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:35:36 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 11.659 | ppl 3233.14 | wps 40020.7 | wpb 510.9 | bsz 1 | num_updates 5596 | best_loss 8.715
2022-03-06 20:35:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 5596 updates
2022-03-06 20:35:36 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-06 20:35:36 | INFO | train | epoch 115 | loss 2.915 | ppl 7.54 | wps 22217.7 | ups 0.34 | wpb 64858.2 | bsz 126.7 | num_updates 5596 | lr 0.000422728 | gnorm 0.986 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 16535
2022-03-06 20:35:36 | INFO | fairseq.trainer | begin training epoch 116
2022-03-06 20:35:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:35:47 | INFO | train_inner | epoch 116:      4 / 49 loss=2.922, ppl=7.58, wps=22241, ups=0.34, wpb=64871.8, bsz=126.7, num_updates=5600, lr=0.000422577, gnorm=0.983, loss_scale=8, train_wall=258, gb_free=21.5, wall=16547
2022-03-06 20:37:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:37:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:37:59 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 11.638 | ppl 3188.08 | wps 39911.5 | wpb 510.9 | bsz 1 | num_updates 5644 | best_loss 8.715
2022-03-06 20:37:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 5644 updates
2022-03-06 20:37:59 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-06 20:37:59 | INFO | train | epoch 116 | loss 2.893 | ppl 7.43 | wps 21768.9 | ups 0.34 | wpb 64844.1 | bsz 126.7 | num_updates 5644 | lr 0.000420927 | gnorm 0.97 | loss_scale 8 | train_wall 127 | gb_free 21.5 | wall 16678
2022-03-06 20:37:59 | INFO | fairseq.trainer | begin training epoch 117
2022-03-06 20:37:59 | INFO | fairseq_cli.train | Start iterating over samples
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
