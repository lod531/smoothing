Sender: LSF System <lsfadmin@eu-g3-067>
Subject: Job 207133290: <w103_size_0.0625_fp16_label_smoothing_0.02_#1> in cluster <euler> Exited

Job <w103_size_0.0625_fp16_label_smoothing_0.02_#1> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:38:27 2022
Job was executed on host(s) <eu-g3-067>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 11:34:19 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 11:34:19 2022
Terminated at Sat Mar  5 11:25:22 2022
Results reported at Sat Mar  5 11:25:22 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.02 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   85791.41 sec.
    Max Memory :                                 8165 MB
    Average Memory :                             4156.86 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11835.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   85862 sec.
    Turnaround time :                            92815 sec.

The output (if any) follows:

2022-03-04 11:34:27 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.02, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 11:34:27 | INFO | fairseq.tasks.language_modeling | dictionary: 138136 types
2022-03-04 11:34:29 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(138136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=138136, bias=False)
  )
)
2022-03-04 11:34:29 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 11:34:29 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 11:34:29 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-04 11:34:29 | INFO | fairseq_cli.train | num. shared model params: 89,639,936 (num. trained: 89,639,936)
2022-03-04 11:34:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 11:34:29 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.0625/valid
2022-03-04 11:34:32 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 11:34:32 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 11:34:32 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-04 11:34:32 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 11:34:32 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 11:34:32 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 11:34:32 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 11:34:32 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 11:34:32 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 11:34:32 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-raw-size-0.0625/train
2022-03-04 11:34:32 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 11:34:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:34:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 11:34:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:34:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:34:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 11:35:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 11:38:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:39:03 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.543 | nll_loss 14.476 | ppl 22782 | wps 44022.5 | wpb 510.9 | bsz 1 | num_updates 92
2022-03-04 11:39:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 92 updates
2022-03-04 11:39:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:39:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:39:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 1 @ 92 updates, score 14.543) (writing took 5.397765722125769 seconds)
2022-03-04 11:39:08 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 11:39:08 | INFO | train | epoch 001 | loss 15.972 | nll_loss 15.934 | ppl 62589.2 | wps 24083.3 | ups 0.37 | wpb 65489.2 | bsz 127.9 | num_updates 92 | lr 1.15977e-05 | gnorm 3.463 | loss_scale 4 | train_wall 244 | gb_free 21 | wall 276
2022-03-04 11:39:08 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 11:39:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:39:29 | INFO | train_inner | epoch 002:      8 / 97 loss=15.859, nll_loss=15.818, ppl=57782.7, wps=24173, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=100, lr=1.25975e-05, gnorm=3.324, loss_scale=4, train_wall=262, gb_free=21, wall=297
2022-03-04 11:43:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:43:26 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.918 | nll_loss 12.817 | ppl 7214.09 | wps 42662 | wpb 510.9 | bsz 1 | num_updates 189 | best_loss 12.918
2022-03-04 11:43:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 189 updates
2022-03-04 11:43:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:43:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:43:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 2 @ 189 updates, score 12.918) (writing took 5.478020855225623 seconds)
2022-03-04 11:43:32 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 11:43:32 | INFO | train | epoch 002 | loss 13.869 | nll_loss 13.789 | ppl 14153 | wps 24123.6 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 189 | lr 2.37203e-05 | gnorm 1.577 | loss_scale 8 | train_wall 230 | gb_free 21 | wall 540
2022-03-04 11:43:32 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 11:43:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:44:01 | INFO | train_inner | epoch 003:     11 / 97 loss=13.705, nll_loss=13.621, ppl=12596.8, wps=24134.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=200, lr=2.5095e-05, gnorm=1.538, loss_scale=8, train_wall=237, gb_free=21, wall=568
2022-03-04 11:47:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:47:51 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.252 | nll_loss 11.106 | ppl 2203.95 | wps 42389 | wpb 510.9 | bsz 1 | num_updates 286 | best_loss 11.252
2022-03-04 11:47:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 286 updates
2022-03-04 11:47:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:47:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:47:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 3 @ 286 updates, score 11.252) (writing took 5.205602750182152 seconds)
2022-03-04 11:47:56 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 11:47:56 | INFO | train | epoch 003 | loss 12.07 | nll_loss 11.949 | ppl 3952.63 | wps 24044.5 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 286 | lr 3.58429e-05 | gnorm 1.112 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 804
2022-03-04 11:47:56 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 11:47:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:48:33 | INFO | train_inner | epoch 004:     14 / 97 loss=11.855, nll_loss=11.727, ppl=3389.3, wps=24063.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=300, lr=3.75925e-05, gnorm=1.04, loss_scale=16, train_wall=238, gb_free=21, wall=841
2022-03-04 11:52:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:52:15 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.454 | nll_loss 10.272 | ppl 1236.81 | wps 41979.8 | wpb 510.9 | bsz 1 | num_updates 383 | best_loss 10.454
2022-03-04 11:52:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 383 updates
2022-03-04 11:52:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:52:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:52:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 4 @ 383 updates, score 10.454) (writing took 5.120009497739375 seconds)
2022-03-04 11:52:20 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 11:52:20 | INFO | train | epoch 004 | loss 10.77 | nll_loss 10.605 | ppl 1557.8 | wps 24049.9 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 383 | lr 4.79654e-05 | gnorm 0.592 | loss_scale 16 | train_wall 230 | gb_free 21 | wall 1068
2022-03-04 11:52:20 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 11:52:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:53:05 | INFO | train_inner | epoch 005:     17 / 97 loss=10.649, nll_loss=10.479, ppl=1427.12, wps=24078.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=400, lr=5.009e-05, gnorm=0.549, loss_scale=32, train_wall=238, gb_free=21, wall=1113
2022-03-04 11:56:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:56:39 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.087 | nll_loss 9.885 | ppl 945.6 | wps 42488.8 | wpb 510.9 | bsz 1 | num_updates 480 | best_loss 10.087
2022-03-04 11:56:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 480 updates
2022-03-04 11:56:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:56:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 11:56:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 5 @ 480 updates, score 10.087) (writing took 5.818483212031424 seconds)
2022-03-04 11:56:45 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 11:56:45 | INFO | train | epoch 005 | loss 10.228 | nll_loss 10.035 | ppl 1049.07 | wps 23971 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 480 | lr 6.0088e-05 | gnorm 0.49 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 1333
2022-03-04 11:56:45 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 11:56:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:57:38 | INFO | train_inner | epoch 006:     20 / 97 loss=10.159, nll_loss=9.963, ppl=997.84, wps=23995.6, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=500, lr=6.25875e-05, gnorm=0.501, loss_scale=32, train_wall=238, gb_free=21, wall=1386
2022-03-04 11:58:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:00:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:01:05 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.785 | nll_loss 9.57 | ppl 760.31 | wps 42031.2 | wpb 510.9 | bsz 1 | num_updates 576 | best_loss 9.785
2022-03-04 12:01:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 576 updates
2022-03-04 12:01:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:01:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:01:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 6 @ 576 updates, score 9.785) (writing took 5.18343847617507 seconds)
2022-03-04 12:01:10 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 12:01:10 | INFO | train | epoch 006 | loss 9.894 | nll_loss 9.686 | ppl 823.57 | wps 23762.6 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 576 | lr 7.20856e-05 | gnorm 0.557 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 1598
2022-03-04 12:01:10 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 12:01:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:02:13 | INFO | train_inner | epoch 007:     24 / 97 loss=9.822, nll_loss=9.61, ppl=781.48, wps=23816, ups=0.36, wpb=65495, bsz=127.9, num_updates=600, lr=7.5085e-05, gnorm=0.567, loss_scale=32, train_wall=240, gb_free=21, wall=1661
2022-03-04 12:04:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:05:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:05:29 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.531 | nll_loss 9.305 | ppl 632.61 | wps 42396.1 | wpb 510.9 | bsz 1 | num_updates 672 | best_loss 9.531
2022-03-04 12:05:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 672 updates
2022-03-04 12:05:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:05:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:05:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 7 @ 672 updates, score 9.531) (writing took 5.512418673373759 seconds)
2022-03-04 12:05:34 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 12:05:34 | INFO | train | epoch 007 | loss 9.607 | nll_loss 9.387 | ppl 669.67 | wps 23757.6 | ups 0.36 | wpb 65493.3 | bsz 127.9 | num_updates 672 | lr 8.40832e-05 | gnorm 0.637 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 1862
2022-03-04 12:05:34 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 12:05:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:06:48 | INFO | train_inner | epoch 008:     28 / 97 loss=9.536, nll_loss=9.314, ppl=636.58, wps=23809.4, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=700, lr=8.75825e-05, gnorm=0.671, loss_scale=32, train_wall=240, gb_free=21, wall=1936
2022-03-04 12:08:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:09:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:09:53 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.309 | nll_loss 9.075 | ppl 539.43 | wps 42300.7 | wpb 510.9 | bsz 1 | num_updates 768 | best_loss 9.309
2022-03-04 12:09:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 768 updates
2022-03-04 12:09:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:09:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:09:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 8 @ 768 updates, score 9.309) (writing took 5.354879584163427 seconds)
2022-03-04 12:09:59 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 12:09:59 | INFO | train | epoch 008 | loss 9.349 | nll_loss 9.12 | ppl 556.59 | wps 23778 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 768 | lr 9.60808e-05 | gnorm 0.756 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 2127
2022-03-04 12:09:59 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 12:09:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:11:23 | INFO | train_inner | epoch 009:     32 / 97 loss=9.273, nll_loss=9.042, ppl=527.12, wps=23814.3, ups=0.36, wpb=65495, bsz=127.9, num_updates=800, lr=0.00010008, gnorm=0.776, loss_scale=16, train_wall=240, gb_free=21, wall=2211
2022-03-04 12:14:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:14:18 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.094 | nll_loss 8.853 | ppl 462.33 | wps 42269.8 | wpb 510.9 | bsz 1 | num_updates 865 | best_loss 9.094
2022-03-04 12:14:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 865 updates
2022-03-04 12:14:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:14:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:14:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 9 @ 865 updates, score 9.094) (writing took 5.305624731816351 seconds)
2022-03-04 12:14:23 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 12:14:23 | INFO | train | epoch 009 | loss 9.108 | nll_loss 8.87 | ppl 467.99 | wps 24001.2 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 865 | lr 0.000108203 | gnorm 0.823 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 2391
2022-03-04 12:14:23 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 12:14:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:15:55 | INFO | train_inner | epoch 010:     35 / 97 loss=9.024, nll_loss=8.784, ppl=440.82, wps=24036.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=900, lr=0.000112578, gnorm=0.855, loss_scale=32, train_wall=238, gb_free=21, wall=2483
2022-03-04 12:18:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:18:43 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.915 | nll_loss 8.67 | ppl 407.35 | wps 42199.9 | wpb 510.9 | bsz 1 | num_updates 962 | best_loss 8.915
2022-03-04 12:18:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 962 updates
2022-03-04 12:18:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:18:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:18:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 10 @ 962 updates, score 8.915) (writing took 5.448337476700544 seconds)
2022-03-04 12:18:48 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 12:18:48 | INFO | train | epoch 010 | loss 8.88 | nll_loss 8.634 | ppl 397.29 | wps 24001.4 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 962 | lr 0.000120326 | gnorm 0.885 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 2656
2022-03-04 12:18:48 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 12:18:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:20:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:20:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:20:33 | INFO | train_inner | epoch 011:     40 / 97 loss=8.795, nll_loss=8.546, ppl=373.89, wps=23586.3, ups=0.36, wpb=65490.8, bsz=127.9, num_updates=1000, lr=0.000125075, gnorm=0.887, loss_scale=16, train_wall=243, gb_free=21, wall=2761
2022-03-04 12:23:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:23:07 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.745 | nll_loss 8.49 | ppl 359.61 | wps 42651.3 | wpb 510.9 | bsz 1 | num_updates 1057 | best_loss 8.745
2022-03-04 12:23:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1057 updates
2022-03-04 12:23:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:23:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:23:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 11 @ 1057 updates, score 8.745) (writing took 5.240688452497125 seconds)
2022-03-04 12:23:13 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 12:23:13 | INFO | train | epoch 011 | loss 8.675 | nll_loss 8.422 | ppl 343.03 | wps 23531.8 | ups 0.36 | wpb 65490.6 | bsz 127.9 | num_updates 1057 | lr 0.000132199 | gnorm 0.891 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 2920
2022-03-04 12:23:13 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 12:23:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:25:05 | INFO | train_inner | epoch 012:     43 / 97 loss=8.595, nll_loss=8.339, ppl=323.81, wps=24049.2, ups=0.37, wpb=65495, bsz=127.9, num_updates=1100, lr=0.000137573, gnorm=0.904, loss_scale=16, train_wall=238, gb_free=21, wall=3033
2022-03-04 12:27:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:27:32 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.605 | nll_loss 8.344 | ppl 324.99 | wps 42367.1 | wpb 510.9 | bsz 1 | num_updates 1154 | best_loss 8.605
2022-03-04 12:27:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1154 updates
2022-03-04 12:27:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:27:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:27:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 12 @ 1154 updates, score 8.605) (writing took 5.296962971799076 seconds)
2022-03-04 12:27:37 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 12:27:37 | INFO | train | epoch 012 | loss 8.489 | nll_loss 8.229 | ppl 299.98 | wps 23996.2 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 1154 | lr 0.000144321 | gnorm 0.945 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 3185
2022-03-04 12:27:37 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 12:27:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:29:38 | INFO | train_inner | epoch 013:     46 / 97 loss=8.404, nll_loss=8.141, ppl=282.29, wps=24009, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=1200, lr=0.00015007, gnorm=0.94, loss_scale=32, train_wall=238, gb_free=21, wall=3306
2022-03-04 12:31:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:31:57 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.473 | nll_loss 8.207 | ppl 295.58 | wps 42767.2 | wpb 510.9 | bsz 1 | num_updates 1251 | best_loss 8.473
2022-03-04 12:31:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1251 updates
2022-03-04 12:31:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:31:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:32:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 13 @ 1251 updates, score 8.473) (writing took 5.423105885274708 seconds)
2022-03-04 12:32:02 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 12:32:02 | INFO | train | epoch 013 | loss 8.312 | nll_loss 8.046 | ppl 264.24 | wps 23987.6 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 1251 | lr 0.000156444 | gnorm 0.941 | loss_scale 64 | train_wall 231 | gb_free 21 | wall 3450
2022-03-04 12:32:02 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 12:32:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:32:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:34:13 | INFO | train_inner | epoch 014:     50 / 97 loss=8.226, nll_loss=7.957, ppl=248.47, wps=23795.9, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=1300, lr=0.000162568, gnorm=0.95, loss_scale=32, train_wall=240, gb_free=21, wall=3581
2022-03-04 12:36:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:36:21 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.371 | nll_loss 8.099 | ppl 274.13 | wps 42098.3 | wpb 510.9 | bsz 1 | num_updates 1347 | best_loss 8.371
2022-03-04 12:36:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1347 updates
2022-03-04 12:36:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:36:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:36:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 14 @ 1347 updates, score 8.371) (writing took 5.309480401687324 seconds)
2022-03-04 12:36:27 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 12:36:27 | INFO | train | epoch 014 | loss 8.145 | nll_loss 7.873 | ppl 234.43 | wps 23752.3 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 1347 | lr 0.000168441 | gnorm 0.951 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 3715
2022-03-04 12:36:27 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 12:36:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:38:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:38:48 | INFO | train_inner | epoch 015:     54 / 97 loss=8.065, nll_loss=7.79, ppl=221.38, wps=23808, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=1400, lr=0.000175065, gnorm=0.992, loss_scale=32, train_wall=240, gb_free=21, wall=3856
2022-03-04 12:40:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:40:46 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.255 | nll_loss 7.98 | ppl 252.43 | wps 42182.1 | wpb 510.9 | bsz 1 | num_updates 1443 | best_loss 8.255
2022-03-04 12:40:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1443 updates
2022-03-04 12:40:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:40:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:40:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 15 @ 1443 updates, score 8.255) (writing took 5.511883155442774 seconds)
2022-03-04 12:40:52 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 12:40:52 | INFO | train | epoch 015 | loss 7.984 | nll_loss 7.706 | ppl 208.82 | wps 23743.2 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 1443 | lr 0.000180439 | gnorm 0.957 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 3979
2022-03-04 12:40:52 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 12:40:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:43:21 | INFO | train_inner | epoch 016:     57 / 97 loss=7.886, nll_loss=7.605, ppl=194.7, wps=24006.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1500, lr=0.000187563, gnorm=0.969, loss_scale=32, train_wall=238, gb_free=21, wall=4129
2022-03-04 12:44:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:45:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:45:11 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.156 | nll_loss 7.872 | ppl 234.23 | wps 42700.4 | wpb 510.9 | bsz 1 | num_updates 1539 | best_loss 8.156
2022-03-04 12:45:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1539 updates
2022-03-04 12:45:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:45:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:45:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 16 @ 1539 updates, score 8.156) (writing took 5.388975198380649 seconds)
2022-03-04 12:45:16 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 12:45:16 | INFO | train | epoch 016 | loss 7.828 | nll_loss 7.545 | ppl 186.75 | wps 23744.1 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 1539 | lr 0.000192437 | gnorm 1.001 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 4244
2022-03-04 12:45:16 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 12:45:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:47:56 | INFO | train_inner | epoch 017:     61 / 97 loss=7.742, nll_loss=7.456, ppl=175.54, wps=23811.4, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=1600, lr=0.00020006, gnorm=0.993, loss_scale=32, train_wall=240, gb_free=21, wall=4404
2022-03-04 12:49:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:49:36 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.07 | nll_loss 7.785 | ppl 220.52 | wps 42570 | wpb 510.9 | bsz 1 | num_updates 1636 | best_loss 8.07
2022-03-04 12:49:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1636 updates
2022-03-04 12:49:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:49:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:49:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 17 @ 1636 updates, score 8.07) (writing took 5.290738488547504 seconds)
2022-03-04 12:49:41 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 12:49:41 | INFO | train | epoch 017 | loss 7.673 | nll_loss 7.385 | ppl 167.11 | wps 24011.3 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 1636 | lr 0.000204559 | gnorm 1 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 4509
2022-03-04 12:49:41 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 12:49:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:50:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:52:31 | INFO | train_inner | epoch 018:     65 / 97 loss=7.568, nll_loss=7.276, ppl=155, wps=23804.2, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=1700, lr=0.000212558, gnorm=0.993, loss_scale=32, train_wall=240, gb_free=21, wall=4679
2022-03-04 12:53:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:54:00 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.988 | nll_loss 7.695 | ppl 207.19 | wps 42408.4 | wpb 510.9 | bsz 1 | num_updates 1732 | best_loss 7.988
2022-03-04 12:54:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1732 updates
2022-03-04 12:54:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:54:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:54:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 18 @ 1732 updates, score 7.988) (writing took 5.391771556809545 seconds)
2022-03-04 12:54:06 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 12:54:06 | INFO | train | epoch 018 | loss 7.518 | nll_loss 7.224 | ppl 149.5 | wps 23760.8 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 1732 | lr 0.000216557 | gnorm 0.98 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 4773
2022-03-04 12:54:06 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 12:54:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:56:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:57:06 | INFO | train_inner | epoch 019:     69 / 97 loss=7.417, nll_loss=7.119, ppl=139.03, wps=23803.9, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=1800, lr=0.000225055, gnorm=0.992, loss_scale=32, train_wall=240, gb_free=21, wall=4954
2022-03-04 12:58:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:58:25 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.915 | nll_loss 7.618 | ppl 196.43 | wps 42613.2 | wpb 510.9 | bsz 1 | num_updates 1828 | best_loss 7.915
2022-03-04 12:58:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1828 updates
2022-03-04 12:58:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:58:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 12:58:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 19 @ 1828 updates, score 7.915) (writing took 5.583654655143619 seconds)
2022-03-04 12:58:30 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 12:58:30 | INFO | train | epoch 019 | loss 7.37 | nll_loss 7.07 | ppl 134.38 | wps 23735.9 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 1828 | lr 0.000228554 | gnorm 1.002 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 5038
2022-03-04 12:58:30 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 12:58:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:01:39 | INFO | train_inner | epoch 020:     72 / 97 loss=7.263, nll_loss=6.96, ppl=124.46, wps=24022.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1900, lr=0.000237553, gnorm=0.995, loss_scale=32, train_wall=238, gb_free=21, wall=5227
2022-03-04 13:02:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:02:50 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.849 | nll_loss 7.549 | ppl 187.22 | wps 42255.6 | wpb 510.9 | bsz 1 | num_updates 1925 | best_loss 7.849
2022-03-04 13:02:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1925 updates
2022-03-04 13:02:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 13:02:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 13:02:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 20 @ 1925 updates, score 7.849) (writing took 5.319692107848823 seconds)
2022-03-04 13:02:55 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 13:02:55 | INFO | train | epoch 020 | loss 7.223 | nll_loss 6.919 | ppl 120.98 | wps 24008.4 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 1925 | lr 0.000240677 | gnorm 0.981 | loss_scale 64 | train_wall 231 | gb_free 21 | wall 5303
2022-03-04 13:02:55 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 13:02:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:03:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:06:14 | INFO | train_inner | epoch 021:     76 / 97 loss=7.114, nll_loss=6.805, ppl=111.82, wps=23803.9, ups=0.36, wpb=65495, bsz=127.9, num_updates=2000, lr=0.00025005, gnorm=0.989, loss_scale=32, train_wall=240, gb_free=21, wall=5502
2022-03-04 13:07:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:07:14 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.791 | nll_loss 7.489 | ppl 179.62 | wps 42340.7 | wpb 510.9 | bsz 1 | num_updates 2021 | best_loss 7.791
2022-03-04 13:07:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2021 updates
2022-03-04 13:07:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 13:07:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 13:07:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 21 @ 2021 updates, score 7.791) (writing took 5.441964623518288 seconds)
2022-03-04 13:07:20 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 13:07:20 | INFO | train | epoch 021 | loss 7.082 | nll_loss 6.772 | ppl 109.32 | wps 23748.1 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 2021 | lr 0.000252674 | gnorm 1.003 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 5568
2022-03-04 13:07:20 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 13:07:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:08:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:10:49 | INFO | train_inner | epoch 022:     80 / 97 loss=6.973, nll_loss=6.659, ppl=101.05, wps=23826.7, ups=0.36, wpb=65490.8, bsz=127.9, num_updates=2100, lr=0.000262548, gnorm=0.987, loss_scale=32, train_wall=240, gb_free=21, wall=5777
2022-03-04 13:11:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:11:39 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.754 | nll_loss 7.451 | ppl 174.98 | wps 42452.8 | wpb 510.9 | bsz 1 | num_updates 2117 | best_loss 7.754
2022-03-04 13:11:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2117 updates
2022-03-04 13:11:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 13:11:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 13:11:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 22 @ 2117 updates, score 7.754) (writing took 5.390984853729606 seconds)
2022-03-04 13:11:44 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 13:11:44 | INFO | train | epoch 022 | loss 6.946 | nll_loss 6.631 | ppl 99.11 | wps 23795 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 2117 | lr 0.000264672 | gnorm 0.988 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 5832
2022-03-04 13:11:44 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 13:11:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:12:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:15:24 | INFO | train_inner | epoch 023:     84 / 97 loss=6.838, nll_loss=6.519, ppl=91.68, wps=23813.8, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=2200, lr=0.000275045, gnorm=0.988, loss_scale=16, train_wall=240, gb_free=21, wall=6052
2022-03-04 13:15:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:16:03 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.724 | nll_loss 7.415 | ppl 170.69 | wps 42506.1 | wpb 510.9 | bsz 1 | num_updates 2213 | best_loss 7.724
2022-03-04 13:16:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2213 updates
2022-03-04 13:16:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 13:16:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 13:16:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 23 @ 2213 updates, score 7.724) (writing took 5.293897808529437 seconds)
2022-03-04 13:16:08 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 13:16:08 | INFO | train | epoch 023 | loss 6.812 | nll_loss 6.492 | ppl 90.03 | wps 23768.8 | ups 0.36 | wpb 65491.1 | bsz 127.9 | num_updates 2213 | lr 0.00027667 | gnorm 0.986 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 6096
2022-03-04 13:16:09 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 13:16:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:19:56 | INFO | train_inner | epoch 024:     87 / 97 loss=6.7, nll_loss=6.375, ppl=83.03, wps=24063.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2300, lr=0.000287543, gnorm=0.991, loss_scale=32, train_wall=238, gb_free=21, wall=6324
2022-03-04 13:20:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:20:28 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.701 | nll_loss 7.388 | ppl 167.56 | wps 42083.6 | wpb 510.9 | bsz 1 | num_updates 2310 | best_loss 7.701
2022-03-04 13:20:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2310 updates
2022-03-04 13:20:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 13:20:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt
2022-03-04 13:20:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_best.pt (epoch 24 @ 2310 updates, score 7.701) (writing took 5.417065986432135 seconds)
2022-03-04 13:20:33 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 13:20:33 | INFO | train | epoch 024 | loss 6.686 | nll_loss 6.361 | ppl 82.21 | wps 24017.7 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 2310 | lr 0.000288792 | gnorm 0.997 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 6361
2022-03-04 13:20:33 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 13:20:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:24:29 | INFO | train_inner | epoch 025:     90 / 97 loss=6.57, nll_loss=6.241, ppl=75.63, wps=24015.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2400, lr=0.00030004, gnorm=0.988, loss_scale=64, train_wall=238, gb_free=21, wall=6597
2022-03-04 13:24:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:24:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:24:52 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.725 | nll_loss 7.417 | ppl 170.93 | wps 41956.8 | wpb 510.9 | bsz 1 | num_updates 2406 | best_loss 7.701
2022-03-04 13:24:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2406 updates
2022-03-04 13:24:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:24:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:24:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 25 @ 2406 updates, score 7.725) (writing took 2.315966559574008 seconds)
2022-03-04 13:24:55 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 13:24:55 | INFO | train | epoch 025 | loss 6.559 | nll_loss 6.229 | ppl 75 | wps 24023 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2406 | lr 0.00030079 | gnorm 1.014 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 6623
2022-03-04 13:24:55 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 13:24:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:29:01 | INFO | train_inner | epoch 026:     94 / 97 loss=6.445, nll_loss=6.111, ppl=69.12, wps=24073.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2500, lr=0.000312538, gnorm=1.015, loss_scale=32, train_wall=240, gb_free=21, wall=6869
2022-03-04 13:29:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:29:14 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.709 | nll_loss 7.399 | ppl 168.79 | wps 42163.8 | wpb 510.9 | bsz 1 | num_updates 2503 | best_loss 7.701
2022-03-04 13:29:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2503 updates
2022-03-04 13:29:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:29:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:29:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 26 @ 2503 updates, score 7.709) (writing took 2.301991894841194 seconds)
2022-03-04 13:29:16 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 13:29:16 | INFO | train | epoch 026 | loss 6.437 | nll_loss 6.102 | ppl 68.7 | wps 24285.9 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 2503 | lr 0.000312912 | gnorm 0.989 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 6884
2022-03-04 13:29:16 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 13:29:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:30:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:32:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:33:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:33:36 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.714 | nll_loss 7.399 | ppl 168.82 | wps 42770.8 | wpb 510.9 | bsz 1 | num_updates 2598 | best_loss 7.701
2022-03-04 13:33:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2598 updates
2022-03-04 13:33:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:33:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:33:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 27 @ 2598 updates, score 7.714) (writing took 2.211899233981967 seconds)
2022-03-04 13:33:38 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 13:33:38 | INFO | train | epoch 027 | loss 6.318 | nll_loss 5.978 | ppl 63.04 | wps 23793.5 | ups 0.36 | wpb 65490.6 | bsz 127.9 | num_updates 2598 | lr 0.000324785 | gnorm 1.004 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 7146
2022-03-04 13:33:38 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 13:33:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:33:43 | INFO | train_inner | epoch 028:      2 / 97 loss=6.319, nll_loss=5.98, ppl=63.11, wps=23203.1, ups=0.35, wpb=65451.9, bsz=127.8, num_updates=2600, lr=0.000325035, gnorm=1.003, loss_scale=16, train_wall=243, gb_free=21, wall=7151
2022-03-04 13:37:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:37:57 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.707 | nll_loss 7.391 | ppl 167.84 | wps 42068.9 | wpb 510.9 | bsz 1 | num_updates 2695 | best_loss 7.701
2022-03-04 13:37:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2695 updates
2022-03-04 13:37:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:37:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:37:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 28 @ 2695 updates, score 7.707) (writing took 2.3163292706012726 seconds)
2022-03-04 13:37:59 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 13:37:59 | INFO | train | epoch 028 | loss 6.201 | nll_loss 5.857 | ppl 57.96 | wps 24288.2 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 2695 | lr 0.000336908 | gnorm 1.001 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 7407
2022-03-04 13:37:59 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 13:37:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:38:13 | INFO | train_inner | epoch 029:      5 / 97 loss=6.194, nll_loss=5.85, ppl=57.67, wps=24306.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2700, lr=0.000337533, gnorm=1, loss_scale=16, train_wall=238, gb_free=21, wall=7420
2022-03-04 13:42:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:42:19 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.747 | nll_loss 7.425 | ppl 171.89 | wps 42430.7 | wpb 510.9 | bsz 1 | num_updates 2792 | best_loss 7.701
2022-03-04 13:42:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2792 updates
2022-03-04 13:42:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:42:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:42:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 29 @ 2792 updates, score 7.747) (writing took 2.306263289414346 seconds)
2022-03-04 13:42:21 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 13:42:21 | INFO | train | epoch 029 | loss 6.086 | nll_loss 5.737 | ppl 53.32 | wps 24285.5 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 2792 | lr 0.00034903 | gnorm 1.026 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 7669
2022-03-04 13:42:21 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 13:42:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:42:42 | INFO | train_inner | epoch 030:      8 / 97 loss=6.075, nll_loss=5.726, ppl=52.94, wps=24300.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2800, lr=0.00035003, gnorm=1.026, loss_scale=32, train_wall=238, gb_free=21, wall=7690
2022-03-04 13:44:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:46:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:46:40 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.772 | nll_loss 7.451 | ppl 174.92 | wps 42274.7 | wpb 510.9 | bsz 1 | num_updates 2888 | best_loss 7.701
2022-03-04 13:46:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2888 updates
2022-03-04 13:46:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:46:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:46:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 30 @ 2888 updates, score 7.772) (writing took 2.287700924091041 seconds)
2022-03-04 13:46:43 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 13:46:43 | INFO | train | epoch 030 | loss 5.971 | nll_loss 5.618 | ppl 49.1 | wps 24013.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2888 | lr 0.000361028 | gnorm 1.015 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 7931
2022-03-04 13:46:43 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 13:46:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:47:14 | INFO | train_inner | epoch 031:     12 / 97 loss=5.954, nll_loss=5.6, ppl=48.51, wps=24059.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2900, lr=0.000362528, gnorm=1.013, loss_scale=32, train_wall=241, gb_free=21, wall=7962
2022-03-04 13:50:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:50:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:51:02 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.808 | nll_loss 7.484 | ppl 179.06 | wps 42364 | wpb 510.9 | bsz 1 | num_updates 2984 | best_loss 7.701
2022-03-04 13:51:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2984 updates
2022-03-04 13:51:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:51:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:51:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 31 @ 2984 updates, score 7.808) (writing took 2.356905714608729 seconds)
2022-03-04 13:51:04 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 13:51:04 | INFO | train | epoch 031 | loss 5.86 | nll_loss 5.502 | ppl 45.32 | wps 24041.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2984 | lr 0.000373025 | gnorm 1.041 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 8192
2022-03-04 13:51:04 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 13:51:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:51:46 | INFO | train_inner | epoch 032:     16 / 97 loss=5.842, nll_loss=5.483, ppl=44.72, wps=24076.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3000, lr=0.000375025, gnorm=1.042, loss_scale=32, train_wall=240, gb_free=21, wall=8234
2022-03-04 13:54:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:55:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:55:24 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.841 | nll_loss 7.523 | ppl 183.87 | wps 42606.4 | wpb 510.9 | bsz 1 | num_updates 3080 | best_loss 7.701
2022-03-04 13:55:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3080 updates
2022-03-04 13:55:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:55:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:55:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 32 @ 3080 updates, score 7.841) (writing took 2.210286215879023 seconds)
2022-03-04 13:55:26 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 13:55:26 | INFO | train | epoch 032 | loss 5.755 | nll_loss 5.392 | ppl 41.98 | wps 24023.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 3080 | lr 0.000385023 | gnorm 1.056 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 8454
2022-03-04 13:55:26 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 13:55:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:56:19 | INFO | train_inner | epoch 033:     20 / 97 loss=5.731, nll_loss=5.367, ppl=41.27, wps=24053.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3100, lr=0.000387523, gnorm=1.037, loss_scale=16, train_wall=241, gb_free=21, wall=8506
2022-03-04 13:59:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:59:46 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.868 | nll_loss 7.541 | ppl 186.3 | wps 42480.8 | wpb 510.9 | bsz 1 | num_updates 3177 | best_loss 7.701
2022-03-04 13:59:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3177 updates
2022-03-04 13:59:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:59:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 13:59:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 33 @ 3177 updates, score 7.868) (writing took 2.251203667372465 seconds)
2022-03-04 13:59:48 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 13:59:48 | INFO | train | epoch 033 | loss 5.646 | nll_loss 5.278 | ppl 38.81 | wps 24249.7 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 3177 | lr 0.000397146 | gnorm 1.05 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 8716
2022-03-04 13:59:48 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 13:59:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:00:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:00:51 | INFO | train_inner | epoch 034:     24 / 97 loss=5.618, nll_loss=5.249, ppl=38.04, wps=24044.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3200, lr=0.00040002, gnorm=1.065, loss_scale=16, train_wall=241, gb_free=21, wall=8779
2022-03-04 14:04:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:04:07 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.946 | nll_loss 7.623 | ppl 197.19 | wps 42010.6 | wpb 510.9 | bsz 1 | num_updates 3273 | best_loss 7.701
2022-03-04 14:04:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3273 updates
2022-03-04 14:04:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:04:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:04:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 34 @ 3273 updates, score 7.946) (writing took 2.3585424283519387 seconds)
2022-03-04 14:04:10 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 14:04:10 | INFO | train | epoch 034 | loss 5.538 | nll_loss 5.165 | ppl 35.89 | wps 24019.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 3273 | lr 0.000409143 | gnorm 1.058 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 8978
2022-03-04 14:04:10 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 14:04:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:05:21 | INFO | train_inner | epoch 035:     27 / 97 loss=5.518, nll_loss=5.145, ppl=35.38, wps=24299.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3300, lr=0.000412518, gnorm=1.099, loss_scale=16, train_wall=238, gb_free=21, wall=9048
2022-03-04 14:08:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:08:29 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.967 | nll_loss 7.643 | ppl 199.84 | wps 42159.7 | wpb 510.9 | bsz 1 | num_updates 3370 | best_loss 7.701
2022-03-04 14:08:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3370 updates
2022-03-04 14:08:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:08:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:08:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 35 @ 3370 updates, score 7.967) (writing took 2.2598654311150312 seconds)
2022-03-04 14:08:31 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 14:08:31 | INFO | train | epoch 035 | loss 5.436 | nll_loss 5.06 | ppl 33.35 | wps 24272.1 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 3370 | lr 0.000421266 | gnorm 1.076 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 9239
2022-03-04 14:08:31 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 14:08:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:09:50 | INFO | train_inner | epoch 036:     30 / 97 loss=5.396, nll_loss=5.017, ppl=32.39, wps=24288.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3400, lr=0.000425015, gnorm=1.064, loss_scale=32, train_wall=238, gb_free=21, wall=9318
2022-03-04 14:11:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:12:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:12:51 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.992 | nll_loss 7.663 | ppl 202.61 | wps 42345.5 | wpb 510.9 | bsz 1 | num_updates 3466 | best_loss 7.701
2022-03-04 14:12:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 3466 updates
2022-03-04 14:12:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:12:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:12:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 36 @ 3466 updates, score 7.992) (writing took 2.2229364905506372 seconds)
2022-03-04 14:12:53 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 14:12:53 | INFO | train | epoch 036 | loss 5.337 | nll_loss 4.956 | ppl 31.03 | wps 24026.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 3466 | lr 0.000433263 | gnorm 1.13 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 9501
2022-03-04 14:12:53 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 14:12:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:14:22 | INFO | train_inner | epoch 037:     34 / 97 loss=5.3, nll_loss=4.917, ppl=30.22, wps=24060, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3500, lr=0.000437513, gnorm=1.1, loss_scale=16, train_wall=240, gb_free=21, wall=9590
2022-03-04 14:17:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:17:13 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.06 | nll_loss 7.731 | ppl 212.43 | wps 42303.3 | wpb 510.9 | bsz 1 | num_updates 3563 | best_loss 7.701
2022-03-04 14:17:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 3563 updates
2022-03-04 14:17:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:17:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:17:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 37 @ 3563 updates, score 8.06) (writing took 2.287008007057011 seconds)
2022-03-04 14:17:15 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 14:17:15 | INFO | train | epoch 037 | loss 5.231 | nll_loss 4.845 | ppl 28.73 | wps 24263.9 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 3563 | lr 0.000445386 | gnorm 1.106 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 9763
2022-03-04 14:17:15 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 14:17:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:18:52 | INFO | train_inner | epoch 038:     37 / 97 loss=5.195, nll_loss=4.807, ppl=27.99, wps=24283.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3600, lr=0.00045001, gnorm=1.147, loss_scale=32, train_wall=238, gb_free=21, wall=9860
2022-03-04 14:19:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:21:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:21:34 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.077 | nll_loss 7.748 | ppl 215 | wps 42341.2 | wpb 510.9 | bsz 1 | num_updates 3659 | best_loss 7.701
2022-03-04 14:21:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3659 updates
2022-03-04 14:21:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:21:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:21:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 38 @ 3659 updates, score 8.077) (writing took 2.316967167891562 seconds)
2022-03-04 14:21:37 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 14:21:37 | INFO | train | epoch 038 | loss 5.13 | nll_loss 4.739 | ppl 26.71 | wps 24020.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 3659 | lr 0.000457384 | gnorm 1.105 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 10025
2022-03-04 14:21:37 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 14:21:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:23:24 | INFO | train_inner | epoch 039:     41 / 97 loss=5.098, nll_loss=4.706, ppl=26.1, wps=24071.1, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=3700, lr=0.000462508, gnorm=1.14, loss_scale=16, train_wall=240, gb_free=21, wall=10132
2022-03-04 14:25:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:25:56 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.123 | nll_loss 7.794 | ppl 221.92 | wps 42338.3 | wpb 510.9 | bsz 1 | num_updates 3756 | best_loss 7.701
2022-03-04 14:25:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 3756 updates
2022-03-04 14:25:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:25:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:25:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 39 @ 3756 updates, score 8.123) (writing took 2.254247239790857 seconds)
2022-03-04 14:25:58 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 14:25:58 | INFO | train | epoch 039 | loss 5.041 | nll_loss 4.646 | ppl 25.03 | wps 24279 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 3756 | lr 0.000469506 | gnorm 1.17 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 10286
2022-03-04 14:25:58 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 14:25:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:27:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:27:56 | INFO | train_inner | epoch 040:     45 / 97 loss=4.985, nll_loss=4.587, ppl=24.04, wps=24068.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3800, lr=0.000475005, gnorm=1.11, loss_scale=16, train_wall=240, gb_free=21, wall=10404
2022-03-04 14:30:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:30:18 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.182 | nll_loss 7.852 | ppl 231.09 | wps 42328.5 | wpb 510.9 | bsz 1 | num_updates 3852 | best_loss 7.701
2022-03-04 14:30:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 3852 updates
2022-03-04 14:30:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:30:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:30:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 40 @ 3852 updates, score 8.182) (writing took 2.284870319068432 seconds)
2022-03-04 14:30:20 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 14:30:20 | INFO | train | epoch 040 | loss 4.939 | nll_loss 4.539 | ppl 23.25 | wps 24030.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 3852 | lr 0.000481504 | gnorm 1.118 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 10548
2022-03-04 14:30:20 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 14:30:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:32:26 | INFO | train_inner | epoch 041:     48 / 97 loss=4.891, nll_loss=4.488, ppl=22.44, wps=24292.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3900, lr=0.000487503, gnorm=1.136, loss_scale=16, train_wall=238, gb_free=21, wall=10674
2022-03-04 14:33:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:34:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:34:39 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.236 | nll_loss 7.898 | ppl 238.45 | wps 42423.8 | wpb 510.9 | bsz 1 | num_updates 3948 | best_loss 7.701
2022-03-04 14:34:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 3948 updates
2022-03-04 14:34:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:34:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:34:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 41 @ 3948 updates, score 8.236) (writing took 2.256705825217068 seconds)
2022-03-04 14:34:42 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 14:34:42 | INFO | train | epoch 041 | loss 4.845 | nll_loss 4.44 | ppl 21.71 | wps 24021.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 3948 | lr 0.000493501 | gnorm 1.185 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 10810
2022-03-04 14:34:42 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 14:34:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:36:58 | INFO | train_inner | epoch 042:     52 / 97 loss=4.802, nll_loss=4.395, ppl=21.04, wps=24072.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4000, lr=0.0005, gnorm=1.234, loss_scale=16, train_wall=240, gb_free=21, wall=10946
2022-03-04 14:38:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:39:01 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.348 | nll_loss 8.013 | ppl 258.35 | wps 42596 | wpb 510.9 | bsz 1 | num_updates 4045 | best_loss 7.701
2022-03-04 14:39:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 4045 updates
2022-03-04 14:39:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:39:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:39:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 42 @ 4045 updates, score 8.348) (writing took 2.244877136312425 seconds)
2022-03-04 14:39:03 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 14:39:03 | INFO | train | epoch 042 | loss 4.752 | nll_loss 4.342 | ppl 20.28 | wps 24290.6 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 4045 | lr 0.000497211 | gnorm 1.153 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 11071
2022-03-04 14:39:03 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 14:39:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:41:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:41:30 | INFO | train_inner | epoch 043:     56 / 97 loss=4.696, nll_loss=4.283, ppl=19.47, wps=24064.9, ups=0.37, wpb=65495, bsz=127.9, num_updates=4100, lr=0.000493865, gnorm=1.144, loss_scale=16, train_wall=240, gb_free=21, wall=11218
2022-03-04 14:43:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:43:23 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.39 | nll_loss 8.052 | ppl 265.32 | wps 41914.3 | wpb 510.9 | bsz 1 | num_updates 4141 | best_loss 7.701
2022-03-04 14:43:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 4141 updates
2022-03-04 14:43:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:43:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:43:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 43 @ 4141 updates, score 8.39) (writing took 2.3414875753223896 seconds)
2022-03-04 14:43:25 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 14:43:25 | INFO | train | epoch 043 | loss 4.648 | nll_loss 4.234 | ppl 18.81 | wps 24008.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 4141 | lr 0.000491414 | gnorm 1.131 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 11333
2022-03-04 14:43:25 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 14:43:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:45:59 | INFO | train_inner | epoch 044:     59 / 97 loss=4.586, nll_loss=4.169, ppl=17.98, wps=24309.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4200, lr=0.00048795, gnorm=1.094, loss_scale=16, train_wall=238, gb_free=21, wall=11487
2022-03-04 14:47:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:47:44 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.473 | nll_loss 8.138 | ppl 281.75 | wps 42229.8 | wpb 510.9 | bsz 1 | num_updates 4238 | best_loss 7.701
2022-03-04 14:47:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 4238 updates
2022-03-04 14:47:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:47:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:47:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 44 @ 4238 updates, score 8.473) (writing took 2.265732202678919 seconds)
2022-03-04 14:47:46 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 14:47:46 | INFO | train | epoch 044 | loss 4.553 | nll_loss 4.133 | ppl 17.55 | wps 24324.2 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 4238 | lr 0.000485758 | gnorm 1.142 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 11594
2022-03-04 14:47:46 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 14:47:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:50:29 | INFO | train_inner | epoch 045:     62 / 97 loss=4.497, nll_loss=4.075, ppl=16.85, wps=24336.4, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=4300, lr=0.000482243, gnorm=1.135, loss_scale=32, train_wall=238, gb_free=21, wall=11756
2022-03-04 14:50:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:52:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:52:05 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.526 | nll_loss 8.183 | ppl 290.54 | wps 42037.1 | wpb 510.9 | bsz 1 | num_updates 4334 | best_loss 7.701
2022-03-04 14:52:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 4334 updates
2022-03-04 14:52:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:52:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:52:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 45 @ 4334 updates, score 8.526) (writing took 2.2589981090277433 seconds)
2022-03-04 14:52:08 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 14:52:08 | INFO | train | epoch 045 | loss 4.454 | nll_loss 4.029 | ppl 16.33 | wps 24045.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 4334 | lr 0.000480348 | gnorm 1.132 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 11856
2022-03-04 14:52:08 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 14:52:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:55:01 | INFO | train_inner | epoch 046:     66 / 97 loss=4.392, nll_loss=3.965, ppl=15.61, wps=24067.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4400, lr=0.000476731, gnorm=1.129, loss_scale=16, train_wall=240, gb_free=21, wall=12029
2022-03-04 14:56:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:56:27 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.601 | nll_loss 8.257 | ppl 305.97 | wps 42452.1 | wpb 510.9 | bsz 1 | num_updates 4431 | best_loss 7.701
2022-03-04 14:56:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 4431 updates
2022-03-04 14:56:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:56:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 14:56:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 46 @ 4431 updates, score 8.601) (writing took 2.25137397646904 seconds)
2022-03-04 14:56:29 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 14:56:29 | INFO | train | epoch 046 | loss 4.361 | nll_loss 3.931 | ppl 15.26 | wps 24297.9 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 4431 | lr 0.000475061 | gnorm 1.136 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 12117
2022-03-04 14:56:29 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 14:56:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:57:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:59:33 | INFO | train_inner | epoch 047:     70 / 97 loss=4.296, nll_loss=3.864, ppl=14.56, wps=24087.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4500, lr=0.000471405, gnorm=1.113, loss_scale=16, train_wall=240, gb_free=21, wall=12301
2022-03-04 15:00:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:00:48 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.677 | nll_loss 8.337 | ppl 323.46 | wps 42777.3 | wpb 510.9 | bsz 1 | num_updates 4527 | best_loss 7.701
2022-03-04 15:00:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4527 updates
2022-03-04 15:00:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:00:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:00:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 47 @ 4527 updates, score 8.677) (writing took 2.368334540165961 seconds)
2022-03-04 15:00:51 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 15:00:51 | INFO | train | epoch 047 | loss 4.27 | nll_loss 3.836 | ppl 14.28 | wps 24030.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 4527 | lr 0.000469997 | gnorm 1.114 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 12379
2022-03-04 15:00:51 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 15:00:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:04:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:04:05 | INFO | train_inner | epoch 048:     74 / 97 loss=4.209, nll_loss=3.771, ppl=13.66, wps=24068.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4600, lr=0.000466252, gnorm=1.131, loss_scale=16, train_wall=240, gb_free=21, wall=12573
2022-03-04 15:05:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:05:10 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.715 | nll_loss 8.37 | ppl 330.95 | wps 42605.8 | wpb 510.9 | bsz 1 | num_updates 4623 | best_loss 7.701
2022-03-04 15:05:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 4623 updates
2022-03-04 15:05:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:05:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:05:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 48 @ 4623 updates, score 8.715) (writing took 2.226308452896774 seconds)
2022-03-04 15:05:12 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 15:05:12 | INFO | train | epoch 048 | loss 4.184 | nll_loss 3.746 | ppl 13.42 | wps 24043.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 4623 | lr 0.000465091 | gnorm 1.116 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 12640
2022-03-04 15:05:12 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 15:05:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:08:34 | INFO | train_inner | epoch 049:     77 / 97 loss=4.127, nll_loss=3.686, ppl=12.87, wps=24321.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4700, lr=0.000461266, gnorm=1.125, loss_scale=16, train_wall=238, gb_free=21, wall=12842
2022-03-04 15:09:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:09:32 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.838 | nll_loss 8.491 | ppl 359.86 | wps 42289.7 | wpb 510.9 | bsz 1 | num_updates 4720 | best_loss 7.701
2022-03-04 15:09:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 4720 updates
2022-03-04 15:09:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:09:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:09:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 49 @ 4720 updates, score 8.838) (writing took 2.2988723516464233 seconds)
2022-03-04 15:09:34 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 15:09:34 | INFO | train | epoch 049 | loss 4.101 | nll_loss 3.659 | ppl 12.63 | wps 24279.5 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 4720 | lr 0.000460287 | gnorm 1.118 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 12902
2022-03-04 15:09:34 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 15:09:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:13:04 | INFO | train_inner | epoch 050:     80 / 97 loss=4.037, nll_loss=3.591, ppl=12.05, wps=24294.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4800, lr=0.000456435, gnorm=1.115, loss_scale=32, train_wall=238, gb_free=21, wall=13111
2022-03-04 15:13:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:13:53 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.901 | nll_loss 8.552 | ppl 375.39 | wps 42533.5 | wpb 510.9 | bsz 1 | num_updates 4817 | best_loss 7.701
2022-03-04 15:13:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 4817 updates
2022-03-04 15:13:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:13:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:13:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 50 @ 4817 updates, score 8.901) (writing took 2.2905108900740743 seconds)
2022-03-04 15:13:56 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 15:13:56 | INFO | train | epoch 050 | loss 4.021 | nll_loss 3.573 | ppl 11.9 | wps 24282.5 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 4817 | lr 0.000455629 | gnorm 1.116 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 13163
2022-03-04 15:13:56 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 15:13:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:14:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:17:36 | INFO | train_inner | epoch 051:     84 / 97 loss=3.957, nll_loss=3.506, ppl=11.36, wps=24070.1, ups=0.37, wpb=65495, bsz=127.9, num_updates=4900, lr=0.000451754, gnorm=1.126, loss_scale=16, train_wall=240, gb_free=21, wall=13384
2022-03-04 15:18:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:18:15 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 8.981 | nll_loss 8.637 | ppl 398 | wps 42403.2 | wpb 510.9 | bsz 1 | num_updates 4913 | best_loss 7.701
2022-03-04 15:18:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 4913 updates
2022-03-04 15:18:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:18:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:18:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 51 @ 4913 updates, score 8.981) (writing took 2.224329580552876 seconds)
2022-03-04 15:18:17 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 15:18:17 | INFO | train | epoch 051 | loss 3.943 | nll_loss 3.492 | ppl 11.25 | wps 24032.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 4913 | lr 0.000451156 | gnorm 1.133 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 13425
2022-03-04 15:18:17 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 15:18:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:22:05 | INFO | train_inner | epoch 052:     87 / 97 loss=3.878, nll_loss=3.423, ppl=10.72, wps=24337.4, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=5000, lr=0.000447214, gnorm=1.102, loss_scale=32, train_wall=238, gb_free=21, wall=13653
2022-03-04 15:22:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:22:36 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.093 | nll_loss 8.752 | ppl 431.04 | wps 42403.5 | wpb 510.9 | bsz 1 | num_updates 5010 | best_loss 7.701
2022-03-04 15:22:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 5010 updates
2022-03-04 15:22:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:22:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:22:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 52 @ 5010 updates, score 9.093) (writing took 2.2186136469244957 seconds)
2022-03-04 15:22:38 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 15:22:38 | INFO | train | epoch 052 | loss 3.866 | nll_loss 3.411 | ppl 10.63 | wps 24333.2 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 5010 | lr 0.000446767 | gnorm 1.104 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 13686
2022-03-04 15:22:38 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 15:22:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:23:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:26:37 | INFO | train_inner | epoch 053:     91 / 97 loss=3.805, nll_loss=3.346, ppl=10.17, wps=24067.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=5100, lr=0.000442807, gnorm=1.112, loss_scale=16, train_wall=240, gb_free=21, wall=13925
2022-03-04 15:26:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:26:58 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.133 | nll_loss 8.789 | ppl 442.39 | wps 42015.6 | wpb 510.9 | bsz 1 | num_updates 5106 | best_loss 7.701
2022-03-04 15:26:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 5106 updates
2022-03-04 15:26:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:27:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:27:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 53 @ 5106 updates, score 9.133) (writing took 2.388416807167232 seconds)
2022-03-04 15:27:00 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 15:27:00 | INFO | train | epoch 053 | loss 3.795 | nll_loss 3.336 | ppl 10.1 | wps 24000.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 5106 | lr 0.000442547 | gnorm 1.109 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 13948
2022-03-04 15:27:00 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 15:27:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:31:06 | INFO | train_inner | epoch 054:     94 / 97 loss=3.734, nll_loss=3.27, ppl=9.65, wps=24294.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=5200, lr=0.000438529, gnorm=1.113, loss_scale=32, train_wall=238, gb_free=21, wall=14194
2022-03-04 15:31:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:31:19 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.255 | nll_loss 8.914 | ppl 482.48 | wps 42543.8 | wpb 510.9 | bsz 1 | num_updates 5203 | best_loss 7.701
2022-03-04 15:31:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 5203 updates
2022-03-04 15:31:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:31:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:31:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 54 @ 5203 updates, score 9.255) (writing took 2.335054522380233 seconds)
2022-03-04 15:31:22 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 15:31:22 | INFO | train | epoch 054 | loss 3.726 | nll_loss 3.262 | ppl 9.59 | wps 24285.2 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 5203 | lr 0.000438403 | gnorm 1.113 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 14210
2022-03-04 15:31:22 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 15:31:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:33:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:35:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:35:41 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.32 | nll_loss 8.974 | ppl 502.74 | wps 42203.1 | wpb 510.9 | bsz 1 | num_updates 5299 | best_loss 7.701
2022-03-04 15:35:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 5299 updates
2022-03-04 15:35:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:35:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:35:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 55 @ 5299 updates, score 9.32) (writing took 2.4468065686523914 seconds)
2022-03-04 15:35:44 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 15:35:44 | INFO | train | epoch 055 | loss 3.659 | nll_loss 3.192 | ppl 9.14 | wps 23996.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 5299 | lr 0.000434413 | gnorm 1.135 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 14472
2022-03-04 15:35:44 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 15:35:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:35:46 | INFO | train_inner | epoch 056:      1 / 97 loss=3.662, nll_loss=3.195, ppl=9.16, wps=23377.2, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=5300, lr=0.000434372, gnorm=1.136, loss_scale=16, train_wall=240, gb_free=21, wall=14474
2022-03-04 15:39:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:40:03 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.428 | nll_loss 9.077 | ppl 539.99 | wps 42375.3 | wpb 510.9 | bsz 1 | num_updates 5396 | best_loss 7.701
2022-03-04 15:40:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 5396 updates
2022-03-04 15:40:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:40:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:40:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 56 @ 5396 updates, score 9.428) (writing took 2.4948667399585247 seconds)
2022-03-04 15:40:06 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 15:40:06 | INFO | train | epoch 056 | loss 3.594 | nll_loss 3.123 | ppl 8.71 | wps 24265.2 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 5396 | lr 0.000430491 | gnorm 1.12 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 14733
2022-03-04 15:40:06 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 15:40:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:40:16 | INFO | train_inner | epoch 057:      4 / 97 loss=3.589, nll_loss=3.117, ppl=8.68, wps=24294.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=5400, lr=0.000430331, gnorm=1.119, loss_scale=32, train_wall=238, gb_free=21, wall=14744
2022-03-04 15:44:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:44:25 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.456 | nll_loss 9.106 | ppl 551.19 | wps 42181.5 | wpb 510.9 | bsz 1 | num_updates 5493 | best_loss 7.701
2022-03-04 15:44:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 5493 updates
2022-03-04 15:44:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:44:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:44:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 57 @ 5493 updates, score 9.456) (writing took 2.3681461084634066 seconds)
2022-03-04 15:44:27 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 15:44:27 | INFO | train | epoch 057 | loss 3.531 | nll_loss 3.056 | ppl 8.32 | wps 24307.2 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 5493 | lr 0.000426673 | gnorm 1.108 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 14995
2022-03-04 15:44:27 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 15:44:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:44:45 | INFO | train_inner | epoch 058:      7 / 97 loss=3.523, nll_loss=3.048, ppl=8.27, wps=24320, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=5500, lr=0.000426401, gnorm=1.103, loss_scale=32, train_wall=238, gb_free=21, wall=15013
2022-03-04 15:45:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:48:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:48:46 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.514 | nll_loss 9.168 | ppl 575.39 | wps 41632.2 | wpb 510.9 | bsz 1 | num_updates 5589 | best_loss 7.701
2022-03-04 15:48:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 5589 updates
2022-03-04 15:48:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:48:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:48:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 58 @ 5589 updates, score 9.514) (writing took 2.4248681608587503 seconds)
2022-03-04 15:48:49 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 15:48:49 | INFO | train | epoch 058 | loss 3.472 | nll_loss 2.993 | ppl 7.96 | wps 24026.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 5589 | lr 0.000422993 | gnorm 1.111 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 15256
2022-03-04 15:48:49 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 15:48:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:49:17 | INFO | train_inner | epoch 059:     11 / 97 loss=3.464, nll_loss=2.985, ppl=7.92, wps=24071.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=5600, lr=0.000422577, gnorm=1.114, loss_scale=32, train_wall=240, gb_free=21, wall=15285
2022-03-04 15:51:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:51:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:53:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:53:05 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.651 | nll_loss 9.306 | ppl 632.89 | wps 43140.6 | wpb 510.9 | bsz 1 | num_updates 5684 | best_loss 7.701
2022-03-04 15:53:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 5684 updates
2022-03-04 15:53:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:53:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:53:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 59 @ 5684 updates, score 9.651) (writing took 2.2886817026883364 seconds)
2022-03-04 15:53:07 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 15:53:07 | INFO | train | epoch 059 | loss 3.412 | nll_loss 2.93 | ppl 7.62 | wps 24058.6 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 5684 | lr 0.000419443 | gnorm 1.12 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 15515
2022-03-04 15:53:07 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 15:53:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:53:49 | INFO | train_inner | epoch 060:     16 / 97 loss=3.401, nll_loss=2.918, ppl=7.56, wps=24160.6, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=5700, lr=0.000418854, gnorm=1.122, loss_scale=16, train_wall=240, gb_free=21, wall=15556
2022-03-04 15:57:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:57:23 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.712 | nll_loss 9.364 | ppl 658.98 | wps 42961.8 | wpb 510.9 | bsz 1 | num_updates 5781 | best_loss 7.701
2022-03-04 15:57:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 5781 updates
2022-03-04 15:57:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:57:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 15:57:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 60 @ 5781 updates, score 9.712) (writing took 2.3348091412335634 seconds)
2022-03-04 15:57:25 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 15:57:25 | INFO | train | epoch 060 | loss 3.359 | nll_loss 2.873 | ppl 7.33 | wps 24619.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5781 | lr 0.000415909 | gnorm 1.119 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 15773
2022-03-04 15:57:25 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 15:57:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:58:14 | INFO | train_inner | epoch 061:     19 / 97 loss=3.347, nll_loss=2.861, ppl=7.26, wps=24627.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=5800, lr=0.000415227, gnorm=1.126, loss_scale=32, train_wall=235, gb_free=21, wall=15822
2022-03-04 16:01:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:01:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:01:41 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.795 | nll_loss 9.449 | ppl 698.92 | wps 43142.6 | wpb 510.9 | bsz 1 | num_updates 5877 | best_loss 7.701
2022-03-04 16:01:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 5877 updates
2022-03-04 16:01:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:01:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:01:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 61 @ 5877 updates, score 9.795) (writing took 2.603987216949463 seconds)
2022-03-04 16:01:44 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 16:01:44 | INFO | train | epoch 061 | loss 3.303 | nll_loss 2.814 | ppl 7.03 | wps 24339.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 5877 | lr 0.000412498 | gnorm 1.125 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 16031
2022-03-04 16:01:44 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 16:01:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:02:43 | INFO | train_inner | epoch 062:     23 / 97 loss=3.289, nll_loss=2.8, ppl=6.96, wps=24383.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=5900, lr=0.000411693, gnorm=1.118, loss_scale=16, train_wall=238, gb_free=21, wall=16091
2022-03-04 16:05:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:05:59 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.879 | nll_loss 9.534 | ppl 741.42 | wps 42780.6 | wpb 510.9 | bsz 1 | num_updates 5974 | best_loss 7.701
2022-03-04 16:05:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 5974 updates
2022-03-04 16:05:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:06:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:06:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 62 @ 5974 updates, score 9.879) (writing took 2.2191346641629934 seconds)
2022-03-04 16:06:01 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 16:06:01 | INFO | train | epoch 062 | loss 3.252 | nll_loss 2.76 | ppl 6.77 | wps 24630.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5974 | lr 0.000409136 | gnorm 1.11 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 16289
2022-03-04 16:06:01 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 16:06:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:07:09 | INFO | train_inner | epoch 063:     26 / 97 loss=3.235, nll_loss=2.742, ppl=6.69, wps=24653.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=6000, lr=0.000408248, gnorm=1.121, loss_scale=16, train_wall=235, gb_free=21, wall=16357
2022-03-04 16:08:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:10:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:10:17 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.95 | nll_loss 9.6 | ppl 776.2 | wps 43068.6 | wpb 510.9 | bsz 1 | num_updates 6070 | best_loss 7.701
2022-03-04 16:10:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 6070 updates
2022-03-04 16:10:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:10:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:10:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 63 @ 6070 updates, score 9.95) (writing took 2.524133282713592 seconds)
2022-03-04 16:10:20 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 16:10:20 | INFO | train | epoch 063 | loss 3.2 | nll_loss 2.705 | ppl 6.52 | wps 24348.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6070 | lr 0.000405887 | gnorm 1.119 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 16548
2022-03-04 16:10:20 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 16:10:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:11:37 | INFO | train_inner | epoch 064:     30 / 97 loss=3.186, nll_loss=2.69, ppl=6.45, wps=24394.6, ups=0.37, wpb=65495, bsz=127.9, num_updates=6100, lr=0.000404888, gnorm=1.119, loss_scale=16, train_wall=238, gb_free=21, wall=16625
2022-03-04 16:14:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:14:35 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 10.044 | nll_loss 9.691 | ppl 826.74 | wps 42970 | wpb 510.9 | bsz 1 | num_updates 6167 | best_loss 7.701
2022-03-04 16:14:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 6167 updates
2022-03-04 16:14:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:14:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:14:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 64 @ 6167 updates, score 10.044) (writing took 2.2453530868515372 seconds)
2022-03-04 16:14:37 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 16:14:37 | INFO | train | epoch 064 | loss 3.153 | nll_loss 2.655 | ppl 6.3 | wps 24653.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6167 | lr 0.000402683 | gnorm 1.133 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 16805
2022-03-04 16:14:37 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 16:14:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:16:03 | INFO | train_inner | epoch 065:     33 / 97 loss=3.133, nll_loss=2.634, ppl=6.21, wps=24657.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6200, lr=0.00040161, gnorm=1.114, loss_scale=32, train_wall=235, gb_free=21, wall=16891
2022-03-04 16:16:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:18:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:18:53 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 10.095 | nll_loss 9.746 | ppl 858.61 | wps 42700 | wpb 510.9 | bsz 1 | num_updates 6263 | best_loss 7.701
2022-03-04 16:18:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 6263 updates
2022-03-04 16:18:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:18:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:18:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 65 @ 6263 updates, score 10.095) (writing took 2.309014465659857 seconds)
2022-03-04 16:18:56 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 16:18:56 | INFO | train | epoch 065 | loss 3.102 | nll_loss 2.602 | ppl 6.07 | wps 24345.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6263 | lr 0.000399585 | gnorm 1.117 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 17063
2022-03-04 16:18:56 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 16:18:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:20:31 | INFO | train_inner | epoch 066:     37 / 97 loss=3.085, nll_loss=2.583, ppl=5.99, wps=24411.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=6300, lr=0.00039841, gnorm=1.114, loss_scale=16, train_wall=238, gb_free=21, wall=17159
2022-03-04 16:23:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:23:11 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 10.166 | nll_loss 9.82 | ppl 903.84 | wps 43621.6 | wpb 510.9 | bsz 1 | num_updates 6360 | best_loss 7.701
2022-03-04 16:23:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 6360 updates
2022-03-04 16:23:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:23:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:23:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 66 @ 6360 updates, score 10.166) (writing took 2.4341077310964465 seconds)
2022-03-04 16:23:13 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 16:23:13 | INFO | train | epoch 066 | loss 3.059 | nll_loss 2.556 | ppl 5.88 | wps 24633.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6360 | lr 0.000396526 | gnorm 1.121 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 17321
2022-03-04 16:23:13 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 16:23:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:24:57 | INFO | train_inner | epoch 067:     40 / 97 loss=3.041, nll_loss=2.537, ppl=5.8, wps=24639.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=6400, lr=0.000395285, gnorm=1.127, loss_scale=32, train_wall=235, gb_free=21, wall=17425
2022-03-04 16:26:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:27:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:27:29 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 10.241 | nll_loss 9.893 | ppl 950.77 | wps 43793.1 | wpb 510.9 | bsz 1 | num_updates 6456 | best_loss 7.701
2022-03-04 16:27:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 6456 updates
2022-03-04 16:27:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:27:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:27:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 67 @ 6456 updates, score 10.241) (writing took 2.237006305716932 seconds)
2022-03-04 16:27:31 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 16:27:31 | INFO | train | epoch 067 | loss 3.015 | nll_loss 2.508 | ppl 5.69 | wps 24379.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6456 | lr 0.000393567 | gnorm 1.128 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 17579
2022-03-04 16:27:31 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 16:27:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:29:25 | INFO | train_inner | epoch 068:     44 / 97 loss=2.998, nll_loss=2.49, ppl=5.62, wps=24416.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=6500, lr=0.000392232, gnorm=1.12, loss_scale=16, train_wall=238, gb_free=21, wall=17693
2022-03-04 16:31:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:31:47 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 10.32 | nll_loss 9.971 | ppl 1003.78 | wps 43912.3 | wpb 510.9 | bsz 1 | num_updates 6553 | best_loss 7.701
2022-03-04 16:31:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 6553 updates
2022-03-04 16:31:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:31:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:31:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 68 @ 6553 updates, score 10.32) (writing took 2.248557349666953 seconds)
2022-03-04 16:31:49 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 16:31:49 | INFO | train | epoch 068 | loss 2.973 | nll_loss 2.464 | ppl 5.52 | wps 24615.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6553 | lr 0.000390643 | gnorm 1.116 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 17837
2022-03-04 16:31:49 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 16:31:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:33:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:33:54 | INFO | train_inner | epoch 069:     48 / 97 loss=2.956, nll_loss=2.446, ppl=5.45, wps=24400.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=6600, lr=0.000389249, gnorm=1.112, loss_scale=16, train_wall=238, gb_free=21, wall=17961
2022-03-04 16:36:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:36:05 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 10.365 | nll_loss 10.016 | ppl 1035.15 | wps 43966.5 | wpb 510.9 | bsz 1 | num_updates 6649 | best_loss 7.701
2022-03-04 16:36:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 6649 updates
2022-03-04 16:36:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:36:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:36:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 69 @ 6649 updates, score 10.365) (writing took 2.3911075638607144 seconds)
2022-03-04 16:36:07 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 16:36:07 | INFO | train | epoch 069 | loss 2.931 | nll_loss 2.42 | ppl 5.35 | wps 24375.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6649 | lr 0.000387813 | gnorm 1.091 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 18095
2022-03-04 16:36:07 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 16:36:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:38:19 | INFO | train_inner | epoch 070:     51 / 97 loss=2.907, nll_loss=2.394, ppl=5.26, wps=24652.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=6700, lr=0.000386334, gnorm=1.103, loss_scale=16, train_wall=235, gb_free=21, wall=18227
2022-03-04 16:40:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:40:23 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 10.508 | nll_loss 10.159 | ppl 1143.09 | wps 43881 | wpb 510.9 | bsz 1 | num_updates 6746 | best_loss 7.701
2022-03-04 16:40:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 6746 updates
2022-03-04 16:40:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:40:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:40:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 70 @ 6746 updates, score 10.508) (writing took 2.248573982156813 seconds)
2022-03-04 16:40:25 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 16:40:25 | INFO | train | epoch 070 | loss 2.894 | nll_loss 2.38 | ppl 5.21 | wps 24634.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6746 | lr 0.000385014 | gnorm 1.12 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 18353
2022-03-04 16:40:25 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 16:40:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:41:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:42:47 | INFO | train_inner | epoch 071:     55 / 97 loss=2.878, nll_loss=2.363, ppl=5.14, wps=24412.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=6800, lr=0.000383482, gnorm=1.119, loss_scale=16, train_wall=238, gb_free=21, wall=18495
2022-03-04 16:44:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:44:41 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 10.574 | nll_loss 10.225 | ppl 1196.65 | wps 43368.3 | wpb 510.9 | bsz 1 | num_updates 6842 | best_loss 7.701
2022-03-04 16:44:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 6842 updates
2022-03-04 16:44:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:44:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:44:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 71 @ 6842 updates, score 10.574) (writing took 2.2523909341543913 seconds)
2022-03-04 16:44:43 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 16:44:43 | INFO | train | epoch 071 | loss 2.853 | nll_loss 2.337 | ppl 5.05 | wps 24376.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6842 | lr 0.000382304 | gnorm 1.118 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 18611
2022-03-04 16:44:43 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 16:44:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:47:14 | INFO | train_inner | epoch 072:     58 / 97 loss=2.826, nll_loss=2.309, ppl=4.95, wps=24567.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=6900, lr=0.000380693, gnorm=1.108, loss_scale=16, train_wall=236, gb_free=21, wall=18762
2022-03-04 16:48:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:49:01 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 10.602 | nll_loss 10.256 | ppl 1222.9 | wps 42301.5 | wpb 510.9 | bsz 1 | num_updates 6939 | best_loss 7.701
2022-03-04 16:49:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 6939 updates
2022-03-04 16:49:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:49:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:49:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 72 @ 6939 updates, score 10.602) (writing took 2.426347414031625 seconds)
2022-03-04 16:49:04 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 16:49:04 | INFO | train | epoch 072 | loss 2.816 | nll_loss 2.298 | ppl 4.92 | wps 24377.9 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 6939 | lr 0.000379622 | gnorm 1.111 | loss_scale 32 | train_wall 230 | gb_free 21 | wall 18872
2022-03-04 16:49:04 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 16:49:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:51:43 | INFO | train_inner | epoch 073:     61 / 97 loss=2.793, nll_loss=2.273, ppl=4.83, wps=24308.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7000, lr=0.000377964, gnorm=1.123, loss_scale=32, train_wall=238, gb_free=21, wall=19031
2022-03-04 16:52:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:53:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:53:23 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 10.697 | nll_loss 10.347 | ppl 1302.64 | wps 42228.6 | wpb 510.9 | bsz 1 | num_updates 7035 | best_loss 7.701
2022-03-04 16:53:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 7035 updates
2022-03-04 16:53:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:53:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:53:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 73 @ 7035 updates, score 10.697) (writing took 2.3742086673155427 seconds)
2022-03-04 16:53:25 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 16:53:25 | INFO | train | epoch 073 | loss 2.778 | nll_loss 2.257 | ppl 4.78 | wps 24039.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7035 | lr 0.000377023 | gnorm 1.118 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 19133
2022-03-04 16:53:25 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 16:53:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:56:16 | INFO | train_inner | epoch 074:     65 / 97 loss=2.767, nll_loss=2.244, ppl=4.74, wps=24066.7, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=7100, lr=0.000375293, gnorm=1.124, loss_scale=16, train_wall=240, gb_free=21, wall=19303
2022-03-04 16:57:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:57:44 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 10.77 | nll_loss 10.433 | ppl 1382.21 | wps 42730.5 | wpb 510.9 | bsz 1 | num_updates 7132 | best_loss 7.701
2022-03-04 16:57:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 7132 updates
2022-03-04 16:57:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:57:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 16:57:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 74 @ 7132 updates, score 10.77) (writing took 2.327979733236134 seconds)
2022-03-04 16:57:47 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 16:57:47 | INFO | train | epoch 074 | loss 2.745 | nll_loss 2.222 | ppl 4.66 | wps 24300.9 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 7132 | lr 0.00037445 | gnorm 1.115 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 19395
2022-03-04 16:57:47 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 16:57:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:58:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:00:47 | INFO | train_inner | epoch 075:     69 / 97 loss=2.718, nll_loss=2.193, ppl=4.57, wps=24094.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7200, lr=0.000372678, gnorm=1.121, loss_scale=16, train_wall=240, gb_free=21, wall=19575
2022-03-04 17:02:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:02:06 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 10.859 | nll_loss 10.513 | ppl 1461.5 | wps 42429.3 | wpb 510.9 | bsz 1 | num_updates 7228 | best_loss 7.701
2022-03-04 17:02:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 7228 updates
2022-03-04 17:02:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:02:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:02:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 75 @ 7228 updates, score 10.859) (writing took 2.39001427218318 seconds)
2022-03-04 17:02:08 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-04 17:02:08 | INFO | train | epoch 075 | loss 2.71 | nll_loss 2.185 | ppl 4.55 | wps 24027.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7228 | lr 0.000371955 | gnorm 1.135 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 19656
2022-03-04 17:02:08 | INFO | fairseq.trainer | begin training epoch 76
2022-03-04 17:02:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:05:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:05:20 | INFO | train_inner | epoch 076:     73 / 97 loss=2.687, nll_loss=2.161, ppl=4.47, wps=24045.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7300, lr=0.000370117, gnorm=1.12, loss_scale=16, train_wall=240, gb_free=21, wall=19848
2022-03-04 17:06:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:06:28 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 10.946 | nll_loss 10.6 | ppl 1552.58 | wps 42573 | wpb 510.9 | bsz 1 | num_updates 7324 | best_loss 7.701
2022-03-04 17:06:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 7324 updates
2022-03-04 17:06:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:06:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:06:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 76 @ 7324 updates, score 10.946) (writing took 2.3467779625207186 seconds)
2022-03-04 17:06:30 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-04 17:06:30 | INFO | train | epoch 076 | loss 2.677 | nll_loss 2.15 | ppl 4.44 | wps 24012.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7324 | lr 0.00036951 | gnorm 1.12 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 19918
2022-03-04 17:06:30 | INFO | fairseq.trainer | begin training epoch 77
2022-03-04 17:06:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:09:49 | INFO | train_inner | epoch 077:     76 / 97 loss=2.653, nll_loss=2.124, ppl=4.36, wps=24295.4, ups=0.37, wpb=65495, bsz=127.9, num_updates=7400, lr=0.000367607, gnorm=1.121, loss_scale=16, train_wall=238, gb_free=21, wall=20117
2022-03-04 17:10:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:10:50 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 10.945 | nll_loss 10.601 | ppl 1552.87 | wps 41863.2 | wpb 510.9 | bsz 1 | num_updates 7421 | best_loss 7.701
2022-03-04 17:10:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 7421 updates
2022-03-04 17:10:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:10:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:10:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 77 @ 7421 updates, score 10.945) (writing took 2.3033444145694375 seconds)
2022-03-04 17:10:52 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-04 17:10:52 | INFO | train | epoch 077 | loss 2.644 | nll_loss 2.114 | ppl 4.33 | wps 24276.2 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 7421 | lr 0.000367087 | gnorm 1.113 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 20180
2022-03-04 17:10:52 | INFO | fairseq.trainer | begin training epoch 78
2022-03-04 17:10:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:12:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:14:21 | INFO | train_inner | epoch 078:     80 / 97 loss=2.623, nll_loss=2.092, ppl=4.26, wps=24098.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=7500, lr=0.000365148, gnorm=1.111, loss_scale=16, train_wall=240, gb_free=21, wall=20389
2022-03-04 17:15:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:15:11 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 11.085 | nll_loss 10.738 | ppl 1707.65 | wps 42651.6 | wpb 510.9 | bsz 1 | num_updates 7517 | best_loss 7.701
2022-03-04 17:15:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 7517 updates
2022-03-04 17:15:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:15:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:15:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 78 @ 7517 updates, score 11.085) (writing took 2.364098164252937 seconds)
2022-03-04 17:15:13 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-04 17:15:13 | INFO | train | epoch 078 | loss 2.614 | nll_loss 2.083 | ppl 4.24 | wps 24068 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7517 | lr 0.000364735 | gnorm 1.121 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 20441
2022-03-04 17:15:13 | INFO | fairseq.trainer | begin training epoch 79
2022-03-04 17:15:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:18:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:18:53 | INFO | train_inner | epoch 079:     84 / 97 loss=2.592, nll_loss=2.059, ppl=4.17, wps=24065.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7600, lr=0.000362738, gnorm=1.101, loss_scale=16, train_wall=240, gb_free=21, wall=20661
2022-03-04 17:19:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:19:32 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 11.111 | nll_loss 10.77 | ppl 1745.61 | wps 42352.8 | wpb 510.9 | bsz 1 | num_updates 7613 | best_loss 7.701
2022-03-04 17:19:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 7613 updates
2022-03-04 17:19:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:19:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:19:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 79 @ 7613 updates, score 11.111) (writing took 2.311524015851319 seconds)
2022-03-04 17:19:35 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-04 17:19:35 | INFO | train | epoch 079 | loss 2.583 | nll_loss 2.049 | ppl 4.14 | wps 24022.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7613 | lr 0.000362428 | gnorm 1.09 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 20703
2022-03-04 17:19:35 | INFO | fairseq.trainer | begin training epoch 80
2022-03-04 17:19:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:23:23 | INFO | train_inner | epoch 080:     87 / 97 loss=2.56, nll_loss=2.025, ppl=4.07, wps=24288.7, ups=0.37, wpb=65495, bsz=127.9, num_updates=7700, lr=0.000360375, gnorm=1.119, loss_scale=16, train_wall=238, gb_free=21, wall=20931
2022-03-04 17:23:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:23:54 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 11.181 | nll_loss 10.841 | ppl 1834.08 | wps 42470.7 | wpb 510.9 | bsz 1 | num_updates 7710 | best_loss 7.701
2022-03-04 17:23:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 7710 updates
2022-03-04 17:23:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:23:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:23:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 80 @ 7710 updates, score 11.181) (writing took 2.3074309853836894 seconds)
2022-03-04 17:23:56 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-04 17:23:56 | INFO | train | epoch 080 | loss 2.555 | nll_loss 2.02 | ppl 4.06 | wps 24274.5 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 7710 | lr 0.000360141 | gnorm 1.114 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 20964
2022-03-04 17:23:57 | INFO | fairseq.trainer | begin training epoch 81
2022-03-04 17:23:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:25:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:27:55 | INFO | train_inner | epoch 081:     91 / 97 loss=2.535, nll_loss=1.999, ppl=4, wps=24082.1, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=7800, lr=0.000358057, gnorm=1.137, loss_scale=16, train_wall=240, gb_free=21, wall=21203
2022-03-04 17:28:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:28:16 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 11.221 | nll_loss 10.883 | ppl 1888.26 | wps 42059.3 | wpb 510.9 | bsz 1 | num_updates 7806 | best_loss 7.701
2022-03-04 17:28:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 7806 updates
2022-03-04 17:28:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:28:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:28:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 81 @ 7806 updates, score 11.221) (writing took 2.382041350007057 seconds)
2022-03-04 17:28:18 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-04 17:28:18 | INFO | train | epoch 081 | loss 2.528 | nll_loss 1.991 | ppl 3.98 | wps 24029.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7806 | lr 0.00035792 | gnorm 1.137 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 21226
2022-03-04 17:28:18 | INFO | fairseq.trainer | begin training epoch 82
2022-03-04 17:28:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:32:24 | INFO | train_inner | epoch 082:     94 / 97 loss=2.502, nll_loss=1.964, ppl=3.9, wps=24304.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7900, lr=0.000355784, gnorm=1.1, loss_scale=32, train_wall=238, gb_free=21, wall=21472
2022-03-04 17:32:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:32:37 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 11.311 | nll_loss 10.97 | ppl 2005.38 | wps 42512.5 | wpb 510.9 | bsz 1 | num_updates 7903 | best_loss 7.701
2022-03-04 17:32:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 7903 updates
2022-03-04 17:32:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:32:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:32:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 82 @ 7903 updates, score 11.311) (writing took 2.303151300176978 seconds)
2022-03-04 17:32:40 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-04 17:32:40 | INFO | train | epoch 082 | loss 2.499 | nll_loss 1.96 | ppl 3.89 | wps 24299 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 7903 | lr 0.000355716 | gnorm 1.102 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 21487
2022-03-04 17:32:40 | INFO | fairseq.trainer | begin training epoch 83
2022-03-04 17:32:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:33:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:36:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:36:59 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 11.394 | nll_loss 11.055 | ppl 2127.82 | wps 42307.2 | wpb 510.9 | bsz 1 | num_updates 7999 | best_loss 7.701
2022-03-04 17:36:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 7999 updates
2022-03-04 17:36:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:37:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:37:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 83 @ 7999 updates, score 11.394) (writing took 2.3307388992980123 seconds)
2022-03-04 17:37:01 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-04 17:37:01 | INFO | train | epoch 083 | loss 2.472 | nll_loss 1.931 | ppl 3.81 | wps 24012.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7999 | lr 0.000353575 | gnorm 1.124 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 21749
2022-03-04 17:37:01 | INFO | fairseq.trainer | begin training epoch 84
2022-03-04 17:37:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:37:04 | INFO | train_inner | epoch 084:      1 / 97 loss=2.474, nll_loss=1.934, ppl=3.82, wps=23392.3, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=8000, lr=0.000353553, gnorm=1.124, loss_scale=16, train_wall=240, gb_free=21, wall=21752
2022-03-04 17:40:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:41:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:41:21 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 11.418 | nll_loss 11.077 | ppl 2160.88 | wps 42478.6 | wpb 510.9 | bsz 1 | num_updates 8095 | best_loss 7.701
2022-03-04 17:41:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 8095 updates
2022-03-04 17:41:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:41:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:41:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 84 @ 8095 updates, score 11.418) (writing took 2.3485838333144784 seconds)
2022-03-04 17:41:23 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-04 17:41:23 | INFO | train | epoch 084 | loss 2.445 | nll_loss 1.903 | ppl 3.74 | wps 24036.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8095 | lr 0.000351473 | gnorm 1.103 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 22011
2022-03-04 17:41:23 | INFO | fairseq.trainer | begin training epoch 85
2022-03-04 17:41:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:41:36 | INFO | train_inner | epoch 085:      5 / 97 loss=2.441, nll_loss=1.899, ppl=3.73, wps=24073, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8100, lr=0.000351364, gnorm=1.101, loss_scale=16, train_wall=240, gb_free=21, wall=22024
2022-03-04 17:45:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:45:42 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 11.472 | nll_loss 11.131 | ppl 2242.6 | wps 42521.9 | wpb 510.9 | bsz 1 | num_updates 8192 | best_loss 7.701
2022-03-04 17:45:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 8192 updates
2022-03-04 17:45:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:45:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:45:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 85 @ 8192 updates, score 11.472) (writing took 2.3234928343445063 seconds)
2022-03-04 17:45:44 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-04 17:45:44 | INFO | train | epoch 085 | loss 2.423 | nll_loss 1.879 | ppl 3.68 | wps 24304.7 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 8192 | lr 0.000349386 | gnorm 1.099 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 22272
2022-03-04 17:45:44 | INFO | fairseq.trainer | begin training epoch 86
2022-03-04 17:45:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:46:05 | INFO | train_inner | epoch 086:      8 / 97 loss=2.418, nll_loss=1.874, ppl=3.66, wps=24324.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8200, lr=0.000349215, gnorm=1.099, loss_scale=32, train_wall=238, gb_free=21, wall=22293
2022-03-04 17:46:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:49:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:50:03 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 11.504 | nll_loss 11.165 | ppl 2295.55 | wps 42350.2 | wpb 510.9 | bsz 1 | num_updates 8288 | best_loss 7.701
2022-03-04 17:50:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 8288 updates
2022-03-04 17:50:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:50:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:50:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 86 @ 8288 updates, score 11.504) (writing took 2.3596247294917703 seconds)
2022-03-04 17:50:06 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-04 17:50:06 | INFO | train | epoch 086 | loss 2.397 | nll_loss 1.852 | ppl 3.61 | wps 24049.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8288 | lr 0.000347356 | gnorm 1.116 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 22534
2022-03-04 17:50:06 | INFO | fairseq.trainer | begin training epoch 87
2022-03-04 17:50:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:50:37 | INFO | train_inner | epoch 087:     12 / 97 loss=2.392, nll_loss=1.846, ppl=3.6, wps=24090.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8300, lr=0.000347105, gnorm=1.118, loss_scale=16, train_wall=240, gb_free=21, wall=22565
2022-03-04 17:52:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:54:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:54:25 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 11.532 | nll_loss 11.192 | ppl 2339.7 | wps 42241.4 | wpb 510.9 | bsz 1 | num_updates 8384 | best_loss 7.701
2022-03-04 17:54:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 8384 updates
2022-03-04 17:54:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:54:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:54:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 87 @ 8384 updates, score 11.532) (writing took 2.328220716677606 seconds)
2022-03-04 17:54:27 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-04 17:54:27 | INFO | train | epoch 087 | loss 2.373 | nll_loss 1.826 | ppl 3.55 | wps 24031.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8384 | lr 0.000345362 | gnorm 1.105 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 22795
2022-03-04 17:54:27 | INFO | fairseq.trainer | begin training epoch 88
2022-03-04 17:54:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:55:09 | INFO | train_inner | epoch 088:     16 / 97 loss=2.366, nll_loss=1.819, ppl=3.53, wps=24072, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8400, lr=0.000345033, gnorm=1.099, loss_scale=16, train_wall=240, gb_free=21, wall=22837
2022-03-04 17:58:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:58:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:58:47 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 11.597 | nll_loss 11.257 | ppl 2447.11 | wps 42261.3 | wpb 510.9 | bsz 1 | num_updates 8480 | best_loss 7.701
2022-03-04 17:58:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 8480 updates
2022-03-04 17:58:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:58:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 17:58:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 88 @ 8480 updates, score 11.597) (writing took 2.3923213006928563 seconds)
2022-03-04 17:58:49 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-04 17:58:49 | INFO | train | epoch 088 | loss 2.35 | nll_loss 1.802 | ppl 3.49 | wps 24039.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8480 | lr 0.000343401 | gnorm 1.113 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 23057
2022-03-04 17:58:49 | INFO | fairseq.trainer | begin training epoch 89
2022-03-04 17:58:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:59:41 | INFO | train_inner | epoch 089:     20 / 97 loss=2.345, nll_loss=1.797, ppl=3.47, wps=24076.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8500, lr=0.000342997, gnorm=1.118, loss_scale=16, train_wall=240, gb_free=21, wall=23109
2022-03-04 18:03:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:03:08 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 11.702 | nll_loss 11.367 | ppl 2641.04 | wps 42867 | wpb 510.9 | bsz 1 | num_updates 8577 | best_loss 7.701
2022-03-04 18:03:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 8577 updates
2022-03-04 18:03:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:03:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:03:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 89 @ 8577 updates, score 11.702) (writing took 2.2841596882790327 seconds)
2022-03-04 18:03:11 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-04 18:03:11 | INFO | train | epoch 089 | loss 2.327 | nll_loss 1.777 | ppl 3.43 | wps 24279.7 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 8577 | lr 0.000341454 | gnorm 1.107 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 23318
2022-03-04 18:03:11 | INFO | fairseq.trainer | begin training epoch 90
2022-03-04 18:03:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:04:11 | INFO | train_inner | epoch 090:     23 / 97 loss=2.319, nll_loss=1.769, ppl=3.41, wps=24299.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8600, lr=0.000340997, gnorm=1.099, loss_scale=16, train_wall=238, gb_free=21, wall=23379
2022-03-04 18:07:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:07:30 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 11.713 | nll_loss 11.378 | ppl 2660.72 | wps 42153.5 | wpb 510.9 | bsz 1 | num_updates 8674 | best_loss 7.701
2022-03-04 18:07:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 8674 updates
2022-03-04 18:07:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:07:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:07:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 90 @ 8674 updates, score 11.713) (writing took 2.3836137764155865 seconds)
2022-03-04 18:07:32 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-04 18:07:32 | INFO | train | epoch 090 | loss 2.306 | nll_loss 1.755 | ppl 3.38 | wps 24259.4 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 8674 | lr 0.000339539 | gnorm 1.1 | loss_scale 32 | train_wall 231 | gb_free 21 | wall 23580
2022-03-04 18:07:32 | INFO | fairseq.trainer | begin training epoch 91
2022-03-04 18:07:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:08:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:08:43 | INFO | train_inner | epoch 091:     27 / 97 loss=2.302, nll_loss=1.751, ppl=3.36, wps=24052.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8700, lr=0.000339032, gnorm=1.11, loss_scale=16, train_wall=240, gb_free=21, wall=23651
2022-03-04 18:11:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:11:52 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 11.799 | nll_loss 11.464 | ppl 2824.13 | wps 41935.5 | wpb 510.9 | bsz 1 | num_updates 8770 | best_loss 7.701
2022-03-04 18:11:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 8770 updates
2022-03-04 18:11:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:11:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:11:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 91 @ 8770 updates, score 11.799) (writing took 2.3880381509661674 seconds)
2022-03-04 18:11:54 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-04 18:11:54 | INFO | train | epoch 091 | loss 2.284 | nll_loss 1.732 | ppl 3.32 | wps 23998.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8770 | lr 0.000337676 | gnorm 1.096 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 23842
2022-03-04 18:11:54 | INFO | fairseq.trainer | begin training epoch 92
2022-03-04 18:11:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:13:13 | INFO | train_inner | epoch 092:     30 / 97 loss=2.275, nll_loss=1.722, ppl=3.3, wps=24260.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=8800, lr=0.0003371, gnorm=1.08, loss_scale=16, train_wall=238, gb_free=21, wall=23921
2022-03-04 18:15:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:16:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:16:14 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 11.839 | nll_loss 11.507 | ppl 2911.3 | wps 42708.7 | wpb 510.9 | bsz 1 | num_updates 8866 | best_loss 7.701
2022-03-04 18:16:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 8866 updates
2022-03-04 18:16:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:16:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:16:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 92 @ 8866 updates, score 11.839) (writing took 2.277931075543165 seconds)
2022-03-04 18:16:16 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-04 18:16:16 | INFO | train | epoch 092 | loss 2.264 | nll_loss 1.711 | ppl 3.27 | wps 24023.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8866 | lr 0.000335843 | gnorm 1.096 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 24104
2022-03-04 18:16:16 | INFO | fairseq.trainer | begin training epoch 93
2022-03-04 18:16:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:17:45 | INFO | train_inner | epoch 093:     34 / 97 loss=2.256, nll_loss=1.702, ppl=3.25, wps=24055.1, ups=0.37, wpb=65495, bsz=127.9, num_updates=8900, lr=0.000335201, gnorm=1.096, loss_scale=16, train_wall=241, gb_free=21, wall=24193
2022-03-04 18:20:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:20:36 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 11.915 | nll_loss 11.583 | ppl 3068.74 | wps 42153.2 | wpb 510.9 | bsz 1 | num_updates 8963 | best_loss 7.701
2022-03-04 18:20:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 8963 updates
2022-03-04 18:20:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:20:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:20:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 93 @ 8963 updates, score 11.915) (writing took 2.373756444081664 seconds)
2022-03-04 18:20:38 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-04 18:20:38 | INFO | train | epoch 093 | loss 2.244 | nll_loss 1.69 | ppl 3.23 | wps 24260.5 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 8963 | lr 0.000334021 | gnorm 1.103 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 24366
2022-03-04 18:20:38 | INFO | fairseq.trainer | begin training epoch 94
2022-03-04 18:20:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:21:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:22:18 | INFO | train_inner | epoch 094:     38 / 97 loss=2.236, nll_loss=1.681, ppl=3.21, wps=24057.7, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=9000, lr=0.000333333, gnorm=1.108, loss_scale=16, train_wall=240, gb_free=21, wall=24466
2022-03-04 18:24:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:24:57 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 11.916 | nll_loss 11.584 | ppl 3070.25 | wps 42341.4 | wpb 510.9 | bsz 1 | num_updates 9059 | best_loss 7.701
2022-03-04 18:24:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 9059 updates
2022-03-04 18:24:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:25:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:25:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 94 @ 9059 updates, score 11.916) (writing took 2.36118106264621 seconds)
2022-03-04 18:25:00 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-04 18:25:00 | INFO | train | epoch 094 | loss 2.224 | nll_loss 1.668 | ppl 3.18 | wps 24017.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9059 | lr 0.000332246 | gnorm 1.088 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 24628
2022-03-04 18:25:00 | INFO | fairseq.trainer | begin training epoch 95
2022-03-04 18:25:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:26:47 | INFO | train_inner | epoch 095:     41 / 97 loss=2.218, nll_loss=1.662, ppl=3.16, wps=24278.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=9100, lr=0.000331497, gnorm=1.091, loss_scale=16, train_wall=238, gb_free=21, wall=24735
2022-03-04 18:27:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:29:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:29:19 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 11.971 | nll_loss 11.637 | ppl 3184.46 | wps 41920 | wpb 510.9 | bsz 1 | num_updates 9155 | best_loss 7.701
2022-03-04 18:29:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 9155 updates
2022-03-04 18:29:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:29:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:29:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 95 @ 9155 updates, score 11.971) (writing took 2.2988629434257746 seconds)
2022-03-04 18:29:22 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-04 18:29:22 | INFO | train | epoch 095 | loss 2.205 | nll_loss 1.648 | ppl 3.13 | wps 24011.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9155 | lr 0.0003305 | gnorm 1.1 | loss_scale 16 | train_wall 231 | gb_free 21 | wall 24889
2022-03-04 18:29:22 | INFO | fairseq.trainer | begin training epoch 96
2022-03-04 18:29:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:31:19 | INFO | train_inner | epoch 096:     45 / 97 loss=2.196, nll_loss=1.639, ppl=3.11, wps=24073.4, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=9200, lr=0.00032969, gnorm=1.096, loss_scale=16, train_wall=240, gb_free=21, wall=25007
2022-03-04 18:33:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:33:39 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 11.982 | nll_loss 11.65 | ppl 3212.81 | wps 43740.1 | wpb 510.9 | bsz 1 | num_updates 9252 | best_loss 7.701
2022-03-04 18:33:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 9252 updates
2022-03-04 18:33:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:33:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:33:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 96 @ 9252 updates, score 11.982) (writing took 2.3807907989248633 seconds)
2022-03-04 18:33:41 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-04 18:33:41 | INFO | train | epoch 096 | loss 2.187 | nll_loss 1.629 | ppl 3.09 | wps 24474.2 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 9252 | lr 0.000328762 | gnorm 1.091 | loss_scale 32 | train_wall 229 | gb_free 21 | wall 25149
2022-03-04 18:33:41 | INFO | fairseq.trainer | begin training epoch 97
2022-03-04 18:33:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:34:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:35:48 | INFO | train_inner | epoch 097:     49 / 97 loss=2.177, nll_loss=1.618, ppl=3.07, wps=24395.9, ups=0.37, wpb=65495, bsz=127.9, num_updates=9300, lr=0.000327913, gnorm=1.079, loss_scale=16, train_wall=238, gb_free=21, wall=25276
2022-03-04 18:37:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:37:57 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 12.123 | nll_loss 11.793 | ppl 3549.59 | wps 43736.5 | wpb 510.9 | bsz 1 | num_updates 9348 | best_loss 7.701
2022-03-04 18:37:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 9348 updates
2022-03-04 18:37:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:37:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:37:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 97 @ 9348 updates, score 12.123) (writing took 2.530720659531653 seconds)
2022-03-04 18:37:59 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-04 18:37:59 | INFO | train | epoch 097 | loss 2.165 | nll_loss 1.606 | ppl 3.04 | wps 24341.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9348 | lr 0.00032707 | gnorm 1.069 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 25407
2022-03-04 18:37:59 | INFO | fairseq.trainer | begin training epoch 98
2022-03-04 18:37:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:40:14 | INFO | train_inner | epoch 098:     52 / 97 loss=2.157, nll_loss=1.597, ppl=3.03, wps=24626.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=9400, lr=0.000326164, gnorm=1.075, loss_scale=16, train_wall=235, gb_free=21, wall=25542
2022-03-04 18:40:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:42:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:42:15 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 12.129 | nll_loss 11.799 | ppl 3563.74 | wps 43945.1 | wpb 510.9 | bsz 1 | num_updates 9444 | best_loss 7.701
2022-03-04 18:42:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 9444 updates
2022-03-04 18:42:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:42:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:42:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 98 @ 9444 updates, score 12.129) (writing took 2.518486146815121 seconds)
2022-03-04 18:42:17 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-04 18:42:17 | INFO | train | epoch 098 | loss 2.15 | nll_loss 1.59 | ppl 3.01 | wps 24365.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9444 | lr 0.000325403 | gnorm 1.092 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 25665
2022-03-04 18:42:17 | INFO | fairseq.trainer | begin training epoch 99
2022-03-04 18:42:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:44:42 | INFO | train_inner | epoch 099:     56 / 97 loss=2.143, nll_loss=1.582, ppl=2.99, wps=24399.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=9500, lr=0.000324443, gnorm=1.095, loss_scale=16, train_wall=238, gb_free=21, wall=25810
2022-03-04 18:46:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:46:33 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 12.102 | nll_loss 11.77 | ppl 3492.74 | wps 43795.7 | wpb 510.9 | bsz 1 | num_updates 9541 | best_loss 7.701
2022-03-04 18:46:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 9541 updates
2022-03-04 18:46:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:46:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:46:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 99 @ 9541 updates, score 12.102) (writing took 2.4766209963709116 seconds)
2022-03-04 18:46:36 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-04 18:46:36 | INFO | train | epoch 099 | loss 2.133 | nll_loss 1.572 | ppl 2.97 | wps 24602.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9541 | lr 0.000323745 | gnorm 1.087 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 25924
2022-03-04 18:46:36 | INFO | fairseq.trainer | begin training epoch 100
2022-03-04 18:46:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:47:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:49:11 | INFO | train_inner | epoch 100:     60 / 97 loss=2.122, nll_loss=1.561, ppl=2.95, wps=24389.3, ups=0.37, wpb=65495, bsz=127.9, num_updates=9600, lr=0.000322749, gnorm=1.084, loss_scale=16, train_wall=238, gb_free=21, wall=26079
2022-03-04 18:50:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:50:51 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 12.164 | nll_loss 11.837 | ppl 3659.25 | wps 43692.5 | wpb 510.9 | bsz 1 | num_updates 9637 | best_loss 7.701
2022-03-04 18:50:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 9637 updates
2022-03-04 18:50:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:50:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:50:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 100 @ 9637 updates, score 12.164) (writing took 2.5527457017451525 seconds)
2022-03-04 18:50:54 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-04 18:50:54 | INFO | train | epoch 100 | loss 2.113 | nll_loss 1.551 | ppl 2.93 | wps 24340.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9637 | lr 0.000322128 | gnorm 1.086 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 26182
2022-03-04 18:50:54 | INFO | fairseq.trainer | begin training epoch 101
2022-03-04 18:50:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:53:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:53:40 | INFO | train_inner | epoch 101:     64 / 97 loss=2.104, nll_loss=1.542, ppl=2.91, wps=24375.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=9700, lr=0.000321081, gnorm=1.087, loss_scale=16, train_wall=238, gb_free=21, wall=26347
2022-03-04 18:55:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:55:10 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 12.192 | nll_loss 11.861 | ppl 3718.5 | wps 43862.1 | wpb 510.9 | bsz 1 | num_updates 9733 | best_loss 7.701
2022-03-04 18:55:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 9733 updates
2022-03-04 18:55:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:55:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:55:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 101 @ 9733 updates, score 12.192) (writing took 2.4800630854442716 seconds)
2022-03-04 18:55:12 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-04 18:55:12 | INFO | train | epoch 101 | loss 2.098 | nll_loss 1.536 | ppl 2.9 | wps 24347.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9733 | lr 0.000320536 | gnorm 1.084 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 26440
2022-03-04 18:55:12 | INFO | fairseq.trainer | begin training epoch 102
2022-03-04 18:55:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:58:05 | INFO | train_inner | epoch 102:     67 / 97 loss=2.092, nll_loss=1.528, ppl=2.88, wps=24627.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=9800, lr=0.000319438, gnorm=1.096, loss_scale=16, train_wall=235, gb_free=21, wall=26613
2022-03-04 18:58:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:59:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:59:28 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 12.186 | nll_loss 11.86 | ppl 3716.06 | wps 44246.6 | wpb 510.9 | bsz 1 | num_updates 9829 | best_loss 7.701
2022-03-04 18:59:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 9829 updates
2022-03-04 18:59:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:59:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 18:59:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 102 @ 9829 updates, score 12.186) (writing took 2.5213729329407215 seconds)
2022-03-04 18:59:30 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-04 18:59:30 | INFO | train | epoch 102 | loss 2.084 | nll_loss 1.52 | ppl 2.87 | wps 24351.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9829 | lr 0.000318967 | gnorm 1.101 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 26698
2022-03-04 18:59:30 | INFO | fairseq.trainer | begin training epoch 103
2022-03-04 18:59:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:02:34 | INFO | train_inner | epoch 103:     71 / 97 loss=2.071, nll_loss=1.506, ppl=2.84, wps=24395.4, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=9900, lr=0.000317821, gnorm=1.083, loss_scale=16, train_wall=238, gb_free=21, wall=26882
2022-03-04 19:03:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:03:46 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 12.284 | nll_loss 11.958 | ppl 3978.2 | wps 43712.5 | wpb 510.9 | bsz 1 | num_updates 9926 | best_loss 7.701
2022-03-04 19:03:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 9926 updates
2022-03-04 19:03:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:03:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:03:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 103 @ 9926 updates, score 12.284) (writing took 2.575970536097884 seconds)
2022-03-04 19:03:49 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-04 19:03:49 | INFO | train | epoch 103 | loss 2.067 | nll_loss 1.502 | ppl 2.83 | wps 24603.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9926 | lr 0.000317404 | gnorm 1.075 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 26956
2022-03-04 19:03:49 | INFO | fairseq.trainer | begin training epoch 104
2022-03-04 19:03:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:04:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:07:02 | INFO | train_inner | epoch 104:     75 / 97 loss=2.058, nll_loss=1.492, ppl=2.81, wps=24386, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10000, lr=0.000316228, gnorm=1.075, loss_scale=16, train_wall=238, gb_free=21, wall=27150
2022-03-04 19:07:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:08:04 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 12.328 | nll_loss 12.005 | ppl 4109.84 | wps 43775 | wpb 510.9 | bsz 1 | num_updates 10022 | best_loss 7.701
2022-03-04 19:08:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 10022 updates
2022-03-04 19:08:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:08:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:08:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 104 @ 10022 updates, score 12.328) (writing took 2.4876097943633795 seconds)
2022-03-04 19:08:07 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-04 19:08:07 | INFO | train | epoch 104 | loss 2.052 | nll_loss 1.486 | ppl 2.8 | wps 24351.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10022 | lr 0.00031588 | gnorm 1.082 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 27215
2022-03-04 19:08:07 | INFO | fairseq.trainer | begin training epoch 105
2022-03-04 19:08:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:10:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:11:31 | INFO | train_inner | epoch 105:     79 / 97 loss=2.041, nll_loss=1.475, ppl=2.78, wps=24389.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10100, lr=0.000314658, gnorm=1.07, loss_scale=16, train_wall=238, gb_free=21, wall=27419
2022-03-04 19:12:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:12:22 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 12.295 | nll_loss 11.967 | ppl 4004.47 | wps 43687.2 | wpb 510.9 | bsz 1 | num_updates 10118 | best_loss 7.701
2022-03-04 19:12:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 10118 updates
2022-03-04 19:12:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:12:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:12:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 105 @ 10118 updates, score 12.295) (writing took 2.519722835160792 seconds)
2022-03-04 19:12:25 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-04 19:12:25 | INFO | train | epoch 105 | loss 2.037 | nll_loss 1.471 | ppl 2.77 | wps 24355.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10118 | lr 0.000314378 | gnorm 1.064 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 27473
2022-03-04 19:12:25 | INFO | fairseq.trainer | begin training epoch 106
2022-03-04 19:12:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:15:57 | INFO | train_inner | epoch 106:     82 / 97 loss=2.028, nll_loss=1.461, ppl=2.75, wps=24638, ups=0.38, wpb=65495, bsz=127.9, num_updates=10200, lr=0.000313112, gnorm=1.085, loss_scale=16, train_wall=235, gb_free=21, wall=27685
2022-03-04 19:16:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:16:41 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 12.352 | nll_loss 12.028 | ppl 4176.13 | wps 43786.2 | wpb 510.9 | bsz 1 | num_updates 10215 | best_loss 7.701
2022-03-04 19:16:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 10215 updates
2022-03-04 19:16:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:16:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:16:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 106 @ 10215 updates, score 12.352) (writing took 2.5063019935041666 seconds)
2022-03-04 19:16:43 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-04 19:16:43 | INFO | train | epoch 106 | loss 2.023 | nll_loss 1.456 | ppl 2.74 | wps 24613.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10215 | lr 0.000312882 | gnorm 1.086 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 27731
2022-03-04 19:16:43 | INFO | fairseq.trainer | begin training epoch 107
2022-03-04 19:16:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:19:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:20:25 | INFO | train_inner | epoch 107:     86 / 97 loss=2.013, nll_loss=1.446, ppl=2.72, wps=24376.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=10300, lr=0.000311588, gnorm=1.079, loss_scale=16, train_wall=238, gb_free=21, wall=27953
2022-03-04 19:20:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:20:59 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 12.338 | nll_loss 12.007 | ppl 4116.75 | wps 44252.2 | wpb 510.9 | bsz 1 | num_updates 10311 | best_loss 7.701
2022-03-04 19:20:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 10311 updates
2022-03-04 19:20:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:21:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:21:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 107 @ 10311 updates, score 12.338) (writing took 2.515049785375595 seconds)
2022-03-04 19:21:01 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-04 19:21:01 | INFO | train | epoch 107 | loss 2.008 | nll_loss 1.44 | ppl 2.71 | wps 24341.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10311 | lr 0.000311422 | gnorm 1.076 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 27989
2022-03-04 19:21:01 | INFO | fairseq.trainer | begin training epoch 108
2022-03-04 19:21:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:24:51 | INFO | train_inner | epoch 108:     89 / 97 loss=1.996, nll_loss=1.428, ppl=2.69, wps=24625.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10400, lr=0.000310087, gnorm=1.073, loss_scale=16, train_wall=236, gb_free=21, wall=28219
2022-03-04 19:25:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:25:17 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 12.401 | nll_loss 12.074 | ppl 4312.13 | wps 43694.2 | wpb 510.9 | bsz 1 | num_updates 10408 | best_loss 7.701
2022-03-04 19:25:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 10408 updates
2022-03-04 19:25:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:25:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:25:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 108 @ 10408 updates, score 12.401) (writing took 2.497540984302759 seconds)
2022-03-04 19:25:20 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-04 19:25:20 | INFO | train | epoch 108 | loss 1.993 | nll_loss 1.425 | ppl 2.68 | wps 24604.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10408 | lr 0.000309968 | gnorm 1.066 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 28247
2022-03-04 19:25:20 | INFO | fairseq.trainer | begin training epoch 109
2022-03-04 19:25:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:25:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:29:20 | INFO | train_inner | epoch 109:     93 / 97 loss=1.984, nll_loss=1.415, ppl=2.67, wps=24395.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10500, lr=0.000308607, gnorm=1.068, loss_scale=16, train_wall=238, gb_free=21, wall=28488
2022-03-04 19:29:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:29:35 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 12.401 | nll_loss 12.073 | ppl 4308.83 | wps 43946.6 | wpb 510.9 | bsz 1 | num_updates 10504 | best_loss 7.701
2022-03-04 19:29:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 10504 updates
2022-03-04 19:29:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:29:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:29:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 109 @ 10504 updates, score 12.401) (writing took 2.479632998816669 seconds)
2022-03-04 19:29:38 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-04 19:29:38 | INFO | train | epoch 109 | loss 1.98 | nll_loss 1.411 | ppl 2.66 | wps 24361.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10504 | lr 0.000308548 | gnorm 1.07 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 28505
2022-03-04 19:29:38 | INFO | fairseq.trainer | begin training epoch 110
2022-03-04 19:29:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:31:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:33:48 | INFO | train_inner | epoch 110:     97 / 97 loss=1.968, nll_loss=1.398, ppl=2.64, wps=24402.2, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=10600, lr=0.000307148, gnorm=1.063, loss_scale=16, train_wall=238, gb_free=21, wall=28756
2022-03-04 19:33:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:33:53 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 12.545 | nll_loss 12.222 | ppl 4777.89 | wps 43840.8 | wpb 510.9 | bsz 1 | num_updates 10600 | best_loss 7.701
2022-03-04 19:33:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 10600 updates
2022-03-04 19:33:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:33:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:33:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 110 @ 10600 updates, score 12.545) (writing took 2.5036968057975173 seconds)
2022-03-04 19:33:56 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-04 19:33:56 | INFO | train | epoch 110 | loss 1.966 | nll_loss 1.396 | ppl 2.63 | wps 24363.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10600 | lr 0.000307148 | gnorm 1.062 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 28764
2022-03-04 19:33:56 | INFO | fairseq.trainer | begin training epoch 111
2022-03-04 19:33:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:36:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:38:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:38:12 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 12.545 | nll_loss 12.222 | ppl 4778.93 | wps 43678.2 | wpb 510.9 | bsz 1 | num_updates 10696 | best_loss 7.701
2022-03-04 19:38:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 10696 updates
2022-03-04 19:38:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:38:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:38:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 111 @ 10696 updates, score 12.545) (writing took 2.5150398444384336 seconds)
2022-03-04 19:38:14 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-04 19:38:14 | INFO | train | epoch 111 | loss 1.955 | nll_loss 1.384 | ppl 2.61 | wps 24331.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10696 | lr 0.000305766 | gnorm 1.06 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 29022
2022-03-04 19:38:14 | INFO | fairseq.trainer | begin training epoch 112
2022-03-04 19:38:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:38:25 | INFO | train_inner | epoch 112:      4 / 97 loss=1.952, nll_loss=1.381, ppl=2.6, wps=23697.6, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=10700, lr=0.000305709, gnorm=1.058, loss_scale=16, train_wall=238, gb_free=21, wall=29032
2022-03-04 19:42:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:42:30 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 12.509 | nll_loss 12.186 | ppl 4660.69 | wps 43814.6 | wpb 510.9 | bsz 1 | num_updates 10793 | best_loss 7.701
2022-03-04 19:42:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 10793 updates
2022-03-04 19:42:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:42:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:42:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 112 @ 10793 updates, score 12.509) (writing took 2.4777276953682303 seconds)
2022-03-04 19:42:32 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-04 19:42:32 | INFO | train | epoch 112 | loss 1.94 | nll_loss 1.369 | ppl 2.58 | wps 24608.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10793 | lr 0.000304389 | gnorm 1.065 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 29280
2022-03-04 19:42:32 | INFO | fairseq.trainer | begin training epoch 113
2022-03-04 19:42:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:42:50 | INFO | train_inner | epoch 113:      7 / 97 loss=1.936, nll_loss=1.365, ppl=2.58, wps=24629.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10800, lr=0.00030429, gnorm=1.068, loss_scale=32, train_wall=236, gb_free=21, wall=29298
2022-03-04 19:43:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:46:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:46:48 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 12.594 | nll_loss 12.271 | ppl 4942.46 | wps 43757.3 | wpb 510.9 | bsz 1 | num_updates 10889 | best_loss 7.701
2022-03-04 19:46:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 10889 updates
2022-03-04 19:46:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:46:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:46:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 113 @ 10889 updates, score 12.594) (writing took 2.4698977982625365 seconds)
2022-03-04 19:46:50 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-04 19:46:50 | INFO | train | epoch 113 | loss 1.929 | nll_loss 1.357 | ppl 2.56 | wps 24358.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10889 | lr 0.000303044 | gnorm 1.074 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 29538
2022-03-04 19:46:50 | INFO | fairseq.trainer | begin training epoch 114
2022-03-04 19:46:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:47:19 | INFO | train_inner | epoch 114:     11 / 97 loss=1.927, nll_loss=1.355, ppl=2.56, wps=24399.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10900, lr=0.000302891, gnorm=1.067, loss_scale=16, train_wall=238, gb_free=21, wall=29567
2022-03-04 19:49:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:51:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:51:06 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 12.582 | nll_loss 12.261 | ppl 4907.46 | wps 43679.7 | wpb 510.9 | bsz 1 | num_updates 10985 | best_loss 7.701
2022-03-04 19:51:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 10985 updates
2022-03-04 19:51:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:51:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:51:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 114 @ 10985 updates, score 12.582) (writing took 2.4981102710589767 seconds)
2022-03-04 19:51:09 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-04 19:51:09 | INFO | train | epoch 114 | loss 1.915 | nll_loss 1.342 | ppl 2.54 | wps 24350.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10985 | lr 0.000301717 | gnorm 1.064 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 29796
2022-03-04 19:51:09 | INFO | fairseq.trainer | begin training epoch 115
2022-03-04 19:51:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:51:47 | INFO | train_inner | epoch 115:     15 / 97 loss=1.913, nll_loss=1.341, ppl=2.53, wps=24384.3, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=11000, lr=0.000301511, gnorm=1.067, loss_scale=16, train_wall=238, gb_free=21, wall=29835
2022-03-04 19:55:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:55:24 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 12.631 | nll_loss 12.314 | ppl 5091.05 | wps 43740.5 | wpb 510.9 | bsz 1 | num_updates 11082 | best_loss 7.701
2022-03-04 19:55:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 11082 updates
2022-03-04 19:55:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:55:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:55:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 115 @ 11082 updates, score 12.631) (writing took 2.4918660279363394 seconds)
2022-03-04 19:55:27 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-04 19:55:27 | INFO | train | epoch 115 | loss 1.904 | nll_loss 1.331 | ppl 2.52 | wps 24606.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11082 | lr 0.000300394 | gnorm 1.055 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 30055
2022-03-04 19:55:27 | INFO | fairseq.trainer | begin training epoch 116
2022-03-04 19:55:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:55:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:56:16 | INFO | train_inner | epoch 116:     19 / 97 loss=1.9, nll_loss=1.326, ppl=2.51, wps=24400.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11100, lr=0.00030015, gnorm=1.049, loss_scale=16, train_wall=238, gb_free=21, wall=30104
2022-03-04 19:59:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:59:42 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 12.639 | nll_loss 12.316 | ppl 5099.51 | wps 43840.6 | wpb 510.9 | bsz 1 | num_updates 11178 | best_loss 7.701
2022-03-04 19:59:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 11178 updates
2022-03-04 19:59:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:59:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 19:59:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 116 @ 11178 updates, score 12.639) (writing took 2.492780761793256 seconds)
2022-03-04 19:59:45 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-04 19:59:45 | INFO | train | epoch 116 | loss 1.893 | nll_loss 1.319 | ppl 2.49 | wps 24374.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11178 | lr 0.000299101 | gnorm 1.055 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 30313
2022-03-04 19:59:45 | INFO | fairseq.trainer | begin training epoch 117
2022-03-04 19:59:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:00:42 | INFO | train_inner | epoch 117:     22 / 97 loss=1.889, nll_loss=1.315, ppl=2.49, wps=24647.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=11200, lr=0.000298807, gnorm=1.057, loss_scale=16, train_wall=235, gb_free=21, wall=30369
2022-03-04 20:01:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:03:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:04:00 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 12.66 | nll_loss 12.345 | ppl 5201.17 | wps 43427.9 | wpb 510.9 | bsz 1 | num_updates 11274 | best_loss 7.701
2022-03-04 20:04:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 11274 updates
2022-03-04 20:04:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:04:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:04:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 117 @ 11274 updates, score 12.66) (writing took 2.5278912940993905 seconds)
2022-03-04 20:04:03 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-04 20:04:03 | INFO | train | epoch 117 | loss 1.881 | nll_loss 1.307 | ppl 2.47 | wps 24348.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11274 | lr 0.000297825 | gnorm 1.053 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 30571
2022-03-04 20:04:03 | INFO | fairseq.trainer | begin training epoch 118
2022-03-04 20:04:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:05:10 | INFO | train_inner | epoch 118:     26 / 97 loss=1.875, nll_loss=1.3, ppl=2.46, wps=24386.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11300, lr=0.000297482, gnorm=1.053, loss_scale=16, train_wall=238, gb_free=21, wall=30638
2022-03-04 20:07:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:08:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:08:19 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 12.661 | nll_loss 12.338 | ppl 5178.52 | wps 43685.4 | wpb 510.9 | bsz 1 | num_updates 11370 | best_loss 7.701
2022-03-04 20:08:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 11370 updates
2022-03-04 20:08:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:08:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:08:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 118 @ 11370 updates, score 12.661) (writing took 2.566432516090572 seconds)
2022-03-04 20:08:21 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-04 20:08:21 | INFO | train | epoch 118 | loss 1.868 | nll_loss 1.293 | ppl 2.45 | wps 24346.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11370 | lr 0.000296565 | gnorm 1.044 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 30829
2022-03-04 20:08:21 | INFO | fairseq.trainer | begin training epoch 119
2022-03-04 20:08:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:09:39 | INFO | train_inner | epoch 119:     30 / 97 loss=1.865, nll_loss=1.29, ppl=2.45, wps=24385.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11400, lr=0.000296174, gnorm=1.046, loss_scale=16, train_wall=238, gb_free=21, wall=30907
2022-03-04 20:12:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:12:37 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 12.689 | nll_loss 12.365 | ppl 5274.78 | wps 43903.7 | wpb 510.9 | bsz 1 | num_updates 11467 | best_loss 7.701
2022-03-04 20:12:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 11467 updates
2022-03-04 20:12:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:12:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:12:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 119 @ 11467 updates, score 12.689) (writing took 2.504842145368457 seconds)
2022-03-04 20:12:39 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-04 20:12:39 | INFO | train | epoch 119 | loss 1.86 | nll_loss 1.284 | ppl 2.44 | wps 24610.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11467 | lr 0.000295308 | gnorm 1.074 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 31087
2022-03-04 20:12:39 | INFO | fairseq.trainer | begin training epoch 120
2022-03-04 20:12:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:13:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:14:07 | INFO | train_inner | epoch 120:     34 / 97 loss=1.855, nll_loss=1.28, ppl=2.43, wps=24392.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11500, lr=0.000294884, gnorm=1.06, loss_scale=16, train_wall=238, gb_free=21, wall=31175
2022-03-04 20:16:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:16:55 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 12.73 | nll_loss 12.408 | ppl 5435.85 | wps 44005.5 | wpb 510.9 | bsz 1 | num_updates 11563 | best_loss 7.701
2022-03-04 20:16:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 11563 updates
2022-03-04 20:16:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:16:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:16:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 120 @ 11563 updates, score 12.73) (writing took 2.515897889621556 seconds)
2022-03-04 20:16:57 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-04 20:16:57 | INFO | train | epoch 120 | loss 1.848 | nll_loss 1.272 | ppl 2.41 | wps 24354.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11563 | lr 0.000294079 | gnorm 1.048 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 31345
2022-03-04 20:16:57 | INFO | fairseq.trainer | begin training epoch 121
2022-03-04 20:16:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:18:33 | INFO | train_inner | epoch 121:     37 / 97 loss=1.845, nll_loss=1.269, ppl=2.41, wps=24640.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=11600, lr=0.00029361, gnorm=1.055, loss_scale=16, train_wall=235, gb_free=21, wall=31441
2022-03-04 20:20:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:21:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:21:13 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 12.801 | nll_loss 12.483 | ppl 5725.14 | wps 43750.1 | wpb 510.9 | bsz 1 | num_updates 11659 | best_loss 7.701
2022-03-04 20:21:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 11659 updates
2022-03-04 20:21:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:21:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:21:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 121 @ 11659 updates, score 12.801) (writing took 2.5289107179269195 seconds)
2022-03-04 20:21:15 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-04 20:21:15 | INFO | train | epoch 121 | loss 1.837 | nll_loss 1.261 | ppl 2.4 | wps 24361.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11659 | lr 0.000292866 | gnorm 1.052 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 31603
2022-03-04 20:21:15 | INFO | fairseq.trainer | begin training epoch 122
2022-03-04 20:21:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:23:01 | INFO | train_inner | epoch 122:     41 / 97 loss=1.831, nll_loss=1.255, ppl=2.39, wps=24394.1, ups=0.37, wpb=65495, bsz=127.9, num_updates=11700, lr=0.000292353, gnorm=1.057, loss_scale=16, train_wall=238, gb_free=21, wall=31709
2022-03-04 20:25:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:25:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:25:31 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 12.795 | nll_loss 12.475 | ppl 5693.11 | wps 43764.3 | wpb 510.9 | bsz 1 | num_updates 11755 | best_loss 7.701
2022-03-04 20:25:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 11755 updates
2022-03-04 20:25:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:25:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:25:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 122 @ 11755 updates, score 12.795) (writing took 2.507735088467598 seconds)
2022-03-04 20:25:34 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-04 20:25:34 | INFO | train | epoch 122 | loss 1.825 | nll_loss 1.249 | ppl 2.38 | wps 24360.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11755 | lr 0.000291668 | gnorm 1.056 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 31861
2022-03-04 20:25:34 | INFO | fairseq.trainer | begin training epoch 123
2022-03-04 20:25:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:27:30 | INFO | train_inner | epoch 123:     45 / 97 loss=1.821, nll_loss=1.244, ppl=2.37, wps=24403.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11800, lr=0.000291111, gnorm=1.042, loss_scale=8, train_wall=238, gb_free=21, wall=31978
2022-03-04 20:29:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:29:49 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 12.787 | nll_loss 12.467 | ppl 5661.75 | wps 43817.6 | wpb 510.9 | bsz 1 | num_updates 11852 | best_loss 7.701
2022-03-04 20:29:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 11852 updates
2022-03-04 20:29:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:29:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:29:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 123 @ 11852 updates, score 12.787) (writing took 2.494240260683 seconds)
2022-03-04 20:29:52 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-04 20:29:52 | INFO | train | epoch 123 | loss 1.817 | nll_loss 1.24 | ppl 2.36 | wps 24625.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11852 | lr 0.000290472 | gnorm 1.04 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 32119
2022-03-04 20:29:52 | INFO | fairseq.trainer | begin training epoch 124
2022-03-04 20:29:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:31:56 | INFO | train_inner | epoch 124:     48 / 97 loss=1.812, nll_loss=1.234, ppl=2.35, wps=24638.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=11900, lr=0.000289886, gnorm=1.054, loss_scale=16, train_wall=235, gb_free=21, wall=32244
2022-03-04 20:34:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:34:07 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 12.814 | nll_loss 12.5 | ppl 5791.25 | wps 43783.9 | wpb 510.9 | bsz 1 | num_updates 11949 | best_loss 7.701
2022-03-04 20:34:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 11949 updates
2022-03-04 20:34:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:34:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:34:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 124 @ 11949 updates, score 12.814) (writing took 2.4139167424291372 seconds)
2022-03-04 20:34:10 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-04 20:34:10 | INFO | train | epoch 124 | loss 1.807 | nll_loss 1.23 | ppl 2.35 | wps 24616.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11949 | lr 0.000289291 | gnorm 1.055 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 32377
2022-03-04 20:34:10 | INFO | fairseq.trainer | begin training epoch 125
2022-03-04 20:34:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:36:22 | INFO | train_inner | epoch 125:     51 / 97 loss=1.802, nll_loss=1.224, ppl=2.34, wps=24625.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=12000, lr=0.000288675, gnorm=1.042, loss_scale=16, train_wall=236, gb_free=21, wall=32509
2022-03-04 20:37:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:38:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:38:25 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 12.883 | nll_loss 12.566 | ppl 6063.64 | wps 43978.6 | wpb 510.9 | bsz 1 | num_updates 12045 | best_loss 7.701
2022-03-04 20:38:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 12045 updates
2022-03-04 20:38:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:38:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:38:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 125 @ 12045 updates, score 12.883) (writing took 2.4324229173362255 seconds)
2022-03-04 20:38:28 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-04 20:38:28 | INFO | train | epoch 125 | loss 1.796 | nll_loss 1.218 | ppl 2.33 | wps 24358.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12045 | lr 0.000288135 | gnorm 1.042 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 32636
2022-03-04 20:38:28 | INFO | fairseq.trainer | begin training epoch 126
2022-03-04 20:38:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:40:50 | INFO | train_inner | epoch 126:     55 / 97 loss=1.791, nll_loss=1.213, ppl=2.32, wps=24418.6, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=12100, lr=0.00028748, gnorm=1.042, loss_scale=16, train_wall=238, gb_free=21, wall=32778
2022-03-04 20:42:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:42:43 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 12.826 | nll_loss 12.505 | ppl 5812.99 | wps 43852.4 | wpb 510.9 | bsz 1 | num_updates 12142 | best_loss 7.701
2022-03-04 20:42:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 12142 updates
2022-03-04 20:42:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:42:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:42:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 126 @ 12142 updates, score 12.826) (writing took 2.4061879869550467 seconds)
2022-03-04 20:42:46 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-04 20:42:46 | INFO | train | epoch 126 | loss 1.788 | nll_loss 1.21 | ppl 2.31 | wps 24637.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12142 | lr 0.000286982 | gnorm 1.041 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 32893
2022-03-04 20:42:46 | INFO | fairseq.trainer | begin training epoch 127
2022-03-04 20:42:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:44:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:45:18 | INFO | train_inner | epoch 127:     59 / 97 loss=1.783, nll_loss=1.205, ppl=2.3, wps=24417.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12200, lr=0.000286299, gnorm=1.047, loss_scale=16, train_wall=238, gb_free=21, wall=33046
2022-03-04 20:46:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:47:01 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 12.915 | nll_loss 12.6 | ppl 6209.26 | wps 43665.9 | wpb 510.9 | bsz 1 | num_updates 12238 | best_loss 7.701
2022-03-04 20:47:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 12238 updates
2022-03-04 20:47:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:47:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:47:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 127 @ 12238 updates, score 12.915) (writing took 2.4571036538109183 seconds)
2022-03-04 20:47:04 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-04 20:47:04 | INFO | train | epoch 127 | loss 1.777 | nll_loss 1.198 | ppl 2.29 | wps 24363 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12238 | lr 0.000285854 | gnorm 1.033 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 33151
2022-03-04 20:47:04 | INFO | fairseq.trainer | begin training epoch 128
2022-03-04 20:47:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:49:44 | INFO | train_inner | epoch 128:     62 / 97 loss=1.774, nll_loss=1.194, ppl=2.29, wps=24647.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12300, lr=0.000285133, gnorm=1.032, loss_scale=16, train_wall=235, gb_free=21, wall=33312
2022-03-04 20:49:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:51:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:51:19 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 12.902 | nll_loss 12.583 | ppl 6136.54 | wps 44333.7 | wpb 510.9 | bsz 1 | num_updates 12334 | best_loss 7.701
2022-03-04 20:51:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 12334 updates
2022-03-04 20:51:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:51:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:51:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 128 @ 12334 updates, score 12.902) (writing took 2.4582846146076918 seconds)
2022-03-04 20:51:22 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-04 20:51:22 | INFO | train | epoch 128 | loss 1.77 | nll_loss 1.191 | ppl 2.28 | wps 24375.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12334 | lr 0.00028474 | gnorm 1.044 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 33409
2022-03-04 20:51:22 | INFO | fairseq.trainer | begin training epoch 129
2022-03-04 20:51:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:53:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:54:15 | INFO | train_inner | epoch 129:     67 / 97 loss=1.766, nll_loss=1.187, ppl=2.28, wps=24173.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12400, lr=0.000283981, gnorm=1.049, loss_scale=8, train_wall=240, gb_free=21, wall=33583
2022-03-04 20:55:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:55:37 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 12.895 | nll_loss 12.581 | ppl 6127.58 | wps 44817.2 | wpb 510.9 | bsz 1 | num_updates 12430 | best_loss 7.701
2022-03-04 20:55:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 12430 updates
2022-03-04 20:55:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:55:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:55:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 129 @ 12430 updates, score 12.895) (writing took 2.44235622510314 seconds)
2022-03-04 20:55:39 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-04 20:55:39 | INFO | train | epoch 129 | loss 1.76 | nll_loss 1.18 | ppl 2.27 | wps 24407.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12430 | lr 0.000283638 | gnorm 1.049 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 33667
2022-03-04 20:55:39 | INFO | fairseq.trainer | begin training epoch 130
2022-03-04 20:55:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:58:39 | INFO | train_inner | epoch 130:     70 / 97 loss=1.752, nll_loss=1.172, ppl=2.25, wps=24773.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12500, lr=0.000282843, gnorm=1.035, loss_scale=8, train_wall=234, gb_free=21, wall=33847
2022-03-04 20:59:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:59:53 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 12.949 | nll_loss 12.632 | ppl 6348.8 | wps 44470.6 | wpb 510.9 | bsz 1 | num_updates 12527 | best_loss 7.701
2022-03-04 20:59:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 12527 updates
2022-03-04 20:59:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:59:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 20:59:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 130 @ 12527 updates, score 12.949) (writing took 2.43730001244694 seconds)
2022-03-04 20:59:56 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-04 20:59:56 | INFO | train | epoch 130 | loss 1.75 | nll_loss 1.17 | ppl 2.25 | wps 24761.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12527 | lr 0.000282538 | gnorm 1.033 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 33924
2022-03-04 20:59:56 | INFO | fairseq.trainer | begin training epoch 131
2022-03-04 20:59:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:03:03 | INFO | train_inner | epoch 131:     73 / 97 loss=1.746, nll_loss=1.166, ppl=2.24, wps=24788.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12600, lr=0.000281718, gnorm=1.032, loss_scale=16, train_wall=234, gb_free=21, wall=34111
2022-03-04 21:04:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:04:10 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 12.938 | nll_loss 12.625 | ppl 6316.71 | wps 44477.9 | wpb 510.9 | bsz 1 | num_updates 12624 | best_loss 7.701
2022-03-04 21:04:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 12624 updates
2022-03-04 21:04:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:04:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:04:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 131 @ 12624 updates, score 12.938) (writing took 2.3821218693628907 seconds)
2022-03-04 21:04:12 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-04 21:04:12 | INFO | train | epoch 131 | loss 1.742 | nll_loss 1.162 | ppl 2.24 | wps 24766.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12624 | lr 0.00028145 | gnorm 1.031 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 34180
2022-03-04 21:04:12 | INFO | fairseq.trainer | begin training epoch 132
2022-03-04 21:04:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:04:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:07:30 | INFO | train_inner | epoch 132:     77 / 97 loss=1.734, nll_loss=1.154, ppl=2.22, wps=24516, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12700, lr=0.000280607, gnorm=1.038, loss_scale=16, train_wall=237, gb_free=21, wall=34378
2022-03-04 21:08:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:08:27 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 13.011 | nll_loss 12.702 | ppl 6664.69 | wps 44510.4 | wpb 510.9 | bsz 1 | num_updates 12720 | best_loss 7.701
2022-03-04 21:08:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 12720 updates
2022-03-04 21:08:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:08:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:08:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 132 @ 12720 updates, score 13.011) (writing took 2.4689751202240586 seconds)
2022-03-04 21:08:29 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-04 21:08:29 | INFO | train | epoch 132 | loss 1.733 | nll_loss 1.152 | ppl 2.22 | wps 24465.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12720 | lr 0.000280386 | gnorm 1.033 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 34437
2022-03-04 21:08:29 | INFO | fairseq.trainer | begin training epoch 133
2022-03-04 21:08:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:10:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:11:57 | INFO | train_inner | epoch 133:     81 / 97 loss=1.73, nll_loss=1.149, ppl=2.22, wps=24525.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12800, lr=0.000279508, gnorm=1.025, loss_scale=16, train_wall=237, gb_free=21, wall=34645
2022-03-04 21:12:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:12:43 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 13.031 | nll_loss 12.719 | ppl 6742.55 | wps 44563.6 | wpb 510.9 | bsz 1 | num_updates 12816 | best_loss 7.701
2022-03-04 21:12:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 12816 updates
2022-03-04 21:12:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:12:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:12:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 133 @ 12816 updates, score 13.031) (writing took 2.4493113635107875 seconds)
2022-03-04 21:12:46 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-04 21:12:46 | INFO | train | epoch 133 | loss 1.725 | nll_loss 1.144 | ppl 2.21 | wps 24492.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12816 | lr 0.000279334 | gnorm 1.023 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 34694
2022-03-04 21:12:46 | INFO | fairseq.trainer | begin training epoch 134
2022-03-04 21:12:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:16:22 | INFO | train_inner | epoch 134:     84 / 97 loss=1.718, nll_loss=1.136, ppl=2.2, wps=24785.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12900, lr=0.000278423, gnorm=1.013, loss_scale=32, train_wall=234, gb_free=21, wall=34910
2022-03-04 21:16:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:16:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:17:00 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 13.004 | nll_loss 12.695 | ppl 6631.85 | wps 44566 | wpb 510.9 | bsz 1 | num_updates 12912 | best_loss 7.701
2022-03-04 21:17:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 12912 updates
2022-03-04 21:17:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:17:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:17:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 134 @ 12912 updates, score 13.004) (writing took 2.430348414927721 seconds)
2022-03-04 21:17:02 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-04 21:17:02 | INFO | train | epoch 134 | loss 1.716 | nll_loss 1.135 | ppl 2.2 | wps 24514.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12912 | lr 0.000278294 | gnorm 1.013 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 34950
2022-03-04 21:17:02 | INFO | fairseq.trainer | begin training epoch 135
2022-03-04 21:17:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:20:49 | INFO | train_inner | epoch 135:     88 / 97 loss=1.713, nll_loss=1.132, ppl=2.19, wps=24534.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13000, lr=0.00027735, gnorm=1.033, loss_scale=16, train_wall=237, gb_free=21, wall=35176
2022-03-04 21:21:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:21:17 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 13.011 | nll_loss 12.699 | ppl 6649.99 | wps 44468.8 | wpb 510.9 | bsz 1 | num_updates 13009 | best_loss 7.701
2022-03-04 21:21:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 13009 updates
2022-03-04 21:21:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:21:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:21:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 135 @ 13009 updates, score 13.011) (writing took 2.468666262924671 seconds)
2022-03-04 21:21:19 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-04 21:21:19 | INFO | train | epoch 135 | loss 1.709 | nll_loss 1.128 | ppl 2.19 | wps 24746.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13009 | lr 0.000277254 | gnorm 1.032 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35207
2022-03-04 21:21:19 | INFO | fairseq.trainer | begin training epoch 136
2022-03-04 21:21:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:22:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:25:16 | INFO | train_inner | epoch 136:     92 / 97 loss=1.702, nll_loss=1.12, ppl=2.17, wps=24535.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13100, lr=0.000276289, gnorm=1.032, loss_scale=16, train_wall=237, gb_free=21, wall=35443
2022-03-04 21:25:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:25:33 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 12.994 | nll_loss 12.682 | ppl 6573.66 | wps 44550 | wpb 510.9 | bsz 1 | num_updates 13105 | best_loss 7.701
2022-03-04 21:25:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 13105 updates
2022-03-04 21:25:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:25:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:25:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 136 @ 13105 updates, score 12.994) (writing took 2.490286444313824 seconds)
2022-03-04 21:25:36 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-04 21:25:36 | INFO | train | epoch 136 | loss 1.7 | nll_loss 1.118 | ppl 2.17 | wps 24493 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13105 | lr 0.000276237 | gnorm 1.032 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35464
2022-03-04 21:25:36 | INFO | fairseq.trainer | begin training epoch 137
2022-03-04 21:25:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:28:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:29:42 | INFO | train_inner | epoch 137:     96 / 97 loss=1.695, nll_loss=1.113, ppl=2.16, wps=24542.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13200, lr=0.000275241, gnorm=1.024, loss_scale=16, train_wall=237, gb_free=21, wall=35710
2022-03-04 21:29:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:29:50 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 12.999 | nll_loss 12.686 | ppl 6588.07 | wps 44496.2 | wpb 510.9 | bsz 1 | num_updates 13201 | best_loss 7.701
2022-03-04 21:29:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 13201 updates
2022-03-04 21:29:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:29:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:29:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 137 @ 13201 updates, score 12.999) (writing took 2.4506390308961272 seconds)
2022-03-04 21:29:52 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-04 21:29:52 | INFO | train | epoch 137 | loss 1.692 | nll_loss 1.11 | ppl 2.16 | wps 24507.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13201 | lr 0.000275231 | gnorm 1.023 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35720
2022-03-04 21:29:52 | INFO | fairseq.trainer | begin training epoch 138
2022-03-04 21:29:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:34:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:34:06 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 13.089 | nll_loss 12.779 | ppl 7030.48 | wps 44463.5 | wpb 510.9 | bsz 1 | num_updates 13298 | best_loss 7.701
2022-03-04 21:34:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 13298 updates
2022-03-04 21:34:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:34:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:34:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 138 @ 13298 updates, score 13.089) (writing took 2.441841314546764 seconds)
2022-03-04 21:34:09 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-04 21:34:09 | INFO | train | epoch 138 | loss 1.686 | nll_loss 1.103 | ppl 2.15 | wps 24755.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13298 | lr 0.000274225 | gnorm 1.02 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 35977
2022-03-04 21:34:09 | INFO | fairseq.trainer | begin training epoch 139
2022-03-04 21:34:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:34:14 | INFO | train_inner | epoch 139:      2 / 97 loss=1.686, nll_loss=1.103, ppl=2.15, wps=24086, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=13300, lr=0.000274204, gnorm=1.02, loss_scale=16, train_wall=234, gb_free=21, wall=35982
2022-03-04 21:34:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:38:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:38:23 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 13.102 | nll_loss 12.794 | ppl 7102.46 | wps 44444.7 | wpb 510.9 | bsz 1 | num_updates 13394 | best_loss 7.701
2022-03-04 21:38:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 13394 updates
2022-03-04 21:38:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:38:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:38:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 139 @ 13394 updates, score 13.102) (writing took 2.4598478889092803 seconds)
2022-03-04 21:38:26 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-04 21:38:26 | INFO | train | epoch 139 | loss 1.678 | nll_loss 1.096 | ppl 2.14 | wps 24496.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13394 | lr 0.00027324 | gnorm 1.021 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 36233
2022-03-04 21:38:26 | INFO | fairseq.trainer | begin training epoch 140
2022-03-04 21:38:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:38:41 | INFO | train_inner | epoch 140:      6 / 97 loss=1.676, nll_loss=1.093, ppl=2.13, wps=24539.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13400, lr=0.000273179, gnorm=1.023, loss_scale=16, train_wall=237, gb_free=21, wall=36249
2022-03-04 21:40:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:42:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:42:40 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 13.094 | nll_loss 12.784 | ppl 7050.6 | wps 44340.8 | wpb 510.9 | bsz 1 | num_updates 13490 | best_loss 7.701
2022-03-04 21:42:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 13490 updates
2022-03-04 21:42:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:42:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:42:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 140 @ 13490 updates, score 13.094) (writing took 2.427454335615039 seconds)
2022-03-04 21:42:42 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-04 21:42:42 | INFO | train | epoch 140 | loss 1.67 | nll_loss 1.087 | ppl 2.12 | wps 24502.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13490 | lr 0.000272266 | gnorm 1.026 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 36490
2022-03-04 21:42:42 | INFO | fairseq.trainer | begin training epoch 141
2022-03-04 21:42:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:43:08 | INFO | train_inner | epoch 141:     10 / 97 loss=1.666, nll_loss=1.083, ppl=2.12, wps=24541.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13500, lr=0.000272166, gnorm=1.023, loss_scale=16, train_wall=237, gb_free=21, wall=36516
2022-03-04 21:46:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:46:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:46:56 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 13.122 | nll_loss 12.812 | ppl 7193.11 | wps 44345.5 | wpb 510.9 | bsz 1 | num_updates 13586 | best_loss 7.701
2022-03-04 21:46:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 13586 updates
2022-03-04 21:46:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:46:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:46:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 141 @ 13586 updates, score 13.122) (writing took 2.4606717778369784 seconds)
2022-03-04 21:46:59 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-04 21:46:59 | INFO | train | epoch 141 | loss 1.661 | nll_loss 1.077 | ppl 2.11 | wps 24494.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13586 | lr 0.000271303 | gnorm 1.008 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 36747
2022-03-04 21:46:59 | INFO | fairseq.trainer | begin training epoch 142
2022-03-04 21:46:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:47:35 | INFO | train_inner | epoch 142:     14 / 97 loss=1.658, nll_loss=1.075, ppl=2.11, wps=24527.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13600, lr=0.000271163, gnorm=1.004, loss_scale=16, train_wall=237, gb_free=21, wall=36783
2022-03-04 21:51:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:51:13 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 13.177 | nll_loss 12.869 | ppl 7480.86 | wps 44333 | wpb 510.9 | bsz 1 | num_updates 13683 | best_loss 7.701
2022-03-04 21:51:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 13683 updates
2022-03-04 21:51:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:51:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:51:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 142 @ 13683 updates, score 13.177) (writing took 2.467989159747958 seconds)
2022-03-04 21:51:16 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-04 21:51:16 | INFO | train | epoch 142 | loss 1.656 | nll_loss 1.072 | ppl 2.1 | wps 24740.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13683 | lr 0.000270339 | gnorm 0.997 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37003
2022-03-04 21:51:16 | INFO | fairseq.trainer | begin training epoch 143
2022-03-04 21:51:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:51:59 | INFO | train_inner | epoch 143:     17 / 97 loss=1.654, nll_loss=1.071, ppl=2.1, wps=24764.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13700, lr=0.000270172, gnorm=1.003, loss_scale=16, train_wall=234, gb_free=21, wall=37047
2022-03-04 21:52:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:55:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:55:30 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 13.156 | nll_loss 12.849 | ppl 7377.57 | wps 44481.6 | wpb 510.9 | bsz 1 | num_updates 13779 | best_loss 7.701
2022-03-04 21:55:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 13779 updates
2022-03-04 21:55:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:55:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:55:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 143 @ 13779 updates, score 13.156) (writing took 2.4232687558978796 seconds)
2022-03-04 21:55:32 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-04 21:55:32 | INFO | train | epoch 143 | loss 1.65 | nll_loss 1.066 | ppl 2.09 | wps 24498 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13779 | lr 0.000269396 | gnorm 1.018 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37260
2022-03-04 21:55:32 | INFO | fairseq.trainer | begin training epoch 144
2022-03-04 21:55:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:56:26 | INFO | train_inner | epoch 144:     21 / 97 loss=1.646, nll_loss=1.063, ppl=2.09, wps=24544.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13800, lr=0.000269191, gnorm=1.01, loss_scale=16, train_wall=237, gb_free=21, wall=37314
2022-03-04 21:58:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:59:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:59:46 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 13.193 | nll_loss 12.884 | ppl 7560.2 | wps 44315 | wpb 510.9 | bsz 1 | num_updates 13875 | best_loss 7.701
2022-03-04 21:59:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 13875 updates
2022-03-04 21:59:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:59:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 21:59:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 144 @ 13875 updates, score 13.193) (writing took 2.4511463958770037 seconds)
2022-03-04 21:59:49 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-04 21:59:49 | INFO | train | epoch 144 | loss 1.64 | nll_loss 1.056 | ppl 2.08 | wps 24510.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13875 | lr 0.000268462 | gnorm 0.995 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37517
2022-03-04 21:59:49 | INFO | fairseq.trainer | begin training epoch 145
2022-03-04 21:59:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:00:53 | INFO | train_inner | epoch 145:     25 / 97 loss=1.639, nll_loss=1.055, ppl=2.08, wps=24550.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13900, lr=0.000268221, gnorm=0.996, loss_scale=16, train_wall=237, gb_free=21, wall=37581
2022-03-04 22:03:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:04:03 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 13.2 | nll_loss 12.892 | ppl 7603.41 | wps 44693.4 | wpb 510.9 | bsz 1 | num_updates 13972 | best_loss 7.701
2022-03-04 22:04:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 13972 updates
2022-03-04 22:04:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:04:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:04:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 145 @ 13972 updates, score 13.2) (writing took 2.45380593650043 seconds)
2022-03-04 22:04:05 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-04 22:04:05 | INFO | train | epoch 145 | loss 1.636 | nll_loss 1.052 | ppl 2.07 | wps 24756.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13972 | lr 0.000267529 | gnorm 0.998 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 37773
2022-03-04 22:04:05 | INFO | fairseq.trainer | begin training epoch 146
2022-03-04 22:04:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:04:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:05:20 | INFO | train_inner | epoch 146:     29 / 97 loss=1.633, nll_loss=1.049, ppl=2.07, wps=24541, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=14000, lr=0.000267261, gnorm=0.997, loss_scale=16, train_wall=237, gb_free=21, wall=37848
2022-03-04 22:08:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:08:19 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 13.145 | nll_loss 12.839 | ppl 7325.8 | wps 44351.4 | wpb 510.9 | bsz 1 | num_updates 14068 | best_loss 7.701
2022-03-04 22:08:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 14068 updates
2022-03-04 22:08:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:08:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:08:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 146 @ 14068 updates, score 13.145) (writing took 2.4496345929801464 seconds)
2022-03-04 22:08:22 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-04 22:08:22 | INFO | train | epoch 146 | loss 1.628 | nll_loss 1.044 | ppl 2.06 | wps 24507 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14068 | lr 0.000266615 | gnorm 1.014 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 38030
2022-03-04 22:08:22 | INFO | fairseq.trainer | begin training epoch 147
2022-03-04 22:08:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:09:44 | INFO | train_inner | epoch 147:     32 / 97 loss=1.624, nll_loss=1.04, ppl=2.06, wps=24776.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14100, lr=0.000266312, gnorm=1.013, loss_scale=16, train_wall=234, gb_free=21, wall=38112
2022-03-04 22:10:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:12:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:12:36 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 13.2 | nll_loss 12.894 | ppl 7613.17 | wps 44501.4 | wpb 510.9 | bsz 1 | num_updates 14164 | best_loss 7.701
2022-03-04 22:12:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 14164 updates
2022-03-04 22:12:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:12:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:12:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 147 @ 14164 updates, score 13.2) (writing took 2.469396924600005 seconds)
2022-03-04 22:12:39 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-04 22:12:39 | INFO | train | epoch 147 | loss 1.62 | nll_loss 1.035 | ppl 2.05 | wps 24495.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14164 | lr 0.000265709 | gnorm 1.001 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 38286
2022-03-04 22:12:39 | INFO | fairseq.trainer | begin training epoch 148
2022-03-04 22:12:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:14:11 | INFO | train_inner | epoch 148:     36 / 97 loss=1.617, nll_loss=1.032, ppl=2.04, wps=24536.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=14200, lr=0.000265372, gnorm=0.998, loss_scale=16, train_wall=237, gb_free=21, wall=38379
2022-03-04 22:16:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:16:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:16:53 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 13.194 | nll_loss 12.886 | ppl 7567.61 | wps 44457.5 | wpb 510.9 | bsz 1 | num_updates 14260 | best_loss 7.701
2022-03-04 22:16:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 14260 updates
2022-03-04 22:16:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:16:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:16:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 148 @ 14260 updates, score 13.194) (writing took 2.4462436698377132 seconds)
2022-03-04 22:16:55 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-04 22:16:55 | INFO | train | epoch 148 | loss 1.614 | nll_loss 1.029 | ppl 2.04 | wps 24495.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14260 | lr 0.000264814 | gnorm 0.996 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 38543
2022-03-04 22:16:55 | INFO | fairseq.trainer | begin training epoch 149
2022-03-04 22:16:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:18:38 | INFO | train_inner | epoch 149:     40 / 97 loss=1.613, nll_loss=1.028, ppl=2.04, wps=24531.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=14300, lr=0.000264443, gnorm=1.003, loss_scale=16, train_wall=237, gb_free=21, wall=38646
2022-03-04 22:21:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:21:09 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 13.22 | nll_loss 12.913 | ppl 7714.62 | wps 44711.9 | wpb 510.9 | bsz 1 | num_updates 14357 | best_loss 7.701
2022-03-04 22:21:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 14357 updates
2022-03-04 22:21:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:21:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:21:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 149 @ 14357 updates, score 13.22) (writing took 2.4504485884681344 seconds)
2022-03-04 22:21:12 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-04 22:21:12 | INFO | train | epoch 149 | loss 1.611 | nll_loss 1.026 | ppl 2.04 | wps 24746 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14357 | lr 0.000263917 | gnorm 1.015 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 38800
2022-03-04 22:21:12 | INFO | fairseq.trainer | begin training epoch 150
2022-03-04 22:21:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:23:02 | INFO | train_inner | epoch 150:     43 / 97 loss=1.607, nll_loss=1.021, ppl=2.03, wps=24773.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14400, lr=0.000263523, gnorm=1.002, loss_scale=32, train_wall=234, gb_free=21, wall=38910
2022-03-04 22:23:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:25:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:25:26 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 13.256 | nll_loss 12.95 | ppl 7913.36 | wps 44301.6 | wpb 510.9 | bsz 1 | num_updates 14453 | best_loss 7.701
2022-03-04 22:25:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 14453 updates
2022-03-04 22:25:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:25:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:25:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 150 @ 14453 updates, score 13.256) (writing took 2.4530080566182733 seconds)
2022-03-04 22:25:28 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-04 22:25:28 | INFO | train | epoch 150 | loss 1.601 | nll_loss 1.016 | ppl 2.02 | wps 24511.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14453 | lr 0.00026304 | gnorm 0.99 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 39056
2022-03-04 22:25:28 | INFO | fairseq.trainer | begin training epoch 151
2022-03-04 22:25:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:27:29 | INFO | train_inner | epoch 151:     47 / 97 loss=1.598, nll_loss=1.013, ppl=2.02, wps=24549.7, ups=0.37, wpb=65495, bsz=127.9, num_updates=14500, lr=0.000262613, gnorm=0.995, loss_scale=16, train_wall=237, gb_free=21, wall=39177
2022-03-04 22:29:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:29:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:29:42 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 13.285 | nll_loss 12.983 | ppl 8096.99 | wps 44642.3 | wpb 510.9 | bsz 1 | num_updates 14549 | best_loss 7.701
2022-03-04 22:29:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 14549 updates
2022-03-04 22:29:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:29:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:29:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 151 @ 14549 updates, score 13.285) (writing took 2.4436278557404876 seconds)
2022-03-04 22:29:45 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-04 22:29:45 | INFO | train | epoch 151 | loss 1.596 | nll_loss 1.011 | ppl 2.02 | wps 24509.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14549 | lr 0.00026217 | gnorm 1.003 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 39313
2022-03-04 22:29:45 | INFO | fairseq.trainer | begin training epoch 152
2022-03-04 22:29:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:31:56 | INFO | train_inner | epoch 152:     51 / 97 loss=1.592, nll_loss=1.007, ppl=2.01, wps=24548.3, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=14600, lr=0.000261712, gnorm=1.002, loss_scale=16, train_wall=237, gb_free=21, wall=39444
2022-03-04 22:33:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:33:59 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 13.252 | nll_loss 12.945 | ppl 7884.47 | wps 44583.9 | wpb 510.9 | bsz 1 | num_updates 14646 | best_loss 7.701
2022-03-04 22:33:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 14646 updates
2022-03-04 22:33:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:34:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:34:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 152 @ 14646 updates, score 13.252) (writing took 2.459406591951847 seconds)
2022-03-04 22:34:02 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-03-04 22:34:02 | INFO | train | epoch 152 | loss 1.59 | nll_loss 1.004 | ppl 2.01 | wps 24737.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14646 | lr 0.000261301 | gnorm 0.989 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 39570
2022-03-04 22:34:02 | INFO | fairseq.trainer | begin training epoch 153
2022-03-04 22:34:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:35:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:36:23 | INFO | train_inner | epoch 153:     55 / 97 loss=1.588, nll_loss=1.003, ppl=2, wps=24524.4, ups=0.37, wpb=65495, bsz=127.9, num_updates=14700, lr=0.00026082, gnorm=0.986, loss_scale=16, train_wall=237, gb_free=21, wall=39711
2022-03-04 22:38:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:38:16 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 13.281 | nll_loss 12.979 | ppl 8072.88 | wps 44611.7 | wpb 510.9 | bsz 1 | num_updates 14742 | best_loss 7.701
2022-03-04 22:38:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 14742 updates
2022-03-04 22:38:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:38:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:38:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 153 @ 14742 updates, score 13.281) (writing took 2.465379148721695 seconds)
2022-03-04 22:38:18 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-03-04 22:38:18 | INFO | train | epoch 153 | loss 1.583 | nll_loss 0.997 | ppl 2 | wps 24494.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14742 | lr 0.000260448 | gnorm 0.988 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 39826
2022-03-04 22:38:18 | INFO | fairseq.trainer | begin training epoch 154
2022-03-04 22:38:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:40:47 | INFO | train_inner | epoch 154:     58 / 97 loss=1.58, nll_loss=0.994, ppl=1.99, wps=24768.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=14800, lr=0.000259938, gnorm=0.984, loss_scale=16, train_wall=234, gb_free=21, wall=39975
2022-03-04 22:41:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:42:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:42:32 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 13.303 | nll_loss 12.997 | ppl 8177.41 | wps 44369.7 | wpb 510.9 | bsz 1 | num_updates 14838 | best_loss 7.701
2022-03-04 22:42:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 14838 updates
2022-03-04 22:42:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:42:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:42:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 154 @ 14838 updates, score 13.303) (writing took 2.4598629251122475 seconds)
2022-03-04 22:42:35 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-03-04 22:42:35 | INFO | train | epoch 154 | loss 1.577 | nll_loss 0.992 | ppl 1.99 | wps 24505.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14838 | lr 0.000259605 | gnorm 0.983 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 40083
2022-03-04 22:42:35 | INFO | fairseq.trainer | begin training epoch 155
2022-03-04 22:42:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:45:14 | INFO | train_inner | epoch 155:     62 / 97 loss=1.577, nll_loss=0.991, ppl=1.99, wps=24539.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=14900, lr=0.000259064, gnorm=0.982, loss_scale=16, train_wall=237, gb_free=21, wall=40242
2022-03-04 22:46:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:46:49 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 13.27 | nll_loss 12.963 | ppl 7983.93 | wps 44497.6 | wpb 510.9 | bsz 1 | num_updates 14935 | best_loss 7.701
2022-03-04 22:46:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 14935 updates
2022-03-04 22:46:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:46:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:46:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 155 @ 14935 updates, score 13.27) (writing took 2.440156899392605 seconds)
2022-03-04 22:46:52 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-03-04 22:46:52 | INFO | train | epoch 155 | loss 1.574 | nll_loss 0.988 | ppl 1.98 | wps 24747.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14935 | lr 0.00025876 | gnorm 0.987 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 40340
2022-03-04 22:46:52 | INFO | fairseq.trainer | begin training epoch 156
2022-03-04 22:46:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:47:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:49:41 | INFO | train_inner | epoch 156:     66 / 97 loss=1.57, nll_loss=0.984, ppl=1.98, wps=24524.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=15000, lr=0.000258199, gnorm=1.001, loss_scale=16, train_wall=237, gb_free=21, wall=40509
2022-03-04 22:51:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:51:06 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 13.284 | nll_loss 12.979 | ppl 8073.32 | wps 44559.2 | wpb 510.9 | bsz 1 | num_updates 15031 | best_loss 7.701
2022-03-04 22:51:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 15031 updates
2022-03-04 22:51:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:51:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:51:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 156 @ 15031 updates, score 13.284) (writing took 2.452876321040094 seconds)
2022-03-04 22:51:08 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-03-04 22:51:08 | INFO | train | epoch 156 | loss 1.567 | nll_loss 0.981 | ppl 1.97 | wps 24486.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15031 | lr 0.000257932 | gnorm 1 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 40596
2022-03-04 22:51:08 | INFO | fairseq.trainer | begin training epoch 157
2022-03-04 22:51:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:53:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:54:08 | INFO | train_inner | epoch 157:     70 / 97 loss=1.564, nll_loss=0.978, ppl=1.97, wps=24528.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=15100, lr=0.000257343, gnorm=0.99, loss_scale=16, train_wall=237, gb_free=21, wall=40776
2022-03-04 22:55:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:55:23 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 13.323 | nll_loss 13.017 | ppl 8291.67 | wps 44452.1 | wpb 510.9 | bsz 1 | num_updates 15127 | best_loss 7.701
2022-03-04 22:55:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 15127 updates
2022-03-04 22:55:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:55:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:55:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 157 @ 15127 updates, score 13.323) (writing took 2.4534236546605825 seconds)
2022-03-04 22:55:25 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-03-04 22:55:25 | INFO | train | epoch 157 | loss 1.562 | nll_loss 0.975 | ppl 1.97 | wps 24490.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15127 | lr 0.000257113 | gnorm 0.986 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 40853
2022-03-04 22:55:25 | INFO | fairseq.trainer | begin training epoch 158
2022-03-04 22:55:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:58:33 | INFO | train_inner | epoch 158:     73 / 97 loss=1.556, nll_loss=0.97, ppl=1.96, wps=24775.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=15200, lr=0.000256495, gnorm=0.989, loss_scale=16, train_wall=234, gb_free=21, wall=41041
2022-03-04 22:59:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:59:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:59:39 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 13.305 | nll_loss 13.003 | ppl 8206.27 | wps 44965.7 | wpb 510.9 | bsz 1 | num_updates 15223 | best_loss 7.701
2022-03-04 22:59:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 15223 updates
2022-03-04 22:59:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:59:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 22:59:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 158 @ 15223 updates, score 13.305) (writing took 2.4267299408093095 seconds)
2022-03-04 22:59:42 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-03-04 22:59:42 | INFO | train | epoch 158 | loss 1.555 | nll_loss 0.969 | ppl 1.96 | wps 24501.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15223 | lr 0.000256301 | gnorm 0.996 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 41110
2022-03-04 22:59:42 | INFO | fairseq.trainer | begin training epoch 159
2022-03-04 22:59:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:03:00 | INFO | train_inner | epoch 159:     77 / 97 loss=1.553, nll_loss=0.967, ppl=1.95, wps=24528.9, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=15300, lr=0.000255655, gnorm=1.002, loss_scale=16, train_wall=237, gb_free=21, wall=41308
2022-03-04 23:03:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:03:56 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 13.351 | nll_loss 13.049 | ppl 8477.44 | wps 44538.8 | wpb 510.9 | bsz 1 | num_updates 15320 | best_loss 7.701
2022-03-04 23:03:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 15320 updates
2022-03-04 23:03:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:03:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:03:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 159 @ 15320 updates, score 13.351) (writing took 2.4495551949366927 seconds)
2022-03-04 23:03:58 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-03-04 23:03:58 | INFO | train | epoch 159 | loss 1.549 | nll_loss 0.962 | ppl 1.95 | wps 24742.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15320 | lr 0.000255488 | gnorm 0.988 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 41366
2022-03-04 23:03:59 | INFO | fairseq.trainer | begin training epoch 160
2022-03-04 23:03:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:05:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:07:27 | INFO | train_inner | epoch 160:     81 / 97 loss=1.546, nll_loss=0.959, ppl=1.94, wps=24529.3, ups=0.37, wpb=65495, bsz=127.9, num_updates=15400, lr=0.000254824, gnorm=0.981, loss_scale=16, train_wall=237, gb_free=21, wall=41575
2022-03-04 23:08:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:08:13 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 13.339 | nll_loss 13.037 | ppl 8403.29 | wps 44470.2 | wpb 510.9 | bsz 1 | num_updates 15416 | best_loss 7.701
2022-03-04 23:08:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 15416 updates
2022-03-04 23:08:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:08:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:08:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 160 @ 15416 updates, score 13.339) (writing took 2.418580463156104 seconds)
2022-03-04 23:08:15 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-03-04 23:08:15 | INFO | train | epoch 160 | loss 1.544 | nll_loss 0.957 | ppl 1.94 | wps 24497.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15416 | lr 0.000254691 | gnorm 0.985 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 41623
2022-03-04 23:08:15 | INFO | fairseq.trainer | begin training epoch 161
2022-03-04 23:08:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:11:51 | INFO | train_inner | epoch 161:     84 / 97 loss=1.54, nll_loss=0.954, ppl=1.94, wps=24756, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=15500, lr=0.000254, gnorm=0.994, loss_scale=32, train_wall=235, gb_free=21, wall=41839
2022-03-04 23:12:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:12:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:12:30 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 13.398 | nll_loss 13.096 | ppl 8757.61 | wps 42902 | wpb 510.9 | bsz 1 | num_updates 15512 | best_loss 7.701
2022-03-04 23:12:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 15512 updates
2022-03-04 23:12:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:12:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:12:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 161 @ 15512 updates, score 13.398) (writing took 2.2181842923164368 seconds)
2022-03-04 23:12:32 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-03-04 23:12:32 | INFO | train | epoch 161 | loss 1.539 | nll_loss 0.952 | ppl 1.93 | wps 24455.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15512 | lr 0.000253902 | gnorm 0.997 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 41880
2022-03-04 23:12:32 | INFO | fairseq.trainer | begin training epoch 162
2022-03-04 23:12:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:16:20 | INFO | train_inner | epoch 162:     88 / 97 loss=1.536, nll_loss=0.949, ppl=1.93, wps=24389.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=15600, lr=0.000253185, gnorm=0.982, loss_scale=16, train_wall=238, gb_free=21, wall=42108
2022-03-04 23:16:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:16:48 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 13.409 | nll_loss 13.109 | ppl 8833.3 | wps 43867.9 | wpb 510.9 | bsz 1 | num_updates 15609 | best_loss 7.701
2022-03-04 23:16:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 15609 updates
2022-03-04 23:16:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:16:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:16:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 162 @ 15609 updates, score 13.409) (writing took 2.3146508364006877 seconds)
2022-03-04 23:16:50 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-03-04 23:16:50 | INFO | train | epoch 162 | loss 1.534 | nll_loss 0.947 | ppl 1.93 | wps 24611.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15609 | lr 0.000253112 | gnorm 0.975 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 42138
2022-03-04 23:16:50 | INFO | fairseq.trainer | begin training epoch 163
2022-03-04 23:16:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:18:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:20:48 | INFO | train_inner | epoch 163:     92 / 97 loss=1.53, nll_loss=0.943, ppl=1.92, wps=24385.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=15700, lr=0.000252377, gnorm=0.964, loss_scale=16, train_wall=238, gb_free=21, wall=42376
2022-03-04 23:21:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:21:06 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 13.396 | nll_loss 13.095 | ppl 8750.38 | wps 43573 | wpb 510.9 | bsz 1 | num_updates 15705 | best_loss 7.701
2022-03-04 23:21:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 15705 updates
2022-03-04 23:21:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:21:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:21:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 163 @ 15705 updates, score 13.396) (writing took 2.336568705737591 seconds)
2022-03-04 23:21:09 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-03-04 23:21:09 | INFO | train | epoch 163 | loss 1.528 | nll_loss 0.941 | ppl 1.92 | wps 24343.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15705 | lr 0.000252337 | gnorm 0.965 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 42396
2022-03-04 23:21:09 | INFO | fairseq.trainer | begin training epoch 164
2022-03-04 23:21:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:23:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:25:17 | INFO | train_inner | epoch 164:     96 / 97 loss=1.526, nll_loss=0.939, ppl=1.92, wps=24398.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=15800, lr=0.000251577, gnorm=0.983, loss_scale=16, train_wall=238, gb_free=21, wall=42645
2022-03-04 23:25:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:25:24 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 13.354 | nll_loss 13.048 | ppl 8467.91 | wps 43801.9 | wpb 510.9 | bsz 1 | num_updates 15801 | best_loss 7.701
2022-03-04 23:25:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 15801 updates
2022-03-04 23:25:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:25:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:25:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 164 @ 15801 updates, score 13.354) (writing took 2.432366691529751 seconds)
2022-03-04 23:25:27 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-03-04 23:25:27 | INFO | train | epoch 164 | loss 1.524 | nll_loss 0.936 | ppl 1.91 | wps 24352.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15801 | lr 0.000251569 | gnorm 0.982 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 42655
2022-03-04 23:25:27 | INFO | fairseq.trainer | begin training epoch 165
2022-03-04 23:25:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:29:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:29:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:29:43 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 13.385 | nll_loss 13.085 | ppl 8690.6 | wps 43761.4 | wpb 510.9 | bsz 1 | num_updates 15897 | best_loss 7.701
2022-03-04 23:29:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 15897 updates
2022-03-04 23:29:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:29:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:29:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 165 @ 15897 updates, score 13.385) (writing took 2.372607762925327 seconds)
2022-03-04 23:29:45 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-03-04 23:29:45 | INFO | train | epoch 165 | loss 1.519 | nll_loss 0.932 | ppl 1.91 | wps 24334.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15897 | lr 0.000250809 | gnorm 0.975 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 42913
2022-03-04 23:29:45 | INFO | fairseq.trainer | begin training epoch 166
2022-03-04 23:29:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:29:53 | INFO | train_inner | epoch 166:      3 / 97 loss=1.519, nll_loss=0.931, ppl=1.91, wps=23701.4, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=15900, lr=0.000250785, gnorm=0.976, loss_scale=16, train_wall=238, gb_free=21, wall=42921
2022-03-04 23:33:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:34:01 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 13.396 | nll_loss 13.096 | ppl 8755.47 | wps 44028.5 | wpb 510.9 | bsz 1 | num_updates 15994 | best_loss 7.701
2022-03-04 23:34:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 15994 updates
2022-03-04 23:34:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:34:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:34:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 166 @ 15994 updates, score 13.396) (writing took 2.487591198645532 seconds)
2022-03-04 23:34:03 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-03-04 23:34:03 | INFO | train | epoch 166 | loss 1.515 | nll_loss 0.927 | ppl 1.9 | wps 24592.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15994 | lr 0.000250047 | gnorm 0.993 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 43171
2022-03-04 23:34:03 | INFO | fairseq.trainer | begin training epoch 167
2022-03-04 23:34:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:34:19 | INFO | train_inner | epoch 167:      6 / 97 loss=1.514, nll_loss=0.926, ppl=1.9, wps=24617.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16000, lr=0.00025, gnorm=0.992, loss_scale=16, train_wall=236, gb_free=21, wall=43187
2022-03-04 23:35:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:38:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:38:19 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 13.396 | nll_loss 13.097 | ppl 8764.57 | wps 43670.9 | wpb 510.9 | bsz 1 | num_updates 16090 | best_loss 7.701
2022-03-04 23:38:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 16090 updates
2022-03-04 23:38:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:38:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:38:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 167 @ 16090 updates, score 13.396) (writing took 2.4055621679872274 seconds)
2022-03-04 23:38:22 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-03-04 23:38:22 | INFO | train | epoch 167 | loss 1.508 | nll_loss 0.92 | ppl 1.89 | wps 24346.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16090 | lr 0.0002493 | gnorm 0.969 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 43430
2022-03-04 23:38:22 | INFO | fairseq.trainer | begin training epoch 168
2022-03-04 23:38:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:38:48 | INFO | train_inner | epoch 168:     10 / 97 loss=1.505, nll_loss=0.917, ppl=1.89, wps=24384.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16100, lr=0.000249222, gnorm=0.969, loss_scale=16, train_wall=238, gb_free=21, wall=43456
2022-03-04 23:42:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:42:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:42:37 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 13.451 | nll_loss 13.148 | ppl 9079.51 | wps 43608.9 | wpb 510.9 | bsz 1 | num_updates 16186 | best_loss 7.701
2022-03-04 23:42:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 16186 updates
2022-03-04 23:42:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:42:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:42:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 168 @ 16186 updates, score 13.451) (writing took 2.4452951280400157 seconds)
2022-03-04 23:42:40 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-03-04 23:42:40 | INFO | train | epoch 168 | loss 1.504 | nll_loss 0.916 | ppl 1.89 | wps 24353.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16186 | lr 0.000248559 | gnorm 0.972 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 43688
2022-03-04 23:42:40 | INFO | fairseq.trainer | begin training epoch 169
2022-03-04 23:42:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:43:16 | INFO | train_inner | epoch 169:     14 / 97 loss=1.504, nll_loss=0.916, ppl=1.89, wps=24391.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16200, lr=0.000248452, gnorm=0.971, loss_scale=16, train_wall=238, gb_free=21, wall=43724
2022-03-04 23:46:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:46:56 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 13.372 | nll_loss 13.069 | ppl 8591.51 | wps 43876.3 | wpb 510.9 | bsz 1 | num_updates 16283 | best_loss 7.701
2022-03-04 23:46:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 16283 updates
2022-03-04 23:46:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:46:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:46:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 169 @ 16283 updates, score 13.372) (writing took 2.3438724000006914 seconds)
2022-03-04 23:46:58 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-03-04 23:46:58 | INFO | train | epoch 169 | loss 1.5 | nll_loss 0.912 | ppl 1.88 | wps 24609.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16283 | lr 0.000247818 | gnorm 0.978 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 43946
2022-03-04 23:46:58 | INFO | fairseq.trainer | begin training epoch 170
2022-03-04 23:46:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:47:42 | INFO | train_inner | epoch 170:     17 / 97 loss=1.497, nll_loss=0.909, ppl=1.88, wps=24635.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16300, lr=0.000247689, gnorm=0.975, loss_scale=16, train_wall=236, gb_free=21, wall=43990
2022-03-04 23:48:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:51:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:51:14 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 13.399 | nll_loss 13.098 | ppl 8769.62 | wps 43813.3 | wpb 510.9 | bsz 1 | num_updates 16379 | best_loss 7.701
2022-03-04 23:51:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 16379 updates
2022-03-04 23:51:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:51:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:51:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 170 @ 16379 updates, score 13.399) (writing took 2.3657522471621633 seconds)
2022-03-04 23:51:16 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-03-04 23:51:16 | INFO | train | epoch 170 | loss 1.493 | nll_loss 0.905 | ppl 1.87 | wps 24372.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16379 | lr 0.000247091 | gnorm 0.969 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 44204
2022-03-04 23:51:16 | INFO | fairseq.trainer | begin training epoch 171
2022-03-04 23:51:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:52:10 | INFO | train_inner | epoch 171:     21 / 97 loss=1.491, nll_loss=0.903, ppl=1.87, wps=24400.6, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=16400, lr=0.000246932, gnorm=0.972, loss_scale=16, train_wall=238, gb_free=21, wall=44258
2022-03-04 23:54:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:55:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:55:32 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 13.44 | nll_loss 13.141 | ppl 9032.07 | wps 43760 | wpb 510.9 | bsz 1 | num_updates 16475 | best_loss 7.701
2022-03-04 23:55:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 16475 updates
2022-03-04 23:55:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:55:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:55:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 171 @ 16475 updates, score 13.44) (writing took 2.405881796963513 seconds)
2022-03-04 23:55:34 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-03-04 23:55:34 | INFO | train | epoch 171 | loss 1.49 | nll_loss 0.902 | ppl 1.87 | wps 24349.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16475 | lr 0.00024637 | gnorm 0.975 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 44462
2022-03-04 23:55:34 | INFO | fairseq.trainer | begin training epoch 172
2022-03-04 23:55:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:56:39 | INFO | train_inner | epoch 172:     25 / 97 loss=1.488, nll_loss=0.899, ppl=1.87, wps=24396.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=16500, lr=0.000246183, gnorm=0.971, loss_scale=16, train_wall=238, gb_free=21, wall=44527
2022-03-04 23:59:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:59:50 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 13.483 | nll_loss 13.186 | ppl 9318.58 | wps 43840.1 | wpb 510.9 | bsz 1 | num_updates 16572 | best_loss 7.701
2022-03-04 23:59:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 16572 updates
2022-03-04 23:59:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:59:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-04 23:59:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 172 @ 16572 updates, score 13.483) (writing took 2.3534927368164062 seconds)
2022-03-04 23:59:52 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-03-04 23:59:52 | INFO | train | epoch 172 | loss 1.485 | nll_loss 0.897 | ppl 1.86 | wps 24615.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16572 | lr 0.000245648 | gnorm 0.971 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 44720
2022-03-04 23:59:52 | INFO | fairseq.trainer | begin training epoch 173
2022-03-04 23:59:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:59:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:01:08 | INFO | train_inner | epoch 173:     29 / 97 loss=1.486, nll_loss=0.898, ppl=1.86, wps=24378.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16600, lr=0.00024544, gnorm=0.97, loss_scale=16, train_wall=238, gb_free=21, wall=44795
2022-03-05 00:04:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:04:08 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 13.446 | nll_loss 13.145 | ppl 9061.17 | wps 43460.5 | wpb 510.9 | bsz 1 | num_updates 16668 | best_loss 7.701
2022-03-05 00:04:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 16668 updates
2022-03-05 00:04:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:04:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:04:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 173 @ 16668 updates, score 13.446) (writing took 2.4231444485485554 seconds)
2022-03-05 00:04:11 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-03-05 00:04:11 | INFO | train | epoch 173 | loss 1.48 | nll_loss 0.892 | ppl 1.86 | wps 24333.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16668 | lr 0.000244939 | gnorm 0.96 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 44979
2022-03-05 00:04:11 | INFO | fairseq.trainer | begin training epoch 174
2022-03-05 00:04:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:05:33 | INFO | train_inner | epoch 174:     32 / 97 loss=1.476, nll_loss=0.888, ppl=1.85, wps=24628.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16700, lr=0.000244704, gnorm=0.957, loss_scale=16, train_wall=236, gb_free=21, wall=45061
2022-03-05 00:05:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:08:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:08:26 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 13.462 | nll_loss 13.165 | ppl 9184.63 | wps 43825 | wpb 510.9 | bsz 1 | num_updates 16764 | best_loss 7.701
2022-03-05 00:08:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 16764 updates
2022-03-05 00:08:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:08:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:08:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 174 @ 16764 updates, score 13.462) (writing took 2.3795315735042095 seconds)
2022-03-05 00:08:29 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-03-05 00:08:29 | INFO | train | epoch 174 | loss 1.476 | nll_loss 0.887 | ppl 1.85 | wps 24363.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16764 | lr 0.000244237 | gnorm 0.961 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 45237
2022-03-05 00:08:29 | INFO | fairseq.trainer | begin training epoch 175
2022-03-05 00:08:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:10:02 | INFO | train_inner | epoch 175:     36 / 97 loss=1.474, nll_loss=0.885, ppl=1.85, wps=24394.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16800, lr=0.000243975, gnorm=0.961, loss_scale=16, train_wall=238, gb_free=21, wall=45330
2022-03-05 00:12:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:12:45 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 13.493 | nll_loss 13.194 | ppl 9370.06 | wps 43632.4 | wpb 510.9 | bsz 1 | num_updates 16861 | best_loss 7.701
2022-03-05 00:12:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 16861 updates
2022-03-05 00:12:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:12:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:12:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 175 @ 16861 updates, score 13.493) (writing took 2.4632143527269363 seconds)
2022-03-05 00:12:47 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-03-05 00:12:47 | INFO | train | epoch 175 | loss 1.472 | nll_loss 0.883 | ppl 1.84 | wps 24585.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16861 | lr 0.000243533 | gnorm 0.958 | loss_scale 32 | train_wall 229 | gb_free 21 | wall 45495
2022-03-05 00:12:47 | INFO | fairseq.trainer | begin training epoch 176
2022-03-05 00:12:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:13:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:14:30 | INFO | train_inner | epoch 176:     40 / 97 loss=1.469, nll_loss=0.881, ppl=1.84, wps=24386.1, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=16900, lr=0.000243252, gnorm=0.959, loss_scale=16, train_wall=238, gb_free=21, wall=45598
2022-03-05 00:16:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:17:03 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 13.513 | nll_loss 13.216 | ppl 9516.96 | wps 44058.9 | wpb 510.9 | bsz 1 | num_updates 16957 | best_loss 7.701
2022-03-05 00:17:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 16957 updates
2022-03-05 00:17:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:17:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:17:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 176 @ 16957 updates, score 13.513) (writing took 2.4371798653155565 seconds)
2022-03-05 00:17:05 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-03-05 00:17:05 | INFO | train | epoch 176 | loss 1.467 | nll_loss 0.879 | ppl 1.84 | wps 24364 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16957 | lr 0.000242843 | gnorm 0.95 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 45753
2022-03-05 00:17:05 | INFO | fairseq.trainer | begin training epoch 177
2022-03-05 00:17:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:18:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:18:59 | INFO | train_inner | epoch 177:     44 / 97 loss=1.467, nll_loss=0.878, ppl=1.84, wps=24403.3, ups=0.37, wpb=65495, bsz=127.9, num_updates=17000, lr=0.000242536, gnorm=0.946, loss_scale=16, train_wall=238, gb_free=21, wall=45867
2022-03-05 00:21:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:21:21 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 13.531 | nll_loss 13.233 | ppl 9626.12 | wps 43915.2 | wpb 510.9 | bsz 1 | num_updates 17053 | best_loss 7.701
2022-03-05 00:21:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 17053 updates
2022-03-05 00:21:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:21:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:21:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 177 @ 17053 updates, score 13.531) (writing took 2.345282042399049 seconds)
2022-03-05 00:21:23 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-03-05 00:21:23 | INFO | train | epoch 177 | loss 1.464 | nll_loss 0.875 | ppl 1.83 | wps 24363.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17053 | lr 0.000242158 | gnorm 0.954 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 46011
2022-03-05 00:21:23 | INFO | fairseq.trainer | begin training epoch 178
2022-03-05 00:21:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:23:25 | INFO | train_inner | epoch 178:     47 / 97 loss=1.459, nll_loss=0.871, ppl=1.83, wps=24632.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=17100, lr=0.000241825, gnorm=0.966, loss_scale=16, train_wall=236, gb_free=21, wall=46133
2022-03-05 00:24:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:25:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:25:39 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 13.444 | nll_loss 13.144 | ppl 9051.29 | wps 43518.8 | wpb 510.9 | bsz 1 | num_updates 17149 | best_loss 7.701
2022-03-05 00:25:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 17149 updates
2022-03-05 00:25:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:25:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:25:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 178 @ 17149 updates, score 13.444) (writing took 2.419615538790822 seconds)
2022-03-05 00:25:41 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-03-05 00:25:41 | INFO | train | epoch 178 | loss 1.458 | nll_loss 0.869 | ppl 1.83 | wps 24358.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17149 | lr 0.00024148 | gnorm 0.962 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 46269
2022-03-05 00:25:41 | INFO | fairseq.trainer | begin training epoch 179
2022-03-05 00:25:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:27:53 | INFO | train_inner | epoch 179:     51 / 97 loss=1.458, nll_loss=0.869, ppl=1.83, wps=24393.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17200, lr=0.000241121, gnorm=0.943, loss_scale=16, train_wall=238, gb_free=21, wall=46401
2022-03-05 00:29:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:29:57 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 13.497 | nll_loss 13.199 | ppl 9401.17 | wps 43922.6 | wpb 510.9 | bsz 1 | num_updates 17246 | best_loss 7.701
2022-03-05 00:29:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 17246 updates
2022-03-05 00:29:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:29:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:29:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 179 @ 17246 updates, score 13.497) (writing took 2.4543116306886077 seconds)
2022-03-05 00:29:59 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-03-05 00:29:59 | INFO | train | epoch 179 | loss 1.455 | nll_loss 0.866 | ppl 1.82 | wps 24605.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17246 | lr 0.0002408 | gnorm 0.947 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 46527
2022-03-05 00:29:59 | INFO | fairseq.trainer | begin training epoch 180
2022-03-05 00:29:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:30:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:32:22 | INFO | train_inner | epoch 180:     55 / 97 loss=1.454, nll_loss=0.865, ppl=1.82, wps=24398.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17300, lr=0.000240424, gnorm=0.963, loss_scale=16, train_wall=238, gb_free=21, wall=46670
2022-03-05 00:34:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:34:15 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 13.611 | nll_loss 13.316 | ppl 10199.3 | wps 43790.1 | wpb 510.9 | bsz 1 | num_updates 17342 | best_loss 7.701
2022-03-05 00:34:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 17342 updates
2022-03-05 00:34:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:34:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:34:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 180 @ 17342 updates, score 13.611) (writing took 2.443503367714584 seconds)
2022-03-05 00:34:18 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-03-05 00:34:18 | INFO | train | epoch 180 | loss 1.451 | nll_loss 0.862 | ppl 1.82 | wps 24357.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17342 | lr 0.000240132 | gnorm 0.96 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 46785
2022-03-05 00:34:18 | INFO | fairseq.trainer | begin training epoch 181
2022-03-05 00:34:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:36:48 | INFO | train_inner | epoch 181:     58 / 97 loss=1.446, nll_loss=0.857, ppl=1.81, wps=24618.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=17400, lr=0.000239732, gnorm=0.946, loss_scale=32, train_wall=236, gb_free=21, wall=46936
2022-03-05 00:36:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:38:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:38:33 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 13.525 | nll_loss 13.227 | ppl 9585.35 | wps 43884.4 | wpb 510.9 | bsz 1 | num_updates 17438 | best_loss 7.701
2022-03-05 00:38:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 17438 updates
2022-03-05 00:38:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:38:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:38:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 181 @ 17438 updates, score 13.525) (writing took 2.4396899957209826 seconds)
2022-03-05 00:38:36 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-03-05 00:38:36 | INFO | train | epoch 181 | loss 1.447 | nll_loss 0.858 | ppl 1.81 | wps 24344.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17438 | lr 0.00023947 | gnorm 0.948 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 47044
2022-03-05 00:38:36 | INFO | fairseq.trainer | begin training epoch 182
2022-03-05 00:38:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:41:16 | INFO | train_inner | epoch 182:     62 / 97 loss=1.446, nll_loss=0.857, ppl=1.81, wps=24385.4, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=17500, lr=0.000239046, gnorm=0.95, loss_scale=16, train_wall=238, gb_free=21, wall=47204
2022-03-05 00:42:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:42:52 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 13.562 | nll_loss 13.264 | ppl 9834.77 | wps 43529.8 | wpb 510.9 | bsz 1 | num_updates 17535 | best_loss 7.701
2022-03-05 00:42:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 17535 updates
2022-03-05 00:42:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:42:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:42:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 182 @ 17535 updates, score 13.562) (writing took 2.4371570516377687 seconds)
2022-03-05 00:42:54 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-03-05 00:42:54 | INFO | train | epoch 182 | loss 1.444 | nll_loss 0.854 | ppl 1.81 | wps 24595.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17535 | lr 0.000238807 | gnorm 0.949 | loss_scale 32 | train_wall 229 | gb_free 21 | wall 47302
2022-03-05 00:42:54 | INFO | fairseq.trainer | begin training epoch 183
2022-03-05 00:42:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:42:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:45:45 | INFO | train_inner | epoch 183:     66 / 97 loss=1.441, nll_loss=0.852, ppl=1.81, wps=24378.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17600, lr=0.000238366, gnorm=0.948, loss_scale=16, train_wall=238, gb_free=21, wall=47473
2022-03-05 00:47:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:47:10 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 13.56 | nll_loss 13.264 | ppl 9839.57 | wps 43767.4 | wpb 510.9 | bsz 1 | num_updates 17631 | best_loss 7.701
2022-03-05 00:47:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 17631 updates
2022-03-05 00:47:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:47:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:47:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 183 @ 17631 updates, score 13.56) (writing took 2.436096105724573 seconds)
2022-03-05 00:47:12 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-03-05 00:47:12 | INFO | train | epoch 183 | loss 1.439 | nll_loss 0.85 | ppl 1.8 | wps 24347.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17631 | lr 0.000238156 | gnorm 0.951 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 47560
2022-03-05 00:47:12 | INFO | fairseq.trainer | begin training epoch 184
2022-03-05 00:47:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:49:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:50:13 | INFO | train_inner | epoch 184:     70 / 97 loss=1.437, nll_loss=0.848, ppl=1.8, wps=24384.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17700, lr=0.000237691, gnorm=0.953, loss_scale=16, train_wall=238, gb_free=21, wall=47741
2022-03-05 00:51:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:51:28 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 13.557 | nll_loss 13.262 | ppl 9826.67 | wps 43547.6 | wpb 510.9 | bsz 1 | num_updates 17727 | best_loss 7.701
2022-03-05 00:51:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 17727 updates
2022-03-05 00:51:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:51:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:51:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 184 @ 17727 updates, score 13.557) (writing took 2.419173558242619 seconds)
2022-03-05 00:51:31 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-03-05 00:51:31 | INFO | train | epoch 184 | loss 1.435 | nll_loss 0.846 | ppl 1.8 | wps 24343.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17727 | lr 0.00023751 | gnorm 0.947 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 47819
2022-03-05 00:51:31 | INFO | fairseq.trainer | begin training epoch 185
2022-03-05 00:51:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:54:39 | INFO | train_inner | epoch 185:     73 / 97 loss=1.434, nll_loss=0.845, ppl=1.8, wps=24621.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17800, lr=0.000237023, gnorm=0.936, loss_scale=16, train_wall=236, gb_free=21, wall=48007
2022-03-05 00:55:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:55:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:55:46 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 13.587 | nll_loss 13.291 | ppl 10022.9 | wps 43779.4 | wpb 510.9 | bsz 1 | num_updates 17823 | best_loss 7.701
2022-03-05 00:55:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 17823 updates
2022-03-05 00:55:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:55:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 00:55:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 185 @ 17823 updates, score 13.587) (writing took 2.4290525568649173 seconds)
2022-03-05 00:55:49 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-03-05 00:55:49 | INFO | train | epoch 185 | loss 1.431 | nll_loss 0.842 | ppl 1.79 | wps 24367.5 | ups 0.37 | wpb 65533.8 | bsz 128 | num_updates 17823 | lr 0.00023687 | gnorm 0.935 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 48077
2022-03-05 00:55:49 | INFO | fairseq.trainer | begin training epoch 186
2022-03-05 00:55:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:59:08 | INFO | train_inner | epoch 186:     77 / 97 loss=1.428, nll_loss=0.839, ppl=1.79, wps=24409.5, ups=0.37, wpb=65533.9, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.948, loss_scale=16, train_wall=238, gb_free=21, wall=48276
2022-03-05 01:00:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:00:05 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 13.555 | nll_loss 13.261 | ppl 9818.21 | wps 43741.3 | wpb 510.9 | bsz 1 | num_updates 17920 | best_loss 7.701
2022-03-05 01:00:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 17920 updates
2022-03-05 01:00:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:00:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:00:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 186 @ 17920 updates, score 13.555) (writing took 2.4223328391090035 seconds)
2022-03-05 01:00:07 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-03-05 01:00:07 | INFO | train | epoch 186 | loss 1.43 | nll_loss 0.84 | ppl 1.79 | wps 24608.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17920 | lr 0.000236228 | gnorm 0.946 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 48335
2022-03-05 01:00:07 | INFO | fairseq.trainer | begin training epoch 187
2022-03-05 01:00:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:01:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:03:37 | INFO | train_inner | epoch 187:     81 / 97 loss=1.426, nll_loss=0.837, ppl=1.79, wps=24388.7, ups=0.37, wpb=65495, bsz=127.9, num_updates=18000, lr=0.000235702, gnorm=0.946, loss_scale=16, train_wall=238, gb_free=21, wall=48544
2022-03-05 01:04:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:04:23 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 13.555 | nll_loss 13.26 | ppl 9808.79 | wps 43491.8 | wpb 510.9 | bsz 1 | num_updates 18016 | best_loss 7.701
2022-03-05 01:04:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 18016 updates
2022-03-05 01:04:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:04:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:04:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 187 @ 18016 updates, score 13.555) (writing took 2.4189539020881057 seconds)
2022-03-05 01:04:25 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-03-05 01:04:25 | INFO | train | epoch 187 | loss 1.423 | nll_loss 0.834 | ppl 1.78 | wps 24347.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18016 | lr 0.000235598 | gnorm 0.946 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 48593
2022-03-05 01:04:25 | INFO | fairseq.trainer | begin training epoch 188
2022-03-05 01:04:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:07:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:08:05 | INFO | train_inner | epoch 188:     85 / 97 loss=1.422, nll_loss=0.832, ppl=1.78, wps=24384, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18100, lr=0.00023505, gnorm=0.942, loss_scale=16, train_wall=238, gb_free=21, wall=48813
2022-03-05 01:08:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:08:41 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 13.604 | nll_loss 13.31 | ppl 10157.7 | wps 43849.3 | wpb 510.9 | bsz 1 | num_updates 18112 | best_loss 7.701
2022-03-05 01:08:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 18112 updates
2022-03-05 01:08:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:08:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:08:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 188 @ 18112 updates, score 13.604) (writing took 2.4565351475030184 seconds)
2022-03-05 01:08:43 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-03-05 01:08:43 | INFO | train | epoch 188 | loss 1.42 | nll_loss 0.831 | ppl 1.78 | wps 24346.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18112 | lr 0.000234972 | gnorm 0.943 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 48851
2022-03-05 01:08:43 | INFO | fairseq.trainer | begin training epoch 189
2022-03-05 01:08:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:12:31 | INFO | train_inner | epoch 189:     88 / 97 loss=1.418, nll_loss=0.828, ppl=1.78, wps=24630.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18200, lr=0.000234404, gnorm=0.936, loss_scale=16, train_wall=236, gb_free=21, wall=49079
2022-03-05 01:12:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:12:59 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 13.651 | nll_loss 13.358 | ppl 10501.9 | wps 43861.7 | wpb 510.9 | bsz 1 | num_updates 18209 | best_loss 7.701
2022-03-05 01:12:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 18209 updates
2022-03-05 01:12:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:13:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:13:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 189 @ 18209 updates, score 13.651) (writing took 2.4553211592137814 seconds)
2022-03-05 01:13:02 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-03-05 01:13:02 | INFO | train | epoch 189 | loss 1.417 | nll_loss 0.827 | ppl 1.77 | wps 24606.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18209 | lr 0.000234346 | gnorm 0.934 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 49109
2022-03-05 01:13:02 | INFO | fairseq.trainer | begin training epoch 190
2022-03-05 01:13:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:13:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:17:00 | INFO | train_inner | epoch 190:     92 / 97 loss=1.415, nll_loss=0.826, ppl=1.77, wps=24384.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=18300, lr=0.000233762, gnorm=0.938, loss_scale=16, train_wall=238, gb_free=21, wall=49347
2022-03-05 01:17:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:17:17 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 13.626 | nll_loss 13.333 | ppl 10318.8 | wps 43772.4 | wpb 510.9 | bsz 1 | num_updates 18305 | best_loss 7.701
2022-03-05 01:17:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 18305 updates
2022-03-05 01:17:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:17:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:17:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 190 @ 18305 updates, score 13.626) (writing took 2.4308287613093853 seconds)
2022-03-05 01:17:20 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-03-05 01:17:20 | INFO | train | epoch 190 | loss 1.413 | nll_loss 0.824 | ppl 1.77 | wps 24346 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18305 | lr 0.00023373 | gnorm 0.94 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 49368
2022-03-05 01:17:20 | INFO | fairseq.trainer | begin training epoch 191
2022-03-05 01:17:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:19:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:21:28 | INFO | train_inner | epoch 191:     96 / 97 loss=1.41, nll_loss=0.82, ppl=1.77, wps=24381.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18400, lr=0.000233126, gnorm=0.943, loss_scale=16, train_wall=238, gb_free=21, wall=49616
2022-03-05 01:21:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:21:36 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 13.587 | nll_loss 13.293 | ppl 10038.2 | wps 43726.7 | wpb 510.9 | bsz 1 | num_updates 18401 | best_loss 7.701
2022-03-05 01:21:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 18401 updates
2022-03-05 01:21:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:21:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:21:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 191 @ 18401 updates, score 13.587) (writing took 2.416898018680513 seconds)
2022-03-05 01:21:38 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-03-05 01:21:38 | INFO | train | epoch 191 | loss 1.408 | nll_loss 0.819 | ppl 1.76 | wps 24343.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18401 | lr 0.00023312 | gnorm 0.941 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 49626
2022-03-05 01:21:38 | INFO | fairseq.trainer | begin training epoch 192
2022-03-05 01:21:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:25:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:25:54 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 13.629 | nll_loss 13.334 | ppl 10329.6 | wps 43812 | wpb 510.9 | bsz 1 | num_updates 18498 | best_loss 7.701
2022-03-05 01:25:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 18498 updates
2022-03-05 01:25:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:25:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:25:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 192 @ 18498 updates, score 13.629) (writing took 2.3610931457951665 seconds)
2022-03-05 01:25:56 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-03-05 01:25:56 | INFO | train | epoch 192 | loss 1.405 | nll_loss 0.816 | ppl 1.76 | wps 24616.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18498 | lr 0.000232508 | gnorm 0.936 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 49884
2022-03-05 01:25:56 | INFO | fairseq.trainer | begin training epoch 193
2022-03-05 01:25:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:26:01 | INFO | train_inner | epoch 193:      2 / 97 loss=1.405, nll_loss=0.816, ppl=1.76, wps=23953.8, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=18500, lr=0.000232495, gnorm=0.935, loss_scale=32, train_wall=235, gb_free=21, wall=49889
2022-03-05 01:26:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:30:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:30:12 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 13.634 | nll_loss 13.339 | ppl 10364.1 | wps 43861.5 | wpb 510.9 | bsz 1 | num_updates 18594 | best_loss 7.701
2022-03-05 01:30:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 18594 updates
2022-03-05 01:30:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:30:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:30:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 193 @ 18594 updates, score 13.634) (writing took 2.4298353251069784 seconds)
2022-03-05 01:30:15 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-03-05 01:30:15 | INFO | train | epoch 193 | loss 1.402 | nll_loss 0.812 | ppl 1.76 | wps 24327.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18594 | lr 0.000231907 | gnorm 0.936 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 50143
2022-03-05 01:30:15 | INFO | fairseq.trainer | begin training epoch 194
2022-03-05 01:30:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:30:30 | INFO | train_inner | epoch 194:      6 / 97 loss=1.4, nll_loss=0.81, ppl=1.75, wps=24367.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18600, lr=0.000231869, gnorm=0.936, loss_scale=16, train_wall=238, gb_free=21, wall=50158
2022-03-05 01:32:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:34:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:34:30 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 13.617 | nll_loss 13.326 | ppl 10266.9 | wps 43661.3 | wpb 510.9 | bsz 1 | num_updates 18690 | best_loss 7.701
2022-03-05 01:34:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 18690 updates
2022-03-05 01:34:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:34:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:34:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 194 @ 18690 updates, score 13.617) (writing took 2.428301118314266 seconds)
2022-03-05 01:34:33 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-03-05 01:34:33 | INFO | train | epoch 194 | loss 1.399 | nll_loss 0.809 | ppl 1.75 | wps 24356.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18690 | lr 0.000231311 | gnorm 0.939 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 50401
2022-03-05 01:34:33 | INFO | fairseq.trainer | begin training epoch 195
2022-03-05 01:34:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:34:59 | INFO | train_inner | epoch 195:     10 / 97 loss=1.397, nll_loss=0.808, ppl=1.75, wps=24394.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18700, lr=0.000231249, gnorm=0.936, loss_scale=16, train_wall=238, gb_free=21, wall=50427
2022-03-05 01:37:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:38:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:38:48 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 13.657 | nll_loss 13.366 | ppl 10557.1 | wps 43559.1 | wpb 510.9 | bsz 1 | num_updates 18786 | best_loss 7.701
2022-03-05 01:38:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 18786 updates
2022-03-05 01:38:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:38:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:38:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 195 @ 18786 updates, score 13.657) (writing took 2.4235220635309815 seconds)
2022-03-05 01:38:51 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-03-05 01:38:51 | INFO | train | epoch 195 | loss 1.395 | nll_loss 0.805 | ppl 1.75 | wps 24355 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18786 | lr 0.000230719 | gnorm 0.926 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 50659
2022-03-05 01:38:51 | INFO | fairseq.trainer | begin training epoch 196
2022-03-05 01:38:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:39:27 | INFO | train_inner | epoch 196:     14 / 97 loss=1.394, nll_loss=0.804, ppl=1.75, wps=24394.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18800, lr=0.000230633, gnorm=0.928, loss_scale=16, train_wall=238, gb_free=21, wall=50695
2022-03-05 01:43:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:43:07 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 13.562 | nll_loss 13.268 | ppl 9864.08 | wps 43694.9 | wpb 510.9 | bsz 1 | num_updates 18883 | best_loss 7.701
2022-03-05 01:43:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 18883 updates
2022-03-05 01:43:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:43:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:43:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 196 @ 18883 updates, score 13.562) (writing took 2.41277714446187 seconds)
2022-03-05 01:43:09 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-03-05 01:43:09 | INFO | train | epoch 196 | loss 1.392 | nll_loss 0.802 | ppl 1.74 | wps 24609.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18883 | lr 0.000230125 | gnorm 0.934 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 50917
2022-03-05 01:43:09 | INFO | fairseq.trainer | begin training epoch 197
2022-03-05 01:43:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:43:53 | INFO | train_inner | epoch 197:     17 / 97 loss=1.39, nll_loss=0.8, ppl=1.74, wps=24627.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18900, lr=0.000230022, gnorm=0.933, loss_scale=32, train_wall=235, gb_free=21, wall=50961
2022-03-05 01:45:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:47:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:47:25 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 13.683 | nll_loss 13.392 | ppl 10747.9 | wps 43659.2 | wpb 510.9 | bsz 1 | num_updates 18979 | best_loss 7.701
2022-03-05 01:47:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 18979 updates
2022-03-05 01:47:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:47:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:47:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 197 @ 18979 updates, score 13.683) (writing took 2.379466414451599 seconds)
2022-03-05 01:47:27 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-03-05 01:47:27 | INFO | train | epoch 197 | loss 1.388 | nll_loss 0.799 | ppl 1.74 | wps 24346.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18979 | lr 0.000229543 | gnorm 0.929 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 51175
2022-03-05 01:47:27 | INFO | fairseq.trainer | begin training epoch 198
2022-03-05 01:47:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:48:22 | INFO | train_inner | epoch 198:     21 / 97 loss=1.386, nll_loss=0.796, ppl=1.74, wps=24387, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=19000, lr=0.000229416, gnorm=0.928, loss_scale=16, train_wall=238, gb_free=21, wall=51230
2022-03-05 01:51:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:51:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:51:43 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 13.659 | nll_loss 13.368 | ppl 10571.5 | wps 43787.7 | wpb 510.9 | bsz 1 | num_updates 19075 | best_loss 7.701
2022-03-05 01:51:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 19075 updates
2022-03-05 01:51:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:51:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:51:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 198 @ 19075 updates, score 13.659) (writing took 2.4382897010073066 seconds)
2022-03-05 01:51:45 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-03-05 01:51:45 | INFO | train | epoch 198 | loss 1.386 | nll_loss 0.796 | ppl 1.74 | wps 24351.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19075 | lr 0.000228964 | gnorm 0.931 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 51433
2022-03-05 01:51:45 | INFO | fairseq.trainer | begin training epoch 199
2022-03-05 01:51:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:52:50 | INFO | train_inner | epoch 199:     25 / 97 loss=1.384, nll_loss=0.794, ppl=1.73, wps=24393.3, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=19100, lr=0.000228814, gnorm=0.931, loss_scale=16, train_wall=238, gb_free=21, wall=51498
2022-03-05 01:55:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:56:01 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 13.642 | nll_loss 13.351 | ppl 10450 | wps 43313.5 | wpb 510.9 | bsz 1 | num_updates 19172 | best_loss 7.701
2022-03-05 01:56:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 19172 updates
2022-03-05 01:56:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:56:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 01:56:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 199 @ 19172 updates, score 13.642) (writing took 2.435405321419239 seconds)
2022-03-05 01:56:04 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-03-05 01:56:04 | INFO | train | epoch 199 | loss 1.382 | nll_loss 0.793 | ppl 1.73 | wps 24596.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19172 | lr 0.000228384 | gnorm 0.918 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 51692
2022-03-05 01:56:04 | INFO | fairseq.trainer | begin training epoch 200
2022-03-05 01:56:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:56:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:57:19 | INFO | train_inner | epoch 200:     29 / 97 loss=1.38, nll_loss=0.79, ppl=1.73, wps=24382.3, ups=0.37, wpb=65495, bsz=127.9, num_updates=19200, lr=0.000228218, gnorm=0.916, loss_scale=16, train_wall=238, gb_free=21, wall=51767
2022-03-05 02:00:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:00:19 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 13.656 | nll_loss 13.363 | ppl 10538 | wps 43807 | wpb 510.9 | bsz 1 | num_updates 19268 | best_loss 7.701
2022-03-05 02:00:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 19268 updates
2022-03-05 02:00:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:00:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:00:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 200 @ 19268 updates, score 13.656) (writing took 2.4130777791142464 seconds)
2022-03-05 02:00:22 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-03-05 02:00:22 | INFO | train | epoch 200 | loss 1.379 | nll_loss 0.789 | ppl 1.73 | wps 24356.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19268 | lr 0.000227815 | gnorm 0.928 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 51950
2022-03-05 02:00:22 | INFO | fairseq.trainer | begin training epoch 201
2022-03-05 02:00:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:01:45 | INFO | train_inner | epoch 201:     32 / 97 loss=1.379, nll_loss=0.789, ppl=1.73, wps=24628.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=19300, lr=0.000227626, gnorm=0.93, loss_scale=16, train_wall=236, gb_free=21, wall=52033
2022-03-05 02:03:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:04:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:04:38 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 13.695 | nll_loss 13.405 | ppl 10843.7 | wps 43830.3 | wpb 510.9 | bsz 1 | num_updates 19364 | best_loss 7.701
2022-03-05 02:04:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 19364 updates
2022-03-05 02:04:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:04:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:04:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 201 @ 19364 updates, score 13.695) (writing took 2.4545034784823656 seconds)
2022-03-05 02:04:40 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-03-05 02:04:40 | INFO | train | epoch 201 | loss 1.375 | nll_loss 0.785 | ppl 1.72 | wps 24347.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19364 | lr 0.000227249 | gnorm 0.915 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 52208
2022-03-05 02:04:40 | INFO | fairseq.trainer | begin training epoch 202
2022-03-05 02:04:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:06:13 | INFO | train_inner | epoch 202:     36 / 97 loss=1.372, nll_loss=0.782, ppl=1.72, wps=24385, ups=0.37, wpb=65495, bsz=127.9, num_updates=19400, lr=0.000227038, gnorm=0.911, loss_scale=16, train_wall=238, gb_free=21, wall=52301
2022-03-05 02:08:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:08:56 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 13.712 | nll_loss 13.422 | ppl 10972.6 | wps 43636.2 | wpb 510.9 | bsz 1 | num_updates 19461 | best_loss 7.701
2022-03-05 02:08:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 19461 updates
2022-03-05 02:08:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:08:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:08:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 202 @ 19461 updates, score 13.712) (writing took 2.409107697196305 seconds)
2022-03-05 02:08:58 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-03-05 02:08:58 | INFO | train | epoch 202 | loss 1.372 | nll_loss 0.782 | ppl 1.72 | wps 24603.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19461 | lr 0.000226682 | gnorm 0.925 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 52466
2022-03-05 02:08:58 | INFO | fairseq.trainer | begin training epoch 203
2022-03-05 02:08:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:10:39 | INFO | train_inner | epoch 203:     39 / 97 loss=1.371, nll_loss=0.781, ppl=1.72, wps=24621.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=19500, lr=0.000226455, gnorm=0.922, loss_scale=32, train_wall=236, gb_free=21, wall=52567
2022-03-05 02:11:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:13:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:13:14 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 13.624 | nll_loss 13.332 | ppl 10312 | wps 43658.9 | wpb 510.9 | bsz 1 | num_updates 19557 | best_loss 7.701
2022-03-05 02:13:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 19557 updates
2022-03-05 02:13:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:13:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:13:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 203 @ 19557 updates, score 13.624) (writing took 2.3652285737916827 seconds)
2022-03-05 02:13:16 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-03-05 02:13:16 | INFO | train | epoch 203 | loss 1.369 | nll_loss 0.779 | ppl 1.72 | wps 24351.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19557 | lr 0.000226125 | gnorm 0.921 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 52724
2022-03-05 02:13:16 | INFO | fairseq.trainer | begin training epoch 204
2022-03-05 02:13:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:15:08 | INFO | train_inner | epoch 204:     43 / 97 loss=1.368, nll_loss=0.778, ppl=1.71, wps=24389.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=19600, lr=0.000225877, gnorm=0.928, loss_scale=16, train_wall=238, gb_free=21, wall=52836
2022-03-05 02:17:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:17:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:17:32 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 13.678 | nll_loss 13.388 | ppl 10722 | wps 43855.5 | wpb 510.9 | bsz 1 | num_updates 19653 | best_loss 7.701
2022-03-05 02:17:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 19653 updates
2022-03-05 02:17:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:17:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:17:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 204 @ 19653 updates, score 13.678) (writing took 2.4638547683134675 seconds)
2022-03-05 02:17:35 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-03-05 02:17:35 | INFO | train | epoch 204 | loss 1.366 | nll_loss 0.776 | ppl 1.71 | wps 24347.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19653 | lr 0.000225572 | gnorm 0.915 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 52983
2022-03-05 02:17:35 | INFO | fairseq.trainer | begin training epoch 205
2022-03-05 02:17:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:19:36 | INFO | train_inner | epoch 205:     47 / 97 loss=1.366, nll_loss=0.776, ppl=1.71, wps=24390.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=19700, lr=0.000225303, gnorm=0.912, loss_scale=16, train_wall=238, gb_free=21, wall=53104
2022-03-05 02:21:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:21:50 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 13.718 | nll_loss 13.43 | ppl 11039.1 | wps 43904.4 | wpb 510.9 | bsz 1 | num_updates 19750 | best_loss 7.701
2022-03-05 02:21:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 19750 updates
2022-03-05 02:21:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:21:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:21:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 205 @ 19750 updates, score 13.718) (writing took 2.408016030676663 seconds)
2022-03-05 02:21:53 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-03-05 02:21:53 | INFO | train | epoch 205 | loss 1.364 | nll_loss 0.774 | ppl 1.71 | wps 24609.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19750 | lr 0.000225018 | gnorm 0.924 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 53241
2022-03-05 02:21:53 | INFO | fairseq.trainer | begin training epoch 206
2022-03-05 02:21:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:23:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:24:05 | INFO | train_inner | epoch 206:     51 / 97 loss=1.362, nll_loss=0.772, ppl=1.71, wps=24400.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=19800, lr=0.000224733, gnorm=0.928, loss_scale=16, train_wall=238, gb_free=21, wall=53373
2022-03-05 02:26:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:26:09 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 13.667 | nll_loss 13.376 | ppl 10627.5 | wps 43746.4 | wpb 510.9 | bsz 1 | num_updates 19846 | best_loss 7.701
2022-03-05 02:26:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 19846 updates
2022-03-05 02:26:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:26:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:26:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 206 @ 19846 updates, score 13.667) (writing took 2.4410899681970477 seconds)
2022-03-05 02:26:11 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-03-05 02:26:11 | INFO | train | epoch 206 | loss 1.36 | nll_loss 0.77 | ppl 1.71 | wps 24357.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19846 | lr 0.000224473 | gnorm 0.924 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 53499
2022-03-05 02:26:11 | INFO | fairseq.trainer | begin training epoch 207
2022-03-05 02:26:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:28:31 | INFO | train_inner | epoch 207:     54 / 97 loss=1.36, nll_loss=0.77, ppl=1.7, wps=24618.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=19900, lr=0.000224168, gnorm=0.921, loss_scale=16, train_wall=236, gb_free=21, wall=53639
2022-03-05 02:30:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:30:27 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 13.722 | nll_loss 13.434 | ppl 11066.5 | wps 43412.6 | wpb 510.9 | bsz 1 | num_updates 19943 | best_loss 7.701
2022-03-05 02:30:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 19943 updates
2022-03-05 02:30:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:30:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:30:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 207 @ 19943 updates, score 13.722) (writing took 2.42992159537971 seconds)
2022-03-05 02:30:29 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-03-05 02:30:29 | INFO | train | epoch 207 | loss 1.357 | nll_loss 0.768 | ppl 1.7 | wps 24589.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19943 | lr 0.000223926 | gnorm 0.912 | loss_scale 32 | train_wall 229 | gb_free 21 | wall 53757
2022-03-05 02:30:29 | INFO | fairseq.trainer | begin training epoch 208
2022-03-05 02:30:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:30:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:32:59 | INFO | train_inner | epoch 208:     58 / 97 loss=1.355, nll_loss=0.765, ppl=1.7, wps=24380.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=20000, lr=0.000223607, gnorm=0.907, loss_scale=16, train_wall=238, gb_free=21, wall=53907
2022-03-05 02:34:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:34:45 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 13.682 | nll_loss 13.391 | ppl 10742.2 | wps 43702.1 | wpb 510.9 | bsz 1 | num_updates 20039 | best_loss 7.701
2022-03-05 02:34:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 20039 updates
2022-03-05 02:34:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:34:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:34:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 208 @ 20039 updates, score 13.682) (writing took 2.4123982889577746 seconds)
2022-03-05 02:34:47 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-03-05 02:34:47 | INFO | train | epoch 208 | loss 1.354 | nll_loss 0.764 | ppl 1.7 | wps 24354.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20039 | lr 0.000223389 | gnorm 0.91 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 54015
2022-03-05 02:34:47 | INFO | fairseq.trainer | begin training epoch 209
2022-03-05 02:34:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:36:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:37:28 | INFO | train_inner | epoch 209:     62 / 97 loss=1.352, nll_loss=0.762, ppl=1.7, wps=24381.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20100, lr=0.00022305, gnorm=0.913, loss_scale=16, train_wall=238, gb_free=21, wall=54176
2022-03-05 02:38:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:39:03 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 13.681 | nll_loss 13.392 | ppl 10748.3 | wps 43798 | wpb 510.9 | bsz 1 | num_updates 20135 | best_loss 7.701
2022-03-05 02:39:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 20135 updates
2022-03-05 02:39:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:39:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:39:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 209 @ 20135 updates, score 13.681) (writing took 2.366813966073096 seconds)
2022-03-05 02:39:06 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-03-05 02:39:06 | INFO | train | epoch 209 | loss 1.351 | nll_loss 0.761 | ppl 1.69 | wps 24344.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20135 | lr 0.000222856 | gnorm 0.916 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 54274
2022-03-05 02:39:06 | INFO | fairseq.trainer | begin training epoch 210
2022-03-05 02:39:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:41:54 | INFO | train_inner | epoch 210:     65 / 97 loss=1.349, nll_loss=0.759, ppl=1.69, wps=24633.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=20200, lr=0.000222497, gnorm=0.913, loss_scale=16, train_wall=236, gb_free=21, wall=54442
2022-03-05 02:42:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:43:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:43:21 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 13.726 | nll_loss 13.438 | ppl 11100.7 | wps 43781.1 | wpb 510.9 | bsz 1 | num_updates 20231 | best_loss 7.701
2022-03-05 02:43:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 20231 updates
2022-03-05 02:43:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:43:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:43:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 210 @ 20231 updates, score 13.726) (writing took 2.4522266453132033 seconds)
2022-03-05 02:43:24 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-03-05 02:43:24 | INFO | train | epoch 210 | loss 1.347 | nll_loss 0.757 | ppl 1.69 | wps 24357.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20231 | lr 0.000222327 | gnorm 0.915 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 54532
2022-03-05 02:43:24 | INFO | fairseq.trainer | begin training epoch 211
2022-03-05 02:43:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:46:22 | INFO | train_inner | epoch 211:     69 / 97 loss=1.347, nll_loss=0.757, ppl=1.69, wps=24387.8, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=20300, lr=0.000221948, gnorm=0.908, loss_scale=16, train_wall=238, gb_free=21, wall=54710
2022-03-05 02:47:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:47:40 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 13.7 | nll_loss 13.412 | ppl 10900 | wps 43925 | wpb 510.9 | bsz 1 | num_updates 20328 | best_loss 7.701
2022-03-05 02:47:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 20328 updates
2022-03-05 02:47:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:47:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:47:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 211 @ 20328 updates, score 13.7) (writing took 2.4657573057338595 seconds)
2022-03-05 02:47:42 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-03-05 02:47:42 | INFO | train | epoch 211 | loss 1.346 | nll_loss 0.756 | ppl 1.69 | wps 24590.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20328 | lr 0.000221795 | gnorm 0.903 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 54790
2022-03-05 02:47:42 | INFO | fairseq.trainer | begin training epoch 212
2022-03-05 02:47:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:48:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:50:51 | INFO | train_inner | epoch 212:     73 / 97 loss=1.344, nll_loss=0.754, ppl=1.69, wps=24380.6, ups=0.37, wpb=65495, bsz=127.9, num_updates=20400, lr=0.000221404, gnorm=0.916, loss_scale=16, train_wall=238, gb_free=21, wall=54979
2022-03-05 02:51:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:51:58 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 13.753 | nll_loss 13.468 | ppl 11328.5 | wps 43688.2 | wpb 510.9 | bsz 1 | num_updates 20424 | best_loss 7.701
2022-03-05 02:51:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 20424 updates
2022-03-05 02:51:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:52:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:52:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 212 @ 20424 updates, score 13.753) (writing took 2.410001313313842 seconds)
2022-03-05 02:52:00 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-03-05 02:52:00 | INFO | train | epoch 212 | loss 1.342 | nll_loss 0.752 | ppl 1.68 | wps 24345.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20424 | lr 0.000221274 | gnorm 0.919 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 55048
2022-03-05 02:52:00 | INFO | fairseq.trainer | begin training epoch 213
2022-03-05 02:52:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:53:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:55:20 | INFO | train_inner | epoch 213:     77 / 97 loss=1.342, nll_loss=0.753, ppl=1.68, wps=24386.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20500, lr=0.000220863, gnorm=0.922, loss_scale=16, train_wall=238, gb_free=21, wall=55248
2022-03-05 02:56:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:56:16 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 13.69 | nll_loss 13.402 | ppl 10827.3 | wps 42970.6 | wpb 510.9 | bsz 1 | num_updates 20520 | best_loss 7.701
2022-03-05 02:56:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 20520 updates
2022-03-05 02:56:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:56:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 02:56:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 213 @ 20520 updates, score 13.69) (writing took 2.4734087074175477 seconds)
2022-03-05 02:56:19 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-03-05 02:56:19 | INFO | train | epoch 213 | loss 1.34 | nll_loss 0.75 | ppl 1.68 | wps 24329.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20520 | lr 0.000220755 | gnorm 0.915 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 55307
2022-03-05 02:56:19 | INFO | fairseq.trainer | begin training epoch 214
2022-03-05 02:56:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:59:46 | INFO | train_inner | epoch 214:     80 / 97 loss=1.338, nll_loss=0.748, ppl=1.68, wps=24570.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=20600, lr=0.000220326, gnorm=0.912, loss_scale=32, train_wall=236, gb_free=21, wall=55514
2022-03-05 03:00:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:00:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:00:35 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 13.784 | nll_loss 13.499 | ppl 11574.2 | wps 43630.4 | wpb 510.9 | bsz 1 | num_updates 20616 | best_loss 7.701
2022-03-05 03:00:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 20616 updates
2022-03-05 03:00:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:00:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:00:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 214 @ 20616 updates, score 13.784) (writing took 2.480907275341451 seconds)
2022-03-05 03:00:38 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-03-05 03:00:38 | INFO | train | epoch 214 | loss 1.337 | nll_loss 0.747 | ppl 1.68 | wps 24306.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20616 | lr 0.000220241 | gnorm 0.915 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 55565
2022-03-05 03:00:38 | INFO | fairseq.trainer | begin training epoch 215
2022-03-05 03:00:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:04:15 | INFO | train_inner | epoch 215:     84 / 97 loss=1.336, nll_loss=0.746, ppl=1.68, wps=24355, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20700, lr=0.000219793, gnorm=0.91, loss_scale=16, train_wall=238, gb_free=21, wall=55783
2022-03-05 03:04:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:04:54 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 13.696 | nll_loss 13.406 | ppl 10855.6 | wps 43463.2 | wpb 510.9 | bsz 1 | num_updates 20713 | best_loss 7.701
2022-03-05 03:04:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 20713 updates
2022-03-05 03:04:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:04:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:04:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 215 @ 20713 updates, score 13.696) (writing took 2.523325925692916 seconds)
2022-03-05 03:04:56 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-03-05 03:04:56 | INFO | train | epoch 215 | loss 1.334 | nll_loss 0.744 | ppl 1.67 | wps 24563 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20713 | lr 0.000219725 | gnorm 0.909 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 55824
2022-03-05 03:04:56 | INFO | fairseq.trainer | begin training epoch 216
2022-03-05 03:04:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:07:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:08:44 | INFO | train_inner | epoch 216:     88 / 97 loss=1.331, nll_loss=0.741, ppl=1.67, wps=24353, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20800, lr=0.000219265, gnorm=0.89, loss_scale=16, train_wall=238, gb_free=21, wall=56052
2022-03-05 03:09:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:09:12 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 13.716 | nll_loss 13.427 | ppl 11017.1 | wps 43566.4 | wpb 510.9 | bsz 1 | num_updates 20809 | best_loss 7.701
2022-03-05 03:09:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 20809 updates
2022-03-05 03:09:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:09:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:09:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 216 @ 20809 updates, score 13.716) (writing took 2.4745668983086944 seconds)
2022-03-05 03:09:15 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-03-05 03:09:15 | INFO | train | epoch 216 | loss 1.33 | nll_loss 0.74 | ppl 1.67 | wps 24318.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20809 | lr 0.000219217 | gnorm 0.891 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 56083
2022-03-05 03:09:15 | INFO | fairseq.trainer | begin training epoch 217
2022-03-05 03:09:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:13:10 | INFO | train_inner | epoch 217:     91 / 97 loss=1.33, nll_loss=0.74, ppl=1.67, wps=24592.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20900, lr=0.000218739, gnorm=0.912, loss_scale=32, train_wall=236, gb_free=21, wall=56318
2022-03-05 03:13:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:13:31 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 13.727 | nll_loss 13.44 | ppl 11109.5 | wps 43623 | wpb 510.9 | bsz 1 | num_updates 20906 | best_loss 7.701
2022-03-05 03:13:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 20906 updates
2022-03-05 03:13:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:13:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:13:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 217 @ 20906 updates, score 13.727) (writing took 2.479348273947835 seconds)
2022-03-05 03:13:33 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-03-05 03:13:33 | INFO | train | epoch 217 | loss 1.329 | nll_loss 0.739 | ppl 1.67 | wps 24571.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20906 | lr 0.000218708 | gnorm 0.909 | loss_scale 32 | train_wall 229 | gb_free 21 | wall 56341
2022-03-05 03:13:33 | INFO | fairseq.trainer | begin training epoch 218
2022-03-05 03:13:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:15:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:17:39 | INFO | train_inner | epoch 218:     95 / 97 loss=1.328, nll_loss=0.738, ppl=1.67, wps=24369.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21000, lr=0.000218218, gnorm=0.905, loss_scale=16, train_wall=238, gb_free=21, wall=56587
2022-03-05 03:17:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:17:49 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 13.736 | nll_loss 13.448 | ppl 11178.1 | wps 43632.2 | wpb 510.9 | bsz 1 | num_updates 21002 | best_loss 7.701
2022-03-05 03:17:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 21002 updates
2022-03-05 03:17:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:17:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:17:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 218 @ 21002 updates, score 13.736) (writing took 2.504921892657876 seconds)
2022-03-05 03:17:52 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-03-05 03:17:52 | INFO | train | epoch 218 | loss 1.327 | nll_loss 0.737 | ppl 1.67 | wps 24326.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21002 | lr 0.000218207 | gnorm 0.906 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 56600
2022-03-05 03:17:52 | INFO | fairseq.trainer | begin training epoch 219
2022-03-05 03:17:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:21:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:22:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:22:08 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 13.744 | nll_loss 13.458 | ppl 11250.4 | wps 43829.8 | wpb 510.9 | bsz 1 | num_updates 21098 | best_loss 7.701
2022-03-05 03:22:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 21098 updates
2022-03-05 03:22:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:22:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:22:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 219 @ 21098 updates, score 13.744) (writing took 2.680421535857022 seconds)
2022-03-05 03:22:10 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-03-05 03:22:10 | INFO | train | epoch 219 | loss 1.323 | nll_loss 0.733 | ppl 1.66 | wps 24294.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21098 | lr 0.00021771 | gnorm 0.91 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 56858
2022-03-05 03:22:10 | INFO | fairseq.trainer | begin training epoch 220
2022-03-05 03:22:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:22:16 | INFO | train_inner | epoch 220:      2 / 97 loss=1.324, nll_loss=0.733, ppl=1.66, wps=23656.9, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=21100, lr=0.0002177, gnorm=0.911, loss_scale=16, train_wall=238, gb_free=21, wall=56864
2022-03-05 03:26:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:26:26 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 13.792 | nll_loss 13.507 | ppl 11640.3 | wps 43410.9 | wpb 510.9 | bsz 1 | num_updates 21195 | best_loss 7.701
2022-03-05 03:26:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 21195 updates
2022-03-05 03:26:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:26:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:26:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 220 @ 21195 updates, score 13.792) (writing took 2.497334321960807 seconds)
2022-03-05 03:26:29 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-03-05 03:26:29 | INFO | train | epoch 220 | loss 1.322 | nll_loss 0.732 | ppl 1.66 | wps 24575.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21195 | lr 0.000217212 | gnorm 0.908 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 57117
2022-03-05 03:26:29 | INFO | fairseq.trainer | begin training epoch 221
2022-03-05 03:26:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:26:42 | INFO | train_inner | epoch 221:      5 / 97 loss=1.321, nll_loss=0.731, ppl=1.66, wps=24598.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21200, lr=0.000217186, gnorm=0.905, loss_scale=16, train_wall=236, gb_free=21, wall=57130
2022-03-05 03:28:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:30:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:30:45 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 13.75 | nll_loss 13.463 | ppl 11290.9 | wps 43636.8 | wpb 510.9 | bsz 1 | num_updates 21291 | best_loss 7.701
2022-03-05 03:30:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 21291 updates
2022-03-05 03:30:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:30:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:30:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 221 @ 21291 updates, score 13.75) (writing took 2.490360983647406 seconds)
2022-03-05 03:30:47 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-03-05 03:30:47 | INFO | train | epoch 221 | loss 1.318 | nll_loss 0.728 | ppl 1.66 | wps 24320.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21291 | lr 0.000216721 | gnorm 0.893 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 57375
2022-03-05 03:30:47 | INFO | fairseq.trainer | begin training epoch 222
2022-03-05 03:30:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:31:11 | INFO | train_inner | epoch 222:      9 / 97 loss=1.317, nll_loss=0.727, ppl=1.66, wps=24360, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21300, lr=0.000216676, gnorm=0.894, loss_scale=16, train_wall=238, gb_free=21, wall=57399
2022-03-05 03:34:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:34:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:35:03 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 13.755 | nll_loss 13.468 | ppl 11334.5 | wps 43635.9 | wpb 510.9 | bsz 1 | num_updates 21387 | best_loss 7.701
2022-03-05 03:35:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 21387 updates
2022-03-05 03:35:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:35:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:35:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 222 @ 21387 updates, score 13.755) (writing took 2.535234591923654 seconds)
2022-03-05 03:35:06 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-03-05 03:35:06 | INFO | train | epoch 222 | loss 1.315 | nll_loss 0.725 | ppl 1.65 | wps 24332 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21387 | lr 0.000216235 | gnorm 0.897 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 57634
2022-03-05 03:35:06 | INFO | fairseq.trainer | begin training epoch 223
2022-03-05 03:35:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:35:40 | INFO | train_inner | epoch 223:     13 / 97 loss=1.314, nll_loss=0.724, ppl=1.65, wps=24366.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21400, lr=0.000216169, gnorm=0.894, loss_scale=16, train_wall=238, gb_free=21, wall=57668
2022-03-05 03:39:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:39:22 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 13.747 | nll_loss 13.459 | ppl 11263 | wps 43594.3 | wpb 510.9 | bsz 1 | num_updates 21484 | best_loss 7.701
2022-03-05 03:39:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 21484 updates
2022-03-05 03:39:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:39:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:39:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 223 @ 21484 updates, score 13.747) (writing took 2.46113402210176 seconds)
2022-03-05 03:39:24 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-03-05 03:39:24 | INFO | train | epoch 223 | loss 1.313 | nll_loss 0.723 | ppl 1.65 | wps 24574.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21484 | lr 0.000215746 | gnorm 0.89 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 57892
2022-03-05 03:39:24 | INFO | fairseq.trainer | begin training epoch 224
2022-03-05 03:39:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:40:06 | INFO | train_inner | epoch 224:     16 / 97 loss=1.31, nll_loss=0.72, ppl=1.65, wps=24601, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21500, lr=0.000215666, gnorm=0.887, loss_scale=32, train_wall=236, gb_free=21, wall=57934
2022-03-05 03:41:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:43:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:43:40 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 13.782 | nll_loss 13.497 | ppl 11558.5 | wps 43652 | wpb 510.9 | bsz 1 | num_updates 21580 | best_loss 7.701
2022-03-05 03:43:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 21580 updates
2022-03-05 03:43:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:43:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:43:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 224 @ 21580 updates, score 13.782) (writing took 2.534555540420115 seconds)
2022-03-05 03:43:43 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-03-05 03:43:43 | INFO | train | epoch 224 | loss 1.31 | nll_loss 0.72 | ppl 1.65 | wps 24321.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21580 | lr 0.000215265 | gnorm 0.893 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 58151
2022-03-05 03:43:43 | INFO | fairseq.trainer | begin training epoch 225
2022-03-05 03:43:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:44:35 | INFO | train_inner | epoch 225:     20 / 97 loss=1.309, nll_loss=0.719, ppl=1.65, wps=24365, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21600, lr=0.000215166, gnorm=0.894, loss_scale=16, train_wall=238, gb_free=21, wall=58203
2022-03-05 03:47:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:47:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:47:59 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 13.793 | nll_loss 13.509 | ppl 11657.3 | wps 43641.3 | wpb 510.9 | bsz 1 | num_updates 21676 | best_loss 7.701
2022-03-05 03:47:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 21676 updates
2022-03-05 03:47:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:48:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:48:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 225 @ 21676 updates, score 13.793) (writing took 2.4983281893655658 seconds)
2022-03-05 03:48:01 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-03-05 03:48:01 | INFO | train | epoch 225 | loss 1.308 | nll_loss 0.718 | ppl 1.64 | wps 24328 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21676 | lr 0.000214788 | gnorm 0.896 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 58409
2022-03-05 03:48:01 | INFO | fairseq.trainer | begin training epoch 226
2022-03-05 03:48:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:49:04 | INFO | train_inner | epoch 226:     24 / 97 loss=1.308, nll_loss=0.717, ppl=1.64, wps=24360.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21700, lr=0.000214669, gnorm=0.896, loss_scale=16, train_wall=238, gb_free=21, wall=58471
2022-03-05 03:52:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:52:17 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 13.789 | nll_loss 13.504 | ppl 11617.3 | wps 43815.5 | wpb 510.9 | bsz 1 | num_updates 21773 | best_loss 7.701
2022-03-05 03:52:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 21773 updates
2022-03-05 03:52:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:52:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:52:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 226 @ 21773 updates, score 13.789) (writing took 2.4889456992968917 seconds)
2022-03-05 03:52:20 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-03-05 03:52:20 | INFO | train | epoch 226 | loss 1.305 | nll_loss 0.715 | ppl 1.64 | wps 24569.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21773 | lr 0.000214309 | gnorm 0.898 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 58668
2022-03-05 03:52:20 | INFO | fairseq.trainer | begin training epoch 227
2022-03-05 03:52:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:53:30 | INFO | train_inner | epoch 227:     27 / 97 loss=1.305, nll_loss=0.715, ppl=1.64, wps=24586.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21800, lr=0.000214176, gnorm=0.9, loss_scale=16, train_wall=236, gb_free=21, wall=58738
2022-03-05 03:54:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:56:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:56:36 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 13.774 | nll_loss 13.489 | ppl 11498.2 | wps 43626.3 | wpb 510.9 | bsz 1 | num_updates 21869 | best_loss 7.701
2022-03-05 03:56:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 21869 updates
2022-03-05 03:56:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:56:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 03:56:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 227 @ 21869 updates, score 13.774) (writing took 2.4973643459379673 seconds)
2022-03-05 03:56:39 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-03-05 03:56:39 | INFO | train | epoch 227 | loss 1.303 | nll_loss 0.713 | ppl 1.64 | wps 24309.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21869 | lr 0.000213838 | gnorm 0.9 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 58926
2022-03-05 03:56:39 | INFO | fairseq.trainer | begin training epoch 228
2022-03-05 03:56:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:57:59 | INFO | train_inner | epoch 228:     31 / 97 loss=1.302, nll_loss=0.712, ppl=1.64, wps=24355.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21900, lr=0.000213687, gnorm=0.9, loss_scale=16, train_wall=238, gb_free=21, wall=59007
2022-03-05 04:00:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:00:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:00:55 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 13.833 | nll_loss 13.548 | ppl 11976.8 | wps 43707.7 | wpb 510.9 | bsz 1 | num_updates 21965 | best_loss 7.701
2022-03-05 04:00:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 21965 updates
2022-03-05 04:00:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:00:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:00:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 228 @ 21965 updates, score 13.833) (writing took 2.4651553379371762 seconds)
2022-03-05 04:00:57 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-03-05 04:00:57 | INFO | train | epoch 228 | loss 1.3 | nll_loss 0.71 | ppl 1.64 | wps 24320.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21965 | lr 0.000213371 | gnorm 0.899 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 59185
2022-03-05 04:00:57 | INFO | fairseq.trainer | begin training epoch 229
2022-03-05 04:00:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:02:28 | INFO | train_inner | epoch 229:     35 / 97 loss=1.298, nll_loss=0.708, ppl=1.63, wps=24362, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=22000, lr=0.000213201, gnorm=0.893, loss_scale=16, train_wall=238, gb_free=21, wall=59275
2022-03-05 04:05:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:05:13 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 13.805 | nll_loss 13.524 | ppl 11782.1 | wps 43580.4 | wpb 510.9 | bsz 1 | num_updates 22062 | best_loss 7.701
2022-03-05 04:05:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 22062 updates
2022-03-05 04:05:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:05:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:05:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 229 @ 22062 updates, score 13.805) (writing took 2.4823470627889037 seconds)
2022-03-05 04:05:16 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-03-05 04:05:16 | INFO | train | epoch 229 | loss 1.298 | nll_loss 0.708 | ppl 1.63 | wps 24573.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22062 | lr 0.000212901 | gnorm 0.885 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 59443
2022-03-05 04:05:16 | INFO | fairseq.trainer | begin training epoch 230
2022-03-05 04:05:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:06:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:06:57 | INFO | train_inner | epoch 230:     39 / 97 loss=1.296, nll_loss=0.706, ppl=1.63, wps=24346.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=22100, lr=0.000212718, gnorm=0.888, loss_scale=16, train_wall=238, gb_free=21, wall=59544
2022-03-05 04:09:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:09:32 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 13.775 | nll_loss 13.493 | ppl 11525.7 | wps 43747.4 | wpb 510.9 | bsz 1 | num_updates 22158 | best_loss 7.701
2022-03-05 04:09:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 22158 updates
2022-03-05 04:09:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:09:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:09:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 230 @ 22158 updates, score 13.775) (writing took 2.5031496128067374 seconds)
2022-03-05 04:09:34 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-03-05 04:09:34 | INFO | train | epoch 230 | loss 1.296 | nll_loss 0.706 | ppl 1.63 | wps 24314.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22158 | lr 0.000212439 | gnorm 0.903 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 59702
2022-03-05 04:09:34 | INFO | fairseq.trainer | begin training epoch 231
2022-03-05 04:09:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:11:23 | INFO | train_inner | epoch 231:     42 / 97 loss=1.295, nll_loss=0.705, ppl=1.63, wps=24596.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=22200, lr=0.000212238, gnorm=0.9, loss_scale=16, train_wall=236, gb_free=21, wall=59811
2022-03-05 04:12:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:13:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:13:50 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 13.842 | nll_loss 13.558 | ppl 12063 | wps 43656.1 | wpb 510.9 | bsz 1 | num_updates 22254 | best_loss 7.701
2022-03-05 04:13:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 22254 updates
2022-03-05 04:13:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:13:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:13:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 231 @ 22254 updates, score 13.842) (writing took 2.543719133362174 seconds)
2022-03-05 04:13:53 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-03-05 04:13:53 | INFO | train | epoch 231 | loss 1.293 | nll_loss 0.703 | ppl 1.63 | wps 24312.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22254 | lr 0.000211981 | gnorm 0.892 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 59961
2022-03-05 04:13:53 | INFO | fairseq.trainer | begin training epoch 232
2022-03-05 04:13:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:15:52 | INFO | train_inner | epoch 232:     46 / 97 loss=1.294, nll_loss=0.704, ppl=1.63, wps=24348.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=22300, lr=0.000211762, gnorm=0.893, loss_scale=16, train_wall=238, gb_free=21, wall=60080
2022-03-05 04:18:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:18:09 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 13.838 | nll_loss 13.554 | ppl 12027.1 | wps 43771.5 | wpb 510.9 | bsz 1 | num_updates 22351 | best_loss 7.701
2022-03-05 04:18:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 22351 updates
2022-03-05 04:18:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:18:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:18:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 232 @ 22351 updates, score 13.838) (writing took 2.526653061620891 seconds)
2022-03-05 04:18:11 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-03-05 04:18:11 | INFO | train | epoch 232 | loss 1.292 | nll_loss 0.702 | ppl 1.63 | wps 24562.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22351 | lr 0.00021152 | gnorm 0.889 | loss_scale 32 | train_wall 229 | gb_free 21 | wall 60219
2022-03-05 04:18:11 | INFO | fairseq.trainer | begin training epoch 233
2022-03-05 04:18:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:19:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:20:21 | INFO | train_inner | epoch 233:     50 / 97 loss=1.291, nll_loss=0.701, ppl=1.63, wps=24352.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=22400, lr=0.000211289, gnorm=0.885, loss_scale=16, train_wall=238, gb_free=21, wall=60349
2022-03-05 04:22:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:22:27 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 13.808 | nll_loss 13.525 | ppl 11785 | wps 43827.3 | wpb 510.9 | bsz 1 | num_updates 22447 | best_loss 7.701
2022-03-05 04:22:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 22447 updates
2022-03-05 04:22:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:22:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:22:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 233 @ 22447 updates, score 13.808) (writing took 2.4855682589113712 seconds)
2022-03-05 04:22:30 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-03-05 04:22:30 | INFO | train | epoch 233 | loss 1.289 | nll_loss 0.699 | ppl 1.62 | wps 24320.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22447 | lr 0.000211067 | gnorm 0.883 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 60478
2022-03-05 04:22:30 | INFO | fairseq.trainer | begin training epoch 234
2022-03-05 04:22:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:24:47 | INFO | train_inner | epoch 234:     53 / 97 loss=1.286, nll_loss=0.696, ppl=1.62, wps=24592.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=22500, lr=0.000210819, gnorm=0.893, loss_scale=16, train_wall=236, gb_free=21, wall=60615
2022-03-05 04:24:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:26:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:26:46 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 13.732 | nll_loss 13.447 | ppl 11167.6 | wps 43733.6 | wpb 510.9 | bsz 1 | num_updates 22543 | best_loss 7.701
2022-03-05 04:26:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 22543 updates
2022-03-05 04:26:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:26:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:26:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 234 @ 22543 updates, score 13.732) (writing took 2.5100181195884943 seconds)
2022-03-05 04:26:48 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-03-05 04:26:48 | INFO | train | epoch 234 | loss 1.287 | nll_loss 0.697 | ppl 1.62 | wps 24332.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22543 | lr 0.000210617 | gnorm 0.898 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 60736
2022-03-05 04:26:48 | INFO | fairseq.trainer | begin training epoch 235
2022-03-05 04:26:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:29:16 | INFO | train_inner | epoch 235:     57 / 97 loss=1.287, nll_loss=0.697, ppl=1.62, wps=24374.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=22600, lr=0.000210352, gnorm=0.898, loss_scale=16, train_wall=238, gb_free=21, wall=60884
2022-03-05 04:30:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:30:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:31:04 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 13.775 | nll_loss 13.491 | ppl 11516.2 | wps 43650.4 | wpb 510.9 | bsz 1 | num_updates 22639 | best_loss 7.701
2022-03-05 04:31:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 22639 updates
2022-03-05 04:31:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:31:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:31:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 235 @ 22639 updates, score 13.775) (writing took 2.511878442019224 seconds)
2022-03-05 04:31:07 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-03-05 04:31:07 | INFO | train | epoch 235 | loss 1.285 | nll_loss 0.695 | ppl 1.62 | wps 24324.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22639 | lr 0.00021017 | gnorm 0.9 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 60995
2022-03-05 04:31:07 | INFO | fairseq.trainer | begin training epoch 236
2022-03-05 04:31:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:33:45 | INFO | train_inner | epoch 236:     61 / 97 loss=1.283, nll_loss=0.693, ppl=1.62, wps=24361.3, ups=0.37, wpb=65495, bsz=127.9, num_updates=22700, lr=0.000209888, gnorm=0.885, loss_scale=16, train_wall=238, gb_free=21, wall=61153
2022-03-05 04:35:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:35:23 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 13.833 | nll_loss 13.552 | ppl 12009 | wps 43646.5 | wpb 510.9 | bsz 1 | num_updates 22736 | best_loss 7.701
2022-03-05 04:35:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 22736 updates
2022-03-05 04:35:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:35:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:35:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 236 @ 22736 updates, score 13.833) (writing took 2.526942097581923 seconds)
2022-03-05 04:35:25 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-03-05 04:35:25 | INFO | train | epoch 236 | loss 1.282 | nll_loss 0.692 | ppl 1.62 | wps 24572.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22736 | lr 0.000209722 | gnorm 0.884 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 61253
2022-03-05 04:35:25 | INFO | fairseq.trainer | begin training epoch 237
2022-03-05 04:35:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:38:11 | INFO | train_inner | epoch 237:     64 / 97 loss=1.281, nll_loss=0.691, ppl=1.61, wps=24596.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=22800, lr=0.000209427, gnorm=0.886, loss_scale=32, train_wall=236, gb_free=21, wall=61419
2022-03-05 04:39:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:39:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:39:41 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 13.801 | nll_loss 13.519 | ppl 11738.3 | wps 43709.7 | wpb 510.9 | bsz 1 | num_updates 22832 | best_loss 7.701
2022-03-05 04:39:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 22832 updates
2022-03-05 04:39:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:39:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:39:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 237 @ 22832 updates, score 13.801) (writing took 2.5101422807201743 seconds)
2022-03-05 04:39:44 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-03-05 04:39:44 | INFO | train | epoch 237 | loss 1.28 | nll_loss 0.69 | ppl 1.61 | wps 24329.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22832 | lr 0.00020928 | gnorm 0.878 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 61512
2022-03-05 04:39:44 | INFO | fairseq.trainer | begin training epoch 238
2022-03-05 04:39:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:42:40 | INFO | train_inner | epoch 238:     68 / 97 loss=1.283, nll_loss=0.693, ppl=1.62, wps=24370.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=22900, lr=0.000208969, gnorm=0.874, loss_scale=16, train_wall=238, gb_free=21, wall=61688
2022-03-05 04:43:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:44:00 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 13.828 | nll_loss 13.546 | ppl 11958.7 | wps 43666.8 | wpb 510.9 | bsz 1 | num_updates 22929 | best_loss 7.701
2022-03-05 04:44:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 22929 updates
2022-03-05 04:44:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:44:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:44:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 238 @ 22929 updates, score 13.828) (writing took 2.4881756799295545 seconds)
2022-03-05 04:44:02 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-03-05 04:44:02 | INFO | train | epoch 238 | loss 1.279 | nll_loss 0.689 | ppl 1.61 | wps 24585.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22929 | lr 0.000208837 | gnorm 0.878 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 61770
2022-03-05 04:44:02 | INFO | fairseq.trainer | begin training epoch 239
2022-03-05 04:44:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:45:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:47:08 | INFO | train_inner | epoch 239:     72 / 97 loss=1.274, nll_loss=0.684, ppl=1.61, wps=24362.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23000, lr=0.000208514, gnorm=0.886, loss_scale=16, train_wall=238, gb_free=21, wall=61956
2022-03-05 04:48:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:48:18 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 13.795 | nll_loss 13.509 | ppl 11655.4 | wps 43661.4 | wpb 510.9 | bsz 1 | num_updates 23025 | best_loss 7.701
2022-03-05 04:48:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 23025 updates
2022-03-05 04:48:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:48:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:48:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 239 @ 23025 updates, score 13.795) (writing took 2.499412934295833 seconds)
2022-03-05 04:48:21 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-03-05 04:48:21 | INFO | train | epoch 239 | loss 1.275 | nll_loss 0.685 | ppl 1.61 | wps 24314 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23025 | lr 0.000208401 | gnorm 0.884 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 62029
2022-03-05 04:48:21 | INFO | fairseq.trainer | begin training epoch 240
2022-03-05 04:48:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:50:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:51:37 | INFO | train_inner | epoch 240:     76 / 97 loss=1.273, nll_loss=0.683, ppl=1.61, wps=24363.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23100, lr=0.000208063, gnorm=0.881, loss_scale=16, train_wall=238, gb_free=21, wall=62225
2022-03-05 04:52:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:52:37 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 13.845 | nll_loss 13.563 | ppl 12101.6 | wps 43842.8 | wpb 510.9 | bsz 1 | num_updates 23121 | best_loss 7.701
2022-03-05 04:52:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 23121 updates
2022-03-05 04:52:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:52:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:52:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 240 @ 23121 updates, score 13.845) (writing took 2.4773876573890448 seconds)
2022-03-05 04:52:39 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-03-05 04:52:39 | INFO | train | epoch 240 | loss 1.272 | nll_loss 0.682 | ppl 1.6 | wps 24333.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23121 | lr 0.000207968 | gnorm 0.877 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 62287
2022-03-05 04:52:39 | INFO | fairseq.trainer | begin training epoch 241
2022-03-05 04:52:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:56:03 | INFO | train_inner | epoch 241:     79 / 97 loss=1.272, nll_loss=0.682, ppl=1.6, wps=24604.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23200, lr=0.000207614, gnorm=0.874, loss_scale=16, train_wall=236, gb_free=21, wall=62491
2022-03-05 04:56:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:56:55 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 13.865 | nll_loss 13.584 | ppl 12281.1 | wps 43687 | wpb 510.9 | bsz 1 | num_updates 23218 | best_loss 7.701
2022-03-05 04:56:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 23218 updates
2022-03-05 04:56:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:56:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 04:56:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 241 @ 23218 updates, score 13.865) (writing took 2.4586464120075107 seconds)
2022-03-05 04:56:57 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-03-05 04:56:57 | INFO | train | epoch 241 | loss 1.271 | nll_loss 0.681 | ppl 1.6 | wps 24582.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23218 | lr 0.000207533 | gnorm 0.874 | loss_scale 32 | train_wall 229 | gb_free 21 | wall 62545
2022-03-05 04:56:57 | INFO | fairseq.trainer | begin training epoch 242
2022-03-05 04:56:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:57:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:00:32 | INFO | train_inner | epoch 242:     83 / 97 loss=1.27, nll_loss=0.68, ppl=1.6, wps=24369.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23300, lr=0.000207168, gnorm=0.874, loss_scale=16, train_wall=238, gb_free=21, wall=62760
2022-03-05 05:01:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:01:13 | INFO | valid | epoch 242 | valid on 'valid' subset | loss 13.823 | nll_loss 13.542 | ppl 11928.5 | wps 43892.6 | wpb 510.9 | bsz 1 | num_updates 23314 | best_loss 7.701
2022-03-05 05:01:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 242 @ 23314 updates
2022-03-05 05:01:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:01:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:01:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 242 @ 23314 updates, score 13.823) (writing took 2.4625944001600146 seconds)
2022-03-05 05:01:16 | INFO | fairseq_cli.train | end of epoch 242 (average epoch stats below)
2022-03-05 05:01:16 | INFO | train | epoch 242 | loss 1.269 | nll_loss 0.679 | ppl 1.6 | wps 24330.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23314 | lr 0.000207105 | gnorm 0.878 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 62804
2022-03-05 05:01:16 | INFO | fairseq.trainer | begin training epoch 243
2022-03-05 05:01:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:04:58 | INFO | train_inner | epoch 243:     86 / 97 loss=1.269, nll_loss=0.679, ppl=1.6, wps=24604, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23400, lr=0.000206725, gnorm=0.88, loss_scale=32, train_wall=236, gb_free=21, wall=63026
2022-03-05 05:05:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:05:32 | INFO | valid | epoch 243 | valid on 'valid' subset | loss 13.883 | nll_loss 13.602 | ppl 12436.1 | wps 43644.2 | wpb 510.9 | bsz 1 | num_updates 23411 | best_loss 7.701
2022-03-05 05:05:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 243 @ 23411 updates
2022-03-05 05:05:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:05:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:05:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 243 @ 23411 updates, score 13.883) (writing took 2.4405833715572953 seconds)
2022-03-05 05:05:34 | INFO | fairseq_cli.train | end of epoch 243 (average epoch stats below)
2022-03-05 05:05:34 | INFO | train | epoch 243 | loss 1.268 | nll_loss 0.678 | ppl 1.6 | wps 24576.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23411 | lr 0.000206676 | gnorm 0.876 | loss_scale 32 | train_wall 229 | gb_free 21 | wall 63062
2022-03-05 05:05:34 | INFO | fairseq.trainer | begin training epoch 244
2022-03-05 05:05:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:06:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:08:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:09:30 | INFO | train_inner | epoch 244:     91 / 97 loss=1.266, nll_loss=0.676, ppl=1.6, wps=24132.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23500, lr=0.000206284, gnorm=0.87, loss_scale=8, train_wall=240, gb_free=21, wall=63298
2022-03-05 05:09:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:09:50 | INFO | valid | epoch 244 | valid on 'valid' subset | loss 13.914 | nll_loss 13.636 | ppl 12730.3 | wps 43740.8 | wpb 510.9 | bsz 1 | num_updates 23506 | best_loss 7.701
2022-03-05 05:09:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 244 @ 23506 updates
2022-03-05 05:09:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:09:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:09:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 244 @ 23506 updates, score 13.914) (writing took 2.4608367839828134 seconds)
2022-03-05 05:09:53 | INFO | fairseq_cli.train | end of epoch 244 (average epoch stats below)
2022-03-05 05:09:53 | INFO | train | epoch 244 | loss 1.264 | nll_loss 0.674 | ppl 1.6 | wps 24079.3 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 23506 | lr 0.000206258 | gnorm 0.867 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 63321
2022-03-05 05:09:53 | INFO | fairseq.trainer | begin training epoch 245
2022-03-05 05:09:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:13:56 | INFO | train_inner | epoch 245:     94 / 97 loss=1.263, nll_loss=0.674, ppl=1.6, wps=24608.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23600, lr=0.000205847, gnorm=0.876, loss_scale=16, train_wall=236, gb_free=21, wall=63564
2022-03-05 05:14:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:14:09 | INFO | valid | epoch 245 | valid on 'valid' subset | loss 13.882 | nll_loss 13.601 | ppl 12426 | wps 43810.4 | wpb 510.9 | bsz 1 | num_updates 23603 | best_loss 7.701
2022-03-05 05:14:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 245 @ 23603 updates
2022-03-05 05:14:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:14:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:14:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 245 @ 23603 updates, score 13.882) (writing took 2.4445416228845716 seconds)
2022-03-05 05:14:11 | INFO | fairseq_cli.train | end of epoch 245 (average epoch stats below)
2022-03-05 05:14:11 | INFO | train | epoch 245 | loss 1.263 | nll_loss 0.673 | ppl 1.59 | wps 24587.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23603 | lr 0.000205834 | gnorm 0.878 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 63579
2022-03-05 05:14:11 | INFO | fairseq.trainer | begin training epoch 246
2022-03-05 05:14:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:18:22 | INFO | train_inner | epoch 246:     97 / 97 loss=1.263, nll_loss=0.673, ppl=1.59, wps=24593.7, ups=0.38, wpb=65451.9, bsz=127.8, num_updates=23700, lr=0.000205412, gnorm=0.872, loss_scale=16, train_wall=236, gb_free=21, wall=63830
2022-03-05 05:18:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:18:27 | INFO | valid | epoch 246 | valid on 'valid' subset | loss 13.844 | nll_loss 13.565 | ppl 12116.9 | wps 43813.9 | wpb 510.9 | bsz 1 | num_updates 23700 | best_loss 7.701
2022-03-05 05:18:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 246 @ 23700 updates
2022-03-05 05:18:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:18:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:18:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 246 @ 23700 updates, score 13.844) (writing took 2.476143897511065 seconds)
2022-03-05 05:18:30 | INFO | fairseq_cli.train | end of epoch 246 (average epoch stats below)
2022-03-05 05:18:30 | INFO | train | epoch 246 | loss 1.262 | nll_loss 0.672 | ppl 1.59 | wps 24570.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23700 | lr 0.000205412 | gnorm 0.871 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 63837
2022-03-05 05:18:30 | INFO | fairseq.trainer | begin training epoch 247
2022-03-05 05:18:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:19:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:22:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:22:46 | INFO | valid | epoch 247 | valid on 'valid' subset | loss 13.889 | nll_loss 13.612 | ppl 12523 | wps 43619.9 | wpb 510.9 | bsz 1 | num_updates 23796 | best_loss 7.701
2022-03-05 05:22:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 247 @ 23796 updates
2022-03-05 05:22:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:22:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:22:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 247 @ 23796 updates, score 13.889) (writing took 2.4545725723728538 seconds)
2022-03-05 05:22:48 | INFO | fairseq_cli.train | end of epoch 247 (average epoch stats below)
2022-03-05 05:22:48 | INFO | train | epoch 247 | loss 1.258 | nll_loss 0.668 | ppl 1.59 | wps 24323.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23796 | lr 0.000204997 | gnorm 0.867 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 64096
2022-03-05 05:22:48 | INFO | fairseq.trainer | begin training epoch 248
2022-03-05 05:22:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:22:59 | INFO | train_inner | epoch 248:      4 / 97 loss=1.257, nll_loss=0.667, ppl=1.59, wps=23691.7, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=23800, lr=0.00020498, gnorm=0.867, loss_scale=16, train_wall=238, gb_free=21, wall=64106
2022-03-05 05:25:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:26:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:27:04 | INFO | valid | epoch 248 | valid on 'valid' subset | loss 13.856 | nll_loss 13.575 | ppl 12201.6 | wps 43642.3 | wpb 510.9 | bsz 1 | num_updates 23892 | best_loss 7.701
2022-03-05 05:27:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 248 @ 23892 updates
2022-03-05 05:27:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:27:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:27:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 248 @ 23892 updates, score 13.856) (writing took 2.4668481117114425 seconds)
2022-03-05 05:27:07 | INFO | fairseq_cli.train | end of epoch 248 (average epoch stats below)
2022-03-05 05:27:07 | INFO | train | epoch 248 | loss 1.258 | nll_loss 0.668 | ppl 1.59 | wps 24319.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23892 | lr 0.000204585 | gnorm 0.873 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 64354
2022-03-05 05:27:07 | INFO | fairseq.trainer | begin training epoch 249
2022-03-05 05:27:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:27:27 | INFO | train_inner | epoch 249:      8 / 97 loss=1.256, nll_loss=0.667, ppl=1.59, wps=24357.7, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=23900, lr=0.000204551, gnorm=0.871, loss_scale=16, train_wall=238, gb_free=21, wall=64375
2022-03-05 05:31:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:31:23 | INFO | valid | epoch 249 | valid on 'valid' subset | loss 13.796 | nll_loss 13.516 | ppl 11710.6 | wps 43891.1 | wpb 510.9 | bsz 1 | num_updates 23989 | best_loss 7.701
2022-03-05 05:31:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 249 @ 23989 updates
2022-03-05 05:31:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:31:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:31:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 249 @ 23989 updates, score 13.796) (writing took 2.4580616327002645 seconds)
2022-03-05 05:31:25 | INFO | fairseq_cli.train | end of epoch 249 (average epoch stats below)
2022-03-05 05:31:25 | INFO | train | epoch 249 | loss 1.255 | nll_loss 0.666 | ppl 1.59 | wps 24583 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23989 | lr 0.000204171 | gnorm 0.871 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 64613
2022-03-05 05:31:25 | INFO | fairseq.trainer | begin training epoch 250
2022-03-05 05:31:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:31:54 | INFO | train_inner | epoch 250:     11 / 97 loss=1.254, nll_loss=0.664, ppl=1.58, wps=24605.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=24000, lr=0.000204124, gnorm=0.871, loss_scale=32, train_wall=236, gb_free=21, wall=64641
2022-03-05 05:32:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:35:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:35:41 | INFO | valid | epoch 250 | valid on 'valid' subset | loss 13.835 | nll_loss 13.556 | ppl 12045 | wps 43584 | wpb 510.9 | bsz 1 | num_updates 24085 | best_loss 7.701
2022-03-05 05:35:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 250 @ 24085 updates
2022-03-05 05:35:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:35:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:35:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 250 @ 24085 updates, score 13.835) (writing took 2.4470404479652643 seconds)
2022-03-05 05:35:43 | INFO | fairseq_cli.train | end of epoch 250 (average epoch stats below)
2022-03-05 05:35:43 | INFO | train | epoch 250 | loss 1.253 | nll_loss 0.663 | ppl 1.58 | wps 24330.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24085 | lr 0.000203764 | gnorm 0.878 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 64871
2022-03-05 05:35:43 | INFO | fairseq.trainer | begin training epoch 251
2022-03-05 05:35:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:36:22 | INFO | train_inner | epoch 251:     15 / 97 loss=1.253, nll_loss=0.663, ppl=1.58, wps=24364.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=24100, lr=0.0002037, gnorm=0.876, loss_scale=16, train_wall=238, gb_free=21, wall=64910
2022-03-05 05:38:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:39:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:39:59 | INFO | valid | epoch 251 | valid on 'valid' subset | loss 13.831 | nll_loss 13.552 | ppl 12008 | wps 43662.1 | wpb 510.9 | bsz 1 | num_updates 24181 | best_loss 7.701
2022-03-05 05:39:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 251 @ 24181 updates
2022-03-05 05:39:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:40:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:40:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 251 @ 24181 updates, score 13.831) (writing took 2.455657545477152 seconds)
2022-03-05 05:40:02 | INFO | fairseq_cli.train | end of epoch 251 (average epoch stats below)
2022-03-05 05:40:02 | INFO | train | epoch 251 | loss 1.249 | nll_loss 0.659 | ppl 1.58 | wps 24321.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24181 | lr 0.000203359 | gnorm 0.872 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 65130
2022-03-05 05:40:02 | INFO | fairseq.trainer | begin training epoch 252
2022-03-05 05:40:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:40:51 | INFO | train_inner | epoch 252:     19 / 97 loss=1.248, nll_loss=0.658, ppl=1.58, wps=24364.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=24200, lr=0.000203279, gnorm=0.872, loss_scale=16, train_wall=238, gb_free=21, wall=65179
2022-03-05 05:44:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:44:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:44:18 | INFO | valid | epoch 252 | valid on 'valid' subset | loss 13.842 | nll_loss 13.565 | ppl 12117.7 | wps 43668.7 | wpb 510.9 | bsz 1 | num_updates 24277 | best_loss 7.701
2022-03-05 05:44:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 252 @ 24277 updates
2022-03-05 05:44:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:44:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:44:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 252 @ 24277 updates, score 13.842) (writing took 2.4966510105878115 seconds)
2022-03-05 05:44:20 | INFO | fairseq_cli.train | end of epoch 252 (average epoch stats below)
2022-03-05 05:44:20 | INFO | train | epoch 252 | loss 1.248 | nll_loss 0.659 | ppl 1.58 | wps 24328.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24277 | lr 0.000202956 | gnorm 0.863 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 65388
2022-03-05 05:44:20 | INFO | fairseq.trainer | begin training epoch 253
2022-03-05 05:44:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:45:20 | INFO | train_inner | epoch 253:     23 / 97 loss=1.247, nll_loss=0.657, ppl=1.58, wps=24365.6, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=24300, lr=0.00020286, gnorm=0.861, loss_scale=16, train_wall=238, gb_free=21, wall=65448
2022-03-05 05:48:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:48:36 | INFO | valid | epoch 253 | valid on 'valid' subset | loss 13.883 | nll_loss 13.605 | ppl 12463.3 | wps 43849.7 | wpb 510.9 | bsz 1 | num_updates 24374 | best_loss 7.701
2022-03-05 05:48:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 253 @ 24374 updates
2022-03-05 05:48:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:48:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:48:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 253 @ 24374 updates, score 13.883) (writing took 2.697963516227901 seconds)
2022-03-05 05:48:39 | INFO | fairseq_cli.train | end of epoch 253 (average epoch stats below)
2022-03-05 05:48:39 | INFO | train | epoch 253 | loss 1.247 | nll_loss 0.658 | ppl 1.58 | wps 24559.1 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 24374 | lr 0.000202552 | gnorm 0.87 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 65647
2022-03-05 05:48:39 | INFO | fairseq.trainer | begin training epoch 254
2022-03-05 05:48:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:49:46 | INFO | train_inner | epoch 254:     26 / 97 loss=1.246, nll_loss=0.657, ppl=1.58, wps=24575.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=24400, lr=0.000202444, gnorm=0.869, loss_scale=16, train_wall=236, gb_free=21, wall=65714
2022-03-05 05:50:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:52:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:52:55 | INFO | valid | epoch 254 | valid on 'valid' subset | loss 13.823 | nll_loss 13.545 | ppl 11951.2 | wps 43735 | wpb 510.9 | bsz 1 | num_updates 24470 | best_loss 7.701
2022-03-05 05:52:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 254 @ 24470 updates
2022-03-05 05:52:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:52:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:52:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 254 @ 24470 updates, score 13.823) (writing took 2.4769129641354084 seconds)
2022-03-05 05:52:57 | INFO | fairseq_cli.train | end of epoch 254 (average epoch stats below)
2022-03-05 05:52:57 | INFO | train | epoch 254 | loss 1.245 | nll_loss 0.656 | ppl 1.58 | wps 24331.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24470 | lr 0.000202154 | gnorm 0.863 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 65905
2022-03-05 05:52:57 | INFO | fairseq.trainer | begin training epoch 255
2022-03-05 05:52:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:54:15 | INFO | train_inner | epoch 255:     30 / 97 loss=1.245, nll_loss=0.655, ppl=1.58, wps=24366.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=24500, lr=0.000202031, gnorm=0.868, loss_scale=16, train_wall=238, gb_free=21, wall=65983
2022-03-05 05:56:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:57:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:57:14 | INFO | valid | epoch 255 | valid on 'valid' subset | loss 13.825 | nll_loss 13.543 | ppl 11932.1 | wps 43619.2 | wpb 510.9 | bsz 1 | num_updates 24566 | best_loss 7.701
2022-03-05 05:57:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 255 @ 24566 updates
2022-03-05 05:57:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:57:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 05:57:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 255 @ 24566 updates, score 13.825) (writing took 2.459293344989419 seconds)
2022-03-05 05:57:16 | INFO | fairseq_cli.train | end of epoch 255 (average epoch stats below)
2022-03-05 05:57:16 | INFO | train | epoch 255 | loss 1.243 | nll_loss 0.654 | ppl 1.57 | wps 24314.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24566 | lr 0.000201759 | gnorm 0.869 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 66164
2022-03-05 05:57:16 | INFO | fairseq.trainer | begin training epoch 256
2022-03-05 05:57:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:58:44 | INFO | train_inner | epoch 256:     34 / 97 loss=1.243, nll_loss=0.654, ppl=1.57, wps=24359, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=24600, lr=0.000201619, gnorm=0.868, loss_scale=16, train_wall=238, gb_free=21, wall=66252
2022-03-05 06:01:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:01:32 | INFO | valid | epoch 256 | valid on 'valid' subset | loss 13.904 | nll_loss 13.626 | ppl 12641.1 | wps 43761.2 | wpb 510.9 | bsz 1 | num_updates 24663 | best_loss 7.701
2022-03-05 06:01:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 256 @ 24663 updates
2022-03-05 06:01:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:01:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:01:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 256 @ 24663 updates, score 13.904) (writing took 2.443312623538077 seconds)
2022-03-05 06:01:34 | INFO | fairseq_cli.train | end of epoch 256 (average epoch stats below)
2022-03-05 06:01:34 | INFO | train | epoch 256 | loss 1.242 | nll_loss 0.653 | ppl 1.57 | wps 24578.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24663 | lr 0.000201362 | gnorm 0.866 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 66422
2022-03-05 06:01:35 | INFO | fairseq.trainer | begin training epoch 257
2022-03-05 06:01:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:03:10 | INFO | train_inner | epoch 257:     37 / 97 loss=1.24, nll_loss=0.65, ppl=1.57, wps=24607.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=24700, lr=0.000201211, gnorm=0.863, loss_scale=32, train_wall=236, gb_free=21, wall=66518
2022-03-05 06:04:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:05:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:05:50 | INFO | valid | epoch 257 | valid on 'valid' subset | loss 13.884 | nll_loss 13.604 | ppl 12454.4 | wps 43636.1 | wpb 510.9 | bsz 1 | num_updates 24759 | best_loss 7.701
2022-03-05 06:05:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 257 @ 24759 updates
2022-03-05 06:05:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:05:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:05:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 257 @ 24759 updates, score 13.884) (writing took 2.4714808026328683 seconds)
2022-03-05 06:05:53 | INFO | fairseq_cli.train | end of epoch 257 (average epoch stats below)
2022-03-05 06:05:53 | INFO | train | epoch 257 | loss 1.239 | nll_loss 0.65 | ppl 1.57 | wps 24336.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24759 | lr 0.000200971 | gnorm 0.863 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 66681
2022-03-05 06:05:53 | INFO | fairseq.trainer | begin training epoch 258
2022-03-05 06:05:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:07:39 | INFO | train_inner | epoch 258:     41 / 97 loss=1.238, nll_loss=0.648, ppl=1.57, wps=24377.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=24800, lr=0.000200805, gnorm=0.86, loss_scale=16, train_wall=238, gb_free=21, wall=66787
2022-03-05 06:10:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:10:09 | INFO | valid | epoch 258 | valid on 'valid' subset | loss 13.852 | nll_loss 13.572 | ppl 12181.5 | wps 43578.9 | wpb 510.9 | bsz 1 | num_updates 24856 | best_loss 7.701
2022-03-05 06:10:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 258 @ 24856 updates
2022-03-05 06:10:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:10:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:10:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 258 @ 24856 updates, score 13.852) (writing took 2.467826319858432 seconds)
2022-03-05 06:10:11 | INFO | fairseq_cli.train | end of epoch 258 (average epoch stats below)
2022-03-05 06:10:11 | INFO | train | epoch 258 | loss 1.237 | nll_loss 0.648 | ppl 1.57 | wps 24607.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24856 | lr 0.000200579 | gnorm 0.854 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 66939
2022-03-05 06:10:11 | INFO | fairseq.trainer | begin training epoch 259
2022-03-05 06:10:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:10:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:12:07 | INFO | train_inner | epoch 259:     45 / 97 loss=1.236, nll_loss=0.647, ppl=1.57, wps=24391, ups=0.37, wpb=65495, bsz=127.9, num_updates=24900, lr=0.000200401, gnorm=0.852, loss_scale=16, train_wall=238, gb_free=21, wall=67055
2022-03-05 06:14:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:14:27 | INFO | valid | epoch 259 | valid on 'valid' subset | loss 13.917 | nll_loss 13.642 | ppl 12781.5 | wps 43869.3 | wpb 510.9 | bsz 1 | num_updates 24952 | best_loss 7.701
2022-03-05 06:14:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 259 @ 24952 updates
2022-03-05 06:14:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:14:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:14:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 259 @ 24952 updates, score 13.917) (writing took 2.4751264983788133 seconds)
2022-03-05 06:14:29 | INFO | fairseq_cli.train | end of epoch 259 (average epoch stats below)
2022-03-05 06:14:29 | INFO | train | epoch 259 | loss 1.235 | nll_loss 0.646 | ppl 1.56 | wps 24345.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24952 | lr 0.000200192 | gnorm 0.854 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 67197
2022-03-05 06:14:29 | INFO | fairseq.trainer | begin training epoch 260
2022-03-05 06:14:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:16:34 | INFO | train_inner | epoch 260:     48 / 97 loss=1.235, nll_loss=0.646, ppl=1.56, wps=24615.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=25000, lr=0.0002, gnorm=0.857, loss_scale=16, train_wall=236, gb_free=21, wall=67321
2022-03-05 06:16:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:18:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:18:45 | INFO | valid | epoch 260 | valid on 'valid' subset | loss 13.896 | nll_loss 13.619 | ppl 12579 | wps 43718.8 | wpb 510.9 | bsz 1 | num_updates 25048 | best_loss 7.701
2022-03-05 06:18:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 260 @ 25048 updates
2022-03-05 06:18:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:18:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:18:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 260 @ 25048 updates, score 13.896) (writing took 2.494555030018091 seconds)
2022-03-05 06:18:48 | INFO | fairseq_cli.train | end of epoch 260 (average epoch stats below)
2022-03-05 06:18:48 | INFO | train | epoch 260 | loss 1.233 | nll_loss 0.644 | ppl 1.56 | wps 24331.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25048 | lr 0.000199808 | gnorm 0.857 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 67456
2022-03-05 06:18:48 | INFO | fairseq.trainer | begin training epoch 261
2022-03-05 06:18:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:21:02 | INFO | train_inner | epoch 261:     52 / 97 loss=1.232, nll_loss=0.643, ppl=1.56, wps=24371.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=25100, lr=0.000199601, gnorm=0.855, loss_scale=16, train_wall=238, gb_free=21, wall=67590
2022-03-05 06:22:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:22:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:23:03 | INFO | valid | epoch 261 | valid on 'valid' subset | loss 13.916 | nll_loss 13.638 | ppl 12747.7 | wps 43886.5 | wpb 510.9 | bsz 1 | num_updates 25144 | best_loss 7.701
2022-03-05 06:23:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 261 @ 25144 updates
2022-03-05 06:23:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:23:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:23:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 261 @ 25144 updates, score 13.916) (writing took 2.429654440842569 seconds)
2022-03-05 06:23:06 | INFO | fairseq_cli.train | end of epoch 261 (average epoch stats below)
2022-03-05 06:23:06 | INFO | train | epoch 261 | loss 1.232 | nll_loss 0.643 | ppl 1.56 | wps 24345.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25144 | lr 0.000199426 | gnorm 0.851 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 67714
2022-03-05 06:23:06 | INFO | fairseq.trainer | begin training epoch 262
2022-03-05 06:23:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:25:31 | INFO | train_inner | epoch 262:     56 / 97 loss=1.231, nll_loss=0.641, ppl=1.56, wps=24394.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=25200, lr=0.000199205, gnorm=0.857, loss_scale=16, train_wall=238, gb_free=21, wall=67859
2022-03-05 06:27:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:27:22 | INFO | valid | epoch 262 | valid on 'valid' subset | loss 13.876 | nll_loss 13.599 | ppl 12408.6 | wps 43797.1 | wpb 510.9 | bsz 1 | num_updates 25241 | best_loss 7.701
2022-03-05 06:27:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 262 @ 25241 updates
2022-03-05 06:27:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:27:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:27:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 262 @ 25241 updates, score 13.876) (writing took 2.451620379462838 seconds)
2022-03-05 06:27:24 | INFO | fairseq_cli.train | end of epoch 262 (average epoch stats below)
2022-03-05 06:27:24 | INFO | train | epoch 262 | loss 1.231 | nll_loss 0.642 | ppl 1.56 | wps 24615.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25241 | lr 0.000199043 | gnorm 0.871 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 67972
2022-03-05 06:27:24 | INFO | fairseq.trainer | begin training epoch 263
2022-03-05 06:27:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:29:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:29:59 | INFO | train_inner | epoch 263:     60 / 97 loss=1.23, nll_loss=0.641, ppl=1.56, wps=24410.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=25300, lr=0.000198811, gnorm=0.862, loss_scale=16, train_wall=238, gb_free=21, wall=68127
2022-03-05 06:31:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:31:39 | INFO | valid | epoch 263 | valid on 'valid' subset | loss 13.87 | nll_loss 13.593 | ppl 12356.4 | wps 43789.5 | wpb 510.9 | bsz 1 | num_updates 25337 | best_loss 7.701
2022-03-05 06:31:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 263 @ 25337 updates
2022-03-05 06:31:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:31:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:31:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 263 @ 25337 updates, score 13.87) (writing took 2.4609109219163656 seconds)
2022-03-05 06:31:42 | INFO | fairseq_cli.train | end of epoch 263 (average epoch stats below)
2022-03-05 06:31:42 | INFO | train | epoch 263 | loss 1.228 | nll_loss 0.638 | ppl 1.56 | wps 24372 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25337 | lr 0.000198665 | gnorm 0.853 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 68230
2022-03-05 06:31:42 | INFO | fairseq.trainer | begin training epoch 264
2022-03-05 06:31:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:34:25 | INFO | train_inner | epoch 264:     63 / 97 loss=1.227, nll_loss=0.637, ppl=1.56, wps=24646.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=25400, lr=0.000198419, gnorm=0.857, loss_scale=16, train_wall=235, gb_free=21, wall=68393
2022-03-05 06:34:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:35:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:35:57 | INFO | valid | epoch 264 | valid on 'valid' subset | loss 13.904 | nll_loss 13.626 | ppl 12644 | wps 43700.2 | wpb 510.9 | bsz 1 | num_updates 25433 | best_loss 7.701
2022-03-05 06:35:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 264 @ 25433 updates
2022-03-05 06:35:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:36:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:36:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 264 @ 25433 updates, score 13.904) (writing took 2.4720455799251795 seconds)
2022-03-05 06:36:00 | INFO | fairseq_cli.train | end of epoch 264 (average epoch stats below)
2022-03-05 06:36:00 | INFO | train | epoch 264 | loss 1.226 | nll_loss 0.637 | ppl 1.56 | wps 24372.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25433 | lr 0.00019829 | gnorm 0.856 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 68488
2022-03-05 06:36:00 | INFO | fairseq.trainer | begin training epoch 265
2022-03-05 06:36:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:38:53 | INFO | train_inner | epoch 265:     67 / 97 loss=1.226, nll_loss=0.637, ppl=1.56, wps=24416.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=25500, lr=0.00019803, gnorm=0.851, loss_scale=16, train_wall=238, gb_free=21, wall=68661
2022-03-05 06:40:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:40:15 | INFO | valid | epoch 265 | valid on 'valid' subset | loss 13.886 | nll_loss 13.606 | ppl 12472.2 | wps 43802.6 | wpb 510.9 | bsz 1 | num_updates 25530 | best_loss 7.701
2022-03-05 06:40:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 265 @ 25530 updates
2022-03-05 06:40:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:40:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:40:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 265 @ 25530 updates, score 13.886) (writing took 2.5074354177340865 seconds)
2022-03-05 06:40:18 | INFO | fairseq_cli.train | end of epoch 265 (average epoch stats below)
2022-03-05 06:40:18 | INFO | train | epoch 265 | loss 1.225 | nll_loss 0.636 | ppl 1.55 | wps 24627.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25530 | lr 0.000197913 | gnorm 0.852 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 68746
2022-03-05 06:40:18 | INFO | fairseq.trainer | begin training epoch 266
2022-03-05 06:40:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:41:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:43:21 | INFO | train_inner | epoch 266:     71 / 97 loss=1.224, nll_loss=0.635, ppl=1.55, wps=24399.1, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=25600, lr=0.000197642, gnorm=0.862, loss_scale=16, train_wall=238, gb_free=21, wall=68929
2022-03-05 06:44:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:44:33 | INFO | valid | epoch 266 | valid on 'valid' subset | loss 13.888 | nll_loss 13.611 | ppl 12514.9 | wps 43829.6 | wpb 510.9 | bsz 1 | num_updates 25626 | best_loss 7.701
2022-03-05 06:44:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 266 @ 25626 updates
2022-03-05 06:44:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:44:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:44:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 266 @ 25626 updates, score 13.888) (writing took 2.5217882469296455 seconds)
2022-03-05 06:44:36 | INFO | fairseq_cli.train | end of epoch 266 (average epoch stats below)
2022-03-05 06:44:36 | INFO | train | epoch 266 | loss 1.223 | nll_loss 0.634 | ppl 1.55 | wps 24358.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25626 | lr 0.000197542 | gnorm 0.86 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 69004
2022-03-05 06:44:36 | INFO | fairseq.trainer | begin training epoch 267
2022-03-05 06:44:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:46:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:47:50 | INFO | train_inner | epoch 267:     75 / 97 loss=1.221, nll_loss=0.632, ppl=1.55, wps=24401.2, ups=0.37, wpb=65495, bsz=127.9, num_updates=25700, lr=0.000197257, gnorm=0.846, loss_scale=16, train_wall=238, gb_free=21, wall=69198
2022-03-05 06:48:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:48:52 | INFO | valid | epoch 267 | valid on 'valid' subset | loss 13.875 | nll_loss 13.6 | ppl 12412.8 | wps 43847.3 | wpb 510.9 | bsz 1 | num_updates 25722 | best_loss 7.701
2022-03-05 06:48:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 267 @ 25722 updates
2022-03-05 06:48:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:48:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:48:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 267 @ 25722 updates, score 13.875) (writing took 2.506404658779502 seconds)
2022-03-05 06:48:54 | INFO | fairseq_cli.train | end of epoch 267 (average epoch stats below)
2022-03-05 06:48:54 | INFO | train | epoch 267 | loss 1.221 | nll_loss 0.632 | ppl 1.55 | wps 24361.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25722 | lr 0.000197173 | gnorm 0.848 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 69262
2022-03-05 06:48:54 | INFO | fairseq.trainer | begin training epoch 268
2022-03-05 06:48:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:52:16 | INFO | train_inner | epoch 268:     78 / 97 loss=1.221, nll_loss=0.632, ppl=1.55, wps=24632.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=25800, lr=0.000196875, gnorm=0.854, loss_scale=16, train_wall=236, gb_free=21, wall=69464
2022-03-05 06:52:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:53:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:53:10 | INFO | valid | epoch 268 | valid on 'valid' subset | loss 13.925 | nll_loss 13.652 | ppl 12871.7 | wps 43906.4 | wpb 510.9 | bsz 1 | num_updates 25818 | best_loss 7.701
2022-03-05 06:53:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 268 @ 25818 updates
2022-03-05 06:53:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:53:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:53:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 268 @ 25818 updates, score 13.925) (writing took 2.48563149664551 seconds)
2022-03-05 06:53:12 | INFO | fairseq_cli.train | end of epoch 268 (average epoch stats below)
2022-03-05 06:53:12 | INFO | train | epoch 268 | loss 1.22 | nll_loss 0.63 | ppl 1.55 | wps 24358.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 25818 | lr 0.000196806 | gnorm 0.851 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 69520
2022-03-05 06:53:12 | INFO | fairseq.trainer | begin training epoch 269
2022-03-05 06:53:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:56:44 | INFO | train_inner | epoch 269:     82 / 97 loss=1.22, nll_loss=0.631, ppl=1.55, wps=24403.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=25900, lr=0.000196494, gnorm=0.852, loss_scale=16, train_wall=238, gb_free=21, wall=69732
2022-03-05 06:57:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:57:28 | INFO | valid | epoch 269 | valid on 'valid' subset | loss 13.942 | nll_loss 13.667 | ppl 13006.4 | wps 44032.5 | wpb 510.9 | bsz 1 | num_updates 25915 | best_loss 7.701
2022-03-05 06:57:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 269 @ 25915 updates
2022-03-05 06:57:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:57:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 06:57:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 269 @ 25915 updates, score 13.942) (writing took 2.5117312958464026 seconds)
2022-03-05 06:57:30 | INFO | fairseq_cli.train | end of epoch 269 (average epoch stats below)
2022-03-05 06:57:30 | INFO | train | epoch 269 | loss 1.219 | nll_loss 0.63 | ppl 1.55 | wps 24621.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25915 | lr 0.000196437 | gnorm 0.853 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 69778
2022-03-05 06:57:30 | INFO | fairseq.trainer | begin training epoch 270
2022-03-05 06:57:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:59:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:01:12 | INFO | train_inner | epoch 270:     86 / 97 loss=1.217, nll_loss=0.628, ppl=1.55, wps=24402.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=26000, lr=0.000196116, gnorm=0.852, loss_scale=16, train_wall=238, gb_free=21, wall=70000
2022-03-05 07:01:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:01:46 | INFO | valid | epoch 270 | valid on 'valid' subset | loss 13.902 | nll_loss 13.626 | ppl 12643.1 | wps 43690.3 | wpb 510.9 | bsz 1 | num_updates 26011 | best_loss 7.701
2022-03-05 07:01:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 270 @ 26011 updates
2022-03-05 07:01:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:01:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:01:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 270 @ 26011 updates, score 13.902) (writing took 2.5161052346229553 seconds)
2022-03-05 07:01:48 | INFO | fairseq_cli.train | end of epoch 270 (average epoch stats below)
2022-03-05 07:01:48 | INFO | train | epoch 270 | loss 1.216 | nll_loss 0.627 | ppl 1.54 | wps 24356.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26011 | lr 0.000196075 | gnorm 0.849 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 70036
2022-03-05 07:01:48 | INFO | fairseq.trainer | begin training epoch 271
2022-03-05 07:01:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:05:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:05:41 | INFO | train_inner | epoch 271:     90 / 97 loss=1.215, nll_loss=0.626, ppl=1.54, wps=24402, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=26100, lr=0.00019574, gnorm=0.842, loss_scale=16, train_wall=238, gb_free=21, wall=70269
2022-03-05 07:05:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:06:04 | INFO | valid | epoch 271 | valid on 'valid' subset | loss 13.91 | nll_loss 13.634 | ppl 12715.2 | wps 43928.4 | wpb 510.9 | bsz 1 | num_updates 26107 | best_loss 7.701
2022-03-05 07:06:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 271 @ 26107 updates
2022-03-05 07:06:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:06:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:06:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 271 @ 26107 updates, score 13.91) (writing took 2.486609648913145 seconds)
2022-03-05 07:06:06 | INFO | fairseq_cli.train | end of epoch 271 (average epoch stats below)
2022-03-05 07:06:06 | INFO | train | epoch 271 | loss 1.215 | nll_loss 0.626 | ppl 1.54 | wps 24369.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26107 | lr 0.000195714 | gnorm 0.844 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 70294
2022-03-05 07:06:06 | INFO | fairseq.trainer | begin training epoch 272
2022-03-05 07:06:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:10:07 | INFO | train_inner | epoch 272:     93 / 97 loss=1.215, nll_loss=0.626, ppl=1.54, wps=24636.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26200, lr=0.000195366, gnorm=0.858, loss_scale=16, train_wall=236, gb_free=21, wall=70535
2022-03-05 07:10:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:10:22 | INFO | valid | epoch 272 | valid on 'valid' subset | loss 13.891 | nll_loss 13.614 | ppl 12541.3 | wps 43826.8 | wpb 510.9 | bsz 1 | num_updates 26204 | best_loss 7.701
2022-03-05 07:10:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 272 @ 26204 updates
2022-03-05 07:10:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:10:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:10:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 272 @ 26204 updates, score 13.891) (writing took 2.501086519099772 seconds)
2022-03-05 07:10:24 | INFO | fairseq_cli.train | end of epoch 272 (average epoch stats below)
2022-03-05 07:10:24 | INFO | train | epoch 272 | loss 1.214 | nll_loss 0.625 | ppl 1.54 | wps 24609.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26204 | lr 0.000195351 | gnorm 0.859 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 70552
2022-03-05 07:10:24 | INFO | fairseq.trainer | begin training epoch 273
2022-03-05 07:10:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:11:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:14:35 | INFO | train_inner | epoch 273:     97 / 97 loss=1.211, nll_loss=0.623, ppl=1.54, wps=24401.1, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=26300, lr=0.000194994, gnorm=0.842, loss_scale=16, train_wall=238, gb_free=21, wall=70803
2022-03-05 07:14:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:14:40 | INFO | valid | epoch 273 | valid on 'valid' subset | loss 13.918 | nll_loss 13.642 | ppl 12784.5 | wps 44030.3 | wpb 510.9 | bsz 1 | num_updates 26300 | best_loss 7.701
2022-03-05 07:14:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 273 @ 26300 updates
2022-03-05 07:14:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:14:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:14:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 273 @ 26300 updates, score 13.918) (writing took 2.5104858754202724 seconds)
2022-03-05 07:14:42 | INFO | fairseq_cli.train | end of epoch 273 (average epoch stats below)
2022-03-05 07:14:42 | INFO | train | epoch 273 | loss 1.211 | nll_loss 0.622 | ppl 1.54 | wps 24367.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26300 | lr 0.000194994 | gnorm 0.84 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 70810
2022-03-05 07:14:42 | INFO | fairseq.trainer | begin training epoch 274
2022-03-05 07:14:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:17:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:18:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:18:58 | INFO | valid | epoch 274 | valid on 'valid' subset | loss 13.894 | nll_loss 13.615 | ppl 12544 | wps 43814.1 | wpb 510.9 | bsz 1 | num_updates 26396 | best_loss 7.701
2022-03-05 07:18:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 274 @ 26396 updates
2022-03-05 07:18:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:19:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:19:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 274 @ 26396 updates, score 13.894) (writing took 2.497152540832758 seconds)
2022-03-05 07:19:00 | INFO | fairseq_cli.train | end of epoch 274 (average epoch stats below)
2022-03-05 07:19:00 | INFO | train | epoch 274 | loss 1.21 | nll_loss 0.621 | ppl 1.54 | wps 24374.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26396 | lr 0.000194639 | gnorm 0.851 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 71068
2022-03-05 07:19:00 | INFO | fairseq.trainer | begin training epoch 275
2022-03-05 07:19:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:19:11 | INFO | train_inner | epoch 275:      4 / 97 loss=1.209, nll_loss=0.62, ppl=1.54, wps=23737.3, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=26400, lr=0.000194625, gnorm=0.85, loss_scale=16, train_wall=238, gb_free=21, wall=71079
2022-03-05 07:23:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:23:16 | INFO | valid | epoch 275 | valid on 'valid' subset | loss 13.94 | nll_loss 13.666 | ppl 13000.8 | wps 43846.6 | wpb 510.9 | bsz 1 | num_updates 26493 | best_loss 7.701
2022-03-05 07:23:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 275 @ 26493 updates
2022-03-05 07:23:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:23:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:23:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 275 @ 26493 updates, score 13.94) (writing took 2.3854430047795177 seconds)
2022-03-05 07:23:18 | INFO | fairseq_cli.train | end of epoch 275 (average epoch stats below)
2022-03-05 07:23:18 | INFO | train | epoch 275 | loss 1.209 | nll_loss 0.62 | ppl 1.54 | wps 24630.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26493 | lr 0.000194283 | gnorm 0.85 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 71326
2022-03-05 07:23:18 | INFO | fairseq.trainer | begin training epoch 276
2022-03-05 07:23:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:23:37 | INFO | train_inner | epoch 276:      7 / 97 loss=1.208, nll_loss=0.619, ppl=1.54, wps=24649.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26500, lr=0.000194257, gnorm=0.848, loss_scale=16, train_wall=236, gb_free=21, wall=71344
2022-03-05 07:24:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:27:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:27:34 | INFO | valid | epoch 276 | valid on 'valid' subset | loss 13.908 | nll_loss 13.634 | ppl 12712.8 | wps 43820.2 | wpb 510.9 | bsz 1 | num_updates 26589 | best_loss 7.701
2022-03-05 07:27:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 276 @ 26589 updates
2022-03-05 07:27:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:27:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:27:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 276 @ 26589 updates, score 13.908) (writing took 2.5333799459040165 seconds)
2022-03-05 07:27:36 | INFO | fairseq_cli.train | end of epoch 276 (average epoch stats below)
2022-03-05 07:27:36 | INFO | train | epoch 276 | loss 1.206 | nll_loss 0.617 | ppl 1.53 | wps 24360.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26589 | lr 0.000193932 | gnorm 0.841 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 71584
2022-03-05 07:27:36 | INFO | fairseq.trainer | begin training epoch 277
2022-03-05 07:27:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:28:05 | INFO | train_inner | epoch 277:     11 / 97 loss=1.205, nll_loss=0.616, ppl=1.53, wps=24399.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=26600, lr=0.000193892, gnorm=0.841, loss_scale=16, train_wall=238, gb_free=21, wall=71613
2022-03-05 07:30:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:31:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:31:52 | INFO | valid | epoch 277 | valid on 'valid' subset | loss 13.919 | nll_loss 13.645 | ppl 12814.6 | wps 43800.7 | wpb 510.9 | bsz 1 | num_updates 26685 | best_loss 7.701
2022-03-05 07:31:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 277 @ 26685 updates
2022-03-05 07:31:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:31:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:31:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 277 @ 26685 updates, score 13.919) (writing took 2.460253627039492 seconds)
2022-03-05 07:31:54 | INFO | fairseq_cli.train | end of epoch 277 (average epoch stats below)
2022-03-05 07:31:54 | INFO | train | epoch 277 | loss 1.205 | nll_loss 0.616 | ppl 1.53 | wps 24368.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26685 | lr 0.000193583 | gnorm 0.843 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 71842
2022-03-05 07:31:54 | INFO | fairseq.trainer | begin training epoch 278
2022-03-05 07:31:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:32:33 | INFO | train_inner | epoch 278:     15 / 97 loss=1.203, nll_loss=0.615, ppl=1.53, wps=24401.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=26700, lr=0.000193528, gnorm=0.841, loss_scale=16, train_wall=238, gb_free=21, wall=71881
2022-03-05 07:36:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:36:10 | INFO | valid | epoch 278 | valid on 'valid' subset | loss 13.888 | nll_loss 13.614 | ppl 12541.2 | wps 43668 | wpb 510.9 | bsz 1 | num_updates 26782 | best_loss 7.701
2022-03-05 07:36:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 278 @ 26782 updates
2022-03-05 07:36:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:36:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:36:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 278 @ 26782 updates, score 13.888) (writing took 2.4898433219641447 seconds)
2022-03-05 07:36:13 | INFO | fairseq_cli.train | end of epoch 278 (average epoch stats below)
2022-03-05 07:36:13 | INFO | train | epoch 278 | loss 1.203 | nll_loss 0.614 | ppl 1.53 | wps 24610.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26782 | lr 0.000193232 | gnorm 0.834 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 72100
2022-03-05 07:36:13 | INFO | fairseq.trainer | begin training epoch 279
2022-03-05 07:36:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:36:59 | INFO | train_inner | epoch 279:     18 / 97 loss=1.202, nll_loss=0.613, ppl=1.53, wps=24638.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26800, lr=0.000193167, gnorm=0.834, loss_scale=32, train_wall=235, gb_free=21, wall=72147
2022-03-05 07:37:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:40:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:40:28 | INFO | valid | epoch 279 | valid on 'valid' subset | loss 13.881 | nll_loss 13.606 | ppl 12470.7 | wps 43844.9 | wpb 510.9 | bsz 1 | num_updates 26878 | best_loss 7.701
2022-03-05 07:40:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 279 @ 26878 updates
2022-03-05 07:40:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:40:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:40:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 279 @ 26878 updates, score 13.881) (writing took 2.523152071982622 seconds)
2022-03-05 07:40:31 | INFO | fairseq_cli.train | end of epoch 279 (average epoch stats below)
2022-03-05 07:40:31 | INFO | train | epoch 279 | loss 1.202 | nll_loss 0.613 | ppl 1.53 | wps 24365.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26878 | lr 0.000192886 | gnorm 0.844 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 72358
2022-03-05 07:40:31 | INFO | fairseq.trainer | begin training epoch 280
2022-03-05 07:40:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:41:28 | INFO | train_inner | epoch 280:     22 / 97 loss=1.201, nll_loss=0.612, ppl=1.53, wps=24404, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=26900, lr=0.000192807, gnorm=0.843, loss_scale=16, train_wall=238, gb_free=21, wall=72415
2022-03-05 07:43:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:44:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:44:46 | INFO | valid | epoch 280 | valid on 'valid' subset | loss 13.937 | nll_loss 13.662 | ppl 12959.5 | wps 43812.2 | wpb 510.9 | bsz 1 | num_updates 26974 | best_loss 7.701
2022-03-05 07:44:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 280 @ 26974 updates
2022-03-05 07:44:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:44:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:44:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 280 @ 26974 updates, score 13.937) (writing took 2.4896638188511133 seconds)
2022-03-05 07:44:49 | INFO | fairseq_cli.train | end of epoch 280 (average epoch stats below)
2022-03-05 07:44:49 | INFO | train | epoch 280 | loss 1.2 | nll_loss 0.611 | ppl 1.53 | wps 24371.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 26974 | lr 0.000192543 | gnorm 0.845 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 72616
2022-03-05 07:44:49 | INFO | fairseq.trainer | begin training epoch 281
2022-03-05 07:44:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:45:56 | INFO | train_inner | epoch 281:     26 / 97 loss=1.2, nll_loss=0.611, ppl=1.53, wps=24409.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=27000, lr=0.00019245, gnorm=0.844, loss_scale=16, train_wall=238, gb_free=21, wall=72684
2022-03-05 07:48:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:49:04 | INFO | valid | epoch 281 | valid on 'valid' subset | loss 13.908 | nll_loss 13.634 | ppl 12710.8 | wps 44323 | wpb 510.9 | bsz 1 | num_updates 27071 | best_loss 7.701
2022-03-05 07:49:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 281 @ 27071 updates
2022-03-05 07:49:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:49:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:49:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 281 @ 27071 updates, score 13.908) (writing took 2.5321335420012474 seconds)
2022-03-05 07:49:07 | INFO | fairseq_cli.train | end of epoch 281 (average epoch stats below)
2022-03-05 07:49:07 | INFO | train | epoch 281 | loss 1.199 | nll_loss 0.61 | ppl 1.53 | wps 24615.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27071 | lr 0.000192198 | gnorm 0.839 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 72874
2022-03-05 07:49:07 | INFO | fairseq.trainer | begin training epoch 282
2022-03-05 07:49:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:50:22 | INFO | train_inner | epoch 282:     29 / 97 loss=1.198, nll_loss=0.61, ppl=1.53, wps=24639.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27100, lr=0.000192095, gnorm=0.839, loss_scale=32, train_wall=236, gb_free=21, wall=72950
2022-03-05 07:51:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:53:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:53:22 | INFO | valid | epoch 282 | valid on 'valid' subset | loss 13.913 | nll_loss 13.641 | ppl 12770.8 | wps 43780.3 | wpb 510.9 | bsz 1 | num_updates 27167 | best_loss 7.701
2022-03-05 07:53:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 282 @ 27167 updates
2022-03-05 07:53:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:53:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:53:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 282 @ 27167 updates, score 13.913) (writing took 2.5350051755085588 seconds)
2022-03-05 07:53:25 | INFO | fairseq_cli.train | end of epoch 282 (average epoch stats below)
2022-03-05 07:53:25 | INFO | train | epoch 282 | loss 1.197 | nll_loss 0.608 | ppl 1.52 | wps 24364.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27167 | lr 0.000191858 | gnorm 0.838 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 73133
2022-03-05 07:53:25 | INFO | fairseq.trainer | begin training epoch 283
2022-03-05 07:53:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:54:50 | INFO | train_inner | epoch 283:     33 / 97 loss=1.196, nll_loss=0.607, ppl=1.52, wps=24398.4, ups=0.37, wpb=65495, bsz=127.9, num_updates=27200, lr=0.000191741, gnorm=0.832, loss_scale=16, train_wall=238, gb_free=21, wall=73218
2022-03-05 07:56:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:57:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:57:40 | INFO | valid | epoch 283 | valid on 'valid' subset | loss 13.897 | nll_loss 13.624 | ppl 12625 | wps 43838.6 | wpb 510.9 | bsz 1 | num_updates 27263 | best_loss 7.701
2022-03-05 07:57:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 283 @ 27263 updates
2022-03-05 07:57:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:57:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 07:57:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 283 @ 27263 updates, score 13.897) (writing took 2.534397470764816 seconds)
2022-03-05 07:57:43 | INFO | fairseq_cli.train | end of epoch 283 (average epoch stats below)
2022-03-05 07:57:43 | INFO | train | epoch 283 | loss 1.195 | nll_loss 0.607 | ppl 1.52 | wps 24357 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27263 | lr 0.00019152 | gnorm 0.834 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 73391
2022-03-05 07:57:43 | INFO | fairseq.trainer | begin training epoch 284
2022-03-05 07:57:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:59:18 | INFO | train_inner | epoch 284:     37 / 97 loss=1.194, nll_loss=0.605, ppl=1.52, wps=24405.1, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=27300, lr=0.00019139, gnorm=0.839, loss_scale=16, train_wall=238, gb_free=21, wall=73486
2022-03-05 08:01:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:01:58 | INFO | valid | epoch 284 | valid on 'valid' subset | loss 13.909 | nll_loss 13.634 | ppl 12714.7 | wps 43666.4 | wpb 510.9 | bsz 1 | num_updates 27360 | best_loss 7.701
2022-03-05 08:01:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 284 @ 27360 updates
2022-03-05 08:01:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:02:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:02:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 284 @ 27360 updates, score 13.909) (writing took 2.5074022728949785 seconds)
2022-03-05 08:02:01 | INFO | fairseq_cli.train | end of epoch 284 (average epoch stats below)
2022-03-05 08:02:01 | INFO | train | epoch 284 | loss 1.195 | nll_loss 0.606 | ppl 1.52 | wps 24620.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27360 | lr 0.00019118 | gnorm 0.84 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 73649
2022-03-05 08:02:01 | INFO | fairseq.trainer | begin training epoch 285
2022-03-05 08:02:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:03:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:03:47 | INFO | train_inner | epoch 285:     41 / 97 loss=1.193, nll_loss=0.605, ppl=1.52, wps=24406.3, ups=0.37, wpb=65495, bsz=127.9, num_updates=27400, lr=0.00019104, gnorm=0.835, loss_scale=16, train_wall=238, gb_free=21, wall=73755
2022-03-05 08:06:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:06:16 | INFO | valid | epoch 285 | valid on 'valid' subset | loss 13.88 | nll_loss 13.604 | ppl 12451.1 | wps 43841.7 | wpb 510.9 | bsz 1 | num_updates 27456 | best_loss 7.701
2022-03-05 08:06:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 285 @ 27456 updates
2022-03-05 08:06:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:06:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:06:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 285 @ 27456 updates, score 13.88) (writing took 2.4306801464408636 seconds)
2022-03-05 08:06:19 | INFO | fairseq_cli.train | end of epoch 285 (average epoch stats below)
2022-03-05 08:06:19 | INFO | train | epoch 285 | loss 1.192 | nll_loss 0.604 | ppl 1.52 | wps 24382.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27456 | lr 0.000190845 | gnorm 0.833 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 73907
2022-03-05 08:06:19 | INFO | fairseq.trainer | begin training epoch 286
2022-03-05 08:06:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:08:12 | INFO | train_inner | epoch 286:     44 / 97 loss=1.193, nll_loss=0.604, ppl=1.52, wps=24651.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27500, lr=0.000190693, gnorm=0.836, loss_scale=16, train_wall=235, gb_free=21, wall=74020
2022-03-05 08:09:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:10:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:10:34 | INFO | valid | epoch 286 | valid on 'valid' subset | loss 13.925 | nll_loss 13.65 | ppl 12851.6 | wps 43868.8 | wpb 510.9 | bsz 1 | num_updates 27552 | best_loss 7.701
2022-03-05 08:10:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 286 @ 27552 updates
2022-03-05 08:10:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:10:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:10:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 286 @ 27552 updates, score 13.925) (writing took 2.523959851823747 seconds)
2022-03-05 08:10:37 | INFO | fairseq_cli.train | end of epoch 286 (average epoch stats below)
2022-03-05 08:10:37 | INFO | train | epoch 286 | loss 1.192 | nll_loss 0.603 | ppl 1.52 | wps 24363.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27552 | lr 0.000190512 | gnorm 0.839 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 74165
2022-03-05 08:10:37 | INFO | fairseq.trainer | begin training epoch 287
2022-03-05 08:10:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:12:41 | INFO | train_inner | epoch 287:     48 / 97 loss=1.19, nll_loss=0.601, ppl=1.52, wps=24409.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=27600, lr=0.000190347, gnorm=0.834, loss_scale=16, train_wall=238, gb_free=21, wall=74289
2022-03-05 08:14:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:14:52 | INFO | valid | epoch 287 | valid on 'valid' subset | loss 13.938 | nll_loss 13.668 | ppl 13012 | wps 43755.2 | wpb 510.9 | bsz 1 | num_updates 27649 | best_loss 7.701
2022-03-05 08:14:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 287 @ 27649 updates
2022-03-05 08:14:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:14:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:14:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 287 @ 27649 updates, score 13.938) (writing took 2.4826269950717688 seconds)
2022-03-05 08:14:55 | INFO | fairseq_cli.train | end of epoch 287 (average epoch stats below)
2022-03-05 08:14:55 | INFO | train | epoch 287 | loss 1.189 | nll_loss 0.6 | ppl 1.52 | wps 24627 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27649 | lr 0.000190178 | gnorm 0.827 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 74423
2022-03-05 08:14:55 | INFO | fairseq.trainer | begin training epoch 288
2022-03-05 08:14:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:17:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:17:09 | INFO | train_inner | epoch 288:     52 / 97 loss=1.19, nll_loss=0.602, ppl=1.52, wps=24408.4, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=27700, lr=0.000190003, gnorm=0.828, loss_scale=16, train_wall=238, gb_free=21, wall=74557
2022-03-05 08:19:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:19:10 | INFO | valid | epoch 288 | valid on 'valid' subset | loss 13.902 | nll_loss 13.628 | ppl 12659 | wps 42464.1 | wpb 510.9 | bsz 1 | num_updates 27745 | best_loss 7.701
2022-03-05 08:19:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 288 @ 27745 updates
2022-03-05 08:19:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:19:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:19:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 288 @ 27745 updates, score 13.902) (writing took 2.5927793430164456 seconds)
2022-03-05 08:19:13 | INFO | fairseq_cli.train | end of epoch 288 (average epoch stats below)
2022-03-05 08:19:13 | INFO | train | epoch 288 | loss 1.188 | nll_loss 0.6 | ppl 1.52 | wps 24341.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27745 | lr 0.000189849 | gnorm 0.83 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 74681
2022-03-05 08:19:13 | INFO | fairseq.trainer | begin training epoch 289
2022-03-05 08:19:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:21:35 | INFO | train_inner | epoch 289:     55 / 97 loss=1.186, nll_loss=0.597, ppl=1.51, wps=24617, ups=0.38, wpb=65495, bsz=127.9, num_updates=27800, lr=0.000189661, gnorm=0.828, loss_scale=16, train_wall=235, gb_free=21, wall=74823
2022-03-05 08:23:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:23:29 | INFO | valid | epoch 289 | valid on 'valid' subset | loss 13.936 | nll_loss 13.662 | ppl 12963.5 | wps 42581.5 | wpb 510.9 | bsz 1 | num_updates 27842 | best_loss 7.701
2022-03-05 08:23:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 289 @ 27842 updates
2022-03-05 08:23:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:23:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:23:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 289 @ 27842 updates, score 13.936) (writing took 2.490108946338296 seconds)
2022-03-05 08:23:31 | INFO | fairseq_cli.train | end of epoch 289 (average epoch stats below)
2022-03-05 08:23:31 | INFO | train | epoch 289 | loss 1.187 | nll_loss 0.598 | ppl 1.51 | wps 24603.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27842 | lr 0.000189518 | gnorm 0.827 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 74939
2022-03-05 08:23:31 | INFO | fairseq.trainer | begin training epoch 290
2022-03-05 08:23:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:23:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:26:04 | INFO | train_inner | epoch 290:     59 / 97 loss=1.186, nll_loss=0.598, ppl=1.51, wps=24393.9, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=27900, lr=0.000189321, gnorm=0.831, loss_scale=16, train_wall=238, gb_free=21, wall=75091
2022-03-05 08:27:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:27:47 | INFO | valid | epoch 290 | valid on 'valid' subset | loss 13.926 | nll_loss 13.654 | ppl 12888.5 | wps 43445 | wpb 510.9 | bsz 1 | num_updates 27938 | best_loss 7.701
2022-03-05 08:27:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 290 @ 27938 updates
2022-03-05 08:27:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:27:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:27:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 290 @ 27938 updates, score 13.926) (writing took 2.6966779958456755 seconds)
2022-03-05 08:27:49 | INFO | fairseq_cli.train | end of epoch 290 (average epoch stats below)
2022-03-05 08:27:49 | INFO | train | epoch 290 | loss 1.185 | nll_loss 0.597 | ppl 1.51 | wps 24349.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 27938 | lr 0.000189192 | gnorm 0.836 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 75197
2022-03-05 08:27:49 | INFO | fairseq.trainer | begin training epoch 291
2022-03-05 08:27:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:30:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:30:32 | INFO | train_inner | epoch 291:     63 / 97 loss=1.185, nll_loss=0.596, ppl=1.51, wps=24386.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=28000, lr=0.000188982, gnorm=0.838, loss_scale=16, train_wall=238, gb_free=21, wall=75360
2022-03-05 08:32:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:32:05 | INFO | valid | epoch 291 | valid on 'valid' subset | loss 13.941 | nll_loss 13.669 | ppl 13028.6 | wps 43940.1 | wpb 510.9 | bsz 1 | num_updates 28034 | best_loss 7.701
2022-03-05 08:32:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 291 @ 28034 updates
2022-03-05 08:32:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:32:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:32:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 291 @ 28034 updates, score 13.941) (writing took 2.505027991719544 seconds)
2022-03-05 08:32:07 | INFO | fairseq_cli.train | end of epoch 291 (average epoch stats below)
2022-03-05 08:32:07 | INFO | train | epoch 291 | loss 1.184 | nll_loss 0.596 | ppl 1.51 | wps 24360.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 28034 | lr 0.000188868 | gnorm 0.836 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 75455
2022-03-05 08:32:07 | INFO | fairseq.trainer | begin training epoch 292
2022-03-05 08:32:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:34:58 | INFO | train_inner | epoch 292:     66 / 97 loss=1.184, nll_loss=0.596, ppl=1.51, wps=24652.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=28100, lr=0.000188646, gnorm=0.839, loss_scale=16, train_wall=235, gb_free=21, wall=75626
2022-03-05 08:36:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:36:23 | INFO | valid | epoch 292 | valid on 'valid' subset | loss 13.941 | nll_loss 13.668 | ppl 13013.8 | wps 44011.5 | wpb 510.9 | bsz 1 | num_updates 28131 | best_loss 7.701
2022-03-05 08:36:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 292 @ 28131 updates
2022-03-05 08:36:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:36:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:36:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 292 @ 28131 updates, score 13.941) (writing took 2.5400758208706975 seconds)
2022-03-05 08:36:25 | INFO | fairseq_cli.train | end of epoch 292 (average epoch stats below)
2022-03-05 08:36:25 | INFO | train | epoch 292 | loss 1.183 | nll_loss 0.594 | ppl 1.51 | wps 24634.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28131 | lr 0.000188542 | gnorm 0.84 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 75713
2022-03-05 08:36:25 | INFO | fairseq.trainer | begin training epoch 293
2022-03-05 08:36:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:36:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:39:26 | INFO | train_inner | epoch 293:     70 / 97 loss=1.181, nll_loss=0.593, ppl=1.51, wps=24411, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=28200, lr=0.000188311, gnorm=0.839, loss_scale=16, train_wall=238, gb_free=21, wall=75894
2022-03-05 08:40:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:40:41 | INFO | valid | epoch 293 | valid on 'valid' subset | loss 13.925 | nll_loss 13.65 | ppl 12853.3 | wps 43879.4 | wpb 510.9 | bsz 1 | num_updates 28227 | best_loss 7.701
2022-03-05 08:40:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 293 @ 28227 updates
2022-03-05 08:40:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:40:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:40:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 293 @ 28227 updates, score 13.925) (writing took 2.536622499115765 seconds)
2022-03-05 08:40:43 | INFO | fairseq_cli.train | end of epoch 293 (average epoch stats below)
2022-03-05 08:40:43 | INFO | train | epoch 293 | loss 1.181 | nll_loss 0.593 | ppl 1.51 | wps 24374.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 28227 | lr 0.000188221 | gnorm 0.84 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 75971
2022-03-05 08:40:43 | INFO | fairseq.trainer | begin training epoch 294
2022-03-05 08:40:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:42:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:43:54 | INFO | train_inner | epoch 294:     74 / 97 loss=1.18, nll_loss=0.592, ppl=1.51, wps=24416.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=28300, lr=0.000187978, gnorm=0.834, loss_scale=16, train_wall=238, gb_free=21, wall=76162
2022-03-05 08:44:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:44:59 | INFO | valid | epoch 294 | valid on 'valid' subset | loss 13.984 | nll_loss 13.711 | ppl 13410.9 | wps 43992.5 | wpb 510.9 | bsz 1 | num_updates 28323 | best_loss 7.701
2022-03-05 08:44:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 294 @ 28323 updates
2022-03-05 08:44:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:45:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:45:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 294 @ 28323 updates, score 13.984) (writing took 2.665615730918944 seconds)
2022-03-05 08:45:01 | INFO | fairseq_cli.train | end of epoch 294 (average epoch stats below)
2022-03-05 08:45:01 | INFO | train | epoch 294 | loss 1.179 | nll_loss 0.591 | ppl 1.51 | wps 24363.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 28323 | lr 0.000187902 | gnorm 0.834 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 76229
2022-03-05 08:45:01 | INFO | fairseq.trainer | begin training epoch 295
2022-03-05 08:45:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:48:20 | INFO | train_inner | epoch 295:     77 / 97 loss=1.179, nll_loss=0.591, ppl=1.51, wps=24647.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28400, lr=0.000187647, gnorm=0.833, loss_scale=16, train_wall=235, gb_free=21, wall=76428
2022-03-05 08:48:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:49:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:49:17 | INFO | valid | epoch 295 | valid on 'valid' subset | loss 13.915 | nll_loss 13.642 | ppl 12779.9 | wps 43893.7 | wpb 510.9 | bsz 1 | num_updates 28419 | best_loss 7.701
2022-03-05 08:49:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 295 @ 28419 updates
2022-03-05 08:49:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:49:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:49:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 295 @ 28419 updates, score 13.915) (writing took 2.566021758131683 seconds)
2022-03-05 08:49:19 | INFO | fairseq_cli.train | end of epoch 295 (average epoch stats below)
2022-03-05 08:49:19 | INFO | train | epoch 295 | loss 1.178 | nll_loss 0.589 | ppl 1.5 | wps 24382.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 28419 | lr 0.000187584 | gnorm 0.828 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 76487
2022-03-05 08:49:19 | INFO | fairseq.trainer | begin training epoch 296
2022-03-05 08:49:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:52:48 | INFO | train_inner | epoch 296:     81 / 97 loss=1.177, nll_loss=0.588, ppl=1.5, wps=24422.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=28500, lr=0.000187317, gnorm=0.828, loss_scale=16, train_wall=238, gb_free=21, wall=76696
2022-03-05 08:53:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:53:35 | INFO | valid | epoch 296 | valid on 'valid' subset | loss 13.957 | nll_loss 13.684 | ppl 13158.5 | wps 43782 | wpb 510.9 | bsz 1 | num_updates 28516 | best_loss 7.701
2022-03-05 08:53:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 296 @ 28516 updates
2022-03-05 08:53:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:53:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:53:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 296 @ 28516 updates, score 13.957) (writing took 2.5929034454748034 seconds)
2022-03-05 08:53:37 | INFO | fairseq_cli.train | end of epoch 296 (average epoch stats below)
2022-03-05 08:53:37 | INFO | train | epoch 296 | loss 1.176 | nll_loss 0.588 | ppl 1.5 | wps 24628 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28516 | lr 0.000187265 | gnorm 0.828 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 76745
2022-03-05 08:53:37 | INFO | fairseq.trainer | begin training epoch 297
2022-03-05 08:53:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:57:14 | INFO | train_inner | epoch 297:     84 / 97 loss=1.175, nll_loss=0.587, ppl=1.5, wps=24647.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28600, lr=0.000186989, gnorm=0.834, loss_scale=32, train_wall=235, gb_free=21, wall=76962
2022-03-05 08:57:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:57:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:57:52 | INFO | valid | epoch 297 | valid on 'valid' subset | loss 13.972 | nll_loss 13.701 | ppl 13321.2 | wps 43923.8 | wpb 510.9 | bsz 1 | num_updates 28612 | best_loss 7.701
2022-03-05 08:57:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 297 @ 28612 updates
2022-03-05 08:57:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:57:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 08:57:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 297 @ 28612 updates, score 13.972) (writing took 2.6620988165959716 seconds)
2022-03-05 08:57:55 | INFO | fairseq_cli.train | end of epoch 297 (average epoch stats below)
2022-03-05 08:57:55 | INFO | train | epoch 297 | loss 1.175 | nll_loss 0.587 | ppl 1.5 | wps 24370.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 28612 | lr 0.00018695 | gnorm 0.834 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 77003
2022-03-05 08:57:55 | INFO | fairseq.trainer | begin training epoch 298
2022-03-05 08:57:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:01:42 | INFO | train_inner | epoch 298:     88 / 97 loss=1.174, nll_loss=0.586, ppl=1.5, wps=24405.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=28700, lr=0.000186663, gnorm=0.819, loss_scale=16, train_wall=238, gb_free=21, wall=77230
2022-03-05 09:02:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:02:10 | INFO | valid | epoch 298 | valid on 'valid' subset | loss 13.929 | nll_loss 13.655 | ppl 12903.4 | wps 44035.9 | wpb 510.9 | bsz 1 | num_updates 28709 | best_loss 7.701
2022-03-05 09:02:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 298 @ 28709 updates
2022-03-05 09:02:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:02:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:02:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 298 @ 28709 updates, score 13.929) (writing took 2.577012646012008 seconds)
2022-03-05 09:02:13 | INFO | fairseq_cli.train | end of epoch 298 (average epoch stats below)
2022-03-05 09:02:13 | INFO | train | epoch 298 | loss 1.173 | nll_loss 0.585 | ppl 1.5 | wps 24627.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28709 | lr 0.000186634 | gnorm 0.817 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 77261
2022-03-05 09:02:13 | INFO | fairseq.trainer | begin training epoch 299
2022-03-05 09:02:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:03:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:06:11 | INFO | train_inner | epoch 299:     92 / 97 loss=1.173, nll_loss=0.585, ppl=1.5, wps=24413.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=28800, lr=0.000186339, gnorm=0.828, loss_scale=16, train_wall=238, gb_free=21, wall=77498
2022-03-05 09:06:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:06:28 | INFO | valid | epoch 299 | valid on 'valid' subset | loss 13.958 | nll_loss 13.688 | ppl 13196.4 | wps 43949 | wpb 510.9 | bsz 1 | num_updates 28805 | best_loss 7.701
2022-03-05 09:06:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 299 @ 28805 updates
2022-03-05 09:06:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:06:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:06:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 299 @ 28805 updates, score 13.958) (writing took 2.5640104515478015 seconds)
2022-03-05 09:06:31 | INFO | fairseq_cli.train | end of epoch 299 (average epoch stats below)
2022-03-05 09:06:31 | INFO | train | epoch 299 | loss 1.172 | nll_loss 0.584 | ppl 1.5 | wps 24378 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 28805 | lr 0.000186323 | gnorm 0.829 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 77519
2022-03-05 09:06:31 | INFO | fairseq.trainer | begin training epoch 300
2022-03-05 09:06:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:10:36 | INFO | train_inner | epoch 300:     95 / 97 loss=1.171, nll_loss=0.583, ppl=1.5, wps=24658.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28900, lr=0.000186016, gnorm=0.823, loss_scale=32, train_wall=235, gb_free=21, wall=77764
2022-03-05 09:10:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:10:46 | INFO | valid | epoch 300 | valid on 'valid' subset | loss 13.973 | nll_loss 13.706 | ppl 13359.6 | wps 43805.2 | wpb 510.9 | bsz 1 | num_updates 28902 | best_loss 7.701
2022-03-05 09:10:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 300 @ 28902 updates
2022-03-05 09:10:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:10:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:10:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 300 @ 28902 updates, score 13.973) (writing took 2.655516413040459 seconds)
2022-03-05 09:10:49 | INFO | fairseq_cli.train | end of epoch 300 (average epoch stats below)
2022-03-05 09:10:49 | INFO | train | epoch 300 | loss 1.171 | nll_loss 0.583 | ppl 1.5 | wps 24627.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28902 | lr 0.00018601 | gnorm 0.822 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 77777
2022-03-05 09:10:49 | INFO | fairseq.trainer | begin training epoch 301
2022-03-05 09:10:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:12:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:14:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:15:04 | INFO | valid | epoch 301 | valid on 'valid' subset | loss 13.946 | nll_loss 13.675 | ppl 13077.6 | wps 43775.6 | wpb 510.9 | bsz 1 | num_updates 28998 | best_loss 7.701
2022-03-05 09:15:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 301 @ 28998 updates
2022-03-05 09:15:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:15:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:15:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 301 @ 28998 updates, score 13.946) (writing took 2.662923994474113 seconds)
2022-03-05 09:15:07 | INFO | fairseq_cli.train | end of epoch 301 (average epoch stats below)
2022-03-05 09:15:07 | INFO | train | epoch 301 | loss 1.17 | nll_loss 0.582 | ppl 1.5 | wps 24365.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 28998 | lr 0.000185702 | gnorm 0.819 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 78035
2022-03-05 09:15:07 | INFO | fairseq.trainer | begin training epoch 302
2022-03-05 09:15:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:15:12 | INFO | train_inner | epoch 302:      2 / 97 loss=1.17, nll_loss=0.582, ppl=1.5, wps=23712.8, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=29000, lr=0.000185695, gnorm=0.819, loss_scale=16, train_wall=238, gb_free=21, wall=78040
2022-03-05 09:18:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:19:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:19:22 | INFO | valid | epoch 302 | valid on 'valid' subset | loss 14 | nll_loss 13.729 | ppl 13576.4 | wps 43823.9 | wpb 510.9 | bsz 1 | num_updates 29094 | best_loss 7.701
2022-03-05 09:19:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 302 @ 29094 updates
2022-03-05 09:19:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:19:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:19:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 302 @ 29094 updates, score 14.0) (writing took 2.583209172822535 seconds)
2022-03-05 09:19:25 | INFO | fairseq_cli.train | end of epoch 302 (average epoch stats below)
2022-03-05 09:19:25 | INFO | train | epoch 302 | loss 1.169 | nll_loss 0.581 | ppl 1.5 | wps 24383.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 29094 | lr 0.000185395 | gnorm 0.833 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 78293
2022-03-05 09:19:25 | INFO | fairseq.trainer | begin training epoch 303
2022-03-05 09:19:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:19:40 | INFO | train_inner | epoch 303:      6 / 97 loss=1.168, nll_loss=0.58, ppl=1.49, wps=24419.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=29100, lr=0.000185376, gnorm=0.833, loss_scale=16, train_wall=238, gb_free=21, wall=78308
2022-03-05 09:23:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:23:40 | INFO | valid | epoch 303 | valid on 'valid' subset | loss 13.986 | nll_loss 13.717 | ppl 13464.5 | wps 43915.9 | wpb 510.9 | bsz 1 | num_updates 29191 | best_loss 7.701
2022-03-05 09:23:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 303 @ 29191 updates
2022-03-05 09:23:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:23:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:23:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 303 @ 29191 updates, score 13.986) (writing took 2.628110290504992 seconds)
2022-03-05 09:23:43 | INFO | fairseq_cli.train | end of epoch 303 (average epoch stats below)
2022-03-05 09:23:43 | INFO | train | epoch 303 | loss 1.167 | nll_loss 0.579 | ppl 1.49 | wps 24628.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29191 | lr 0.000185087 | gnorm 0.828 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 78551
2022-03-05 09:23:43 | INFO | fairseq.trainer | begin training epoch 304
2022-03-05 09:23:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:24:06 | INFO | train_inner | epoch 304:      9 / 97 loss=1.167, nll_loss=0.579, ppl=1.49, wps=24654, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29200, lr=0.000185058, gnorm=0.827, loss_scale=16, train_wall=235, gb_free=21, wall=78574
2022-03-05 09:26:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:27:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:27:58 | INFO | valid | epoch 304 | valid on 'valid' subset | loss 13.965 | nll_loss 13.696 | ppl 13270 | wps 43879.3 | wpb 510.9 | bsz 1 | num_updates 29287 | best_loss 7.701
2022-03-05 09:27:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 304 @ 29287 updates
2022-03-05 09:27:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:28:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:28:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 304 @ 29287 updates, score 13.965) (writing took 2.67913684155792 seconds)
2022-03-05 09:28:01 | INFO | fairseq_cli.train | end of epoch 304 (average epoch stats below)
2022-03-05 09:28:01 | INFO | train | epoch 304 | loss 1.166 | nll_loss 0.578 | ppl 1.49 | wps 24365.4 | ups 0.37 | wpb 65493.3 | bsz 127.9 | num_updates 29287 | lr 0.000184783 | gnorm 0.817 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 78809
2022-03-05 09:28:01 | INFO | fairseq.trainer | begin training epoch 305
2022-03-05 09:28:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:28:34 | INFO | train_inner | epoch 305:     13 / 97 loss=1.165, nll_loss=0.577, ppl=1.49, wps=24406.9, ups=0.37, wpb=65495, bsz=127.9, num_updates=29300, lr=0.000184742, gnorm=0.818, loss_scale=16, train_wall=238, gb_free=21, wall=78842
2022-03-05 09:32:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:32:16 | INFO | valid | epoch 305 | valid on 'valid' subset | loss 13.984 | nll_loss 13.715 | ppl 13450.4 | wps 43829.7 | wpb 510.9 | bsz 1 | num_updates 29384 | best_loss 7.701
2022-03-05 09:32:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 305 @ 29384 updates
2022-03-05 09:32:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:32:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:32:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 305 @ 29384 updates, score 13.984) (writing took 2.547811359167099 seconds)
2022-03-05 09:32:19 | INFO | fairseq_cli.train | end of epoch 305 (average epoch stats below)
2022-03-05 09:32:19 | INFO | train | epoch 305 | loss 1.164 | nll_loss 0.576 | ppl 1.49 | wps 24632.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29384 | lr 0.000184478 | gnorm 0.82 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 79067
2022-03-05 09:32:19 | INFO | fairseq.trainer | begin training epoch 306
2022-03-05 09:32:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:33:00 | INFO | train_inner | epoch 306:     16 / 97 loss=1.164, nll_loss=0.576, ppl=1.49, wps=24653.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29400, lr=0.000184428, gnorm=0.819, loss_scale=32, train_wall=235, gb_free=21, wall=79108
2022-03-05 09:33:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:36:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:36:34 | INFO | valid | epoch 306 | valid on 'valid' subset | loss 14.015 | nll_loss 13.745 | ppl 13730.3 | wps 43875.6 | wpb 510.9 | bsz 1 | num_updates 29480 | best_loss 7.701
2022-03-05 09:36:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 306 @ 29480 updates
2022-03-05 09:36:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:36:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:36:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 306 @ 29480 updates, score 14.015) (writing took 2.630352910608053 seconds)
2022-03-05 09:36:37 | INFO | fairseq_cli.train | end of epoch 306 (average epoch stats below)
2022-03-05 09:36:37 | INFO | train | epoch 306 | loss 1.163 | nll_loss 0.575 | ppl 1.49 | wps 24377.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 29480 | lr 0.000184177 | gnorm 0.819 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 79324
2022-03-05 09:36:37 | INFO | fairseq.trainer | begin training epoch 307
2022-03-05 09:36:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:37:28 | INFO | train_inner | epoch 307:     20 / 97 loss=1.161, nll_loss=0.574, ppl=1.49, wps=24415.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=29500, lr=0.000184115, gnorm=0.819, loss_scale=16, train_wall=238, gb_free=21, wall=79376
2022-03-05 09:40:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:40:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:40:52 | INFO | valid | epoch 307 | valid on 'valid' subset | loss 13.996 | nll_loss 13.726 | ppl 13553.2 | wps 44053.1 | wpb 510.9 | bsz 1 | num_updates 29576 | best_loss 7.701
2022-03-05 09:40:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 307 @ 29576 updates
2022-03-05 09:40:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:40:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:40:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 307 @ 29576 updates, score 13.996) (writing took 2.690454540774226 seconds)
2022-03-05 09:40:55 | INFO | fairseq_cli.train | end of epoch 307 (average epoch stats below)
2022-03-05 09:40:55 | INFO | train | epoch 307 | loss 1.162 | nll_loss 0.574 | ppl 1.49 | wps 24363.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 29576 | lr 0.000183878 | gnorm 0.816 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 79583
2022-03-05 09:40:55 | INFO | fairseq.trainer | begin training epoch 308
2022-03-05 09:40:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:41:57 | INFO | train_inner | epoch 308:     24 / 97 loss=1.162, nll_loss=0.575, ppl=1.49, wps=24409.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=29600, lr=0.000183804, gnorm=0.814, loss_scale=16, train_wall=238, gb_free=21, wall=79644
2022-03-05 09:45:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:45:10 | INFO | valid | epoch 308 | valid on 'valid' subset | loss 14.029 | nll_loss 13.76 | ppl 13868.9 | wps 43772.6 | wpb 510.9 | bsz 1 | num_updates 29673 | best_loss 7.701
2022-03-05 09:45:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 308 @ 29673 updates
2022-03-05 09:45:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:45:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:45:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 308 @ 29673 updates, score 14.029) (writing took 2.5584355033934116 seconds)
2022-03-05 09:45:12 | INFO | fairseq_cli.train | end of epoch 308 (average epoch stats below)
2022-03-05 09:45:12 | INFO | train | epoch 308 | loss 1.161 | nll_loss 0.574 | ppl 1.49 | wps 24638.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29673 | lr 0.000183577 | gnorm 0.822 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 79840
2022-03-05 09:45:12 | INFO | fairseq.trainer | begin training epoch 309
2022-03-05 09:45:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:46:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:46:25 | INFO | train_inner | epoch 309:     28 / 97 loss=1.16, nll_loss=0.572, ppl=1.49, wps=24425, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=29700, lr=0.000183494, gnorm=0.821, loss_scale=16, train_wall=238, gb_free=21, wall=79913
2022-03-05 09:49:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:49:28 | INFO | valid | epoch 309 | valid on 'valid' subset | loss 14.022 | nll_loss 13.754 | ppl 13811.2 | wps 44209.3 | wpb 510.9 | bsz 1 | num_updates 29769 | best_loss 7.701
2022-03-05 09:49:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 309 @ 29769 updates
2022-03-05 09:49:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:49:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:49:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 309 @ 29769 updates, score 14.022) (writing took 2.6007101638242602 seconds)
2022-03-05 09:49:30 | INFO | fairseq_cli.train | end of epoch 309 (average epoch stats below)
2022-03-05 09:49:30 | INFO | train | epoch 309 | loss 1.158 | nll_loss 0.571 | ppl 1.49 | wps 24384.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 29769 | lr 0.000183281 | gnorm 0.811 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 80098
2022-03-05 09:49:30 | INFO | fairseq.trainer | begin training epoch 310
2022-03-05 09:49:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:50:50 | INFO | train_inner | epoch 310:     31 / 97 loss=1.157, nll_loss=0.569, ppl=1.48, wps=24650.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29800, lr=0.000183186, gnorm=0.81, loss_scale=16, train_wall=235, gb_free=21, wall=80178
2022-03-05 09:52:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:53:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:53:46 | INFO | valid | epoch 310 | valid on 'valid' subset | loss 14.013 | nll_loss 13.744 | ppl 13718 | wps 43855.2 | wpb 510.9 | bsz 1 | num_updates 29865 | best_loss 7.701
2022-03-05 09:53:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 310 @ 29865 updates
2022-03-05 09:53:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:53:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:53:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 310 @ 29865 updates, score 14.013) (writing took 2.6628083027899265 seconds)
2022-03-05 09:53:48 | INFO | fairseq_cli.train | end of epoch 310 (average epoch stats below)
2022-03-05 09:53:48 | INFO | train | epoch 310 | loss 1.158 | nll_loss 0.571 | ppl 1.49 | wps 24368.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 29865 | lr 0.000182986 | gnorm 0.818 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 80356
2022-03-05 09:53:48 | INFO | fairseq.trainer | begin training epoch 311
2022-03-05 09:53:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:55:19 | INFO | train_inner | epoch 311:     35 / 97 loss=1.157, nll_loss=0.57, ppl=1.48, wps=24411.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=29900, lr=0.000182879, gnorm=0.817, loss_scale=16, train_wall=238, gb_free=21, wall=80447
2022-03-05 09:57:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:58:04 | INFO | valid | epoch 311 | valid on 'valid' subset | loss 13.998 | nll_loss 13.73 | ppl 13587.3 | wps 43877.5 | wpb 510.9 | bsz 1 | num_updates 29962 | best_loss 7.701
2022-03-05 09:58:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 311 @ 29962 updates
2022-03-05 09:58:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:58:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 09:58:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 311 @ 29962 updates, score 13.998) (writing took 3.009031551890075 seconds)
2022-03-05 09:58:07 | INFO | fairseq_cli.train | end of epoch 311 (average epoch stats below)
2022-03-05 09:58:07 | INFO | train | epoch 311 | loss 1.156 | nll_loss 0.569 | ppl 1.48 | wps 24586.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29962 | lr 0.00018269 | gnorm 0.818 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 80615
2022-03-05 09:58:07 | INFO | fairseq.trainer | begin training epoch 312
2022-03-05 09:58:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:58:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:59:47 | INFO | train_inner | epoch 312:     39 / 97 loss=1.156, nll_loss=0.569, ppl=1.48, wps=24369.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=30000, lr=0.000182574, gnorm=0.82, loss_scale=16, train_wall=238, gb_free=21, wall=80715
2022-03-05 10:02:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:02:22 | INFO | valid | epoch 312 | valid on 'valid' subset | loss 13.976 | nll_loss 13.706 | ppl 13364.2 | wps 43747.1 | wpb 510.9 | bsz 1 | num_updates 30058 | best_loss 7.701
2022-03-05 10:02:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 312 @ 30058 updates
2022-03-05 10:02:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:02:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:02:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 312 @ 30058 updates, score 13.976) (writing took 2.6098961858078837 seconds)
2022-03-05 10:02:25 | INFO | fairseq_cli.train | end of epoch 312 (average epoch stats below)
2022-03-05 10:02:25 | INFO | train | epoch 312 | loss 1.155 | nll_loss 0.568 | ppl 1.48 | wps 24366.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 30058 | lr 0.000182398 | gnorm 0.818 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 80873
2022-03-05 10:02:25 | INFO | fairseq.trainer | begin training epoch 313
2022-03-05 10:02:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:04:13 | INFO | train_inner | epoch 313:     42 / 97 loss=1.155, nll_loss=0.567, ppl=1.48, wps=24652, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=30100, lr=0.000182271, gnorm=0.817, loss_scale=32, train_wall=235, gb_free=21, wall=80981
2022-03-05 10:05:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:06:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:06:40 | INFO | valid | epoch 313 | valid on 'valid' subset | loss 13.975 | nll_loss 13.705 | ppl 13352.6 | wps 43706 | wpb 510.9 | bsz 1 | num_updates 30154 | best_loss 7.701
2022-03-05 10:06:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 313 @ 30154 updates
2022-03-05 10:06:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:06:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:06:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 313 @ 30154 updates, score 13.975) (writing took 5.019821671769023 seconds)
2022-03-05 10:06:45 | INFO | fairseq_cli.train | end of epoch 313 (average epoch stats below)
2022-03-05 10:06:45 | INFO | train | epoch 313 | loss 1.155 | nll_loss 0.568 | ppl 1.48 | wps 24149 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 30154 | lr 0.000182107 | gnorm 0.819 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 81133
2022-03-05 10:06:45 | INFO | fairseq.trainer | begin training epoch 314
2022-03-05 10:06:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:08:44 | INFO | train_inner | epoch 314:     46 / 97 loss=1.154, nll_loss=0.567, ppl=1.48, wps=24185.3, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=30200, lr=0.000181969, gnorm=0.823, loss_scale=16, train_wall=238, gb_free=21, wall=81252
2022-03-05 10:10:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:11:00 | INFO | valid | epoch 314 | valid on 'valid' subset | loss 13.997 | nll_loss 13.727 | ppl 13559.3 | wps 43909 | wpb 510.9 | bsz 1 | num_updates 30251 | best_loss 7.701
2022-03-05 10:11:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 314 @ 30251 updates
2022-03-05 10:11:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:11:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:11:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 314 @ 30251 updates, score 13.997) (writing took 2.5739899575710297 seconds)
2022-03-05 10:11:03 | INFO | fairseq_cli.train | end of epoch 314 (average epoch stats below)
2022-03-05 10:11:03 | INFO | train | epoch 314 | loss 1.153 | nll_loss 0.565 | ppl 1.48 | wps 24622 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 30251 | lr 0.000181815 | gnorm 0.818 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 81391
2022-03-05 10:11:03 | INFO | fairseq.trainer | begin training epoch 315
2022-03-05 10:11:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:11:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:13:12 | INFO | train_inner | epoch 315:     50 / 97 loss=1.151, nll_loss=0.564, ppl=1.48, wps=24412, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=30300, lr=0.000181668, gnorm=0.81, loss_scale=16, train_wall=238, gb_free=21, wall=81520
2022-03-05 10:15:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:15:18 | INFO | valid | epoch 315 | valid on 'valid' subset | loss 14.025 | nll_loss 13.756 | ppl 13830.3 | wps 44113.2 | wpb 510.9 | bsz 1 | num_updates 30347 | best_loss 7.701
2022-03-05 10:15:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 315 @ 30347 updates
2022-03-05 10:15:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:15:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:15:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 315 @ 30347 updates, score 14.025) (writing took 2.5432972200214863 seconds)
2022-03-05 10:15:21 | INFO | fairseq_cli.train | end of epoch 315 (average epoch stats below)
2022-03-05 10:15:21 | INFO | train | epoch 315 | loss 1.152 | nll_loss 0.565 | ppl 1.48 | wps 24376.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 30347 | lr 0.000181527 | gnorm 0.811 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 81649
2022-03-05 10:15:21 | INFO | fairseq.trainer | begin training epoch 316
2022-03-05 10:15:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:17:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:17:40 | INFO | train_inner | epoch 316:     54 / 97 loss=1.152, nll_loss=0.564, ppl=1.48, wps=24413.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=30400, lr=0.000181369, gnorm=0.817, loss_scale=16, train_wall=238, gb_free=21, wall=81788
2022-03-05 10:19:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:19:36 | INFO | valid | epoch 316 | valid on 'valid' subset | loss 13.999 | nll_loss 13.73 | ppl 13588.5 | wps 43498.3 | wpb 510.9 | bsz 1 | num_updates 30443 | best_loss 7.701
2022-03-05 10:19:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 316 @ 30443 updates
2022-03-05 10:19:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:19:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:19:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 316 @ 30443 updates, score 13.999) (writing took 2.6785719767212868 seconds)
2022-03-05 10:19:39 | INFO | fairseq_cli.train | end of epoch 316 (average epoch stats below)
2022-03-05 10:19:39 | INFO | train | epoch 316 | loss 1.15 | nll_loss 0.563 | ppl 1.48 | wps 24356 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 30443 | lr 0.000181241 | gnorm 0.818 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 81907
2022-03-05 10:19:39 | INFO | fairseq.trainer | begin training epoch 317
2022-03-05 10:19:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:22:06 | INFO | train_inner | epoch 317:     57 / 97 loss=1.15, nll_loss=0.563, ppl=1.48, wps=24638.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=30500, lr=0.000181071, gnorm=0.819, loss_scale=16, train_wall=235, gb_free=21, wall=82054
2022-03-05 10:23:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:23:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:23:54 | INFO | valid | epoch 317 | valid on 'valid' subset | loss 14.029 | nll_loss 13.762 | ppl 13894.6 | wps 43897.2 | wpb 510.9 | bsz 1 | num_updates 30539 | best_loss 7.701
2022-03-05 10:23:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 317 @ 30539 updates
2022-03-05 10:23:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:23:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:23:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 317 @ 30539 updates, score 14.029) (writing took 2.6236875308677554 seconds)
2022-03-05 10:23:57 | INFO | fairseq_cli.train | end of epoch 317 (average epoch stats below)
2022-03-05 10:23:57 | INFO | train | epoch 317 | loss 1.15 | nll_loss 0.562 | ppl 1.48 | wps 24374.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 30539 | lr 0.000180956 | gnorm 0.815 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 82165
2022-03-05 10:23:57 | INFO | fairseq.trainer | begin training epoch 318
2022-03-05 10:23:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:26:35 | INFO | train_inner | epoch 318:     61 / 97 loss=1.15, nll_loss=0.562, ppl=1.48, wps=24410.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=30600, lr=0.000180775, gnorm=0.811, loss_scale=16, train_wall=238, gb_free=21, wall=82322
2022-03-05 10:28:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:28:12 | INFO | valid | epoch 318 | valid on 'valid' subset | loss 13.998 | nll_loss 13.728 | ppl 13573 | wps 44007.9 | wpb 510.9 | bsz 1 | num_updates 30636 | best_loss 7.701
2022-03-05 10:28:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 318 @ 30636 updates
2022-03-05 10:28:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:28:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:28:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 318 @ 30636 updates, score 13.998) (writing took 2.5991720613092184 seconds)
2022-03-05 10:28:15 | INFO | fairseq_cli.train | end of epoch 318 (average epoch stats below)
2022-03-05 10:28:15 | INFO | train | epoch 318 | loss 1.148 | nll_loss 0.561 | ppl 1.48 | wps 24627.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 30636 | lr 0.000180669 | gnorm 0.814 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 82423
2022-03-05 10:28:15 | INFO | fairseq.trainer | begin training epoch 319
2022-03-05 10:28:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:30:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:31:03 | INFO | train_inner | epoch 319:     65 / 97 loss=1.146, nll_loss=0.559, ppl=1.47, wps=24413.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=30700, lr=0.000180481, gnorm=0.824, loss_scale=16, train_wall=238, gb_free=21, wall=82591
2022-03-05 10:32:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:32:30 | INFO | valid | epoch 319 | valid on 'valid' subset | loss 14.002 | nll_loss 13.735 | ppl 13638.8 | wps 43727.7 | wpb 510.9 | bsz 1 | num_updates 30732 | best_loss 7.701
2022-03-05 10:32:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 319 @ 30732 updates
2022-03-05 10:32:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:32:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:32:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 319 @ 30732 updates, score 14.002) (writing took 2.677775506861508 seconds)
2022-03-05 10:32:33 | INFO | fairseq_cli.train | end of epoch 319 (average epoch stats below)
2022-03-05 10:32:33 | INFO | train | epoch 319 | loss 1.147 | nll_loss 0.559 | ppl 1.47 | wps 24364.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 30732 | lr 0.000180387 | gnorm 0.819 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 82681
2022-03-05 10:32:33 | INFO | fairseq.trainer | begin training epoch 320
2022-03-05 10:32:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:35:29 | INFO | train_inner | epoch 320:     68 / 97 loss=1.146, nll_loss=0.559, ppl=1.47, wps=24643.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=30800, lr=0.000180187, gnorm=0.803, loss_scale=16, train_wall=235, gb_free=21, wall=82856
2022-03-05 10:36:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:36:48 | INFO | valid | epoch 320 | valid on 'valid' subset | loss 13.975 | nll_loss 13.705 | ppl 13353.9 | wps 43795.7 | wpb 510.9 | bsz 1 | num_updates 30829 | best_loss 7.701
2022-03-05 10:36:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 320 @ 30829 updates
2022-03-05 10:36:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:36:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:36:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 320 @ 30829 updates, score 13.975) (writing took 2.5702706258744 seconds)
2022-03-05 10:36:51 | INFO | fairseq_cli.train | end of epoch 320 (average epoch stats below)
2022-03-05 10:36:51 | INFO | train | epoch 320 | loss 1.145 | nll_loss 0.558 | ppl 1.47 | wps 24634 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 30829 | lr 0.000180103 | gnorm 0.805 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 82939
2022-03-05 10:36:51 | INFO | fairseq.trainer | begin training epoch 321
2022-03-05 10:36:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:38:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:39:57 | INFO | train_inner | epoch 321:     72 / 97 loss=1.146, nll_loss=0.558, ppl=1.47, wps=24407.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=30900, lr=0.000179896, gnorm=0.809, loss_scale=16, train_wall=238, gb_free=21, wall=83125
2022-03-05 10:41:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:41:06 | INFO | valid | epoch 321 | valid on 'valid' subset | loss 14.019 | nll_loss 13.75 | ppl 13778.1 | wps 43822.4 | wpb 510.9 | bsz 1 | num_updates 30925 | best_loss 7.701
2022-03-05 10:41:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 321 @ 30925 updates
2022-03-05 10:41:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:41:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:41:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 321 @ 30925 updates, score 14.019) (writing took 2.633513149805367 seconds)
2022-03-05 10:41:09 | INFO | fairseq_cli.train | end of epoch 321 (average epoch stats below)
2022-03-05 10:41:09 | INFO | train | epoch 321 | loss 1.145 | nll_loss 0.557 | ppl 1.47 | wps 24358.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 30925 | lr 0.000179823 | gnorm 0.808 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 83197
2022-03-05 10:41:09 | INFO | fairseq.trainer | begin training epoch 322
2022-03-05 10:41:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:44:23 | INFO | train_inner | epoch 322:     75 / 97 loss=1.144, nll_loss=0.557, ppl=1.47, wps=24645.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31000, lr=0.000179605, gnorm=0.814, loss_scale=32, train_wall=235, gb_free=21, wall=83391
2022-03-05 10:45:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:45:24 | INFO | valid | epoch 322 | valid on 'valid' subset | loss 14.039 | nll_loss 13.773 | ppl 13997.9 | wps 43887.3 | wpb 510.9 | bsz 1 | num_updates 31022 | best_loss 7.701
2022-03-05 10:45:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 322 @ 31022 updates
2022-03-05 10:45:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:45:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:45:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 322 @ 31022 updates, score 14.039) (writing took 2.6590800685808063 seconds)
2022-03-05 10:45:27 | INFO | fairseq_cli.train | end of epoch 322 (average epoch stats below)
2022-03-05 10:45:27 | INFO | train | epoch 322 | loss 1.144 | nll_loss 0.557 | ppl 1.47 | wps 24629.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 31022 | lr 0.000179542 | gnorm 0.814 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 83455
2022-03-05 10:45:27 | INFO | fairseq.trainer | begin training epoch 323
2022-03-05 10:45:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:46:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:48:51 | INFO | train_inner | epoch 323:     79 / 97 loss=1.142, nll_loss=0.555, ppl=1.47, wps=24415.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=31100, lr=0.000179316, gnorm=0.806, loss_scale=16, train_wall=238, gb_free=21, wall=83659
2022-03-05 10:49:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:49:42 | INFO | valid | epoch 323 | valid on 'valid' subset | loss 13.982 | nll_loss 13.714 | ppl 13441.5 | wps 44028.6 | wpb 510.9 | bsz 1 | num_updates 31118 | best_loss 7.701
2022-03-05 10:49:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 323 @ 31118 updates
2022-03-05 10:49:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:49:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:49:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 323 @ 31118 updates, score 13.982) (writing took 2.588463787920773 seconds)
2022-03-05 10:49:45 | INFO | fairseq_cli.train | end of epoch 323 (average epoch stats below)
2022-03-05 10:49:45 | INFO | train | epoch 323 | loss 1.142 | nll_loss 0.555 | ppl 1.47 | wps 24379.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 31118 | lr 0.000179264 | gnorm 0.805 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 83713
2022-03-05 10:49:45 | INFO | fairseq.trainer | begin training epoch 324
2022-03-05 10:49:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:52:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:53:19 | INFO | train_inner | epoch 324:     83 / 97 loss=1.143, nll_loss=0.556, ppl=1.47, wps=24409, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=31200, lr=0.000179029, gnorm=0.816, loss_scale=16, train_wall=238, gb_free=21, wall=83927
2022-03-05 10:53:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:54:00 | INFO | valid | epoch 324 | valid on 'valid' subset | loss 13.976 | nll_loss 13.709 | ppl 13392.3 | wps 43844.9 | wpb 510.9 | bsz 1 | num_updates 31214 | best_loss 7.701
2022-03-05 10:54:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 324 @ 31214 updates
2022-03-05 10:54:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:54:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:54:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 324 @ 31214 updates, score 13.976) (writing took 2.567401075735688 seconds)
2022-03-05 10:54:03 | INFO | fairseq_cli.train | end of epoch 324 (average epoch stats below)
2022-03-05 10:54:03 | INFO | train | epoch 324 | loss 1.14 | nll_loss 0.553 | ppl 1.47 | wps 24370.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 31214 | lr 0.000178989 | gnorm 0.814 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 83971
2022-03-05 10:54:03 | INFO | fairseq.trainer | begin training epoch 325
2022-03-05 10:54:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:57:45 | INFO | train_inner | epoch 325:     86 / 97 loss=1.139, nll_loss=0.552, ppl=1.47, wps=24651.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31300, lr=0.000178743, gnorm=0.811, loss_scale=16, train_wall=235, gb_free=21, wall=84193
2022-03-05 10:58:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:58:18 | INFO | valid | epoch 325 | valid on 'valid' subset | loss 14.02 | nll_loss 13.757 | ppl 13843.7 | wps 43410.6 | wpb 510.9 | bsz 1 | num_updates 31311 | best_loss 7.701
2022-03-05 10:58:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 325 @ 31311 updates
2022-03-05 10:58:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:58:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 10:58:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 325 @ 31311 updates, score 14.02) (writing took 2.689563600346446 seconds)
2022-03-05 10:58:21 | INFO | fairseq_cli.train | end of epoch 325 (average epoch stats below)
2022-03-05 10:58:21 | INFO | train | epoch 325 | loss 1.14 | nll_loss 0.553 | ppl 1.47 | wps 24615.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 31311 | lr 0.000178711 | gnorm 0.809 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 84229
2022-03-05 10:58:21 | INFO | fairseq.trainer | begin training epoch 326
2022-03-05 10:58:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:59:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:02:13 | INFO | train_inner | epoch 326:     90 / 97 loss=1.139, nll_loss=0.553, ppl=1.47, wps=24403.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=31400, lr=0.000178458, gnorm=0.803, loss_scale=16, train_wall=238, gb_free=21, wall=84461
2022-03-05 11:02:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:02:36 | INFO | valid | epoch 326 | valid on 'valid' subset | loss 13.996 | nll_loss 13.726 | ppl 13553.1 | wps 43889.5 | wpb 510.9 | bsz 1 | num_updates 31407 | best_loss 7.701
2022-03-05 11:02:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 326 @ 31407 updates
2022-03-05 11:02:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:02:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:02:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 326 @ 31407 updates, score 13.996) (writing took 2.453146107494831 seconds)
2022-03-05 11:02:39 | INFO | fairseq_cli.train | end of epoch 326 (average epoch stats below)
2022-03-05 11:02:39 | INFO | train | epoch 326 | loss 1.139 | nll_loss 0.552 | ppl 1.47 | wps 24393.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 31407 | lr 0.000178438 | gnorm 0.804 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 84487
2022-03-05 11:02:39 | INFO | fairseq.trainer | begin training epoch 327
2022-03-05 11:02:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:05:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:06:41 | INFO | train_inner | epoch 327:     94 / 97 loss=1.139, nll_loss=0.552, ppl=1.47, wps=24437.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=31500, lr=0.000178174, gnorm=0.804, loss_scale=16, train_wall=238, gb_free=21, wall=84729
2022-03-05 11:06:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:06:54 | INFO | valid | epoch 327 | valid on 'valid' subset | loss 13.991 | nll_loss 13.723 | ppl 13526.5 | wps 43865 | wpb 510.9 | bsz 1 | num_updates 31503 | best_loss 7.701
2022-03-05 11:06:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 327 @ 31503 updates
2022-03-05 11:06:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:06:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:06:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 327 @ 31503 updates, score 13.991) (writing took 2.389790345914662 seconds)
2022-03-05 11:06:56 | INFO | fairseq_cli.train | end of epoch 327 (average epoch stats below)
2022-03-05 11:06:56 | INFO | train | epoch 327 | loss 1.138 | nll_loss 0.551 | ppl 1.46 | wps 24403.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 31503 | lr 0.000178166 | gnorm 0.804 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 84744
2022-03-05 11:06:56 | INFO | fairseq.trainer | begin training epoch 328
2022-03-05 11:06:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:11:07 | INFO | train_inner | epoch 328:     97 / 97 loss=1.139, nll_loss=0.552, ppl=1.47, wps=24667.8, ups=0.38, wpb=65451.9, bsz=127.8, num_updates=31600, lr=0.000177892, gnorm=0.808, loss_scale=32, train_wall=235, gb_free=21, wall=84995
2022-03-05 11:11:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:11:12 | INFO | valid | epoch 328 | valid on 'valid' subset | loss 13.986 | nll_loss 13.719 | ppl 13486 | wps 43735.8 | wpb 510.9 | bsz 1 | num_updates 31600 | best_loss 7.701
2022-03-05 11:11:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 328 @ 31600 updates
2022-03-05 11:11:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:11:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:11:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 328 @ 31600 updates, score 13.986) (writing took 2.457731085829437 seconds)
2022-03-05 11:11:14 | INFO | fairseq_cli.train | end of epoch 328 (average epoch stats below)
2022-03-05 11:11:14 | INFO | train | epoch 328 | loss 1.138 | nll_loss 0.552 | ppl 1.47 | wps 24640.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 31600 | lr 0.000177892 | gnorm 0.808 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 85002
2022-03-05 11:11:14 | INFO | fairseq.trainer | begin training epoch 329
2022-03-05 11:11:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:13:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:15:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:15:29 | INFO | valid | epoch 329 | valid on 'valid' subset | loss 14.016 | nll_loss 13.751 | ppl 13783.9 | wps 44188.4 | wpb 510.9 | bsz 1 | num_updates 31696 | best_loss 7.701
2022-03-05 11:15:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 329 @ 31696 updates
2022-03-05 11:15:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:15:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:15:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 329 @ 31696 updates, score 14.016) (writing took 2.5317608499899507 seconds)
2022-03-05 11:15:32 | INFO | fairseq_cli.train | end of epoch 329 (average epoch stats below)
2022-03-05 11:15:32 | INFO | train | epoch 329 | loss 1.136 | nll_loss 0.549 | ppl 1.46 | wps 24387.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 31696 | lr 0.000177622 | gnorm 0.809 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 85260
2022-03-05 11:15:32 | INFO | fairseq.trainer | begin training epoch 330
2022-03-05 11:15:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:15:42 | INFO | train_inner | epoch 330:      4 / 97 loss=1.135, nll_loss=0.548, ppl=1.46, wps=23751.1, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=31700, lr=0.000177611, gnorm=0.808, loss_scale=16, train_wall=238, gb_free=21, wall=85270
2022-03-05 11:19:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:19:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:19:47 | INFO | valid | epoch 330 | valid on 'valid' subset | loss 14.012 | nll_loss 13.745 | ppl 13729.6 | wps 43897.7 | wpb 510.9 | bsz 1 | num_updates 31792 | best_loss 7.701
2022-03-05 11:19:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 330 @ 31792 updates
2022-03-05 11:19:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:19:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:19:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 330 @ 31792 updates, score 14.012) (writing took 2.4574099723249674 seconds)
2022-03-05 11:19:50 | INFO | fairseq_cli.train | end of epoch 330 (average epoch stats below)
2022-03-05 11:19:50 | INFO | train | epoch 330 | loss 1.133 | nll_loss 0.547 | ppl 1.46 | wps 24388.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 31792 | lr 0.000177354 | gnorm 0.801 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 85518
2022-03-05 11:19:50 | INFO | fairseq.trainer | begin training epoch 331
2022-03-05 11:19:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:20:10 | INFO | train_inner | epoch 331:      8 / 97 loss=1.133, nll_loss=0.546, ppl=1.46, wps=24430.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=31800, lr=0.000177332, gnorm=0.801, loss_scale=16, train_wall=238, gb_free=21, wall=85538
2022-03-05 11:24:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:24:05 | INFO | valid | epoch 331 | valid on 'valid' subset | loss 14.007 | nll_loss 13.742 | ppl 13697 | wps 43955.3 | wpb 510.9 | bsz 1 | num_updates 31889 | best_loss 7.701
2022-03-05 11:24:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 331 @ 31889 updates
2022-03-05 11:24:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:24:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt
2022-03-05 11:24:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.02_#1/checkpoint_last.pt (epoch 331 @ 31889 updates, score 14.007) (writing took 2.483506552875042 seconds)
2022-03-05 11:24:07 | INFO | fairseq_cli.train | end of epoch 331 (average epoch stats below)
2022-03-05 11:24:07 | INFO | train | epoch 331 | loss 1.133 | nll_loss 0.547 | ppl 1.46 | wps 24646 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 31889 | lr 0.000177084 | gnorm 0.795 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 85775
2022-03-05 11:24:08 | INFO | fairseq.trainer | begin training epoch 332
2022-03-05 11:24:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:24:36 | INFO | train_inner | epoch 332:     11 / 97 loss=1.132, nll_loss=0.546, ppl=1.46, wps=24668.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=31900, lr=0.000177054, gnorm=0.793, loss_scale=16, train_wall=235, gb_free=21, wall=85804
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
