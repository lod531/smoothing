Sender: LSF System <lsfadmin@eu-g3-071>
Subject: Job 207014385: <w103_fp16_size_0.25_jelinek_0.04_0.06_0.9_#4> in cluster <euler> Exited

Job <w103_fp16_size_0.25_jelinek_0.04_0.06_0.9_#4> was submitted from host <eu-login-19> by user <andriusb> in cluster <euler> at Thu Mar  3 10:52:49 2022
Job was executed on host(s) <eu-g3-071>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Thu Mar  3 20:37:39 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Mar  3 20:37:39 2022
Terminated at Sat Mar  5 08:35:25 2022
Results reported at Sat Mar  5 08:35:25 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.04, 0.06, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321664 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   129044.57 sec.
    Max Memory :                                 8373 MB
    Average Memory :                             2941.74 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11627.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   129466 sec.
    Turnaround time :                            164556 sec.

The output (if any) follows:

2022-03-03 20:37:48 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321664, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321664, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.04, 0.06, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-03 20:37:48 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-03 20:37:53 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
Calculating frequency stats:
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 685/450337 [00:00<01:05, 6844.10it/s]  0%|          | 1370/450337 [00:00<01:13, 6123.57it/s]  0%|          | 1988/450337 [00:00<01:16, 5894.59it/s]  1%|          | 2610/450337 [00:00<01:14, 6012.87it/s]  1%|          | 3285/450337 [00:00<01:11, 6264.41it/s]  1%|          | 3968/450337 [00:00<01:09, 6442.85it/s]  1%|          | 4686/450337 [00:00<01:06, 6673.00it/s]  1%|          | 5466/450337 [00:00<01:03, 7016.06it/s]  1%|▏         | 6170/450337 [00:00<01:04, 6918.08it/s]  2%|▏         | 6864/450337 [00:01<01:10, 6308.98it/s]  2%|▏         | 7537/450337 [00:01<01:08, 6426.29it/s]  2%|▏         | 8188/450337 [00:01<01:10, 6243.90it/s]  2%|▏         | 8819/450337 [00:01<01:11, 6184.72it/s]  2%|▏         | 9461/450337 [00:01<01:10, 6251.83it/s]  2%|▏         | 10128/450337 [00:01<01:09, 6370.26it/s]  2%|▏         | 10768/450337 [00:01<01:10, 6269.16it/s]  3%|▎         | 11397/450337 [00:01<01:11, 6175.13it/s]  3%|▎         | 12095/450337 [00:01<01:08, 6400.31it/s]  3%|▎         | 12737/450337 [00:02<01:10, 6209.30it/s]  3%|▎         | 13390/450337 [00:02<01:09, 6293.54it/s]  3%|▎         | 14038/450337 [00:02<01:08, 6344.63it/s]  3%|▎         | 14683/450337 [00:02<01:08, 6374.07it/s]  3%|▎         | 15347/450337 [00:02<01:07, 6444.11it/s]  4%|▎         | 15993/450337 [00:02<01:10, 6169.30it/s]  4%|▎         | 16613/450337 [00:02<01:11, 6101.42it/s]  4%|▍         | 17275/450337 [00:02<01:09, 6250.42it/s]  4%|▍         | 17923/450337 [00:02<01:08, 6315.80it/s]  4%|▍         | 18557/450337 [00:02<01:10, 6127.26it/s]  4%|▍         | 19424/450337 [00:03<01:02, 6861.24it/s]  4%|▍         | 20115/450337 [00:03<01:06, 6506.25it/s]  5%|▍         | 20772/450337 [00:03<01:07, 6386.56it/s]  5%|▍         | 21415/450337 [00:03<01:10, 6091.02it/s]  5%|▍         | 22073/450337 [00:03<01:08, 6216.69it/s]  5%|▌         | 22770/450337 [00:03<01:06, 6429.76it/s]  5%|▌         | 23418/450337 [00:03<01:06, 6406.80it/s]  5%|▌         | 24214/450337 [00:03<01:02, 6853.78it/s]  6%|▌         | 24945/450337 [00:03<01:00, 6985.18it/s]  6%|▌         | 25647/450337 [00:03<01:01, 6865.38it/s]  6%|▌         | 26336/450337 [00:04<01:06, 6414.94it/s]  6%|▌         | 26985/450337 [00:04<01:07, 6283.85it/s]  6%|▌         | 27619/450337 [00:04<01:09, 6092.39it/s]  6%|▋         | 28288/450337 [00:04<01:07, 6256.71it/s]  6%|▋         | 28955/450337 [00:04<01:06, 6369.31it/s]  7%|▋         | 29596/450337 [00:04<01:06, 6327.75it/s]  7%|▋         | 30231/450337 [00:04<01:06, 6324.93it/s]  7%|▋         | 30865/450337 [00:04<01:09, 5993.55it/s]  7%|▋         | 31515/450337 [00:04<01:08, 6135.61it/s]  7%|▋         | 32133/450337 [00:05<01:10, 5929.51it/s]  7%|▋         | 32765/450337 [00:05<01:09, 6033.19it/s]  7%|▋         | 33372/450337 [00:05<01:10, 5878.20it/s]  8%|▊         | 34034/450337 [00:05<01:08, 6086.38it/s]  8%|▊         | 34742/450337 [00:05<01:05, 6372.59it/s]  8%|▊         | 35383/450337 [00:05<01:07, 6166.04it/s]  8%|▊         | 36003/450337 [00:05<01:07, 6138.27it/s]  8%|▊         | 36645/450337 [00:05<01:06, 6215.94it/s]  8%|▊         | 37269/450337 [00:05<01:09, 5940.25it/s]  8%|▊         | 37867/450337 [00:06<01:10, 5872.90it/s]  9%|▊         | 38474/450337 [00:06<01:09, 5924.29it/s]  9%|▊         | 39086/450337 [00:06<01:08, 5980.39it/s]  9%|▉         | 39691/450337 [00:06<01:08, 5999.68it/s]  9%|▉         | 40389/450337 [00:06<01:05, 6285.80it/s]  9%|▉         | 41032/450337 [00:06<01:04, 6325.08it/s]  9%|▉         | 41666/450337 [00:06<01:05, 6196.37it/s]  9%|▉         | 42287/450337 [00:06<01:08, 5933.73it/s] 10%|▉         | 42884/450337 [00:06<01:08, 5924.72it/s] 10%|▉         | 43479/450337 [00:06<01:08, 5923.09it/s] 10%|▉         | 44133/450337 [00:07<01:06, 6103.23it/s] 10%|▉         | 44809/450337 [00:07<01:04, 6296.63it/s] 10%|█         | 45451/450337 [00:07<01:03, 6332.98it/s] 10%|█         | 46158/450337 [00:07<01:01, 6539.43it/s] 10%|█         | 46813/450337 [00:07<01:02, 6465.52it/s] 11%|█         | 47826/450337 [00:07<00:53, 7535.85it/s] 11%|█         | 48582/450337 [00:07<00:55, 7277.96it/s] 11%|█         | 49329/450337 [00:07<00:54, 7328.27it/s] 11%|█         | 50064/450337 [00:07<00:57, 6963.29it/s] 11%|█▏        | 50765/450337 [00:08<01:01, 6522.14it/s] 11%|█▏        | 51425/450337 [00:08<01:01, 6492.63it/s] 12%|█▏        | 52155/450337 [00:08<00:59, 6718.99it/s] 12%|█▏        | 52950/450337 [00:08<00:56, 7067.03it/s] 12%|█▏        | 53662/450337 [00:08<00:59, 6711.04it/s] 12%|█▏        | 54340/450337 [00:08<01:02, 6334.42it/s] 12%|█▏        | 54981/450337 [00:08<01:03, 6237.08it/s] 12%|█▏        | 55727/450337 [00:08<00:59, 6576.89it/s] 13%|█▎        | 56391/450337 [00:08<01:02, 6336.08it/s] 13%|█▎        | 57030/450337 [00:08<01:05, 6017.63it/s] 13%|█▎        | 57638/450337 [00:09<01:05, 5990.45it/s] 13%|█▎        | 58300/450337 [00:09<01:03, 6158.21it/s] 13%|█▎        | 59027/450337 [00:09<01:00, 6475.58it/s] 13%|█▎        | 59679/450337 [00:09<01:01, 6309.78it/s] 13%|█▎        | 60314/450337 [00:09<01:02, 6213.85it/s] 14%|█▎        | 61056/450337 [00:09<00:59, 6558.46it/s] 14%|█▎        | 61720/450337 [00:09<00:59, 6581.11it/s] 14%|█▍        | 62381/450337 [00:09<00:59, 6517.37it/s] 14%|█▍        | 63035/450337 [00:09<01:01, 6340.52it/s] 14%|█▍        | 63696/450337 [00:10<01:00, 6412.25it/s] 14%|█▍        | 64339/450337 [00:10<01:01, 6278.05it/s] 14%|█▍        | 64992/450337 [00:10<01:00, 6342.15it/s] 15%|█▍        | 65628/450337 [00:10<01:01, 6237.38it/s] 15%|█▍        | 66253/450337 [00:10<01:02, 6153.72it/s] 15%|█▍        | 66969/450337 [00:10<00:59, 6442.84it/s] 15%|█▌        | 67615/450337 [00:10<01:02, 6126.64it/s] 15%|█▌        | 68232/450337 [00:10<01:03, 5976.18it/s] 15%|█▌        | 68996/450337 [00:10<00:59, 6448.10it/s] 15%|█▌        | 69646/450337 [00:10<00:58, 6455.69it/s] 16%|█▌        | 70298/450337 [00:11<00:58, 6473.36it/s] 16%|█▌        | 70948/450337 [00:11<01:00, 6308.69it/s] 16%|█▌        | 71609/450337 [00:11<00:59, 6392.18it/s] 16%|█▌        | 72251/450337 [00:11<00:59, 6395.92it/s] 16%|█▌        | 73022/450337 [00:11<00:55, 6782.07it/s] 16%|█▋        | 73738/450337 [00:11<00:54, 6889.03it/s] 17%|█▋        | 74560/450337 [00:11<00:51, 7282.19it/s] 17%|█▋        | 75290/450337 [00:11<00:54, 6933.02it/s] 17%|█▋        | 75988/450337 [00:11<00:55, 6770.09it/s] 17%|█▋        | 76669/450337 [00:12<00:55, 6749.33it/s] 17%|█▋        | 77347/450337 [00:12<00:56, 6621.03it/s] 17%|█▋        | 78026/450337 [00:12<00:55, 6667.31it/s] 17%|█▋        | 78695/450337 [00:12<00:59, 6283.75it/s] 18%|█▊        | 79329/450337 [00:12<01:03, 5879.07it/s] 18%|█▊        | 79962/450337 [00:12<01:01, 5995.66it/s] 18%|█▊        | 80568/450337 [00:12<01:02, 5954.05it/s] 18%|█▊        | 81326/450337 [00:12<00:57, 6413.89it/s] 18%|█▊        | 81973/450337 [00:12<00:58, 6249.34it/s] 18%|█▊        | 82628/450337 [00:12<00:58, 6334.03it/s] 19%|█▊        | 83335/450337 [00:13<00:56, 6542.29it/s] 19%|█▊        | 84019/450337 [00:13<00:55, 6621.37it/s] 19%|█▉        | 84684/450337 [00:13<00:56, 6524.67it/s] 19%|█▉        | 85339/450337 [00:13<00:59, 6170.17it/s] 19%|█▉        | 86040/450337 [00:13<00:56, 6402.27it/s] 19%|█▉        | 86685/450337 [00:13<00:56, 6401.38it/s] 19%|█▉        | 87385/450337 [00:13<00:55, 6573.75it/s] 20%|█▉        | 88081/450337 [00:13<00:54, 6680.91it/s] 20%|█▉        | 88752/450337 [00:13<00:58, 6192.50it/s] 20%|█▉        | 89380/450337 [00:14<00:59, 6099.35it/s] 20%|█▉        | 89996/450337 [00:14<00:59, 6044.32it/s] 20%|██        | 90605/450337 [00:14<00:59, 6037.96it/s] 20%|██        | 91300/450337 [00:14<00:57, 6298.75it/s] 20%|██        | 91933/450337 [00:14<00:56, 6292.07it/s] 21%|██        | 92640/450337 [00:14<00:54, 6510.91it/s] 21%|██        | 93293/450337 [00:14<00:57, 6247.77it/s] 21%|██        | 93930/450337 [00:14<00:56, 6279.35it/s] 21%|██        | 94623/450337 [00:14<00:54, 6468.13it/s] 21%|██        | 95273/450337 [00:14<00:54, 6476.28it/s] 21%|██▏       | 95923/450337 [00:15<00:55, 6378.46it/s] 21%|██▏       | 96799/450337 [00:15<00:49, 7077.09it/s] 22%|██▏       | 97510/450337 [00:15<00:52, 6708.96it/s] 22%|██▏       | 98187/450337 [00:15<00:53, 6527.43it/s] 22%|██▏       | 98844/450337 [00:15<00:55, 6351.79it/s] 22%|██▏       | 99483/450337 [00:15<00:56, 6173.37it/s] 22%|██▏       | 100103/450337 [00:15<00:56, 6178.04it/s] 22%|██▏       | 100723/450337 [00:15<00:57, 6068.21it/s] 23%|██▎       | 101383/450337 [00:15<00:56, 6219.35it/s] 23%|██▎       | 102088/450337 [00:16<00:53, 6457.33it/s] 23%|██▎       | 102736/450337 [00:16<00:54, 6362.32it/s] 23%|██▎       | 103374/450337 [00:16<00:55, 6289.22it/s] 23%|██▎       | 104004/450337 [00:16<00:57, 6043.13it/s] 23%|██▎       | 104666/450337 [00:16<00:55, 6201.68it/s] 23%|██▎       | 105289/450337 [00:16<00:56, 6140.26it/s] 24%|██▎       | 105974/450337 [00:16<00:54, 6341.02it/s] 24%|██▎       | 106610/450337 [00:16<00:54, 6260.13it/s] 24%|██▍       | 107238/450337 [00:16<00:56, 6094.38it/s] 24%|██▍       | 107872/450337 [00:16<00:55, 6157.73it/s] 24%|██▍       | 108503/450337 [00:17<00:55, 6201.43it/s] 24%|██▍       | 109125/450337 [00:17<00:56, 5998.59it/s] 24%|██▍       | 109730/450337 [00:17<00:56, 6012.74it/s] 25%|██▍       | 110396/450337 [00:17<00:54, 6196.96it/s] 25%|██▍       | 111144/450337 [00:17<00:51, 6567.74it/s] 25%|██▍       | 111803/450337 [00:17<00:53, 6326.65it/s] 25%|██▍       | 112467/450337 [00:17<00:52, 6410.55it/s] 25%|██▌       | 113111/450337 [00:17<00:53, 6314.77it/s] 25%|██▌       | 113745/450337 [00:17<00:55, 6099.88it/s] 25%|██▌       | 114387/450337 [00:18<00:54, 6187.18it/s] 26%|██▌       | 115031/450337 [00:18<00:53, 6255.78it/s] 26%|██▌       | 115659/450337 [00:18<00:57, 5803.91it/s] 26%|██▌       | 116303/450337 [00:18<00:55, 5981.34it/s] 26%|██▌       | 116908/450337 [00:18<00:56, 5925.55it/s] 26%|██▌       | 117522/450337 [00:18<00:55, 5981.30it/s] 26%|██▌       | 118124/450337 [00:18<00:55, 5973.58it/s] 26%|██▋       | 118794/450337 [00:18<00:53, 6185.93it/s] 27%|██▋       | 119415/450337 [00:18<00:54, 6098.25it/s] 27%|██▋       | 120152/450337 [00:18<00:51, 6470.75it/s] 27%|██▋       | 120825/450337 [00:19<00:50, 6540.79it/s] 27%|██▋       | 121481/450337 [00:19<00:52, 6257.39it/s] 27%|██▋       | 122111/450337 [00:19<00:52, 6218.92it/s] 27%|██▋       | 122776/450337 [00:19<00:51, 6341.84it/s] 27%|██▋       | 123413/450337 [00:19<00:54, 6052.19it/s] 28%|██▊       | 124063/450337 [00:19<00:52, 6167.66it/s] 28%|██▊       | 124683/450337 [00:19<00:53, 6089.20it/s] 28%|██▊       | 125326/450337 [00:19<00:52, 6183.20it/s] 28%|██▊       | 126015/450337 [00:19<00:50, 6385.23it/s] 28%|██▊       | 126716/450337 [00:19<00:49, 6567.33it/s] 28%|██▊       | 127375/450337 [00:20<00:50, 6350.06it/s] 28%|██▊       | 128062/450337 [00:20<00:49, 6495.16it/s] 29%|██▊       | 128714/450337 [00:20<00:51, 6246.54it/s] 29%|██▊       | 129395/450337 [00:20<00:50, 6407.89it/s] 29%|██▉       | 130039/450337 [00:20<00:52, 6154.48it/s] 29%|██▉       | 130659/450337 [00:20<00:52, 6111.61it/s] 29%|██▉       | 131276/450337 [00:20<00:52, 6123.25it/s] 29%|██▉       | 131891/450337 [00:20<00:57, 5504.41it/s] 29%|██▉       | 132615/450337 [00:20<00:53, 5972.09it/s] 30%|██▉       | 133322/450337 [00:21<00:50, 6276.20it/s] 30%|██▉       | 133979/450337 [00:21<00:49, 6356.28it/s] 30%|██▉       | 134695/450337 [00:21<00:47, 6580.81it/s] 30%|███       | 135476/450337 [00:21<00:45, 6939.61it/s] 30%|███       | 136176/450337 [00:21<00:46, 6747.49it/s] 30%|███       | 136936/450337 [00:21<00:44, 6994.42it/s] 31%|███       | 137640/450337 [00:21<00:45, 6879.83it/s] 31%|███       | 138332/450337 [00:21<00:46, 6673.38it/s] 31%|███       | 139018/450337 [00:21<00:46, 6721.37it/s] 31%|███       | 139693/450337 [00:22<00:47, 6580.04it/s] 31%|███       | 140353/450337 [00:22<00:48, 6343.05it/s] 31%|███▏      | 141021/450337 [00:22<00:48, 6436.25it/s] 31%|███▏      | 141667/450337 [00:22<00:48, 6315.83it/s] 32%|███▏      | 142301/450337 [00:22<00:50, 6154.66it/s] 32%|███▏      | 142919/450337 [00:22<00:50, 6069.17it/s] 32%|███▏      | 143662/450337 [00:22<00:47, 6456.49it/s] 32%|███▏      | 144311/450337 [00:22<00:47, 6422.91it/s] 32%|███▏      | 145075/450337 [00:22<00:45, 6775.25it/s] 32%|███▏      | 145755/450337 [00:22<00:45, 6737.01it/s] 33%|███▎      | 146431/450337 [00:23<00:46, 6557.03it/s] 33%|███▎      | 147089/450337 [00:23<00:48, 6272.54it/s] 33%|███▎      | 147720/450337 [00:23<00:50, 6039.15it/s] 33%|███▎      | 148328/450337 [00:23<00:49, 6047.56it/s] 33%|███▎      | 149002/450337 [00:23<00:48, 6238.24it/s] 33%|███▎      | 149629/450337 [00:23<00:49, 6062.37it/s] 33%|███▎      | 150238/450337 [00:23<00:50, 5996.08it/s] 33%|███▎      | 150853/450337 [00:23<00:49, 6038.18it/s] 34%|███▎      | 151481/450337 [00:23<00:48, 6104.51it/s] 34%|███▍      | 152093/450337 [00:24<00:50, 5916.51it/s] 34%|███▍      | 152762/450337 [00:24<00:48, 6138.28it/s] 34%|███▍      | 153491/450337 [00:24<00:45, 6472.73it/s] 34%|███▍      | 154141/450337 [00:24<00:48, 6108.21it/s] 34%|███▍      | 154758/450337 [00:24<00:48, 6084.39it/s] 35%|███▍      | 155455/450337 [00:24<00:46, 6337.19it/s] 35%|███▍      | 156127/450337 [00:24<00:45, 6442.07it/s] 35%|███▍      | 156775/450337 [00:24<00:45, 6397.25it/s] 35%|███▍      | 157502/450337 [00:24<00:44, 6647.02it/s] 35%|███▌      | 158198/450337 [00:24<00:43, 6738.51it/s] 35%|███▌      | 158874/450337 [00:25<00:44, 6511.71it/s] 35%|███▌      | 159528/450337 [00:25<00:46, 6210.04it/s] 36%|███▌      | 160264/450337 [00:25<00:44, 6530.55it/s] 36%|███▌      | 160922/450337 [00:25<00:44, 6531.06it/s] 36%|███▌      | 161579/450337 [00:25<00:45, 6392.30it/s] 36%|███▌      | 162227/450337 [00:25<00:44, 6413.04it/s] 36%|███▌      | 162892/450337 [00:25<00:44, 6476.89it/s] 36%|███▋      | 163542/450337 [00:25<00:45, 6371.92it/s] 36%|███▋      | 164181/450337 [00:25<00:45, 6253.16it/s] 37%|███▋      | 164870/450337 [00:26<00:44, 6432.74it/s] 37%|███▋      | 165608/450337 [00:26<00:42, 6710.15it/s] 37%|███▋      | 166281/450337 [00:26<00:43, 6471.46it/s] 37%|███▋      | 167021/450337 [00:26<00:42, 6737.82it/s] 37%|███▋      | 167698/450337 [00:26<00:42, 6620.44it/s] 37%|███▋      | 168363/450337 [00:26<00:45, 6252.99it/s] 38%|███▊      | 169044/450337 [00:26<00:43, 6406.47it/s] 38%|███▊      | 169690/450337 [00:26<00:44, 6282.12it/s] 38%|███▊      | 170322/450337 [00:26<00:46, 6026.56it/s] 38%|███▊      | 170956/450337 [00:26<00:45, 6111.30it/s] 38%|███▊      | 171586/450337 [00:27<00:45, 6163.17it/s] 38%|███▊      | 172205/450337 [00:27<00:46, 5960.63it/s] 38%|███▊      | 172870/450337 [00:27<00:45, 6157.56it/s] 39%|███▊      | 173497/450337 [00:27<00:44, 6183.24it/s] 39%|███▊      | 174121/450337 [00:27<00:44, 6193.97it/s] 39%|███▉      | 174836/450337 [00:27<00:42, 6471.16it/s] 39%|███▉      | 175485/450337 [00:27<00:44, 6142.26it/s] 39%|███▉      | 176104/450337 [00:27<00:45, 6008.75it/s] 39%|███▉      | 176734/450337 [00:27<00:44, 6086.54it/s] 39%|███▉      | 177346/450337 [00:28<00:45, 6016.27it/s] 40%|███▉      | 177950/450337 [00:28<00:48, 5672.46it/s] 40%|███▉      | 178604/450337 [00:28<00:46, 5907.21it/s] 40%|███▉      | 179272/450337 [00:28<00:44, 6114.01it/s] 40%|███▉      | 179888/450337 [00:28<00:44, 6087.09it/s] 40%|████      | 180589/450337 [00:28<00:42, 6349.76it/s] 40%|████      | 181227/450337 [00:28<00:44, 6075.58it/s] 40%|████      | 181839/450337 [00:28<00:45, 5894.29it/s] 41%|████      | 182432/450337 [00:28<00:45, 5862.18it/s] 41%|████      | 183095/450337 [00:28<00:43, 6082.22it/s] 41%|████      | 183706/450337 [00:29<00:45, 5812.80it/s] 41%|████      | 184291/450337 [00:29<00:47, 5639.27it/s] 41%|████      | 184940/450337 [00:29<00:45, 5877.83it/s] 41%|████      | 185532/450337 [00:29<00:46, 5719.07it/s] 41%|████▏     | 186149/450337 [00:29<00:45, 5843.18it/s] 41%|████▏     | 186746/450337 [00:29<00:44, 5871.44it/s] 42%|████▏     | 187414/450337 [00:29<00:43, 6104.91it/s] 42%|████▏     | 188027/450337 [00:29<00:43, 6097.98it/s] 42%|████▏     | 188721/450337 [00:29<00:41, 6346.38it/s] 42%|████▏     | 189357/450337 [00:30<00:42, 6145.15it/s] 42%|████▏     | 190018/450337 [00:30<00:41, 6258.64it/s] 42%|████▏     | 190685/450337 [00:30<00:40, 6373.71it/s] 42%|████▏     | 191324/450337 [00:30<00:40, 6351.76it/s] 43%|████▎     | 191961/450337 [00:30<00:41, 6249.62it/s] 43%|████▎     | 192676/450337 [00:30<00:39, 6510.98it/s] 43%|████▎     | 193329/450337 [00:30<00:40, 6381.13it/s] 43%|████▎     | 193975/450337 [00:30<00:40, 6400.71it/s] 43%|████▎     | 194617/450337 [00:30<00:40, 6309.91it/s] 43%|████▎     | 195267/450337 [00:30<00:40, 6361.15it/s] 44%|████▎     | 195953/450337 [00:31<00:39, 6504.84it/s] 44%|████▎     | 196605/450337 [00:31<00:40, 6198.01it/s] 44%|████▍     | 197512/450337 [00:31<00:36, 7019.22it/s] 44%|████▍     | 198220/450337 [00:31<00:38, 6619.55it/s] 44%|████▍     | 198890/450337 [00:31<00:38, 6537.97it/s] 44%|████▍     | 199550/450337 [00:31<00:38, 6476.36it/s] 44%|████▍     | 200202/450337 [00:31<00:39, 6348.85it/s] 45%|████▍     | 200840/450337 [00:31<00:40, 6110.58it/s] 45%|████▍     | 201454/450337 [00:31<00:41, 6049.65it/s] 45%|████▍     | 202110/450337 [00:32<00:40, 6194.72it/s] 45%|████▌     | 202732/450337 [00:32<00:40, 6151.35it/s] 45%|████▌     | 203453/450337 [00:32<00:38, 6457.31it/s] 45%|████▌     | 204113/450337 [00:32<00:37, 6495.19it/s] 45%|████▌     | 204764/450337 [00:32<00:39, 6189.40it/s] 46%|████▌     | 205394/450337 [00:32<00:39, 6217.54it/s] 46%|████▌     | 206019/450337 [00:32<00:39, 6139.30it/s] 46%|████▌     | 206661/450337 [00:32<00:39, 6212.06it/s] 46%|████▌     | 207284/450337 [00:32<00:40, 6013.02it/s] 46%|████▌     | 207901/450337 [00:32<00:40, 6049.96it/s] 46%|████▋     | 208508/450337 [00:33<00:40, 6009.27it/s] 46%|████▋     | 209111/450337 [00:33<00:41, 5774.13it/s] 47%|████▋     | 209787/450337 [00:33<00:39, 6052.71it/s] 47%|████▋     | 210503/450337 [00:33<00:37, 6370.18it/s] 47%|████▋     | 211144/450337 [00:33<00:37, 6372.97it/s] 47%|████▋     | 211784/450337 [00:33<00:37, 6278.77it/s] 47%|████▋     | 212432/450337 [00:33<00:37, 6336.63it/s] 47%|████▋     | 213166/450337 [00:33<00:35, 6628.30it/s] 47%|████▋     | 213831/450337 [00:33<00:38, 6111.35it/s] 48%|████▊     | 214451/450337 [00:34<00:38, 6072.51it/s] 48%|████▊     | 215065/450337 [00:34<00:39, 5989.24it/s] 48%|████▊     | 215668/450337 [00:34<00:39, 5868.09it/s] 48%|████▊     | 216317/450337 [00:34<00:38, 6043.65it/s] 48%|████▊     | 216925/450337 [00:34<00:38, 6037.36it/s] 48%|████▊     | 217558/450337 [00:34<00:38, 6118.72it/s] 48%|████▊     | 218243/450337 [00:34<00:36, 6327.18it/s] 49%|████▊     | 218878/450337 [00:34<00:36, 6289.14it/s] 49%|████▊     | 219536/450337 [00:34<00:36, 6363.12it/s] 49%|████▉     | 220174/450337 [00:34<00:36, 6294.04it/s] 49%|████▉     | 220805/450337 [00:35<00:37, 6154.03it/s] 49%|████▉     | 221479/450337 [00:35<00:36, 6322.59it/s] 49%|████▉     | 222113/450337 [00:35<00:37, 6064.89it/s] 49%|████▉     | 222723/450337 [00:35<00:38, 5959.67it/s] 50%|████▉     | 223321/450337 [00:35<00:39, 5811.22it/s] 50%|████▉     | 223924/450337 [00:35<00:38, 5868.07it/s] 50%|████▉     | 224640/450337 [00:35<00:36, 6232.95it/s] 50%|█████     | 225315/450337 [00:35<00:35, 6382.88it/s] 50%|█████     | 226113/450337 [00:35<00:32, 6850.31it/s] 50%|█████     | 226801/450337 [00:35<00:34, 6526.46it/s] 51%|█████     | 227459/450337 [00:36<00:34, 6388.09it/s] 51%|█████     | 228102/450337 [00:36<00:35, 6214.23it/s] 51%|█████     | 228727/450337 [00:36<00:36, 6080.76it/s] 51%|█████     | 229337/450337 [00:36<00:37, 5940.69it/s] 51%|█████     | 229933/450337 [00:36<00:38, 5692.54it/s] 51%|█████     | 230652/450337 [00:36<00:35, 6108.10it/s] 51%|█████▏    | 231268/450337 [00:36<00:36, 6070.57it/s] 51%|█████▏    | 231879/450337 [00:36<00:37, 5895.26it/s] 52%|█████▏    | 232507/450337 [00:36<00:36, 5998.69it/s] 52%|█████▏    | 233299/450337 [00:37<00:33, 6552.63it/s] 52%|█████▏    | 233958/450337 [00:37<00:33, 6541.08it/s] 52%|█████▏    | 234645/450337 [00:37<00:32, 6637.55it/s] 52%|█████▏    | 235339/450337 [00:37<00:31, 6724.66it/s] 52%|█████▏    | 236013/450337 [00:37<00:32, 6554.05it/s] 53%|█████▎    | 236671/450337 [00:37<00:34, 6174.33it/s] 53%|█████▎    | 237295/450337 [00:37<00:34, 6185.95it/s] 53%|█████▎    | 237918/450337 [00:37<00:35, 6012.14it/s] 53%|█████▎    | 238710/450337 [00:37<00:32, 6545.95it/s] 53%|█████▎    | 239370/450337 [00:37<00:32, 6545.75it/s] 53%|█████▎    | 240028/450337 [00:38<00:34, 6174.24it/s] 53%|█████▎    | 240851/450337 [00:38<00:31, 6752.59it/s] 54%|█████▎    | 241535/450337 [00:38<00:31, 6631.08it/s] 54%|█████▍    | 242206/450337 [00:38<00:31, 6648.90it/s] 54%|█████▍    | 242875/450337 [00:38<00:31, 6542.14it/s] 54%|█████▍    | 243563/450337 [00:38<00:31, 6627.00it/s] 54%|█████▍    | 244229/450337 [00:38<00:32, 6368.96it/s] 54%|█████▍    | 244873/450337 [00:38<00:32, 6386.26it/s] 55%|█████▍    | 245527/450337 [00:38<00:31, 6425.93it/s] 55%|█████▍    | 246218/450337 [00:39<00:31, 6560.21it/s] 55%|█████▍    | 247042/450337 [00:39<00:28, 7053.11it/s] 55%|█████▌    | 247750/450337 [00:39<00:29, 6793.12it/s] 55%|█████▌    | 248433/450337 [00:39<00:30, 6523.05it/s] 55%|█████▌    | 249156/450337 [00:39<00:29, 6720.56it/s] 55%|█████▌    | 249869/450337 [00:39<00:29, 6833.80it/s] 56%|█████▌    | 250556/450337 [00:39<00:30, 6643.26it/s] 56%|█████▌    | 251224/450337 [00:39<00:30, 6623.61it/s] 56%|█████▌    | 251889/450337 [00:39<00:30, 6593.02it/s] 56%|█████▌    | 252550/450337 [00:39<00:30, 6522.08it/s] 56%|█████▌    | 253204/450337 [00:40<00:31, 6301.61it/s] 56%|█████▋    | 253936/450337 [00:40<00:29, 6590.47it/s] 57%|█████▋    | 254598/450337 [00:40<00:31, 6242.77it/s] 57%|█████▋    | 255244/450337 [00:40<00:30, 6301.24it/s] 57%|█████▋    | 255992/450337 [00:40<00:29, 6639.45it/s] 57%|█████▋    | 256661/450337 [00:40<00:29, 6598.82it/s] 57%|█████▋    | 257324/450337 [00:40<00:29, 6533.65it/s] 57%|█████▋    | 258020/450337 [00:40<00:28, 6655.32it/s] 57%|█████▋    | 258688/450337 [00:40<00:31, 6008.71it/s] 58%|█████▊    | 259302/450337 [00:41<00:32, 5899.55it/s] 58%|█████▊    | 259937/450337 [00:41<00:31, 6022.39it/s] 58%|█████▊    | 260695/450337 [00:41<00:29, 6463.87it/s] 58%|█████▊    | 261349/450337 [00:41<00:30, 6166.25it/s] 58%|█████▊    | 262009/450337 [00:41<00:29, 6287.37it/s] 58%|█████▊    | 262644/450337 [00:41<00:31, 6021.50it/s] 58%|█████▊    | 263252/450337 [00:41<00:31, 6034.37it/s] 59%|█████▊    | 263908/450337 [00:41<00:30, 6178.79it/s] 59%|█████▊    | 264530/450337 [00:41<00:30, 6059.03it/s] 59%|█████▉    | 265176/450337 [00:42<00:30, 6167.13it/s] 59%|█████▉    | 265795/450337 [00:42<00:30, 6049.61it/s] 59%|█████▉    | 266415/450337 [00:42<00:30, 6092.90it/s] 59%|█████▉    | 267026/450337 [00:42<00:30, 5962.58it/s] 59%|█████▉    | 267639/450337 [00:42<00:30, 6008.13it/s] 60%|█████▉    | 268241/450337 [00:42<00:30, 5991.16it/s] 60%|█████▉    | 268930/450337 [00:42<00:29, 6254.56it/s] 60%|█████▉    | 269558/450337 [00:42<00:28, 6257.89it/s] 60%|██████    | 270228/450337 [00:42<00:28, 6386.56it/s] 60%|██████    | 270868/450337 [00:42<00:28, 6264.09it/s] 60%|██████    | 271496/450337 [00:43<00:28, 6181.10it/s] 60%|██████    | 272185/450337 [00:43<00:27, 6388.26it/s] 61%|██████    | 273013/450337 [00:43<00:25, 6944.29it/s] 61%|██████    | 273710/450337 [00:43<00:25, 6902.95it/s] 61%|██████    | 274402/450337 [00:43<00:25, 6777.22it/s] 61%|██████    | 275081/450337 [00:43<00:27, 6327.20it/s] 61%|██████    | 275720/450337 [00:43<00:28, 6139.12it/s] 61%|██████▏   | 276339/450337 [00:43<00:29, 5966.85it/s] 62%|██████▏   | 276963/450337 [00:43<00:28, 6036.97it/s] 62%|██████▏   | 277570/450337 [00:44<00:28, 6009.91it/s] 62%|██████▏   | 278229/450337 [00:44<00:27, 6175.28it/s] 62%|██████▏   | 278945/450337 [00:44<00:26, 6461.85it/s] 62%|██████▏   | 279594/450337 [00:44<00:26, 6338.88it/s] 62%|██████▏   | 280285/450337 [00:44<00:26, 6498.94it/s] 62%|██████▏   | 280937/450337 [00:44<00:28, 6037.30it/s] 63%|██████▎   | 281598/450337 [00:44<00:27, 6192.46it/s] 63%|██████▎   | 282224/450337 [00:44<00:27, 6156.76it/s] 63%|██████▎   | 282844/450337 [00:44<00:27, 6082.48it/s] 63%|██████▎   | 283553/450337 [00:44<00:26, 6369.34it/s] 63%|██████▎   | 284212/450337 [00:45<00:25, 6427.79it/s] 63%|██████▎   | 284858/450337 [00:45<00:26, 6300.44it/s] 63%|██████▎   | 285491/450337 [00:45<00:26, 6179.55it/s] 64%|██████▎   | 286154/450337 [00:45<00:26, 6309.64it/s] 64%|██████▎   | 286787/450337 [00:45<00:26, 6175.90it/s] 64%|██████▍   | 287407/450337 [00:45<00:26, 6095.35it/s] 64%|██████▍   | 288138/450337 [00:45<00:25, 6446.26it/s] 64%|██████▍   | 288785/450337 [00:45<00:25, 6372.87it/s] 64%|██████▍   | 289424/450337 [00:45<00:25, 6262.70it/s] 64%|██████▍   | 290052/450337 [00:46<00:26, 5968.14it/s] 65%|██████▍   | 290673/450337 [00:46<00:26, 6033.72it/s] 65%|██████▍   | 291287/450337 [00:46<00:26, 6064.28it/s] 65%|██████▍   | 291921/450337 [00:46<00:25, 6141.74it/s] 65%|██████▍   | 292580/450337 [00:46<00:25, 6272.64it/s] 65%|██████▌   | 293209/450337 [00:46<00:25, 6187.12it/s] 65%|██████▌   | 293829/450337 [00:46<00:25, 6058.97it/s] 65%|██████▌   | 294478/450337 [00:46<00:25, 6180.40it/s] 66%|██████▌   | 295207/450337 [00:46<00:23, 6497.90it/s] 66%|██████▌   | 295859/450337 [00:46<00:24, 6404.16it/s] 66%|██████▌   | 296501/450337 [00:47<00:24, 6294.82it/s] 66%|██████▌   | 297159/450337 [00:47<00:24, 6377.75it/s] 66%|██████▌   | 297857/450337 [00:47<00:23, 6549.87it/s] 66%|██████▋   | 298513/450337 [00:47<00:23, 6359.61it/s] 66%|██████▋   | 299151/450337 [00:47<00:24, 6248.62it/s] 67%|██████▋   | 299901/450337 [00:47<00:22, 6610.74it/s] 67%|██████▋   | 300565/450337 [00:47<00:23, 6472.18it/s] 67%|██████▋   | 301215/450337 [00:47<00:23, 6250.16it/s] 67%|██████▋   | 301843/450337 [00:47<00:24, 5966.09it/s] 67%|██████▋   | 302540/450337 [00:47<00:23, 6246.38it/s] 67%|██████▋   | 303236/450337 [00:48<00:22, 6438.01it/s] 67%|██████▋   | 303884/450337 [00:48<00:22, 6439.20it/s] 68%|██████▊   | 304531/450337 [00:48<00:23, 6274.74it/s] 68%|██████▊   | 305175/450337 [00:48<00:22, 6317.68it/s] 68%|██████▊   | 305809/450337 [00:48<00:23, 6203.30it/s] 68%|██████▊   | 306431/450337 [00:48<00:23, 6184.45it/s] 68%|██████▊   | 307051/450337 [00:48<00:23, 6123.35it/s] 68%|██████▊   | 307764/450337 [00:48<00:22, 6413.26it/s] 68%|██████▊   | 308407/450337 [00:48<00:24, 5771.17it/s] 69%|██████▊   | 309231/450337 [00:49<00:21, 6445.47it/s] 69%|██████▉   | 309989/450337 [00:49<00:20, 6764.39it/s] 69%|██████▉   | 310678/450337 [00:49<00:21, 6573.16it/s] 69%|██████▉   | 311345/450337 [00:49<00:21, 6420.01it/s] 69%|██████▉   | 311994/450337 [00:49<00:22, 6251.94it/s] 69%|██████▉   | 312648/450337 [00:49<00:21, 6325.36it/s] 70%|██████▉   | 313314/450337 [00:49<00:21, 6417.72it/s] 70%|██████▉   | 314053/450337 [00:49<00:20, 6696.48it/s] 70%|██████▉   | 314726/450337 [00:49<00:21, 6429.68it/s] 70%|███████   | 315373/450337 [00:50<00:21, 6156.64it/s] 70%|███████   | 316018/450337 [00:50<00:21, 6237.16it/s] 70%|███████   | 316646/450337 [00:50<00:21, 6115.61it/s] 70%|███████   | 317261/450337 [00:50<00:21, 6068.57it/s] 71%|███████   | 317870/450337 [00:50<00:22, 5856.00it/s] 71%|███████   | 318508/450337 [00:50<00:21, 6003.16it/s] 71%|███████   | 319171/450337 [00:50<00:21, 6183.79it/s] 71%|███████   | 319810/450337 [00:50<00:20, 6240.98it/s] 71%|███████   | 320436/450337 [00:50<00:21, 5973.41it/s] 71%|███████▏  | 321191/450337 [00:50<00:20, 6420.88it/s] 71%|███████▏  | 321838/450337 [00:51<00:20, 6199.69it/s] 72%|███████▏  | 322463/450337 [00:51<00:21, 6005.40it/s] 72%|███████▏  | 323088/450337 [00:51<00:20, 6072.87it/s] 72%|███████▏  | 323748/450337 [00:51<00:20, 6219.13it/s] 72%|███████▏  | 324402/450337 [00:51<00:19, 6307.63it/s] 72%|███████▏  | 325035/450337 [00:51<00:19, 6283.80it/s] 72%|███████▏  | 325665/450337 [00:51<00:21, 5933.08it/s] 72%|███████▏  | 326266/450337 [00:51<00:20, 5954.83it/s] 73%|███████▎  | 327005/450337 [00:51<00:19, 6364.72it/s] 73%|███████▎  | 327713/450337 [00:52<00:18, 6571.37it/s] 73%|███████▎  | 328507/450337 [00:52<00:17, 6967.53it/s] 73%|███████▎  | 329207/450337 [00:52<00:18, 6662.53it/s] 73%|███████▎  | 329878/450337 [00:52<00:18, 6472.86it/s] 73%|███████▎  | 330530/450337 [00:52<00:19, 6006.89it/s] 74%|███████▎  | 331212/450337 [00:52<00:19, 6222.39it/s] 74%|███████▎  | 331842/450337 [00:52<00:19, 6076.69it/s] 74%|███████▍  | 332546/450337 [00:52<00:18, 6346.70it/s] 74%|███████▍  | 333187/450337 [00:52<00:19, 6116.62it/s] 74%|███████▍  | 333868/450337 [00:52<00:18, 6311.95it/s] 74%|███████▍  | 334504/450337 [00:53<00:18, 6251.33it/s] 74%|███████▍  | 335153/450337 [00:53<00:18, 6319.78it/s] 75%|███████▍  | 335806/450337 [00:53<00:17, 6375.61it/s] 75%|███████▍  | 336446/450337 [00:53<00:17, 6358.94it/s] 75%|███████▍  | 337119/450337 [00:53<00:17, 6467.89it/s] 75%|███████▌  | 337834/450337 [00:53<00:16, 6665.16it/s] 75%|███████▌  | 338502/450337 [00:53<00:17, 6513.92it/s] 75%|███████▌  | 339155/450337 [00:53<00:17, 6474.54it/s] 75%|███████▌  | 339804/450337 [00:53<00:17, 6280.49it/s] 76%|███████▌  | 340434/450337 [00:54<00:18, 5947.93it/s] 76%|███████▌  | 341063/450337 [00:54<00:18, 6041.95it/s] 76%|███████▌  | 341722/450337 [00:54<00:17, 6191.49it/s] 76%|███████▌  | 342399/450337 [00:54<00:16, 6356.03it/s] 76%|███████▌  | 343053/450337 [00:54<00:16, 6401.96it/s] 76%|███████▋  | 343755/450337 [00:54<00:16, 6581.55it/s] 76%|███████▋  | 344415/450337 [00:54<00:17, 6143.99it/s] 77%|███████▋  | 345122/450337 [00:54<00:16, 6402.17it/s] 77%|███████▋  | 345769/450337 [00:54<00:17, 5910.79it/s] 77%|███████▋  | 346438/450337 [00:54<00:16, 6120.67it/s] 77%|███████▋  | 347073/450337 [00:55<00:16, 6184.16it/s] 77%|███████▋  | 347699/450337 [00:55<00:16, 6173.28it/s] 77%|███████▋  | 348322/450337 [00:55<00:16, 6080.05it/s] 77%|███████▋  | 348984/450337 [00:55<00:16, 6233.98it/s] 78%|███████▊  | 349614/450337 [00:55<00:16, 6252.27it/s] 78%|███████▊  | 350476/450337 [00:55<00:14, 6948.94it/s] 78%|███████▊  | 351174/450337 [00:55<00:15, 6542.16it/s] 78%|███████▊  | 351835/450337 [00:55<00:15, 6463.71it/s] 78%|███████▊  | 352486/450337 [00:55<00:16, 5837.55it/s] 78%|███████▊  | 353151/450337 [00:56<00:16, 6053.49it/s] 79%|███████▊  | 353768/450337 [00:56<00:16, 5920.30it/s] 79%|███████▊  | 354546/450337 [00:56<00:14, 6437.45it/s] 79%|███████▉  | 355200/450337 [00:56<00:15, 6037.41it/s] 79%|███████▉  | 355860/450337 [00:56<00:15, 6190.26it/s] 79%|███████▉  | 356496/450337 [00:56<00:15, 6237.99it/s] 79%|███████▉  | 357127/450337 [00:56<00:15, 5917.45it/s] 79%|███████▉  | 357750/450337 [00:56<00:15, 6004.66it/s] 80%|███████▉  | 358357/450337 [00:56<00:15, 6004.95it/s] 80%|███████▉  | 359022/450337 [00:57<00:14, 6190.99it/s] 80%|███████▉  | 359645/450337 [00:57<00:14, 6150.45it/s] 80%|████████  | 360319/450337 [00:57<00:14, 6322.38it/s] 80%|████████  | 360999/450337 [00:57<00:13, 6462.83it/s] 80%|████████  | 361685/450337 [00:57<00:13, 6580.08it/s] 80%|████████  | 362393/450337 [00:57<00:13, 6728.05it/s] 81%|████████  | 363084/450337 [00:57<00:12, 6779.81it/s] 81%|████████  | 363763/450337 [00:57<00:12, 6740.86it/s] 81%|████████  | 364461/450337 [00:57<00:12, 6807.15it/s] 81%|████████  | 365164/450337 [00:57<00:12, 6871.79it/s] 81%|████████  | 365852/450337 [00:58<00:12, 6801.93it/s] 81%|████████▏ | 366533/450337 [00:58<00:12, 6607.76it/s] 82%|████████▏ | 367196/450337 [00:58<00:12, 6422.42it/s] 82%|████████▏ | 367840/450337 [00:58<00:13, 5969.14it/s] 82%|████████▏ | 368497/450337 [00:58<00:13, 6133.32it/s] 82%|████████▏ | 369156/450337 [00:58<00:12, 6261.71it/s] 82%|████████▏ | 369818/450337 [00:58<00:12, 6359.44it/s] 82%|████████▏ | 370458/450337 [00:58<00:12, 6223.83it/s] 82%|████████▏ | 371084/450337 [00:58<00:12, 6218.45it/s] 83%|████████▎ | 371708/450337 [00:58<00:12, 6144.92it/s] 83%|████████▎ | 372324/450337 [00:59<00:12, 6009.32it/s] 83%|████████▎ | 372957/450337 [00:59<00:12, 6100.07it/s] 83%|████████▎ | 373613/450337 [00:59<00:12, 6230.54it/s] 83%|████████▎ | 374295/450337 [00:59<00:11, 6397.06it/s] 83%|████████▎ | 374973/450337 [00:59<00:11, 6505.92it/s] 83%|████████▎ | 375625/450337 [00:59<00:12, 6049.32it/s] 84%|████████▎ | 376260/450337 [00:59<00:12, 6126.25it/s] 84%|████████▎ | 376878/450337 [00:59<00:12, 5793.92it/s] 84%|████████▍ | 377493/450337 [00:59<00:12, 5891.02it/s] 84%|████████▍ | 378220/450337 [01:00<00:11, 6281.59it/s] 84%|████████▍ | 378854/450337 [01:00<00:11, 6264.67it/s] 84%|████████▍ | 379544/450337 [01:00<00:10, 6445.41it/s] 84%|████████▍ | 380262/450337 [01:00<00:10, 6654.93it/s] 85%|████████▍ | 380983/450337 [01:00<00:10, 6811.68it/s] 85%|████████▍ | 381667/450337 [01:00<00:10, 6554.21it/s] 85%|████████▍ | 382326/450337 [01:00<00:10, 6191.78it/s] 85%|████████▌ | 382951/450337 [01:00<00:11, 6017.64it/s] 85%|████████▌ | 383566/450337 [01:00<00:11, 6049.06it/s] 85%|████████▌ | 384269/450337 [01:00<00:10, 6325.94it/s] 85%|████████▌ | 384956/450337 [01:01<00:10, 6481.23it/s] 86%|████████▌ | 385608/450337 [01:01<00:10, 6396.85it/s] 86%|████████▌ | 386250/450337 [01:01<00:10, 6117.42it/s] 86%|████████▌ | 386953/450337 [01:01<00:09, 6376.84it/s] 86%|████████▌ | 387757/450337 [01:01<00:09, 6857.27it/s] 86%|████████▋ | 388448/450337 [01:01<00:09, 6453.63it/s] 86%|████████▋ | 389101/450337 [01:01<00:09, 6389.18it/s] 87%|████████▋ | 389776/450337 [01:01<00:09, 6491.32it/s] 87%|████████▋ | 390430/450337 [01:01<00:09, 6475.13it/s] 87%|████████▋ | 391089/450337 [01:02<00:09, 6504.13it/s] 87%|████████▋ | 391742/450337 [01:02<00:09, 6065.91it/s] 87%|████████▋ | 392356/450337 [01:02<00:09, 6049.52it/s] 87%|████████▋ | 392966/450337 [01:02<00:09, 6021.31it/s] 87%|████████▋ | 393572/450337 [01:02<00:09, 5842.30it/s] 88%|████████▊ | 394294/450337 [01:02<00:08, 6234.04it/s] 88%|████████▊ | 394922/450337 [01:02<00:09, 6146.04it/s] 88%|████████▊ | 395577/450337 [01:02<00:08, 6259.63it/s] 88%|████████▊ | 396206/450337 [01:02<00:08, 6222.93it/s] 88%|████████▊ | 396830/450337 [01:02<00:08, 6089.81it/s] 88%|████████▊ | 397441/450337 [01:03<00:09, 5876.30it/s] 88%|████████▊ | 398171/450337 [01:03<00:08, 6281.27it/s] 89%|████████▊ | 398813/450337 [01:03<00:08, 6309.34it/s] 89%|████████▊ | 399503/450337 [01:03<00:07, 6481.41it/s] 89%|████████▉ | 400154/450337 [01:03<00:07, 6420.40it/s] 89%|████████▉ | 400853/450337 [01:03<00:07, 6580.98it/s] 89%|████████▉ | 401513/450337 [01:03<00:07, 6283.94it/s] 89%|████████▉ | 402145/450337 [01:03<00:08, 5907.89it/s] 89%|████████▉ | 402742/450337 [01:03<00:08, 5841.72it/s] 90%|████████▉ | 403396/450337 [01:04<00:07, 6037.22it/s] 90%|████████▉ | 404004/450337 [01:04<00:07, 5967.38it/s] 90%|████████▉ | 404684/450337 [01:04<00:07, 6199.98it/s] 90%|█████████ | 405307/450337 [01:04<00:07, 5926.89it/s] 90%|█████████ | 405930/450337 [01:04<00:07, 6001.29it/s] 90%|█████████ | 406534/450337 [01:04<00:07, 5825.42it/s] 90%|█████████ | 407204/450337 [01:04<00:07, 6064.82it/s] 91%|█████████ | 407814/450337 [01:04<00:07, 5908.92it/s] 91%|█████████ | 408475/450337 [01:04<00:06, 6104.56it/s] 91%|█████████ | 409164/450337 [01:05<00:06, 6324.17it/s] 91%|█████████ | 409799/450337 [01:05<00:06, 6166.97it/s] 91%|█████████ | 410419/450337 [01:05<00:06, 6164.14it/s] 91%|█████████▏| 411044/450337 [01:05<00:06, 6188.28it/s] 91%|█████████▏| 411665/450337 [01:05<00:06, 6150.87it/s] 92%|█████████▏| 412281/450337 [01:05<00:06, 5857.86it/s] 92%|█████████▏| 412870/450337 [01:05<00:06, 5644.24it/s] 92%|█████████▏| 413482/450337 [01:05<00:06, 5777.67it/s] 92%|█████████▏| 414063/450337 [01:05<00:06, 5575.39it/s] 92%|█████████▏| 414624/450337 [01:05<00:06, 5570.27it/s] 92%|█████████▏| 415222/450337 [01:06<00:06, 5685.68it/s] 92%|█████████▏| 415834/450337 [01:06<00:05, 5811.87it/s] 92%|█████████▏| 416417/450337 [01:06<00:05, 5719.63it/s] 93%|█████████▎| 417040/450337 [01:06<00:05, 5868.44it/s] 93%|█████████▎| 417680/450337 [01:06<00:05, 6020.65it/s] 93%|█████████▎| 418310/450337 [01:06<00:05, 6092.91it/s] 93%|█████████▎| 418931/450337 [01:06<00:05, 6112.73it/s] 93%|█████████▎| 419578/450337 [01:06<00:04, 6206.44it/s] 93%|█████████▎| 420212/450337 [01:06<00:04, 6245.43it/s] 93%|█████████▎| 420842/450337 [01:06<00:04, 6260.95it/s] 94%|█████████▎| 421469/450337 [01:07<00:04, 5873.16it/s] 94%|█████████▎| 422070/450337 [01:07<00:04, 5911.21it/s] 94%|█████████▍| 422796/450337 [01:07<00:04, 6292.83it/s] 94%|█████████▍| 423430/450337 [01:07<00:04, 6198.42it/s] 94%|█████████▍| 424053/450337 [01:07<00:04, 6110.93it/s] 94%|█████████▍| 424667/450337 [01:07<00:04, 6025.53it/s] 94%|█████████▍| 425272/450337 [01:07<00:04, 6015.82it/s] 95%|█████████▍| 425943/450337 [01:07<00:03, 6217.68it/s] 95%|█████████▍| 426716/450337 [01:07<00:03, 6659.15it/s] 95%|█████████▍| 427384/450337 [01:08<00:03, 6477.52it/s] 95%|█████████▌| 428034/450337 [01:08<00:03, 6269.40it/s] 95%|█████████▌| 428816/450337 [01:08<00:03, 6706.01it/s] 95%|█████████▌| 429491/450337 [01:08<00:03, 6551.67it/s] 96%|█████████▌| 430156/450337 [01:08<00:03, 6579.74it/s] 96%|█████████▌| 430817/450337 [01:08<00:03, 6357.35it/s] 96%|█████████▌| 431456/450337 [01:08<00:03, 6077.94it/s] 96%|█████████▌| 432068/450337 [01:08<00:03, 5932.44it/s] 96%|█████████▌| 432985/450337 [01:08<00:02, 6839.96it/s] 96%|█████████▋| 433677/450337 [01:08<00:02, 6544.47it/s] 96%|█████████▋| 434339/450337 [01:09<00:02, 6173.29it/s] 97%|█████████▋| 434965/450337 [01:09<00:02, 6030.13it/s] 97%|█████████▋| 435680/450337 [01:09<00:02, 6329.71it/s] 97%|█████████▋| 436320/450337 [01:09<00:02, 6062.72it/s] 97%|█████████▋| 436978/450337 [01:09<00:02, 6206.18it/s] 97%|█████████▋| 437616/450337 [01:09<00:02, 6254.98it/s] 97%|█████████▋| 438246/450337 [01:09<00:01, 6233.59it/s] 97%|█████████▋| 438872/450337 [01:09<00:01, 6114.91it/s] 98%|█████████▊| 439486/450337 [01:09<00:01, 6072.03it/s] 98%|█████████▊| 440095/450337 [01:10<00:01, 6004.61it/s] 98%|█████████▊| 440697/450337 [01:10<00:01, 5859.99it/s] 98%|█████████▊| 441406/450337 [01:10<00:01, 6207.88it/s] 98%|█████████▊| 442044/450337 [01:10<00:01, 6256.10it/s] 98%|█████████▊| 442672/450337 [01:10<00:01, 6141.10it/s] 98%|█████████▊| 443344/450337 [01:10<00:01, 6290.87it/s] 99%|█████████▊| 444060/450337 [01:10<00:00, 6537.81it/s] 99%|█████████▉| 444716/450337 [01:10<00:00, 6020.53it/s] 99%|█████████▉| 445327/450337 [01:10<00:00, 5771.93it/s] 99%|█████████▉| 445957/450337 [01:11<00:00, 5916.91it/s] 99%|█████████▉| 446555/450337 [01:11<00:00, 5916.02it/s] 99%|█████████▉| 447181/450337 [01:11<00:00, 6013.10it/s] 99%|█████████▉| 447852/450337 [01:11<00:00, 6212.68it/s]100%|█████████▉| 448516/450337 [01:11<00:00, 6331.61it/s]100%|█████████▉| 449152/450337 [01:11<00:00, 6299.39it/s]100%|█████████▉| 449784/450337 [01:11<00:00, 6056.03it/s]100%|██████████| 450337/450337 [01:11<00:00, 6279.32it/s]

gathering stats for n=1
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 1947/450337 [00:00<00:23, 19465.56it/s]  1%|          | 4093/450337 [00:00<00:21, 20628.36it/s]  1%|▏         | 6366/450337 [00:00<00:20, 21586.66it/s]  2%|▏         | 8525/450337 [00:00<00:21, 20364.78it/s]  2%|▏         | 10585/450337 [00:00<00:21, 20437.75it/s]  3%|▎         | 12649/450337 [00:00<00:21, 20501.86it/s]  3%|▎         | 14739/450337 [00:00<00:21, 20627.05it/s]  4%|▎         | 16805/450337 [00:00<00:21, 20283.50it/s]  4%|▍         | 18901/450337 [00:00<00:21, 20489.36it/s]  5%|▍         | 20963/450337 [00:01<00:20, 20526.49it/s]  5%|▌         | 23018/450337 [00:01<00:20, 20459.59it/s]  6%|▌         | 25399/450337 [00:01<00:19, 21469.00it/s]  6%|▌         | 27548/450337 [00:01<00:20, 20564.01it/s]  7%|▋         | 29666/450337 [00:01<00:20, 20737.64it/s]  7%|▋         | 31747/450337 [00:01<00:20, 20455.22it/s]  8%|▊         | 33798/450337 [00:01<00:20, 19966.86it/s]  8%|▊         | 35839/450337 [00:01<00:20, 20082.24it/s]  8%|▊         | 37852/450337 [00:01<00:20, 19751.09it/s]  9%|▉         | 39831/450337 [00:01<00:20, 19743.80it/s]  9%|▉         | 41838/450337 [00:02<00:20, 19833.57it/s] 10%|▉         | 43839/450337 [00:02<00:20, 19885.18it/s] 10%|█         | 45921/450337 [00:02<00:20, 20157.42it/s] 11%|█         | 48390/450337 [00:02<00:18, 21506.08it/s] 11%|█         | 50543/450337 [00:02<00:18, 21374.71it/s] 12%|█▏        | 52761/450337 [00:02<00:18, 21610.29it/s] 12%|█▏        | 54924/450337 [00:02<00:19, 20746.16it/s] 13%|█▎        | 57007/450337 [00:02<00:19, 20472.27it/s] 13%|█▎        | 59087/450337 [00:02<00:19, 20562.48it/s] 14%|█▎        | 61196/450337 [00:02<00:18, 20716.14it/s] 14%|█▍        | 63271/450337 [00:03<00:18, 20471.90it/s] 15%|█▍        | 65321/450337 [00:03<00:18, 20394.84it/s] 15%|█▍        | 67397/450337 [00:03<00:18, 20499.75it/s] 15%|█▌        | 69450/450337 [00:03<00:18, 20507.83it/s] 16%|█▌        | 71502/450337 [00:03<00:18, 20419.53it/s] 16%|█▋        | 73753/450337 [00:03<00:17, 21039.18it/s] 17%|█▋        | 75956/450337 [00:03<00:17, 21324.26it/s] 17%|█▋        | 78090/450337 [00:03<00:17, 21178.44it/s] 18%|█▊        | 80209/450337 [00:03<00:18, 20100.92it/s] 18%|█▊        | 82311/450337 [00:04<00:18, 20357.52it/s] 19%|█▊        | 84359/450337 [00:04<00:17, 20391.44it/s] 19%|█▉        | 86439/450337 [00:04<00:17, 20505.36it/s] 20%|█▉        | 88504/450337 [00:04<00:17, 20544.90it/s] 20%|██        | 90562/450337 [00:04<00:17, 20054.65it/s] 21%|██        | 92695/450337 [00:04<00:17, 20419.24it/s] 21%|██        | 94741/450337 [00:04<00:17, 20387.79it/s] 22%|██▏       | 96933/450337 [00:04<00:16, 20839.27it/s] 22%|██▏       | 99020/450337 [00:04<00:17, 20388.01it/s] 22%|██▏       | 101063/450337 [00:04<00:17, 19929.94it/s] 23%|██▎       | 103060/450337 [00:05<00:17, 19934.11it/s] 23%|██▎       | 105057/450337 [00:05<00:17, 19775.37it/s] 24%|██▍       | 107066/450337 [00:05<00:17, 19867.14it/s] 24%|██▍       | 109055/450337 [00:05<00:17, 19607.35it/s] 25%|██▍       | 111159/450337 [00:05<00:16, 20024.48it/s] 25%|██▌       | 113164/450337 [00:05<00:16, 19959.35it/s] 26%|██▌       | 115162/450337 [00:05<00:16, 19868.42it/s] 26%|██▌       | 117150/450337 [00:05<00:17, 19398.61it/s] 26%|██▋       | 119136/450337 [00:05<00:16, 19524.67it/s] 27%|██▋       | 121250/450337 [00:05<00:16, 19997.20it/s] 27%|██▋       | 123253/450337 [00:06<00:16, 19721.85it/s] 28%|██▊       | 125254/450337 [00:06<00:16, 19803.32it/s] 28%|██▊       | 127363/450337 [00:06<00:16, 20181.88it/s] 29%|██▊       | 129407/450337 [00:06<00:15, 20257.21it/s] 29%|██▉       | 131434/450337 [00:06<00:16, 19807.27it/s] 30%|██▉       | 133418/450337 [00:06<00:16, 19556.18it/s] 30%|███       | 135694/450337 [00:06<00:15, 20489.35it/s] 31%|███       | 137877/450337 [00:06<00:14, 20883.60it/s] 31%|███       | 139969/450337 [00:06<00:14, 20731.67it/s] 32%|███▏      | 142045/450337 [00:06<00:15, 20334.52it/s] 32%|███▏      | 144152/450337 [00:07<00:14, 20547.90it/s] 33%|███▎      | 146380/450337 [00:07<00:14, 21056.11it/s] 33%|███▎      | 148489/450337 [00:07<00:14, 20171.21it/s] 33%|███▎      | 150516/450337 [00:07<00:15, 19979.85it/s] 34%|███▍      | 152521/450337 [00:07<00:15, 19736.72it/s] 34%|███▍      | 154565/450337 [00:07<00:14, 19930.41it/s] 35%|███▍      | 156672/450337 [00:07<00:14, 20255.73it/s] 35%|███▌      | 158829/450337 [00:07<00:14, 20639.36it/s] 36%|███▌      | 160896/450337 [00:07<00:14, 20609.03it/s] 36%|███▌      | 162959/450337 [00:08<00:14, 20446.83it/s] 37%|███▋      | 165017/450337 [00:08<00:13, 20483.68it/s] 37%|███▋      | 167204/450337 [00:08<00:13, 20892.02it/s] 38%|███▊      | 169295/450337 [00:08<00:13, 20560.27it/s] 38%|███▊      | 171353/450337 [00:08<00:13, 20086.05it/s] 38%|███▊      | 173365/450337 [00:08<00:13, 20001.70it/s] 39%|███▉      | 175368/450337 [00:08<00:13, 19963.25it/s] 39%|███▉      | 177366/450337 [00:08<00:13, 19505.33it/s] 40%|███▉      | 179320/450337 [00:08<00:13, 19498.70it/s] 40%|████      | 181272/450337 [00:08<00:13, 19482.61it/s] 41%|████      | 183222/450337 [00:09<00:13, 19373.33it/s] 41%|████      | 185161/450337 [00:09<00:13, 19131.29it/s] 42%|████▏     | 187076/450337 [00:09<00:13, 19119.82it/s] 42%|████▏     | 189161/450337 [00:09<00:13, 19628.58it/s] 42%|████▏     | 191191/450337 [00:09<00:13, 19826.09it/s] 43%|████▎     | 193213/450337 [00:09<00:12, 19940.37it/s] 43%|████▎     | 195267/450337 [00:09<00:12, 20117.52it/s] 44%|████▍     | 197522/450337 [00:09<00:12, 20839.56it/s] 44%|████▍     | 199607/450337 [00:09<00:12, 20498.51it/s] 45%|████▍     | 201659/450337 [00:09<00:12, 20057.61it/s] 45%|████▌     | 203793/450337 [00:10<00:12, 20431.75it/s] 46%|████▌     | 205840/450337 [00:10<00:12, 20046.93it/s] 46%|████▌     | 207848/450337 [00:10<00:12, 19941.60it/s] 47%|████▋     | 209845/450337 [00:10<00:12, 19642.61it/s] 47%|████▋     | 211919/450337 [00:10<00:11, 19957.02it/s] 48%|████▊     | 213917/450337 [00:10<00:11, 19902.84it/s] 48%|████▊     | 215909/450337 [00:10<00:11, 19636.45it/s] 48%|████▊     | 217927/450337 [00:10<00:11, 19794.10it/s] 49%|████▉     | 219951/450337 [00:10<00:11, 19923.49it/s] 49%|████▉     | 221945/450337 [00:10<00:11, 19823.30it/s] 50%|████▉     | 223929/450337 [00:11<00:11, 19422.20it/s] 50%|█████     | 226245/450337 [00:11<00:10, 20516.05it/s] 51%|█████     | 228301/450337 [00:11<00:11, 20017.88it/s] 51%|█████     | 230308/450337 [00:11<00:11, 19585.70it/s] 52%|█████▏    | 232271/450337 [00:11<00:11, 19495.51it/s] 52%|█████▏    | 234557/450337 [00:11<00:10, 20466.57it/s] 53%|█████▎    | 236609/450337 [00:11<00:10, 20280.73it/s] 53%|█████▎    | 238703/450337 [00:11<00:10, 20467.28it/s] 53%|█████▎    | 240817/450337 [00:11<00:10, 20662.55it/s] 54%|█████▍    | 242886/450337 [00:12<00:10, 20663.54it/s] 54%|█████▍    | 244954/450337 [00:12<00:10, 20474.42it/s] 55%|█████▍    | 247212/450337 [00:12<00:09, 21096.98it/s] 55%|█████▌    | 249324/450337 [00:12<00:09, 21059.03it/s] 56%|█████▌    | 251449/450337 [00:12<00:09, 21114.28it/s] 56%|█████▋    | 253562/450337 [00:12<00:09, 20807.67it/s] 57%|█████▋    | 255645/450337 [00:12<00:09, 20582.88it/s] 57%|█████▋    | 257836/450337 [00:12<00:09, 20973.48it/s] 58%|█████▊    | 259935/450337 [00:12<00:09, 20122.84it/s] 58%|█████▊    | 262012/450337 [00:12<00:09, 20298.83it/s] 59%|█████▊    | 264048/450337 [00:13<00:09, 19930.31it/s] 59%|█████▉    | 266046/450337 [00:13<00:09, 19696.52it/s] 60%|█████▉    | 268019/450337 [00:13<00:09, 19698.86it/s] 60%|█████▉    | 270102/450337 [00:13<00:08, 20030.17it/s] 60%|██████    | 272141/450337 [00:13<00:08, 20123.73it/s] 61%|██████    | 274409/450337 [00:13<00:08, 20876.99it/s] 61%|██████▏   | 276499/450337 [00:13<00:08, 19867.88it/s] 62%|██████▏   | 278604/450337 [00:13<00:08, 20194.84it/s] 62%|██████▏   | 280633/450337 [00:13<00:08, 20039.86it/s] 63%|██████▎   | 282644/450337 [00:13<00:08, 19880.69it/s] 63%|██████▎   | 284728/450337 [00:14<00:08, 20155.16it/s] 64%|██████▎   | 286748/450337 [00:14<00:08, 20041.15it/s] 64%|██████▍   | 288828/450337 [00:14<00:07, 20255.10it/s] 65%|██████▍   | 290856/450337 [00:14<00:08, 19919.21it/s] 65%|██████▌   | 292851/450337 [00:14<00:07, 19815.25it/s] 65%|██████▌   | 294953/450337 [00:14<00:07, 20169.40it/s] 66%|██████▌   | 296978/450337 [00:14<00:07, 20192.50it/s] 66%|██████▋   | 298999/450337 [00:14<00:07, 20047.68it/s] 67%|██████▋   | 301057/450337 [00:14<00:07, 20201.56it/s] 67%|██████▋   | 303079/450337 [00:14<00:07, 20109.39it/s] 68%|██████▊   | 305111/450337 [00:15<00:07, 20163.15it/s] 68%|██████▊   | 307128/450337 [00:15<00:07, 19914.40it/s] 69%|██████▊   | 309189/450337 [00:15<00:07, 20118.43it/s] 69%|██████▉   | 311287/450337 [00:15<00:06, 20369.76it/s] 70%|██████▉   | 313394/450337 [00:15<00:06, 20577.38it/s] 70%|███████   | 315453/450337 [00:15<00:06, 20211.81it/s] 70%|███████   | 317477/450337 [00:15<00:06, 19984.02it/s] 71%|███████   | 319477/450337 [00:15<00:06, 19838.79it/s] 71%|███████▏  | 321533/450337 [00:15<00:06, 20046.76it/s] 72%|███████▏  | 323539/450337 [00:16<00:06, 19755.24it/s] 72%|███████▏  | 325516/450337 [00:16<00:06, 19524.16it/s] 73%|███████▎  | 327740/450337 [00:16<00:06, 20319.29it/s] 73%|███████▎  | 329834/450337 [00:16<00:05, 20502.08it/s] 74%|███████▎  | 331887/450337 [00:16<00:05, 19973.70it/s] 74%|███████▍  | 333956/450337 [00:16<00:05, 20178.40it/s] 75%|███████▍  | 336006/450337 [00:16<00:05, 20263.62it/s] 75%|███████▌  | 338098/450337 [00:16<00:05, 20456.16it/s] 76%|███████▌  | 340146/450337 [00:16<00:05, 20004.57it/s] 76%|███████▌  | 342229/450337 [00:16<00:05, 20238.34it/s] 76%|███████▋  | 344307/450337 [00:17<00:05, 20389.46it/s] 77%|███████▋  | 346349/450337 [00:17<00:05, 20004.32it/s] 77%|███████▋  | 348353/450337 [00:17<00:05, 20013.15it/s] 78%|███████▊  | 350652/450337 [00:17<00:04, 20888.73it/s] 78%|███████▊  | 352744/450337 [00:17<00:04, 19868.22it/s] 79%|███████▉  | 354824/450337 [00:17<00:04, 20134.18it/s] 79%|███████▉  | 356847/450337 [00:17<00:04, 19830.61it/s] 80%|███████▉  | 358837/450337 [00:17<00:04, 19787.69it/s] 80%|████████  | 360901/450337 [00:17<00:04, 20036.27it/s] 81%|████████  | 363104/450337 [00:17<00:04, 20619.74it/s] 81%|████████  | 365276/450337 [00:18<00:04, 20941.16it/s] 82%|████████▏ | 367374/450337 [00:18<00:03, 20841.16it/s] 82%|████████▏ | 369461/450337 [00:18<00:03, 20399.13it/s] 82%|████████▏ | 371505/450337 [00:18<00:03, 20212.83it/s] 83%|████████▎ | 373529/450337 [00:18<00:03, 20137.62it/s] 83%|████████▎ | 375545/450337 [00:18<00:03, 20033.63it/s] 84%|████████▍ | 377550/450337 [00:18<00:03, 19560.76it/s] 84%|████████▍ | 379719/450337 [00:18<00:03, 20179.30it/s] 85%|████████▍ | 381855/450337 [00:18<00:03, 20524.75it/s] 85%|████████▌ | 383911/450337 [00:18<00:03, 19815.54it/s] 86%|████████▌ | 385987/450337 [00:19<00:03, 20088.36it/s] 86%|████████▌ | 388161/450337 [00:19<00:03, 20571.08it/s] 87%|████████▋ | 390253/450337 [00:19<00:02, 20668.98it/s] 87%|████████▋ | 392324/450337 [00:19<00:02, 19932.75it/s] 88%|████████▊ | 394325/450337 [00:19<00:02, 19802.93it/s] 88%|████████▊ | 396311/450337 [00:19<00:02, 19691.21it/s] 88%|████████▊ | 398300/450337 [00:19<00:02, 19742.71it/s] 89%|████████▉ | 400484/450337 [00:19<00:02, 20355.89it/s] 89%|████████▉ | 402523/450337 [00:19<00:02, 19389.65it/s] 90%|████████▉ | 404556/450337 [00:20<00:02, 19655.46it/s] 90%|█████████ | 406531/450337 [00:20<00:02, 19209.81it/s] 91%|█████████ | 408509/450337 [00:20<00:02, 19373.65it/s] 91%|█████████ | 410493/450337 [00:20<00:02, 19507.30it/s] 92%|█████████▏| 412449/450337 [00:20<00:01, 19066.48it/s] 92%|█████████▏| 414361/450337 [00:20<00:01, 18871.89it/s] 92%|█████████▏| 416252/450337 [00:20<00:01, 18663.35it/s] 93%|█████████▎| 418293/450337 [00:20<00:01, 19162.73it/s] 93%|█████████▎| 420307/450337 [00:20<00:01, 19448.53it/s] 94%|█████████▍| 422255/450337 [00:20<00:01, 19105.28it/s] 94%|█████████▍| 424320/450337 [00:21<00:01, 19550.81it/s] 95%|█████████▍| 426377/450337 [00:21<00:01, 19846.16it/s] 95%|█████████▌| 428464/450337 [00:21<00:01, 20146.78it/s] 96%|█████████▌| 430506/450337 [00:21<00:00, 20226.95it/s] 96%|█████████▌| 432531/450337 [00:21<00:00, 20150.01it/s] 96%|█████████▋| 434548/450337 [00:21<00:00, 19953.42it/s] 97%|█████████▋| 436545/450337 [00:21<00:00, 19809.57it/s] 97%|█████████▋| 438560/450337 [00:21<00:00, 19909.06it/s] 98%|█████████▊| 440552/450337 [00:21<00:00, 19450.02it/s] 98%|█████████▊| 442596/450337 [00:21<00:00, 19735.22it/s] 99%|█████████▊| 444589/450337 [00:22<00:00, 19786.70it/s] 99%|█████████▉| 446570/450337 [00:22<00:00, 19338.56it/s]100%|█████████▉| 448655/450337 [00:22<00:00, 19774.46it/s]100%|██████████| 450337/450337 [00:22<00:00, 20135.97it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 21.66it/s]2022-03-03 20:39:35 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-03 20:39:35 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-03 20:39:35 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-03 20:39:35 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-03 20:39:35 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-03 20:39:35 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-03 20:39:35 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-03 20:39:35 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-03 20:39:35 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 20:39:35 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-03 20:39:35 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 20:39:35 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-03 20:39:35 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-03 20:39:35 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-03 20:39:35 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-03 20:39:35 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-03 20:39:35 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-03 20:39:35 | INFO | fairseq.trainer | begin training epoch 1
2022-03-03 20:39:35 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-03 20:39:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-03 20:39:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 20:39:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 20:40:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:40:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 20:47:39 | INFO | train_inner | epoch 001:    105 / 393 loss=17.083, ppl=138832, wps=14686.9, ups=0.22, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.482, loss_scale=4, train_wall=478, gb_free=10.1, wall=484
2022-03-03 20:55:05 | INFO | train_inner | epoch 001:    205 / 393 loss=14.705, ppl=26711.7, wps=14687.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.583, loss_scale=4, train_wall=441, gb_free=10.1, wall=930
2022-03-03 21:02:31 | INFO | train_inner | epoch 001:    305 / 393 loss=12.667, ppl=6504.57, wps=14698.9, ups=0.22, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.037, loss_scale=4, train_wall=441, gb_free=10.1, wall=1376
2022-03-03 21:09:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:09:07 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.873 | ppl 1875.65 | wps 33957 | wpb 2034.1 | bsz 4 | num_updates 388
2022-03-03 21:09:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-03 21:09:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-03 21:09:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-03 21:09:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 1 @ 388 updates, score 10.873) (writing took 4.840196767821908 seconds)
2022-03-03 21:09:12 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-03 21:09:12 | INFO | train | epoch 001 | loss 14.016 | ppl 16567.5 | wps 14600 | ups 0.22 | wpb 65460.6 | bsz 127.9 | num_updates 388 | lr 4.85903e-05 | gnorm 1.714 | loss_scale 4 | train_wall 1746 | gb_free 10.1 | wall 1777
2022-03-03 21:09:12 | INFO | fairseq.trainer | begin training epoch 2
2022-03-03 21:09:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:10:06 | INFO | train_inner | epoch 002:     12 / 393 loss=11.219, ppl=2383.34, wps=14342.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.609, loss_scale=4, train_wall=439, gb_free=10.1, wall=1831
2022-03-03 21:17:31 | INFO | train_inner | epoch 002:    112 / 393 loss=10.667, ppl=1625.89, wps=14711.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.485, loss_scale=4, train_wall=441, gb_free=10.1, wall=2276
2022-03-03 21:24:57 | INFO | train_inner | epoch 002:    212 / 393 loss=10.392, ppl=1344.07, wps=14698.8, ups=0.22, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.536, loss_scale=8, train_wall=441, gb_free=10.1, wall=2722
2022-03-03 21:32:23 | INFO | train_inner | epoch 002:    312 / 393 loss=10.145, ppl=1132.14, wps=14702, ups=0.22, wpb=65535.4, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.583, loss_scale=8, train_wall=441, gb_free=10.1, wall=3168
2022-03-03 21:38:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:38:28 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.829 | ppl 909.82 | wps 33874.1 | wpb 2034.1 | bsz 4 | num_updates 781 | best_loss 9.829
2022-03-03 21:38:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 781 updates
2022-03-03 21:38:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-03 21:38:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-03 21:38:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 2 @ 781 updates, score 9.829) (writing took 4.539092826656997 seconds)
2022-03-03 21:38:33 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-03 21:38:33 | INFO | train | epoch 002 | loss 10.325 | ppl 1282.58 | wps 14610.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 0.553 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 3538
2022-03-03 21:38:33 | INFO | fairseq.trainer | begin training epoch 3
2022-03-03 21:38:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:39:58 | INFO | train_inner | epoch 003:     19 / 393 loss=9.937, ppl=980.02, wps=14341.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.642, loss_scale=8, train_wall=439, gb_free=10.1, wall=3623
2022-03-03 21:47:23 | INFO | train_inner | epoch 003:    119 / 393 loss=9.74, ppl=854.87, wps=14698.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.723, loss_scale=8, train_wall=441, gb_free=10.1, wall=4068
2022-03-03 21:54:50 | INFO | train_inner | epoch 003:    219 / 393 loss=9.575, ppl=762.82, wps=14684.3, ups=0.22, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.797, loss_scale=8, train_wall=441, gb_free=10.1, wall=4515
2022-03-03 22:02:16 | INFO | train_inner | epoch 003:    319 / 393 loss=9.421, ppl=685.67, wps=14686.2, ups=0.22, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.799, loss_scale=16, train_wall=441, gb_free=10.1, wall=4961
2022-03-03 22:07:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:07:51 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.181 | ppl 580.61 | wps 33908.9 | wpb 2034.1 | bsz 4 | num_updates 1174 | best_loss 9.181
2022-03-03 22:07:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1174 updates
2022-03-03 22:07:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-03 22:07:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-03 22:07:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 3 @ 1174 updates, score 9.181) (writing took 4.55519634950906 seconds)
2022-03-03 22:07:55 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-03 22:07:55 | INFO | train | epoch 003 | loss 9.538 | ppl 743.44 | wps 14597.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1174 | lr 0.000146821 | gnorm 0.774 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 5300
2022-03-03 22:07:55 | INFO | fairseq.trainer | begin training epoch 4
2022-03-03 22:07:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:09:51 | INFO | train_inner | epoch 004:     26 / 393 loss=9.266, ppl=615.81, wps=14332, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.801, loss_scale=16, train_wall=439, gb_free=10.1, wall=5416
2022-03-03 22:17:17 | INFO | train_inner | epoch 004:    126 / 393 loss=9.122, ppl=557.19, wps=14690, ups=0.22, wpb=65530.9, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.806, loss_scale=16, train_wall=441, gb_free=10.1, wall=5862
2022-03-03 22:24:43 | INFO | train_inner | epoch 004:    226 / 393 loss=9.011, ppl=515.98, wps=14694, ups=0.22, wpb=65535.4, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.82, loss_scale=16, train_wall=441, gb_free=10.1, wall=6308
2022-03-03 22:32:10 | INFO | train_inner | epoch 004:    326 / 393 loss=8.912, ppl=481.72, wps=14688.3, ups=0.22, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.809, loss_scale=16, train_wall=441, gb_free=10.1, wall=6755
2022-03-03 22:37:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:37:13 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.718 | ppl 420.96 | wps 33648.6 | wpb 2034.1 | bsz 4 | num_updates 1567 | best_loss 8.718
2022-03-03 22:37:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1567 updates
2022-03-03 22:37:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-03 22:37:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-03 22:37:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 4 @ 1567 updates, score 8.718) (writing took 4.37807338591665 seconds)
2022-03-03 22:37:17 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-03 22:37:17 | INFO | train | epoch 004 | loss 8.993 | ppl 509.69 | wps 14598.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1567 | lr 0.000195936 | gnorm 0.818 | loss_scale 32 | train_wall 1732 | gb_free 10.1 | wall 7062
2022-03-03 22:37:18 | INFO | fairseq.trainer | begin training epoch 5
2022-03-03 22:37:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:39:45 | INFO | train_inner | epoch 005:     33 / 393 loss=8.781, ppl=439.88, wps=14330.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.835, loss_scale=32, train_wall=440, gb_free=10.1, wall=7210
2022-03-03 22:47:11 | INFO | train_inner | epoch 005:    133 / 393 loss=8.66, ppl=404.59, wps=14683.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.831, loss_scale=32, train_wall=441, gb_free=10.1, wall=7656
2022-03-03 22:54:37 | INFO | train_inner | epoch 005:    233 / 393 loss=8.567, ppl=379.12, wps=14683.9, ups=0.22, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.802, loss_scale=32, train_wall=441, gb_free=10.1, wall=8102
2022-03-03 23:02:04 | INFO | train_inner | epoch 005:    333 / 393 loss=8.492, ppl=359.97, wps=14687.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.805, loss_scale=32, train_wall=441, gb_free=10.1, wall=8549
2022-03-03 23:06:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:06:36 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.368 | ppl 330.42 | wps 33788.1 | wpb 2034.1 | bsz 4 | num_updates 1960 | best_loss 8.368
2022-03-03 23:06:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1960 updates
2022-03-03 23:06:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-03 23:06:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-03 23:06:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 5 @ 1960 updates, score 8.368) (writing took 4.392495543695986 seconds)
2022-03-03 23:06:40 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-03 23:06:40 | INFO | train | epoch 005 | loss 8.561 | ppl 377.55 | wps 14594.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1960 | lr 0.000245051 | gnorm 0.81 | loss_scale 32 | train_wall 1733 | gb_free 10.1 | wall 8825
2022-03-03 23:06:40 | INFO | fairseq.trainer | begin training epoch 6
2022-03-03 23:06:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:09:39 | INFO | train_inner | epoch 006:     40 / 393 loss=8.373, ppl=331.48, wps=14337.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.793, loss_scale=32, train_wall=439, gb_free=10.1, wall=9004
2022-03-03 23:13:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 23:17:09 | INFO | train_inner | epoch 006:    141 / 393 loss=8.264, ppl=307.38, wps=14541.5, ups=0.22, wpb=65530.2, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.776, loss_scale=32, train_wall=446, gb_free=10.1, wall=9454
2022-03-03 23:24:36 | INFO | train_inner | epoch 006:    241 / 393 loss=8.211, ppl=296.28, wps=14685.6, ups=0.22, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.773, loss_scale=32, train_wall=441, gb_free=10.1, wall=9901
2022-03-03 23:32:02 | INFO | train_inner | epoch 006:    341 / 393 loss=8.135, ppl=281.04, wps=14680.7, ups=0.22, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.742, loss_scale=32, train_wall=442, gb_free=10.1, wall=10347
2022-03-03 23:35:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:35:59 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.093 | ppl 272.97 | wps 33710 | wpb 2034.1 | bsz 4 | num_updates 2352 | best_loss 8.093
2022-03-03 23:35:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2352 updates
2022-03-03 23:35:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-03 23:36:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-03 23:36:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 6 @ 2352 updates, score 8.093) (writing took 4.394252143800259 seconds)
2022-03-03 23:36:03 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-03 23:36:03 | INFO | train | epoch 006 | loss 8.199 | ppl 293.95 | wps 14556.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 2352 | lr 0.000294041 | gnorm 0.766 | loss_scale 32 | train_wall 1733 | gb_free 10.1 | wall 10588
2022-03-03 23:36:03 | INFO | fairseq.trainer | begin training epoch 7
2022-03-03 23:36:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:39:37 | INFO | train_inner | epoch 007:     48 / 393 loss=8.032, ppl=261.72, wps=14327.1, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.743, loss_scale=32, train_wall=440, gb_free=10.1, wall=10802
2022-03-03 23:47:04 | INFO | train_inner | epoch 007:    148 / 393 loss=7.941, ppl=245.74, wps=14680, ups=0.22, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.723, loss_scale=32, train_wall=442, gb_free=10.1, wall=11249
2022-03-03 23:52:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 23:54:35 | INFO | train_inner | epoch 007:    249 / 393 loss=7.892, ppl=237.56, wps=14540.5, ups=0.22, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.726, loss_scale=32, train_wall=446, gb_free=10.1, wall=11700
2022-03-03 23:56:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 00:02:05 | INFO | train_inner | epoch 007:    350 / 393 loss=7.863, ppl=232.85, wps=14543.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.729, loss_scale=16, train_wall=446, gb_free=10.1, wall=12150
2022-03-04 00:05:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:05:22 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.872 | ppl 234.34 | wps 33985.9 | wpb 2034.1 | bsz 4 | num_updates 2743 | best_loss 7.872
2022-03-04 00:05:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2743 updates
2022-03-04 00:05:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 00:05:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 00:05:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 7 @ 2743 updates, score 7.872) (writing took 4.397848612628877 seconds)
2022-03-04 00:05:26 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 00:05:26 | INFO | train | epoch 007 | loss 7.897 | ppl 238.29 | wps 14518.2 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 2743 | lr 0.000342906 | gnorm 0.721 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 12351
2022-03-04 00:05:26 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 00:05:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:09:40 | INFO | train_inner | epoch 008:     57 / 393 loss=7.742, ppl=214.07, wps=14330.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.705, loss_scale=16, train_wall=440, gb_free=10.1, wall=12605
2022-03-04 00:17:07 | INFO | train_inner | epoch 008:    157 / 393 loss=7.668, ppl=203.43, wps=14691.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.711, loss_scale=16, train_wall=441, gb_free=10.1, wall=13052
2022-03-04 00:24:33 | INFO | train_inner | epoch 008:    257 / 393 loss=7.64, ppl=199.41, wps=14688.4, ups=0.22, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.674, loss_scale=16, train_wall=441, gb_free=10.1, wall=13498
2022-03-04 00:31:59 | INFO | train_inner | epoch 008:    357 / 393 loss=7.616, ppl=196.21, wps=14687.3, ups=0.22, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.666, loss_scale=16, train_wall=441, gb_free=10.1, wall=13944
2022-03-04 00:34:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:34:44 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.705 | ppl 208.62 | wps 33969.5 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 7.705
2022-03-04 00:34:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3136 updates
2022-03-04 00:34:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 00:34:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 00:34:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 8 @ 3136 updates, score 7.705) (writing took 4.43325225263834 seconds)
2022-03-04 00:34:49 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 00:34:49 | INFO | train | epoch 008 | loss 7.644 | ppl 200 | wps 14596.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 3136 | lr 0.000392022 | gnorm 0.69 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 14113
2022-03-04 00:34:49 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 00:34:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:36:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 00:39:39 | INFO | train_inner | epoch 009:     65 / 393 loss=7.497, ppl=180.6, wps=14187.5, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.688, loss_scale=16, train_wall=444, gb_free=10.1, wall=14404
2022-03-04 00:47:05 | INFO | train_inner | epoch 009:    165 / 393 loss=7.445, ppl=174.26, wps=14687.1, ups=0.22, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.67, loss_scale=16, train_wall=441, gb_free=10.1, wall=14850
2022-03-04 00:54:31 | INFO | train_inner | epoch 009:    265 / 393 loss=7.434, ppl=172.9, wps=14689.8, ups=0.22, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.672, loss_scale=16, train_wall=441, gb_free=10.1, wall=15296
2022-03-04 01:01:57 | INFO | train_inner | epoch 009:    365 / 393 loss=7.415, ppl=170.61, wps=14690.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.647, loss_scale=16, train_wall=441, gb_free=10.1, wall=15742
2022-03-04 01:04:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:04:07 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.572 | ppl 190.26 | wps 34006.2 | wpb 2034.1 | bsz 4 | num_updates 3528 | best_loss 7.572
2022-03-04 01:04:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3528 updates
2022-03-04 01:04:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 01:04:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 01:04:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 9 @ 3528 updates, score 7.572) (writing took 4.407784438692033 seconds)
2022-03-04 01:04:11 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 01:04:11 | INFO | train | epoch 009 | loss 7.432 | ppl 172.66 | wps 14558 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3528 | lr 0.000441012 | gnorm 0.666 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 15876
2022-03-04 01:04:11 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 01:04:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:09:32 | INFO | train_inner | epoch 010:     72 / 393 loss=7.288, ppl=156.28, wps=14332.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.653, loss_scale=16, train_wall=439, gb_free=10.1, wall=16197
2022-03-04 01:15:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 01:17:03 | INFO | train_inner | epoch 010:    173 / 393 loss=7.257, ppl=152.99, wps=14541.9, ups=0.22, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.647, loss_scale=16, train_wall=446, gb_free=10.1, wall=16648
2022-03-04 01:18:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 01:24:34 | INFO | train_inner | epoch 010:    274 / 393 loss=7.261, ppl=153.35, wps=14546.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.655, loss_scale=8, train_wall=446, gb_free=10.1, wall=17099
2022-03-04 01:32:00 | INFO | train_inner | epoch 010:    374 / 393 loss=7.254, ppl=152.65, wps=14688.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.643, loss_scale=8, train_wall=441, gb_free=10.1, wall=17545
2022-03-04 01:33:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:33:29 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.473 | ppl 177.62 | wps 33932.2 | wpb 2034.1 | bsz 4 | num_updates 3919 | best_loss 7.473
2022-03-04 01:33:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3919 updates
2022-03-04 01:33:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 01:33:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 01:33:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 10 @ 3919 updates, score 7.473) (writing took 4.4632007190957665 seconds)
2022-03-04 01:33:34 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 01:33:34 | INFO | train | epoch 010 | loss 7.255 | ppl 152.71 | wps 14522.4 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 3919 | lr 0.000489877 | gnorm 0.646 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 17639
2022-03-04 01:33:34 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 01:33:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:39:35 | INFO | train_inner | epoch 011:     81 / 393 loss=7.115, ppl=138.65, wps=14330, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.634, loss_scale=8, train_wall=439, gb_free=10.1, wall=18000
2022-03-04 01:47:01 | INFO | train_inner | epoch 011:    181 / 393 loss=7.101, ppl=137.3, wps=14688, ups=0.22, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.635, loss_scale=8, train_wall=441, gb_free=10.1, wall=18446
2022-03-04 01:54:27 | INFO | train_inner | epoch 011:    281 / 393 loss=7.103, ppl=137.46, wps=14694.3, ups=0.22, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.612, loss_scale=8, train_wall=441, gb_free=10.1, wall=18892
2022-03-04 02:01:54 | INFO | train_inner | epoch 011:    381 / 393 loss=7.099, ppl=137.1, wps=14686.1, ups=0.22, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.603, loss_scale=16, train_wall=441, gb_free=10.1, wall=19339
2022-03-04 02:02:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:02:52 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.391 | ppl 167.88 | wps 33991.9 | wpb 2034.1 | bsz 4 | num_updates 4312 | best_loss 7.391
2022-03-04 02:02:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4312 updates
2022-03-04 02:02:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 02:02:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 02:02:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 11 @ 4312 updates, score 7.391) (writing took 4.375472337007523 seconds)
2022-03-04 02:02:56 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 02:02:56 | INFO | train | epoch 011 | loss 7.097 | ppl 136.87 | wps 14598.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 4312 | lr 0.000481571 | gnorm 0.621 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 19401
2022-03-04 02:02:56 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 02:02:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:09:29 | INFO | train_inner | epoch 012:     88 / 393 loss=6.945, ppl=123.22, wps=14329.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.577, loss_scale=16, train_wall=440, gb_free=10.1, wall=19794
2022-03-04 02:16:55 | INFO | train_inner | epoch 012:    188 / 393 loss=6.942, ppl=122.92, wps=14677.5, ups=0.22, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.592, loss_scale=16, train_wall=442, gb_free=10.1, wall=20240
2022-03-04 02:24:22 | INFO | train_inner | epoch 012:    288 / 393 loss=6.958, ppl=124.33, wps=14679.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.562, loss_scale=16, train_wall=442, gb_free=10.1, wall=20687
2022-03-04 02:31:48 | INFO | train_inner | epoch 012:    388 / 393 loss=6.959, ppl=124.39, wps=14689.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.588, loss_scale=16, train_wall=441, gb_free=10.1, wall=21133
2022-03-04 02:32:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:32:15 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.331 | ppl 161.04 | wps 34022.8 | wpb 2034.1 | bsz 4 | num_updates 4705 | best_loss 7.331
2022-03-04 02:32:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4705 updates
2022-03-04 02:32:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 02:32:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 02:32:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 12 @ 4705 updates, score 7.331) (writing took 4.390037961304188 seconds)
2022-03-04 02:32:19 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 02:32:19 | INFO | train | epoch 012 | loss 6.947 | ppl 123.42 | wps 14590.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 4705 | lr 0.00046102 | gnorm 0.58 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 21164
2022-03-04 02:32:19 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 02:32:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:35:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 02:39:28 | INFO | train_inner | epoch 013:     96 / 393 loss=6.802, ppl=111.55, wps=14192.8, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.55, loss_scale=16, train_wall=444, gb_free=10.1, wall=21593
2022-03-04 02:46:54 | INFO | train_inner | epoch 013:    196 / 393 loss=6.818, ppl=112.81, wps=14682.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.578, loss_scale=16, train_wall=441, gb_free=10.1, wall=22039
2022-03-04 02:54:20 | INFO | train_inner | epoch 013:    296 / 393 loss=6.83, ppl=113.74, wps=14683.7, ups=0.22, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.567, loss_scale=16, train_wall=441, gb_free=10.1, wall=22485
2022-03-04 03:01:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:01:38 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.284 | ppl 155.88 | wps 34004.1 | wpb 2034.1 | bsz 4 | num_updates 5097 | best_loss 7.284
2022-03-04 03:01:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5097 updates
2022-03-04 03:01:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 03:01:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 03:01:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 13 @ 5097 updates, score 7.284) (writing took 4.423934388905764 seconds)
2022-03-04 03:01:42 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 03:01:42 | INFO | train | epoch 013 | loss 6.822 | ppl 113.16 | wps 14556.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5097 | lr 0.000442938 | gnorm 0.565 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 22927
2022-03-04 03:01:42 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 03:01:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:01:56 | INFO | train_inner | epoch 014:      3 / 393 loss=6.843, ppl=114.82, wps=14334.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.566, loss_scale=16, train_wall=439, gb_free=10.1, wall=22941
2022-03-04 03:09:22 | INFO | train_inner | epoch 014:    103 / 393 loss=6.682, ppl=102.7, wps=14686.9, ups=0.22, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.566, loss_scale=16, train_wall=441, gb_free=10.1, wall=23387
2022-03-04 03:14:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 03:16:53 | INFO | train_inner | epoch 014:    204 / 393 loss=6.71, ppl=104.67, wps=14537.2, ups=0.22, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.557, loss_scale=16, train_wall=446, gb_free=10.1, wall=23838
2022-03-04 03:24:19 | INFO | train_inner | epoch 014:    304 / 393 loss=6.731, ppl=106.22, wps=14687.4, ups=0.22, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.542, loss_scale=16, train_wall=441, gb_free=10.1, wall=24284
2022-03-04 03:30:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:31:00 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.259 | ppl 153.2 | wps 33981.1 | wpb 2034.1 | bsz 4 | num_updates 5489 | best_loss 7.259
2022-03-04 03:31:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5489 updates
2022-03-04 03:31:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 03:31:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 03:31:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 14 @ 5489 updates, score 7.259) (writing took 4.137380219064653 seconds)
2022-03-04 03:31:05 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 03:31:05 | INFO | train | epoch 014 | loss 6.716 | ppl 105.15 | wps 14558.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5489 | lr 0.000426828 | gnorm 0.556 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 24690
2022-03-04 03:31:05 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 03:31:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:31:54 | INFO | train_inner | epoch 015:     11 / 393 loss=6.729, ppl=106.05, wps=14339.2, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.566, loss_scale=16, train_wall=440, gb_free=10.1, wall=24739
2022-03-04 03:39:20 | INFO | train_inner | epoch 015:    111 / 393 loss=6.581, ppl=95.75, wps=14685.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.55, loss_scale=16, train_wall=441, gb_free=10.1, wall=25185
2022-03-04 03:46:46 | INFO | train_inner | epoch 015:    211 / 393 loss=6.62, ppl=98.35, wps=14683.3, ups=0.22, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.558, loss_scale=16, train_wall=441, gb_free=10.1, wall=25631
2022-03-04 03:53:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 03:54:17 | INFO | train_inner | epoch 015:    312 / 393 loss=6.651, ppl=100.48, wps=14538.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.575, loss_scale=16, train_wall=446, gb_free=10.1, wall=26082
2022-03-04 04:00:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:00:23 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.232 | ppl 150.37 | wps 34011.5 | wpb 2034.1 | bsz 4 | num_updates 5881 | best_loss 7.232
2022-03-04 04:00:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5881 updates
2022-03-04 04:00:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 04:00:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 04:00:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 15 @ 5881 updates, score 7.232) (writing took 4.287686388939619 seconds)
2022-03-04 04:00:27 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 04:00:27 | INFO | train | epoch 015 | loss 6.625 | ppl 98.67 | wps 14556.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5881 | lr 0.000412358 | gnorm 0.56 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 26452
2022-03-04 04:00:27 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 04:00:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:01:52 | INFO | train_inner | epoch 016:     19 / 393 loss=6.625, ppl=98.68, wps=14332.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.535, loss_scale=16, train_wall=440, gb_free=10.1, wall=26537
2022-03-04 04:09:19 | INFO | train_inner | epoch 016:    119 / 393 loss=6.5, ppl=90.48, wps=14682, ups=0.22, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.558, loss_scale=16, train_wall=441, gb_free=10.1, wall=26984
2022-03-04 04:16:45 | INFO | train_inner | epoch 016:    219 / 393 loss=6.542, ppl=93.17, wps=14679.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.553, loss_scale=16, train_wall=441, gb_free=10.1, wall=27430
2022-03-04 04:24:12 | INFO | train_inner | epoch 016:    319 / 393 loss=6.568, ppl=94.88, wps=14681.8, ups=0.22, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.545, loss_scale=16, train_wall=441, gb_free=10.1, wall=27877
2022-03-04 04:29:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:29:46 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.226 | ppl 149.75 | wps 33923.3 | wpb 2034.1 | bsz 4 | num_updates 6274 | best_loss 7.226
2022-03-04 04:29:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6274 updates
2022-03-04 04:29:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 04:29:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 04:29:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 16 @ 6274 updates, score 7.226) (writing took 4.270441710948944 seconds)
2022-03-04 04:29:50 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 04:29:50 | INFO | train | epoch 016 | loss 6.544 | ppl 93.3 | wps 14592.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 6274 | lr 0.000399234 | gnorm 0.55 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 28215
2022-03-04 04:29:50 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 04:29:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:31:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 04:31:51 | INFO | train_inner | epoch 017:     27 / 393 loss=6.54, ppl=93.03, wps=14201.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.564, loss_scale=16, train_wall=444, gb_free=10.1, wall=28336
2022-03-04 04:39:17 | INFO | train_inner | epoch 017:    127 / 393 loss=6.437, ppl=86.63, wps=14686.3, ups=0.22, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.546, loss_scale=16, train_wall=441, gb_free=10.1, wall=28782
2022-03-04 04:46:44 | INFO | train_inner | epoch 017:    227 / 393 loss=6.472, ppl=88.76, wps=14679.5, ups=0.22, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.574, loss_scale=16, train_wall=442, gb_free=10.1, wall=29229
2022-03-04 04:54:10 | INFO | train_inner | epoch 017:    327 / 393 loss=6.489, ppl=89.83, wps=14684.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.564, loss_scale=16, train_wall=441, gb_free=10.1, wall=29675
2022-03-04 04:59:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:59:09 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.216 | ppl 148.69 | wps 34046.1 | wpb 2034.1 | bsz 4 | num_updates 6666 | best_loss 7.216
2022-03-04 04:59:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6666 updates
2022-03-04 04:59:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 04:59:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 04:59:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 17 @ 6666 updates, score 7.216) (writing took 4.252235418185592 seconds)
2022-03-04 04:59:13 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 04:59:13 | INFO | train | epoch 017 | loss 6.471 | ppl 88.68 | wps 14558.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6666 | lr 0.000387318 | gnorm 0.558 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 29978
2022-03-04 04:59:13 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 04:59:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:01:45 | INFO | train_inner | epoch 018:     34 / 393 loss=6.459, ppl=88, wps=14344.1, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.55, loss_scale=16, train_wall=439, gb_free=10.1, wall=30130
2022-03-04 05:08:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 05:09:16 | INFO | train_inner | epoch 018:    135 / 393 loss=6.369, ppl=82.65, wps=14537.1, ups=0.22, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.555, loss_scale=8, train_wall=446, gb_free=10.1, wall=30581
2022-03-04 05:16:42 | INFO | train_inner | epoch 018:    235 / 393 loss=6.4, ppl=84.45, wps=14691.3, ups=0.22, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.532, loss_scale=8, train_wall=441, gb_free=10.1, wall=31027
2022-03-04 05:24:08 | INFO | train_inner | epoch 018:    335 / 393 loss=6.438, ppl=86.68, wps=14694.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.563, loss_scale=8, train_wall=441, gb_free=10.1, wall=31473
2022-03-04 05:28:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:28:31 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.212 | ppl 148.23 | wps 33897.9 | wpb 2034.1 | bsz 4 | num_updates 7058 | best_loss 7.212
2022-03-04 05:28:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7058 updates
2022-03-04 05:28:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 05:28:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-04 05:28:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 18 @ 7058 updates, score 7.212) (writing took 4.2723689349368215 seconds)
2022-03-04 05:28:35 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 05:28:35 | INFO | train | epoch 018 | loss 6.405 | ppl 84.76 | wps 14562.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7058 | lr 0.000376408 | gnorm 0.553 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 31740
2022-03-04 05:28:35 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 05:28:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:31:43 | INFO | train_inner | epoch 019:     42 / 393 loss=6.379, ppl=83.23, wps=14342.9, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.539, loss_scale=8, train_wall=439, gb_free=10.1, wall=31928
2022-03-04 05:39:09 | INFO | train_inner | epoch 019:    142 / 393 loss=6.31, ppl=79.37, wps=14687.5, ups=0.22, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.565, loss_scale=8, train_wall=441, gb_free=10.1, wall=32374
2022-03-04 05:46:35 | INFO | train_inner | epoch 019:    242 / 393 loss=6.348, ppl=81.44, wps=14690.2, ups=0.22, wpb=65530.2, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.564, loss_scale=8, train_wall=441, gb_free=10.1, wall=32820
2022-03-04 05:54:01 | INFO | train_inner | epoch 019:    342 / 393 loss=6.378, ppl=83.18, wps=14680.1, ups=0.22, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.577, loss_scale=16, train_wall=442, gb_free=10.1, wall=33266
2022-03-04 05:57:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:57:53 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.22 | ppl 149.12 | wps 33972.9 | wpb 2034.1 | bsz 4 | num_updates 7451 | best_loss 7.212
2022-03-04 05:57:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7451 updates
2022-03-04 05:57:53 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 05:57:53 | INFO | train | epoch 019 | loss 6.345 | ppl 81.31 | wps 14632.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 7451 | lr 0.000366347 | gnorm 0.56 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 33498
2022-03-04 05:57:53 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 05:57:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:01:32 | INFO | train_inner | epoch 020:     49 / 393 loss=6.313, ppl=79.48, wps=14476.3, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.557, loss_scale=16, train_wall=439, gb_free=10.1, wall=33717
2022-03-04 06:08:58 | INFO | train_inner | epoch 020:    149 / 393 loss=6.253, ppl=76.26, wps=14680, ups=0.22, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.557, loss_scale=16, train_wall=442, gb_free=10.1, wall=34163
2022-03-04 06:16:25 | INFO | train_inner | epoch 020:    249 / 393 loss=6.295, ppl=78.52, wps=14679.1, ups=0.22, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.568, loss_scale=16, train_wall=442, gb_free=10.1, wall=34610
2022-03-04 06:23:51 | INFO | train_inner | epoch 020:    349 / 393 loss=6.331, ppl=80.48, wps=14689, ups=0.22, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.567, loss_scale=16, train_wall=441, gb_free=10.1, wall=35056
2022-03-04 06:25:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 06:27:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:27:12 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.22 | ppl 149.11 | wps 33910.4 | wpb 2034.1 | bsz 4 | num_updates 7843 | best_loss 7.212
2022-03-04 06:27:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7843 updates
2022-03-04 06:27:12 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 06:27:12 | INFO | train | epoch 020 | loss 6.29 | ppl 78.24 | wps 14591.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7843 | lr 0.000357075 | gnorm 0.568 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 35257
2022-03-04 06:27:12 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 06:27:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:31:26 | INFO | train_inner | epoch 021:     57 / 393 loss=6.248, ppl=75.99, wps=14331, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.548, loss_scale=16, train_wall=444, gb_free=10.1, wall=35511
2022-03-04 06:38:52 | INFO | train_inner | epoch 021:    157 / 393 loss=6.201, ppl=73.58, wps=14689.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.586, loss_scale=16, train_wall=441, gb_free=10.1, wall=35957
2022-03-04 06:46:19 | INFO | train_inner | epoch 021:    257 / 393 loss=6.249, ppl=76.04, wps=14692.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.562, loss_scale=16, train_wall=441, gb_free=10.1, wall=36404
2022-03-04 06:53:45 | INFO | train_inner | epoch 021:    357 / 393 loss=6.283, ppl=77.89, wps=14690.6, ups=0.22, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.552, loss_scale=16, train_wall=441, gb_free=10.1, wall=36850
2022-03-04 06:54:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 06:56:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:56:30 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.227 | ppl 149.83 | wps 33918.3 | wpb 2034.1 | bsz 4 | num_updates 8235 | best_loss 7.212
2022-03-04 06:56:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8235 updates
2022-03-04 06:56:30 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 06:56:30 | INFO | train | epoch 021 | loss 6.239 | ppl 75.51 | wps 14599.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8235 | lr 0.000348472 | gnorm 0.562 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 37015
2022-03-04 06:56:30 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 06:56:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:01:20 | INFO | train_inner | epoch 022:     65 / 393 loss=6.186, ppl=72.8, wps=14342.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.586, loss_scale=8, train_wall=444, gb_free=10.1, wall=37305
2022-03-04 07:08:46 | INFO | train_inner | epoch 022:    165 / 393 loss=6.16, ppl=71.5, wps=14689.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.574, loss_scale=8, train_wall=441, gb_free=10.1, wall=37751
2022-03-04 07:16:12 | INFO | train_inner | epoch 022:    265 / 393 loss=6.201, ppl=73.57, wps=14690.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.577, loss_scale=8, train_wall=441, gb_free=10.1, wall=38197
2022-03-04 07:23:38 | INFO | train_inner | epoch 022:    365 / 393 loss=6.242, ppl=75.7, wps=14691, ups=0.22, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.59, loss_scale=8, train_wall=441, gb_free=10.1, wall=38643
2022-03-04 07:25:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:25:47 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.228 | ppl 149.92 | wps 33850.1 | wpb 2034.1 | bsz 4 | num_updates 8628 | best_loss 7.212
2022-03-04 07:25:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8628 updates
2022-03-04 07:25:47 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 07:25:47 | INFO | train | epoch 022 | loss 6.192 | ppl 73.12 | wps 14637 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 8628 | lr 0.000340443 | gnorm 0.582 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 38772
2022-03-04 07:25:47 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 07:25:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:31:09 | INFO | train_inner | epoch 023:     72 / 393 loss=6.126, ppl=69.82, wps=14475.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.579, loss_scale=8, train_wall=439, gb_free=10.1, wall=39094
2022-03-04 07:38:35 | INFO | train_inner | epoch 023:    172 / 393 loss=6.114, ppl=69.24, wps=14684.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.561, loss_scale=16, train_wall=441, gb_free=10.1, wall=39540
2022-03-04 07:40:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 07:46:06 | INFO | train_inner | epoch 023:    273 / 393 loss=6.166, ppl=71.78, wps=14541.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.587, loss_scale=8, train_wall=446, gb_free=10.1, wall=39991
2022-03-04 07:53:32 | INFO | train_inner | epoch 023:    373 / 393 loss=6.2, ppl=73.54, wps=14682.4, ups=0.22, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.593, loss_scale=8, train_wall=442, gb_free=10.1, wall=40437
2022-03-04 07:54:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:55:06 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.245 | ppl 151.74 | wps 33921.2 | wpb 2034.1 | bsz 4 | num_updates 9020 | best_loss 7.212
2022-03-04 07:55:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9020 updates
2022-03-04 07:55:06 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 07:55:06 | INFO | train | epoch 023 | loss 6.147 | ppl 70.89 | wps 14594 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 9020 | lr 0.000332964 | gnorm 0.579 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 40531
2022-03-04 07:55:06 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 07:55:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:01:02 | INFO | train_inner | epoch 024:     80 / 393 loss=6.081, ppl=67.7, wps=14482.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.584, loss_scale=8, train_wall=439, gb_free=10.1, wall=40887
2022-03-04 08:08:29 | INFO | train_inner | epoch 024:    180 / 393 loss=6.076, ppl=67.47, wps=14684.7, ups=0.22, wpb=65530.2, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.578, loss_scale=8, train_wall=441, gb_free=10.1, wall=41334
2022-03-04 08:15:55 | INFO | train_inner | epoch 024:    280 / 393 loss=6.12, ppl=69.56, wps=14688.6, ups=0.22, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.577, loss_scale=8, train_wall=441, gb_free=10.1, wall=41780
2022-03-04 08:23:21 | INFO | train_inner | epoch 024:    380 / 393 loss=6.168, ppl=71.92, wps=14688.7, ups=0.22, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.577, loss_scale=16, train_wall=441, gb_free=10.1, wall=42226
2022-03-04 08:24:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:24:23 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.256 | ppl 152.85 | wps 34053.1 | wpb 2034.1 | bsz 4 | num_updates 9413 | best_loss 7.212
2022-03-04 08:24:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9413 updates
2022-03-04 08:24:23 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 08:24:23 | INFO | train | epoch 024 | loss 6.107 | ppl 68.92 | wps 14634.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9413 | lr 0.000325939 | gnorm 0.579 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 42288
2022-03-04 08:24:23 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 08:24:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:30:52 | INFO | train_inner | epoch 025:     87 / 393 loss=6.015, ppl=64.69, wps=14475.1, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.572, loss_scale=16, train_wall=439, gb_free=10.1, wall=42677
2022-03-04 08:35:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 08:38:22 | INFO | train_inner | epoch 025:    188 / 393 loss=6.05, ppl=66.24, wps=14546.6, ups=0.22, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.603, loss_scale=8, train_wall=446, gb_free=10.1, wall=43127
2022-03-04 08:45:48 | INFO | train_inner | epoch 025:    288 / 393 loss=6.086, ppl=67.92, wps=14690, ups=0.22, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.59, loss_scale=8, train_wall=441, gb_free=10.1, wall=43573
2022-03-04 08:53:14 | INFO | train_inner | epoch 025:    388 / 393 loss=6.125, ppl=69.78, wps=14696.8, ups=0.22, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.611, loss_scale=8, train_wall=441, gb_free=10.1, wall=44019
2022-03-04 08:53:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:53:41 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.27 | ppl 154.34 | wps 33943.3 | wpb 2034.1 | bsz 4 | num_updates 9805 | best_loss 7.212
2022-03-04 08:53:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9805 updates
2022-03-04 08:53:41 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 08:53:41 | INFO | train | epoch 025 | loss 6.068 | ppl 67.07 | wps 14599.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 9805 | lr 0.000319357 | gnorm 0.595 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 44046
2022-03-04 08:53:41 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 08:53:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:00:45 | INFO | train_inner | epoch 026:     95 / 393 loss=5.973, ppl=62.79, wps=14483.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.576, loss_scale=8, train_wall=439, gb_free=10.1, wall=44470
2022-03-04 09:08:11 | INFO | train_inner | epoch 026:    195 / 393 loss=6.017, ppl=64.78, wps=14700.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.581, loss_scale=8, train_wall=441, gb_free=10.1, wall=44916
2022-03-04 09:15:37 | INFO | train_inner | epoch 026:    295 / 393 loss=6.056, ppl=66.51, wps=14692.1, ups=0.22, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.593, loss_scale=16, train_wall=441, gb_free=10.1, wall=45362
2022-03-04 09:22:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:22:58 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.279 | ppl 155.27 | wps 33906.9 | wpb 2034.1 | bsz 4 | num_updates 10198 | best_loss 7.212
2022-03-04 09:22:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10198 updates
2022-03-04 09:22:58 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 09:22:58 | INFO | train | epoch 026 | loss 6.031 | ppl 65.39 | wps 14640.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 10198 | lr 0.000313143 | gnorm 0.583 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 45803
2022-03-04 09:22:58 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 09:22:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:23:07 | INFO | train_inner | epoch 027:      2 / 393 loss=6.081, ppl=67.72, wps=14475.9, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.581, loss_scale=16, train_wall=439, gb_free=10.1, wall=45812
2022-03-04 09:23:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:30:38 | INFO | train_inner | epoch 027:    103 / 393 loss=5.929, ppl=60.92, wps=14543.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.624, loss_scale=8, train_wall=446, gb_free=10.1, wall=46263
2022-03-04 09:38:04 | INFO | train_inner | epoch 027:    203 / 393 loss=5.975, ppl=62.88, wps=14692, ups=0.22, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.605, loss_scale=8, train_wall=441, gb_free=10.1, wall=46709
2022-03-04 09:45:30 | INFO | train_inner | epoch 027:    303 / 393 loss=6.025, ppl=65.13, wps=14686.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.594, loss_scale=8, train_wall=441, gb_free=10.1, wall=47155
2022-03-04 09:52:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:52:16 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.305 | ppl 158.15 | wps 33908.8 | wpb 2034.1 | bsz 4 | num_updates 10590 | best_loss 7.212
2022-03-04 09:52:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10590 updates
2022-03-04 09:52:16 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 09:52:16 | INFO | train | epoch 027 | loss 5.997 | ppl 63.87 | wps 14596.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10590 | lr 0.000307293 | gnorm 0.607 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 47561
2022-03-04 09:52:16 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 09:52:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:53:01 | INFO | train_inner | epoch 028:     10 / 393 loss=6.05, ppl=66.24, wps=14473.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.6, loss_scale=8, train_wall=439, gb_free=10.1, wall=47606
2022-03-04 10:00:27 | INFO | train_inner | epoch 028:    110 / 393 loss=5.903, ppl=59.86, wps=14688.3, ups=0.22, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.599, loss_scale=8, train_wall=441, gb_free=10.1, wall=48052
2022-03-04 10:07:54 | INFO | train_inner | epoch 028:    210 / 393 loss=5.95, ppl=61.82, wps=14679.8, ups=0.22, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.601, loss_scale=16, train_wall=442, gb_free=10.1, wall=48499
2022-03-04 10:09:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 10:15:24 | INFO | train_inner | epoch 028:    311 / 393 loss=5.989, ppl=63.5, wps=14542.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.617, loss_scale=8, train_wall=446, gb_free=10.1, wall=48949
2022-03-04 10:21:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:21:35 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.298 | ppl 157.33 | wps 33836.2 | wpb 2034.1 | bsz 4 | num_updates 10982 | best_loss 7.212
2022-03-04 10:21:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10982 updates
2022-03-04 10:21:35 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 10:21:35 | INFO | train | epoch 028 | loss 5.964 | ppl 62.43 | wps 14594.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10982 | lr 0.000301758 | gnorm 0.604 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 49320
2022-03-04 10:21:35 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 10:21:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:22:55 | INFO | train_inner | epoch 029:     18 / 393 loss=6.004, ppl=64.16, wps=14473.5, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.607, loss_scale=8, train_wall=439, gb_free=10.1, wall=49400
2022-03-04 10:30:21 | INFO | train_inner | epoch 029:    118 / 393 loss=5.886, ppl=59.12, wps=14687.4, ups=0.22, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.634, loss_scale=8, train_wall=441, gb_free=10.1, wall=49846
2022-03-04 10:37:48 | INFO | train_inner | epoch 029:    218 / 393 loss=5.93, ppl=60.96, wps=14681.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.611, loss_scale=8, train_wall=441, gb_free=10.1, wall=50293
2022-03-04 10:45:14 | INFO | train_inner | epoch 029:    318 / 393 loss=5.96, ppl=62.23, wps=14699.2, ups=0.22, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.631, loss_scale=8, train_wall=441, gb_free=10.1, wall=50739
2022-03-04 10:49:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 10:50:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:50:52 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.328 | ppl 160.65 | wps 34027 | wpb 2034.1 | bsz 4 | num_updates 11374 | best_loss 7.212
2022-03-04 10:50:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11374 updates
2022-03-04 10:50:52 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 10:50:52 | INFO | train | epoch 029 | loss 5.934 | ppl 61.12 | wps 14598.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 11374 | lr 0.000296513 | gnorm 0.623 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 51077
2022-03-04 10:50:52 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 10:50:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:52:48 | INFO | train_inner | epoch 030:     26 / 393 loss=5.945, ppl=61.6, wps=14342.2, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.61, loss_scale=8, train_wall=444, gb_free=10.1, wall=51193
2022-03-04 11:00:15 | INFO | train_inner | epoch 030:    126 / 393 loss=5.843, ppl=57.39, wps=14688.3, ups=0.22, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.624, loss_scale=8, train_wall=441, gb_free=10.1, wall=51640
2022-03-04 11:07:41 | INFO | train_inner | epoch 030:    226 / 393 loss=5.899, ppl=59.67, wps=14690, ups=0.22, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.604, loss_scale=8, train_wall=441, gb_free=10.1, wall=52086
2022-03-04 11:15:07 | INFO | train_inner | epoch 030:    326 / 393 loss=5.947, ppl=61.68, wps=14684.5, ups=0.22, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.62, loss_scale=8, train_wall=441, gb_free=10.1, wall=52532
2022-03-04 11:20:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:20:10 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.333 | ppl 161.2 | wps 33958 | wpb 2034.1 | bsz 4 | num_updates 11767 | best_loss 7.212
2022-03-04 11:20:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11767 updates
2022-03-04 11:20:10 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 11:20:10 | INFO | train | epoch 030 | loss 5.905 | ppl 59.9 | wps 14633.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 11767 | lr 0.000291519 | gnorm 0.62 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 52835
2022-03-04 11:20:10 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 11:20:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:22:38 | INFO | train_inner | epoch 031:     33 / 393 loss=5.916, ppl=60.38, wps=14479, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.626, loss_scale=8, train_wall=439, gb_free=10.1, wall=52983
2022-03-04 11:27:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 11:30:08 | INFO | train_inner | epoch 031:    134 / 393 loss=5.831, ppl=56.93, wps=14550.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.619, loss_scale=8, train_wall=445, gb_free=10.1, wall=53433
2022-03-04 11:37:34 | INFO | train_inner | epoch 031:    234 / 393 loss=5.878, ppl=58.79, wps=14681.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.612, loss_scale=8, train_wall=441, gb_free=10.1, wall=53879
2022-03-04 11:45:00 | INFO | train_inner | epoch 031:    334 / 393 loss=5.909, ppl=60.1, wps=14693.9, ups=0.22, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.636, loss_scale=8, train_wall=441, gb_free=10.1, wall=54325
2022-03-04 11:49:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:49:28 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.357 | ppl 163.99 | wps 34058.5 | wpb 2034.1 | bsz 4 | num_updates 12159 | best_loss 7.212
2022-03-04 11:49:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12159 updates
2022-03-04 11:49:28 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 11:49:28 | INFO | train | epoch 031 | loss 5.877 | ppl 58.75 | wps 14599.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12159 | lr 0.000286781 | gnorm 0.62 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 54593
2022-03-04 11:49:28 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 11:49:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:52:31 | INFO | train_inner | epoch 032:     41 / 393 loss=5.872, ppl=58.55, wps=14476.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.614, loss_scale=8, train_wall=439, gb_free=10.1, wall=54776
2022-03-04 11:59:57 | INFO | train_inner | epoch 032:    141 / 393 loss=5.811, ppl=56.14, wps=14689.3, ups=0.22, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.652, loss_scale=8, train_wall=441, gb_free=10.1, wall=55222
2022-03-04 12:06:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 12:07:28 | INFO | train_inner | epoch 032:    242 / 393 loss=5.851, ppl=57.71, wps=14540.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.618, loss_scale=8, train_wall=446, gb_free=10.1, wall=55673
2022-03-04 12:14:54 | INFO | train_inner | epoch 032:    342 / 393 loss=5.889, ppl=59.28, wps=14694.9, ups=0.22, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.627, loss_scale=8, train_wall=441, gb_free=10.1, wall=56119
2022-03-04 12:18:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:18:46 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.368 | ppl 165.22 | wps 33989.9 | wpb 2034.1 | bsz 4 | num_updates 12551 | best_loss 7.212
2022-03-04 12:18:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12551 updates
2022-03-04 12:18:46 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 12:18:46 | INFO | train | epoch 032 | loss 5.85 | ppl 57.68 | wps 14597.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12551 | lr 0.000282267 | gnorm 0.634 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 56351
2022-03-04 12:18:46 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 12:18:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:22:25 | INFO | train_inner | epoch 033:     49 / 393 loss=5.829, ppl=56.86, wps=14473.7, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.646, loss_scale=8, train_wall=439, gb_free=10.1, wall=56570
2022-03-04 12:29:51 | INFO | train_inner | epoch 033:    149 / 393 loss=5.778, ppl=54.88, wps=14693.8, ups=0.22, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.608, loss_scale=8, train_wall=441, gb_free=10.1, wall=57016
2022-03-04 12:37:17 | INFO | train_inner | epoch 033:    249 / 393 loss=5.829, ppl=56.84, wps=14687.4, ups=0.22, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.661, loss_scale=8, train_wall=441, gb_free=10.1, wall=57462
2022-03-04 12:44:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 12:44:48 | INFO | train_inner | epoch 033:    350 / 393 loss=5.876, ppl=58.73, wps=14540.8, ups=0.22, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.645, loss_scale=8, train_wall=446, gb_free=10.1, wall=57913
2022-03-04 12:47:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:48:04 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.392 | ppl 167.98 | wps 33890 | wpb 2034.1 | bsz 4 | num_updates 12943 | best_loss 7.212
2022-03-04 12:48:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12943 updates
2022-03-04 12:48:04 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 12:48:04 | INFO | train | epoch 033 | loss 5.825 | ppl 56.68 | wps 14597.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12943 | lr 0.00027796 | gnorm 0.635 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 58109
2022-03-04 12:48:04 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 12:48:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:52:18 | INFO | train_inner | epoch 034:     57 / 393 loss=5.794, ppl=55.49, wps=14473.8, ups=0.22, wpb=65238.4, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.619, loss_scale=8, train_wall=439, gb_free=10.1, wall=58363
2022-03-04 12:59:45 | INFO | train_inner | epoch 034:    157 / 393 loss=5.773, ppl=54.7, wps=14685.4, ups=0.22, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.67, loss_scale=8, train_wall=441, gb_free=10.1, wall=58810
2022-03-04 13:07:11 | INFO | train_inner | epoch 034:    257 / 393 loss=5.805, ppl=55.89, wps=14686.5, ups=0.22, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.646, loss_scale=8, train_wall=441, gb_free=10.1, wall=59256
2022-03-04 13:14:37 | INFO | train_inner | epoch 034:    357 / 393 loss=5.841, ppl=57.34, wps=14689.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.632, loss_scale=8, train_wall=441, gb_free=10.1, wall=59702
2022-03-04 13:17:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:17:22 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.406 | ppl 169.55 | wps 33967.2 | wpb 2034.1 | bsz 4 | num_updates 13336 | best_loss 7.212
2022-03-04 13:17:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13336 updates
2022-03-04 13:17:22 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 13:17:22 | INFO | train | epoch 034 | loss 5.801 | ppl 55.77 | wps 14632.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 13336 | lr 0.000273834 | gnorm 0.642 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 59867
2022-03-04 13:17:22 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 13:17:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:22:08 | INFO | train_inner | epoch 035:     64 / 393 loss=5.767, ppl=54.45, wps=14477.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.661, loss_scale=8, train_wall=439, gb_free=10.1, wall=60153
2022-03-04 13:23:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 13:29:38 | INFO | train_inner | epoch 035:    165 / 393 loss=5.746, ppl=53.66, wps=14548.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.64, loss_scale=8, train_wall=446, gb_free=10.1, wall=60603
2022-03-04 13:37:04 | INFO | train_inner | epoch 035:    265 / 393 loss=5.792, ppl=55.4, wps=14694, ups=0.22, wpb=65530.9, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.649, loss_scale=8, train_wall=441, gb_free=10.1, wall=61049
2022-03-04 13:44:30 | INFO | train_inner | epoch 035:    365 / 393 loss=5.826, ppl=56.74, wps=14691, ups=0.22, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.651, loss_scale=8, train_wall=441, gb_free=10.1, wall=61495
2022-03-04 13:46:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:46:40 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.423 | ppl 171.65 | wps 33983.4 | wpb 2034.1 | bsz 4 | num_updates 13728 | best_loss 7.212
2022-03-04 13:46:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13728 updates
2022-03-04 13:46:40 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 13:46:40 | INFO | train | epoch 035 | loss 5.778 | ppl 54.89 | wps 14600.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 13728 | lr 0.000269896 | gnorm 0.653 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 61625
2022-03-04 13:46:40 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 13:46:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:52:01 | INFO | train_inner | epoch 036:     72 / 393 loss=5.736, ppl=53.3, wps=14485.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.653, loss_scale=8, train_wall=439, gb_free=10.1, wall=61946
2022-03-04 13:59:27 | INFO | train_inner | epoch 036:    172 / 393 loss=5.729, ppl=53.04, wps=14692.2, ups=0.22, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.671, loss_scale=8, train_wall=441, gb_free=10.1, wall=62392
2022-03-04 14:02:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 14:06:57 | INFO | train_inner | epoch 036:    273 / 393 loss=5.772, ppl=54.66, wps=14553, ups=0.22, wpb=65530.2, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.63, loss_scale=8, train_wall=445, gb_free=10.1, wall=62842
2022-03-04 14:14:23 | INFO | train_inner | epoch 036:    373 / 393 loss=5.808, ppl=56.03, wps=14694.3, ups=0.22, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.664, loss_scale=8, train_wall=441, gb_free=10.1, wall=63288
2022-03-04 14:15:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:15:57 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.438 | ppl 173.35 | wps 34058.6 | wpb 2034.1 | bsz 4 | num_updates 14120 | best_loss 7.212
2022-03-04 14:15:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14120 updates
2022-03-04 14:15:57 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 14:15:57 | INFO | train | epoch 036 | loss 5.757 | ppl 54.08 | wps 14604.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14120 | lr 0.000266123 | gnorm 0.653 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 63382
2022-03-04 14:15:57 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 14:15:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:21:54 | INFO | train_inner | epoch 037:     80 / 393 loss=5.696, ppl=51.84, wps=14477.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.639, loss_scale=8, train_wall=439, gb_free=10.1, wall=63739
2022-03-04 14:29:20 | INFO | train_inner | epoch 037:    180 / 393 loss=5.718, ppl=52.63, wps=14686.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.659, loss_scale=8, train_wall=441, gb_free=10.1, wall=64185
2022-03-04 14:36:46 | INFO | train_inner | epoch 037:    280 / 393 loss=5.75, ppl=53.83, wps=14693.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.666, loss_scale=8, train_wall=441, gb_free=10.1, wall=64631
2022-03-04 14:41:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 14:44:16 | INFO | train_inner | epoch 037:    381 / 393 loss=5.787, ppl=55.21, wps=14551.8, ups=0.22, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.661, loss_scale=8, train_wall=445, gb_free=10.1, wall=65081
2022-03-04 14:45:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:45:14 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.442 | ppl 173.89 | wps 34000.4 | wpb 2034.1 | bsz 4 | num_updates 14512 | best_loss 7.212
2022-03-04 14:45:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14512 updates
2022-03-04 14:45:14 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 14:45:14 | INFO | train | epoch 037 | loss 5.737 | ppl 53.32 | wps 14599.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14512 | lr 0.000262504 | gnorm 0.658 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 65139
2022-03-04 14:45:14 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 14:45:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:51:47 | INFO | train_inner | epoch 038:     88 / 393 loss=5.663, ppl=50.66, wps=14478, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.657, loss_scale=8, train_wall=439, gb_free=10.1, wall=65532
2022-03-04 14:59:13 | INFO | train_inner | epoch 038:    188 / 393 loss=5.698, ppl=51.9, wps=14684.4, ups=0.22, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.665, loss_scale=8, train_wall=441, gb_free=10.1, wall=65978
2022-03-04 15:06:40 | INFO | train_inner | epoch 038:    288 / 393 loss=5.74, ppl=53.45, wps=14683.4, ups=0.22, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.649, loss_scale=8, train_wall=441, gb_free=10.1, wall=66425
2022-03-04 15:14:06 | INFO | train_inner | epoch 038:    388 / 393 loss=5.774, ppl=54.73, wps=14690.5, ups=0.22, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.659, loss_scale=8, train_wall=441, gb_free=10.1, wall=66871
2022-03-04 15:14:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:14:33 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.467 | ppl 176.9 | wps 33888.4 | wpb 2034.1 | bsz 4 | num_updates 14905 | best_loss 7.212
2022-03-04 15:14:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14905 updates
2022-03-04 15:14:33 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 15:14:33 | INFO | train | epoch 038 | loss 5.717 | ppl 52.59 | wps 14632.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 14905 | lr 0.00025902 | gnorm 0.66 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 66898
2022-03-04 15:14:33 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 15:14:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:21:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 15:21:41 | INFO | train_inner | epoch 039:     96 / 393 loss=5.64, ppl=49.87, wps=14330.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.662, loss_scale=8, train_wall=444, gb_free=10.1, wall=67326
2022-03-04 15:21:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 15:29:11 | INFO | train_inner | epoch 039:    197 / 393 loss=5.676, ppl=51.13, wps=14549.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.687, loss_scale=4, train_wall=445, gb_free=10.1, wall=67776
2022-03-04 15:36:37 | INFO | train_inner | epoch 039:    297 / 393 loss=5.72, ppl=52.7, wps=14696.7, ups=0.22, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.686, loss_scale=4, train_wall=441, gb_free=10.1, wall=68222
2022-03-04 15:43:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:43:50 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.477 | ppl 178.14 | wps 34006 | wpb 2034.1 | bsz 4 | num_updates 15296 | best_loss 7.212
2022-03-04 15:43:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15296 updates
2022-03-04 15:43:50 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 15:43:50 | INFO | train | epoch 039 | loss 5.697 | ppl 51.88 | wps 14562.8 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 15296 | lr 0.000255688 | gnorm 0.676 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 68655
2022-03-04 15:43:50 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 15:43:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:44:08 | INFO | train_inner | epoch 040:      4 / 393 loss=5.752, ppl=53.88, wps=14477, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.68, loss_scale=4, train_wall=439, gb_free=10.1, wall=68673
2022-03-04 15:51:34 | INFO | train_inner | epoch 040:    104 / 393 loss=5.611, ppl=48.89, wps=14688.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.662, loss_scale=4, train_wall=441, gb_free=10.1, wall=69119
2022-03-04 15:59:00 | INFO | train_inner | epoch 040:    204 / 393 loss=5.658, ppl=50.48, wps=14693.7, ups=0.22, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.646, loss_scale=4, train_wall=441, gb_free=10.1, wall=69565
2022-03-04 16:06:26 | INFO | train_inner | epoch 040:    304 / 393 loss=5.712, ppl=52.41, wps=14689, ups=0.22, wpb=65530.9, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.687, loss_scale=8, train_wall=441, gb_free=10.1, wall=70011
2022-03-04 16:13:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:13:08 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.492 | ppl 180.02 | wps 33944.3 | wpb 2034.1 | bsz 4 | num_updates 15689 | best_loss 7.212
2022-03-04 16:13:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15689 updates
2022-03-04 16:13:08 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 16:13:08 | INFO | train | epoch 040 | loss 5.68 | ppl 51.25 | wps 14636.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 15689 | lr 0.000252466 | gnorm 0.665 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 70413
2022-03-04 16:13:08 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 16:13:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:13:57 | INFO | train_inner | epoch 041:     11 / 393 loss=5.73, ppl=53.09, wps=14478.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.673, loss_scale=8, train_wall=439, gb_free=10.1, wall=70462
2022-03-04 16:21:23 | INFO | train_inner | epoch 041:    111 / 393 loss=5.603, ppl=48.6, wps=14685.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.68, loss_scale=8, train_wall=441, gb_free=10.1, wall=70908
2022-03-04 16:28:49 | INFO | train_inner | epoch 041:    211 / 393 loss=5.649, ppl=50.17, wps=14691.1, ups=0.22, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.674, loss_scale=8, train_wall=441, gb_free=10.1, wall=71354
2022-03-04 16:36:16 | INFO | train_inner | epoch 041:    311 / 393 loss=5.694, ppl=51.78, wps=14685.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.682, loss_scale=8, train_wall=441, gb_free=10.1, wall=71801
2022-03-04 16:38:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:42:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:42:26 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.498 | ppl 180.81 | wps 33835.3 | wpb 2034.1 | bsz 4 | num_updates 16081 | best_loss 7.212
2022-03-04 16:42:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16081 updates
2022-03-04 16:42:26 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 16:42:26 | INFO | train | epoch 041 | loss 5.662 | ppl 50.62 | wps 14594.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16081 | lr 0.00024937 | gnorm 0.679 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 72171
2022-03-04 16:42:26 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 16:42:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:43:51 | INFO | train_inner | epoch 042:     19 / 393 loss=5.689, ppl=51.59, wps=14330.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.672, loss_scale=8, train_wall=444, gb_free=10.1, wall=72256
2022-03-04 16:51:17 | INFO | train_inner | epoch 042:    119 / 393 loss=5.579, ppl=47.79, wps=14684.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.678, loss_scale=8, train_wall=441, gb_free=10.1, wall=72702
2022-03-04 16:58:44 | INFO | train_inner | epoch 042:    219 / 393 loss=5.642, ppl=49.94, wps=14686.1, ups=0.22, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.675, loss_scale=8, train_wall=441, gb_free=10.1, wall=73149
2022-03-04 17:06:10 | INFO | train_inner | epoch 042:    319 / 393 loss=5.682, ppl=51.35, wps=14687.5, ups=0.22, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.684, loss_scale=8, train_wall=441, gb_free=10.1, wall=73595
2022-03-04 17:11:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:11:44 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.497 | ppl 180.68 | wps 34008.1 | wpb 2034.1 | bsz 4 | num_updates 16474 | best_loss 7.212
2022-03-04 17:11:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16474 updates
2022-03-04 17:11:44 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 17:11:44 | INFO | train | epoch 042 | loss 5.645 | ppl 50.05 | wps 14631.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 16474 | lr 0.000246377 | gnorm 0.673 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 73929
2022-03-04 17:11:44 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 17:11:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:13:41 | INFO | train_inner | epoch 043:     26 / 393 loss=5.671, ppl=50.96, wps=14472.9, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.667, loss_scale=8, train_wall=439, gb_free=10.1, wall=74046
2022-03-04 17:18:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:21:11 | INFO | train_inner | epoch 043:    127 / 393 loss=5.579, ppl=47.79, wps=14540, ups=0.22, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.672, loss_scale=8, train_wall=446, gb_free=10.1, wall=74496
2022-03-04 17:28:38 | INFO | train_inner | epoch 043:    227 / 393 loss=5.624, ppl=49.31, wps=14684.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.665, loss_scale=8, train_wall=441, gb_free=10.1, wall=74943
2022-03-04 17:36:04 | INFO | train_inner | epoch 043:    327 / 393 loss=5.666, ppl=50.77, wps=14689.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.681, loss_scale=8, train_wall=441, gb_free=10.1, wall=75389
2022-03-04 17:40:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:41:03 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.529 | ppl 184.74 | wps 34059.3 | wpb 2034.1 | bsz 4 | num_updates 16866 | best_loss 7.212
2022-03-04 17:41:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16866 updates
2022-03-04 17:41:03 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 17:41:03 | INFO | train | epoch 043 | loss 5.63 | ppl 49.52 | wps 14596 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16866 | lr 0.000243497 | gnorm 0.68 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 75688
2022-03-04 17:41:03 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 17:41:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:43:34 | INFO | train_inner | epoch 044:     34 / 393 loss=5.638, ppl=49.79, wps=14481.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.702, loss_scale=8, train_wall=439, gb_free=10.1, wall=75839
2022-03-04 17:51:01 | INFO | train_inner | epoch 044:    134 / 393 loss=5.566, ppl=47.37, wps=14682.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.664, loss_scale=8, train_wall=441, gb_free=10.1, wall=76286
2022-03-04 17:56:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:58:31 | INFO | train_inner | epoch 044:    235 / 393 loss=5.609, ppl=48.81, wps=14537.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.685, loss_scale=8, train_wall=446, gb_free=10.1, wall=76736
2022-03-04 18:05:58 | INFO | train_inner | epoch 044:    335 / 393 loss=5.656, ppl=50.43, wps=14688, ups=0.22, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.693, loss_scale=8, train_wall=441, gb_free=10.1, wall=77183
2022-03-04 18:10:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:10:21 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.537 | ppl 185.74 | wps 33981.7 | wpb 2034.1 | bsz 4 | num_updates 17258 | best_loss 7.212
2022-03-04 18:10:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17258 updates
2022-03-04 18:10:21 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 18:10:21 | INFO | train | epoch 044 | loss 5.614 | ppl 48.98 | wps 14593.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 17258 | lr 0.000240716 | gnorm 0.681 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 77446
2022-03-04 18:10:21 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 18:10:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:13:28 | INFO | train_inner | epoch 045:     42 / 393 loss=5.614, ppl=48.99, wps=14476, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.692, loss_scale=8, train_wall=439, gb_free=10.1, wall=77633
2022-03-04 18:20:54 | INFO | train_inner | epoch 045:    142 / 393 loss=5.557, ppl=47.07, wps=14686.5, ups=0.22, wpb=65530.2, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.67, loss_scale=8, train_wall=441, gb_free=10.1, wall=78079
2022-03-04 18:28:21 | INFO | train_inner | epoch 045:    242 / 393 loss=5.595, ppl=48.34, wps=14687.7, ups=0.22, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.702, loss_scale=8, train_wall=441, gb_free=10.1, wall=78526
2022-03-04 18:35:47 | INFO | train_inner | epoch 045:    342 / 393 loss=5.645, ppl=50.02, wps=14691.3, ups=0.22, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.706, loss_scale=16, train_wall=441, gb_free=10.1, wall=78972
2022-03-04 18:37:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:39:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:39:39 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.542 | ppl 186.4 | wps 33959.3 | wpb 2034.1 | bsz 4 | num_updates 17650 | best_loss 7.212
2022-03-04 18:39:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17650 updates
2022-03-04 18:39:39 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 18:39:39 | INFO | train | epoch 045 | loss 5.599 | ppl 48.47 | wps 14597.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 17650 | lr 0.000238028 | gnorm 0.694 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 79204
2022-03-04 18:39:39 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 18:39:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:43:22 | INFO | train_inner | epoch 046:     50 / 393 loss=5.592, ppl=48.23, wps=14337, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.698, loss_scale=8, train_wall=444, gb_free=10.1, wall=79427
2022-03-04 18:50:48 | INFO | train_inner | epoch 046:    150 / 393 loss=5.539, ppl=46.48, wps=14689.5, ups=0.22, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.683, loss_scale=8, train_wall=441, gb_free=10.1, wall=79873
2022-03-04 18:58:14 | INFO | train_inner | epoch 046:    250 / 393 loss=5.592, ppl=48.24, wps=14697.3, ups=0.22, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.687, loss_scale=8, train_wall=441, gb_free=10.1, wall=80319
2022-03-04 19:05:40 | INFO | train_inner | epoch 046:    350 / 393 loss=5.624, ppl=49.31, wps=14692, ups=0.22, wpb=65535.4, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.706, loss_scale=8, train_wall=441, gb_free=10.1, wall=80765
2022-03-04 19:08:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:08:56 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.557 | ppl 188.37 | wps 33947.2 | wpb 2034.1 | bsz 4 | num_updates 18043 | best_loss 7.212
2022-03-04 19:08:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18043 updates
2022-03-04 19:08:56 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 19:08:56 | INFO | train | epoch 046 | loss 5.586 | ppl 48.03 | wps 14637.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 18043 | lr 0.000235421 | gnorm 0.696 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 80961
2022-03-04 19:08:56 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 19:08:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:13:11 | INFO | train_inner | epoch 047:     57 / 393 loss=5.572, ppl=47.57, wps=14473.5, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.695, loss_scale=8, train_wall=439, gb_free=10.1, wall=81216
2022-03-04 19:17:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:20:41 | INFO | train_inner | epoch 047:    158 / 393 loss=5.528, ppl=46.14, wps=14544.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.684, loss_scale=8, train_wall=446, gb_free=10.1, wall=81666
2022-03-04 19:28:07 | INFO | train_inner | epoch 047:    258 / 393 loss=5.578, ppl=47.75, wps=14699.3, ups=0.22, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.709, loss_scale=8, train_wall=441, gb_free=10.1, wall=82112
2022-03-04 19:35:33 | INFO | train_inner | epoch 047:    358 / 393 loss=5.627, ppl=49.41, wps=14695.2, ups=0.22, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.693, loss_scale=8, train_wall=441, gb_free=10.1, wall=82558
2022-03-04 19:36:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 19:38:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:38:14 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.582 | ppl 191.64 | wps 33970.4 | wpb 2034.1 | bsz 4 | num_updates 18434 | best_loss 7.212
2022-03-04 19:38:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18434 updates
2022-03-04 19:38:14 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 19:38:14 | INFO | train | epoch 047 | loss 5.572 | ppl 47.57 | wps 14564.7 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 18434 | lr 0.000232911 | gnorm 0.698 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 82719
2022-03-04 19:38:14 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 19:38:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:43:08 | INFO | train_inner | epoch 048:     66 / 393 loss=5.54, ppl=46.54, wps=14342, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.708, loss_scale=4, train_wall=444, gb_free=10.1, wall=83013
2022-03-04 19:50:34 | INFO | train_inner | epoch 048:    166 / 393 loss=5.524, ppl=46.01, wps=14694.3, ups=0.22, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.69, loss_scale=4, train_wall=441, gb_free=10.1, wall=83459
2022-03-04 19:58:00 | INFO | train_inner | epoch 048:    266 / 393 loss=5.565, ppl=47.33, wps=14694.2, ups=0.22, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.706, loss_scale=4, train_wall=441, gb_free=10.1, wall=83905
2022-03-04 20:05:26 | INFO | train_inner | epoch 048:    366 / 393 loss=5.622, ppl=49.24, wps=14689.2, ups=0.22, wpb=65530.2, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.703, loss_scale=4, train_wall=441, gb_free=10.1, wall=84351
2022-03-04 20:07:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:07:31 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.584 | ppl 191.91 | wps 33973.7 | wpb 2034.1 | bsz 4 | num_updates 18827 | best_loss 7.212
2022-03-04 20:07:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18827 updates
2022-03-04 20:07:31 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 20:07:31 | INFO | train | epoch 048 | loss 5.559 | ppl 47.15 | wps 14638.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 18827 | lr 0.000230467 | gnorm 0.696 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 84476
2022-03-04 20:07:31 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 20:07:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:12:57 | INFO | train_inner | epoch 049:     73 / 393 loss=5.516, ppl=45.76, wps=14477.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.678, loss_scale=4, train_wall=439, gb_free=10.1, wall=84802
2022-03-04 20:20:23 | INFO | train_inner | epoch 049:    173 / 393 loss=5.521, ppl=45.92, wps=14691.1, ups=0.22, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.7, loss_scale=8, train_wall=441, gb_free=10.1, wall=85248
2022-03-04 20:27:49 | INFO | train_inner | epoch 049:    273 / 393 loss=5.561, ppl=47.19, wps=14696.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.703, loss_scale=8, train_wall=441, gb_free=10.1, wall=85694
2022-03-04 20:35:15 | INFO | train_inner | epoch 049:    373 / 393 loss=5.596, ppl=48.38, wps=14688.1, ups=0.22, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.72, loss_scale=8, train_wall=441, gb_free=10.1, wall=86140
2022-03-04 20:36:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:36:49 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.587 | ppl 192.33 | wps 33952.6 | wpb 2034.1 | bsz 4 | num_updates 19220 | best_loss 7.212
2022-03-04 20:36:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19220 updates
2022-03-04 20:36:49 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 20:36:49 | INFO | train | epoch 049 | loss 5.547 | ppl 46.76 | wps 14637.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 19220 | lr 0.000228099 | gnorm 0.705 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 86234
2022-03-04 20:36:49 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 20:36:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:42:46 | INFO | train_inner | epoch 050:     80 / 393 loss=5.493, ppl=45.04, wps=14477, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=19300, lr=0.000227626, gnorm=0.711, loss_scale=8, train_wall=439, gb_free=10.1, wall=86591
2022-03-04 20:44:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 20:50:16 | INFO | train_inner | epoch 050:    181 / 393 loss=5.505, ppl=45.42, wps=14544.7, ups=0.22, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.708, loss_scale=4, train_wall=446, gb_free=10.1, wall=87041
2022-03-04 20:57:42 | INFO | train_inner | epoch 050:    281 / 393 loss=5.553, ppl=46.94, wps=14690.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.702, loss_scale=4, train_wall=441, gb_free=10.1, wall=87487
2022-03-04 21:05:09 | INFO | train_inner | epoch 050:    381 / 393 loss=5.591, ppl=48.21, wps=14690.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.704, loss_scale=4, train_wall=441, gb_free=10.1, wall=87934
2022-03-04 21:06:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:06:07 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.585 | ppl 192.01 | wps 33869.8 | wpb 2034.1 | bsz 4 | num_updates 19612 | best_loss 7.212
2022-03-04 21:06:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19612 updates
2022-03-04 21:06:07 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 21:06:07 | INFO | train | epoch 050 | loss 5.534 | ppl 46.32 | wps 14598.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 19612 | lr 0.000225808 | gnorm 0.705 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 87992
2022-03-04 21:06:07 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 21:06:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:12:39 | INFO | train_inner | epoch 051:     88 / 393 loss=5.469, ppl=44.29, wps=14486.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.701, loss_scale=4, train_wall=439, gb_free=10.1, wall=88384
2022-03-04 21:20:05 | INFO | train_inner | epoch 051:    188 / 393 loss=5.504, ppl=45.37, wps=14691.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.709, loss_scale=4, train_wall=441, gb_free=10.1, wall=88830
2022-03-04 21:27:31 | INFO | train_inner | epoch 051:    288 / 393 loss=5.546, ppl=46.73, wps=14698.8, ups=0.22, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.735, loss_scale=8, train_wall=441, gb_free=10.1, wall=89276
2022-03-04 21:34:57 | INFO | train_inner | epoch 051:    388 / 393 loss=5.58, ppl=47.84, wps=14696, ups=0.22, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.703, loss_scale=8, train_wall=441, gb_free=10.1, wall=89722
2022-03-04 21:35:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:35:24 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.601 | ppl 194.19 | wps 34057 | wpb 2034.1 | bsz 4 | num_updates 20005 | best_loss 7.212
2022-03-04 21:35:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20005 updates
2022-03-04 21:35:24 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 21:35:24 | INFO | train | epoch 051 | loss 5.523 | ppl 45.98 | wps 14642.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 20005 | lr 0.000223579 | gnorm 0.712 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 89749
2022-03-04 21:35:24 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 21:35:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:42:27 | INFO | train_inner | epoch 052:     95 / 393 loss=5.452, ppl=43.77, wps=14483.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=20100, lr=0.00022305, gnorm=0.708, loss_scale=8, train_wall=439, gb_free=10.1, wall=90172
2022-03-04 21:49:53 | INFO | train_inner | epoch 052:    195 / 393 loss=5.486, ppl=44.82, wps=14702.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.71, loss_scale=8, train_wall=441, gb_free=10.1, wall=90618
2022-03-04 21:57:19 | INFO | train_inner | epoch 052:    295 / 393 loss=5.539, ppl=46.49, wps=14696.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.705, loss_scale=8, train_wall=441, gb_free=10.1, wall=91064
2022-03-04 22:01:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:04:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:04:41 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.619 | ppl 196.64 | wps 33949.2 | wpb 2034.1 | bsz 4 | num_updates 20397 | best_loss 7.212
2022-03-04 22:04:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20397 updates
2022-03-04 22:04:41 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 22:04:41 | INFO | train | epoch 052 | loss 5.512 | ppl 45.62 | wps 14605.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20397 | lr 0.00022142 | gnorm 0.717 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 91506
2022-03-04 22:04:41 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 22:04:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:04:54 | INFO | train_inner | epoch 053:      3 / 393 loss=5.572, ppl=47.56, wps=14340, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.745, loss_scale=8, train_wall=444, gb_free=10.1, wall=91519
2022-03-04 22:12:20 | INFO | train_inner | epoch 053:    103 / 393 loss=5.435, ppl=43.25, wps=14689.9, ups=0.22, wpb=65536, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.692, loss_scale=8, train_wall=441, gb_free=10.1, wall=91965
2022-03-04 22:19:46 | INFO | train_inner | epoch 053:    203 / 393 loss=5.486, ppl=44.81, wps=14685.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.723, loss_scale=8, train_wall=441, gb_free=10.1, wall=92411
2022-03-04 22:27:13 | INFO | train_inner | epoch 053:    303 / 393 loss=5.529, ppl=46.16, wps=14683.5, ups=0.22, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.714, loss_scale=8, train_wall=441, gb_free=10.1, wall=92858
2022-03-04 22:33:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:33:59 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.636 | ppl 198.88 | wps 33823.1 | wpb 2034.1 | bsz 4 | num_updates 20790 | best_loss 7.212
2022-03-04 22:33:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20790 updates
2022-03-04 22:33:59 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 22:33:59 | INFO | train | epoch 053 | loss 5.501 | ppl 45.29 | wps 14631 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 20790 | lr 0.000219317 | gnorm 0.709 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 93264
2022-03-04 22:33:59 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 22:33:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:34:44 | INFO | train_inner | epoch 054:     10 / 393 loss=5.549, ppl=46.82, wps=14470.9, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=20800, lr=0.000219265, gnorm=0.705, loss_scale=8, train_wall=439, gb_free=10.1, wall=93309
2022-03-04 22:39:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:42:14 | INFO | train_inner | epoch 054:    111 / 393 loss=5.431, ppl=43.13, wps=14542, ups=0.22, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.711, loss_scale=8, train_wall=446, gb_free=10.1, wall=93759
2022-03-04 22:49:40 | INFO | train_inner | epoch 054:    211 / 393 loss=5.473, ppl=44.41, wps=14686.4, ups=0.22, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.713, loss_scale=8, train_wall=441, gb_free=10.1, wall=94205
2022-03-04 22:57:07 | INFO | train_inner | epoch 054:    311 / 393 loss=5.52, ppl=45.9, wps=14690.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.728, loss_scale=8, train_wall=441, gb_free=10.1, wall=94652
2022-03-04 23:03:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:03:17 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.654 | ppl 201.45 | wps 33979.9 | wpb 2034.1 | bsz 4 | num_updates 21182 | best_loss 7.212
2022-03-04 23:03:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21182 updates
2022-03-04 23:03:17 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 23:03:17 | INFO | train | epoch 054 | loss 5.491 | ppl 44.97 | wps 14598.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 21182 | lr 0.000217278 | gnorm 0.715 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 95022
2022-03-04 23:03:17 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 23:03:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:04:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 23:04:41 | INFO | train_inner | epoch 055:     19 / 393 loss=5.528, ppl=46.13, wps=14344.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.722, loss_scale=4, train_wall=443, gb_free=10.1, wall=95106
2022-03-04 23:12:07 | INFO | train_inner | epoch 055:    119 / 393 loss=5.43, ppl=43.11, wps=14702.7, ups=0.22, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.716, loss_scale=4, train_wall=441, gb_free=10.1, wall=95552
2022-03-04 23:19:33 | INFO | train_inner | epoch 055:    219 / 393 loss=5.467, ppl=44.24, wps=14691.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.725, loss_scale=4, train_wall=441, gb_free=10.1, wall=95998
2022-03-04 23:26:59 | INFO | train_inner | epoch 055:    319 / 393 loss=5.51, ppl=45.57, wps=14690.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.742, loss_scale=4, train_wall=441, gb_free=10.1, wall=96444
2022-03-04 23:32:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:32:34 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.649 | ppl 200.73 | wps 33800.3 | wpb 2034.1 | bsz 4 | num_updates 21574 | best_loss 7.212
2022-03-04 23:32:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 21574 updates
2022-03-04 23:32:34 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 23:32:34 | INFO | train | epoch 055 | loss 5.481 | ppl 44.67 | wps 14604.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 21574 | lr 0.000215295 | gnorm 0.733 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 96779
2022-03-04 23:32:34 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 23:32:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:34:30 | INFO | train_inner | epoch 056:     26 / 393 loss=5.51, ppl=45.56, wps=14483.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=21600, lr=0.000215166, gnorm=0.743, loss_scale=4, train_wall=439, gb_free=10.1, wall=96895
2022-03-04 23:41:56 | INFO | train_inner | epoch 056:    126 / 393 loss=5.417, ppl=42.74, wps=14693.6, ups=0.22, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.722, loss_scale=4, train_wall=441, gb_free=10.1, wall=97341
2022-03-04 23:49:22 | INFO | train_inner | epoch 056:    226 / 393 loss=5.459, ppl=43.99, wps=14692.5, ups=0.22, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.719, loss_scale=8, train_wall=441, gb_free=10.1, wall=97787
2022-03-04 23:56:48 | INFO | train_inner | epoch 056:    326 / 393 loss=5.517, ppl=45.79, wps=14684, ups=0.22, wpb=65530.9, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.729, loss_scale=8, train_wall=441, gb_free=10.1, wall=98233
2022-03-05 00:01:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:01:52 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.67 | ppl 203.69 | wps 33985.6 | wpb 2034.1 | bsz 4 | num_updates 21967 | best_loss 7.212
2022-03-05 00:01:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 21967 updates
2022-03-05 00:01:52 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-05 00:01:52 | INFO | train | epoch 056 | loss 5.471 | ppl 44.37 | wps 14634.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 21967 | lr 0.000213361 | gnorm 0.728 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 98537
2022-03-05 00:01:52 | INFO | fairseq.trainer | begin training epoch 57
2022-03-05 00:01:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:04:19 | INFO | train_inner | epoch 057:     33 / 393 loss=5.484, ppl=44.75, wps=14471.1, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=22000, lr=0.000213201, gnorm=0.746, loss_scale=8, train_wall=440, gb_free=10.1, wall=98684
2022-03-05 00:11:45 | INFO | train_inner | epoch 057:    133 / 393 loss=5.416, ppl=42.69, wps=14684.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.726, loss_scale=8, train_wall=441, gb_free=10.1, wall=99130
2022-03-05 00:19:11 | INFO | train_inner | epoch 057:    233 / 393 loss=5.448, ppl=43.65, wps=14692.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.716, loss_scale=8, train_wall=441, gb_free=10.1, wall=99576
2022-03-05 00:20:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:26:42 | INFO | train_inner | epoch 057:    334 / 393 loss=5.5, ppl=45.26, wps=14545.7, ups=0.22, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.75, loss_scale=8, train_wall=446, gb_free=10.1, wall=100027
2022-03-05 00:31:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:31:10 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.685 | ppl 205.81 | wps 33941.9 | wpb 2034.1 | bsz 4 | num_updates 22359 | best_loss 7.212
2022-03-05 00:31:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 22359 updates
2022-03-05 00:31:10 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-05 00:31:10 | INFO | train | epoch 057 | loss 5.461 | ppl 44.06 | wps 14596.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 22359 | lr 0.000211482 | gnorm 0.734 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 100295
2022-03-05 00:31:10 | INFO | fairseq.trainer | begin training epoch 58
2022-03-05 00:31:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:34:13 | INFO | train_inner | epoch 058:     41 / 393 loss=5.473, ppl=44.42, wps=14474.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=22400, lr=0.000211289, gnorm=0.751, loss_scale=8, train_wall=439, gb_free=10.1, wall=100478
2022-03-05 00:41:39 | INFO | train_inner | epoch 058:    141 / 393 loss=5.405, ppl=42.36, wps=14697, ups=0.22, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.721, loss_scale=8, train_wall=441, gb_free=10.1, wall=100924
2022-03-05 00:49:05 | INFO | train_inner | epoch 058:    241 / 393 loss=5.449, ppl=43.68, wps=14695.2, ups=0.22, wpb=65530.2, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.74, loss_scale=8, train_wall=441, gb_free=10.1, wall=101370
2022-03-05 00:56:31 | INFO | train_inner | epoch 058:    341 / 393 loss=5.502, ppl=45.3, wps=14696, ups=0.22, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.733, loss_scale=8, train_wall=441, gb_free=10.1, wall=101816
2022-03-05 00:59:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:00:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:00:27 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.685 | ppl 205.74 | wps 34042.3 | wpb 2034.1 | bsz 4 | num_updates 22751 | best_loss 7.212
2022-03-05 01:00:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 22751 updates
2022-03-05 01:00:27 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-05 01:00:27 | INFO | train | epoch 058 | loss 5.453 | ppl 43.79 | wps 14602.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 22751 | lr 0.000209652 | gnorm 0.736 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 102052
2022-03-05 01:00:27 | INFO | fairseq.trainer | begin training epoch 59
2022-03-05 01:00:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:04:06 | INFO | train_inner | epoch 059:     49 / 393 loss=5.448, ppl=43.67, wps=14339, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=22800, lr=0.000209427, gnorm=0.73, loss_scale=8, train_wall=444, gb_free=10.1, wall=102271
2022-03-05 01:11:32 | INFO | train_inner | epoch 059:    149 / 393 loss=5.397, ppl=42.14, wps=14691.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.728, loss_scale=8, train_wall=441, gb_free=10.1, wall=102717
2022-03-05 01:17:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-05 01:19:02 | INFO | train_inner | epoch 059:    250 / 393 loss=5.453, ppl=43.79, wps=14551.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.761, loss_scale=4, train_wall=445, gb_free=10.1, wall=103167
2022-03-05 01:26:28 | INFO | train_inner | epoch 059:    350 / 393 loss=5.483, ppl=44.72, wps=14697.2, ups=0.22, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.736, loss_scale=4, train_wall=441, gb_free=10.1, wall=103613
2022-03-05 01:29:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:29:44 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.715 | ppl 210.13 | wps 33995.5 | wpb 2034.1 | bsz 4 | num_updates 23143 | best_loss 7.212
2022-03-05 01:29:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 23143 updates
2022-03-05 01:29:44 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-05 01:29:44 | INFO | train | epoch 059 | loss 5.444 | ppl 43.53 | wps 14603.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 23143 | lr 0.000207869 | gnorm 0.741 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 103809
2022-03-05 01:29:44 | INFO | fairseq.trainer | begin training epoch 60
2022-03-05 01:29:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:33:59 | INFO | train_inner | epoch 060:     57 / 393 loss=5.425, ppl=42.95, wps=14479, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=23200, lr=0.000207614, gnorm=0.743, loss_scale=4, train_wall=439, gb_free=10.1, wall=104064
2022-03-05 01:41:24 | INFO | train_inner | epoch 060:    157 / 393 loss=5.404, ppl=42.35, wps=14696.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.723, loss_scale=4, train_wall=441, gb_free=10.1, wall=104509
2022-03-05 01:48:50 | INFO | train_inner | epoch 060:    257 / 393 loss=5.441, ppl=43.44, wps=14691.8, ups=0.22, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.757, loss_scale=4, train_wall=441, gb_free=10.1, wall=104955
2022-03-05 01:56:16 | INFO | train_inner | epoch 060:    357 / 393 loss=5.482, ppl=44.71, wps=14697.3, ups=0.22, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.731, loss_scale=8, train_wall=441, gb_free=10.1, wall=105401
2022-03-05 01:58:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:59:01 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.72 | ppl 210.87 | wps 34034.7 | wpb 2034.1 | bsz 4 | num_updates 23536 | best_loss 7.212
2022-03-05 01:59:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 23536 updates
2022-03-05 01:59:01 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-05 01:59:01 | INFO | train | epoch 060 | loss 5.436 | ppl 43.29 | wps 14640.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 23536 | lr 0.000206126 | gnorm 0.737 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 105566
2022-03-05 01:59:01 | INFO | fairseq.trainer | begin training epoch 61
2022-03-05 01:59:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:03:47 | INFO | train_inner | epoch 061:     64 / 393 loss=5.407, ppl=42.44, wps=14489.9, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=23600, lr=0.000205847, gnorm=0.748, loss_scale=8, train_wall=439, gb_free=10.1, wall=105852
2022-03-05 02:11:13 | INFO | train_inner | epoch 061:    164 / 393 loss=5.393, ppl=42.02, wps=14694.6, ups=0.22, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.733, loss_scale=8, train_wall=441, gb_free=10.1, wall=106298
2022-03-05 02:17:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-05 02:18:43 | INFO | train_inner | epoch 061:    265 / 393 loss=5.441, ppl=43.45, wps=14551.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.755, loss_scale=4, train_wall=445, gb_free=10.1, wall=106748
2022-03-05 02:26:09 | INFO | train_inner | epoch 061:    365 / 393 loss=5.48, ppl=44.64, wps=14703.9, ups=0.22, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.738, loss_scale=4, train_wall=441, gb_free=10.1, wall=107194
2022-03-05 02:28:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:28:18 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.704 | ppl 208.57 | wps 34008.6 | wpb 2034.1 | bsz 4 | num_updates 23928 | best_loss 7.212
2022-03-05 02:28:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 23928 updates
2022-03-05 02:28:18 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-05 02:28:18 | INFO | train | epoch 061 | loss 5.427 | ppl 43.04 | wps 14607.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 23928 | lr 0.000204431 | gnorm 0.743 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 107323
2022-03-05 02:28:18 | INFO | fairseq.trainer | begin training epoch 62
2022-03-05 02:28:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:33:39 | INFO | train_inner | epoch 062:     72 / 393 loss=5.392, ppl=41.99, wps=14490.1, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=24000, lr=0.000204124, gnorm=0.744, loss_scale=4, train_wall=439, gb_free=10.1, wall=107644
2022-03-05 02:41:05 | INFO | train_inner | epoch 062:    172 / 393 loss=5.386, ppl=41.82, wps=14700.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.739, loss_scale=4, train_wall=441, gb_free=10.1, wall=108090
2022-03-05 02:48:31 | INFO | train_inner | epoch 062:    272 / 393 loss=5.431, ppl=43.15, wps=14695.9, ups=0.22, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.731, loss_scale=4, train_wall=441, gb_free=10.1, wall=108536
2022-03-05 02:55:57 | INFO | train_inner | epoch 062:    372 / 393 loss=5.475, ppl=44.49, wps=14689.2, ups=0.22, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.754, loss_scale=8, train_wall=441, gb_free=10.1, wall=108982
2022-03-05 02:57:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:57:35 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 7.719 | ppl 210.71 | wps 33910.2 | wpb 2034.1 | bsz 4 | num_updates 24321 | best_loss 7.212
2022-03-05 02:57:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 24321 updates
2022-03-05 02:57:35 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-05 02:57:35 | INFO | train | epoch 062 | loss 5.42 | ppl 42.81 | wps 14641.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 24321 | lr 0.000202773 | gnorm 0.743 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 109080
2022-03-05 02:57:35 | INFO | fairseq.trainer | begin training epoch 63
2022-03-05 02:57:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:03:28 | INFO | train_inner | epoch 063:     79 / 393 loss=5.37, ppl=41.36, wps=14474.5, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=24400, lr=0.000202444, gnorm=0.741, loss_scale=8, train_wall=439, gb_free=10.1, wall=109433
2022-03-05 03:10:54 | INFO | train_inner | epoch 063:    179 / 393 loss=5.39, ppl=41.93, wps=14689.2, ups=0.22, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.759, loss_scale=8, train_wall=441, gb_free=10.1, wall=109879
2022-03-05 03:18:20 | INFO | train_inner | epoch 063:    279 / 393 loss=5.426, ppl=42.98, wps=14693, ups=0.22, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.731, loss_scale=8, train_wall=441, gb_free=10.1, wall=110325
2022-03-05 03:25:46 | INFO | train_inner | epoch 063:    379 / 393 loss=5.468, ppl=44.25, wps=14693.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.743, loss_scale=8, train_wall=441, gb_free=10.1, wall=110771
2022-03-05 03:26:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:26:53 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 7.742 | ppl 214.11 | wps 33898 | wpb 2034.1 | bsz 4 | num_updates 24714 | best_loss 7.212
2022-03-05 03:26:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 24714 updates
2022-03-05 03:26:53 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-05 03:26:53 | INFO | train | epoch 063 | loss 5.412 | ppl 42.57 | wps 14636.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 24714 | lr 0.000201154 | gnorm 0.744 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 110838
2022-03-05 03:26:53 | INFO | fairseq.trainer | begin training epoch 64
2022-03-05 03:26:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:27:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-05 03:33:21 | INFO | train_inner | epoch 064:     87 / 393 loss=5.358, ppl=41.01, wps=14348.2, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=24800, lr=0.000200805, gnorm=0.759, loss_scale=4, train_wall=443, gb_free=10.1, wall=111226
2022-03-05 03:40:46 | INFO | train_inner | epoch 064:    187 / 393 loss=5.38, ppl=41.65, wps=14701.4, ups=0.22, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.753, loss_scale=4, train_wall=441, gb_free=10.1, wall=111671
2022-03-05 03:48:12 | INFO | train_inner | epoch 064:    287 / 393 loss=5.42, ppl=42.83, wps=14697.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.757, loss_scale=4, train_wall=441, gb_free=10.1, wall=112117
2022-03-05 03:55:38 | INFO | train_inner | epoch 064:    387 / 393 loss=5.467, ppl=44.23, wps=14696.4, ups=0.22, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.757, loss_scale=4, train_wall=441, gb_free=10.1, wall=112563
2022-03-05 03:56:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:56:09 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 7.731 | ppl 212.45 | wps 33986.3 | wpb 2034.1 | bsz 4 | num_updates 25106 | best_loss 7.212
2022-03-05 03:56:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 25106 updates
2022-03-05 03:56:09 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-05 03:56:09 | INFO | train | epoch 064 | loss 5.404 | ppl 42.35 | wps 14607.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 25106 | lr 0.000199577 | gnorm 0.755 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 112594
2022-03-05 03:56:09 | INFO | fairseq.trainer | begin training epoch 65
2022-03-05 03:56:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:03:08 | INFO | train_inner | epoch 065:     94 / 393 loss=5.336, ppl=40.4, wps=14490.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=25200, lr=0.000199205, gnorm=0.737, loss_scale=4, train_wall=439, gb_free=10.1, wall=113013
2022-03-05 04:10:35 | INFO | train_inner | epoch 065:    194 / 393 loss=5.374, ppl=41.48, wps=14689.5, ups=0.22, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.765, loss_scale=8, train_wall=441, gb_free=10.1, wall=113460
2022-03-05 04:18:01 | INFO | train_inner | epoch 065:    294 / 393 loss=5.42, ppl=42.81, wps=14683.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.747, loss_scale=8, train_wall=441, gb_free=10.1, wall=113906
2022-03-05 04:25:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:25:27 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 7.738 | ppl 213.55 | wps 34001.6 | wpb 2034.1 | bsz 4 | num_updates 25499 | best_loss 7.212
2022-03-05 04:25:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 25499 updates
2022-03-05 04:25:27 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-05 04:25:27 | INFO | train | epoch 065 | loss 5.398 | ppl 42.15 | wps 14636.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 25499 | lr 0.000198033 | gnorm 0.755 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 114352
2022-03-05 04:25:27 | INFO | fairseq.trainer | begin training epoch 66
2022-03-05 04:25:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:25:32 | INFO | train_inner | epoch 066:      1 / 393 loss=5.463, ppl=44.1, wps=14476.4, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=25500, lr=0.00019803, gnorm=0.767, loss_scale=8, train_wall=439, gb_free=10.1, wall=114357
2022-03-05 04:32:58 | INFO | train_inner | epoch 066:    101 / 393 loss=5.326, ppl=40.11, wps=14692.9, ups=0.22, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.743, loss_scale=8, train_wall=441, gb_free=10.1, wall=114803
2022-03-05 04:40:24 | INFO | train_inner | epoch 066:    201 / 393 loss=5.37, ppl=41.37, wps=14695.2, ups=0.22, wpb=65530.2, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.75, loss_scale=8, train_wall=441, gb_free=10.1, wall=115249
2022-03-05 04:43:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:47:54 | INFO | train_inner | epoch 066:    302 / 393 loss=5.412, ppl=42.56, wps=14548.5, ups=0.22, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.762, loss_scale=8, train_wall=446, gb_free=10.1, wall=115699
2022-03-05 04:54:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:54:45 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 7.735 | ppl 213.01 | wps 33958.4 | wpb 2034.1 | bsz 4 | num_updates 25891 | best_loss 7.212
2022-03-05 04:54:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 25891 updates
2022-03-05 04:54:45 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-05 04:54:45 | INFO | train | epoch 066 | loss 5.39 | ppl 41.93 | wps 14600.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 25891 | lr 0.000196529 | gnorm 0.752 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 116110
2022-03-05 04:54:45 | INFO | fairseq.trainer | begin training epoch 67
2022-03-05 04:54:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:55:25 | INFO | train_inner | epoch 067:      9 / 393 loss=5.445, ppl=43.55, wps=14474.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=25900, lr=0.000196494, gnorm=0.755, loss_scale=8, train_wall=439, gb_free=10.1, wall=116150
2022-03-05 05:02:51 | INFO | train_inner | epoch 067:    109 / 393 loss=5.318, ppl=39.88, wps=14691.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.768, loss_scale=8, train_wall=441, gb_free=10.1, wall=116596
2022-03-05 05:10:17 | INFO | train_inner | epoch 067:    209 / 393 loss=5.373, ppl=41.43, wps=14688.6, ups=0.22, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.758, loss_scale=8, train_wall=441, gb_free=10.1, wall=117042
2022-03-05 05:17:43 | INFO | train_inner | epoch 067:    309 / 393 loss=5.413, ppl=42.61, wps=14694.6, ups=0.22, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.76, loss_scale=8, train_wall=441, gb_free=10.1, wall=117488
2022-03-05 05:21:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:23:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:24:02 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 7.746 | ppl 214.68 | wps 33909.5 | wpb 2034.1 | bsz 4 | num_updates 26283 | best_loss 7.212
2022-03-05 05:24:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 26283 updates
2022-03-05 05:24:02 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-05 05:24:02 | INFO | train | epoch 067 | loss 5.384 | ppl 41.75 | wps 14600.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 26283 | lr 0.000195057 | gnorm 0.762 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 117867
2022-03-05 05:24:02 | INFO | fairseq.trainer | begin training epoch 68
2022-03-05 05:24:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:25:18 | INFO | train_inner | epoch 068:     17 / 393 loss=5.425, ppl=42.97, wps=14341.7, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=26300, lr=0.000194994, gnorm=0.762, loss_scale=8, train_wall=444, gb_free=10.1, wall=117943
2022-03-05 05:32:44 | INFO | train_inner | epoch 068:    117 / 393 loss=5.321, ppl=39.97, wps=14696.2, ups=0.22, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.758, loss_scale=8, train_wall=441, gb_free=10.1, wall=118389
2022-03-05 05:40:10 | INFO | train_inner | epoch 068:    217 / 393 loss=5.363, ppl=41.14, wps=14693.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.76, loss_scale=8, train_wall=441, gb_free=10.1, wall=118835
2022-03-05 05:47:36 | INFO | train_inner | epoch 068:    317 / 393 loss=5.41, ppl=42.53, wps=14701, ups=0.22, wpb=65530.9, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.765, loss_scale=8, train_wall=441, gb_free=10.1, wall=119281
2022-03-05 05:52:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-05 05:53:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:53:19 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 7.774 | ppl 218.9 | wps 33904.6 | wpb 2034.1 | bsz 4 | num_updates 26675 | best_loss 7.212
2022-03-05 05:53:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 26675 updates
2022-03-05 05:53:19 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-05 05:53:19 | INFO | train | epoch 068 | loss 5.377 | ppl 41.55 | wps 14605 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 26675 | lr 0.000193619 | gnorm 0.763 | loss_scale 4 | train_wall 1731 | gb_free 10.1 | wall 119624
2022-03-05 05:53:19 | INFO | fairseq.trainer | begin training epoch 69
2022-03-05 05:53:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:55:11 | INFO | train_inner | epoch 069:     25 / 393 loss=5.405, ppl=42.37, wps=14341.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=26700, lr=0.000193528, gnorm=0.76, loss_scale=4, train_wall=444, gb_free=10.1, wall=119736
2022-03-05 06:02:37 | INFO | train_inner | epoch 069:    125 / 393 loss=5.322, ppl=40.01, wps=14695.2, ups=0.22, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.765, loss_scale=4, train_wall=441, gb_free=10.1, wall=120182
2022-03-05 06:10:03 | INFO | train_inner | epoch 069:    225 / 393 loss=5.365, ppl=41.2, wps=14692.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.76, loss_scale=4, train_wall=441, gb_free=10.1, wall=120628
2022-03-05 06:17:29 | INFO | train_inner | epoch 069:    325 / 393 loss=5.402, ppl=42.29, wps=14696.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.783, loss_scale=4, train_wall=441, gb_free=10.1, wall=121074
2022-03-05 06:22:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:22:36 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 7.745 | ppl 214.49 | wps 34002.7 | wpb 2034.1 | bsz 4 | num_updates 27068 | best_loss 7.212
2022-03-05 06:22:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 27068 updates
2022-03-05 06:22:36 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-05 06:22:36 | INFO | train | epoch 069 | loss 5.37 | ppl 41.37 | wps 14639.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 27068 | lr 0.000192208 | gnorm 0.765 | loss_scale 4 | train_wall 1732 | gb_free 10.1 | wall 121381
2022-03-05 06:22:36 | INFO | fairseq.trainer | begin training epoch 70
2022-03-05 06:22:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:24:59 | INFO | train_inner | epoch 070:     32 / 393 loss=5.388, ppl=41.87, wps=14479.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=27100, lr=0.000192095, gnorm=0.757, loss_scale=4, train_wall=439, gb_free=10.1, wall=121524
2022-03-05 06:32:25 | INFO | train_inner | epoch 070:    132 / 393 loss=5.314, ppl=39.77, wps=14693.5, ups=0.22, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.758, loss_scale=8, train_wall=441, gb_free=10.1, wall=121970
2022-03-05 06:39:52 | INFO | train_inner | epoch 070:    232 / 393 loss=5.355, ppl=40.92, wps=14685.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.764, loss_scale=8, train_wall=441, gb_free=10.1, wall=122417
2022-03-05 06:47:18 | INFO | train_inner | epoch 070:    332 / 393 loss=5.4, ppl=42.23, wps=14686.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.759, loss_scale=8, train_wall=441, gb_free=10.1, wall=122863
2022-03-05 06:51:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:51:54 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 7.771 | ppl 218.4 | wps 34100.6 | wpb 2034.1 | bsz 4 | num_updates 27461 | best_loss 7.212
2022-03-05 06:51:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 27461 updates
2022-03-05 06:51:55 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-05 06:51:55 | INFO | train | epoch 070 | loss 5.365 | ppl 41.2 | wps 14633.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 27461 | lr 0.000190828 | gnorm 0.764 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 123140
2022-03-05 06:51:55 | INFO | fairseq.trainer | begin training epoch 71
2022-03-05 06:51:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:54:49 | INFO | train_inner | epoch 071:     39 / 393 loss=5.383, ppl=41.73, wps=14471.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=27500, lr=0.000190693, gnorm=0.771, loss_scale=8, train_wall=439, gb_free=10.1, wall=123314
2022-03-05 07:02:15 | INFO | train_inner | epoch 071:    139 / 393 loss=5.308, ppl=39.62, wps=14682.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.76, loss_scale=8, train_wall=441, gb_free=10.1, wall=123760
2022-03-05 07:09:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:09:45 | INFO | train_inner | epoch 071:    240 / 393 loss=5.364, ppl=41.18, wps=14548.2, ups=0.22, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.768, loss_scale=8, train_wall=446, gb_free=10.1, wall=124210
2022-03-05 07:17:12 | INFO | train_inner | epoch 071:    340 / 393 loss=5.392, ppl=41.99, wps=14689.5, ups=0.22, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.757, loss_scale=8, train_wall=441, gb_free=10.1, wall=124657
2022-03-05 07:21:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:21:13 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 7.775 | ppl 218.98 | wps 33973 | wpb 2034.1 | bsz 4 | num_updates 27853 | best_loss 7.212
2022-03-05 07:21:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 27853 updates
2022-03-05 07:21:13 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-05 07:21:13 | INFO | train | epoch 071 | loss 5.358 | ppl 41.02 | wps 14596.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 27853 | lr 0.00018948 | gnorm 0.763 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 124898
2022-03-05 07:21:13 | INFO | fairseq.trainer | begin training epoch 72
2022-03-05 07:21:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:24:42 | INFO | train_inner | epoch 072:     47 / 393 loss=5.36, ppl=41.06, wps=14469.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=27900, lr=0.000189321, gnorm=0.773, loss_scale=8, train_wall=440, gb_free=10.1, wall=125107
2022-03-05 07:32:09 | INFO | train_inner | epoch 072:    147 / 393 loss=5.306, ppl=39.55, wps=14688.8, ups=0.22, wpb=65536, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.748, loss_scale=8, train_wall=441, gb_free=10.1, wall=125554
2022-03-05 07:39:35 | INFO | train_inner | epoch 072:    247 / 393 loss=5.353, ppl=40.87, wps=14689, ups=0.22, wpb=65530.2, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.771, loss_scale=8, train_wall=441, gb_free=10.1, wall=126000
2022-03-05 07:47:01 | INFO | train_inner | epoch 072:    347 / 393 loss=5.401, ppl=42.26, wps=14692.7, ups=0.22, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.77, loss_scale=8, train_wall=441, gb_free=10.1, wall=126446
2022-03-05 07:48:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:50:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:50:30 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 7.795 | ppl 222.16 | wps 34051.8 | wpb 2034.1 | bsz 4 | num_updates 28245 | best_loss 7.212
2022-03-05 07:50:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 28245 updates
2022-03-05 07:50:30 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-05 07:50:30 | INFO | train | epoch 072 | loss 5.353 | ppl 40.86 | wps 14597.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 28245 | lr 0.000188161 | gnorm 0.766 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 126655
2022-03-05 07:50:30 | INFO | fairseq.trainer | begin training epoch 73
2022-03-05 07:50:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:54:36 | INFO | train_inner | epoch 073:     55 / 393 loss=5.34, ppl=40.51, wps=14340.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=28300, lr=0.000187978, gnorm=0.774, loss_scale=8, train_wall=444, gb_free=10.1, wall=126901
2022-03-05 08:02:02 | INFO | train_inner | epoch 073:    155 / 393 loss=5.305, ppl=39.52, wps=14687.2, ups=0.22, wpb=65536, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.755, loss_scale=8, train_wall=441, gb_free=10.1, wall=127347
2022-03-05 08:09:28 | INFO | train_inner | epoch 073:    255 / 393 loss=5.354, ppl=40.89, wps=14687.9, ups=0.22, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.774, loss_scale=8, train_wall=441, gb_free=10.1, wall=127793
2022-03-05 08:16:54 | INFO | train_inner | epoch 073:    355 / 393 loss=5.395, ppl=42.07, wps=14685.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.767, loss_scale=8, train_wall=441, gb_free=10.1, wall=128239
2022-03-05 08:19:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:19:48 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 7.801 | ppl 222.97 | wps 33914.1 | wpb 2034.1 | bsz 4 | num_updates 28638 | best_loss 7.212
2022-03-05 08:19:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 28638 updates
2022-03-05 08:19:48 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-05 08:19:48 | INFO | train | epoch 073 | loss 5.347 | ppl 40.7 | wps 14633.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 28638 | lr 0.000186865 | gnorm 0.765 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 128413
2022-03-05 08:19:48 | INFO | fairseq.trainer | begin training epoch 74
2022-03-05 08:19:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:24:25 | INFO | train_inner | epoch 074:     62 / 393 loss=5.324, ppl=40.06, wps=14473, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=28700, lr=0.000186663, gnorm=0.77, loss_scale=8, train_wall=439, gb_free=10.1, wall=128690
2022-03-05 08:26:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:31:56 | INFO | train_inner | epoch 074:    163 / 393 loss=5.3, ppl=39.4, wps=14538.1, ups=0.22, wpb=65530.2, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.758, loss_scale=8, train_wall=446, gb_free=10.1, wall=129141
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
