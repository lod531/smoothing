Sender: LSF System <lsfadmin@eu-g3-074>
Subject: Job 207014370: <w103_fp16_size_0.25_jelinek_0.04_0.06_0.9_#2> in cluster <euler> Exited

Job <w103_fp16_size_0.25_jelinek_0.04_0.06_0.9_#2> was submitted from host <eu-login-19> by user <andriusb> in cluster <euler> at Thu Mar  3 10:52:49 2022
Job was executed on host(s) <eu-g3-074>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Thu Mar  3 10:53:19 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Mar  3 10:53:19 2022
Terminated at Fri Mar  4 13:21:57 2022
Results reported at Fri Mar  4 13:21:57 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.04, 0.06, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321662 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   94935.98 sec.
    Max Memory :                                 8310 MB
    Average Memory :                             2920.98 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11690.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   95317 sec.
    Turnaround time :                            95348 sec.

The output (if any) follows:

2022-03-03 10:53:29 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321662, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321662, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.04, 0.06, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-03 10:53:30 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-03 10:53:33 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
Calculating frequency stats:
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 669/450337 [00:00<01:07, 6659.43it/s]  0%|          | 1335/450337 [00:00<01:17, 5820.35it/s]  0%|          | 1924/450337 [00:00<01:20, 5554.72it/s]  1%|          | 2483/450337 [00:00<01:21, 5474.66it/s]  1%|          | 3199/450337 [00:00<01:14, 6039.22it/s]  1%|          | 3808/450337 [00:00<01:15, 5926.05it/s]  1%|          | 4486/450337 [00:00<01:12, 6188.05it/s]  1%|          | 5183/450337 [00:00<01:09, 6426.12it/s]  1%|▏         | 5854/450337 [00:00<01:08, 6512.34it/s]  1%|▏         | 6508/450337 [00:01<01:14, 5951.83it/s]  2%|▏         | 7114/450337 [00:01<01:16, 5820.03it/s]  2%|▏         | 7753/450337 [00:01<01:14, 5978.02it/s]  2%|▏         | 8357/450337 [00:01<01:14, 5940.11it/s]  2%|▏         | 8956/450337 [00:01<01:15, 5818.34it/s]  2%|▏         | 9541/450337 [00:01<01:15, 5801.93it/s]  2%|▏         | 10189/450337 [00:01<01:13, 5996.55it/s]  2%|▏         | 10791/450337 [00:01<01:15, 5798.81it/s]  3%|▎         | 11374/450337 [00:01<01:16, 5755.42it/s]  3%|▎         | 12046/450337 [00:02<01:12, 6030.59it/s]  3%|▎         | 12652/450337 [00:02<01:15, 5819.97it/s]  3%|▎         | 13270/450337 [00:02<01:13, 5921.48it/s]  3%|▎         | 13912/450337 [00:02<01:11, 6063.06it/s]  3%|▎         | 14521/450337 [00:02<01:13, 5906.53it/s]  3%|▎         | 15201/450337 [00:02<01:10, 6162.44it/s]  4%|▎         | 15820/450337 [00:02<01:14, 5870.40it/s]  4%|▎         | 16412/450337 [00:02<01:15, 5719.90it/s]  4%|▍         | 16995/450337 [00:02<01:15, 5747.45it/s]  4%|▍         | 17576/450337 [00:02<01:15, 5762.26it/s]  4%|▍         | 18188/450337 [00:03<01:13, 5861.17it/s]  4%|▍         | 18803/450337 [00:03<01:12, 5936.74it/s]  4%|▍         | 19560/450337 [00:03<01:07, 6415.15it/s]  4%|▍         | 20204/450337 [00:03<01:11, 6019.83it/s]  5%|▍         | 20812/450337 [00:03<01:11, 5997.40it/s]  5%|▍         | 21416/450337 [00:03<01:15, 5703.98it/s]  5%|▍         | 22033/450337 [00:03<01:13, 5827.54it/s]  5%|▌         | 22657/450337 [00:03<01:11, 5945.34it/s]  5%|▌         | 23267/450337 [00:03<01:11, 5985.21it/s]  5%|▌         | 24019/450337 [00:04<01:06, 6432.53it/s]  5%|▌         | 24762/450337 [00:04<01:03, 6718.48it/s]  6%|▌         | 25437/450337 [00:04<01:04, 6600.31it/s]  6%|▌         | 26100/450337 [00:04<01:08, 6215.65it/s]  6%|▌         | 26728/450337 [00:04<01:11, 5963.33it/s]  6%|▌         | 27330/450337 [00:04<01:14, 5709.50it/s]  6%|▌         | 27906/450337 [00:04<01:14, 5647.29it/s]  6%|▋         | 28629/450337 [00:04<01:09, 6088.00it/s]  6%|▋         | 29243/450337 [00:04<01:10, 5958.93it/s]  7%|▋         | 29843/450337 [00:04<01:11, 5917.07it/s]  7%|▋         | 30438/450337 [00:05<01:11, 5892.71it/s]  7%|▋         | 31029/450337 [00:05<01:17, 5391.75it/s]  7%|▋         | 31664/450337 [00:05<01:14, 5651.63it/s]  7%|▋         | 32238/450337 [00:05<01:17, 5376.04it/s]  7%|▋         | 32847/450337 [00:05<01:15, 5564.71it/s]  7%|▋         | 33411/450337 [00:05<01:16, 5444.76it/s]  8%|▊         | 34033/450337 [00:05<01:13, 5659.49it/s]  8%|▊         | 34694/450337 [00:05<01:10, 5928.05it/s]  8%|▊         | 35292/450337 [00:05<01:11, 5796.19it/s]  8%|▊         | 35876/450337 [00:06<01:12, 5753.81it/s]  8%|▊         | 36485/450337 [00:06<01:10, 5837.70it/s]  8%|▊         | 37071/450337 [00:06<01:12, 5683.11it/s]  8%|▊         | 37642/450337 [00:06<01:15, 5455.83it/s]  8%|▊         | 38232/450337 [00:06<01:13, 5576.78it/s]  9%|▊         | 38826/450337 [00:06<01:12, 5669.55it/s]  9%|▊         | 39396/450337 [00:06<01:13, 5573.99it/s]  9%|▉         | 40011/450337 [00:06<01:11, 5737.17it/s]  9%|▉         | 40676/450337 [00:06<01:08, 6001.09it/s]  9%|▉         | 41289/450337 [00:07<01:07, 6038.50it/s]  9%|▉         | 41895/450337 [00:07<01:12, 5647.78it/s]  9%|▉         | 42466/450337 [00:07<01:13, 5518.60it/s] 10%|▉         | 43022/450337 [00:07<01:13, 5524.64it/s] 10%|▉         | 43613/450337 [00:07<01:12, 5632.76it/s] 10%|▉         | 44196/450337 [00:07<01:11, 5688.37it/s] 10%|▉         | 44862/450337 [00:07<01:07, 5971.06it/s] 10%|█         | 45462/450337 [00:07<01:08, 5951.05it/s] 10%|█         | 46110/450337 [00:07<01:06, 6107.26it/s] 10%|█         | 46739/450337 [00:07<01:05, 6157.58it/s] 11%|█         | 47582/450337 [00:08<00:58, 6832.42it/s] 11%|█         | 48290/450337 [00:08<00:58, 6906.13it/s] 11%|█         | 48982/450337 [00:08<00:59, 6723.84it/s] 11%|█         | 49656/450337 [00:08<00:59, 6700.10it/s] 11%|█         | 50328/450337 [00:08<01:04, 6194.17it/s] 11%|█▏        | 50956/450337 [00:08<01:06, 6020.74it/s] 11%|█▏        | 51564/450337 [00:08<01:07, 5934.53it/s] 12%|█▏        | 52429/450337 [00:08<00:59, 6695.80it/s] 12%|█▏        | 53107/450337 [00:08<01:02, 6373.94it/s] 12%|█▏        | 53752/450337 [00:09<01:03, 6262.27it/s] 12%|█▏        | 54384/450337 [00:09<01:07, 5900.05it/s] 12%|█▏        | 54981/450337 [00:09<01:07, 5821.74it/s] 12%|█▏        | 55661/450337 [00:09<01:04, 6093.62it/s] 12%|█▏        | 56276/450337 [00:09<01:05, 6058.54it/s] 13%|█▎        | 56886/450337 [00:09<01:07, 5792.07it/s] 13%|█▎        | 57470/450337 [00:09<01:10, 5604.62it/s] 13%|█▎        | 58081/450337 [00:09<01:08, 5741.04it/s] 13%|█▎        | 58803/450337 [00:09<01:03, 6152.24it/s] 13%|█▎        | 59423/450337 [00:10<01:07, 5827.67it/s] 13%|█▎        | 60012/450337 [00:10<01:07, 5820.26it/s] 13%|█▎        | 60606/450337 [00:10<01:06, 5851.43it/s] 14%|█▎        | 61389/450337 [00:10<01:00, 6419.17it/s] 14%|█▍        | 62035/450337 [00:10<01:04, 6059.78it/s] 14%|█▍        | 62653/450337 [00:10<01:03, 6090.79it/s] 14%|█▍        | 63267/450337 [00:10<01:04, 5956.77it/s] 14%|█▍        | 63899/450337 [00:10<01:03, 6060.35it/s] 14%|█▍        | 64508/450337 [00:10<01:05, 5854.52it/s] 14%|█▍        | 65115/450337 [00:10<01:05, 5915.99it/s] 15%|█▍        | 65710/450337 [00:11<01:05, 5846.60it/s] 15%|█▍        | 66306/450337 [00:11<01:05, 5873.96it/s] 15%|█▍        | 66984/450337 [00:11<01:02, 6134.83it/s] 15%|█▌        | 67599/450337 [00:11<01:06, 5778.66it/s] 15%|█▌        | 68182/450337 [00:11<01:07, 5638.99it/s] 15%|█▌        | 68924/450337 [00:11<01:02, 6139.20it/s] 15%|█▌        | 69544/450337 [00:11<01:03, 6026.42it/s] 16%|█▌        | 70171/450337 [00:11<01:02, 6093.88it/s] 16%|█▌        | 70784/450337 [00:11<01:04, 5909.26it/s] 16%|█▌        | 71413/450337 [00:12<01:02, 6018.35it/s] 16%|█▌        | 72028/450337 [00:12<01:02, 6052.73it/s] 16%|█▌        | 72689/450337 [00:12<01:00, 6215.12it/s] 16%|█▋        | 73423/450337 [00:12<00:57, 6546.46it/s] 16%|█▋        | 74171/450337 [00:12<00:55, 6813.17it/s] 17%|█▋        | 74854/450337 [00:12<00:57, 6488.59it/s] 17%|█▋        | 75507/450337 [00:12<01:00, 6189.09it/s] 17%|█▋        | 76131/450337 [00:12<01:00, 6200.92it/s] 17%|█▋        | 76757/450337 [00:12<01:00, 6213.12it/s] 17%|█▋        | 77381/450337 [00:12<01:00, 6215.32it/s] 17%|█▋        | 78005/450337 [00:13<01:00, 6175.44it/s] 17%|█▋        | 78624/450337 [00:13<01:04, 5772.97it/s] 18%|█▊        | 79207/450337 [00:13<01:07, 5460.65it/s] 18%|█▊        | 79786/450337 [00:13<01:06, 5550.15it/s] 18%|█▊        | 80372/450337 [00:13<01:05, 5630.52it/s] 18%|█▊        | 80995/450337 [00:13<01:03, 5800.98it/s] 18%|█▊        | 81630/450337 [00:13<01:01, 5953.53it/s] 18%|█▊        | 82260/450337 [00:13<01:00, 6054.83it/s] 18%|█▊        | 82874/450337 [00:13<01:00, 6074.29it/s] 19%|█▊        | 83523/450337 [00:13<00:59, 6195.08it/s] 19%|█▊        | 84144/450337 [00:14<01:00, 6069.88it/s] 19%|█▉        | 84804/450337 [00:14<00:58, 6224.69it/s] 19%|█▉        | 85428/450337 [00:14<01:01, 5915.36it/s] 19%|█▉        | 86048/450337 [00:14<01:00, 5992.91it/s] 19%|█▉        | 86651/450337 [00:14<01:00, 5974.89it/s] 19%|█▉        | 87322/450337 [00:14<00:58, 6185.89it/s] 20%|█▉        | 87943/450337 [00:14<00:58, 6169.31it/s] 20%|█▉        | 88562/450337 [00:14<01:01, 5911.74it/s] 20%|█▉        | 89157/450337 [00:14<01:02, 5761.86it/s] 20%|█▉        | 89783/450337 [00:15<01:01, 5903.03it/s] 20%|██        | 90376/450337 [00:15<01:03, 5639.34it/s] 20%|██        | 90944/450337 [00:15<01:04, 5554.50it/s] 20%|██        | 91638/450337 [00:15<01:00, 5944.11it/s] 20%|██        | 92237/450337 [00:15<01:00, 5898.70it/s] 21%|██        | 92886/450337 [00:15<00:58, 6060.24it/s] 21%|██        | 93495/450337 [00:15<00:59, 5990.30it/s] 21%|██        | 94096/450337 [00:15<01:01, 5837.88it/s] 21%|██        | 94805/450337 [00:15<00:57, 6199.44it/s] 21%|██        | 95428/450337 [00:15<00:58, 6033.64it/s] 21%|██▏       | 96034/450337 [00:16<00:59, 5925.75it/s] 22%|██▏       | 96878/450337 [00:16<00:53, 6648.00it/s] 22%|██▏       | 97548/450337 [00:16<00:57, 6118.59it/s] 22%|██▏       | 98178/450337 [00:16<00:57, 6166.39it/s] 22%|██▏       | 98803/450337 [00:16<00:59, 5951.19it/s] 22%|██▏       | 99405/450337 [00:16<01:00, 5793.03it/s] 22%|██▏       | 99989/450337 [00:16<01:00, 5752.84it/s] 22%|██▏       | 100568/450337 [00:16<01:03, 5550.60it/s] 22%|██▏       | 101225/450337 [00:16<00:59, 5836.04it/s] 23%|██▎       | 101813/450337 [00:17<00:59, 5820.16it/s] 23%|██▎       | 102413/450337 [00:17<00:59, 5869.73it/s] 23%|██▎       | 103023/450337 [00:17<00:58, 5933.43it/s] 23%|██▎       | 103618/450337 [00:17<00:59, 5829.85it/s] 23%|██▎       | 104203/450337 [00:17<01:01, 5673.47it/s] 23%|██▎       | 104836/450337 [00:17<00:59, 5852.57it/s] 23%|██▎       | 105423/450337 [00:17<00:59, 5833.15it/s] 24%|██▎       | 106022/450337 [00:17<00:58, 5867.84it/s] 24%|██▎       | 106611/450337 [00:17<00:58, 5874.28it/s] 24%|██▍       | 107200/450337 [00:18<01:00, 5692.46it/s] 24%|██▍       | 107813/450337 [00:18<00:58, 5819.44it/s] 24%|██▍       | 108412/450337 [00:18<00:58, 5868.51it/s] 24%|██▍       | 109000/450337 [00:18<01:01, 5567.51it/s] 24%|██▍       | 109590/450337 [00:18<01:00, 5658.20it/s] 24%|██▍       | 110172/450337 [00:18<00:59, 5704.70it/s] 25%|██▍       | 110815/450337 [00:18<00:57, 5915.09it/s] 25%|██▍       | 111480/450337 [00:18<00:55, 6123.62it/s] 25%|██▍       | 112095/450337 [00:18<00:56, 5946.59it/s] 25%|██▌       | 112767/450337 [00:18<00:54, 6169.06it/s] 25%|██▌       | 113387/450337 [00:19<00:57, 5854.98it/s] 25%|██▌       | 113977/450337 [00:19<00:57, 5812.81it/s] 25%|██▌       | 114562/450337 [00:19<00:58, 5785.48it/s] 26%|██▌       | 115155/450337 [00:19<00:57, 5814.06it/s] 26%|██▌       | 115738/450337 [00:19<01:00, 5512.68it/s] 26%|██▌       | 116313/450337 [00:19<00:59, 5568.66it/s] 26%|██▌       | 116873/450337 [00:19<01:00, 5544.13it/s] 26%|██▌       | 117435/450337 [00:19<00:59, 5560.86it/s] 26%|██▌       | 118018/450337 [00:19<00:59, 5631.35it/s] 26%|██▋       | 118655/450337 [00:19<00:56, 5848.39it/s] 26%|██▋       | 119242/450337 [00:20<00:57, 5779.78it/s] 27%|██▋       | 119821/450337 [00:20<00:57, 5732.41it/s] 27%|██▋       | 120573/450337 [00:20<00:52, 6255.25it/s] 27%|██▋       | 121200/450337 [00:20<00:54, 6063.63it/s] 27%|██▋       | 121809/450337 [00:20<00:57, 5723.74it/s] 27%|██▋       | 122421/450337 [00:20<00:56, 5829.67it/s] 27%|██▋       | 123008/450337 [00:20<00:56, 5743.30it/s] 27%|██▋       | 123586/450337 [00:20<00:57, 5732.38it/s] 28%|██▊       | 124162/450337 [00:20<00:57, 5699.60it/s] 28%|██▊       | 124734/450337 [00:21<00:58, 5613.07it/s] 28%|██▊       | 125360/450337 [00:21<00:56, 5796.32it/s] 28%|██▊       | 126007/450337 [00:21<00:54, 5993.22it/s] 28%|██▊       | 126681/450337 [00:21<00:52, 6209.92it/s] 28%|██▊       | 127304/450337 [00:21<00:53, 6011.02it/s] 28%|██▊       | 127936/450337 [00:21<00:52, 6099.86it/s] 29%|██▊       | 128548/450337 [00:21<00:54, 5856.32it/s] 29%|██▊       | 129190/450337 [00:21<00:53, 6015.50it/s] 29%|██▉       | 129795/450337 [00:21<00:55, 5729.04it/s] 29%|██▉       | 130388/450337 [00:21<00:55, 5784.78it/s] 29%|██▉       | 130970/450337 [00:22<00:56, 5645.65it/s] 29%|██▉       | 131538/450337 [00:22<00:57, 5589.92it/s] 29%|██▉       | 132099/450337 [00:22<01:00, 5240.72it/s] 29%|██▉       | 132793/450337 [00:22<00:55, 5711.74it/s] 30%|██▉       | 133380/450337 [00:22<00:55, 5754.69it/s] 30%|██▉       | 134029/450337 [00:22<00:53, 5965.11it/s] 30%|██▉       | 134720/450337 [00:22<00:50, 6240.29it/s] 30%|███       | 135452/450337 [00:22<00:48, 6552.63it/s] 30%|███       | 136111/450337 [00:22<00:49, 6401.86it/s] 30%|███       | 136805/450337 [00:23<00:47, 6558.15it/s] 31%|███       | 137490/450337 [00:23<00:47, 6643.79it/s] 31%|███       | 138157/450337 [00:23<00:50, 6212.50it/s] 31%|███       | 138800/450337 [00:23<00:49, 6270.73it/s] 31%|███       | 139433/450337 [00:23<00:51, 6025.13it/s] 31%|███       | 140041/450337 [00:23<00:51, 5973.26it/s] 31%|███       | 140642/450337 [00:23<00:52, 5869.33it/s] 31%|███▏      | 141232/450337 [00:23<00:53, 5788.90it/s] 31%|███▏      | 141813/450337 [00:23<00:53, 5719.17it/s] 32%|███▏      | 142402/450337 [00:23<00:53, 5764.78it/s] 32%|███▏      | 142980/450337 [00:24<00:54, 5617.11it/s] 32%|███▏      | 143729/450337 [00:24<00:49, 6155.28it/s] 32%|███▏      | 144348/450337 [00:24<00:50, 6062.18it/s] 32%|███▏      | 145080/450337 [00:24<00:47, 6427.42it/s] 32%|███▏      | 145726/450337 [00:24<00:47, 6381.59it/s] 33%|███▎      | 146367/450337 [00:24<00:47, 6358.44it/s] 33%|███▎      | 147005/450337 [00:24<00:51, 5919.79it/s] 33%|███▎      | 147604/450337 [00:24<00:53, 5674.80it/s] 33%|███▎      | 148178/450337 [00:24<00:53, 5613.35it/s] 33%|███▎      | 148776/450337 [00:25<00:52, 5711.79it/s] 33%|███▎      | 149395/450337 [00:25<00:51, 5847.66it/s] 33%|███▎      | 149983/450337 [00:25<00:53, 5641.81it/s] 33%|███▎      | 150551/450337 [00:25<00:53, 5625.42it/s] 34%|███▎      | 151116/450337 [00:25<00:53, 5595.00it/s] 34%|███▎      | 151705/450337 [00:25<00:52, 5679.71it/s] 34%|███▍      | 152275/450337 [00:25<00:54, 5480.33it/s] 34%|███▍      | 152912/450337 [00:25<00:51, 5732.38it/s] 34%|███▍      | 153576/450337 [00:25<00:49, 5990.86it/s] 34%|███▍      | 154178/450337 [00:26<00:51, 5761.33it/s] 34%|███▍      | 154758/450337 [00:26<00:51, 5740.28it/s] 35%|███▍      | 155419/450337 [00:26<00:49, 5988.34it/s] 35%|███▍      | 156037/450337 [00:26<00:48, 6040.18it/s] 35%|███▍      | 156646/450337 [00:26<00:48, 6048.44it/s] 35%|███▍      | 157287/450337 [00:26<00:47, 6151.24it/s] 35%|███▌      | 157914/450337 [00:26<00:47, 6185.20it/s] 35%|███▌      | 158534/450337 [00:26<00:47, 6155.68it/s] 35%|███▌      | 159151/450337 [00:26<00:50, 5740.29it/s] 35%|███▌      | 159825/450337 [00:26<00:48, 6023.35it/s] 36%|███▌      | 160434/450337 [00:27<00:48, 5970.37it/s] 36%|███▌      | 161105/450337 [00:27<00:46, 6178.82it/s] 36%|███▌      | 161727/450337 [00:27<00:49, 5778.47it/s] 36%|███▌      | 162374/450337 [00:27<00:48, 5969.75it/s] 36%|███▌      | 163080/450337 [00:27<00:45, 6281.20it/s] 36%|███▋      | 163715/450337 [00:27<00:49, 5821.05it/s] 36%|███▋      | 164330/450337 [00:27<00:48, 5911.41it/s] 37%|███▋      | 164981/450337 [00:27<00:47, 6070.87it/s] 37%|███▋      | 165681/450337 [00:27<00:44, 6331.67it/s] 37%|███▋      | 166320/450337 [00:28<00:46, 6103.19it/s] 37%|███▋      | 167009/450337 [00:28<00:44, 6326.45it/s] 37%|███▋      | 167647/450337 [00:28<00:45, 6166.20it/s] 37%|███▋      | 168268/450337 [00:28<00:49, 5743.67it/s] 38%|███▊      | 168936/450337 [00:28<00:46, 5996.51it/s] 38%|███▊      | 169543/450337 [00:28<00:48, 5821.15it/s] 38%|███▊      | 170131/450337 [00:28<00:49, 5635.87it/s] 38%|███▊      | 170699/450337 [00:28<00:49, 5611.04it/s] 38%|███▊      | 171273/450337 [00:28<00:49, 5647.51it/s] 38%|███▊      | 171889/450337 [00:28<00:48, 5790.43it/s] 38%|███▊      | 172471/450337 [00:29<00:48, 5778.15it/s] 38%|███▊      | 173058/450337 [00:29<00:47, 5801.90it/s] 39%|███▊      | 173667/450337 [00:29<00:47, 5883.45it/s] 39%|███▊      | 174257/450337 [00:29<00:48, 5727.63it/s] 39%|███▉      | 174980/450337 [00:29<00:44, 6162.95it/s] 39%|███▉      | 175599/450337 [00:29<00:47, 5752.04it/s] 39%|███▉      | 176181/450337 [00:29<00:48, 5603.92it/s] 39%|███▉      | 176747/450337 [00:29<00:48, 5594.02it/s] 39%|███▉      | 177310/450337 [00:29<00:48, 5601.21it/s] 39%|███▉      | 177873/450337 [00:30<00:51, 5279.89it/s] 40%|███▉      | 178483/450337 [00:30<00:49, 5501.45it/s] 40%|███▉      | 179110/450337 [00:30<00:47, 5710.47it/s] 40%|███▉      | 179686/450337 [00:30<00:47, 5642.63it/s] 40%|████      | 180255/450337 [00:30<00:47, 5654.10it/s] 40%|████      | 180851/450337 [00:30<00:46, 5741.14it/s] 40%|████      | 181427/450337 [00:30<00:48, 5578.62it/s] 40%|████      | 181987/450337 [00:30<00:48, 5484.92it/s] 41%|████      | 182570/450337 [00:30<00:47, 5583.00it/s] 41%|████      | 183168/450337 [00:30<00:46, 5696.40it/s] 41%|████      | 183739/450337 [00:31<00:49, 5336.57it/s] 41%|████      | 184278/450337 [00:31<00:50, 5223.56it/s] 41%|████      | 184896/450337 [00:31<00:48, 5486.04it/s] 41%|████      | 185449/450337 [00:31<00:49, 5334.90it/s] 41%|████▏     | 186002/450337 [00:31<00:49, 5389.63it/s] 41%|████▏     | 186544/450337 [00:31<00:48, 5387.61it/s] 42%|████▏     | 187185/450337 [00:31<00:46, 5680.50it/s] 42%|████▏     | 187778/450337 [00:31<00:45, 5746.10it/s] 42%|████▏     | 188388/450337 [00:31<00:44, 5850.27it/s] 42%|████▏     | 189004/450337 [00:32<00:43, 5940.36it/s] 42%|████▏     | 189631/450337 [00:32<00:43, 6035.96it/s] 42%|████▏     | 190236/450337 [00:32<00:43, 5934.76it/s] 42%|████▏     | 190838/450337 [00:32<00:43, 5958.67it/s] 43%|████▎     | 191435/450337 [00:32<00:44, 5850.78it/s] 43%|████▎     | 192070/450337 [00:32<00:43, 5991.44it/s] 43%|████▎     | 192714/450337 [00:32<00:42, 6121.78it/s] 43%|████▎     | 193327/450337 [00:32<00:43, 5976.01it/s] 43%|████▎     | 193926/450337 [00:32<00:43, 5914.84it/s] 43%|████▎     | 194521/450337 [00:32<00:43, 5921.22it/s] 43%|████▎     | 195114/450337 [00:33<00:43, 5882.77it/s] 43%|████▎     | 195757/450337 [00:33<00:42, 6043.23it/s] 44%|████▎     | 196362/450337 [00:33<00:42, 5911.35it/s] 44%|████▎     | 197011/450337 [00:33<00:41, 6080.62it/s] 44%|████▍     | 197829/450337 [00:33<00:37, 6696.95it/s] 44%|████▍     | 198501/450337 [00:33<00:40, 6163.72it/s] 44%|████▍     | 199127/450337 [00:33<00:41, 6091.83it/s] 44%|████▍     | 199743/450337 [00:33<00:42, 5924.50it/s] 44%|████▍     | 200341/450337 [00:33<00:42, 5817.63it/s] 45%|████▍     | 200926/450337 [00:33<00:42, 5814.19it/s] 45%|████▍     | 201510/450337 [00:34<00:43, 5683.60it/s] 45%|████▍     | 202110/450337 [00:34<00:43, 5770.72it/s] 45%|████▌     | 202689/450337 [00:34<00:43, 5678.64it/s] 45%|████▌     | 203352/450337 [00:34<00:41, 5950.72it/s] 45%|████▌     | 204001/450337 [00:34<00:40, 6104.68it/s] 45%|████▌     | 204614/450337 [00:34<00:42, 5840.12it/s] 46%|████▌     | 205202/450337 [00:34<00:42, 5744.25it/s] 46%|████▌     | 205779/450337 [00:34<00:43, 5641.68it/s] 46%|████▌     | 206449/450337 [00:34<00:41, 5938.58it/s] 46%|████▌     | 207046/450337 [00:35<00:42, 5771.77it/s] 46%|████▌     | 207626/450337 [00:35<00:42, 5732.73it/s] 46%|████▌     | 208201/450337 [00:35<00:42, 5651.30it/s] 46%|████▋     | 208768/450337 [00:35<00:44, 5448.75it/s] 46%|████▋     | 209329/450337 [00:35<00:43, 5494.33it/s] 47%|████▋     | 209983/450337 [00:35<00:41, 5795.98it/s] 47%|████▋     | 210593/450337 [00:35<00:40, 5873.89it/s] 47%|████▋     | 211196/450337 [00:35<00:40, 5919.20it/s] 47%|████▋     | 211790/450337 [00:35<00:41, 5803.24it/s] 47%|████▋     | 212386/450337 [00:35<00:40, 5848.11it/s] 47%|████▋     | 213082/450337 [00:36<00:38, 6174.15it/s] 47%|████▋     | 213701/450337 [00:36<00:41, 5729.40it/s] 48%|████▊     | 214281/450337 [00:36<00:41, 5683.56it/s] 48%|████▊     | 214855/450337 [00:36<00:43, 5357.04it/s] 48%|████▊     | 215416/450337 [00:36<00:43, 5423.34it/s] 48%|████▊     | 215998/450337 [00:36<00:42, 5527.44it/s] 48%|████▊     | 216555/450337 [00:36<00:43, 5414.29it/s] 48%|████▊     | 217145/450337 [00:36<00:42, 5551.83it/s] 48%|████▊     | 217759/450337 [00:36<00:40, 5721.96it/s] 49%|████▊     | 218415/450337 [00:37<00:38, 5965.17it/s] 49%|████▊     | 219033/450337 [00:37<00:38, 6026.91it/s] 49%|████▉     | 219638/450337 [00:37<00:39, 5871.84it/s] 49%|████▉     | 220228/450337 [00:37<00:39, 5783.68it/s] 49%|████▉     | 220808/450337 [00:37<00:40, 5628.31it/s] 49%|████▉     | 221448/450337 [00:37<00:39, 5847.43it/s] 49%|████▉     | 222035/450337 [00:37<00:41, 5564.15it/s] 49%|████▉     | 222596/450337 [00:37<00:41, 5490.92it/s] 50%|████▉     | 223190/450337 [00:37<00:40, 5612.40it/s] 50%|████▉     | 223754/450337 [00:38<00:42, 5317.95it/s] 50%|████▉     | 224453/450337 [00:38<00:39, 5787.42it/s] 50%|████▉     | 225064/450337 [00:38<00:38, 5878.47it/s] 50%|█████     | 225739/450337 [00:38<00:36, 6129.40it/s] 50%|█████     | 226426/450337 [00:38<00:35, 6332.48it/s] 50%|█████     | 227063/450337 [00:38<00:36, 6093.28it/s] 51%|█████     | 227677/450337 [00:38<00:37, 5932.19it/s] 51%|█████     | 228274/450337 [00:38<00:38, 5843.52it/s] 51%|█████     | 228861/450337 [00:38<00:39, 5597.67it/s] 51%|█████     | 229424/450337 [00:38<00:40, 5454.62it/s] 51%|█████     | 229972/450337 [00:39<00:41, 5296.89it/s] 51%|█████     | 230610/450337 [00:39<00:39, 5596.92it/s] 51%|█████▏    | 231181/450337 [00:39<00:38, 5625.61it/s] 51%|█████▏    | 231746/450337 [00:39<00:40, 5435.94it/s] 52%|█████▏    | 232306/450337 [00:39<00:39, 5478.12it/s] 52%|█████▏    | 233001/450337 [00:39<00:36, 5899.66it/s] 52%|█████▏    | 233708/450337 [00:39<00:34, 6229.10it/s] 52%|█████▏    | 234343/450337 [00:39<00:34, 6253.37it/s] 52%|█████▏    | 234971/450337 [00:39<00:34, 6211.61it/s] 52%|█████▏    | 235615/450337 [00:39<00:34, 6277.92it/s] 52%|█████▏    | 236244/450337 [00:40<00:34, 6142.76it/s] 53%|█████▎    | 236860/450337 [00:40<00:36, 5789.63it/s] 53%|█████▎    | 237444/450337 [00:40<00:36, 5802.45it/s] 53%|█████▎    | 238028/450337 [00:40<00:38, 5540.15it/s] 53%|█████▎    | 238806/450337 [00:40<00:34, 6168.19it/s] 53%|█████▎    | 239430/450337 [00:40<00:35, 5945.17it/s] 53%|█████▎    | 240031/450337 [00:40<00:36, 5760.99it/s] 53%|█████▎    | 240803/450337 [00:40<00:33, 6308.93it/s] 54%|█████▎    | 241441/450337 [00:40<00:33, 6269.42it/s] 54%|█████▍    | 242086/450337 [00:41<00:32, 6314.80it/s] 54%|█████▍    | 242721/450337 [00:41<00:33, 6152.63it/s] 54%|█████▍    | 243340/450337 [00:41<00:34, 6009.73it/s] 54%|█████▍    | 243944/450337 [00:41<00:34, 5975.33it/s] 54%|█████▍    | 244604/450337 [00:41<00:33, 6149.94it/s] 54%|█████▍    | 245221/450337 [00:41<00:34, 5887.46it/s] 55%|█████▍    | 245897/450337 [00:41<00:33, 6135.98it/s] 55%|█████▍    | 246618/450337 [00:41<00:31, 6442.38it/s] 55%|█████▍    | 247267/450337 [00:41<00:31, 6453.55it/s] 55%|█████▌    | 247915/450337 [00:42<00:32, 6199.56it/s] 55%|█████▌    | 248539/450337 [00:42<00:33, 6006.75it/s] 55%|█████▌    | 249203/450337 [00:42<00:32, 6186.77it/s] 55%|█████▌    | 249901/450337 [00:42<00:31, 6409.60it/s] 56%|█████▌    | 250546/450337 [00:42<00:32, 6201.83it/s] 56%|█████▌    | 251175/450337 [00:42<00:32, 6219.33it/s] 56%|█████▌    | 251800/450337 [00:42<00:32, 6183.32it/s] 56%|█████▌    | 252420/450337 [00:42<00:32, 6164.69it/s] 56%|█████▌    | 253038/450337 [00:42<00:33, 5922.02it/s] 56%|█████▋    | 253663/450337 [00:42<00:32, 6009.27it/s] 56%|█████▋    | 254297/450337 [00:43<00:32, 6104.66it/s] 57%|█████▋    | 254910/450337 [00:43<00:32, 5938.23it/s] 57%|█████▋    | 255506/450337 [00:43<00:33, 5898.37it/s] 57%|█████▋    | 256182/450337 [00:43<00:31, 6138.53it/s] 57%|█████▋    | 256827/450337 [00:43<00:31, 6229.62it/s] 57%|█████▋    | 257452/450337 [00:43<00:31, 6111.82it/s] 57%|█████▋    | 258081/450337 [00:43<00:31, 6162.87it/s] 57%|█████▋    | 258699/450337 [00:43<00:34, 5525.52it/s] 58%|█████▊    | 259265/450337 [00:43<00:35, 5414.35it/s] 58%|█████▊    | 259880/450337 [00:44<00:33, 5613.26it/s] 58%|█████▊    | 260643/450337 [00:44<00:30, 6175.62it/s] 58%|█████▊    | 261270/450337 [00:44<00:32, 5827.49it/s] 58%|█████▊    | 261926/450337 [00:44<00:31, 6026.60it/s] 58%|█████▊    | 262537/450337 [00:44<00:32, 5746.40it/s] 58%|█████▊    | 263120/450337 [00:44<00:32, 5679.44it/s] 59%|█████▊    | 263743/450337 [00:44<00:32, 5829.65it/s] 59%|█████▊    | 264331/450337 [00:44<00:33, 5579.91it/s] 59%|█████▉    | 264957/450337 [00:44<00:32, 5765.47it/s] 59%|█████▉    | 265538/450337 [00:45<00:32, 5705.40it/s] 59%|█████▉    | 266112/450337 [00:45<00:33, 5515.03it/s] 59%|█████▉    | 266667/450337 [00:45<00:33, 5471.01it/s] 59%|█████▉    | 267283/450337 [00:45<00:32, 5666.45it/s] 59%|█████▉    | 267852/450337 [00:45<00:33, 5527.31it/s] 60%|█████▉    | 268453/450337 [00:45<00:32, 5662.76it/s] 60%|█████▉    | 269154/450337 [00:45<00:29, 6052.67it/s] 60%|█████▉    | 269762/450337 [00:45<00:31, 5769.27it/s] 60%|██████    | 270390/450337 [00:45<00:30, 5912.91it/s] 60%|██████    | 270985/450337 [00:45<00:30, 5885.45it/s] 60%|██████    | 271577/450337 [00:46<00:31, 5736.28it/s] 60%|██████    | 272297/450337 [00:46<00:28, 6157.59it/s] 61%|██████    | 273037/450337 [00:46<00:27, 6514.50it/s] 61%|██████    | 273700/450337 [00:46<00:26, 6544.58it/s] 61%|██████    | 274357/450337 [00:46<00:27, 6366.38it/s] 61%|██████    | 274997/450337 [00:46<00:29, 5871.98it/s] 61%|██████    | 275593/450337 [00:46<00:30, 5652.97it/s] 61%|██████▏   | 276174/450337 [00:46<00:30, 5695.14it/s] 61%|██████▏   | 276749/450337 [00:46<00:31, 5520.92it/s] 62%|██████▏   | 277355/450337 [00:47<00:30, 5671.13it/s] 62%|██████▏   | 277965/450337 [00:47<00:29, 5792.94it/s] 62%|██████▏   | 278638/450337 [00:47<00:28, 6055.99it/s] 62%|██████▏   | 279247/450337 [00:47<00:29, 5763.28it/s] 62%|██████▏   | 279829/450337 [00:47<00:29, 5771.86it/s] 62%|██████▏   | 280497/450337 [00:47<00:28, 6027.29it/s] 62%|██████▏   | 281103/450337 [00:47<00:29, 5727.22it/s] 63%|██████▎   | 281732/450337 [00:47<00:28, 5881.94it/s] 63%|██████▎   | 282325/450337 [00:47<00:29, 5622.94it/s] 63%|██████▎   | 282915/450337 [00:47<00:29, 5693.70it/s] 63%|██████▎   | 283539/450337 [00:48<00:28, 5849.84it/s] 63%|██████▎   | 284154/450337 [00:48<00:27, 5935.26it/s] 63%|██████▎   | 284751/450337 [00:48<00:28, 5738.72it/s] 63%|██████▎   | 285328/450337 [00:48<00:28, 5738.66it/s] 64%|██████▎   | 285981/450337 [00:48<00:27, 5966.68it/s] 64%|██████▎   | 286580/450337 [00:48<00:29, 5620.60it/s] 64%|██████▍   | 287159/450337 [00:48<00:28, 5667.56it/s] 64%|██████▍   | 287759/450337 [00:48<00:28, 5761.10it/s] 64%|██████▍   | 288403/450337 [00:48<00:27, 5958.75it/s] 64%|██████▍   | 289057/450337 [00:49<00:26, 6129.11it/s] 64%|██████▍   | 289673/450337 [00:49<00:28, 5686.90it/s] 64%|██████▍   | 290250/450337 [00:49<00:28, 5624.01it/s] 65%|██████▍   | 290877/450337 [00:49<00:27, 5804.19it/s] 65%|██████▍   | 291463/450337 [00:49<00:27, 5797.14it/s] 65%|██████▍   | 292046/450337 [00:49<00:27, 5673.54it/s] 65%|██████▍   | 292697/450337 [00:49<00:26, 5911.96it/s] 65%|██████▌   | 293291/450337 [00:49<00:27, 5732.60it/s] 65%|██████▌   | 293867/450337 [00:49<00:27, 5624.00it/s] 65%|██████▌   | 294525/450337 [00:49<00:26, 5898.07it/s] 66%|██████▌   | 295185/450337 [00:50<00:25, 6098.43it/s] 66%|██████▌   | 295798/450337 [00:50<00:25, 6051.30it/s] 66%|██████▌   | 296405/450337 [00:50<00:26, 5887.38it/s] 66%|██████▌   | 297031/450337 [00:50<00:25, 5992.63it/s] 66%|██████▌   | 297632/450337 [00:50<00:25, 5969.69it/s] 66%|██████▌   | 298258/450337 [00:50<00:25, 6038.23it/s] 66%|██████▋   | 298863/450337 [00:50<00:25, 5993.54it/s] 67%|██████▋   | 299504/450337 [00:50<00:24, 6112.60it/s] 67%|██████▋   | 300116/450337 [00:50<00:24, 6113.17it/s] 67%|██████▋   | 300728/450337 [00:51<00:25, 5968.57it/s] 67%|██████▋   | 301326/450337 [00:51<00:25, 5756.15it/s] 67%|██████▋   | 301904/450337 [00:51<00:27, 5427.76it/s] 67%|██████▋   | 302588/450337 [00:51<00:25, 5818.55it/s] 67%|██████▋   | 303232/450337 [00:51<00:24, 5994.63it/s] 67%|██████▋   | 303849/450337 [00:51<00:24, 6040.37it/s] 68%|██████▊   | 304457/450337 [00:51<00:24, 5854.95it/s] 68%|██████▊   | 305059/450337 [00:51<00:24, 5896.47it/s] 68%|██████▊   | 305652/450337 [00:51<00:25, 5683.37it/s] 68%|██████▊   | 306295/450337 [00:51<00:24, 5896.48it/s] 68%|██████▊   | 306888/450337 [00:52<00:25, 5619.89it/s] 68%|██████▊   | 307551/450337 [00:52<00:24, 5903.62it/s] 68%|██████▊   | 308147/450337 [00:52<00:25, 5667.76it/s] 69%|██████▊   | 308719/450337 [00:52<00:25, 5507.67it/s] 69%|██████▊   | 309606/450337 [00:52<00:21, 6450.59it/s] 69%|██████▉   | 310260/450337 [00:52<00:22, 6155.60it/s] 69%|██████▉   | 310884/450337 [00:52<00:22, 6098.27it/s] 69%|██████▉   | 311500/450337 [00:52<00:23, 6010.88it/s] 69%|██████▉   | 312155/450337 [00:52<00:22, 6160.74it/s] 69%|██████▉   | 312775/450337 [00:53<00:22, 6081.33it/s] 70%|██████▉   | 313393/450337 [00:53<00:22, 6109.02it/s] 70%|██████▉   | 314087/450337 [00:53<00:21, 6351.51it/s] 70%|██████▉   | 314725/450337 [00:53<00:22, 6058.02it/s] 70%|███████   | 315335/450337 [00:53<00:23, 5801.62it/s] 70%|███████   | 315940/450337 [00:53<00:22, 5860.09it/s] 70%|███████   | 316530/450337 [00:53<00:23, 5733.63it/s] 70%|███████   | 317123/450337 [00:53<00:23, 5789.52it/s] 71%|███████   | 317704/450337 [00:53<00:24, 5440.14it/s] 71%|███████   | 318355/450337 [00:54<00:22, 5738.96it/s] 71%|███████   | 318935/450337 [00:54<00:23, 5628.12it/s] 71%|███████   | 319553/450337 [00:54<00:22, 5783.72it/s] 71%|███████   | 320135/450337 [00:54<00:23, 5584.31it/s] 71%|███████   | 320722/450337 [00:54<00:22, 5665.01it/s] 71%|███████▏  | 321437/450337 [00:54<00:21, 6093.56it/s] 72%|███████▏  | 322050/450337 [00:54<00:22, 5825.63it/s] 72%|███████▏  | 322638/450337 [00:54<00:22, 5687.32it/s] 72%|███████▏  | 323227/450337 [00:54<00:22, 5743.01it/s] 72%|███████▏  | 323804/450337 [00:54<00:22, 5686.26it/s] 72%|███████▏  | 324433/450337 [00:55<00:21, 5860.94it/s] 72%|███████▏  | 325026/450337 [00:55<00:21, 5880.28it/s] 72%|███████▏  | 325616/450337 [00:55<00:22, 5497.74it/s] 72%|███████▏  | 326218/450337 [00:55<00:21, 5644.75it/s] 73%|███████▎  | 326912/450337 [00:55<00:20, 6016.62it/s] 73%|███████▎  | 327548/450337 [00:55<00:20, 6115.48it/s] 73%|███████▎  | 328306/450337 [00:55<00:18, 6545.23it/s] 73%|███████▎  | 328965/450337 [00:55<00:18, 6408.88it/s] 73%|███████▎  | 329609/450337 [00:55<00:20, 5917.34it/s] 73%|███████▎  | 330210/450337 [00:56<00:21, 5625.69it/s] 73%|███████▎  | 330781/450337 [00:56<00:21, 5632.11it/s] 74%|███████▎  | 331362/450337 [00:56<00:20, 5676.90it/s] 74%|███████▎  | 331934/450337 [00:56<00:20, 5646.39it/s] 74%|███████▍  | 332580/450337 [00:56<00:20, 5880.41it/s] 74%|███████▍  | 333171/450337 [00:56<00:20, 5755.51it/s] 74%|███████▍  | 333804/450337 [00:56<00:19, 5921.57it/s] 74%|███████▍  | 334399/450337 [00:56<00:19, 5852.36it/s] 74%|███████▍  | 334988/450337 [00:56<00:19, 5862.73it/s] 75%|███████▍  | 335578/450337 [00:56<00:19, 5871.39it/s] 75%|███████▍  | 336187/450337 [00:57<00:19, 5935.64it/s] 75%|███████▍  | 336782/450337 [00:57<00:19, 5901.34it/s] 75%|███████▍  | 337432/450337 [00:57<00:18, 6076.61it/s] 75%|███████▌  | 338041/450337 [00:57<00:18, 6057.07it/s] 75%|███████▌  | 338651/450337 [00:57<00:18, 6068.33it/s] 75%|███████▌  | 339272/450337 [00:57<00:18, 6107.50it/s] 75%|███████▌  | 339883/450337 [00:57<00:19, 5780.29it/s] 76%|███████▌  | 340465/450337 [00:57<00:19, 5551.90it/s] 76%|███████▌  | 341053/450337 [00:57<00:19, 5638.44it/s] 76%|███████▌  | 341680/450337 [00:58<00:18, 5817.42it/s] 76%|███████▌  | 342300/450337 [00:58<00:18, 5927.93it/s] 76%|███████▌  | 342905/450337 [00:58<00:18, 5952.08it/s] 76%|███████▋  | 343526/450337 [00:58<00:17, 6022.81it/s] 76%|███████▋  | 344190/450337 [00:58<00:17, 6204.56it/s] 77%|███████▋  | 344812/450337 [00:58<00:17, 5972.89it/s] 77%|███████▋  | 345422/450337 [00:58<00:17, 6007.44it/s] 77%|███████▋  | 346025/450337 [00:58<00:19, 5488.79it/s] 77%|███████▋  | 346679/450337 [00:58<00:17, 5771.09it/s] 77%|███████▋  | 347265/450337 [00:58<00:17, 5779.25it/s] 77%|███████▋  | 347850/450337 [00:59<00:17, 5789.63it/s] 77%|███████▋  | 348434/450337 [00:59<00:17, 5681.78it/s] 78%|███████▊  | 349048/450337 [00:59<00:17, 5808.55it/s] 78%|███████▊  | 349632/450337 [00:59<00:17, 5815.19it/s] 78%|███████▊  | 350454/450337 [00:59<00:15, 6518.05it/s] 78%|███████▊  | 351109/450337 [00:59<00:16, 6079.99it/s] 78%|███████▊  | 351727/450337 [00:59<00:16, 6105.71it/s] 78%|███████▊  | 352343/450337 [00:59<00:17, 5636.89it/s] 78%|███████▊  | 352917/450337 [00:59<00:17, 5533.07it/s] 79%|███████▊  | 353547/450337 [01:00<00:16, 5737.73it/s] 79%|███████▊  | 354127/450337 [01:00<00:17, 5557.31it/s] 79%|███████▉  | 354824/450337 [01:00<00:16, 5949.35it/s] 79%|███████▉  | 355425/450337 [01:00<00:16, 5640.03it/s] 79%|███████▉  | 356069/450337 [01:00<00:16, 5861.97it/s] 79%|███████▉  | 356662/450337 [01:00<00:16, 5713.14it/s] 79%|███████▉  | 357238/450337 [01:00<00:17, 5424.45it/s] 79%|███████▉  | 357843/450337 [01:00<00:16, 5595.62it/s] 80%|███████▉  | 358408/450337 [01:00<00:16, 5574.26it/s] 80%|███████▉  | 359032/450337 [01:00<00:15, 5763.17it/s] 80%|███████▉  | 359612/450337 [01:01<00:15, 5760.19it/s] 80%|███████▉  | 360249/450337 [01:01<00:15, 5938.36it/s] 80%|████████  | 360883/450337 [01:01<00:14, 6056.54it/s] 80%|████████  | 361498/450337 [01:01<00:14, 6083.06it/s] 80%|████████  | 362250/450337 [01:01<00:13, 6501.78it/s] 81%|████████  | 362902/450337 [01:01<00:13, 6377.20it/s] 81%|████████  | 363541/450337 [01:01<00:13, 6344.14it/s] 81%|████████  | 364177/450337 [01:01<00:13, 6281.53it/s] 81%|████████  | 364816/450337 [01:01<00:13, 6297.38it/s] 81%|████████  | 365451/450337 [01:02<00:13, 6312.13it/s] 81%|████████▏ | 366094/450337 [01:02<00:13, 6346.75it/s] 81%|████████▏ | 366729/450337 [01:02<00:14, 5953.80it/s] 82%|████████▏ | 367365/450337 [01:02<00:13, 6066.71it/s] 82%|████████▏ | 367976/450337 [01:02<00:14, 5584.33it/s] 82%|████████▏ | 368584/450337 [01:02<00:14, 5716.05it/s] 82%|████████▏ | 369234/450337 [01:02<00:13, 5936.05it/s] 82%|████████▏ | 369835/450337 [01:02<00:13, 5952.13it/s] 82%|████████▏ | 370436/450337 [01:02<00:13, 5897.72it/s] 82%|████████▏ | 371030/450337 [01:02<00:13, 5815.60it/s] 83%|████████▎ | 371622/450337 [01:03<00:13, 5841.77it/s] 83%|████████▎ | 372208/450337 [01:03<00:13, 5620.46it/s] 83%|████████▎ | 372832/450337 [01:03<00:13, 5794.79it/s] 83%|████████▎ | 373444/450337 [01:03<00:13, 5885.85it/s] 83%|████████▎ | 374035/450337 [01:03<00:13, 5794.43it/s] 83%|████████▎ | 374719/450337 [01:03<00:12, 6095.28it/s] 83%|████████▎ | 375331/450337 [01:03<00:12, 5954.70it/s] 83%|████████▎ | 375929/450337 [01:03<00:13, 5571.19it/s] 84%|████████▎ | 376507/450337 [01:03<00:13, 5618.97it/s] 84%|████████▎ | 377074/450337 [01:04<00:13, 5422.12it/s] 84%|████████▍ | 377648/450337 [01:04<00:13, 5508.74it/s] 84%|████████▍ | 378332/450337 [01:04<00:12, 5887.80it/s] 84%|████████▍ | 378925/450337 [01:04<00:12, 5892.21it/s] 84%|████████▍ | 379553/450337 [01:04<00:11, 6003.97it/s] 84%|████████▍ | 380249/450337 [01:04<00:11, 6285.29it/s] 85%|████████▍ | 380911/450337 [01:04<00:10, 6383.25it/s] 85%|████████▍ | 381551/450337 [01:04<00:11, 6182.65it/s] 85%|████████▍ | 382172/450337 [01:04<00:11, 5782.20it/s] 85%|████████▍ | 382757/450337 [01:04<00:12, 5615.67it/s] 85%|████████▌ | 383338/450337 [01:05<00:11, 5669.58it/s] 85%|████████▌ | 383909/450337 [01:05<00:11, 5562.85it/s] 85%|████████▌ | 384652/450337 [01:05<00:10, 6093.00it/s] 86%|████████▌ | 385266/450337 [01:05<00:10, 6032.65it/s] 86%|████████▌ | 385873/450337 [01:05<00:10, 6007.54it/s] 86%|████████▌ | 386476/450337 [01:05<00:11, 5582.52it/s] 86%|████████▌ | 387168/450337 [01:05<00:10, 5953.09it/s] 86%|████████▌ | 387887/450337 [01:05<00:09, 6304.38it/s] 86%|████████▋ | 388525/450337 [01:05<00:10, 5896.94it/s] 86%|████████▋ | 389134/450337 [01:06<00:10, 5949.80it/s] 87%|████████▋ | 389773/450337 [01:06<00:09, 6065.46it/s] 87%|████████▋ | 390386/450337 [01:06<00:09, 6078.62it/s] 87%|████████▋ | 391002/450337 [01:06<00:09, 6097.80it/s] 87%|████████▋ | 391615/450337 [01:06<00:10, 5682.11it/s] 87%|████████▋ | 392191/450337 [01:06<00:10, 5476.98it/s] 87%|████████▋ | 392775/450337 [01:06<00:10, 5573.66it/s] 87%|████████▋ | 393338/450337 [01:06<00:10, 5466.01it/s] 87%|████████▋ | 393970/450337 [01:06<00:09, 5703.82it/s] 88%|████████▊ | 394566/450337 [01:06<00:09, 5771.76it/s] 88%|████████▊ | 395146/450337 [01:07<00:09, 5769.73it/s] 88%|████████▊ | 395768/450337 [01:07<00:09, 5895.51it/s] 88%|████████▊ | 396360/450337 [01:07<00:09, 5801.98it/s] 88%|████████▊ | 396942/450337 [01:07<00:09, 5681.39it/s] 88%|████████▊ | 397512/450337 [01:07<00:09, 5489.18it/s] 88%|████████▊ | 398213/450337 [01:07<00:08, 5924.00it/s] 89%|████████▊ | 398853/450337 [01:07<00:08, 6057.94it/s] 89%|████████▊ | 399509/450337 [01:07<00:08, 6202.32it/s] 89%|████████▉ | 400132/450337 [01:07<00:08, 6140.54it/s] 89%|████████▉ | 400788/450337 [01:08<00:07, 6257.59it/s] 89%|████████▉ | 401416/450337 [01:08<00:08, 5952.45it/s] 89%|████████▉ | 402016/450337 [01:08<00:08, 5506.31it/s] 89%|████████▉ | 402575/450337 [01:08<00:08, 5349.01it/s] 90%|████████▉ | 403170/450337 [01:08<00:08, 5511.62it/s] 90%|████████▉ | 403776/450337 [01:08<00:08, 5665.62it/s] 90%|████████▉ | 404348/450337 [01:08<00:08, 5569.12it/s] 90%|████████▉ | 404995/450337 [01:08<00:07, 5825.52it/s] 90%|█████████ | 405582/450337 [01:08<00:08, 5575.15it/s] 90%|█████████ | 406175/450337 [01:09<00:07, 5673.47it/s] 90%|█████████ | 406746/450337 [01:09<00:07, 5501.74it/s] 90%|█████████ | 407393/450337 [01:09<00:07, 5775.41it/s] 91%|█████████ | 407975/450337 [01:09<00:07, 5464.04it/s] 91%|█████████ | 408616/450337 [01:09<00:07, 5727.47it/s] 91%|█████████ | 409240/450337 [01:09<00:07, 5868.95it/s] 91%|█████████ | 409832/450337 [01:09<00:07, 5652.87it/s] 91%|█████████ | 410417/450337 [01:09<00:06, 5707.39it/s] 91%|█████████▏| 411025/450337 [01:09<00:06, 5801.51it/s] 91%|█████████▏| 411608/450337 [01:09<00:06, 5689.94it/s] 92%|█████████▏| 412180/450337 [01:10<00:06, 5598.00it/s] 92%|█████████▏| 412742/450337 [01:10<00:07, 5175.53it/s] 92%|█████████▏| 413292/450337 [01:10<00:07, 5263.65it/s] 92%|█████████▏| 413864/450337 [01:10<00:06, 5391.97it/s] 92%|█████████▏| 414408/450337 [01:10<00:06, 5203.45it/s] 92%|█████████▏| 414933/450337 [01:10<00:06, 5159.94it/s] 92%|█████████▏| 415529/450337 [01:10<00:06, 5388.50it/s] 92%|█████████▏| 416100/450337 [01:10<00:06, 5474.13it/s] 93%|█████████▎| 416663/450337 [01:10<00:06, 5505.29it/s] 93%|█████████▎| 417226/450337 [01:11<00:05, 5532.34it/s] 93%|█████████▎| 417863/450337 [01:11<00:05, 5775.03it/s] 93%|█████████▎| 418442/450337 [01:11<00:05, 5489.49it/s] 93%|█████████▎| 419066/450337 [01:11<00:05, 5704.46it/s] 93%|█████████▎| 419665/450337 [01:11<00:05, 5787.38it/s] 93%|█████████▎| 420271/450337 [01:11<00:05, 5862.60it/s] 93%|█████████▎| 420860/450337 [01:11<00:05, 5815.49it/s] 94%|█████████▎| 421444/450337 [01:11<00:05, 5440.51it/s] 94%|█████████▎| 422026/450337 [01:11<00:05, 5543.17it/s] 94%|█████████▍| 422684/450337 [01:11<00:04, 5836.87it/s] 94%|█████████▍| 423273/450337 [01:12<00:04, 5829.28it/s] 94%|█████████▍| 423859/450337 [01:12<00:04, 5828.28it/s] 94%|█████████▍| 424444/450337 [01:12<00:04, 5728.56it/s] 94%|█████████▍| 425019/450337 [01:12<00:04, 5644.89it/s] 95%|█████████▍| 425585/450337 [01:12<00:04, 5606.76it/s] 95%|█████████▍| 426277/450337 [01:12<00:04, 5983.40it/s] 95%|█████████▍| 426947/450337 [01:12<00:03, 6194.09it/s] 95%|█████████▍| 427568/450337 [01:12<00:03, 6012.91it/s] 95%|█████████▌| 428172/450337 [01:12<00:03, 5859.14it/s] 95%|█████████▌| 428845/450337 [01:13<00:03, 6104.19it/s] 95%|█████████▌| 429469/450337 [01:13<00:03, 6143.05it/s] 96%|█████████▌| 430097/450337 [01:13<00:03, 6177.92it/s] 96%|█████████▌| 430717/450337 [01:13<00:03, 6015.87it/s] 96%|█████████▌| 431321/450337 [01:13<00:03, 5638.67it/s] 96%|█████████▌| 431891/450337 [01:13<00:03, 5534.70it/s] 96%|█████████▌| 432702/450337 [01:13<00:02, 6258.21it/s] 96%|█████████▌| 433335/450337 [01:13<00:02, 6190.31it/s] 96%|█████████▋| 433959/450337 [01:13<00:02, 6167.02it/s] 97%|█████████▋| 434580/450337 [01:13<00:02, 5713.44it/s] 97%|█████████▋| 435160/450337 [01:14<00:02, 5704.83it/s] 97%|█████████▋| 435808/450337 [01:14<00:02, 5918.12it/s] 97%|█████████▋| 436406/450337 [01:14<00:02, 5613.25it/s] 97%|█████████▋| 436974/450337 [01:14<00:02, 5570.61it/s] 97%|█████████▋| 437579/450337 [01:14<00:02, 5706.26it/s] 97%|█████████▋| 438154/450337 [01:14<00:02, 5716.56it/s] 97%|█████████▋| 438748/450337 [01:14<00:02, 5775.01it/s] 98%|█████████▊| 439328/450337 [01:14<00:01, 5571.07it/s] 98%|█████████▊| 439888/450337 [01:14<00:01, 5449.44it/s] 98%|█████████▊| 440452/450337 [01:15<00:01, 5503.79it/s] 98%|█████████▊| 441026/450337 [01:15<00:01, 5563.46it/s] 98%|█████████▊| 441636/450337 [01:15<00:01, 5713.69it/s] 98%|█████████▊| 442295/450337 [01:15<00:01, 5962.87it/s] 98%|█████████▊| 442893/450337 [01:15<00:01, 5824.48it/s] 98%|█████████▊| 443534/450337 [01:15<00:01, 5995.15it/s] 99%|█████████▊| 444177/450337 [01:15<00:01, 6121.83it/s] 99%|█████████▉| 444791/450337 [01:15<00:01, 5523.40it/s] 99%|█████████▉| 445355/450337 [01:15<00:00, 5352.87it/s] 99%|█████████▉| 445943/450337 [01:15<00:00, 5497.86it/s] 99%|█████████▉| 446500/450337 [01:16<00:00, 5470.06it/s] 99%|█████████▉| 447082/450337 [01:16<00:00, 5565.61it/s] 99%|█████████▉| 447744/450337 [01:16<00:00, 5869.69it/s]100%|█████████▉| 448366/450337 [01:16<00:00, 5961.13it/s]100%|█████████▉| 448965/450337 [01:16<00:00, 5910.46it/s]100%|█████████▉| 449559/450337 [01:16<00:00, 5715.67it/s]100%|█████████▉| 450193/450337 [01:16<00:00, 5893.14it/s]100%|██████████| 450337/450337 [01:16<00:00, 5869.37it/s]

gathering stats for n=1
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 1903/450337 [00:00<00:23, 19003.47it/s]  1%|          | 3967/450337 [00:00<00:22, 19953.26it/s]  1%|▏         | 6202/450337 [00:00<00:21, 21046.62it/s]  2%|▏         | 8307/450337 [00:00<00:22, 19682.10it/s]  2%|▏         | 10288/450337 [00:00<00:22, 19509.19it/s]  3%|▎         | 12247/450337 [00:00<00:23, 19034.63it/s]  3%|▎         | 14286/450337 [00:00<00:22, 19452.13it/s]  4%|▎         | 16237/450337 [00:00<00:22, 19415.70it/s]  4%|▍         | 18209/450337 [00:00<00:22, 19500.47it/s]  5%|▍         | 20278/450337 [00:01<00:21, 19858.56it/s]  5%|▍         | 22267/450337 [00:01<00:21, 19756.42it/s]  5%|▌         | 24501/450337 [00:01<00:20, 20525.41it/s]  6%|▌         | 26556/450337 [00:01<00:20, 20336.13it/s]  6%|▋         | 28592/450337 [00:01<00:20, 20269.31it/s]  7%|▋         | 30621/450337 [00:01<00:21, 19832.90it/s]  7%|▋         | 32607/450337 [00:01<00:21, 19498.50it/s]  8%|▊         | 34633/450337 [00:01<00:21, 19716.27it/s]  8%|▊         | 36607/450337 [00:01<00:21, 19651.19it/s]  9%|▊         | 38574/450337 [00:01<00:21, 18864.70it/s]  9%|▉         | 40531/450337 [00:02<00:21, 19064.84it/s]  9%|▉         | 42444/450337 [00:02<00:21, 18561.52it/s] 10%|▉         | 44410/450337 [00:02<00:21, 18877.77it/s] 10%|█         | 46473/450337 [00:02<00:20, 19382.85it/s] 11%|█         | 48778/450337 [00:02<00:19, 20458.71it/s] 11%|█▏        | 50830/450337 [00:02<00:19, 20281.01it/s] 12%|█▏        | 53031/450337 [00:02<00:19, 20789.01it/s] 12%|█▏        | 55114/450337 [00:02<00:19, 20170.61it/s] 13%|█▎        | 57138/450337 [00:02<00:19, 19844.89it/s] 13%|█▎        | 59200/450337 [00:02<00:19, 20069.24it/s] 14%|█▎        | 61340/450337 [00:03<00:19, 20456.78it/s] 14%|█▍        | 63390/450337 [00:03<00:19, 20059.07it/s] 15%|█▍        | 65400/450337 [00:03<00:19, 20027.88it/s] 15%|█▍        | 67406/450337 [00:03<00:19, 20013.96it/s] 15%|█▌        | 69410/450337 [00:03<00:19, 19809.45it/s] 16%|█▌        | 71393/450337 [00:03<00:19, 19656.54it/s] 16%|█▋        | 73483/450337 [00:03<00:18, 20016.91it/s] 17%|█▋        | 75630/450337 [00:03<00:18, 20440.35it/s] 17%|█▋        | 77715/450337 [00:03<00:18, 20557.43it/s] 18%|█▊        | 79772/450337 [00:04<00:18, 19603.02it/s] 18%|█▊        | 81792/450337 [00:04<00:18, 19768.28it/s] 19%|█▊        | 83825/450337 [00:04<00:18, 19931.81it/s] 19%|█▉        | 85824/450337 [00:04<00:18, 19884.13it/s] 20%|█▉        | 87934/450337 [00:04<00:17, 20241.68it/s] 20%|█▉        | 89962/450337 [00:04<00:18, 19746.89it/s] 20%|██        | 91942/450337 [00:04<00:18, 19754.53it/s] 21%|██        | 93942/450337 [00:04<00:17, 19826.56it/s] 21%|██▏       | 96002/450337 [00:04<00:17, 20053.87it/s] 22%|██▏       | 98178/450337 [00:04<00:17, 20557.68it/s] 22%|██▏       | 100236/450337 [00:05<00:17, 19696.85it/s] 23%|██▎       | 102215/450337 [00:05<00:17, 19683.32it/s] 23%|██▎       | 104190/450337 [00:05<00:18, 19197.98it/s] 24%|██▎       | 106208/450337 [00:05<00:17, 19479.27it/s] 24%|██▍       | 108162/450337 [00:05<00:17, 19420.03it/s] 24%|██▍       | 110108/450337 [00:05<00:17, 19220.35it/s] 25%|██▍       | 112200/450337 [00:05<00:17, 19718.57it/s] 25%|██▌       | 114175/450337 [00:05<00:17, 19624.40it/s] 26%|██▌       | 116140/450337 [00:05<00:17, 19222.11it/s] 26%|██▌       | 118065/450337 [00:05<00:17, 18899.06it/s] 27%|██▋       | 120114/450337 [00:06<00:17, 19361.11it/s] 27%|██▋       | 122054/450337 [00:06<00:17, 19275.39it/s] 28%|██▊       | 124003/450337 [00:06<00:16, 19338.29it/s] 28%|██▊       | 126014/450337 [00:06<00:16, 19562.64it/s] 28%|██▊       | 128036/450337 [00:06<00:16, 19748.04it/s] 29%|██▉       | 130012/450337 [00:06<00:16, 19176.41it/s] 29%|██▉       | 131934/450337 [00:06<00:17, 18381.35it/s] 30%|██▉       | 134002/450337 [00:06<00:16, 19036.18it/s] 30%|███       | 136159/450337 [00:06<00:15, 19770.30it/s] 31%|███       | 138236/450337 [00:07<00:15, 20059.73it/s] 31%|███       | 140249/450337 [00:07<00:15, 19952.47it/s] 32%|███▏      | 142250/450337 [00:07<00:15, 19647.79it/s] 32%|███▏      | 144303/450337 [00:07<00:15, 19906.04it/s] 33%|███▎      | 146419/450337 [00:07<00:14, 20264.81it/s] 33%|███▎      | 148449/450337 [00:07<00:15, 19554.91it/s] 33%|███▎      | 150412/450337 [00:07<00:15, 19476.00it/s] 34%|███▍      | 152365/450337 [00:07<00:15, 19246.74it/s] 34%|███▍      | 154354/450337 [00:07<00:15, 19432.51it/s] 35%|███▍      | 156401/450337 [00:07<00:14, 19737.33it/s] 35%|███▌      | 158457/450337 [00:08<00:14, 19979.07it/s] 36%|███▌      | 160458/450337 [00:08<00:14, 19425.13it/s] 36%|███▌      | 162405/450337 [00:08<00:14, 19259.18it/s] 36%|███▋      | 164334/450337 [00:08<00:14, 19185.31it/s] 37%|███▋      | 166425/450337 [00:08<00:14, 19685.02it/s] 37%|███▋      | 168396/450337 [00:08<00:14, 19561.55it/s] 38%|███▊      | 170354/450337 [00:08<00:14, 19308.12it/s] 38%|███▊      | 172287/450337 [00:08<00:14, 19230.45it/s] 39%|███▊      | 174217/450337 [00:08<00:14, 19248.33it/s] 39%|███▉      | 176143/450337 [00:08<00:14, 19106.53it/s] 40%|███▉      | 178055/450337 [00:09<00:14, 18743.13it/s] 40%|███▉      | 180006/450337 [00:09<00:14, 18967.07it/s] 40%|████      | 181932/450337 [00:09<00:14, 19048.65it/s] 41%|████      | 183839/450337 [00:09<00:14, 18759.29it/s] 41%|████      | 185717/450337 [00:09<00:14, 18744.56it/s] 42%|████▏     | 187593/450337 [00:09<00:14, 18652.42it/s] 42%|████▏     | 189486/450337 [00:09<00:13, 18729.60it/s] 43%|████▎     | 191394/450337 [00:09<00:13, 18827.34it/s] 43%|████▎     | 193359/450337 [00:09<00:13, 19071.02it/s] 43%|████▎     | 195329/450337 [00:09<00:13, 19254.31it/s] 44%|████▍     | 197484/450337 [00:10<00:12, 19938.14it/s] 44%|████▍     | 199479/450337 [00:10<00:12, 19798.61it/s] 45%|████▍     | 201460/450337 [00:10<00:12, 19245.01it/s] 45%|████▌     | 203509/450337 [00:10<00:12, 19606.56it/s] 46%|████▌     | 205473/450337 [00:10<00:12, 19365.67it/s] 46%|████▌     | 207413/450337 [00:10<00:12, 19252.79it/s] 46%|████▋     | 209341/450337 [00:10<00:12, 19006.20it/s] 47%|████▋     | 211380/450337 [00:10<00:12, 19405.56it/s] 47%|████▋     | 213420/450337 [00:10<00:12, 19697.22it/s] 48%|████▊     | 215392/450337 [00:11<00:12, 19214.41it/s] 48%|████▊     | 217317/450337 [00:11<00:12, 19040.07it/s] 49%|████▊     | 219315/450337 [00:11<00:11, 19309.46it/s] 49%|████▉     | 221249/450337 [00:11<00:11, 19113.41it/s] 50%|████▉     | 223163/450337 [00:11<00:12, 18529.48it/s] 50%|████▉     | 225092/450337 [00:11<00:12, 18748.71it/s] 50%|█████     | 227167/450337 [00:11<00:11, 19332.01it/s] 51%|█████     | 229105/450337 [00:11<00:11, 19057.08it/s] 51%|█████▏    | 231015/450337 [00:11<00:11, 18882.65it/s] 52%|█████▏    | 233001/450337 [00:11<00:11, 19167.70it/s] 52%|█████▏    | 235170/450337 [00:12<00:10, 19911.73it/s] 53%|█████▎    | 237165/450337 [00:12<00:10, 19671.04it/s] 53%|█████▎    | 239197/450337 [00:12<00:10, 19858.92it/s] 54%|█████▎    | 241350/450337 [00:12<00:10, 20342.67it/s] 54%|█████▍    | 243387/450337 [00:12<00:10, 20134.62it/s] 54%|█████▍    | 245403/450337 [00:12<00:10, 20080.09it/s] 55%|█████▍    | 247590/450337 [00:12<00:09, 20605.86it/s] 55%|█████▌    | 249652/450337 [00:12<00:09, 20459.03it/s] 56%|█████▌    | 251700/450337 [00:12<00:09, 20006.29it/s] 56%|█████▋    | 253704/450337 [00:12<00:10, 19602.90it/s] 57%|█████▋    | 255668/450337 [00:13<00:09, 19569.53it/s] 57%|█████▋    | 257805/450337 [00:13<00:09, 20096.39it/s] 58%|█████▊    | 259818/450337 [00:13<00:10, 19031.59it/s] 58%|█████▊    | 261905/450337 [00:13<00:09, 19551.66it/s] 59%|█████▊    | 263872/450337 [00:13<00:09, 19284.93it/s] 59%|█████▉    | 265809/450337 [00:13<00:09, 19056.86it/s] 59%|█████▉    | 267721/450337 [00:13<00:09, 18946.78it/s] 60%|█████▉    | 269691/450337 [00:13<00:09, 19150.49it/s] 60%|██████    | 271689/450337 [00:13<00:09, 19393.73it/s] 61%|██████    | 273923/450337 [00:13<00:08, 20263.16it/s] 61%|██████▏   | 275953/450337 [00:14<00:08, 19771.83it/s] 62%|██████▏   | 277935/450337 [00:14<00:08, 19319.08it/s] 62%|██████▏   | 279872/450337 [00:14<00:08, 19143.66it/s] 63%|██████▎   | 281790/450337 [00:14<00:08, 19032.07it/s] 63%|██████▎   | 283696/450337 [00:14<00:08, 18900.95it/s] 63%|██████▎   | 285593/450337 [00:14<00:08, 18920.18it/s] 64%|██████▍   | 287497/450337 [00:14<00:08, 18951.96it/s] 64%|██████▍   | 289507/450337 [00:14<00:08, 19283.82it/s] 65%|██████▍   | 291437/450337 [00:14<00:08, 19186.99it/s] 65%|██████▌   | 293398/450337 [00:15<00:08, 19299.74it/s] 66%|██████▌   | 295439/450337 [00:15<00:07, 19628.75it/s] 66%|██████▌   | 297403/450337 [00:15<00:07, 19548.39it/s] 66%|██████▋   | 299451/450337 [00:15<00:07, 19824.87it/s] 67%|██████▋   | 301434/450337 [00:15<00:07, 19676.53it/s] 67%|██████▋   | 303413/450337 [00:15<00:07, 19708.13it/s] 68%|██████▊   | 305390/450337 [00:15<00:07, 19725.67it/s] 68%|██████▊   | 307363/450337 [00:15<00:07, 19594.50it/s] 69%|██████▊   | 309323/450337 [00:15<00:07, 19379.02it/s] 69%|██████▉   | 311288/450337 [00:15<00:07, 19458.59it/s] 70%|██████▉   | 313259/450337 [00:16<00:07, 19531.85it/s] 70%|██████▉   | 315213/450337 [00:16<00:06, 19501.75it/s] 70%|███████   | 317164/450337 [00:16<00:06, 19334.43it/s] 71%|███████   | 319098/450337 [00:16<00:06, 19134.05it/s] 71%|███████▏  | 321031/450337 [00:16<00:06, 19191.18it/s] 72%|███████▏  | 322951/450337 [00:16<00:06, 19040.88it/s] 72%|███████▏  | 324993/450337 [00:16<00:06, 19443.61it/s] 73%|███████▎  | 326939/450337 [00:16<00:06, 19426.06it/s] 73%|███████▎  | 329098/450337 [00:16<00:06, 20066.26it/s] 74%|███████▎  | 331106/450337 [00:16<00:06, 19590.40it/s] 74%|███████▍  | 333068/450337 [00:17<00:06, 19398.54it/s] 74%|███████▍  | 335100/450337 [00:17<00:05, 19664.27it/s] 75%|███████▍  | 337138/450337 [00:17<00:05, 19875.01it/s] 75%|███████▌  | 339128/450337 [00:17<00:05, 19781.52it/s] 76%|███████▌  | 341108/450337 [00:17<00:05, 18994.97it/s] 76%|███████▌  | 343066/450337 [00:17<00:05, 19163.34it/s] 77%|███████▋  | 345054/450337 [00:17<00:05, 19370.55it/s] 77%|███████▋  | 346996/450337 [00:17<00:05, 19161.41it/s] 77%|███████▋  | 348916/450337 [00:17<00:05, 19125.61it/s] 78%|███████▊  | 351044/450337 [00:17<00:05, 19759.92it/s] 78%|███████▊  | 353023/450337 [00:18<00:05, 19303.54it/s] 79%|███████▉  | 354987/450337 [00:18<00:04, 19401.64it/s] 79%|███████▉  | 356931/450337 [00:18<00:04, 19233.98it/s] 80%|███████▉  | 358878/450337 [00:18<00:04, 19299.34it/s] 80%|████████  | 360891/450337 [00:18<00:04, 19540.54it/s] 81%|████████  | 363049/450337 [00:18<00:04, 20142.03it/s] 81%|████████  | 365195/450337 [00:18<00:04, 20533.59it/s] 82%|████████▏ | 367250/450337 [00:18<00:04, 20299.42it/s] 82%|████████▏ | 369282/450337 [00:18<00:04, 19572.68it/s] 82%|████████▏ | 371246/450337 [00:19<00:04, 19332.87it/s] 83%|████████▎ | 373184/450337 [00:19<00:04, 19091.14it/s] 83%|████████▎ | 375168/450337 [00:19<00:03, 19306.75it/s] 84%|████████▎ | 377102/450337 [00:19<00:03, 18755.60it/s] 84%|████████▍ | 379142/450337 [00:19<00:03, 19226.39it/s] 85%|████████▍ | 381239/450337 [00:19<00:03, 19732.78it/s] 85%|████████▌ | 383217/450337 [00:19<00:03, 19177.49it/s] 86%|████████▌ | 385277/450337 [00:19<00:03, 19585.57it/s] 86%|████████▌ | 387286/450337 [00:19<00:03, 19724.87it/s] 86%|████████▋ | 389321/450337 [00:19<00:03, 19907.14it/s] 87%|████████▋ | 391315/450337 [00:20<00:02, 19788.94it/s] 87%|████████▋ | 393297/450337 [00:20<00:02, 19367.89it/s] 88%|████████▊ | 395313/450337 [00:20<00:02, 19598.92it/s] 88%|████████▊ | 397276/450337 [00:20<00:02, 19368.72it/s] 89%|████████▊ | 399220/450337 [00:20<00:02, 19387.33it/s] 89%|████████▉ | 401308/450337 [00:20<00:02, 19822.82it/s] 90%|████████▉ | 403293/450337 [00:20<00:02, 19038.37it/s] 90%|████████▉ | 405205/450337 [00:20<00:02, 18933.55it/s] 90%|█████████ | 407129/450337 [00:20<00:02, 19017.43it/s] 91%|█████████ | 409118/450337 [00:20<00:02, 19273.05it/s] 91%|█████████▏| 411049/450337 [00:21<00:02, 19117.26it/s] 92%|█████████▏| 412963/450337 [00:21<00:02, 18489.94it/s] 92%|█████████▏| 414818/450337 [00:21<00:01, 18319.33it/s] 93%|█████████▎| 416736/450337 [00:21<00:01, 18568.92it/s] 93%|█████████▎| 418654/450337 [00:21<00:01, 18746.57it/s] 93%|█████████▎| 420682/450337 [00:21<00:01, 19194.57it/s] 94%|█████████▍| 422605/450337 [00:21<00:01, 19157.40it/s] 94%|█████████▍| 424524/450337 [00:21<00:01, 19164.63it/s] 95%|█████████▍| 426566/450337 [00:21<00:01, 19538.18it/s] 95%|█████████▌| 428585/450337 [00:21<00:01, 19731.77it/s] 96%|█████████▌| 430560/450337 [00:22<00:01, 19416.17it/s] 96%|█████████▌| 432504/450337 [00:22<00:00, 19205.98it/s] 96%|█████████▋| 434450/450337 [00:22<00:00, 19277.73it/s] 97%|█████████▋| 436379/450337 [00:22<00:00, 19198.30it/s] 97%|█████████▋| 438352/450337 [00:22<00:00, 19353.65it/s] 98%|█████████▊| 440289/450337 [00:22<00:00, 18957.12it/s] 98%|█████████▊| 442312/450337 [00:22<00:00, 19321.73it/s] 99%|█████████▊| 444297/450337 [00:22<00:00, 19476.44it/s] 99%|█████████▉| 446247/450337 [00:22<00:00, 18963.33it/s]100%|█████████▉| 448226/450337 [00:23<00:00, 19203.12it/s]100%|█████████▉| 450226/450337 [00:23<00:00, 19436.05it/s]100%|██████████| 450337/450337 [00:23<00:00, 19485.01it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 25.28it/s]2022-03-03 10:55:21 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-03 10:55:21 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-03 10:55:21 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-03 10:55:21 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-03 10:55:21 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-03 10:55:21 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-03 10:55:21 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-03 10:55:22 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-03 10:55:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 10:55:22 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-03 10:55:22 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 10:55:22 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-03 10:55:22 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-03 10:55:22 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_last.pt
2022-03-03 10:55:22 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_last.pt
2022-03-03 10:55:22 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-03 10:55:22 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-03 10:55:22 | INFO | fairseq.trainer | begin training epoch 1
2022-03-03 10:55:22 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-03 10:55:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-03 10:55:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 10:55:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 10:55:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 10:56:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 11:03:24 | INFO | train_inner | epoch 001:    105 / 393 loss=17.035, ppl=134262, wps=14487.6, ups=0.22, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.349, loss_scale=4, train_wall=476, gb_free=10.1, wall=482
2022-03-03 11:10:51 | INFO | train_inner | epoch 001:    205 / 393 loss=14.701, ppl=26625.7, wps=14634.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.543, loss_scale=4, train_wall=443, gb_free=10.1, wall=930
2022-03-03 11:18:19 | INFO | train_inner | epoch 001:    305 / 393 loss=12.671, ppl=6521.18, wps=14631.6, ups=0.22, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.07, loss_scale=4, train_wall=443, gb_free=10.1, wall=1378
2022-03-03 11:24:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 11:24:58 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.879 | ppl 1883.4 | wps 33769.6 | wpb 2034.1 | bsz 4 | num_updates 388
2022-03-03 11:24:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-03 11:24:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 11:25:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 11:25:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 1 @ 388 updates, score 10.879) (writing took 4.568231836892664 seconds)
2022-03-03 11:25:02 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-03 11:25:02 | INFO | train | epoch 001 | loss 14.005 | ppl 16442.9 | wps 14503.5 | ups 0.22 | wpb 65460.6 | bsz 127.9 | num_updates 388 | lr 4.85903e-05 | gnorm 1.677 | loss_scale 4 | train_wall 1750 | gb_free 10.1 | wall 1781
2022-03-03 11:25:02 | INFO | fairseq.trainer | begin training epoch 2
2022-03-03 11:25:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 11:25:56 | INFO | train_inner | epoch 002:     12 / 393 loss=11.225, ppl=2394.37, wps=14276.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.595, loss_scale=4, train_wall=441, gb_free=10.1, wall=1835
2022-03-03 11:33:24 | INFO | train_inner | epoch 002:    112 / 393 loss=10.667, ppl=1625.61, wps=14633.6, ups=0.22, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.462, loss_scale=4, train_wall=443, gb_free=10.1, wall=2283
2022-03-03 11:40:52 | INFO | train_inner | epoch 002:    212 / 393 loss=10.385, ppl=1337.29, wps=14642, ups=0.22, wpb=65535.4, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.534, loss_scale=8, train_wall=443, gb_free=10.1, wall=2730
2022-03-03 11:48:19 | INFO | train_inner | epoch 002:    312 / 393 loss=10.144, ppl=1131.32, wps=14638.6, ups=0.22, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.566, loss_scale=8, train_wall=443, gb_free=10.1, wall=3178
2022-03-03 11:54:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 11:54:27 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.824 | ppl 906.5 | wps 33701 | wpb 2034.1 | bsz 4 | num_updates 781 | best_loss 9.824
2022-03-03 11:54:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 781 updates
2022-03-03 11:54:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 11:54:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 11:54:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 2 @ 781 updates, score 9.824) (writing took 4.706556846387684 seconds)
2022-03-03 11:54:31 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-03 11:54:31 | INFO | train | epoch 002 | loss 10.321 | ppl 1279.48 | wps 14543.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 0.539 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 3550
2022-03-03 11:54:31 | INFO | fairseq.trainer | begin training epoch 3
2022-03-03 11:54:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 11:55:57 | INFO | train_inner | epoch 003:     19 / 393 loss=9.934, ppl=978.06, wps=14272, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.654, loss_scale=8, train_wall=441, gb_free=10.1, wall=3635
2022-03-03 12:03:25 | INFO | train_inner | epoch 003:    119 / 393 loss=9.734, ppl=851.56, wps=14627.5, ups=0.22, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.733, loss_scale=8, train_wall=443, gb_free=10.1, wall=4083
2022-03-03 12:10:52 | INFO | train_inner | epoch 003:    219 / 393 loss=9.56, ppl=754.85, wps=14636.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.809, loss_scale=8, train_wall=443, gb_free=10.1, wall=4531
2022-03-03 12:18:20 | INFO | train_inner | epoch 003:    319 / 393 loss=9.41, ppl=680.3, wps=14629.5, ups=0.22, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.804, loss_scale=16, train_wall=443, gb_free=10.1, wall=4979
2022-03-03 12:23:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:23:56 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.173 | ppl 577.13 | wps 33815.5 | wpb 2034.1 | bsz 4 | num_updates 1174 | best_loss 9.173
2022-03-03 12:23:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1174 updates
2022-03-03 12:23:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 12:24:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 12:24:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 3 @ 1174 updates, score 9.173) (writing took 4.583618811331689 seconds)
2022-03-03 12:24:01 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-03 12:24:01 | INFO | train | epoch 003 | loss 9.53 | ppl 739.48 | wps 14539.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1174 | lr 0.000146821 | gnorm 0.79 | loss_scale 16 | train_wall 1739 | gb_free 10.1 | wall 5319
2022-03-03 12:24:01 | INFO | fairseq.trainer | begin training epoch 4
2022-03-03 12:24:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:25:57 | INFO | train_inner | epoch 004:     26 / 393 loss=9.27, ppl=617.43, wps=14278.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.795, loss_scale=16, train_wall=441, gb_free=10.1, wall=5436
2022-03-03 12:33:25 | INFO | train_inner | epoch 004:    126 / 393 loss=9.118, ppl=555.8, wps=14639.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.815, loss_scale=16, train_wall=443, gb_free=10.1, wall=5883
2022-03-03 12:40:53 | INFO | train_inner | epoch 004:    226 / 393 loss=9.012, ppl=516.24, wps=14633.3, ups=0.22, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.832, loss_scale=16, train_wall=443, gb_free=10.1, wall=6331
2022-03-03 12:48:21 | INFO | train_inner | epoch 004:    326 / 393 loss=8.9, ppl=477.6, wps=14636.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.815, loss_scale=16, train_wall=443, gb_free=10.1, wall=6779
2022-03-03 12:53:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:53:25 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.711 | ppl 418.92 | wps 33701 | wpb 2034.1 | bsz 4 | num_updates 1567 | best_loss 8.711
2022-03-03 12:53:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1567 updates
2022-03-03 12:53:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 12:53:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 12:53:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 4 @ 1567 updates, score 8.711) (writing took 4.459388214163482 seconds)
2022-03-03 12:53:30 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-03 12:53:30 | INFO | train | epoch 004 | loss 8.99 | ppl 508.39 | wps 14543.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1567 | lr 0.000195936 | gnorm 0.814 | loss_scale 32 | train_wall 1738 | gb_free 10.1 | wall 7088
2022-03-03 12:53:30 | INFO | fairseq.trainer | begin training epoch 5
2022-03-03 12:53:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:55:58 | INFO | train_inner | epoch 005:     33 / 393 loss=8.78, ppl=439.56, wps=14273.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.846, loss_scale=32, train_wall=441, gb_free=10.1, wall=7236
2022-03-03 13:03:26 | INFO | train_inner | epoch 005:    133 / 393 loss=8.664, ppl=405.76, wps=14625.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.799, loss_scale=32, train_wall=443, gb_free=10.1, wall=7684
2022-03-03 13:09:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 13:10:58 | INFO | train_inner | epoch 005:    234 / 393 loss=8.571, ppl=380.25, wps=14477.6, ups=0.22, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.82, loss_scale=16, train_wall=448, gb_free=10.1, wall=8137
2022-03-03 13:18:27 | INFO | train_inner | epoch 005:    334 / 393 loss=8.484, ppl=358.11, wps=14623.5, ups=0.22, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.782, loss_scale=16, train_wall=443, gb_free=10.1, wall=8585
2022-03-03 13:22:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 13:22:56 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.367 | ppl 330.06 | wps 33619.2 | wpb 2034.1 | bsz 4 | num_updates 1959 | best_loss 8.367
2022-03-03 13:22:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1959 updates
2022-03-03 13:22:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 13:23:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 13:23:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 5 @ 1959 updates, score 8.367) (writing took 4.6455820761621 seconds)
2022-03-03 13:23:00 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-03 13:23:00 | INFO | train | epoch 005 | loss 8.562 | ppl 377.91 | wps 14494 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 1959 | lr 0.000244926 | gnorm 0.811 | loss_scale 16 | train_wall 1740 | gb_free 10.1 | wall 8859
2022-03-03 13:23:00 | INFO | fairseq.trainer | begin training epoch 6
2022-03-03 13:23:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 13:26:04 | INFO | train_inner | epoch 006:     41 / 393 loss=8.382, ppl=333.51, wps=14259.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.809, loss_scale=16, train_wall=441, gb_free=10.1, wall=9043
2022-03-03 13:33:32 | INFO | train_inner | epoch 006:    141 / 393 loss=8.265, ppl=307.63, wps=14615.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.775, loss_scale=16, train_wall=443, gb_free=10.1, wall=9491
2022-03-03 13:41:01 | INFO | train_inner | epoch 006:    241 / 393 loss=8.206, ppl=295.37, wps=14619.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.768, loss_scale=16, train_wall=443, gb_free=10.1, wall=9939
2022-03-03 13:48:29 | INFO | train_inner | epoch 006:    341 / 393 loss=8.141, ppl=282.3, wps=14623.8, ups=0.22, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.759, loss_scale=32, train_wall=443, gb_free=10.1, wall=10387
2022-03-03 13:52:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 13:52:26 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.089 | ppl 272.3 | wps 33685.2 | wpb 2034.1 | bsz 4 | num_updates 2352 | best_loss 8.089
2022-03-03 13:52:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2352 updates
2022-03-03 13:52:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 13:52:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 13:52:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 6 @ 2352 updates, score 8.089) (writing took 4.764197874814272 seconds)
2022-03-03 13:52:31 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-03 13:52:31 | INFO | train | epoch 006 | loss 8.202 | ppl 294.48 | wps 14526.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 2352 | lr 0.000294041 | gnorm 0.766 | loss_scale 32 | train_wall 1740 | gb_free 10.1 | wall 10630
2022-03-03 13:52:31 | INFO | fairseq.trainer | begin training epoch 7
2022-03-03 13:52:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 13:56:06 | INFO | train_inner | epoch 007:     48 / 393 loss=8.036, ppl=262.45, wps=14262.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.754, loss_scale=32, train_wall=441, gb_free=10.1, wall=10845
2022-03-03 14:03:35 | INFO | train_inner | epoch 007:    148 / 393 loss=7.949, ppl=247.05, wps=14622.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.744, loss_scale=32, train_wall=443, gb_free=10.1, wall=11293
2022-03-03 14:11:03 | INFO | train_inner | epoch 007:    248 / 393 loss=7.896, ppl=238.21, wps=14609.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.741, loss_scale=32, train_wall=444, gb_free=10.1, wall=11742
2022-03-03 14:18:32 | INFO | train_inner | epoch 007:    348 / 393 loss=7.855, ppl=231.48, wps=14612.4, ups=0.22, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.736, loss_scale=32, train_wall=443, gb_free=10.1, wall=12190
2022-03-03 14:21:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:21:58 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.887 | ppl 236.7 | wps 33580.8 | wpb 2034.1 | bsz 4 | num_updates 2745 | best_loss 7.887
2022-03-03 14:21:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2745 updates
2022-03-03 14:21:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 14:22:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 14:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 7 @ 2745 updates, score 7.887) (writing took 4.764388799667358 seconds)
2022-03-03 14:22:03 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-03 14:22:03 | INFO | train | epoch 007 | loss 7.901 | ppl 239.03 | wps 14521 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 2745 | lr 0.000343156 | gnorm 0.744 | loss_scale 32 | train_wall 1741 | gb_free 10.1 | wall 12401
2022-03-03 14:22:03 | INFO | fairseq.trainer | begin training epoch 8
2022-03-03 14:22:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:26:09 | INFO | train_inner | epoch 008:     55 / 393 loss=7.748, ppl=214.96, wps=14252.5, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.729, loss_scale=32, train_wall=441, gb_free=10.1, wall=12648
2022-03-03 14:26:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 14:33:42 | INFO | train_inner | epoch 008:    156 / 393 loss=7.68, ppl=205.1, wps=14477.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.723, loss_scale=16, train_wall=448, gb_free=10.1, wall=13100
2022-03-03 14:41:10 | INFO | train_inner | epoch 008:    256 / 393 loss=7.655, ppl=201.48, wps=14622.1, ups=0.22, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.729, loss_scale=16, train_wall=443, gb_free=10.1, wall=13549
2022-03-03 14:48:39 | INFO | train_inner | epoch 008:    356 / 393 loss=7.617, ppl=196.31, wps=14616.4, ups=0.22, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.693, loss_scale=16, train_wall=443, gb_free=10.1, wall=13997
2022-03-03 14:51:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:51:29 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.713 | ppl 209.75 | wps 33205.3 | wpb 2034.1 | bsz 4 | num_updates 3137 | best_loss 7.713
2022-03-03 14:51:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3137 updates
2022-03-03 14:51:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 14:51:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 14:51:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 8 @ 3137 updates, score 7.713) (writing took 4.910661173053086 seconds)
2022-03-03 14:51:34 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-03 14:51:34 | INFO | train | epoch 008 | loss 7.651 | ppl 200.96 | wps 14485.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3137 | lr 0.000392147 | gnorm 0.714 | loss_scale 16 | train_wall 1740 | gb_free 10.1 | wall 14173
2022-03-03 14:51:34 | INFO | fairseq.trainer | begin training epoch 9
2022-03-03 14:51:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:56:17 | INFO | train_inner | epoch 009:     63 / 393 loss=7.511, ppl=182.4, wps=14226.3, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.7, loss_scale=16, train_wall=442, gb_free=10.1, wall=14456
2022-03-03 15:03:46 | INFO | train_inner | epoch 009:    163 / 393 loss=7.456, ppl=175.59, wps=14594.4, ups=0.22, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.71, loss_scale=16, train_wall=444, gb_free=10.1, wall=14905
2022-03-03 15:08:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 15:11:20 | INFO | train_inner | epoch 009:    264 / 393 loss=7.445, ppl=174.3, wps=14452.3, ups=0.22, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.672, loss_scale=16, train_wall=448, gb_free=10.1, wall=15358
2022-03-03 15:18:49 | INFO | train_inner | epoch 009:    364 / 393 loss=7.426, ppl=171.94, wps=14601.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.678, loss_scale=16, train_wall=444, gb_free=10.1, wall=15807
2022-03-03 15:20:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 15:21:03 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.579 | ppl 191.21 | wps 33402.5 | wpb 2034.1 | bsz 4 | num_updates 3529 | best_loss 7.579
2022-03-03 15:21:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3529 updates
2022-03-03 15:21:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 15:21:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 15:21:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 9 @ 3529 updates, score 7.579) (writing took 4.675119020976126 seconds)
2022-03-03 15:21:08 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-03 15:21:08 | INFO | train | epoch 009 | loss 7.443 | ppl 173.97 | wps 14467.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3529 | lr 0.000441137 | gnorm 0.689 | loss_scale 16 | train_wall 1742 | gb_free 10.1 | wall 15946
2022-03-03 15:21:08 | INFO | fairseq.trainer | begin training epoch 10
2022-03-03 15:21:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 15:26:27 | INFO | train_inner | epoch 010:     71 / 393 loss=7.311, ppl=158.84, wps=14242.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.69, loss_scale=16, train_wall=442, gb_free=10.1, wall=16265
2022-03-03 15:33:56 | INFO | train_inner | epoch 010:    171 / 393 loss=7.28, ppl=155.41, wps=14601.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.691, loss_scale=16, train_wall=444, gb_free=10.1, wall=16714
2022-03-03 15:41:24 | INFO | train_inner | epoch 010:    271 / 393 loss=7.259, ppl=153.16, wps=14601.7, ups=0.22, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.673, loss_scale=16, train_wall=444, gb_free=10.1, wall=17163
2022-03-03 15:48:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 15:48:58 | INFO | train_inner | epoch 010:    372 / 393 loss=7.262, ppl=153.53, wps=14454.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.658, loss_scale=16, train_wall=448, gb_free=10.1, wall=17616
2022-03-03 15:50:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 15:50:37 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.478 | ppl 178.28 | wps 33580.6 | wpb 2034.1 | bsz 4 | num_updates 3921 | best_loss 7.478
2022-03-03 15:50:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3921 updates
2022-03-03 15:50:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 15:50:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 15:50:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 10 @ 3921 updates, score 7.478) (writing took 4.560366273857653 seconds)
2022-03-03 15:50:41 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-03 15:50:41 | INFO | train | epoch 010 | loss 7.267 | ppl 153.98 | wps 14472 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3921 | lr 0.000490127 | gnorm 0.673 | loss_scale 16 | train_wall 1742 | gb_free 10.1 | wall 17720
2022-03-03 15:50:41 | INFO | fairseq.trainer | begin training epoch 11
2022-03-03 15:50:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 15:56:36 | INFO | train_inner | epoch 011:     79 / 393 loss=7.128, ppl=139.86, wps=14245.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.657, loss_scale=16, train_wall=442, gb_free=10.1, wall=18074
2022-03-03 16:04:04 | INFO | train_inner | epoch 011:    179 / 393 loss=7.118, ppl=138.94, wps=14629.3, ups=0.22, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.663, loss_scale=16, train_wall=443, gb_free=10.1, wall=18522
2022-03-03 16:11:32 | INFO | train_inner | epoch 011:    279 / 393 loss=7.107, ppl=137.82, wps=14632.5, ups=0.22, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.641, loss_scale=16, train_wall=443, gb_free=10.1, wall=18970
2022-03-03 16:18:59 | INFO | train_inner | epoch 011:    379 / 393 loss=7.114, ppl=138.5, wps=14637.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.608, loss_scale=16, train_wall=443, gb_free=10.1, wall=19418
2022-03-03 16:20:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:20:06 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.395 | ppl 168.31 | wps 33784.5 | wpb 2034.1 | bsz 4 | num_updates 4314 | best_loss 7.395
2022-03-03 16:20:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4314 updates
2022-03-03 16:20:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 16:20:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 16:20:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 11 @ 4314 updates, score 7.395) (writing took 4.54747305996716 seconds)
2022-03-03 16:20:11 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-03 16:20:11 | INFO | train | epoch 011 | loss 7.109 | ppl 138.06 | wps 14535.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 4314 | lr 0.00048146 | gnorm 0.645 | loss_scale 16 | train_wall 1739 | gb_free 10.1 | wall 19489
2022-03-03 16:20:11 | INFO | fairseq.trainer | begin training epoch 12
2022-03-03 16:20:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:26:36 | INFO | train_inner | epoch 012:     86 / 393 loss=6.957, ppl=124.2, wps=14274.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.624, loss_scale=16, train_wall=441, gb_free=10.1, wall=19875
2022-03-03 16:29:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 16:34:09 | INFO | train_inner | epoch 012:    187 / 393 loss=6.96, ppl=124.54, wps=14481.6, ups=0.22, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.611, loss_scale=16, train_wall=448, gb_free=10.1, wall=20327
2022-03-03 16:41:37 | INFO | train_inner | epoch 012:    287 / 393 loss=6.968, ppl=125.18, wps=14615.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.598, loss_scale=16, train_wall=443, gb_free=10.1, wall=20776
2022-03-03 16:49:05 | INFO | train_inner | epoch 012:    387 / 393 loss=6.98, ppl=126.26, wps=14628.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.59, loss_scale=16, train_wall=443, gb_free=10.1, wall=21224
2022-03-03 16:49:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:49:37 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.342 | ppl 162.28 | wps 33722.1 | wpb 2034.1 | bsz 4 | num_updates 4706 | best_loss 7.342
2022-03-03 16:49:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4706 updates
2022-03-03 16:49:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 16:49:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 16:49:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 12 @ 4706 updates, score 7.342) (writing took 4.636482954956591 seconds)
2022-03-03 16:49:41 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-03 16:49:41 | INFO | train | epoch 012 | loss 6.961 | ppl 124.62 | wps 14494.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 4706 | lr 0.000460971 | gnorm 0.606 | loss_scale 16 | train_wall 1740 | gb_free 10.1 | wall 21260
2022-03-03 16:49:41 | INFO | fairseq.trainer | begin training epoch 13
2022-03-03 16:49:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:56:43 | INFO | train_inner | epoch 013:     94 / 393 loss=6.817, ppl=112.77, wps=14267.1, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.602, loss_scale=16, train_wall=441, gb_free=10.1, wall=21681
2022-03-03 16:58:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 17:04:15 | INFO | train_inner | epoch 013:    195 / 393 loss=6.827, ppl=113.54, wps=14493.1, ups=0.22, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.594, loss_scale=8, train_wall=447, gb_free=10.1, wall=22133
2022-03-03 17:11:43 | INFO | train_inner | epoch 013:    295 / 393 loss=6.848, ppl=115.23, wps=14628.2, ups=0.22, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.578, loss_scale=8, train_wall=443, gb_free=10.1, wall=22581
2022-03-03 17:19:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:19:07 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.291 | ppl 156.57 | wps 33626.5 | wpb 2034.1 | bsz 4 | num_updates 5098 | best_loss 7.291
2022-03-03 17:19:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5098 updates
2022-03-03 17:19:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 17:19:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 17:19:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 13 @ 5098 updates, score 7.291) (writing took 4.712252755649388 seconds)
2022-03-03 17:19:11 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-03 17:19:11 | INFO | train | epoch 013 | loss 6.837 | ppl 114.29 | wps 14498.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5098 | lr 0.000442894 | gnorm 0.584 | loss_scale 8 | train_wall 1739 | gb_free 10.1 | wall 23030
2022-03-03 17:19:11 | INFO | fairseq.trainer | begin training epoch 14
2022-03-03 17:19:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 17:19:20 | INFO | train_inner | epoch 014:      2 / 393 loss=6.856, ppl=115.86, wps=14261.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.57, loss_scale=8, train_wall=441, gb_free=10.1, wall=23039
2022-03-03 17:26:49 | INFO | train_inner | epoch 014:    102 / 393 loss=6.688, ppl=103.08, wps=14615.9, ups=0.22, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.593, loss_scale=8, train_wall=443, gb_free=10.1, wall=23487
2022-03-03 17:34:17 | INFO | train_inner | epoch 014:    202 / 393 loss=6.73, ppl=106.14, wps=14623.5, ups=0.22, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.594, loss_scale=8, train_wall=443, gb_free=10.1, wall=23935
2022-03-03 17:41:45 | INFO | train_inner | epoch 014:    302 / 393 loss=6.747, ppl=107.44, wps=14621.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.57, loss_scale=16, train_wall=443, gb_free=10.1, wall=24384
2022-03-03 17:45:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 17:48:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:48:37 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.27 | ppl 154.34 | wps 33754.6 | wpb 2034.1 | bsz 4 | num_updates 5490 | best_loss 7.27
2022-03-03 17:48:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5490 updates
2022-03-03 17:48:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 17:48:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 17:48:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 14 @ 5490 updates, score 7.27) (writing took 4.671516794711351 seconds)
2022-03-03 17:48:42 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-03 17:48:42 | INFO | train | epoch 014 | loss 6.731 | ppl 106.21 | wps 14492.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5490 | lr 0.00042679 | gnorm 0.583 | loss_scale 8 | train_wall 1740 | gb_free 10.1 | wall 24800
2022-03-03 17:48:42 | INFO | fairseq.trainer | begin training epoch 15
2022-03-03 17:48:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 17:49:27 | INFO | train_inner | epoch 015:     10 / 393 loss=6.742, ppl=107.06, wps=14129.3, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.585, loss_scale=8, train_wall=446, gb_free=10.1, wall=24845
2022-03-03 17:56:55 | INFO | train_inner | epoch 015:    110 / 393 loss=6.599, ppl=96.92, wps=14634.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.56, loss_scale=8, train_wall=443, gb_free=10.1, wall=25293
2022-03-03 18:04:23 | INFO | train_inner | epoch 015:    210 / 393 loss=6.633, ppl=99.25, wps=14634.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.581, loss_scale=8, train_wall=443, gb_free=10.1, wall=25741
2022-03-03 18:11:50 | INFO | train_inner | epoch 015:    310 / 393 loss=6.662, ppl=101.26, wps=14638.8, ups=0.22, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.582, loss_scale=8, train_wall=443, gb_free=10.1, wall=26189
2022-03-03 18:18:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:18:06 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.251 | ppl 152.32 | wps 33765.9 | wpb 2034.1 | bsz 4 | num_updates 5883 | best_loss 7.251
2022-03-03 18:18:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5883 updates
2022-03-03 18:18:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 18:18:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 18:18:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 15 @ 5883 updates, score 7.251) (writing took 4.639699449762702 seconds)
2022-03-03 18:18:11 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-03 18:18:11 | INFO | train | epoch 015 | loss 6.639 | ppl 99.7 | wps 14542.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 5883 | lr 0.000412288 | gnorm 0.574 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 26570
2022-03-03 18:18:11 | INFO | fairseq.trainer | begin training epoch 16
2022-03-03 18:18:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:19:27 | INFO | train_inner | epoch 016:     17 / 393 loss=6.647, ppl=100.23, wps=14275.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.562, loss_scale=8, train_wall=441, gb_free=10.1, wall=26646
2022-03-03 18:26:55 | INFO | train_inner | epoch 016:    117 / 393 loss=6.521, ppl=91.82, wps=14631, ups=0.22, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.583, loss_scale=16, train_wall=443, gb_free=10.1, wall=27094
2022-03-03 18:31:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 18:34:27 | INFO | train_inner | epoch 016:    218 / 393 loss=6.551, ppl=93.74, wps=14490.7, ups=0.22, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.573, loss_scale=8, train_wall=447, gb_free=10.1, wall=27546
2022-03-03 18:41:55 | INFO | train_inner | epoch 016:    318 / 393 loss=6.581, ppl=95.73, wps=14635, ups=0.22, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.588, loss_scale=8, train_wall=443, gb_free=10.1, wall=27994
2022-03-03 18:47:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:47:36 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.226 | ppl 149.67 | wps 33897.4 | wpb 2034.1 | bsz 4 | num_updates 6275 | best_loss 7.226
2022-03-03 18:47:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6275 updates
2022-03-03 18:47:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 18:47:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 18:47:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 16 @ 6275 updates, score 7.226) (writing took 4.513690003193915 seconds)
2022-03-03 18:47:40 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-03 18:47:40 | INFO | train | epoch 016 | loss 6.557 | ppl 94.19 | wps 14506.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6275 | lr 0.000399202 | gnorm 0.581 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 28338
2022-03-03 18:47:40 | INFO | fairseq.trainer | begin training epoch 17
2022-03-03 18:47:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:49:32 | INFO | train_inner | epoch 017:     25 / 393 loss=6.552, ppl=93.83, wps=14284.4, ups=0.22, wpb=65238.4, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.573, loss_scale=8, train_wall=441, gb_free=10.1, wall=28450
2022-03-03 18:57:00 | INFO | train_inner | epoch 017:    125 / 393 loss=6.444, ppl=87.03, wps=14633.8, ups=0.22, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.589, loss_scale=8, train_wall=443, gb_free=10.1, wall=28898
2022-03-03 19:04:28 | INFO | train_inner | epoch 017:    225 / 393 loss=6.482, ppl=89.36, wps=14620.9, ups=0.22, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.577, loss_scale=8, train_wall=443, gb_free=10.1, wall=29346
2022-03-03 19:10:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 19:12:00 | INFO | train_inner | epoch 017:    326 / 393 loss=6.515, ppl=91.47, wps=14488.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.571, loss_scale=8, train_wall=447, gb_free=10.1, wall=29799
2022-03-03 19:16:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:17:05 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.225 | ppl 149.56 | wps 33866.3 | wpb 2034.1 | bsz 4 | num_updates 6667 | best_loss 7.225
2022-03-03 19:17:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6667 updates
2022-03-03 19:17:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 19:17:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 19:17:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 17 @ 6667 updates, score 7.225) (writing took 4.708884218707681 seconds)
2022-03-03 19:17:10 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-03 19:17:10 | INFO | train | epoch 017 | loss 6.484 | ppl 89.5 | wps 14500 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6667 | lr 0.000387289 | gnorm 0.574 | loss_scale 8 | train_wall 1739 | gb_free 10.1 | wall 30108
2022-03-03 19:17:10 | INFO | fairseq.trainer | begin training epoch 18
2022-03-03 19:17:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 19:19:38 | INFO | train_inner | epoch 018:     33 / 393 loss=6.471, ppl=88.73, wps=14272.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.573, loss_scale=8, train_wall=441, gb_free=10.1, wall=30256
2022-03-03 19:27:05 | INFO | train_inner | epoch 018:    133 / 393 loss=6.384, ppl=83.51, wps=14633.9, ups=0.22, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.587, loss_scale=8, train_wall=443, gb_free=10.1, wall=30704
2022-03-03 19:34:33 | INFO | train_inner | epoch 018:    233 / 393 loss=6.41, ppl=85.01, wps=14639.1, ups=0.22, wpb=65530.2, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.571, loss_scale=8, train_wall=443, gb_free=10.1, wall=31151
2022-03-03 19:42:01 | INFO | train_inner | epoch 018:    333 / 393 loss=6.455, ppl=87.74, wps=14639, ups=0.22, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.578, loss_scale=8, train_wall=443, gb_free=10.1, wall=31599
2022-03-03 19:46:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:46:34 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.235 | ppl 150.61 | wps 33786.5 | wpb 2034.1 | bsz 4 | num_updates 7060 | best_loss 7.225
2022-03-03 19:46:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7060 updates
2022-03-03 19:46:34 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-03 19:46:34 | INFO | train | epoch 018 | loss 6.418 | ppl 85.53 | wps 14583.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 7060 | lr 0.000376355 | gnorm 0.582 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 31872
2022-03-03 19:46:34 | INFO | fairseq.trainer | begin training epoch 19
2022-03-03 19:46:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 19:49:33 | INFO | train_inner | epoch 019:     40 / 393 loss=6.396, ppl=84.22, wps=14426.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.577, loss_scale=16, train_wall=441, gb_free=10.1, wall=32051
2022-03-03 19:57:01 | INFO | train_inner | epoch 019:    140 / 393 loss=6.315, ppl=79.64, wps=14631, ups=0.22, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.598, loss_scale=16, train_wall=443, gb_free=10.1, wall=32499
2022-03-03 20:04:29 | INFO | train_inner | epoch 019:    240 / 393 loss=6.359, ppl=82.11, wps=14632.4, ups=0.22, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.595, loss_scale=16, train_wall=443, gb_free=10.1, wall=32947
2022-03-03 20:04:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:12:01 | INFO | train_inner | epoch 019:    341 / 393 loss=6.398, ppl=84.35, wps=14495.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.6, loss_scale=8, train_wall=447, gb_free=10.1, wall=33399
2022-03-03 20:15:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:15:58 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.23 | ppl 150.08 | wps 33853.4 | wpb 2034.1 | bsz 4 | num_updates 7452 | best_loss 7.225
2022-03-03 20:15:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7452 updates
2022-03-03 20:15:58 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-03 20:15:58 | INFO | train | epoch 019 | loss 6.357 | ppl 81.98 | wps 14543.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7452 | lr 0.000366322 | gnorm 0.589 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 33637
2022-03-03 20:15:58 | INFO | fairseq.trainer | begin training epoch 20
2022-03-03 20:15:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:19:33 | INFO | train_inner | epoch 020:     48 / 393 loss=6.323, ppl=80.05, wps=14425.5, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.569, loss_scale=8, train_wall=441, gb_free=10.1, wall=33852
2022-03-03 20:27:01 | INFO | train_inner | epoch 020:    148 / 393 loss=6.268, ppl=77.05, wps=14634.6, ups=0.22, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.601, loss_scale=8, train_wall=443, gb_free=10.1, wall=34299
2022-03-03 20:34:29 | INFO | train_inner | epoch 020:    248 / 393 loss=6.305, ppl=79.07, wps=14633.8, ups=0.22, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.588, loss_scale=8, train_wall=443, gb_free=10.1, wall=34747
2022-03-03 20:41:57 | INFO | train_inner | epoch 020:    348 / 393 loss=6.339, ppl=80.97, wps=14625.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.602, loss_scale=8, train_wall=443, gb_free=10.1, wall=35195
2022-03-03 20:45:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:45:23 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.225 | ppl 149.61 | wps 33864.6 | wpb 2034.1 | bsz 4 | num_updates 7845 | best_loss 7.225
2022-03-03 20:45:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7845 updates
2022-03-03 20:45:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 20:45:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt
2022-03-03 20:45:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.04_0.06_0.9_#2/checkpoint_best.pt (epoch 20 @ 7845 updates, score 7.225) (writing took 4.489944043569267 seconds)
2022-03-03 20:45:28 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-03 20:45:28 | INFO | train | epoch 020 | loss 6.302 | ppl 78.91 | wps 14540 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 7845 | lr 0.000357029 | gnorm 0.589 | loss_scale 16 | train_wall 1739 | gb_free 10.1 | wall 35406
2022-03-03 20:45:28 | INFO | fairseq.trainer | begin training epoch 21
2022-03-03 20:45:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:49:34 | INFO | train_inner | epoch 021:     55 / 393 loss=6.254, ppl=76.3, wps=14276.9, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.604, loss_scale=16, train_wall=441, gb_free=10.1, wall=35652
2022-03-03 20:56:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:57:06 | INFO | train_inner | epoch 021:    156 / 393 loss=6.219, ppl=74.51, wps=14484.1, ups=0.22, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.59, loss_scale=8, train_wall=447, gb_free=10.1, wall=36105
2022-03-03 21:04:34 | INFO | train_inner | epoch 021:    256 / 393 loss=6.263, ppl=76.8, wps=14634.7, ups=0.22, wpb=65530.2, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.593, loss_scale=8, train_wall=443, gb_free=10.1, wall=36553
2022-03-03 21:12:02 | INFO | train_inner | epoch 021:    356 / 393 loss=6.293, ppl=78.42, wps=14633.8, ups=0.22, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.595, loss_scale=8, train_wall=443, gb_free=10.1, wall=37000
2022-03-03 21:14:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:14:52 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.233 | ppl 150.4 | wps 33828.5 | wpb 2034.1 | bsz 4 | num_updates 8237 | best_loss 7.225
2022-03-03 21:14:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8237 updates
2022-03-03 21:14:52 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-03 21:14:52 | INFO | train | epoch 021 | loss 6.251 | ppl 76.14 | wps 14542 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8237 | lr 0.00034843 | gnorm 0.601 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 37171
2022-03-03 21:14:52 | INFO | fairseq.trainer | begin training epoch 22
2022-03-03 21:14:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:19:34 | INFO | train_inner | epoch 022:     63 / 393 loss=6.197, ppl=73.34, wps=14428.2, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.622, loss_scale=8, train_wall=441, gb_free=10.1, wall=37453
2022-03-03 21:27:02 | INFO | train_inner | epoch 022:    163 / 393 loss=6.168, ppl=71.89, wps=14639.3, ups=0.22, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.599, loss_scale=8, train_wall=443, gb_free=10.1, wall=37900
2022-03-03 21:34:30 | INFO | train_inner | epoch 022:    263 / 393 loss=6.227, ppl=74.89, wps=14628.8, ups=0.22, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.622, loss_scale=8, train_wall=443, gb_free=10.1, wall=38348
2022-03-03 21:37:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 21:42:02 | INFO | train_inner | epoch 022:    364 / 393 loss=6.238, ppl=75.48, wps=14485.5, ups=0.22, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.608, loss_scale=8, train_wall=447, gb_free=10.1, wall=38801
2022-03-03 21:44:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:44:17 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.259 | ppl 153.12 | wps 33764.7 | wpb 2034.1 | bsz 4 | num_updates 8629 | best_loss 7.225
2022-03-03 21:44:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8629 updates
2022-03-03 21:44:17 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-03 21:44:17 | INFO | train | epoch 022 | loss 6.202 | ppl 73.64 | wps 14542 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8629 | lr 0.000340424 | gnorm 0.611 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 38935
2022-03-03 21:44:17 | INFO | fairseq.trainer | begin training epoch 23
2022-03-03 21:44:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:49:35 | INFO | train_inner | epoch 023:     71 / 393 loss=6.146, ppl=70.79, wps=14417.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.606, loss_scale=8, train_wall=441, gb_free=10.1, wall=39253
2022-03-03 21:57:03 | INFO | train_inner | epoch 023:    171 / 393 loss=6.135, ppl=70.29, wps=14633.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.61, loss_scale=8, train_wall=443, gb_free=10.1, wall=39701
2022-03-03 22:04:31 | INFO | train_inner | epoch 023:    271 / 393 loss=6.168, ppl=71.89, wps=14605, ups=0.22, wpb=65535.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.62, loss_scale=8, train_wall=444, gb_free=10.1, wall=40150
2022-03-03 22:12:00 | INFO | train_inner | epoch 023:    371 / 393 loss=6.201, ppl=73.58, wps=14605.2, ups=0.22, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.607, loss_scale=8, train_wall=444, gb_free=10.1, wall=40599
2022-03-03 22:13:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:13:44 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.262 | ppl 153.47 | wps 33402.7 | wpb 2034.1 | bsz 4 | num_updates 9022 | best_loss 7.225
2022-03-03 22:13:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9022 updates
2022-03-03 22:13:44 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-03 22:13:44 | INFO | train | epoch 023 | loss 6.158 | ppl 71.43 | wps 14561.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9022 | lr 0.000332927 | gnorm 0.613 | loss_scale 8 | train_wall 1740 | gb_free 10.1 | wall 40702
2022-03-03 22:13:44 | INFO | fairseq.trainer | begin training epoch 24
2022-03-03 22:13:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:19:34 | INFO | train_inner | epoch 024:     78 / 393 loss=6.093, ppl=68.25, wps=14378.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.608, loss_scale=16, train_wall=442, gb_free=10.1, wall=41052
2022-03-03 22:19:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 22:27:07 | INFO | train_inner | epoch 024:    179 / 393 loss=6.089, ppl=68.07, wps=14461.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.63, loss_scale=8, train_wall=448, gb_free=10.1, wall=41506
2022-03-03 22:34:36 | INFO | train_inner | epoch 024:    279 / 393 loss=6.129, ppl=70, wps=14605.6, ups=0.22, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.63, loss_scale=8, train_wall=444, gb_free=10.1, wall=41954
2022-03-03 22:42:04 | INFO | train_inner | epoch 024:    379 / 393 loss=6.172, ppl=72.11, wps=14606.9, ups=0.22, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.613, loss_scale=8, train_wall=444, gb_free=10.1, wall=42403
2022-03-03 22:43:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:43:12 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.276 | ppl 155 | wps 33417.8 | wpb 2034.1 | bsz 4 | num_updates 9414 | best_loss 7.225
2022-03-03 22:43:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9414 updates
2022-03-03 22:43:12 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-03 22:43:12 | INFO | train | epoch 024 | loss 6.117 | ppl 69.39 | wps 14511.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 9414 | lr 0.000325921 | gnorm 0.622 | loss_scale 8 | train_wall 1742 | gb_free 10.1 | wall 42470
2022-03-03 22:43:12 | INFO | fairseq.trainer | begin training epoch 25
2022-03-03 22:43:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:49:38 | INFO | train_inner | epoch 025:     86 / 393 loss=6.039, ppl=65.76, wps=14395.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.617, loss_scale=8, train_wall=442, gb_free=10.1, wall=42856
2022-03-03 22:57:06 | INFO | train_inner | epoch 025:    186 / 393 loss=6.054, ppl=66.42, wps=14603.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.619, loss_scale=8, train_wall=444, gb_free=10.1, wall=43305
2022-03-03 22:59:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 23:04:40 | INFO | train_inner | epoch 025:    287 / 393 loss=6.102, ppl=68.7, wps=14454.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.671, loss_scale=8, train_wall=448, gb_free=10.1, wall=43758
2022-03-03 23:12:08 | INFO | train_inner | epoch 025:    387 / 393 loss=6.127, ppl=69.91, wps=14617.3, ups=0.22, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.612, loss_scale=8, train_wall=443, gb_free=10.1, wall=44207
2022-03-03 23:12:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:12:40 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.292 | ppl 156.69 | wps 33690.1 | wpb 2034.1 | bsz 4 | num_updates 9806 | best_loss 7.225
2022-03-03 23:12:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9806 updates
2022-03-03 23:12:40 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-03 23:12:40 | INFO | train | epoch 025 | loss 6.078 | ppl 67.55 | wps 14516.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 9806 | lr 0.000319341 | gnorm 0.628 | loss_scale 8 | train_wall 1741 | gb_free 10.1 | wall 44238
2022-03-03 23:12:40 | INFO | fairseq.trainer | begin training epoch 26
2022-03-03 23:12:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:19:41 | INFO | train_inner | epoch 026:     94 / 393 loss=5.99, ppl=63.56, wps=14420.4, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.652, loss_scale=8, train_wall=441, gb_free=10.1, wall=44659
2022-03-03 23:27:09 | INFO | train_inner | epoch 026:    194 / 393 loss=6.025, ppl=65.12, wps=14627.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.611, loss_scale=8, train_wall=443, gb_free=10.1, wall=45107
2022-03-03 23:34:37 | INFO | train_inner | epoch 026:    294 / 393 loss=6.055, ppl=66.5, wps=14630, ups=0.22, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.644, loss_scale=8, train_wall=443, gb_free=10.1, wall=45555
2022-03-03 23:38:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 23:41:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:42:04 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.303 | ppl 157.93 | wps 33740.8 | wpb 2034.1 | bsz 4 | num_updates 10198 | best_loss 7.225
2022-03-03 23:42:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10198 updates
2022-03-03 23:42:04 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-03 23:42:04 | INFO | train | epoch 026 | loss 6.04 | ppl 65.82 | wps 14541 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10198 | lr 0.000313143 | gnorm 0.627 | loss_scale 8 | train_wall 1739 | gb_free 10.1 | wall 46003
2022-03-03 23:42:04 | INFO | fairseq.trainer | begin training epoch 27
2022-03-03 23:42:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:42:13 | INFO | train_inner | epoch 027:      2 / 393 loss=6.094, ppl=68.31, wps=14285, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.599, loss_scale=8, train_wall=445, gb_free=10.1, wall=46012
2022-03-03 23:49:41 | INFO | train_inner | epoch 027:    102 / 393 loss=5.943, ppl=61.52, wps=14635.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.662, loss_scale=8, train_wall=443, gb_free=10.1, wall=46460
2022-03-03 23:57:09 | INFO | train_inner | epoch 027:    202 / 393 loss=5.993, ppl=63.69, wps=14629.8, ups=0.22, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.66, loss_scale=8, train_wall=443, gb_free=10.1, wall=46908
2022-03-04 00:04:37 | INFO | train_inner | epoch 027:    302 / 393 loss=6.034, ppl=65.52, wps=14640.9, ups=0.22, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.65, loss_scale=8, train_wall=443, gb_free=10.1, wall=47355
2022-03-04 00:11:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:11:29 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.318 | ppl 159.59 | wps 33777.5 | wpb 2034.1 | bsz 4 | num_updates 10591 | best_loss 7.225
2022-03-04 00:11:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10591 updates
2022-03-04 00:11:29 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 00:11:29 | INFO | train | epoch 027 | loss 6.006 | ppl 64.25 | wps 14582 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 10591 | lr 0.000307278 | gnorm 0.654 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 47767
2022-03-04 00:11:29 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 00:11:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:12:09 | INFO | train_inner | epoch 028:      9 / 393 loss=6.041, ppl=65.85, wps=14427.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.64, loss_scale=8, train_wall=441, gb_free=10.1, wall=47807
2022-03-04 00:19:37 | INFO | train_inner | epoch 028:    109 / 393 loss=5.91, ppl=60.14, wps=14636.3, ups=0.22, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.618, loss_scale=16, train_wall=443, gb_free=10.1, wall=48255
2022-03-04 00:20:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 00:20:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 00:27:13 | INFO | train_inner | epoch 028:    211 / 393 loss=5.964, ppl=62.43, wps=14348.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.656, loss_scale=4, train_wall=451, gb_free=10.1, wall=48712
2022-03-04 00:34:41 | INFO | train_inner | epoch 028:    311 / 393 loss=6.008, ppl=64.36, wps=14634.8, ups=0.22, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.668, loss_scale=4, train_wall=443, gb_free=10.1, wall=49160
2022-03-04 00:40:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:40:53 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.333 | ppl 161.21 | wps 33663.9 | wpb 2034.1 | bsz 4 | num_updates 10982 | best_loss 7.225
2022-03-04 00:40:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10982 updates
2022-03-04 00:40:53 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 00:40:53 | INFO | train | epoch 028 | loss 5.972 | ppl 62.78 | wps 14506.4 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 10982 | lr 0.000301758 | gnorm 0.644 | loss_scale 4 | train_wall 1738 | gb_free 10.1 | wall 49531
2022-03-04 00:40:53 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 00:40:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:42:14 | INFO | train_inner | epoch 029:     18 / 393 loss=5.998, ppl=63.92, wps=14418, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.655, loss_scale=4, train_wall=441, gb_free=10.1, wall=49612
2022-03-04 00:49:42 | INFO | train_inner | epoch 029:    118 / 393 loss=5.89, ppl=59.31, wps=14628.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.647, loss_scale=4, train_wall=443, gb_free=10.1, wall=50060
2022-03-04 00:57:10 | INFO | train_inner | epoch 029:    218 / 393 loss=5.931, ppl=61.01, wps=14636.1, ups=0.22, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.648, loss_scale=4, train_wall=443, gb_free=10.1, wall=50508
2022-03-04 01:04:37 | INFO | train_inner | epoch 029:    318 / 393 loss=5.972, ppl=62.77, wps=14632.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.654, loss_scale=8, train_wall=443, gb_free=10.1, wall=50956
2022-03-04 01:10:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:10:18 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.35 | ppl 163.18 | wps 33753.8 | wpb 2034.1 | bsz 4 | num_updates 11375 | best_loss 7.225
2022-03-04 01:10:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11375 updates
2022-03-04 01:10:18 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 01:10:18 | INFO | train | epoch 029 | loss 5.943 | ppl 61.51 | wps 14577.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 11375 | lr 0.0002965 | gnorm 0.657 | loss_scale 8 | train_wall 1739 | gb_free 10.1 | wall 51296
2022-03-04 01:10:18 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 01:10:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:12:10 | INFO | train_inner | epoch 030:     25 / 393 loss=5.96, ppl=62.26, wps=14424.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.654, loss_scale=8, train_wall=441, gb_free=10.1, wall=51408
2022-03-04 01:19:38 | INFO | train_inner | epoch 030:    125 / 393 loss=5.864, ppl=58.24, wps=14636.8, ups=0.22, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.648, loss_scale=8, train_wall=443, gb_free=10.1, wall=51856
2022-03-04 01:27:05 | INFO | train_inner | epoch 030:    225 / 393 loss=5.904, ppl=59.89, wps=14634.9, ups=0.22, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.674, loss_scale=8, train_wall=443, gb_free=10.1, wall=52304
2022-03-04 01:34:33 | INFO | train_inner | epoch 030:    325 / 393 loss=5.944, ppl=61.55, wps=14628.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.662, loss_scale=8, train_wall=443, gb_free=10.1, wall=52752
2022-03-04 01:37:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 01:39:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:39:42 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.362 | ppl 164.48 | wps 33781.3 | wpb 2034.1 | bsz 4 | num_updates 11767 | best_loss 7.225
2022-03-04 01:39:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11767 updates
2022-03-04 01:39:42 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 01:39:42 | INFO | train | epoch 030 | loss 5.913 | ppl 60.24 | wps 14543.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 11767 | lr 0.000291519 | gnorm 0.666 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 53061
2022-03-04 01:39:42 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 01:39:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:42:10 | INFO | train_inner | epoch 031:     33 / 393 loss=5.923, ppl=60.69, wps=14281.8, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.686, loss_scale=8, train_wall=445, gb_free=10.1, wall=53209
2022-03-04 01:49:38 | INFO | train_inner | epoch 031:    133 / 393 loss=5.836, ppl=57.12, wps=14641.4, ups=0.22, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.649, loss_scale=8, train_wall=443, gb_free=10.1, wall=53656
2022-03-04 01:57:05 | INFO | train_inner | epoch 031:    233 / 393 loss=5.884, ppl=59.08, wps=14638.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.659, loss_scale=8, train_wall=443, gb_free=10.1, wall=54104
2022-03-04 02:04:33 | INFO | train_inner | epoch 031:    333 / 393 loss=5.923, ppl=60.67, wps=14639.4, ups=0.22, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.664, loss_scale=8, train_wall=443, gb_free=10.1, wall=54552
2022-03-04 02:09:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:09:06 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.374 | ppl 165.91 | wps 33737 | wpb 2034.1 | bsz 4 | num_updates 12160 | best_loss 7.225
2022-03-04 02:09:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12160 updates
2022-03-04 02:09:06 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 02:09:06 | INFO | train | epoch 031 | loss 5.885 | ppl 59.09 | wps 14583.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 12160 | lr 0.00028677 | gnorm 0.66 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 54825
2022-03-04 02:09:06 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 02:09:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:12:06 | INFO | train_inner | epoch 032:     40 / 393 loss=5.88, ppl=58.88, wps=14417.8, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.664, loss_scale=8, train_wall=441, gb_free=10.1, wall=55004
2022-03-04 02:17:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 02:19:38 | INFO | train_inner | epoch 032:    141 / 393 loss=5.813, ppl=56.22, wps=14490.1, ups=0.22, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.67, loss_scale=8, train_wall=447, gb_free=10.1, wall=55456
2022-03-04 02:27:06 | INFO | train_inner | epoch 032:    241 / 393 loss=5.861, ppl=58.13, wps=14638.1, ups=0.22, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.66, loss_scale=8, train_wall=443, gb_free=10.1, wall=55904
2022-03-04 02:34:33 | INFO | train_inner | epoch 032:    341 / 393 loss=5.902, ppl=59.79, wps=14636.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.672, loss_scale=8, train_wall=443, gb_free=10.1, wall=56352
2022-03-04 02:38:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:38:31 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.409 | ppl 169.94 | wps 33727 | wpb 2034.1 | bsz 4 | num_updates 12552 | best_loss 7.225
2022-03-04 02:38:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12552 updates
2022-03-04 02:38:31 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 02:38:31 | INFO | train | epoch 032 | loss 5.859 | ppl 58.03 | wps 14543.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12552 | lr 0.000282256 | gnorm 0.667 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 56589
2022-03-04 02:38:31 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 02:38:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:42:06 | INFO | train_inner | epoch 033:     48 / 393 loss=5.843, ppl=57.38, wps=14423.7, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.667, loss_scale=8, train_wall=441, gb_free=10.1, wall=56804
2022-03-04 02:49:33 | INFO | train_inner | epoch 033:    148 / 393 loss=5.786, ppl=55.19, wps=14640.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.673, loss_scale=8, train_wall=443, gb_free=10.1, wall=57252
2022-03-04 02:57:01 | INFO | train_inner | epoch 033:    248 / 393 loss=5.841, ppl=57.32, wps=14635.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.653, loss_scale=16, train_wall=443, gb_free=10.1, wall=57700
2022-03-04 02:57:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 02:58:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 03:04:38 | INFO | train_inner | epoch 033:    350 / 393 loss=5.88, ppl=58.88, wps=14343.2, ups=0.22, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.693, loss_scale=4, train_wall=452, gb_free=10.1, wall=58156
2022-03-04 03:07:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:07:55 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.418 | ppl 170.98 | wps 33786.2 | wpb 2034.1 | bsz 4 | num_updates 12943 | best_loss 7.225
2022-03-04 03:07:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12943 updates
2022-03-04 03:07:55 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 03:07:55 | INFO | train | epoch 033 | loss 5.833 | ppl 57.02 | wps 14507.8 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 12943 | lr 0.00027796 | gnorm 0.672 | loss_scale 4 | train_wall 1738 | gb_free 10.1 | wall 58353
2022-03-04 03:07:55 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 03:07:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:12:10 | INFO | train_inner | epoch 034:     57 / 393 loss=5.804, ppl=55.86, wps=14426.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.668, loss_scale=4, train_wall=441, gb_free=10.1, wall=58609
2022-03-04 03:19:38 | INFO | train_inner | epoch 034:    157 / 393 loss=5.776, ppl=54.81, wps=14638.1, ups=0.22, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.686, loss_scale=4, train_wall=443, gb_free=10.1, wall=59056
2022-03-04 03:27:06 | INFO | train_inner | epoch 034:    257 / 393 loss=5.825, ppl=56.68, wps=14638.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.694, loss_scale=4, train_wall=443, gb_free=10.1, wall=59504
2022-03-04 03:34:33 | INFO | train_inner | epoch 034:    357 / 393 loss=5.856, ppl=57.91, wps=14638.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.693, loss_scale=4, train_wall=443, gb_free=10.1, wall=59952
2022-03-04 03:37:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:37:19 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.433 | ppl 172.77 | wps 33771.3 | wpb 2034.1 | bsz 4 | num_updates 13336 | best_loss 7.225
2022-03-04 03:37:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13336 updates
2022-03-04 03:37:19 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 03:37:19 | INFO | train | epoch 034 | loss 5.809 | ppl 56.08 | wps 14582 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 13336 | lr 0.000273834 | gnorm 0.684 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 60118
2022-03-04 03:37:19 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 03:37:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:42:06 | INFO | train_inner | epoch 035:     64 / 393 loss=5.768, ppl=54.48, wps=14419.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.672, loss_scale=8, train_wall=441, gb_free=10.1, wall=60404
2022-03-04 03:49:34 | INFO | train_inner | epoch 035:    164 / 393 loss=5.749, ppl=53.78, wps=14635.1, ups=0.22, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.67, loss_scale=8, train_wall=443, gb_free=10.1, wall=60852
2022-03-04 03:57:01 | INFO | train_inner | epoch 035:    264 / 393 loss=5.802, ppl=55.78, wps=14635.3, ups=0.22, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.699, loss_scale=8, train_wall=443, gb_free=10.1, wall=61300
2022-03-04 04:04:29 | INFO | train_inner | epoch 035:    364 / 393 loss=5.835, ppl=57.06, wps=14634.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.691, loss_scale=8, train_wall=443, gb_free=10.1, wall=61748
2022-03-04 04:06:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:06:44 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.457 | ppl 175.72 | wps 33694.9 | wpb 2034.1 | bsz 4 | num_updates 13729 | best_loss 7.225
2022-03-04 04:06:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13729 updates
2022-03-04 04:06:44 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 04:06:44 | INFO | train | epoch 035 | loss 5.787 | ppl 55.21 | wps 14580.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 13729 | lr 0.000269886 | gnorm 0.684 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 61882
2022-03-04 04:06:44 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 04:06:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:06:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 04:12:06 | INFO | train_inner | epoch 036:     72 / 393 loss=5.742, ppl=53.5, wps=14287.2, ups=0.22, wpb=65239, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.687, loss_scale=4, train_wall=445, gb_free=10.1, wall=62204
2022-03-04 04:19:34 | INFO | train_inner | epoch 036:    172 / 393 loss=5.742, ppl=53.52, wps=14640.3, ups=0.22, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.683, loss_scale=4, train_wall=443, gb_free=10.1, wall=62652
2022-03-04 04:27:01 | INFO | train_inner | epoch 036:    272 / 393 loss=5.772, ppl=54.64, wps=14636.9, ups=0.22, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.719, loss_scale=4, train_wall=443, gb_free=10.1, wall=63100
2022-03-04 04:34:29 | INFO | train_inner | epoch 036:    372 / 393 loss=5.816, ppl=56.35, wps=14636.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.694, loss_scale=4, train_wall=443, gb_free=10.1, wall=63547
2022-03-04 04:36:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:36:08 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.454 | ppl 175.37 | wps 33697 | wpb 2034.1 | bsz 4 | num_updates 14121 | best_loss 7.225
2022-03-04 04:36:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14121 updates
2022-03-04 04:36:08 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 04:36:08 | INFO | train | epoch 036 | loss 5.765 | ppl 54.37 | wps 14548.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14121 | lr 0.000266114 | gnorm 0.696 | loss_scale 4 | train_wall 1738 | gb_free 10.1 | wall 63646
2022-03-04 04:36:08 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 04:36:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:42:01 | INFO | train_inner | epoch 037:     79 / 393 loss=5.701, ppl=52.03, wps=14425.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.669, loss_scale=4, train_wall=441, gb_free=10.1, wall=64000
2022-03-04 04:49:29 | INFO | train_inner | epoch 037:    179 / 393 loss=5.717, ppl=52.6, wps=14636.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.709, loss_scale=8, train_wall=443, gb_free=10.1, wall=64448
2022-03-04 04:56:57 | INFO | train_inner | epoch 037:    279 / 393 loss=5.765, ppl=54.38, wps=14634.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.699, loss_scale=8, train_wall=443, gb_free=10.1, wall=64895
2022-03-04 05:04:25 | INFO | train_inner | epoch 037:    379 / 393 loss=5.802, ppl=55.8, wps=14628.4, ups=0.22, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.689, loss_scale=8, train_wall=443, gb_free=10.1, wall=65343
2022-03-04 05:05:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:05:32 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.478 | ppl 178.34 | wps 33872.1 | wpb 2034.1 | bsz 4 | num_updates 14514 | best_loss 7.225
2022-03-04 05:05:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14514 updates
2022-03-04 05:05:32 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 05:05:32 | INFO | train | epoch 037 | loss 5.745 | ppl 53.62 | wps 14579.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 14514 | lr 0.000262486 | gnorm 0.69 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 65410
2022-03-04 05:05:32 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 05:05:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:11:58 | INFO | train_inner | epoch 038:     86 / 393 loss=5.68, ppl=51.27, wps=14392.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.722, loss_scale=8, train_wall=442, gb_free=10.1, wall=65797
2022-03-04 05:19:27 | INFO | train_inner | epoch 038:    186 / 393 loss=5.698, ppl=51.93, wps=14600.5, ups=0.22, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.68, loss_scale=8, train_wall=444, gb_free=10.1, wall=66246
2022-03-04 05:23:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 05:24:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 05:27:05 | INFO | train_inner | epoch 038:    288 / 393 loss=5.742, ppl=53.51, wps=14309.7, ups=0.22, wpb=65530.2, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.719, loss_scale=4, train_wall=453, gb_free=10.1, wall=66704
2022-03-04 05:34:34 | INFO | train_inner | epoch 038:    388 / 393 loss=5.791, ppl=55.36, wps=14604.6, ups=0.22, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.69, loss_scale=4, train_wall=444, gb_free=10.1, wall=67152
2022-03-04 05:34:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:35:01 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.495 | ppl 180.4 | wps 33330.6 | wpb 2034.1 | bsz 4 | num_updates 14905 | best_loss 7.225
2022-03-04 05:35:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14905 updates
2022-03-04 05:35:01 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 05:35:01 | INFO | train | epoch 038 | loss 5.725 | ppl 52.89 | wps 14470.4 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 14905 | lr 0.00025902 | gnorm 0.704 | loss_scale 4 | train_wall 1742 | gb_free 10.1 | wall 67179
2022-03-04 05:35:01 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 05:35:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:42:07 | INFO | train_inner | epoch 039:     95 / 393 loss=5.648, ppl=50.16, wps=14388.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.7, loss_scale=4, train_wall=442, gb_free=10.1, wall=67606
2022-03-04 05:49:36 | INFO | train_inner | epoch 039:    195 / 393 loss=5.687, ppl=51.53, wps=14602.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.706, loss_scale=4, train_wall=444, gb_free=10.1, wall=68055
2022-03-04 05:57:05 | INFO | train_inner | epoch 039:    295 / 393 loss=5.733, ppl=53.2, wps=14610.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.718, loss_scale=4, train_wall=443, gb_free=10.1, wall=68503
2022-03-04 06:04:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:04:29 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.501 | ppl 181.19 | wps 33266.8 | wpb 2034.1 | bsz 4 | num_updates 15298 | best_loss 7.225
2022-03-04 06:04:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15298 updates
2022-03-04 06:04:29 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 06:04:29 | INFO | train | epoch 039 | loss 5.706 | ppl 52.22 | wps 14551.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 15298 | lr 0.000255672 | gnorm 0.708 | loss_scale 8 | train_wall 1741 | gb_free 10.1 | wall 68947
2022-03-04 06:04:29 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 06:04:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:04:38 | INFO | train_inner | epoch 040:      2 / 393 loss=5.759, ppl=54.17, wps=14394.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.708, loss_scale=8, train_wall=442, gb_free=10.1, wall=68956
2022-03-04 06:12:07 | INFO | train_inner | epoch 040:    102 / 393 loss=5.629, ppl=49.49, wps=14593.8, ups=0.22, wpb=65530.2, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.704, loss_scale=8, train_wall=444, gb_free=10.1, wall=69405
2022-03-04 06:19:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 06:19:40 | INFO | train_inner | epoch 040:    203 / 393 loss=5.671, ppl=50.97, wps=14466.8, ups=0.22, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.716, loss_scale=4, train_wall=448, gb_free=10.1, wall=69858
2022-03-04 06:27:08 | INFO | train_inner | epoch 040:    303 / 393 loss=5.712, ppl=52.44, wps=14639.5, ups=0.22, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.691, loss_scale=4, train_wall=443, gb_free=10.1, wall=70306
2022-03-04 06:33:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:33:55 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.514 | ppl 182.76 | wps 33669.6 | wpb 2034.1 | bsz 4 | num_updates 15690 | best_loss 7.225
2022-03-04 06:33:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15690 updates
2022-03-04 06:33:55 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 06:33:55 | INFO | train | epoch 040 | loss 5.688 | ppl 51.54 | wps 14529.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 15690 | lr 0.000252458 | gnorm 0.704 | loss_scale 4 | train_wall 1740 | gb_free 10.1 | wall 70713
2022-03-04 06:33:55 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 06:33:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:34:40 | INFO | train_inner | epoch 041:     10 / 393 loss=5.729, ppl=53.04, wps=14427, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.712, loss_scale=4, train_wall=441, gb_free=10.1, wall=70758
2022-03-04 06:42:07 | INFO | train_inner | epoch 041:    110 / 393 loss=5.604, ppl=48.64, wps=14641.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.718, loss_scale=4, train_wall=443, gb_free=10.1, wall=71206
2022-03-04 06:49:35 | INFO | train_inner | epoch 041:    210 / 393 loss=5.664, ppl=50.72, wps=14635.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.72, loss_scale=4, train_wall=443, gb_free=10.1, wall=71654
2022-03-04 06:57:03 | INFO | train_inner | epoch 041:    310 / 393 loss=5.696, ppl=51.84, wps=14639.8, ups=0.22, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.727, loss_scale=4, train_wall=443, gb_free=10.1, wall=72101
2022-03-04 07:03:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:03:19 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.536 | ppl 185.58 | wps 33745.9 | wpb 2034.1 | bsz 4 | num_updates 16083 | best_loss 7.225
2022-03-04 07:03:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16083 updates
2022-03-04 07:03:19 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 07:03:19 | INFO | train | epoch 041 | loss 5.671 | ppl 50.95 | wps 14581.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 16083 | lr 0.000249354 | gnorm 0.717 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 72478
2022-03-04 07:03:19 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 07:03:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:04:36 | INFO | train_inner | epoch 042:     17 / 393 loss=5.713, ppl=52.45, wps=14414.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.702, loss_scale=8, train_wall=441, gb_free=10.1, wall=72554
2022-03-04 07:12:03 | INFO | train_inner | epoch 042:    117 / 393 loss=5.602, ppl=48.55, wps=14637.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.716, loss_scale=8, train_wall=443, gb_free=10.1, wall=73002
2022-03-04 07:19:31 | INFO | train_inner | epoch 042:    217 / 393 loss=5.634, ppl=49.65, wps=14636.8, ups=0.22, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.702, loss_scale=8, train_wall=443, gb_free=10.1, wall=73449
2022-03-04 07:26:59 | INFO | train_inner | epoch 042:    317 / 393 loss=5.689, ppl=51.6, wps=14626.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.734, loss_scale=8, train_wall=443, gb_free=10.1, wall=73897
2022-03-04 07:29:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 07:32:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:32:44 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.544 | ppl 186.66 | wps 33730.5 | wpb 2034.1 | bsz 4 | num_updates 16475 | best_loss 7.225
2022-03-04 07:32:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16475 updates
2022-03-04 07:32:44 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 07:32:44 | INFO | train | epoch 042 | loss 5.655 | ppl 50.39 | wps 14542.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16475 | lr 0.00024637 | gnorm 0.721 | loss_scale 4 | train_wall 1738 | gb_free 10.1 | wall 74242
2022-03-04 07:32:44 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 07:32:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:34:36 | INFO | train_inner | epoch 043:     25 / 393 loss=5.684, ppl=51.41, wps=14282.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.728, loss_scale=4, train_wall=445, gb_free=10.1, wall=74354
2022-03-04 07:42:04 | INFO | train_inner | epoch 043:    125 / 393 loss=5.587, ppl=48.08, wps=14624.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.736, loss_scale=4, train_wall=443, gb_free=10.1, wall=74802
2022-03-04 07:49:32 | INFO | train_inner | epoch 043:    225 / 393 loss=5.631, ppl=49.57, wps=14638.8, ups=0.22, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.723, loss_scale=4, train_wall=443, gb_free=10.1, wall=75250
2022-03-04 07:56:59 | INFO | train_inner | epoch 043:    325 / 393 loss=5.672, ppl=50.98, wps=14640, ups=0.22, wpb=65535.4, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.74, loss_scale=4, train_wall=443, gb_free=10.1, wall=75698
2022-03-04 08:02:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:02:08 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.568 | ppl 189.73 | wps 33762.9 | wpb 2034.1 | bsz 4 | num_updates 16868 | best_loss 7.225
2022-03-04 08:02:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16868 updates
2022-03-04 08:02:08 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 08:02:08 | INFO | train | epoch 043 | loss 5.639 | ppl 49.83 | wps 14579.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 16868 | lr 0.000243483 | gnorm 0.729 | loss_scale 4 | train_wall 1738 | gb_free 10.1 | wall 76007
2022-03-04 08:02:08 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 08:02:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:04:32 | INFO | train_inner | epoch 044:     32 / 393 loss=5.658, ppl=50.51, wps=14415.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.729, loss_scale=4, train_wall=441, gb_free=10.1, wall=76150
2022-03-04 08:12:00 | INFO | train_inner | epoch 044:    132 / 393 loss=5.571, ppl=47.53, wps=14618.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.718, loss_scale=8, train_wall=443, gb_free=10.1, wall=76599
2022-03-04 08:19:28 | INFO | train_inner | epoch 044:    232 / 393 loss=5.617, ppl=49.07, wps=14632.2, ups=0.22, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.718, loss_scale=8, train_wall=443, gb_free=10.1, wall=77047
2022-03-04 08:26:56 | INFO | train_inner | epoch 044:    332 / 393 loss=5.657, ppl=50.45, wps=14637, ups=0.22, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.72, loss_scale=8, train_wall=443, gb_free=10.1, wall=77494
2022-03-04 08:31:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:31:34 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.576 | ppl 190.85 | wps 33587.8 | wpb 2034.1 | bsz 4 | num_updates 17261 | best_loss 7.225
2022-03-04 08:31:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17261 updates
2022-03-04 08:31:34 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 08:31:34 | INFO | train | epoch 044 | loss 5.624 | ppl 49.3 | wps 14575.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 17261 | lr 0.000240695 | gnorm 0.726 | loss_scale 8 | train_wall 1739 | gb_free 10.1 | wall 77772
2022-03-04 08:31:34 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 08:31:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:34:28 | INFO | train_inner | epoch 045:     39 / 393 loss=5.634, ppl=49.68, wps=14424.8, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.732, loss_scale=8, train_wall=441, gb_free=10.1, wall=77947
2022-03-04 08:41:56 | INFO | train_inner | epoch 045:    139 / 393 loss=5.566, ppl=47.36, wps=14635.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.755, loss_scale=8, train_wall=443, gb_free=10.1, wall=78394
2022-03-04 08:47:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 08:49:28 | INFO | train_inner | epoch 045:    240 / 393 loss=5.604, ppl=48.63, wps=14490.9, ups=0.22, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.73, loss_scale=8, train_wall=447, gb_free=10.1, wall=78847
2022-03-04 08:56:56 | INFO | train_inner | epoch 045:    340 / 393 loss=5.651, ppl=50.24, wps=14636, ups=0.22, wpb=65535.4, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.726, loss_scale=8, train_wall=443, gb_free=10.1, wall=79294
2022-03-04 09:00:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:00:58 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.594 | ppl 193.17 | wps 33770.2 | wpb 2034.1 | bsz 4 | num_updates 17653 | best_loss 7.225
2022-03-04 09:00:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17653 updates
2022-03-04 09:00:58 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 09:00:58 | INFO | train | epoch 045 | loss 5.609 | ppl 48.81 | wps 14544.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 17653 | lr 0.000238008 | gnorm 0.731 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 79536
2022-03-04 09:00:58 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 09:00:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:04:28 | INFO | train_inner | epoch 046:     47 / 393 loss=5.609, ppl=48.8, wps=14424.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.721, loss_scale=8, train_wall=441, gb_free=10.1, wall=79747
2022-03-04 09:11:56 | INFO | train_inner | epoch 046:    147 / 393 loss=5.548, ppl=46.77, wps=14635.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.738, loss_scale=8, train_wall=443, gb_free=10.1, wall=80195
2022-03-04 09:19:24 | INFO | train_inner | epoch 046:    247 / 393 loss=5.601, ppl=48.52, wps=14640.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.741, loss_scale=8, train_wall=443, gb_free=10.1, wall=80642
2022-03-04 09:26:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:26:56 | INFO | train_inner | epoch 046:    348 / 393 loss=5.636, ppl=49.74, wps=14486.7, ups=0.22, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.74, loss_scale=8, train_wall=447, gb_free=10.1, wall=81095
2022-03-04 09:30:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:30:22 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.596 | ppl 193.41 | wps 33727.6 | wpb 2034.1 | bsz 4 | num_updates 18045 | best_loss 7.225
2022-03-04 09:30:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18045 updates
2022-03-04 09:30:22 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 09:30:22 | INFO | train | epoch 046 | loss 5.595 | ppl 48.34 | wps 14543.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18045 | lr 0.000235408 | gnorm 0.74 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 81301
2022-03-04 09:30:22 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 09:30:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:34:29 | INFO | train_inner | epoch 047:     55 / 393 loss=5.581, ppl=47.88, wps=14421.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.756, loss_scale=8, train_wall=441, gb_free=10.1, wall=81547
2022-03-04 09:41:56 | INFO | train_inner | epoch 047:    155 / 393 loss=5.543, ppl=46.64, wps=14632.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.718, loss_scale=8, train_wall=443, gb_free=10.1, wall=81995
2022-03-04 09:49:24 | INFO | train_inner | epoch 047:    255 / 393 loss=5.591, ppl=48.19, wps=14632.1, ups=0.22, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.75, loss_scale=8, train_wall=443, gb_free=10.1, wall=82443
2022-03-04 09:56:52 | INFO | train_inner | epoch 047:    355 / 393 loss=5.622, ppl=49.25, wps=14637.5, ups=0.22, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.734, loss_scale=8, train_wall=443, gb_free=10.1, wall=82890
2022-03-04 09:59:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:59:47 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.59 | ppl 192.62 | wps 33770.5 | wpb 2034.1 | bsz 4 | num_updates 18438 | best_loss 7.225
2022-03-04 09:59:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18438 updates
2022-03-04 09:59:47 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 09:59:47 | INFO | train | epoch 047 | loss 5.582 | ppl 47.91 | wps 14580.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 18438 | lr 0.000232886 | gnorm 0.739 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 83065
2022-03-04 09:59:47 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 09:59:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:04:24 | INFO | train_inner | epoch 048:     62 / 393 loss=5.56, ppl=47.18, wps=14427.7, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.758, loss_scale=8, train_wall=441, gb_free=10.1, wall=83343
2022-03-04 10:05:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 10:11:56 | INFO | train_inner | epoch 048:    163 / 393 loss=5.533, ppl=46.3, wps=14497.2, ups=0.22, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.741, loss_scale=8, train_wall=447, gb_free=10.1, wall=83795
2022-03-04 10:19:24 | INFO | train_inner | epoch 048:    263 / 393 loss=5.577, ppl=47.73, wps=14645.5, ups=0.22, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.734, loss_scale=8, train_wall=442, gb_free=10.1, wall=84242
2022-03-04 10:26:52 | INFO | train_inner | epoch 048:    363 / 393 loss=5.621, ppl=49.2, wps=14637.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.735, loss_scale=8, train_wall=443, gb_free=10.1, wall=84690
2022-03-04 10:29:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:29:10 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.609 | ppl 195.28 | wps 33911 | wpb 2034.1 | bsz 4 | num_updates 18830 | best_loss 7.225
2022-03-04 10:29:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18830 updates
2022-03-04 10:29:10 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 10:29:10 | INFO | train | epoch 048 | loss 5.569 | ppl 47.49 | wps 14550.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18830 | lr 0.000230449 | gnorm 0.744 | loss_scale 8 | train_wall 1737 | gb_free 10.1 | wall 84829
2022-03-04 10:29:10 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 10:29:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:34:24 | INFO | train_inner | epoch 049:     70 / 393 loss=5.541, ppl=46.57, wps=14426.5, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.775, loss_scale=8, train_wall=441, gb_free=10.1, wall=85142
2022-03-04 10:41:51 | INFO | train_inner | epoch 049:    170 / 393 loss=5.516, ppl=45.76, wps=14640.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.773, loss_scale=8, train_wall=443, gb_free=10.1, wall=85590
2022-03-04 10:44:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 10:49:24 | INFO | train_inner | epoch 049:    271 / 393 loss=5.573, ppl=47.6, wps=14493.3, ups=0.22, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.747, loss_scale=8, train_wall=447, gb_free=10.1, wall=86042
2022-03-04 10:56:51 | INFO | train_inner | epoch 049:    371 / 393 loss=5.61, ppl=48.82, wps=14635.7, ups=0.22, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.761, loss_scale=8, train_wall=443, gb_free=10.1, wall=86490
2022-03-04 10:58:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:58:35 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.62 | ppl 196.77 | wps 33592.4 | wpb 2034.1 | bsz 4 | num_updates 19222 | best_loss 7.225
2022-03-04 10:58:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19222 updates
2022-03-04 10:58:35 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 10:58:35 | INFO | train | epoch 049 | loss 5.558 | ppl 47.1 | wps 14544.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 19222 | lr 0.000228087 | gnorm 0.761 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 86593
2022-03-04 10:58:35 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 10:58:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:04:24 | INFO | train_inner | epoch 050:     78 / 393 loss=5.508, ppl=45.49, wps=14416.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=19300, lr=0.000227626, gnorm=0.739, loss_scale=8, train_wall=441, gb_free=10.1, wall=86942
2022-03-04 11:11:52 | INFO | train_inner | epoch 050:    178 / 393 loss=5.515, ppl=45.72, wps=14635, ups=0.22, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.755, loss_scale=8, train_wall=443, gb_free=10.1, wall=87390
2022-03-04 11:19:20 | INFO | train_inner | epoch 050:    278 / 393 loss=5.567, ppl=47.39, wps=14636, ups=0.22, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.75, loss_scale=8, train_wall=443, gb_free=10.1, wall=87838
2022-03-04 11:22:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 11:26:52 | INFO | train_inner | epoch 050:    379 / 393 loss=5.6, ppl=48.49, wps=14492.6, ups=0.22, wpb=65530.2, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.751, loss_scale=8, train_wall=447, gb_free=10.1, wall=88290
2022-03-04 11:27:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:27:59 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.624 | ppl 197.23 | wps 33795.6 | wpb 2034.1 | bsz 4 | num_updates 19614 | best_loss 7.225
2022-03-04 11:27:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19614 updates
2022-03-04 11:27:59 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 11:27:59 | INFO | train | epoch 050 | loss 5.545 | ppl 46.69 | wps 14544 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 19614 | lr 0.000225796 | gnorm 0.751 | loss_scale 8 | train_wall 1738 | gb_free 10.1 | wall 88357
2022-03-04 11:27:59 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 11:27:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:34:24 | INFO | train_inner | epoch 051:     86 / 393 loss=5.486, ppl=44.83, wps=14423.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.749, loss_scale=8, train_wall=441, gb_free=10.1, wall=88742
2022-03-04 11:34:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 11:41:56 | INFO | train_inner | epoch 051:    187 / 393 loss=5.506, ppl=45.44, wps=14488.6, ups=0.22, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.78, loss_scale=4, train_wall=447, gb_free=10.1, wall=89195
2022-03-04 11:49:24 | INFO | train_inner | epoch 051:    287 / 393 loss=5.563, ppl=47.27, wps=14640.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.757, loss_scale=4, train_wall=443, gb_free=10.1, wall=89642
2022-03-04 11:56:52 | INFO | train_inner | epoch 051:    387 / 393 loss=5.592, ppl=48.23, wps=14635.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.76, loss_scale=4, train_wall=443, gb_free=10.1, wall=90090
2022-03-04 11:57:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:57:23 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.62 | ppl 196.75 | wps 33677.7 | wpb 2034.1 | bsz 4 | num_updates 20006 | best_loss 7.225
2022-03-04 11:57:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20006 updates
2022-03-04 11:57:23 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 11:57:23 | INFO | train | epoch 051 | loss 5.534 | ppl 46.34 | wps 14544.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20006 | lr 0.000223573 | gnorm 0.761 | loss_scale 4 | train_wall 1738 | gb_free 10.1 | wall 90122
2022-03-04 11:57:23 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 11:57:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:04:24 | INFO | train_inner | epoch 052:     94 / 393 loss=5.467, ppl=44.23, wps=14423.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=20100, lr=0.00022305, gnorm=0.766, loss_scale=4, train_wall=441, gb_free=10.1, wall=90543
2022-03-04 12:11:52 | INFO | train_inner | epoch 052:    194 / 393 loss=5.498, ppl=45.2, wps=14635.4, ups=0.22, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.767, loss_scale=4, train_wall=443, gb_free=10.1, wall=90990
2022-03-04 12:19:21 | INFO | train_inner | epoch 052:    294 / 393 loss=5.551, ppl=46.89, wps=14605.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.752, loss_scale=8, train_wall=444, gb_free=10.1, wall=91439
2022-03-04 12:26:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:26:50 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.652 | ppl 201.19 | wps 33492.9 | wpb 2034.1 | bsz 4 | num_updates 20399 | best_loss 7.225
2022-03-04 12:26:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20399 updates
2022-03-04 12:26:50 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 12:26:50 | INFO | train | epoch 052 | loss 5.523 | ppl 45.99 | wps 14563.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 20399 | lr 0.000221409 | gnorm 0.763 | loss_scale 8 | train_wall 1740 | gb_free 10.1 | wall 91888
2022-03-04 12:26:50 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 12:26:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:26:54 | INFO | train_inner | epoch 053:      1 / 393 loss=5.58, ppl=47.82, wps=14381.7, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.774, loss_scale=8, train_wall=442, gb_free=10.1, wall=91893
2022-03-04 12:34:23 | INFO | train_inner | epoch 053:    101 / 393 loss=5.449, ppl=43.7, wps=14594, ups=0.22, wpb=65536, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.767, loss_scale=8, train_wall=444, gb_free=10.1, wall=92342
2022-03-04 12:41:52 | INFO | train_inner | epoch 053:    201 / 393 loss=5.49, ppl=44.95, wps=14595.5, ups=0.22, wpb=65530.2, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.77, loss_scale=8, train_wall=444, gb_free=10.1, wall=92791
2022-03-04 12:49:21 | INFO | train_inner | epoch 053:    301 / 393 loss=5.538, ppl=46.47, wps=14595.5, ups=0.22, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.757, loss_scale=8, train_wall=444, gb_free=10.1, wall=93240
2022-03-04 12:52:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 12:56:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:56:19 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.663 | ppl 202.7 | wps 33355.4 | wpb 2034.1 | bsz 4 | num_updates 20791 | best_loss 7.225
2022-03-04 12:56:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20791 updates
2022-03-04 12:56:19 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 12:56:19 | INFO | train | epoch 053 | loss 5.512 | ppl 45.65 | wps 14503.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20791 | lr 0.000219312 | gnorm 0.764 | loss_scale 8 | train_wall 1742 | gb_free 10.1 | wall 93657
2022-03-04 12:56:19 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 12:56:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:57:00 | INFO | train_inner | epoch 054:      9 / 393 loss=5.566, ppl=47.37, wps=14241, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=20800, lr=0.000219265, gnorm=0.761, loss_scale=8, train_wall=446, gb_free=10.1, wall=93698
2022-03-04 13:04:29 | INFO | train_inner | epoch 054:    109 / 393 loss=5.434, ppl=43.22, wps=14593.1, ups=0.22, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.739, loss_scale=8, train_wall=444, gb_free=10.1, wall=94147
2022-03-04 13:11:58 | INFO | train_inner | epoch 054:    209 / 393 loss=5.492, ppl=45, wps=14596.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.779, loss_scale=8, train_wall=444, gb_free=10.1, wall=94596
2022-03-04 13:19:27 | INFO | train_inner | epoch 054:    309 / 393 loss=5.539, ppl=46.48, wps=14597.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.772, loss_scale=8, train_wall=444, gb_free=10.1, wall=95045
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
