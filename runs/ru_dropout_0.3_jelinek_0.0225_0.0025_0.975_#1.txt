Sender: LSF System <lsfadmin@eu-g3-073>
Subject: Job 209213394: <ru_dropout_0.3_jelinek_0.0225_0.0025_0.975_#1> in cluster <euler> Done

Job <ru_dropout_0.3_jelinek_0.0225_0.0025_0.975_#1> was submitted from host <eu-login-02> by user <andriusb> in cluster <euler> at Thu Mar 17 09:23:07 2022
Job was executed on host(s) <eu-g3-073>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Thu Mar 17 09:23:33 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Mar 17 09:23:33 2022
Terminated at Thu Mar 17 19:15:13 2022
Results reported at Thu Mar 17 19:15:13 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/ru --save-dir /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.3 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas \(0.0225,0.0025,0.975\) --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --seed 66575611 --fp16 --no-epoch-checkpoints --patience 3 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   35482.82 sec.
    Max Memory :                                 3725 MB
    Average Memory :                             2920.39 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               16275.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   35500 sec.
    Turnaround time :                            35526 sec.

The output (if any) follows:

2022-03-17 09:23:39 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.3, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/ru', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.0225,0.0025,0.975)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-17 09:23:39 | INFO | fairseq.tasks.language_modeling | dictionary: 35920 types
2022-03-17 09:23:40 | INFO | fairseq.data.data_utils | loaded 53,136 examples from: data-bin/ru/train
Calculating frequency stats:
  0%|          | 0/53136 [00:00<?, ?it/s]  0%|          | 81/53136 [00:00<01:09, 763.14it/s]  0%|          | 160/53136 [00:00<01:08, 774.53it/s]  0%|          | 245/53136 [00:00<01:05, 804.31it/s]  1%|          | 326/53136 [00:00<01:07, 782.87it/s]  1%|          | 405/53136 [00:00<01:10, 752.42it/s]  1%|          | 504/53136 [00:00<01:03, 828.67it/s]  1%|          | 601/53136 [00:00<01:00, 868.62it/s]  1%|▏         | 689/53136 [00:00<01:00, 867.29it/s]  1%|▏         | 777/53136 [00:00<01:02, 842.97it/s]  2%|▏         | 862/53136 [00:01<01:10, 742.54it/s]  2%|▏         | 952/53136 [00:01<01:06, 783.32it/s]  2%|▏         | 1033/53136 [00:01<01:15, 693.69it/s]  2%|▏         | 1119/53136 [00:01<01:11, 732.44it/s]  2%|▏         | 1197/53136 [00:01<01:09, 742.58it/s]  2%|▏         | 1292/53136 [00:01<01:05, 795.70it/s]  3%|▎         | 1374/53136 [00:01<01:06, 778.63it/s]  3%|▎         | 1467/53136 [00:01<01:02, 820.79it/s]  3%|▎         | 1566/53136 [00:01<01:00, 853.07it/s]  3%|▎         | 1678/53136 [00:02<00:55, 926.07it/s]  3%|▎         | 1781/53136 [00:02<00:54, 946.90it/s]  4%|▎         | 1877/53136 [00:02<00:55, 927.92it/s]  4%|▎         | 1971/53136 [00:02<01:00, 839.81it/s]  4%|▍         | 2077/53136 [00:02<00:57, 892.74it/s]  4%|▍         | 2168/53136 [00:02<00:59, 856.29it/s]  4%|▍         | 2271/53136 [00:02<00:56, 902.02it/s]  4%|▍         | 2376/53136 [00:02<00:54, 933.03it/s]  5%|▍         | 2471/53136 [00:03<01:03, 800.28it/s]  5%|▍         | 2555/53136 [00:03<01:05, 766.98it/s]  5%|▌         | 2657/53136 [00:03<01:00, 828.54it/s]  5%|▌         | 2752/53136 [00:03<00:59, 852.07it/s]  5%|▌         | 2845/53136 [00:03<00:57, 873.25it/s]  6%|▌         | 2934/53136 [00:03<01:03, 794.75it/s]  6%|▌         | 3016/53136 [00:03<01:04, 772.52it/s]  6%|▌         | 3095/53136 [00:03<01:11, 695.88it/s]  6%|▌         | 3167/53136 [00:03<01:12, 687.16it/s]  6%|▌         | 3238/53136 [00:04<01:14, 666.61it/s]  6%|▋         | 3323/53136 [00:04<01:10, 707.79it/s]  6%|▋         | 3395/53136 [00:04<01:13, 679.81it/s]  7%|▋         | 3475/53136 [00:04<01:10, 704.43it/s]  7%|▋         | 3568/53136 [00:04<01:04, 766.20it/s]  7%|▋         | 3646/53136 [00:04<01:17, 641.81it/s]  7%|▋         | 3715/53136 [00:04<01:17, 634.54it/s]  7%|▋         | 3810/53136 [00:04<01:08, 714.93it/s]  7%|▋         | 3889/53136 [00:04<01:07, 733.26it/s]  7%|▋         | 3977/53136 [00:05<01:03, 770.70it/s]  8%|▊         | 4056/53136 [00:05<01:03, 770.64it/s]  8%|▊         | 4151/53136 [00:05<01:00, 815.16it/s]  8%|▊         | 4234/53136 [00:05<01:01, 792.06it/s]  8%|▊         | 4319/53136 [00:05<01:00, 805.25it/s]  8%|▊         | 4401/53136 [00:05<01:10, 692.75it/s]  8%|▊         | 4485/53136 [00:05<01:08, 706.83it/s]  9%|▊         | 4569/53136 [00:05<01:05, 740.87it/s]  9%|▉         | 4659/53136 [00:05<01:01, 784.45it/s]  9%|▉         | 4740/53136 [00:06<01:03, 766.87it/s]  9%|▉         | 4818/53136 [00:06<01:15, 639.36it/s]  9%|▉         | 4893/53136 [00:06<01:12, 665.27it/s]  9%|▉         | 4972/53136 [00:06<01:09, 697.70it/s] 10%|▉         | 5063/53136 [00:06<01:04, 749.47it/s] 10%|▉         | 5141/53136 [00:06<01:12, 661.08it/s] 10%|▉         | 5216/53136 [00:06<01:10, 680.85it/s] 10%|▉         | 5290/53136 [00:06<01:08, 694.41it/s] 10%|█         | 5362/53136 [00:07<01:15, 628.89it/s] 10%|█         | 5452/53136 [00:07<01:08, 699.42it/s] 10%|█         | 5536/53136 [00:07<01:04, 737.44it/s] 11%|█         | 5624/53136 [00:07<01:01, 773.21it/s] 11%|█         | 5704/53136 [00:07<01:02, 764.86it/s] 11%|█         | 5796/53136 [00:07<00:58, 808.38it/s] 11%|█         | 5878/53136 [00:07<01:00, 785.88it/s] 11%|█         | 5958/53136 [00:07<01:02, 753.89it/s] 11%|█▏        | 6061/53136 [00:07<00:56, 827.29it/s] 12%|█▏        | 6145/53136 [00:07<00:57, 820.32it/s] 12%|█▏        | 6228/53136 [00:08<01:07, 699.77it/s] 12%|█▏        | 6309/53136 [00:08<01:04, 727.66it/s] 12%|█▏        | 6404/53136 [00:08<00:59, 783.04it/s] 12%|█▏        | 6495/53136 [00:08<00:57, 817.82it/s] 12%|█▏        | 6602/53136 [00:08<00:52, 889.12it/s] 13%|█▎        | 6700/53136 [00:08<00:51, 908.74it/s] 13%|█▎        | 6797/53136 [00:08<00:51, 894.50it/s] 13%|█▎        | 6888/53136 [00:08<00:55, 831.25it/s] 13%|█▎        | 6973/53136 [00:09<01:10, 657.35it/s] 13%|█▎        | 7060/53136 [00:09<01:05, 704.26it/s] 13%|█▎        | 7153/53136 [00:09<01:00, 758.05it/s] 14%|█▎        | 7244/53136 [00:09<00:58, 788.84it/s] 14%|█▍        | 7327/53136 [00:09<01:03, 720.19it/s] 14%|█▍        | 7403/53136 [00:09<01:15, 603.59it/s] 14%|█▍        | 7473/53136 [00:09<01:16, 596.10it/s] 14%|█▍        | 7559/53136 [00:09<01:09, 659.14it/s] 14%|█▍        | 7648/53136 [00:10<01:03, 714.38it/s] 15%|█▍        | 7733/53136 [00:10<01:00, 749.36it/s] 15%|█▍        | 7829/53136 [00:10<00:56, 807.19it/s] 15%|█▍        | 7925/53136 [00:10<00:53, 849.67it/s] 15%|█▌        | 8013/53136 [00:10<00:53, 843.68it/s] 15%|█▌        | 8099/53136 [00:10<00:56, 802.21it/s] 15%|█▌        | 8181/53136 [00:10<00:57, 778.36it/s] 16%|█▌        | 8260/53136 [00:10<01:06, 670.97it/s] 16%|█▌        | 8336/53136 [00:10<01:04, 692.79it/s] 16%|█▌        | 8414/53136 [00:11<01:02, 715.90it/s] 16%|█▌        | 8493/53136 [00:11<01:00, 735.85it/s] 16%|█▌        | 8585/53136 [00:11<00:56, 784.77it/s] 16%|█▋        | 8665/53136 [00:11<01:02, 711.58it/s] 16%|█▋        | 8739/53136 [00:11<01:04, 689.64it/s] 17%|█▋        | 8833/53136 [00:11<00:58, 756.64it/s] 17%|█▋        | 8911/53136 [00:11<00:58, 756.27it/s] 17%|█▋        | 8988/53136 [00:11<00:59, 736.06it/s] 17%|█▋        | 9063/53136 [00:11<01:02, 710.77it/s] 17%|█▋        | 9156/53136 [00:12<00:57, 770.61it/s] 17%|█▋        | 9245/53136 [00:12<00:54, 802.68it/s] 18%|█▊        | 9327/53136 [00:12<01:05, 670.96it/s] 18%|█▊        | 9425/53136 [00:12<00:58, 746.24it/s] 18%|█▊        | 9522/53136 [00:12<00:54, 801.52it/s] 18%|█▊        | 9623/53136 [00:12<00:50, 857.81it/s] 18%|█▊        | 9712/53136 [00:12<00:52, 831.20it/s] 18%|█▊        | 9798/53136 [00:12<01:14, 580.89it/s] 19%|█▊        | 9877/53136 [00:13<01:09, 625.63it/s] 19%|█▊        | 9950/53136 [00:13<01:09, 622.66it/s] 19%|█▉        | 10020/53136 [00:13<01:08, 626.49it/s] 19%|█▉        | 10123/53136 [00:13<00:59, 728.10it/s] 19%|█▉        | 10205/53136 [00:13<00:57, 750.97it/s] 19%|█▉        | 10285/53136 [00:13<01:00, 704.55it/s] 19%|█▉        | 10359/53136 [00:13<01:01, 693.67it/s] 20%|█▉        | 10431/53136 [00:13<01:07, 629.32it/s] 20%|█▉        | 10503/53136 [00:13<01:05, 649.82it/s] 20%|█▉        | 10570/53136 [00:14<01:11, 596.27it/s] 20%|██        | 10667/53136 [00:14<01:01, 690.97it/s] 20%|██        | 10739/53136 [00:14<01:02, 679.97it/s] 20%|██        | 10811/53136 [00:14<01:01, 685.39it/s] 21%|██        | 10899/53136 [00:14<00:57, 737.85it/s] 21%|██        | 11014/53136 [00:14<00:49, 851.37it/s] 21%|██        | 11101/53136 [00:14<01:08, 616.88it/s] 21%|██        | 11195/53136 [00:14<01:00, 689.66it/s] 21%|██        | 11274/53136 [00:15<01:00, 692.52it/s] 21%|██▏       | 11351/53136 [00:15<00:59, 704.88it/s] 22%|██▏       | 11427/53136 [00:15<01:00, 684.96it/s] 22%|██▏       | 11502/53136 [00:15<01:02, 669.03it/s] 22%|██▏       | 11572/53136 [00:15<01:02, 669.51it/s] 22%|██▏       | 11655/53136 [00:15<00:58, 709.70it/s] 22%|██▏       | 11730/53136 [00:15<00:57, 719.70it/s] 22%|██▏       | 11807/53136 [00:15<00:56, 733.21it/s] 22%|██▏       | 11901/53136 [00:15<00:52, 789.67it/s] 23%|██▎       | 11981/53136 [00:16<00:53, 765.15it/s] 23%|██▎       | 12059/53136 [00:16<00:57, 714.09it/s] 23%|██▎       | 12132/53136 [00:16<01:01, 671.01it/s] 23%|██▎       | 12201/53136 [00:16<01:02, 650.70it/s] 23%|██▎       | 12302/53136 [00:16<00:54, 747.44it/s] 23%|██▎       | 12406/53136 [00:16<00:49, 828.02it/s] 24%|██▎       | 12498/53136 [00:16<00:47, 851.90it/s] 24%|██▎       | 12585/53136 [00:16<01:00, 668.27it/s] 24%|██▍       | 12659/53136 [00:17<01:05, 621.82it/s] 24%|██▍       | 12732/53136 [00:17<01:02, 647.65it/s] 24%|██▍       | 12801/53136 [00:17<01:07, 599.09it/s] 24%|██▍       | 12884/53136 [00:17<01:01, 656.36it/s] 24%|██▍       | 12982/53136 [00:17<00:54, 736.93it/s] 25%|██▍       | 13063/53136 [00:17<00:53, 751.23it/s] 25%|██▍       | 13142/53136 [00:17<00:54, 739.62it/s] 25%|██▍       | 13227/53136 [00:17<00:51, 769.65it/s] 25%|██▌       | 13308/53136 [00:17<00:51, 780.20it/s] 25%|██▌       | 13392/53136 [00:18<00:50, 793.01it/s] 25%|██▌       | 13478/53136 [00:18<00:48, 810.75it/s] 26%|██▌       | 13560/53136 [00:18<00:56, 698.45it/s] 26%|██▌       | 13647/53136 [00:18<00:53, 739.08it/s] 26%|██▌       | 13724/53136 [00:18<00:53, 733.75it/s] 26%|██▌       | 13812/53136 [00:18<00:52, 749.47it/s] 26%|██▌       | 13889/53136 [00:18<01:02, 631.48it/s] 26%|██▋       | 13978/53136 [00:18<00:56, 689.68it/s] 26%|██▋       | 14051/53136 [00:18<00:56, 691.48it/s] 27%|██▋       | 14123/53136 [00:19<00:55, 697.67it/s] 27%|██▋       | 14195/53136 [00:19<00:56, 694.20it/s] 27%|██▋       | 14269/53136 [00:19<00:55, 705.04it/s] 27%|██▋       | 14341/53136 [00:19<00:55, 693.19it/s] 27%|██▋       | 14412/53136 [00:19<00:58, 664.30it/s] 27%|██▋       | 14485/53136 [00:19<00:56, 680.69it/s] 27%|██▋       | 14564/53136 [00:19<00:54, 711.24it/s] 28%|██▊       | 14681/53136 [00:19<00:45, 843.43it/s] 28%|██▊       | 14767/53136 [00:19<00:45, 846.92it/s] 28%|██▊       | 14853/53136 [00:20<00:45, 836.27it/s] 28%|██▊       | 14938/53136 [00:20<00:46, 829.17it/s] 28%|██▊       | 15022/53136 [00:20<00:56, 677.91it/s] 28%|██▊       | 15095/53136 [00:20<00:57, 657.29it/s] 29%|██▊       | 15180/53136 [00:20<00:53, 704.87it/s] 29%|██▊       | 15254/53136 [00:20<00:53, 704.00it/s] 29%|██▉       | 15327/53136 [00:20<00:57, 658.01it/s] 29%|██▉       | 15400/53136 [00:20<00:56, 673.05it/s] 29%|██▉       | 15469/53136 [00:21<01:01, 612.33it/s] 29%|██▉       | 15541/53136 [00:21<00:58, 637.76it/s] 29%|██▉       | 15607/53136 [00:21<00:58, 643.32it/s] 30%|██▉       | 15685/53136 [00:21<00:55, 679.44it/s] 30%|██▉       | 15777/53136 [00:21<00:50, 746.12it/s] 30%|██▉       | 15853/53136 [00:21<01:00, 619.44it/s] 30%|██▉       | 15938/53136 [00:21<00:55, 668.27it/s] 30%|███       | 16016/53136 [00:21<00:53, 691.11it/s] 30%|███       | 16097/53136 [00:21<00:51, 718.93it/s] 30%|███       | 16172/53136 [00:21<00:51, 722.31it/s] 31%|███       | 16246/53136 [00:22<00:55, 665.57it/s] 31%|███       | 16339/53136 [00:22<00:51, 716.85it/s] 31%|███       | 16421/53136 [00:22<00:49, 744.70it/s] 31%|███       | 16511/53136 [00:22<00:46, 786.02it/s] 31%|███       | 16600/53136 [00:22<00:44, 812.56it/s] 31%|███▏      | 16685/53136 [00:22<00:47, 770.61it/s] 32%|███▏      | 16777/53136 [00:22<00:44, 811.59it/s] 32%|███▏      | 16869/53136 [00:22<00:43, 836.12it/s] 32%|███▏      | 16954/53136 [00:22<00:44, 815.11it/s] 32%|███▏      | 17037/53136 [00:23<00:45, 787.43it/s] 32%|███▏      | 17131/53136 [00:23<00:43, 828.51it/s] 32%|███▏      | 17215/53136 [00:23<00:43, 824.90it/s] 33%|███▎      | 17298/53136 [00:23<00:45, 780.50it/s] 33%|███▎      | 17380/53136 [00:23<00:47, 749.52it/s] 33%|███▎      | 17469/53136 [00:23<00:45, 786.83it/s] 33%|███▎      | 17549/53136 [00:23<00:49, 713.91it/s] 33%|███▎      | 17627/53136 [00:23<00:49, 723.42it/s] 33%|███▎      | 17724/53136 [00:24<00:47, 741.48it/s] 34%|███▎      | 17806/53136 [00:24<00:46, 759.78it/s] 34%|███▎      | 17883/53136 [00:24<00:51, 690.77it/s] 34%|███▍      | 17954/53136 [00:24<00:52, 671.70it/s] 34%|███▍      | 18043/53136 [00:24<00:48, 729.15it/s] 34%|███▍      | 18118/53136 [00:24<00:48, 715.70it/s] 34%|███▍      | 18199/53136 [00:24<00:47, 737.38it/s] 34%|███▍      | 18274/53136 [00:24<00:51, 676.28it/s] 35%|███▍      | 18343/53136 [00:24<00:56, 619.61it/s] 35%|███▍      | 18410/53136 [00:25<00:55, 631.36it/s] 35%|███▍      | 18477/53136 [00:25<00:54, 637.74it/s] 35%|███▍      | 18542/53136 [00:25<00:56, 613.62it/s] 35%|███▌      | 18616/53136 [00:25<00:53, 648.23it/s] 35%|███▌      | 18682/53136 [00:25<00:56, 608.57it/s] 35%|███▌      | 18782/53136 [00:25<00:52, 654.27it/s] 36%|███▌      | 18880/53136 [00:25<00:46, 733.76it/s] 36%|███▌      | 18960/53136 [00:25<00:45, 748.66it/s] 36%|███▌      | 19066/53136 [00:25<00:41, 822.63it/s] 36%|███▌      | 19150/53136 [00:26<00:41, 815.30it/s] 36%|███▌      | 19233/53136 [00:26<00:42, 801.84it/s] 36%|███▋      | 19314/53136 [00:26<00:47, 718.33it/s] 36%|███▋      | 19390/53136 [00:26<00:46, 728.16it/s] 37%|███▋      | 19465/53136 [00:26<00:49, 676.25it/s] 37%|███▋      | 19546/53136 [00:26<00:47, 711.62it/s] 37%|███▋      | 19619/53136 [00:26<00:51, 652.57it/s] 37%|███▋      | 19706/53136 [00:26<00:47, 709.67it/s] 37%|███▋      | 19779/53136 [00:26<00:48, 681.23it/s] 37%|███▋      | 19849/53136 [00:27<00:49, 678.57it/s] 38%|███▊      | 19940/53136 [00:27<00:44, 740.88it/s] 38%|███▊      | 20021/53136 [00:27<00:43, 756.16it/s] 38%|███▊      | 20103/53136 [00:27<00:42, 772.03it/s] 38%|███▊      | 20181/53136 [00:27<00:46, 715.92it/s] 38%|███▊      | 20254/53136 [00:27<00:53, 616.83it/s] 38%|███▊      | 20323/53136 [00:27<00:52, 627.41it/s] 38%|███▊      | 20393/53136 [00:27<00:50, 646.23it/s] 39%|███▊      | 20471/53136 [00:27<00:47, 681.97it/s] 39%|███▊      | 20541/53136 [00:28<00:55, 585.47it/s] 39%|███▉      | 20608/53136 [00:28<00:53, 605.17it/s] 39%|███▉      | 20688/53136 [00:28<00:49, 656.89it/s] 39%|███▉      | 20788/53136 [00:28<00:43, 747.32it/s] 39%|███▉      | 20866/53136 [00:28<00:45, 715.25it/s] 39%|███▉      | 20950/53136 [00:28<00:44, 722.14it/s] 40%|███▉      | 21030/53136 [00:28<00:43, 739.25it/s] 40%|███▉      | 21105/53136 [00:28<00:43, 740.90it/s] 40%|███▉      | 21180/53136 [00:29<00:51, 622.72it/s] 40%|████      | 21269/53136 [00:29<00:46, 689.91it/s] 40%|████      | 21353/53136 [00:29<00:43, 727.48it/s] 40%|████      | 21429/53136 [00:29<00:44, 718.33it/s] 41%|████      | 21529/53136 [00:29<00:39, 796.36it/s] 41%|████      | 21611/53136 [00:29<00:43, 718.04it/s] 41%|████      | 21686/53136 [00:29<00:52, 602.82it/s] 41%|████      | 21751/53136 [00:29<00:56, 560.25it/s] 41%|████      | 21825/53136 [00:30<00:52, 601.52it/s] 41%|████      | 21917/53136 [00:30<00:45, 681.65it/s] 41%|████▏     | 21994/53136 [00:30<00:44, 704.31it/s] 42%|████▏     | 22068/53136 [00:30<00:46, 664.30it/s] 42%|████▏     | 22140/53136 [00:30<00:45, 677.06it/s] 42%|████▏     | 22214/53136 [00:30<00:44, 694.20it/s] 42%|████▏     | 22293/53136 [00:30<00:42, 718.49it/s] 42%|████▏     | 22366/53136 [00:30<00:44, 686.63it/s] 42%|████▏     | 22452/53136 [00:30<00:42, 730.56it/s] 42%|████▏     | 22536/53136 [00:30<00:40, 761.36it/s] 43%|████▎     | 22613/53136 [00:31<00:40, 755.88it/s] 43%|████▎     | 22705/53136 [00:31<00:37, 801.86it/s] 43%|████▎     | 22786/53136 [00:31<00:38, 778.92it/s] 43%|████▎     | 22865/53136 [00:31<00:40, 740.11it/s] 43%|████▎     | 22957/53136 [00:31<00:38, 786.99it/s] 43%|████▎     | 23037/53136 [00:31<00:46, 646.64it/s] 44%|████▎     | 23137/53136 [00:31<00:40, 733.16it/s] 44%|████▎     | 23216/53136 [00:31<00:41, 723.11it/s] 44%|████▍     | 23301/53136 [00:32<00:43, 687.33it/s] 44%|████▍     | 23387/53136 [00:32<00:41, 725.33it/s] 44%|████▍     | 23462/53136 [00:32<00:40, 725.51it/s] 44%|████▍     | 23537/53136 [00:32<00:44, 666.26it/s] 44%|████▍     | 23606/53136 [00:32<00:50, 582.51it/s] 45%|████▍     | 23672/53136 [00:32<00:49, 601.21it/s] 45%|████▍     | 23783/53136 [00:32<00:40, 730.86it/s] 45%|████▍     | 23860/53136 [00:32<00:46, 632.44it/s] 45%|████▌     | 23947/53136 [00:32<00:42, 688.97it/s] 45%|████▌     | 24021/53136 [00:33<00:41, 696.62it/s] 45%|████▌     | 24105/53136 [00:33<00:39, 734.94it/s] 46%|████▌     | 24189/53136 [00:33<00:37, 764.08it/s] 46%|████▌     | 24273/53136 [00:33<00:36, 782.13it/s] 46%|████▌     | 24353/53136 [00:33<00:37, 758.72it/s] 46%|████▌     | 24435/53136 [00:33<00:37, 774.51it/s] 46%|████▌     | 24529/53136 [00:33<00:34, 820.47it/s] 46%|████▋     | 24625/53136 [00:33<00:33, 859.51it/s] 47%|████▋     | 24712/53136 [00:33<00:33, 861.23it/s] 47%|████▋     | 24799/53136 [00:34<00:38, 730.93it/s] 47%|████▋     | 24884/53136 [00:34<00:37, 761.62it/s] 47%|████▋     | 24964/53136 [00:34<00:38, 735.67it/s] 47%|████▋     | 25065/53136 [00:34<00:34, 809.74it/s] 47%|████▋     | 25149/53136 [00:34<00:34, 809.46it/s] 47%|████▋     | 25232/53136 [00:34<00:39, 708.79it/s] 48%|████▊     | 25338/53136 [00:34<00:34, 799.11it/s] 48%|████▊     | 25422/53136 [00:34<00:36, 758.03it/s] 48%|████▊     | 25501/53136 [00:35<00:39, 706.69it/s] 48%|████▊     | 25574/53136 [00:35<00:38, 710.26it/s] 48%|████▊     | 25647/53136 [00:35<00:38, 708.39it/s] 48%|████▊     | 25737/53136 [00:35<00:36, 756.98it/s] 49%|████▊     | 25823/53136 [00:35<00:34, 785.29it/s] 49%|████▊     | 25903/53136 [00:35<00:38, 711.52it/s] 49%|████▉     | 25983/53136 [00:35<00:36, 734.40it/s] 49%|████▉     | 26069/53136 [00:35<00:35, 768.94it/s] 49%|████▉     | 26148/53136 [00:35<00:42, 642.00it/s] 49%|████▉     | 26217/53136 [00:36<00:44, 611.28it/s] 49%|████▉     | 26282/53136 [00:36<00:43, 618.33it/s] 50%|████▉     | 26356/53136 [00:36<00:41, 639.58it/s] 50%|████▉     | 26438/53136 [00:36<00:38, 688.21it/s] 50%|████▉     | 26525/53136 [00:36<00:38, 688.24it/s] 50%|█████     | 26599/53136 [00:36<00:37, 700.92it/s] 50%|█████     | 26674/53136 [00:36<00:37, 714.33it/s] 50%|█████     | 26763/53136 [00:36<00:34, 762.61it/s] 51%|█████     | 26841/53136 [00:36<00:34, 756.60it/s] 51%|█████     | 26918/53136 [00:37<00:38, 685.56it/s] 51%|█████     | 26998/53136 [00:37<00:36, 708.56it/s] 51%|█████     | 27098/53136 [00:37<00:33, 787.75it/s] 51%|█████     | 27179/53136 [00:37<00:37, 700.00it/s] 51%|█████▏    | 27252/53136 [00:37<00:37, 694.90it/s] 51%|█████▏    | 27326/53136 [00:37<00:37, 679.59it/s] 52%|█████▏    | 27421/53136 [00:37<00:34, 748.94it/s] 52%|█████▏    | 27498/53136 [00:37<00:38, 671.63it/s] 52%|█████▏    | 27568/53136 [00:37<00:38, 672.09it/s] 52%|█████▏    | 27637/53136 [00:38<00:37, 671.07it/s] 52%|█████▏    | 27716/53136 [00:38<00:36, 703.61it/s] 52%|█████▏    | 27813/53136 [00:38<00:32, 776.65it/s] 52%|█████▏    | 27892/53136 [00:38<00:38, 664.25it/s] 53%|█████▎    | 27973/53136 [00:38<00:35, 700.66it/s] 53%|█████▎    | 28062/53136 [00:38<00:33, 742.51it/s] 53%|█████▎    | 28139/53136 [00:38<00:38, 648.31it/s] 53%|█████▎    | 28211/53136 [00:38<00:38, 640.11it/s] 53%|█████▎    | 28296/53136 [00:39<00:36, 671.84it/s] 53%|█████▎    | 28380/53136 [00:39<00:34, 715.47it/s] 54%|█████▎    | 28468/53136 [00:39<00:32, 757.37it/s] 54%|█████▍    | 28578/53136 [00:39<00:29, 844.73it/s] 54%|█████▍    | 28665/53136 [00:39<00:29, 830.82it/s] 54%|█████▍    | 28750/53136 [00:39<00:30, 811.87it/s] 54%|█████▍    | 28833/53136 [00:39<00:31, 760.84it/s] 54%|█████▍    | 28929/53136 [00:39<00:29, 811.87it/s] 55%|█████▍    | 29012/53136 [00:39<00:30, 781.41it/s] 55%|█████▍    | 29092/53136 [00:39<00:30, 777.96it/s] 55%|█████▍    | 29171/53136 [00:40<00:32, 743.53it/s] 55%|█████▌    | 29246/53136 [00:40<00:33, 712.06it/s] 55%|█████▌    | 29318/53136 [00:40<00:34, 697.10it/s] 55%|█████▌    | 29391/53136 [00:40<00:33, 702.82it/s] 55%|█████▌    | 29477/53136 [00:40<00:31, 747.05it/s] 56%|█████▌    | 29573/53136 [00:40<00:29, 798.40it/s] 56%|█████▌    | 29654/53136 [00:40<00:30, 780.31it/s] 56%|█████▌    | 29733/53136 [00:40<00:38, 605.97it/s] 56%|█████▌    | 29803/53136 [00:41<00:39, 596.49it/s] 56%|█████▋    | 29904/53136 [00:41<00:33, 697.13it/s] 56%|█████▋    | 29979/53136 [00:41<00:32, 704.55it/s] 57%|█████▋    | 30054/53136 [00:41<00:41, 561.30it/s] 57%|█████▋    | 30130/53136 [00:41<00:37, 607.01it/s] 57%|█████▋    | 30207/53136 [00:41<00:35, 640.64it/s] 57%|█████▋    | 30307/53136 [00:41<00:31, 733.34it/s] 57%|█████▋    | 30395/53136 [00:41<00:29, 765.39it/s] 57%|█████▋    | 30476/53136 [00:42<00:36, 613.89it/s] 57%|█████▋    | 30545/53136 [00:42<00:36, 618.67it/s] 58%|█████▊    | 30623/53136 [00:42<00:34, 652.62it/s] 58%|█████▊    | 30708/53136 [00:42<00:32, 691.61it/s] 58%|█████▊    | 30793/53136 [00:42<00:31, 711.83it/s] 58%|█████▊    | 30867/53136 [00:42<00:31, 717.34it/s] 58%|█████▊    | 30943/53136 [00:42<00:30, 724.25it/s] 58%|█████▊    | 31017/53136 [00:42<00:32, 671.14it/s] 59%|█████▊    | 31092/53136 [00:42<00:31, 691.46it/s] 59%|█████▊    | 31163/53136 [00:43<00:33, 651.02it/s] 59%|█████▉    | 31274/53136 [00:43<00:28, 773.35it/s] 59%|█████▉    | 31368/53136 [00:43<00:26, 818.89it/s] 59%|█████▉    | 31452/53136 [00:43<00:26, 824.84it/s] 59%|█████▉    | 31536/53136 [00:43<00:26, 813.40it/s] 60%|█████▉    | 31619/53136 [00:43<00:27, 780.61it/s] 60%|█████▉    | 31698/53136 [00:43<00:28, 741.49it/s] 60%|█████▉    | 31805/53136 [00:43<00:25, 831.31it/s] 60%|██████    | 31890/53136 [00:43<00:27, 782.44it/s] 60%|██████    | 31970/53136 [00:44<00:29, 706.13it/s] 60%|██████    | 32047/53136 [00:44<00:29, 722.02it/s] 60%|██████    | 32145/53136 [00:44<00:26, 790.73it/s] 61%|██████    | 32232/53136 [00:44<00:25, 809.79it/s] 61%|██████    | 32315/53136 [00:44<00:29, 700.08it/s] 61%|██████    | 32393/53136 [00:44<00:28, 718.25it/s] 61%|██████    | 32474/53136 [00:44<00:27, 741.54it/s] 61%|██████▏   | 32551/53136 [00:44<00:28, 731.83it/s] 61%|██████▏   | 32639/53136 [00:44<00:26, 767.96it/s] 62%|██████▏   | 32718/53136 [00:45<00:27, 745.38it/s] 62%|██████▏   | 32809/53136 [00:45<00:25, 788.77it/s] 62%|██████▏   | 32903/53136 [00:45<00:24, 829.05it/s] 62%|██████▏   | 32987/53136 [00:45<00:24, 824.93it/s] 62%|██████▏   | 33071/53136 [00:45<00:25, 797.63it/s] 62%|██████▏   | 33152/53136 [00:45<00:25, 784.10it/s] 63%|██████▎   | 33231/53136 [00:45<00:28, 705.03it/s] 63%|██████▎   | 33304/53136 [00:45<00:30, 659.83it/s] 63%|██████▎   | 33373/53136 [00:45<00:29, 666.63it/s] 63%|██████▎   | 33475/53136 [00:46<00:25, 761.23it/s] 63%|██████▎   | 33553/53136 [00:46<00:29, 663.39it/s] 63%|██████▎   | 33630/53136 [00:46<00:28, 690.17it/s] 63%|██████▎   | 33705/53136 [00:46<00:27, 706.06it/s] 64%|██████▎   | 33778/53136 [00:46<00:31, 616.60it/s] 64%|██████▎   | 33856/53136 [00:46<00:29, 656.70it/s] 64%|██████▍   | 33925/53136 [00:46<00:29, 658.07it/s] 64%|██████▍   | 34014/53136 [00:46<00:26, 720.12it/s] 64%|██████▍   | 34091/53136 [00:46<00:25, 734.01it/s] 64%|██████▍   | 34191/53136 [00:47<00:23, 808.25it/s] 65%|██████▍   | 34274/53136 [00:47<00:24, 775.40it/s] 65%|██████▍   | 34353/53136 [00:47<00:29, 630.32it/s] 65%|██████▍   | 34449/53136 [00:47<00:26, 710.17it/s] 65%|██████▍   | 34526/53136 [00:47<00:26, 699.81it/s] 65%|██████▌   | 34600/53136 [00:47<00:27, 678.36it/s] 65%|██████▌   | 34679/53136 [00:47<00:26, 705.37it/s] 65%|██████▌   | 34772/53136 [00:47<00:23, 766.64it/s] 66%|██████▌   | 34851/53136 [00:48<00:30, 592.34it/s] 66%|██████▌   | 34923/53136 [00:48<00:29, 615.12it/s] 66%|██████▌   | 34994/53136 [00:48<00:28, 636.90it/s] 66%|██████▌   | 35063/53136 [00:48<00:31, 571.14it/s] 66%|██████▌   | 35125/53136 [00:48<00:32, 558.84it/s] 66%|██████▌   | 35184/53136 [00:48<00:31, 566.03it/s] 66%|██████▋   | 35261/53136 [00:48<00:28, 619.38it/s] 66%|██████▋   | 35333/53136 [00:48<00:27, 640.95it/s] 67%|██████▋   | 35410/53136 [00:49<00:26, 676.49it/s] 67%|██████▋   | 35480/53136 [00:49<00:27, 635.10it/s] 67%|██████▋   | 35566/53136 [00:49<00:25, 695.45it/s] 67%|██████▋   | 35650/53136 [00:49<00:23, 735.36it/s] 67%|██████▋   | 35725/53136 [00:49<00:24, 701.75it/s] 67%|██████▋   | 35818/53136 [00:49<00:22, 762.65it/s] 68%|██████▊   | 35896/53136 [00:49<00:23, 744.09it/s] 68%|██████▊   | 35972/53136 [00:49<00:27, 633.64it/s] 68%|██████▊   | 36040/53136 [00:49<00:27, 632.84it/s] 68%|██████▊   | 36106/53136 [00:50<00:27, 630.27it/s] 68%|██████▊   | 36184/53136 [00:50<00:26, 629.61it/s] 68%|██████▊   | 36249/53136 [00:50<00:31, 533.51it/s] 68%|██████▊   | 36329/53136 [00:50<00:28, 595.00it/s] 69%|██████▊   | 36424/53136 [00:50<00:24, 685.07it/s] 69%|██████▊   | 36497/53136 [00:50<00:24, 673.80it/s] 69%|██████▉   | 36577/53136 [00:50<00:23, 705.91it/s] 69%|██████▉   | 36650/53136 [00:50<00:24, 668.73it/s] 69%|██████▉   | 36719/53136 [00:51<00:24, 663.61it/s] 69%|██████▉   | 36787/53136 [00:51<00:27, 588.58it/s] 69%|██████▉   | 36855/53136 [00:51<00:26, 607.45it/s] 70%|██████▉   | 36934/53136 [00:51<00:24, 654.95it/s] 70%|██████▉   | 37011/53136 [00:51<00:24, 669.77it/s] 70%|██████▉   | 37080/53136 [00:51<00:23, 671.99it/s] 70%|██████▉   | 37149/53136 [00:51<00:24, 650.14it/s] 70%|███████   | 37228/53136 [00:51<00:23, 687.08it/s] 70%|███████   | 37298/53136 [00:51<00:25, 612.73it/s] 70%|███████   | 37383/53136 [00:52<00:23, 674.71it/s] 70%|███████   | 37454/53136 [00:52<00:23, 681.22it/s] 71%|███████   | 37546/53136 [00:52<00:21, 738.66it/s] 71%|███████   | 37633/53136 [00:52<00:20, 740.45it/s] 71%|███████   | 37730/53136 [00:52<00:19, 776.51it/s] 71%|███████   | 37811/53136 [00:52<00:19, 781.24it/s] 71%|███████▏  | 37897/53136 [00:52<00:18, 803.18it/s] 71%|███████▏  | 37978/53136 [00:52<00:19, 787.32it/s] 72%|███████▏  | 38058/53136 [00:52<00:19, 763.06it/s] 72%|███████▏  | 38150/53136 [00:52<00:18, 805.34it/s] 72%|███████▏  | 38231/53136 [00:53<00:24, 608.83it/s] 72%|███████▏  | 38300/53136 [00:53<00:23, 623.22it/s] 72%|███████▏  | 38370/53136 [00:53<00:23, 641.52it/s] 72%|███████▏  | 38439/53136 [00:53<00:22, 644.60it/s] 72%|███████▏  | 38522/53136 [00:53<00:21, 692.45it/s] 73%|███████▎  | 38594/53136 [00:53<00:21, 677.83it/s] 73%|███████▎  | 38664/53136 [00:53<00:21, 660.64it/s] 73%|███████▎  | 38739/53136 [00:53<00:22, 643.52it/s] 73%|███████▎  | 38821/53136 [00:54<00:20, 690.37it/s] 73%|███████▎  | 38892/53136 [00:54<00:20, 689.35it/s] 73%|███████▎  | 38989/53136 [00:54<00:18, 767.34it/s] 74%|███████▎  | 39067/53136 [00:54<00:18, 750.40it/s] 74%|███████▎  | 39144/53136 [00:54<00:18, 752.77it/s] 74%|███████▍  | 39237/53136 [00:54<00:17, 801.51it/s] 74%|███████▍  | 39321/53136 [00:54<00:17, 812.30it/s] 74%|███████▍  | 39403/53136 [00:54<00:18, 737.12it/s] 74%|███████▍  | 39479/53136 [00:54<00:18, 729.11it/s] 74%|███████▍  | 39553/53136 [00:55<00:20, 648.01it/s] 75%|███████▍  | 39620/53136 [00:55<00:21, 636.57it/s] 75%|███████▍  | 39686/53136 [00:55<00:22, 604.19it/s] 75%|███████▍  | 39768/53136 [00:55<00:20, 660.92it/s] 75%|███████▌  | 39859/53136 [00:55<00:18, 726.60it/s] 75%|███████▌  | 39944/53136 [00:55<00:17, 760.92it/s] 75%|███████▌  | 40034/53136 [00:55<00:16, 800.41it/s] 75%|███████▌  | 40116/53136 [00:55<00:17, 756.11it/s] 76%|███████▌  | 40212/53136 [00:55<00:15, 812.78it/s] 76%|███████▌  | 40295/53136 [00:56<00:18, 688.50it/s] 76%|███████▌  | 40368/53136 [00:56<00:18, 697.89it/s] 76%|███████▌  | 40447/53136 [00:56<00:17, 720.34it/s] 76%|███████▋  | 40533/53136 [00:56<00:16, 750.88it/s] 76%|███████▋  | 40619/53136 [00:56<00:16, 779.38it/s] 77%|███████▋  | 40713/53136 [00:56<00:15, 806.95it/s] 77%|███████▋  | 40795/53136 [00:56<00:16, 756.71it/s] 77%|███████▋  | 40872/53136 [00:56<00:16, 723.07it/s] 77%|███████▋  | 40975/53136 [00:56<00:15, 798.54it/s] 77%|███████▋  | 41060/53136 [00:57<00:14, 809.08it/s] 77%|███████▋  | 41142/53136 [00:57<00:17, 684.37it/s] 78%|███████▊  | 41215/53136 [00:57<00:17, 679.26it/s] 78%|███████▊  | 41286/53136 [00:57<00:18, 645.53it/s] 78%|███████▊  | 41373/53136 [00:57<00:16, 700.38it/s] 78%|███████▊  | 41462/53136 [00:57<00:15, 749.74it/s] 78%|███████▊  | 41551/53136 [00:57<00:14, 784.45it/s] 78%|███████▊  | 41648/53136 [00:57<00:13, 832.53it/s] 79%|███████▊  | 41733/53136 [00:58<00:16, 688.46it/s] 79%|███████▊  | 41812/53136 [00:58<00:15, 711.89it/s] 79%|███████▉  | 41888/53136 [00:58<00:16, 688.12it/s] 79%|███████▉  | 41960/53136 [00:58<00:16, 695.97it/s] 79%|███████▉  | 42033/53136 [00:58<00:15, 699.60it/s] 79%|███████▉  | 42113/53136 [00:58<00:15, 727.27it/s] 79%|███████▉  | 42189/53136 [00:58<00:14, 736.27it/s] 80%|███████▉  | 42269/53136 [00:58<00:14, 751.87it/s] 80%|███████▉  | 42345/53136 [00:58<00:15, 710.02it/s] 80%|███████▉  | 42442/53136 [00:58<00:13, 782.06it/s] 80%|████████  | 42527/53136 [00:59<00:13, 801.51it/s] 80%|████████  | 42608/53136 [00:59<00:13, 795.31it/s] 80%|████████  | 42705/53136 [00:59<00:12, 837.23it/s] 81%|████████  | 42790/53136 [00:59<00:12, 828.83it/s] 81%|████████  | 42874/53136 [00:59<00:12, 807.22it/s] 81%|████████  | 42956/53136 [00:59<00:13, 754.21it/s] 81%|████████  | 43033/53136 [00:59<00:13, 734.78it/s] 81%|████████  | 43108/53136 [00:59<00:14, 670.00it/s] 81%|████████▏ | 43177/53136 [00:59<00:15, 644.69it/s] 81%|████████▏ | 43269/53136 [01:00<00:13, 716.96it/s] 82%|████████▏ | 43357/53136 [01:00<00:12, 755.54it/s] 82%|████████▏ | 43434/53136 [01:00<00:14, 674.85it/s] 82%|████████▏ | 43504/53136 [01:00<00:14, 653.03it/s] 82%|████████▏ | 43571/53136 [01:00<00:15, 626.10it/s] 82%|████████▏ | 43648/53136 [01:00<00:14, 659.32it/s] 82%|████████▏ | 43716/53136 [01:00<00:15, 627.21it/s] 82%|████████▏ | 43780/53136 [01:00<00:15, 599.51it/s] 83%|████████▎ | 43857/53136 [01:01<00:14, 641.04it/s] 83%|████████▎ | 43938/53136 [01:01<00:13, 684.44it/s] 83%|████████▎ | 44008/53136 [01:01<00:13, 683.82it/s] 83%|████████▎ | 44085/53136 [01:01<00:12, 706.55it/s] 83%|████████▎ | 44157/53136 [01:01<00:15, 566.34it/s] 83%|████████▎ | 44235/53136 [01:01<00:14, 616.46it/s] 83%|████████▎ | 44321/53136 [01:01<00:12, 679.53it/s] 84%|████████▎ | 44402/53136 [01:01<00:13, 643.25it/s] 84%|████████▎ | 44483/53136 [01:01<00:12, 684.59it/s] 84%|████████▍ | 44555/53136 [01:02<00:12, 673.37it/s] 84%|████████▍ | 44645/53136 [01:02<00:11, 728.76it/s] 84%|████████▍ | 44725/53136 [01:02<00:11, 746.54it/s] 84%|████████▍ | 44811/53136 [01:02<00:10, 778.42it/s] 84%|████████▍ | 44898/53136 [01:02<00:10, 804.32it/s] 85%|████████▍ | 44980/53136 [01:02<00:10, 803.72it/s] 85%|████████▍ | 45074/53136 [01:02<00:09, 842.65it/s] 85%|████████▍ | 45163/53136 [01:02<00:09, 856.36it/s] 85%|████████▌ | 45250/53136 [01:02<00:09, 833.21it/s] 85%|████████▌ | 45334/53136 [01:03<00:09, 792.81it/s] 85%|████████▌ | 45414/53136 [01:03<00:10, 741.77it/s] 86%|████████▌ | 45495/53136 [01:03<00:10, 759.26it/s] 86%|████████▌ | 45572/53136 [01:03<00:11, 650.35it/s] 86%|████████▌ | 45641/53136 [01:03<00:11, 644.86it/s] 86%|████████▌ | 45708/53136 [01:03<00:11, 651.17it/s] 86%|████████▌ | 45784/53136 [01:03<00:10, 679.12it/s] 86%|████████▋ | 45865/53136 [01:03<00:10, 714.44it/s] 86%|████████▋ | 45955/53136 [01:03<00:09, 766.13it/s] 87%|████████▋ | 46033/53136 [01:04<00:10, 687.07it/s] 87%|████████▋ | 46122/53136 [01:04<00:09, 741.62it/s] 87%|████████▋ | 46199/53136 [01:04<00:09, 704.25it/s] 87%|████████▋ | 46277/53136 [01:04<00:09, 720.59it/s] 87%|████████▋ | 46364/53136 [01:04<00:08, 761.05it/s] 87%|████████▋ | 46459/53136 [01:04<00:08, 813.07it/s] 88%|████████▊ | 46547/53136 [01:04<00:07, 830.64it/s] 88%|████████▊ | 46631/53136 [01:04<00:08, 724.35it/s] 88%|████████▊ | 46709/53136 [01:04<00:08, 736.14it/s] 88%|████████▊ | 46785/53136 [01:05<00:09, 681.50it/s] 88%|████████▊ | 46868/53136 [01:05<00:08, 718.42it/s] 88%|████████▊ | 46947/53136 [01:05<00:08, 734.81it/s] 88%|████████▊ | 47025/53136 [01:05<00:08, 742.76it/s] 89%|████████▊ | 47101/53136 [01:05<00:08, 741.51it/s] 89%|████████▉ | 47186/53136 [01:05<00:08, 723.55it/s] 89%|████████▉ | 47260/53136 [01:05<00:08, 676.45it/s] 89%|████████▉ | 47344/53136 [01:05<00:08, 708.64it/s] 89%|████████▉ | 47416/53136 [01:05<00:08, 705.04it/s] 89%|████████▉ | 47490/53136 [01:06<00:07, 714.31it/s] 90%|████████▉ | 47562/53136 [01:06<00:08, 662.35it/s] 90%|████████▉ | 47652/53136 [01:06<00:07, 725.90it/s] 90%|████████▉ | 47726/53136 [01:06<00:07, 715.67it/s] 90%|████████▉ | 47802/53136 [01:06<00:07, 723.23it/s] 90%|█████████ | 47875/53136 [01:06<00:09, 539.29it/s] 90%|█████████ | 47971/53136 [01:06<00:08, 635.82it/s] 90%|█████████ | 48045/53136 [01:06<00:07, 657.97it/s] 91%|█████████ | 48117/53136 [01:07<00:07, 653.07it/s] 91%|█████████ | 48187/53136 [01:07<00:07, 638.03it/s] 91%|█████████ | 48270/53136 [01:07<00:07, 688.77it/s] 91%|█████████ | 48342/53136 [01:07<00:07, 647.74it/s] 91%|█████████ | 48420/53136 [01:07<00:06, 683.15it/s] 91%|█████████▏| 48495/53136 [01:07<00:06, 680.90it/s] 91%|█████████▏| 48572/53136 [01:07<00:06, 702.75it/s] 92%|█████████▏| 48657/53136 [01:07<00:06, 737.95it/s] 92%|█████████▏| 48732/53136 [01:07<00:06, 658.51it/s] 92%|█████████▏| 48800/53136 [01:08<00:08, 530.84it/s] 92%|█████████▏| 48876/53136 [01:08<00:07, 584.26it/s] 92%|█████████▏| 48956/53136 [01:08<00:06, 631.60it/s] 92%|█████████▏| 49050/53136 [01:08<00:05, 711.91it/s] 92%|█████████▏| 49126/53136 [01:08<00:05, 699.05it/s] 93%|█████████▎| 49199/53136 [01:08<00:05, 701.54it/s] 93%|█████████▎| 49272/53136 [01:08<00:05, 655.89it/s] 93%|█████████▎| 49340/53136 [01:08<00:06, 576.51it/s] 93%|█████████▎| 49420/53136 [01:09<00:05, 629.11it/s] 93%|█████████▎| 49496/53136 [01:09<00:05, 663.36it/s] 93%|█████████▎| 49567/53136 [01:09<00:05, 668.03it/s] 93%|█████████▎| 49642/53136 [01:09<00:05, 690.28it/s] 94%|█████████▎| 49724/53136 [01:09<00:04, 727.09it/s] 94%|█████████▎| 49798/53136 [01:09<00:04, 696.78it/s] 94%|█████████▍| 49889/53136 [01:09<00:04, 756.12it/s] 94%|█████████▍| 49966/53136 [01:09<00:04, 716.44it/s] 94%|█████████▍| 50056/53136 [01:09<00:04, 760.22it/s] 94%|█████████▍| 50148/53136 [01:09<00:03, 803.54it/s] 95%|█████████▍| 50230/53136 [01:10<00:03, 772.28it/s] 95%|█████████▍| 50309/53136 [01:10<00:03, 755.45it/s] 95%|█████████▍| 50397/53136 [01:10<00:03, 787.53it/s] 95%|█████████▍| 50477/53136 [01:10<00:03, 768.86it/s] 95%|█████████▌| 50555/53136 [01:10<00:04, 602.77it/s] 95%|█████████▌| 50633/53136 [01:10<00:03, 642.78it/s] 95%|█████████▌| 50711/53136 [01:10<00:03, 671.80it/s] 96%|█████████▌| 50814/53136 [01:10<00:03, 766.05it/s] 96%|█████████▌| 50895/53136 [01:11<00:03, 663.15it/s] 96%|█████████▌| 50971/53136 [01:11<00:03, 686.50it/s] 96%|█████████▌| 51044/53136 [01:11<00:03, 658.79it/s] 96%|█████████▌| 51113/53136 [01:11<00:03, 647.58it/s] 96%|█████████▋| 51203/53136 [01:11<00:02, 711.88it/s] 97%|█████████▋| 51277/53136 [01:11<00:02, 719.38it/s] 97%|█████████▋| 51351/53136 [01:11<00:02, 719.08it/s] 97%|█████████▋| 51424/53136 [01:11<00:02, 590.29it/s] 97%|█████████▋| 51500/53136 [01:11<00:02, 627.53it/s] 97%|█████████▋| 51567/53136 [01:12<00:02, 581.42it/s] 97%|█████████▋| 51641/53136 [01:12<00:03, 497.79it/s] 97%|█████████▋| 51738/53136 [01:12<00:02, 603.34it/s] 98%|█████████▊| 51816/53136 [01:12<00:02, 641.80it/s] 98%|█████████▊| 51897/53136 [01:12<00:01, 682.18it/s] 98%|█████████▊| 51978/53136 [01:12<00:01, 716.16it/s] 98%|█████████▊| 52056/53136 [01:12<00:01, 706.20it/s] 98%|█████████▊| 52130/53136 [01:12<00:01, 658.11it/s] 98%|█████████▊| 52198/53136 [01:13<00:01, 626.59it/s] 98%|█████████▊| 52263/53136 [01:13<00:01, 473.81it/s] 99%|█████████▊| 52346/53136 [01:13<00:01, 551.61it/s] 99%|█████████▊| 52430/53136 [01:13<00:01, 613.55it/s] 99%|█████████▉| 52498/53136 [01:13<00:01, 613.04it/s] 99%|█████████▉| 52564/53136 [01:13<00:00, 623.72it/s] 99%|█████████▉| 52644/53136 [01:13<00:00, 669.80it/s] 99%|█████████▉| 52714/53136 [01:13<00:00, 609.46it/s] 99%|█████████▉| 52789/53136 [01:14<00:00, 645.08it/s]100%|█████████▉| 52889/53136 [01:14<00:00, 735.35it/s]100%|█████████▉| 52965/53136 [01:14<00:00, 702.34it/s]100%|█████████▉| 53038/53136 [01:14<00:00, 632.96it/s]100%|█████████▉| 53104/53136 [01:14<00:00, 586.22it/s]100%|██████████| 53136/53136 [01:14<00:00, 711.93it/s]

gathering stats for n=1
  0%|          | 0/53136 [00:00<?, ?it/s]  1%|          | 269/53136 [00:00<00:19, 2689.87it/s]  1%|          | 566/53136 [00:00<00:18, 2853.86it/s]  2%|▏         | 852/53136 [00:00<00:19, 2668.05it/s]  2%|▏         | 1121/53136 [00:00<00:19, 2645.97it/s]  3%|▎         | 1408/53136 [00:00<00:18, 2722.59it/s]  3%|▎         | 1738/53136 [00:00<00:17, 2913.91it/s]  4%|▍         | 2031/53136 [00:00<00:17, 2850.27it/s]  4%|▍         | 2353/53136 [00:00<00:17, 2957.96it/s]  5%|▍         | 2650/53136 [00:00<00:17, 2851.80it/s]  6%|▌         | 2937/53136 [00:01<00:17, 2804.78it/s]  6%|▌         | 3219/53136 [00:01<00:19, 2561.52it/s]  7%|▋         | 3480/53136 [00:01<00:19, 2567.51it/s]  7%|▋         | 3740/53136 [00:01<00:19, 2495.54it/s]  8%|▊         | 4020/53136 [00:01<00:19, 2573.18it/s]  8%|▊         | 4301/53136 [00:01<00:18, 2639.71it/s]  9%|▊         | 4567/53136 [00:01<00:19, 2554.21it/s]  9%|▉         | 4824/53136 [00:01<00:19, 2493.53it/s] 10%|▉         | 5077/53136 [00:01<00:19, 2435.17it/s] 10%|█         | 5322/53136 [00:02<00:20, 2387.57it/s] 11%|█         | 5614/53136 [00:02<00:18, 2537.83it/s] 11%|█         | 5875/53136 [00:02<00:18, 2555.31it/s] 12%|█▏        | 6149/53136 [00:02<00:18, 2609.00it/s] 12%|█▏        | 6415/53136 [00:02<00:17, 2604.48it/s] 13%|█▎        | 6743/53136 [00:02<00:16, 2802.86it/s] 13%|█▎        | 7025/53136 [00:02<00:17, 2589.63it/s] 14%|█▍        | 7314/53136 [00:02<00:17, 2661.23it/s] 14%|█▍        | 7584/53136 [00:02<00:18, 2421.88it/s] 15%|█▍        | 7880/53136 [00:03<00:17, 2565.95it/s] 15%|█▌        | 8143/53136 [00:03<00:17, 2582.50it/s] 16%|█▌        | 8406/53136 [00:03<00:17, 2535.18it/s] 16%|█▋        | 8663/53136 [00:03<00:17, 2508.82it/s] 17%|█▋        | 8927/53136 [00:03<00:17, 2540.73it/s] 17%|█▋        | 9192/53136 [00:03<00:17, 2567.05it/s] 18%|█▊        | 9458/53136 [00:03<00:16, 2592.96it/s] 18%|█▊        | 9736/53136 [00:03<00:16, 2645.47it/s] 19%|█▉        | 10002/53136 [00:03<00:18, 2362.00it/s] 19%|█▉        | 10288/53136 [00:03<00:17, 2497.35it/s] 20%|█▉        | 10544/53136 [00:04<00:17, 2378.77it/s] 20%|██        | 10787/53136 [00:04<00:17, 2388.88it/s] 21%|██        | 11042/53136 [00:04<00:17, 2422.19it/s] 21%|██        | 11287/53136 [00:04<00:17, 2393.07it/s] 22%|██▏       | 11541/53136 [00:04<00:17, 2433.89it/s] 22%|██▏       | 11786/53136 [00:04<00:17, 2424.30it/s] 23%|██▎       | 12054/53136 [00:04<00:16, 2493.93it/s] 23%|██▎       | 12305/53136 [00:04<00:16, 2472.40it/s] 24%|██▎       | 12559/53136 [00:04<00:16, 2471.22it/s] 24%|██▍       | 12807/53136 [00:05<00:17, 2333.88it/s] 25%|██▍       | 13115/53136 [00:05<00:15, 2545.14it/s] 25%|██▌       | 13372/53136 [00:05<00:15, 2548.97it/s] 26%|██▌       | 13635/53136 [00:05<00:15, 2570.69it/s] 26%|██▌       | 13894/53136 [00:05<00:15, 2488.39it/s] 27%|██▋       | 14147/53136 [00:05<00:15, 2498.50it/s] 27%|██▋       | 14398/53136 [00:05<00:15, 2445.03it/s] 28%|██▊       | 14711/53136 [00:05<00:14, 2642.03it/s] 28%|██▊       | 14977/53136 [00:05<00:14, 2591.21it/s] 29%|██▊       | 15238/53136 [00:05<00:14, 2543.62it/s] 29%|██▉       | 15494/53136 [00:06<00:15, 2395.25it/s] 30%|██▉       | 15736/53136 [00:06<00:15, 2383.75it/s] 30%|███       | 15976/53136 [00:06<00:15, 2362.96it/s] 31%|███       | 16214/53136 [00:06<00:15, 2355.56it/s] 31%|███       | 16502/53136 [00:06<00:14, 2505.94it/s] 32%|███▏      | 16798/53136 [00:06<00:13, 2638.36it/s] 32%|███▏      | 17063/53136 [00:06<00:13, 2625.37it/s] 33%|███▎      | 17327/53136 [00:06<00:13, 2599.08it/s] 33%|███▎      | 17588/53136 [00:06<00:13, 2588.40it/s] 34%|███▎      | 17848/53136 [00:06<00:13, 2540.81it/s] 34%|███▍      | 18103/53136 [00:07<00:13, 2539.34it/s] 35%|███▍      | 18358/53136 [00:07<00:14, 2423.29it/s] 35%|███▌      | 18602/53136 [00:07<00:14, 2340.66it/s] 35%|███▌      | 18862/53136 [00:07<00:14, 2413.13it/s] 36%|███▌      | 19154/53136 [00:07<00:13, 2558.01it/s] 37%|███▋      | 19412/53136 [00:07<00:13, 2541.88it/s] 37%|███▋      | 19668/53136 [00:07<00:13, 2492.92it/s] 37%|███▋      | 19919/53136 [00:07<00:13, 2437.81it/s] 38%|███▊      | 20183/53136 [00:07<00:13, 2413.88it/s] 38%|███▊      | 20425/53136 [00:08<00:13, 2354.87it/s] 39%|███▉      | 20661/53136 [00:08<00:13, 2355.43it/s] 39%|███▉      | 20923/53136 [00:08<00:13, 2431.47it/s] 40%|███▉      | 21167/53136 [00:08<00:13, 2378.05it/s] 40%|████      | 21431/53136 [00:08<00:12, 2453.38it/s] 41%|████      | 21677/53136 [00:08<00:13, 2387.77it/s] 41%|████      | 21917/53136 [00:08<00:13, 2331.27it/s] 42%|████▏     | 22151/53136 [00:08<00:13, 2305.49it/s] 42%|████▏     | 22425/53136 [00:08<00:12, 2426.62it/s] 43%|████▎     | 22697/53136 [00:09<00:12, 2505.14it/s] 43%|████▎     | 22953/53136 [00:09<00:11, 2520.87it/s] 44%|████▎     | 23206/53136 [00:09<00:12, 2460.83it/s] 44%|████▍     | 23453/53136 [00:09<00:12, 2459.89it/s] 45%|████▍     | 23700/53136 [00:09<00:12, 2341.92it/s] 45%|████▌     | 23954/53136 [00:09<00:12, 2397.03it/s] 46%|████▌     | 24219/53136 [00:09<00:11, 2469.10it/s] 46%|████▌     | 24492/53136 [00:09<00:11, 2543.24it/s] 47%|████▋     | 24772/53136 [00:09<00:10, 2618.41it/s] 47%|████▋     | 25040/53136 [00:09<00:10, 2635.80it/s] 48%|████▊     | 25328/53136 [00:10<00:10, 2708.19it/s] 48%|████▊     | 25600/53136 [00:10<00:10, 2516.41it/s] 49%|████▊     | 25878/53136 [00:10<00:10, 2585.82it/s] 49%|████▉     | 26140/53136 [00:10<00:10, 2462.32it/s] 50%|████▉     | 26389/53136 [00:10<00:11, 2416.68it/s] 50%|█████     | 26646/53136 [00:10<00:10, 2459.27it/s] 51%|█████     | 26898/53136 [00:10<00:10, 2474.77it/s] 51%|█████     | 27147/53136 [00:10<00:10, 2454.70it/s] 52%|█████▏    | 27394/53136 [00:10<00:10, 2443.55it/s] 52%|█████▏    | 27639/53136 [00:10<00:10, 2406.35it/s] 52%|█████▏    | 27893/53136 [00:11<00:10, 2444.82it/s] 53%|█████▎    | 28138/53136 [00:11<00:10, 2384.82it/s] 53%|█████▎    | 28402/53136 [00:11<00:10, 2454.19it/s] 54%|█████▍    | 28711/53136 [00:11<00:09, 2632.51it/s] 55%|█████▍    | 28976/53136 [00:11<00:09, 2637.14it/s] 55%|█████▌    | 29241/53136 [00:11<00:09, 2544.13it/s] 56%|█████▌    | 29497/53136 [00:11<00:09, 2533.09it/s] 56%|█████▌    | 29751/53136 [00:11<00:09, 2400.58it/s] 56%|█████▋    | 30011/53136 [00:11<00:09, 2455.06it/s] 57%|█████▋    | 30258/53136 [00:12<00:09, 2350.20it/s] 57%|█████▋    | 30497/53136 [00:12<00:09, 2361.09it/s] 58%|█████▊    | 30749/53136 [00:12<00:09, 2405.62it/s] 58%|█████▊    | 30991/53136 [00:12<00:09, 2367.18it/s] 59%|█████▉    | 31254/53136 [00:12<00:08, 2442.24it/s] 59%|█████▉    | 31543/53136 [00:12<00:08, 2569.30it/s] 60%|█████▉    | 31833/53136 [00:12<00:07, 2664.74it/s] 60%|██████    | 32101/53136 [00:12<00:08, 2537.22it/s] 61%|██████    | 32358/53136 [00:12<00:08, 2542.70it/s] 61%|██████▏   | 32627/53136 [00:12<00:07, 2585.31it/s] 62%|██████▏   | 32903/53136 [00:13<00:07, 2629.97it/s] 62%|██████▏   | 33167/53136 [00:13<00:07, 2626.83it/s] 63%|██████▎   | 33431/53136 [00:13<00:07, 2524.98it/s] 63%|██████▎   | 33685/53136 [00:13<00:07, 2495.53it/s] 64%|██████▍   | 33936/53136 [00:13<00:07, 2413.62it/s] 64%|██████▍   | 34222/53136 [00:13<00:07, 2539.64it/s] 65%|██████▍   | 34478/53136 [00:13<00:07, 2467.69it/s] 65%|██████▌   | 34726/53136 [00:13<00:07, 2466.46it/s] 66%|██████▌   | 34974/53136 [00:13<00:07, 2355.56it/s] 66%|██████▋   | 35211/53136 [00:14<00:08, 2187.32it/s] 67%|██████▋   | 35448/53136 [00:14<00:07, 2236.10it/s] 67%|██████▋   | 35721/53136 [00:14<00:07, 2373.81it/s] 68%|██████▊   | 35962/53136 [00:14<00:07, 2331.13it/s] 68%|██████▊   | 36197/53136 [00:14<00:07, 2321.82it/s] 69%|██████▊   | 36431/53136 [00:14<00:07, 2304.46it/s] 69%|██████▉   | 36663/53136 [00:14<00:07, 2281.54it/s] 69%|██████▉   | 36892/53136 [00:14<00:07, 2238.90it/s] 70%|██████▉   | 37122/53136 [00:14<00:07, 2252.39it/s] 70%|███████   | 37361/53136 [00:14<00:06, 2291.15it/s] 71%|███████   | 37624/53136 [00:15<00:06, 2385.04it/s] 71%|███████▏  | 37890/53136 [00:15<00:06, 2464.59it/s] 72%|███████▏  | 38149/53136 [00:15<00:05, 2501.26it/s] 72%|███████▏  | 38400/53136 [00:15<00:06, 2318.73it/s] 73%|███████▎  | 38635/53136 [00:15<00:06, 2310.96it/s] 73%|███████▎  | 38875/53136 [00:15<00:06, 2335.54it/s] 74%|███████▎  | 39142/53136 [00:15<00:05, 2432.48it/s] 74%|███████▍  | 39413/53136 [00:15<00:05, 2510.44it/s] 75%|███████▍  | 39666/53136 [00:15<00:05, 2354.39it/s] 75%|███████▌  | 39944/53136 [00:16<00:05, 2468.75it/s] 76%|███████▌  | 40219/53136 [00:16<00:05, 2542.14it/s] 76%|███████▌  | 40476/53136 [00:16<00:05, 2521.69it/s] 77%|███████▋  | 40739/53136 [00:16<00:04, 2548.79it/s] 77%|███████▋  | 41006/53136 [00:16<00:04, 2581.77it/s] 78%|███████▊  | 41265/53136 [00:16<00:04, 2472.75it/s] 78%|███████▊  | 41538/53136 [00:16<00:04, 2545.18it/s] 79%|███████▊  | 41794/53136 [00:16<00:04, 2531.67it/s] 79%|███████▉  | 42049/53136 [00:16<00:04, 2467.01it/s] 80%|███████▉  | 42304/53136 [00:16<00:04, 2490.50it/s] 80%|████████  | 42606/53136 [00:17<00:03, 2639.22it/s] 81%|████████  | 42889/53136 [00:17<00:03, 2694.24it/s] 81%|████████  | 43160/53136 [00:17<00:03, 2539.60it/s] 82%|████████▏ | 43417/53136 [00:17<00:03, 2511.83it/s] 82%|████████▏ | 43670/53136 [00:17<00:03, 2426.86it/s] 83%|████████▎ | 43915/53136 [00:17<00:03, 2409.46it/s] 83%|████████▎ | 44157/53136 [00:17<00:03, 2299.14it/s] 84%|████████▎ | 44402/53136 [00:17<00:03, 2333.07it/s] 84%|████████▍ | 44687/53136 [00:17<00:03, 2475.82it/s] 85%|████████▍ | 44973/53136 [00:18<00:03, 2579.34it/s] 85%|████████▌ | 45262/53136 [00:18<00:02, 2667.01it/s] 86%|████████▌ | 45530/53136 [00:18<00:02, 2569.44it/s] 86%|████████▌ | 45789/53136 [00:18<00:03, 2438.36it/s] 87%|████████▋ | 46057/53136 [00:18<00:02, 2505.66it/s] 87%|████████▋ | 46310/53136 [00:18<00:02, 2509.81it/s] 88%|████████▊ | 46592/53136 [00:18<00:02, 2589.28it/s] 88%|████████▊ | 46853/53136 [00:18<00:02, 2555.23it/s] 89%|████████▊ | 47110/53136 [00:18<00:02, 2559.16it/s] 89%|████████▉ | 47367/53136 [00:18<00:02, 2535.54it/s] 90%|████████▉ | 47621/53136 [00:19<00:02, 2523.28it/s] 90%|█████████ | 47874/53136 [00:19<00:02, 2350.84it/s] 91%|█████████ | 48128/53136 [00:19<00:02, 2376.05it/s] 91%|█████████ | 48376/53136 [00:19<00:01, 2404.25it/s] 92%|█████████▏| 48635/53136 [00:19<00:01, 2456.68it/s] 92%|█████████▏| 48882/53136 [00:19<00:01, 2250.70it/s] 93%|█████████▎| 49163/53136 [00:19<00:01, 2403.36it/s] 93%|█████████▎| 49408/53136 [00:19<00:01, 2289.05it/s] 93%|█████████▎| 49649/53136 [00:19<00:01, 2318.62it/s] 94%|█████████▍| 49915/53136 [00:20<00:01, 2409.03it/s] 94%|█████████▍| 50184/53136 [00:20<00:01, 2478.95it/s] 95%|█████████▍| 50463/53136 [00:20<00:01, 2542.57it/s] 95%|█████████▌| 50719/53136 [00:20<00:00, 2426.56it/s] 96%|█████████▌| 50980/53136 [00:20<00:00, 2474.35it/s] 96%|█████████▋| 51229/53136 [00:20<00:00, 2448.49it/s] 97%|█████████▋| 51475/53136 [00:20<00:00, 2360.92it/s] 97%|█████████▋| 51713/53136 [00:20<00:00, 2225.83it/s] 98%|█████████▊| 51989/53136 [00:20<00:00, 2371.06it/s] 98%|█████████▊| 52229/53136 [00:21<00:00, 2140.27it/s] 99%|█████████▉| 52475/53136 [00:21<00:00, 2225.12it/s] 99%|█████████▉| 52717/53136 [00:21<00:00, 2278.80it/s]100%|█████████▉| 52970/53136 [00:21<00:00, 2324.34it/s]100%|██████████| 53136/53136 [00:21<00:00, 2475.48it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 58.96it/s]2022-03-17 09:25:19 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(35920, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=35920, bias=False)
  )
)
2022-03-17 09:25:19 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-17 09:25:19 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-17 09:25:19 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-17 09:25:19 | INFO | fairseq_cli.train | num. shared model params: 37,305,344 (num. trained: 37,305,344)
2022-03-17 09:25:19 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-17 09:25:19 | INFO | fairseq.data.data_utils | loaded 2,558 examples from: data-bin/ru/valid
2022-03-17 09:25:19 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-17 09:25:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-17 09:25:19 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-17 09:25:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-17 09:25:19 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-17 09:25:19 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-17 09:25:19 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_last.pt
2022-03-17 09:25:19 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_last.pt
2022-03-17 09:25:19 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-17 09:25:19 | INFO | fairseq.data.data_utils | loaded 53,136 examples from: data-bin/ru/train
2022-03-17 09:25:19 | INFO | fairseq.trainer | begin training epoch 1
2022-03-17 09:25:19 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-17 09:25:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-17 09:25:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 09:25:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 09:25:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-17 09:27:59 | INFO | train_inner | epoch 001:    104 / 407 loss=14.839, ppl=29302.7, wps=43691.2, ups=0.67, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.19, loss_scale=8, train_wall=155, gb_free=21, wall=160
2022-03-17 09:30:29 | INFO | train_inner | epoch 001:    204 / 407 loss=13.374, ppl=10617.2, wps=43690, ups=0.67, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=0.646, loss_scale=8, train_wall=145, gb_free=21, wall=310
2022-03-17 09:32:59 | INFO | train_inner | epoch 001:    304 / 407 loss=12.502, ppl=5799.85, wps=43616.1, ups=0.67, wpb=65525.8, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.416, loss_scale=8, train_wall=146, gb_free=21, wall=461
2022-03-17 09:35:29 | INFO | train_inner | epoch 001:    404 / 407 loss=12.066, ppl=4288.95, wps=43623.6, ups=0.67, wpb=65534.2, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.399, loss_scale=8, train_wall=146, gb_free=21, wall=611
2022-03-17 09:35:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 09:35:53 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.885 | ppl 3782.7 | wps 70404.7 | wpb 2047.5 | bsz 4 | num_updates 403
2022-03-17 09:35:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 403 updates
2022-03-17 09:35:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 09:35:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 09:35:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 1 @ 403 updates, score 11.885) (writing took 1.7939553344622254 seconds)
2022-03-17 09:35:54 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-17 09:35:54 | INFO | train | epoch 001 | loss 13.187 | ppl 9323.9 | wps 42198.2 | ups 0.64 | wpb 65492.3 | bsz 127.9 | num_updates 403 | lr 5.04649e-05 | gnorm 0.909 | loss_scale 8 | train_wall 596 | gb_free 21 | wall 636
KL Stats: Epoch 1 Divergences: Uniform: 0.6957314152636475 Unigram: 0.8219066029436
2022-03-17 09:35:54 | INFO | fairseq.trainer | begin training epoch 2
2022-03-17 09:35:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 09:38:20 | INFO | train_inner | epoch 002:     97 / 407 loss=11.874, ppl=3752.63, wps=38287.2, ups=0.59, wpb=65361.9, bsz=127.7, num_updates=500, lr=6.25875e-05, gnorm=0.401, loss_scale=8, train_wall=145, gb_free=21, wall=782
2022-03-17 09:40:50 | INFO | train_inner | epoch 002:    197 / 407 loss=11.647, ppl=3206.26, wps=43615.9, ups=0.67, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.459, loss_scale=16, train_wall=146, gb_free=21, wall=932
2022-03-17 09:43:21 | INFO | train_inner | epoch 002:    297 / 407 loss=11.351, ppl=2611.41, wps=43569.8, ups=0.66, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.482, loss_scale=16, train_wall=146, gb_free=21, wall=1082
2022-03-17 09:45:51 | INFO | train_inner | epoch 002:    397 / 407 loss=11.009, ppl=2060.33, wps=43602.1, ups=0.67, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.513, loss_scale=16, train_wall=146, gb_free=21, wall=1233
2022-03-17 09:46:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 09:46:25 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.594 | ppl 1545.87 | wps 70495.3 | wpb 2047.5 | bsz 4 | num_updates 810 | best_loss 10.594
2022-03-17 09:46:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 810 updates
2022-03-17 09:46:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 09:46:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 09:46:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 2 @ 810 updates, score 10.594) (writing took 1.755839180201292 seconds)
2022-03-17 09:46:27 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-17 09:46:27 | INFO | train | epoch 002 | loss 11.45 | ppl 2798.41 | wps 42167.4 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 810 | lr 0.00010133 | gnorm 0.468 | loss_scale 16 | train_wall 593 | gb_free 21 | wall 1268
KL Stats: Epoch 2 Divergences: Uniform: 1.3831596970710802 Unigram: 0.5720653810197134
2022-03-17 09:46:27 | INFO | fairseq.trainer | begin training epoch 3
2022-03-17 09:46:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 09:48:42 | INFO | train_inner | epoch 003:     90 / 407 loss=10.659, ppl=1617.06, wps=38283.7, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=900, lr=0.000112578, gnorm=0.507, loss_scale=16, train_wall=145, gb_free=21, wall=1403
2022-03-17 09:51:12 | INFO | train_inner | epoch 003:    190 / 407 loss=10.383, ppl=1335.16, wps=43602, ups=0.67, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.537, loss_scale=16, train_wall=146, gb_free=21, wall=1554
2022-03-17 09:53:43 | INFO | train_inner | epoch 003:    290 / 407 loss=10.157, ppl=1141.4, wps=43603.8, ups=0.67, wpb=65525.8, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.602, loss_scale=32, train_wall=146, gb_free=21, wall=1704
2022-03-17 09:56:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 09:56:14 | INFO | train_inner | epoch 003:    391 / 407 loss=9.954, ppl=991.87, wps=43205.5, ups=0.66, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.618, loss_scale=16, train_wall=147, gb_free=21, wall=1856
2022-03-17 09:56:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 09:56:57 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.615 | ppl 784.07 | wps 70439.3 | wpb 2047.5 | bsz 4 | num_updates 1216 | best_loss 9.615
2022-03-17 09:56:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1216 updates
2022-03-17 09:56:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 09:56:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 09:56:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 3 @ 1216 updates, score 9.615) (writing took 1.8162452643737197 seconds)
2022-03-17 09:56:59 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-17 09:56:59 | INFO | train | epoch 003 | loss 10.259 | ppl 1224.96 | wps 42063.4 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 1216 | lr 0.00015207 | gnorm 0.568 | loss_scale 16 | train_wall 593 | gb_free 21 | wall 1900
KL Stats: Epoch 3 Divergences: Uniform: 1.9487529417006215 Unigram: 1.5643422893835026
2022-03-17 09:56:59 | INFO | fairseq.trainer | begin training epoch 4
2022-03-17 09:56:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 09:59:05 | INFO | train_inner | epoch 004:     84 / 407 loss=9.747, ppl=859.01, wps=38277.6, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=1300, lr=0.000162568, gnorm=0.645, loss_scale=16, train_wall=145, gb_free=21, wall=2026
2022-03-17 10:01:35 | INFO | train_inner | epoch 004:    184 / 407 loss=9.557, ppl=753.31, wps=43629.3, ups=0.67, wpb=65525.8, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.698, loss_scale=16, train_wall=146, gb_free=21, wall=2177
2022-03-17 10:04:05 | INFO | train_inner | epoch 004:    284 / 407 loss=9.36, ppl=657.23, wps=43602.8, ups=0.67, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.716, loss_scale=16, train_wall=146, gb_free=21, wall=2327
2022-03-17 10:06:36 | INFO | train_inner | epoch 004:    384 / 407 loss=9.187, ppl=582.78, wps=43597, ups=0.67, wpb=65534.2, bsz=128, num_updates=1600, lr=0.00020006, gnorm=0.692, loss_scale=16, train_wall=146, gb_free=21, wall=2477
2022-03-17 10:07:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 10:07:29 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.851 | ppl 461.79 | wps 70513 | wpb 2047.5 | bsz 4 | num_updates 1623 | best_loss 8.851
2022-03-17 10:07:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1623 updates
2022-03-17 10:07:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 10:07:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 10:07:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 4 @ 1623 updates, score 8.851) (writing took 1.8067023484036326 seconds)
2022-03-17 10:07:31 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-17 10:07:31 | INFO | train | epoch 004 | loss 9.426 | ppl 688.06 | wps 42171.9 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 1623 | lr 0.000202934 | gnorm 0.692 | loss_scale 16 | train_wall 593 | gb_free 21 | wall 2532
KL Stats: Epoch 4 Divergences: Uniform: 2.474336863628183 Unigram: 2.14424895706878
2022-03-17 10:07:31 | INFO | fairseq.trainer | begin training epoch 5
2022-03-17 10:07:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 10:09:27 | INFO | train_inner | epoch 005:     77 / 407 loss=9.011, ppl=515.98, wps=38291, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=1700, lr=0.000212558, gnorm=0.754, loss_scale=16, train_wall=145, gb_free=21, wall=2648
2022-03-17 10:11:57 | INFO | train_inner | epoch 005:    177 / 407 loss=8.854, ppl=462.72, wps=43621.1, ups=0.67, wpb=65534.2, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.74, loss_scale=32, train_wall=146, gb_free=21, wall=2798
2022-03-17 10:14:27 | INFO | train_inner | epoch 005:    277 / 407 loss=8.71, ppl=418.89, wps=43589.6, ups=0.67, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.725, loss_scale=32, train_wall=146, gb_free=21, wall=2948
2022-03-17 10:16:57 | INFO | train_inner | epoch 005:    377 / 407 loss=8.566, ppl=379.05, wps=43598, ups=0.67, wpb=65525.8, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.731, loss_scale=32, train_wall=146, gb_free=21, wall=3099
2022-03-17 10:17:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 10:18:01 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.26 | ppl 306.59 | wps 70320.4 | wpb 2047.5 | bsz 4 | num_updates 2030 | best_loss 8.26
2022-03-17 10:18:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 2030 updates
2022-03-17 10:18:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 10:18:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 10:18:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 5 @ 2030 updates, score 8.26) (writing took 1.8430053237825632 seconds)
2022-03-17 10:18:03 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-17 10:18:03 | INFO | train | epoch 005 | loss 8.746 | ppl 429.26 | wps 42160 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 2030 | lr 0.000253799 | gnorm 0.739 | loss_scale 32 | train_wall 593 | gb_free 21 | wall 3164
KL Stats: Epoch 5 Divergences: Uniform: 2.9169326896224397 Unigram: 2.5409727322285542
2022-03-17 10:18:03 | INFO | fairseq.trainer | begin training epoch 6
2022-03-17 10:18:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 10:19:48 | INFO | train_inner | epoch 006:     70 / 407 loss=8.418, ppl=341.97, wps=38236.1, ups=0.58, wpb=65370.3, bsz=127.7, num_updates=2100, lr=0.000262548, gnorm=0.732, loss_scale=32, train_wall=145, gb_free=21, wall=3270
2022-03-17 10:22:19 | INFO | train_inner | epoch 006:    170 / 407 loss=8.3, ppl=315.15, wps=43585.9, ups=0.67, wpb=65525.8, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.715, loss_scale=32, train_wall=146, gb_free=21, wall=3420
2022-03-17 10:22:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 10:24:51 | INFO | train_inner | epoch 006:    271 / 407 loss=8.195, ppl=292.96, wps=43152.5, ups=0.66, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.709, loss_scale=16, train_wall=147, gb_free=21, wall=3572
2022-03-17 10:27:21 | INFO | train_inner | epoch 006:    371 / 407 loss=8.1, ppl=274.35, wps=43610.9, ups=0.67, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.7, loss_scale=16, train_wall=146, gb_free=21, wall=3722
2022-03-17 10:28:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 10:28:34 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.809 | ppl 224.25 | wps 70514.7 | wpb 2047.5 | bsz 4 | num_updates 2436 | best_loss 7.809
2022-03-17 10:28:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2436 updates
2022-03-17 10:28:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 10:28:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 10:28:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 6 @ 2436 updates, score 7.809) (writing took 1.9902687836438417 seconds)
2022-03-17 10:28:36 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-17 10:28:36 | INFO | train | epoch 006 | loss 8.218 | ppl 297.75 | wps 42036.5 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 2436 | lr 0.000304539 | gnorm 0.713 | loss_scale 16 | train_wall 593 | gb_free 21 | wall 3797
KL Stats: Epoch 6 Divergences: Uniform: 3.2838213415060395 Unigram: 2.8488090300660303
2022-03-17 10:28:36 | INFO | fairseq.trainer | begin training epoch 7
2022-03-17 10:28:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 10:30:12 | INFO | train_inner | epoch 007:     64 / 407 loss=7.975, ppl=251.59, wps=38245, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=2500, lr=0.000312538, gnorm=0.705, loss_scale=16, train_wall=145, gb_free=21, wall=3893
2022-03-17 10:32:42 | INFO | train_inner | epoch 007:    164 / 407 loss=7.89, ppl=237.15, wps=43632.6, ups=0.67, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.67, loss_scale=16, train_wall=146, gb_free=21, wall=4043
2022-03-17 10:35:12 | INFO | train_inner | epoch 007:    264 / 407 loss=7.816, ppl=225.31, wps=43596.9, ups=0.67, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.658, loss_scale=16, train_wall=146, gb_free=21, wall=4194
2022-03-17 10:37:43 | INFO | train_inner | epoch 007:    364 / 407 loss=7.747, ppl=214.83, wps=43626.1, ups=0.67, wpb=65534.2, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.661, loss_scale=32, train_wall=146, gb_free=21, wall=4344
2022-03-17 10:38:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 10:38:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 10:39:06 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.503 | ppl 181.43 | wps 70511.1 | wpb 2047.5 | bsz 4 | num_updates 2842 | best_loss 7.503
2022-03-17 10:39:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2842 updates
2022-03-17 10:39:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 10:39:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 10:39:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 7 @ 2842 updates, score 7.503) (writing took 1.8247310081496835 seconds)
2022-03-17 10:39:08 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-17 10:39:08 | INFO | train | epoch 007 | loss 7.826 | ppl 226.84 | wps 42074.5 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 2842 | lr 0.000355279 | gnorm 0.667 | loss_scale 16 | train_wall 593 | gb_free 21 | wall 4429
KL Stats: Epoch 7 Divergences: Uniform: 3.547712795944172 Unigram: 3.0825271517741566
2022-03-17 10:39:08 | INFO | fairseq.trainer | begin training epoch 8
2022-03-17 10:39:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 10:40:35 | INFO | train_inner | epoch 008:     58 / 407 loss=7.642, ppl=199.78, wps=37952, ups=0.58, wpb=65349.8, bsz=127.6, num_updates=2900, lr=0.000362528, gnorm=0.652, loss_scale=16, train_wall=147, gb_free=21, wall=4516
2022-03-17 10:43:05 | INFO | train_inner | epoch 008:    158 / 407 loss=7.589, ppl=192.55, wps=43594.1, ups=0.67, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.638, loss_scale=16, train_wall=146, gb_free=21, wall=4666
2022-03-17 10:45:35 | INFO | train_inner | epoch 008:    258 / 407 loss=7.523, ppl=183.95, wps=43572.4, ups=0.66, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.611, loss_scale=16, train_wall=146, gb_free=21, wall=4817
2022-03-17 10:48:06 | INFO | train_inner | epoch 008:    358 / 407 loss=7.456, ppl=175.62, wps=43608.6, ups=0.67, wpb=65536, bsz=128, num_updates=3200, lr=0.00040002, gnorm=0.601, loss_scale=16, train_wall=146, gb_free=21, wall=4967
2022-03-17 10:49:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 10:49:38 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.229 | ppl 149.98 | wps 70547 | wpb 2047.5 | bsz 4 | num_updates 3249 | best_loss 7.229
2022-03-17 10:49:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3249 updates
2022-03-17 10:49:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 10:49:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 10:49:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 8 @ 3249 updates, score 7.229) (writing took 1.8405833207070827 seconds)
2022-03-17 10:49:40 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-17 10:49:40 | INFO | train | epoch 008 | loss 7.52 | ppl 183.6 | wps 42160.2 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 3249 | lr 0.000406144 | gnorm 0.621 | loss_scale 16 | train_wall 593 | gb_free 21 | wall 5061
KL Stats: Epoch 8 Divergences: Uniform: 3.7300450060907044 Unigram: 3.24591679830361
2022-03-17 10:49:40 | INFO | fairseq.trainer | begin training epoch 9
2022-03-17 10:49:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 10:50:56 | INFO | train_inner | epoch 009:     51 / 407 loss=7.38, ppl=166.61, wps=38287.8, ups=0.59, wpb=65361.9, bsz=127.7, num_updates=3300, lr=0.000412518, gnorm=0.603, loss_scale=16, train_wall=145, gb_free=21, wall=5138
2022-03-17 10:53:27 | INFO | train_inner | epoch 009:    151 / 407 loss=7.303, ppl=157.97, wps=43613.4, ups=0.67, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.594, loss_scale=32, train_wall=146, gb_free=21, wall=5288
2022-03-17 10:54:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 10:55:59 | INFO | train_inner | epoch 009:    252 / 407 loss=7.261, ppl=153.36, wps=43152, ups=0.66, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.584, loss_scale=16, train_wall=147, gb_free=21, wall=5440
2022-03-17 10:58:29 | INFO | train_inner | epoch 009:    352 / 407 loss=7.213, ppl=148.36, wps=43630.9, ups=0.67, wpb=65534.2, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.569, loss_scale=16, train_wall=146, gb_free=21, wall=5590
2022-03-17 10:59:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 11:00:10 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.974 | ppl 125.68 | wps 70458.5 | wpb 2047.5 | bsz 4 | num_updates 3655 | best_loss 6.974
2022-03-17 11:00:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3655 updates
2022-03-17 11:00:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 11:00:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 11:00:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 9 @ 3655 updates, score 6.974) (writing took 1.8839066084474325 seconds)
2022-03-17 11:00:12 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-17 11:00:12 | INFO | train | epoch 009 | loss 7.257 | ppl 152.99 | wps 42067.8 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 3655 | lr 0.000456884 | gnorm 0.583 | loss_scale 16 | train_wall 593 | gb_free 21 | wall 5693
KL Stats: Epoch 9 Divergences: Uniform: 3.868524657225552 Unigram: 3.374561900928554
2022-03-17 11:00:12 | INFO | fairseq.trainer | begin training epoch 10
2022-03-17 11:00:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 11:01:19 | INFO | train_inner | epoch 010:     45 / 407 loss=7.121, ppl=139.2, wps=38297.1, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.578, loss_scale=16, train_wall=145, gb_free=21, wall=5761
2022-03-17 11:03:50 | INFO | train_inner | epoch 010:    145 / 407 loss=7.061, ppl=133.52, wps=43646.9, ups=0.67, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.56, loss_scale=16, train_wall=146, gb_free=21, wall=5911
2022-03-17 11:06:20 | INFO | train_inner | epoch 010:    245 / 407 loss=7.02, ppl=129.82, wps=43620.5, ups=0.67, wpb=65525.8, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.565, loss_scale=16, train_wall=146, gb_free=21, wall=6061
2022-03-17 11:08:50 | INFO | train_inner | epoch 010:    345 / 407 loss=6.984, ppl=126.63, wps=43650, ups=0.67, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.559, loss_scale=32, train_wall=146, gb_free=21, wall=6211
2022-03-17 11:10:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 11:10:42 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.76 | ppl 108.42 | wps 70762 | wpb 2047.5 | bsz 4 | num_updates 4062 | best_loss 6.76
2022-03-17 11:10:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 4062 updates
2022-03-17 11:10:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 11:10:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 11:10:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 10 @ 4062 updates, score 6.76) (writing took 1.8154312269762158 seconds)
2022-03-17 11:10:43 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-17 11:10:43 | INFO | train | epoch 010 | loss 7.017 | ppl 129.51 | wps 42207.9 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 4062 | lr 0.000496169 | gnorm 0.561 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 6325
KL Stats: Epoch 10 Divergences: Uniform: 3.9829520099141686 Unigram: 3.4842869203269906
2022-03-17 11:10:43 | INFO | fairseq.trainer | begin training epoch 11
2022-03-17 11:10:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 11:11:41 | INFO | train_inner | epoch 011:     38 / 407 loss=6.912, ppl=120.4, wps=38319.3, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.544, loss_scale=32, train_wall=145, gb_free=21, wall=6382
2022-03-17 11:12:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 11:14:12 | INFO | train_inner | epoch 011:    139 / 407 loss=6.837, ppl=114.36, wps=43231.8, ups=0.66, wpb=65534.2, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.532, loss_scale=16, train_wall=147, gb_free=21, wall=6534
2022-03-17 11:16:42 | INFO | train_inner | epoch 011:    239 / 407 loss=6.806, ppl=111.92, wps=43622.3, ups=0.67, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.525, loss_scale=16, train_wall=146, gb_free=21, wall=6684
2022-03-17 11:19:13 | INFO | train_inner | epoch 011:    339 / 407 loss=6.784, ppl=110.21, wps=43601.9, ups=0.67, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.512, loss_scale=16, train_wall=146, gb_free=21, wall=6834
2022-03-17 11:20:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 11:21:13 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.595 | ppl 96.69 | wps 70399.2 | wpb 2047.5 | bsz 4 | num_updates 4468 | best_loss 6.595
2022-03-17 11:21:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4468 updates
2022-03-17 11:21:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 11:21:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 11:21:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 11 @ 4468 updates, score 6.595) (writing took 2.541304737329483 seconds)
2022-03-17 11:21:16 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-17 11:21:16 | INFO | train | epoch 011 | loss 6.802 | ppl 111.61 | wps 42032.7 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 4468 | lr 0.00047309 | gnorm 0.523 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 6957
KL Stats: Epoch 11 Divergences: Uniform: 4.077908803323341 Unigram: 3.5827719572760492
2022-03-17 11:21:16 | INFO | fairseq.trainer | begin training epoch 12
2022-03-17 11:21:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 11:22:04 | INFO | train_inner | epoch 012:     32 / 407 loss=6.719, ppl=105.35, wps=38138.5, ups=0.58, wpb=65361.9, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.516, loss_scale=16, train_wall=145, gb_free=21, wall=7005
2022-03-17 11:24:34 | INFO | train_inner | epoch 012:    132 / 407 loss=6.651, ppl=100.48, wps=43658.5, ups=0.67, wpb=65525.8, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.507, loss_scale=16, train_wall=146, gb_free=21, wall=7156
2022-03-17 11:27:04 | INFO | train_inner | epoch 012:    232 / 407 loss=6.642, ppl=99.89, wps=43629.9, ups=0.67, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.498, loss_scale=32, train_wall=146, gb_free=21, wall=7306
2022-03-17 11:28:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 11:29:36 | INFO | train_inner | epoch 012:    333 / 407 loss=6.629, ppl=98.94, wps=43221.1, ups=0.66, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.495, loss_scale=16, train_wall=147, gb_free=21, wall=7457
2022-03-17 11:31:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 11:31:46 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.48 | ppl 89.23 | wps 70252.6 | wpb 2047.5 | bsz 4 | num_updates 4874 | best_loss 6.48
2022-03-17 11:31:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4874 updates
2022-03-17 11:31:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 11:31:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 11:31:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 12 @ 4874 updates, score 6.48) (writing took 1.8398831598460674 seconds)
2022-03-17 11:31:48 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-17 11:31:48 | INFO | train | epoch 012 | loss 6.636 | ppl 99.43 | wps 42090.5 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 4874 | lr 0.000452957 | gnorm 0.5 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 7589
KL Stats: Epoch 12 Divergences: Uniform: 4.153998761371565 Unigram: 3.6623572275148835
2022-03-17 11:31:48 | INFO | fairseq.trainer | begin training epoch 13
2022-03-17 11:31:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 11:32:27 | INFO | train_inner | epoch 013:     26 / 407 loss=6.58, ppl=95.69, wps=38259.3, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.49, loss_scale=16, train_wall=145, gb_free=21, wall=7628
2022-03-17 11:34:57 | INFO | train_inner | epoch 013:    126 / 407 loss=6.519, ppl=91.7, wps=43646.4, ups=0.67, wpb=65534.2, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.486, loss_scale=16, train_wall=146, gb_free=21, wall=7778
2022-03-17 11:37:27 | INFO | train_inner | epoch 013:    226 / 407 loss=6.512, ppl=91.28, wps=43587.5, ups=0.67, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.49, loss_scale=16, train_wall=146, gb_free=21, wall=7929
2022-03-17 11:39:58 | INFO | train_inner | epoch 013:    326 / 407 loss=6.502, ppl=90.64, wps=43605, ups=0.67, wpb=65525.8, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.483, loss_scale=16, train_wall=146, gb_free=21, wall=8079
2022-03-17 11:41:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 11:42:18 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.387 | ppl 83.68 | wps 70429.8 | wpb 2047.5 | bsz 4 | num_updates 5281 | best_loss 6.387
2022-03-17 11:42:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5281 updates
2022-03-17 11:42:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 11:42:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 11:42:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 13 @ 5281 updates, score 6.387) (writing took 1.80391994304955 seconds)
2022-03-17 11:42:20 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-17 11:42:20 | INFO | train | epoch 013 | loss 6.508 | ppl 91.02 | wps 42177.8 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 5281 | lr 0.000435153 | gnorm 0.484 | loss_scale 32 | train_wall 593 | gb_free 21 | wall 8221
KL Stats: Epoch 13 Divergences: Uniform: 4.210275865599816 Unigram: 3.7232861454665214
2022-03-17 11:42:20 | INFO | fairseq.trainer | begin training epoch 14
2022-03-17 11:42:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 11:42:48 | INFO | train_inner | epoch 014:     19 / 407 loss=6.478, ppl=89.16, wps=38303.6, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.473, loss_scale=32, train_wall=145, gb_free=21, wall=8250
2022-03-17 11:44:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 11:45:20 | INFO | train_inner | epoch 014:    120 / 407 loss=6.405, ppl=84.74, wps=43231.9, ups=0.66, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.476, loss_scale=16, train_wall=147, gb_free=21, wall=8401
2022-03-17 11:47:50 | INFO | train_inner | epoch 014:    220 / 407 loss=6.412, ppl=85.14, wps=43618.4, ups=0.67, wpb=65523.9, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.479, loss_scale=16, train_wall=146, gb_free=21, wall=8551
2022-03-17 11:50:20 | INFO | train_inner | epoch 014:    320 / 407 loss=6.406, ppl=84.8, wps=43648.6, ups=0.67, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.473, loss_scale=16, train_wall=146, gb_free=21, wall=8702
2022-03-17 11:52:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 11:52:50 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.312 | ppl 79.44 | wps 70443.2 | wpb 2047.5 | bsz 4 | num_updates 5687 | best_loss 6.312
2022-03-17 11:52:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5687 updates
2022-03-17 11:52:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 11:52:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 11:52:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 14 @ 5687 updates, score 6.312) (writing took 1.7722947606816888 seconds)
2022-03-17 11:52:51 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-17 11:52:51 | INFO | train | epoch 014 | loss 6.405 | ppl 84.76 | wps 42100.5 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 5687 | lr 0.000419332 | gnorm 0.475 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 8853
KL Stats: Epoch 14 Divergences: Uniform: 4.256441033228848 Unigram: 3.7748267183884723
2022-03-17 11:52:51 | INFO | fairseq.trainer | begin training epoch 15
2022-03-17 11:52:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 11:53:11 | INFO | train_inner | epoch 015:     13 / 407 loss=6.389, ppl=83.79, wps=38313.5, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.475, loss_scale=16, train_wall=145, gb_free=21, wall=8872
2022-03-17 11:55:41 | INFO | train_inner | epoch 015:    113 / 407 loss=6.311, ppl=79.37, wps=43641.2, ups=0.67, wpb=65523.9, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.467, loss_scale=16, train_wall=146, gb_free=21, wall=9022
2022-03-17 11:57:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 11:58:13 | INFO | train_inner | epoch 015:    214 / 407 loss=6.326, ppl=80.24, wps=43181.1, ups=0.66, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.474, loss_scale=16, train_wall=147, gb_free=21, wall=9174
2022-03-17 12:00:43 | INFO | train_inner | epoch 015:    314 / 407 loss=6.33, ppl=80.45, wps=43650, ups=0.67, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.474, loss_scale=16, train_wall=146, gb_free=21, wall=9324
2022-03-17 12:03:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 12:03:21 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.253 | ppl 76.26 | wps 70401.5 | wpb 2047.5 | bsz 4 | num_updates 6093 | best_loss 6.253
2022-03-17 12:03:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 6093 updates
2022-03-17 12:03:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 12:03:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 12:03:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 15 @ 6093 updates, score 6.253) (writing took 1.8273755619302392 seconds)
2022-03-17 12:03:23 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-17 12:03:23 | INFO | train | epoch 015 | loss 6.321 | ppl 79.97 | wps 42089.3 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 6093 | lr 0.000405121 | gnorm 0.473 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 9484
KL Stats: Epoch 15 Divergences: Uniform: 4.293149953719705 Unigram: 3.816978431912386
2022-03-17 12:03:23 | INFO | fairseq.trainer | begin training epoch 16
2022-03-17 12:03:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 12:03:34 | INFO | train_inner | epoch 016:      7 / 407 loss=6.315, ppl=79.62, wps=38288.1, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.474, loss_scale=16, train_wall=145, gb_free=21, wall=9495
2022-03-17 12:06:04 | INFO | train_inner | epoch 016:    107 / 407 loss=6.245, ppl=75.83, wps=43637.8, ups=0.67, wpb=65534.2, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.471, loss_scale=16, train_wall=146, gb_free=21, wall=9645
2022-03-17 12:08:34 | INFO | train_inner | epoch 016:    207 / 407 loss=6.253, ppl=76.25, wps=43622.4, ups=0.67, wpb=65536, bsz=128, num_updates=6300, lr=0.00039841, gnorm=0.467, loss_scale=16, train_wall=146, gb_free=21, wall=9795
2022-03-17 12:11:04 | INFO | train_inner | epoch 016:    307 / 407 loss=6.241, ppl=75.63, wps=43630.9, ups=0.67, wpb=65525.8, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.471, loss_scale=32, train_wall=146, gb_free=21, wall=9946
2022-03-17 12:13:34 | INFO | train_inner | epoch 016:    407 / 407 loss=6.259, ppl=76.58, wps=43649.1, ups=0.67, wpb=65372.2, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.471, loss_scale=32, train_wall=145, gb_free=21, wall=10095
2022-03-17 12:13:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 12:13:53 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.202 | ppl 73.61 | wps 70419.1 | wpb 2047.5 | bsz 4 | num_updates 6500 | best_loss 6.202
2022-03-17 12:13:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6500 updates
2022-03-17 12:13:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 12:13:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 12:13:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 16 @ 6500 updates, score 6.202) (writing took 1.816689238883555 seconds)
2022-03-17 12:13:55 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-17 12:13:55 | INFO | train | epoch 016 | loss 6.249 | ppl 76.06 | wps 42186.3 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 6500 | lr 0.000392232 | gnorm 0.47 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 10116
KL Stats: Epoch 16 Divergences: Uniform: 4.329052814574701 Unigram: 3.853262481511522
2022-03-17 12:13:55 | INFO | fairseq.trainer | begin training epoch 17
2022-03-17 12:13:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 12:16:25 | INFO | train_inner | epoch 017:    100 / 407 loss=6.173, ppl=72.13, wps=38285, ups=0.58, wpb=65525.8, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.469, loss_scale=32, train_wall=146, gb_free=21, wall=10267
2022-03-17 12:18:55 | INFO | train_inner | epoch 017:    200 / 407 loss=6.19, ppl=73.01, wps=43639.6, ups=0.67, wpb=65534.2, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.469, loss_scale=32, train_wall=146, gb_free=21, wall=10417
2022-03-17 12:21:26 | INFO | train_inner | epoch 017:    300 / 407 loss=6.194, ppl=73.2, wps=43603.5, ups=0.67, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.464, loss_scale=32, train_wall=146, gb_free=21, wall=10567
2022-03-17 12:23:56 | INFO | train_inner | epoch 017:    400 / 407 loss=6.191, ppl=73.08, wps=43647.7, ups=0.67, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.465, loss_scale=32, train_wall=146, gb_free=21, wall=10717
2022-03-17 12:24:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 12:24:25 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.156 | ppl 71.3 | wps 70470.9 | wpb 2047.5 | bsz 4 | num_updates 6907 | best_loss 6.156
2022-03-17 12:24:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6907 updates
2022-03-17 12:24:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 12:24:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 12:24:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 17 @ 6907 updates, score 6.156) (writing took 1.8310414534062147 seconds)
2022-03-17 12:24:27 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-17 12:24:27 | INFO | train | epoch 017 | loss 6.187 | ppl 72.86 | wps 42183.7 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 6907 | lr 0.000380501 | gnorm 0.467 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 10748
KL Stats: Epoch 17 Divergences: Uniform: 4.357102179540366 Unigram: 3.8868081263700396
2022-03-17 12:24:27 | INFO | fairseq.trainer | begin training epoch 18
2022-03-17 12:24:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 12:24:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 12:26:48 | INFO | train_inner | epoch 018:     94 / 407 loss=6.117, ppl=69.4, wps=37960.1, ups=0.58, wpb=65360.1, bsz=127.7, num_updates=7000, lr=0.000377964, gnorm=0.465, loss_scale=32, train_wall=147, gb_free=21, wall=10889
2022-03-17 12:29:18 | INFO | train_inner | epoch 018:    194 / 407 loss=6.135, ppl=70.28, wps=43630.8, ups=0.67, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.47, loss_scale=32, train_wall=146, gb_free=21, wall=11040
2022-03-17 12:31:48 | INFO | train_inner | epoch 018:    294 / 407 loss=6.137, ppl=70.38, wps=43624, ups=0.67, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.464, loss_scale=32, train_wall=146, gb_free=21, wall=11190
2022-03-17 12:34:19 | INFO | train_inner | epoch 018:    394 / 407 loss=6.144, ppl=70.74, wps=43606.2, ups=0.67, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.467, loss_scale=32, train_wall=146, gb_free=21, wall=11340
2022-03-17 12:34:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 12:34:57 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.119 | ppl 69.49 | wps 70436.3 | wpb 2047.5 | bsz 4 | num_updates 7313 | best_loss 6.119
2022-03-17 12:34:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7313 updates
2022-03-17 12:34:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 12:34:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 12:34:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 18 @ 7313 updates, score 6.119) (writing took 1.8298881324008107 seconds)
2022-03-17 12:34:59 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-17 12:34:59 | INFO | train | epoch 018 | loss 6.133 | ppl 70.18 | wps 42075.7 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 7313 | lr 0.000369787 | gnorm 0.467 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 11380
KL Stats: Epoch 18 Divergences: Uniform: 4.382301704447906 Unigram: 3.9147818477624337
2022-03-17 12:34:59 | INFO | fairseq.trainer | begin training epoch 19
2022-03-17 12:34:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 12:37:09 | INFO | train_inner | epoch 019:     87 / 407 loss=6.074, ppl=67.39, wps=38287.3, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.471, loss_scale=32, train_wall=145, gb_free=21, wall=11511
2022-03-17 12:37:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 12:39:41 | INFO | train_inner | epoch 019:    188 / 407 loss=6.087, ppl=67.98, wps=43188, ups=0.66, wpb=65523.9, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.465, loss_scale=32, train_wall=147, gb_free=21, wall=11663
2022-03-17 12:42:12 | INFO | train_inner | epoch 019:    288 / 407 loss=6.095, ppl=68.37, wps=43582.6, ups=0.67, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.464, loss_scale=32, train_wall=146, gb_free=21, wall=11813
2022-03-17 12:44:42 | INFO | train_inner | epoch 019:    388 / 407 loss=6.089, ppl=68.09, wps=43634.9, ups=0.67, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.467, loss_scale=32, train_wall=146, gb_free=21, wall=11963
2022-03-17 12:45:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 12:45:29 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.088 | ppl 68.01 | wps 70324.1 | wpb 2047.5 | bsz 4 | num_updates 7719 | best_loss 6.088
2022-03-17 12:45:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7719 updates
2022-03-17 12:45:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 12:45:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 12:45:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 19 @ 7719 updates, score 6.088) (writing took 1.8350806031376123 seconds)
2022-03-17 12:45:31 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-17 12:45:31 | INFO | train | epoch 019 | loss 6.086 | ppl 67.93 | wps 42068.2 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 7719 | lr 0.000359931 | gnorm 0.467 | loss_scale 32 | train_wall 593 | gb_free 21 | wall 12012
KL Stats: Epoch 19 Divergences: Uniform: 4.406097210061949 Unigram: 3.9429458127458767
2022-03-17 12:45:31 | INFO | fairseq.trainer | begin training epoch 20
2022-03-17 12:45:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 12:47:33 | INFO | train_inner | epoch 020:     81 / 407 loss=6.025, ppl=65.12, wps=38278.4, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.468, loss_scale=32, train_wall=145, gb_free=21, wall=12134
2022-03-17 12:50:03 | INFO | train_inner | epoch 020:    181 / 407 loss=6.05, ppl=66.27, wps=43648.6, ups=0.67, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.47, loss_scale=32, train_wall=146, gb_free=21, wall=12284
2022-03-17 12:50:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 12:52:34 | INFO | train_inner | epoch 020:    282 / 407 loss=6.048, ppl=66.17, wps=43186.4, ups=0.66, wpb=65525.8, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.465, loss_scale=32, train_wall=147, gb_free=21, wall=12436
2022-03-17 12:55:05 | INFO | train_inner | epoch 020:    382 / 407 loss=6.062, ppl=66.8, wps=43639.9, ups=0.67, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.467, loss_scale=32, train_wall=146, gb_free=21, wall=12586
2022-03-17 12:55:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 12:56:01 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.061 | ppl 66.78 | wps 70323.2 | wpb 2047.5 | bsz 4 | num_updates 8125 | best_loss 6.061
2022-03-17 12:56:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 8125 updates
2022-03-17 12:56:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 12:56:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 12:56:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 20 @ 8125 updates, score 6.061) (writing took 1.8309460114687681 seconds)
2022-03-17 12:56:03 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-17 12:56:03 | INFO | train | epoch 020 | loss 6.043 | ppl 65.95 | wps 42085.1 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 8125 | lr 0.000350823 | gnorm 0.468 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 12644
KL Stats: Epoch 20 Divergences: Uniform: 4.428256345261859 Unigram: 3.9689092354122932
2022-03-17 12:56:03 | INFO | fairseq.trainer | begin training epoch 21
2022-03-17 12:56:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 12:57:55 | INFO | train_inner | epoch 021:     75 / 407 loss=5.996, ppl=63.8, wps=38285.8, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.474, loss_scale=32, train_wall=145, gb_free=21, wall=12757
2022-03-17 13:00:26 | INFO | train_inner | epoch 021:    175 / 407 loss=6.003, ppl=64.13, wps=43634, ups=0.67, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.471, loss_scale=32, train_wall=146, gb_free=21, wall=12907
2022-03-17 13:01:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 13:02:57 | INFO | train_inner | epoch 021:    276 / 407 loss=6.009, ppl=64.38, wps=43183.9, ups=0.66, wpb=65525.8, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.473, loss_scale=16, train_wall=147, gb_free=21, wall=13059
2022-03-17 13:05:27 | INFO | train_inner | epoch 021:    376 / 407 loss=6.018, ppl=64.79, wps=43672.7, ups=0.67, wpb=65534.2, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.466, loss_scale=16, train_wall=145, gb_free=21, wall=13209
2022-03-17 13:06:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 13:06:32 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.029 | ppl 65.3 | wps 70618.9 | wpb 2047.5 | bsz 4 | num_updates 8531 | best_loss 6.029
2022-03-17 13:06:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8531 updates
2022-03-17 13:06:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 13:06:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 13:06:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 21 @ 8531 updates, score 6.029) (writing took 1.811142093501985 seconds)
2022-03-17 13:06:34 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-17 13:06:34 | INFO | train | epoch 021 | loss 6.004 | ppl 64.19 | wps 42095.4 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 8531 | lr 0.000342373 | gnorm 0.47 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 13276
KL Stats: Epoch 21 Divergences: Uniform: 4.4456020941873 Unigram: 3.989123532051183
2022-03-17 13:06:34 | INFO | fairseq.trainer | begin training epoch 22
2022-03-17 13:06:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 13:08:18 | INFO | train_inner | epoch 022:     69 / 407 loss=5.981, ppl=63.17, wps=38324.6, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.471, loss_scale=16, train_wall=145, gb_free=21, wall=13379
2022-03-17 13:10:48 | INFO | train_inner | epoch 022:    169 / 407 loss=5.949, ppl=61.79, wps=43644.4, ups=0.67, wpb=65534.2, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.47, loss_scale=16, train_wall=146, gb_free=21, wall=13529
2022-03-17 13:13:18 | INFO | train_inner | epoch 022:    269 / 407 loss=5.972, ppl=62.77, wps=43614, ups=0.67, wpb=65525.8, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.473, loss_scale=16, train_wall=146, gb_free=21, wall=13680
2022-03-17 13:15:48 | INFO | train_inner | epoch 022:    369 / 407 loss=5.982, ppl=63.19, wps=43668.7, ups=0.67, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.47, loss_scale=32, train_wall=146, gb_free=21, wall=13830
2022-03-17 13:16:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 13:17:04 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.012 | ppl 64.52 | wps 70403.8 | wpb 2047.5 | bsz 4 | num_updates 8938 | best_loss 6.012
2022-03-17 13:17:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8938 updates
2022-03-17 13:17:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 13:17:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 13:17:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 22 @ 8938 updates, score 6.012) (writing took 1.8226726967841387 seconds)
2022-03-17 13:17:06 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-17 13:17:06 | INFO | train | epoch 022 | loss 5.97 | ppl 62.68 | wps 42200.6 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 8938 | lr 0.000334487 | gnorm 0.471 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 13907
KL Stats: Epoch 22 Divergences: Uniform: 4.465754271417077 Unigram: 4.011461606574316
2022-03-17 13:17:06 | INFO | fairseq.trainer | begin training epoch 23
2022-03-17 13:17:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 13:18:39 | INFO | train_inner | epoch 023:     62 / 407 loss=5.948, ppl=61.71, wps=38295.6, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.473, loss_scale=32, train_wall=145, gb_free=21, wall=14000
2022-03-17 13:21:09 | INFO | train_inner | epoch 023:    162 / 407 loss=5.93, ppl=60.95, wps=43643.7, ups=0.67, wpb=65534.2, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.475, loss_scale=32, train_wall=146, gb_free=21, wall=14151
2022-03-17 13:23:39 | INFO | train_inner | epoch 023:    262 / 407 loss=5.935, ppl=61.17, wps=43623.6, ups=0.67, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.468, loss_scale=32, train_wall=146, gb_free=21, wall=14301
2022-03-17 13:26:10 | INFO | train_inner | epoch 023:    362 / 407 loss=5.954, ppl=61.97, wps=43611.2, ups=0.67, wpb=65525.8, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.472, loss_scale=32, train_wall=146, gb_free=21, wall=14451
2022-03-17 13:27:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 13:27:36 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.995 | ppl 63.78 | wps 70264.6 | wpb 2047.5 | bsz 4 | num_updates 9345 | best_loss 5.995
2022-03-17 13:27:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9345 updates
2022-03-17 13:27:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 13:27:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 13:27:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 23 @ 9345 updates, score 5.995) (writing took 1.8611877812072635 seconds)
2022-03-17 13:27:38 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-17 13:27:38 | INFO | train | epoch 023 | loss 5.938 | ppl 61.31 | wps 42177.9 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 9345 | lr 0.000327122 | gnorm 0.473 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 14539
KL Stats: Epoch 23 Divergences: Uniform: 4.483135597810942 Unigram: 4.0302622620663
2022-03-17 13:27:38 | INFO | fairseq.trainer | begin training epoch 24
2022-03-17 13:27:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 13:27:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 13:29:02 | INFO | train_inner | epoch 024:     56 / 407 loss=5.904, ppl=59.87, wps=37923.1, ups=0.58, wpb=65372.2, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.474, loss_scale=32, train_wall=147, gb_free=21, wall=14623
2022-03-17 13:31:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 13:31:34 | INFO | train_inner | epoch 024:    157 / 407 loss=5.899, ppl=59.69, wps=43202.1, ups=0.66, wpb=65534.2, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.474, loss_scale=16, train_wall=147, gb_free=21, wall=14775
2022-03-17 13:34:04 | INFO | train_inner | epoch 024:    257 / 407 loss=5.917, ppl=60.44, wps=43609.9, ups=0.67, wpb=65525.8, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.476, loss_scale=16, train_wall=146, gb_free=21, wall=14925
2022-03-17 13:36:34 | INFO | train_inner | epoch 024:    357 / 407 loss=5.932, ppl=61.05, wps=43632.7, ups=0.67, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.474, loss_scale=16, train_wall=146, gb_free=21, wall=15076
2022-03-17 13:37:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 13:38:08 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.973 | ppl 62.8 | wps 70474.3 | wpb 2047.5 | bsz 4 | num_updates 9750 | best_loss 5.973
2022-03-17 13:38:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9750 updates
2022-03-17 13:38:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 13:38:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 13:38:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 24 @ 9750 updates, score 5.973) (writing took 1.843468826264143 seconds)
2022-03-17 13:38:10 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-17 13:38:10 | INFO | train | epoch 024 | loss 5.909 | ppl 60.1 | wps 41978.9 | ups 0.64 | wpb 65492.6 | bsz 127.9 | num_updates 9750 | lr 0.000320256 | gnorm 0.474 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 15171
KL Stats: Epoch 24 Divergences: Uniform: 4.496433735225239 Unigram: 4.046600633123707
2022-03-17 13:38:10 | INFO | fairseq.trainer | begin training epoch 25
2022-03-17 13:38:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 13:39:25 | INFO | train_inner | epoch 025:     50 / 407 loss=5.892, ppl=59.39, wps=38302.6, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.473, loss_scale=16, train_wall=145, gb_free=21, wall=15246
2022-03-17 13:41:55 | INFO | train_inner | epoch 025:    150 / 407 loss=5.868, ppl=58.38, wps=43654.4, ups=0.67, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.476, loss_scale=16, train_wall=146, gb_free=21, wall=15396
2022-03-17 13:44:25 | INFO | train_inner | epoch 025:    250 / 407 loss=5.886, ppl=59.12, wps=43625.6, ups=0.67, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.473, loss_scale=32, train_wall=146, gb_free=21, wall=15547
2022-03-17 13:46:55 | INFO | train_inner | epoch 025:    350 / 407 loss=5.892, ppl=59.38, wps=43642.3, ups=0.67, wpb=65525.8, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.472, loss_scale=32, train_wall=146, gb_free=21, wall=15697
2022-03-17 13:48:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 13:48:40 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.967 | ppl 62.57 | wps 70571.7 | wpb 2047.5 | bsz 4 | num_updates 10157 | best_loss 5.967
2022-03-17 13:48:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 10157 updates
2022-03-17 13:48:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 13:48:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 13:48:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 25 @ 10157 updates, score 5.967) (writing took 1.8619983224198222 seconds)
2022-03-17 13:48:41 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-17 13:48:41 | INFO | train | epoch 025 | loss 5.883 | ppl 59 | wps 42198.9 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 10157 | lr 0.000313774 | gnorm 0.473 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 15803
KL Stats: Epoch 25 Divergences: Uniform: 4.5152739325977445 Unigram: 4.065805090683916
2022-03-17 13:48:41 | INFO | fairseq.trainer | begin training epoch 26
2022-03-17 13:48:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 13:49:46 | INFO | train_inner | epoch 026:     43 / 407 loss=5.866, ppl=58.32, wps=38301.6, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.472, loss_scale=32, train_wall=145, gb_free=21, wall=15867
2022-03-17 13:52:16 | INFO | train_inner | epoch 026:    143 / 407 loss=5.839, ppl=57.24, wps=43657.7, ups=0.67, wpb=65525.8, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.474, loss_scale=32, train_wall=146, gb_free=21, wall=16018
2022-03-17 13:52:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 13:54:48 | INFO | train_inner | epoch 026:    244 / 407 loss=5.862, ppl=58.15, wps=43186.8, ups=0.66, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.481, loss_scale=16, train_wall=147, gb_free=21, wall=16169
2022-03-17 13:57:18 | INFO | train_inner | epoch 026:    344 / 407 loss=5.881, ppl=58.92, wps=43653.7, ups=0.67, wpb=65534.2, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.481, loss_scale=16, train_wall=146, gb_free=21, wall=16319
2022-03-17 13:58:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 13:59:11 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.944 | ppl 61.56 | wps 70538.1 | wpb 2047.5 | bsz 4 | num_updates 10563 | best_loss 5.944
2022-03-17 13:59:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10563 updates
2022-03-17 13:59:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 13:59:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 13:59:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 26 @ 10563 updates, score 5.944) (writing took 1.8370744390413165 seconds)
2022-03-17 13:59:13 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-17 13:59:13 | INFO | train | epoch 026 | loss 5.858 | ppl 58 | wps 42095.4 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 10563 | lr 0.000307685 | gnorm 0.478 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 16434
KL Stats: Epoch 26 Divergences: Uniform: 4.525996741617055 Unigram: 4.079337406646885
2022-03-17 13:59:13 | INFO | fairseq.trainer | begin training epoch 27
2022-03-17 13:59:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 14:00:09 | INFO | train_inner | epoch 027:     37 / 407 loss=5.845, ppl=57.47, wps=38300.4, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.475, loss_scale=16, train_wall=145, gb_free=21, wall=16490
2022-03-17 14:02:39 | INFO | train_inner | epoch 027:    137 / 407 loss=5.817, ppl=56.39, wps=43645.4, ups=0.67, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.477, loss_scale=16, train_wall=146, gb_free=21, wall=16640
2022-03-17 14:05:09 | INFO | train_inner | epoch 027:    237 / 407 loss=5.844, ppl=57.43, wps=43608, ups=0.67, wpb=65534.2, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.473, loss_scale=16, train_wall=146, gb_free=21, wall=16791
2022-03-17 14:07:39 | INFO | train_inner | epoch 027:    337 / 407 loss=5.841, ppl=57.33, wps=43669.4, ups=0.67, wpb=65525.8, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.473, loss_scale=32, train_wall=146, gb_free=21, wall=16941
2022-03-17 14:09:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 14:09:43 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.933 | ppl 61.09 | wps 70492 | wpb 2047.5 | bsz 4 | num_updates 10970 | best_loss 5.933
2022-03-17 14:09:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10970 updates
2022-03-17 14:09:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 14:09:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 14:09:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 27 @ 10970 updates, score 5.933) (writing took 1.8335148487240076 seconds)
2022-03-17 14:09:45 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-17 14:09:45 | INFO | train | epoch 027 | loss 5.834 | ppl 57.06 | wps 42196.9 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 10970 | lr 0.000301923 | gnorm 0.476 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 17066
KL Stats: Epoch 27 Divergences: Uniform: 4.542381853923864 Unigram: 4.095861237232703
2022-03-17 14:09:45 | INFO | fairseq.trainer | begin training epoch 28
2022-03-17 14:09:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 14:10:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 14:10:31 | INFO | train_inner | epoch 028:     31 / 407 loss=5.834, ppl=57.03, wps=37964, ups=0.58, wpb=65372.2, bsz=127.7, num_updates=11000, lr=0.000301511, gnorm=0.48, loss_scale=16, train_wall=147, gb_free=21, wall=17113
2022-03-17 14:13:02 | INFO | train_inner | epoch 028:    131 / 407 loss=5.794, ppl=55.48, wps=43658.7, ups=0.67, wpb=65534.2, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.473, loss_scale=16, train_wall=146, gb_free=21, wall=17263
2022-03-17 14:15:32 | INFO | train_inner | epoch 028:    231 / 407 loss=5.815, ppl=56.29, wps=43632.3, ups=0.67, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.474, loss_scale=16, train_wall=146, gb_free=21, wall=17413
2022-03-17 14:18:02 | INFO | train_inner | epoch 028:    331 / 407 loss=5.828, ppl=56.82, wps=43672.2, ups=0.67, wpb=65525.8, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.478, loss_scale=16, train_wall=145, gb_free=21, wall=17563
2022-03-17 14:19:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 14:20:14 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.924 | ppl 60.71 | wps 70668.6 | wpb 2047.5 | bsz 4 | num_updates 11376 | best_loss 5.924
2022-03-17 14:20:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 11376 updates
2022-03-17 14:20:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 14:20:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 14:20:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 28 @ 11376 updates, score 5.924) (writing took 1.8370620608329773 seconds)
2022-03-17 14:20:16 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-17 14:20:16 | INFO | train | epoch 028 | loss 5.813 | ppl 56.23 | wps 42103.2 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 11376 | lr 0.000296487 | gnorm 0.475 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 17698
KL Stats: Epoch 28 Divergences: Uniform: 4.552222451750269 Unigram: 4.107805641796028
2022-03-17 14:20:16 | INFO | fairseq.trainer | begin training epoch 29
2022-03-17 14:20:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 14:20:52 | INFO | train_inner | epoch 029:     24 / 407 loss=5.815, ppl=56.28, wps=38301.1, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=11400, lr=0.000296174, gnorm=0.477, loss_scale=16, train_wall=145, gb_free=21, wall=17734
2022-03-17 14:23:23 | INFO | train_inner | epoch 029:    124 / 407 loss=5.777, ppl=54.82, wps=43640.2, ups=0.67, wpb=65534.2, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.477, loss_scale=16, train_wall=146, gb_free=21, wall=17884
2022-03-17 14:25:53 | INFO | train_inner | epoch 029:    224 / 407 loss=5.793, ppl=55.45, wps=43605.3, ups=0.67, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.473, loss_scale=32, train_wall=146, gb_free=21, wall=18034
2022-03-17 14:28:23 | INFO | train_inner | epoch 029:    324 / 407 loss=5.806, ppl=55.93, wps=43628.6, ups=0.67, wpb=65525.8, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.479, loss_scale=32, train_wall=146, gb_free=21, wall=18184
2022-03-17 14:29:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 14:30:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 14:30:46 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.909 | ppl 60.11 | wps 70374.5 | wpb 2047.5 | bsz 4 | num_updates 11782 | best_loss 5.909
2022-03-17 14:30:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11782 updates
2022-03-17 14:30:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 14:30:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 14:30:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 29 @ 11782 updates, score 5.909) (writing took 1.7198441987857223 seconds)
2022-03-17 14:30:48 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-17 14:30:48 | INFO | train | epoch 029 | loss 5.793 | ppl 55.46 | wps 42094.8 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 11782 | lr 0.000291334 | gnorm 0.478 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 18329
KL Stats: Epoch 29 Divergences: Uniform: 4.566007321142989 Unigram: 4.123119495964713
2022-03-17 14:30:48 | INFO | fairseq.trainer | begin training epoch 30
2022-03-17 14:30:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 14:31:15 | INFO | train_inner | epoch 030:     18 / 407 loss=5.798, ppl=55.62, wps=38006.2, ups=0.58, wpb=65372.2, bsz=127.7, num_updates=11800, lr=0.000291111, gnorm=0.481, loss_scale=16, train_wall=147, gb_free=21, wall=18356
2022-03-17 14:33:45 | INFO | train_inner | epoch 030:    118 / 407 loss=5.756, ppl=54.03, wps=43651.6, ups=0.67, wpb=65525.8, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.473, loss_scale=16, train_wall=146, gb_free=21, wall=18507
2022-03-17 14:36:15 | INFO | train_inner | epoch 030:    218 / 407 loss=5.767, ppl=54.47, wps=43613.3, ups=0.67, wpb=65534.2, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.482, loss_scale=16, train_wall=146, gb_free=21, wall=18657
2022-03-17 14:38:46 | INFO | train_inner | epoch 030:    318 / 407 loss=5.79, ppl=55.34, wps=43681.3, ups=0.67, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.484, loss_scale=16, train_wall=145, gb_free=21, wall=18807
2022-03-17 14:40:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 14:41:18 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.892 | ppl 59.4 | wps 70521.7 | wpb 2047.5 | bsz 4 | num_updates 12189 | best_loss 5.892
2022-03-17 14:41:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 12189 updates
2022-03-17 14:41:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 14:41:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 14:41:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 30 @ 12189 updates, score 5.892) (writing took 1.8033434050157666 seconds)
2022-03-17 14:41:20 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-17 14:41:20 | INFO | train | epoch 030 | loss 5.775 | ppl 54.75 | wps 42208.6 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 12189 | lr 0.000286428 | gnorm 0.479 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 18961
KL Stats: Epoch 30 Divergences: Uniform: 4.574482229315474 Unigram: 4.132647448962407
2022-03-17 14:41:20 | INFO | fairseq.trainer | begin training epoch 31
2022-03-17 14:41:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 14:41:36 | INFO | train_inner | epoch 031:     11 / 407 loss=5.785, ppl=55.15, wps=38314.7, ups=0.59, wpb=65361.9, bsz=127.7, num_updates=12200, lr=0.000286299, gnorm=0.477, loss_scale=16, train_wall=145, gb_free=21, wall=18977
2022-03-17 14:43:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 14:44:08 | INFO | train_inner | epoch 031:    112 / 407 loss=5.736, ppl=53.29, wps=43244.5, ups=0.66, wpb=65534.2, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.485, loss_scale=16, train_wall=147, gb_free=21, wall=19129
2022-03-17 14:46:38 | INFO | train_inner | epoch 031:    212 / 407 loss=5.754, ppl=53.97, wps=43604.6, ups=0.67, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.489, loss_scale=16, train_wall=146, gb_free=21, wall=19279
2022-03-17 14:49:08 | INFO | train_inner | epoch 031:    312 / 407 loss=5.769, ppl=54.54, wps=43666.3, ups=0.67, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.478, loss_scale=16, train_wall=146, gb_free=21, wall=19429
2022-03-17 14:51:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 14:51:49 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.883 | ppl 59.01 | wps 70510 | wpb 2047.5 | bsz 4 | num_updates 12595 | best_loss 5.883
2022-03-17 14:51:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12595 updates
2022-03-17 14:51:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 14:51:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 14:51:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 31 @ 12595 updates, score 5.883) (writing took 1.7698000203818083 seconds)
2022-03-17 14:51:51 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-17 14:51:51 | INFO | train | epoch 031 | loss 5.757 | ppl 54.1 | wps 42104.9 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 12595 | lr 0.000281774 | gnorm 0.484 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 19592
KL Stats: Epoch 31 Divergences: Uniform: 4.587241989336001 Unigram: 4.145549050445027
2022-03-17 14:51:51 | INFO | fairseq.trainer | begin training epoch 32
2022-03-17 14:51:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 14:51:59 | INFO | train_inner | epoch 032:      5 / 407 loss=5.772, ppl=54.65, wps=38317.9, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=12600, lr=0.000281718, gnorm=0.483, loss_scale=16, train_wall=145, gb_free=21, wall=19600
2022-03-17 14:54:29 | INFO | train_inner | epoch 032:    105 / 407 loss=5.714, ppl=52.48, wps=43634.3, ups=0.67, wpb=65534.2, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.477, loss_scale=16, train_wall=146, gb_free=21, wall=19750
2022-03-17 14:56:59 | INFO | train_inner | epoch 032:    205 / 407 loss=5.737, ppl=53.32, wps=43608.4, ups=0.67, wpb=65525.8, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.479, loss_scale=32, train_wall=146, gb_free=21, wall=19900
2022-03-17 14:59:29 | INFO | train_inner | epoch 032:    305 / 407 loss=5.743, ppl=53.54, wps=43657.7, ups=0.67, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.474, loss_scale=32, train_wall=146, gb_free=21, wall=20051
2022-03-17 15:01:59 | INFO | train_inner | epoch 032:    405 / 407 loss=5.77, ppl=54.57, wps=43679.7, ups=0.67, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.479, loss_scale=32, train_wall=146, gb_free=21, wall=20201
2022-03-17 15:02:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 15:02:21 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.87 | ppl 58.5 | wps 70530.5 | wpb 2047.5 | bsz 4 | num_updates 13002 | best_loss 5.87
2022-03-17 15:02:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 13002 updates
2022-03-17 15:02:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 15:02:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 15:02:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 32 @ 13002 updates, score 5.87) (writing took 1.7706787148490548 seconds)
2022-03-17 15:02:23 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-17 15:02:23 | INFO | train | epoch 032 | loss 5.74 | ppl 53.45 | wps 42205.2 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 13002 | lr 0.000277329 | gnorm 0.478 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 20224
KL Stats: Epoch 32 Divergences: Uniform: 4.599260775254025 Unigram: 4.156449185129448
2022-03-17 15:02:23 | INFO | fairseq.trainer | begin training epoch 33
2022-03-17 15:02:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 15:04:50 | INFO | train_inner | epoch 033:     98 / 407 loss=5.694, ppl=51.77, wps=38336, ups=0.59, wpb=65361.9, bsz=127.7, num_updates=13100, lr=0.000276289, gnorm=0.484, loss_scale=32, train_wall=145, gb_free=21, wall=20371
2022-03-17 15:06:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 15:07:21 | INFO | train_inner | epoch 033:    199 / 407 loss=5.715, ppl=52.54, wps=43224.9, ups=0.66, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.483, loss_scale=16, train_wall=147, gb_free=21, wall=20523
2022-03-17 15:09:51 | INFO | train_inner | epoch 033:    299 / 407 loss=5.744, ppl=53.6, wps=43671.6, ups=0.67, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.492, loss_scale=16, train_wall=146, gb_free=21, wall=20673
2022-03-17 15:12:21 | INFO | train_inner | epoch 033:    399 / 407 loss=5.743, ppl=53.57, wps=43687.9, ups=0.67, wpb=65534.2, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.477, loss_scale=16, train_wall=145, gb_free=21, wall=20823
2022-03-17 15:12:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 15:12:52 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.865 | ppl 58.27 | wps 70546.2 | wpb 2047.5 | bsz 4 | num_updates 13408 | best_loss 5.865
2022-03-17 15:12:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 13408 updates
2022-03-17 15:12:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 15:12:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 15:12:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 33 @ 13408 updates, score 5.865) (writing took 1.7865644078701735 seconds)
2022-03-17 15:12:54 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-17 15:12:54 | INFO | train | epoch 033 | loss 5.724 | ppl 52.87 | wps 42124.1 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 13408 | lr 0.000273098 | gnorm 0.484 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 20855
KL Stats: Epoch 33 Divergences: Uniform: 4.608413035747053 Unigram: 4.167853660681184
2022-03-17 15:12:54 | INFO | fairseq.trainer | begin training epoch 34
2022-03-17 15:12:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 15:15:12 | INFO | train_inner | epoch 034:     92 / 407 loss=5.685, ppl=51.43, wps=38312.5, ups=0.59, wpb=65361.9, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=0.489, loss_scale=16, train_wall=145, gb_free=21, wall=20993
2022-03-17 15:17:42 | INFO | train_inner | epoch 034:    192 / 407 loss=5.71, ppl=52.36, wps=43631.3, ups=0.67, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.481, loss_scale=16, train_wall=146, gb_free=21, wall=21144
2022-03-17 15:20:12 | INFO | train_inner | epoch 034:    292 / 407 loss=5.715, ppl=52.52, wps=43618.2, ups=0.67, wpb=65534.2, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.481, loss_scale=32, train_wall=146, gb_free=21, wall=21294
2022-03-17 15:22:43 | INFO | train_inner | epoch 034:    392 / 407 loss=5.728, ppl=53.02, wps=43677.3, ups=0.67, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.478, loss_scale=32, train_wall=145, gb_free=21, wall=21444
2022-03-17 15:23:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 15:23:24 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.858 | ppl 58.01 | wps 70423.3 | wpb 2047.5 | bsz 4 | num_updates 13815 | best_loss 5.858
2022-03-17 15:23:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13815 updates
2022-03-17 15:23:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 15:23:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 15:23:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 34 @ 13815 updates, score 5.858) (writing took 1.7987555135041475 seconds)
2022-03-17 15:23:26 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-17 15:23:26 | INFO | train | epoch 034 | loss 5.71 | ppl 52.34 | wps 42199.9 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 13815 | lr 0.000269045 | gnorm 0.483 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 21487
KL Stats: Epoch 34 Divergences: Uniform: 4.619210236386972 Unigram: 4.178748928172641
2022-03-17 15:23:26 | INFO | fairseq.trainer | begin training epoch 35
2022-03-17 15:23:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 15:25:33 | INFO | train_inner | epoch 035:     85 / 407 loss=5.674, ppl=51.04, wps=38275.1, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=0.488, loss_scale=32, train_wall=145, gb_free=21, wall=21615
2022-03-17 15:28:04 | INFO | train_inner | epoch 035:    185 / 407 loss=5.681, ppl=51.3, wps=43612.4, ups=0.67, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.486, loss_scale=32, train_wall=146, gb_free=21, wall=21765
2022-03-17 15:29:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 15:30:35 | INFO | train_inner | epoch 035:    286 / 407 loss=5.71, ppl=52.35, wps=43175.4, ups=0.66, wpb=65525.8, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.49, loss_scale=16, train_wall=147, gb_free=21, wall=21917
2022-03-17 15:33:05 | INFO | train_inner | epoch 035:    386 / 407 loss=5.72, ppl=52.69, wps=43672.6, ups=0.67, wpb=65534.2, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.482, loss_scale=16, train_wall=146, gb_free=21, wall=22067
2022-03-17 15:33:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 15:33:56 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.849 | ppl 57.62 | wps 70283.2 | wpb 2047.5 | bsz 4 | num_updates 14221 | best_loss 5.849
2022-03-17 15:33:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 14221 updates
2022-03-17 15:33:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 15:33:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 15:33:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 35 @ 14221 updates, score 5.849) (writing took 1.7502898639068007 seconds)
2022-03-17 15:33:57 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-17 15:33:57 | INFO | train | epoch 035 | loss 5.696 | ppl 51.83 | wps 42077.1 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 14221 | lr 0.000265176 | gnorm 0.486 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 22119
KL Stats: Epoch 35 Divergences: Uniform: 4.627813212047749 Unigram: 4.18823241435655
2022-03-17 15:33:57 | INFO | fairseq.trainer | begin training epoch 36
2022-03-17 15:33:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 15:35:56 | INFO | train_inner | epoch 036:     79 / 407 loss=5.664, ppl=50.71, wps=38291.2, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=0.483, loss_scale=16, train_wall=145, gb_free=21, wall=22238
2022-03-17 15:38:26 | INFO | train_inner | epoch 036:    179 / 407 loss=5.673, ppl=51.02, wps=43632.7, ups=0.67, wpb=65534.2, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.487, loss_scale=16, train_wall=146, gb_free=21, wall=22388
2022-03-17 15:40:57 | INFO | train_inner | epoch 036:    279 / 407 loss=5.687, ppl=51.52, wps=43599.2, ups=0.67, wpb=65525.8, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.483, loss_scale=16, train_wall=146, gb_free=21, wall=22538
2022-03-17 15:43:27 | INFO | train_inner | epoch 036:    379 / 407 loss=5.704, ppl=52.14, wps=43641.5, ups=0.67, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.483, loss_scale=32, train_wall=146, gb_free=21, wall=22688
2022-03-17 15:44:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 15:44:27 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.842 | ppl 57.35 | wps 70476.7 | wpb 2047.5 | bsz 4 | num_updates 14628 | best_loss 5.842
2022-03-17 15:44:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14628 updates
2022-03-17 15:44:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 15:44:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 15:44:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 36 @ 14628 updates, score 5.842) (writing took 1.8794047208502889 seconds)
2022-03-17 15:44:29 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-17 15:44:29 | INFO | train | epoch 036 | loss 5.682 | ppl 51.35 | wps 42183.2 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 14628 | lr 0.000261461 | gnorm 0.484 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 22751
KL Stats: Epoch 36 Divergences: Uniform: 4.6381014294482155 Unigram: 4.198592081902034
2022-03-17 15:44:29 | INFO | fairseq.trainer | begin training epoch 37
2022-03-17 15:44:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 15:46:18 | INFO | train_inner | epoch 037:     72 / 407 loss=5.652, ppl=50.3, wps=38286.2, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=14700, lr=0.00026082, gnorm=0.485, loss_scale=32, train_wall=145, gb_free=21, wall=22859
2022-03-17 15:46:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 15:48:49 | INFO | train_inner | epoch 037:    173 / 407 loss=5.657, ppl=50.44, wps=43157, ups=0.66, wpb=65525.8, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.481, loss_scale=16, train_wall=147, gb_free=21, wall=23011
2022-03-17 15:51:21 | INFO | train_inner | epoch 037:    273 / 407 loss=5.679, ppl=51.22, wps=43321.2, ups=0.66, wpb=65534.2, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.492, loss_scale=16, train_wall=147, gb_free=21, wall=23162
2022-03-17 15:53:52 | INFO | train_inner | epoch 037:    373 / 407 loss=5.695, ppl=51.79, wps=43347.1, ups=0.66, wpb=65536, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.485, loss_scale=16, train_wall=147, gb_free=21, wall=23313
2022-03-17 15:54:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 15:55:02 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.832 | ppl 56.96 | wps 69814.6 | wpb 2047.5 | bsz 4 | num_updates 15034 | best_loss 5.832
2022-03-17 15:55:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 15034 updates
2022-03-17 15:55:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 15:55:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 15:55:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 37 @ 15034 updates, score 5.832) (writing took 1.8317035902291536 seconds)
2022-03-17 15:55:04 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-17 15:55:04 | INFO | train | epoch 037 | loss 5.67 | ppl 50.9 | wps 41901.5 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 15034 | lr 0.000257907 | gnorm 0.486 | loss_scale 16 | train_wall 595 | gb_free 21 | wall 23385
KL Stats: Epoch 37 Divergences: Uniform: 4.64550671662058 Unigram: 4.206425695373957
2022-03-17 15:55:04 | INFO | fairseq.trainer | begin training epoch 38
2022-03-17 15:55:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 15:56:44 | INFO | train_inner | epoch 038:     66 / 407 loss=5.643, ppl=49.95, wps=38032.4, ups=0.58, wpb=65372.2, bsz=127.7, num_updates=15100, lr=0.000257343, gnorm=0.485, loss_scale=16, train_wall=146, gb_free=21, wall=23485
2022-03-17 15:59:15 | INFO | train_inner | epoch 038:    166 / 407 loss=5.646, ppl=50.08, wps=43355.9, ups=0.66, wpb=65525.8, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.49, loss_scale=16, train_wall=147, gb_free=21, wall=23636
2022-03-17 16:01:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 16:01:48 | INFO | train_inner | epoch 038:    267 / 407 loss=5.673, ppl=51.01, wps=42918.1, ups=0.65, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.487, loss_scale=16, train_wall=148, gb_free=21, wall=23789
2022-03-17 16:04:19 | INFO | train_inner | epoch 038:    367 / 407 loss=5.669, ppl=50.88, wps=43379, ups=0.66, wpb=65534.2, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.486, loss_scale=16, train_wall=146, gb_free=21, wall=23940
2022-03-17 16:05:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 16:05:38 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.828 | ppl 56.79 | wps 69750.1 | wpb 2047.5 | bsz 4 | num_updates 15440 | best_loss 5.828
2022-03-17 16:05:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 15440 updates
2022-03-17 16:05:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 16:05:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 16:05:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 38 @ 15440 updates, score 5.828) (writing took 1.7603040635585785 seconds)
2022-03-17 16:05:40 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-17 16:05:40 | INFO | train | epoch 038 | loss 5.658 | ppl 50.48 | wps 41824.7 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 15440 | lr 0.000254493 | gnorm 0.488 | loss_scale 16 | train_wall 596 | gb_free 21 | wall 24021
KL Stats: Epoch 38 Divergences: Uniform: 4.653679262045774 Unigram: 4.214625458693797
2022-03-17 16:05:40 | INFO | fairseq.trainer | begin training epoch 39
2022-03-17 16:05:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 16:07:10 | INFO | train_inner | epoch 039:     60 / 407 loss=5.641, ppl=49.91, wps=38055.1, ups=0.58, wpb=65361.9, bsz=127.7, num_updates=15500, lr=0.000254, gnorm=0.49, loss_scale=16, train_wall=146, gb_free=21, wall=24112
2022-03-17 16:09:41 | INFO | train_inner | epoch 039:    160 / 407 loss=5.635, ppl=49.71, wps=43411.3, ups=0.66, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.484, loss_scale=16, train_wall=146, gb_free=21, wall=24263
2022-03-17 16:12:13 | INFO | train_inner | epoch 039:    260 / 407 loss=5.645, ppl=50.06, wps=43337.5, ups=0.66, wpb=65534.2, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.488, loss_scale=16, train_wall=147, gb_free=21, wall=24414
2022-03-17 16:14:44 | INFO | train_inner | epoch 039:    360 / 407 loss=5.666, ppl=50.77, wps=43419.1, ups=0.66, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.49, loss_scale=16, train_wall=146, gb_free=21, wall=24565
2022-03-17 16:15:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 16:16:13 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.817 | ppl 56.37 | wps 69846.5 | wpb 2047.5 | bsz 4 | num_updates 15847 | best_loss 5.817
2022-03-17 16:16:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15847 updates
2022-03-17 16:16:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 16:16:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 16:16:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 39 @ 15847 updates, score 5.817) (writing took 1.8309111557900906 seconds)
2022-03-17 16:16:15 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-17 16:16:15 | INFO | train | epoch 039 | loss 5.646 | ppl 50.07 | wps 41947.2 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 15847 | lr 0.000251204 | gnorm 0.488 | loss_scale 32 | train_wall 596 | gb_free 21 | wall 24656
KL Stats: Epoch 39 Divergences: Uniform: 4.662396303766145 Unigram: 4.223936407976446
2022-03-17 16:16:15 | INFO | fairseq.trainer | begin training epoch 40
2022-03-17 16:16:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 16:17:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 16:17:37 | INFO | train_inner | epoch 040:     54 / 407 loss=5.636, ppl=49.74, wps=37779.9, ups=0.58, wpb=65361.9, bsz=127.7, num_updates=15900, lr=0.000250785, gnorm=0.486, loss_scale=16, train_wall=147, gb_free=21, wall=24738
2022-03-17 16:20:07 | INFO | train_inner | epoch 040:    154 / 407 loss=5.624, ppl=49.31, wps=43446.2, ups=0.66, wpb=65534.2, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.491, loss_scale=16, train_wall=146, gb_free=21, wall=24889
2022-03-17 16:22:38 | INFO | train_inner | epoch 040:    254 / 407 loss=5.63, ppl=49.51, wps=43563.1, ups=0.66, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.488, loss_scale=16, train_wall=146, gb_free=21, wall=25039
2022-03-17 16:25:08 | INFO | train_inner | epoch 040:    354 / 407 loss=5.652, ppl=50.28, wps=43617.4, ups=0.67, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.488, loss_scale=16, train_wall=146, gb_free=21, wall=25189
2022-03-17 16:26:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 16:26:46 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.812 | ppl 56.16 | wps 70353.2 | wpb 2047.5 | bsz 4 | num_updates 16253 | best_loss 5.812
2022-03-17 16:26:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 16253 updates
2022-03-17 16:26:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 16:26:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 16:26:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 40 @ 16253 updates, score 5.812) (writing took 1.7915229192003608 seconds)
2022-03-17 16:26:48 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-17 16:26:48 | INFO | train | epoch 040 | loss 5.635 | ppl 49.68 | wps 42004 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 16253 | lr 0.000248047 | gnorm 0.489 | loss_scale 16 | train_wall 594 | gb_free 21 | wall 25290
KL Stats: Epoch 40 Divergences: Uniform: 4.669823695005393 Unigram: 4.233022143222829
2022-03-17 16:26:48 | INFO | fairseq.trainer | begin training epoch 41
2022-03-17 16:26:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 16:27:59 | INFO | train_inner | epoch 041:     47 / 407 loss=5.626, ppl=49.38, wps=38271.1, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=16300, lr=0.000247689, gnorm=0.487, loss_scale=16, train_wall=145, gb_free=21, wall=25360
2022-03-17 16:30:29 | INFO | train_inner | epoch 041:    147 / 407 loss=5.613, ppl=48.94, wps=43592.4, ups=0.67, wpb=65523.9, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.492, loss_scale=16, train_wall=146, gb_free=21, wall=25511
2022-03-17 16:33:00 | INFO | train_inner | epoch 041:    247 / 407 loss=5.634, ppl=49.66, wps=43573.8, ups=0.66, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.492, loss_scale=32, train_wall=146, gb_free=21, wall=25661
2022-03-17 16:35:30 | INFO | train_inner | epoch 041:    347 / 407 loss=5.631, ppl=49.57, wps=43582, ups=0.67, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.489, loss_scale=32, train_wall=146, gb_free=21, wall=25811
2022-03-17 16:37:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 16:37:19 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 5.807 | ppl 56 | wps 70364 | wpb 2047.5 | bsz 4 | num_updates 16660 | best_loss 5.807
2022-03-17 16:37:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16660 updates
2022-03-17 16:37:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 16:37:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 16:37:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 41 @ 16660 updates, score 5.807) (writing took 1.8253331063315272 seconds)
2022-03-17 16:37:21 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-17 16:37:21 | INFO | train | epoch 041 | loss 5.624 | ppl 49.32 | wps 42144.3 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 16660 | lr 0.000244998 | gnorm 0.49 | loss_scale 32 | train_wall 593 | gb_free 21 | wall 25922
KL Stats: Epoch 41 Divergences: Uniform: 4.678236442888287 Unigram: 4.239571577848062
2022-03-17 16:37:21 | INFO | fairseq.trainer | begin training epoch 42
2022-03-17 16:37:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 16:38:21 | INFO | train_inner | epoch 042:     40 / 407 loss=5.614, ppl=48.96, wps=38255.6, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=16700, lr=0.000244704, gnorm=0.486, loss_scale=32, train_wall=145, gb_free=21, wall=25982
2022-03-17 16:40:51 | INFO | train_inner | epoch 042:    140 / 407 loss=5.602, ppl=48.59, wps=43580.8, ups=0.67, wpb=65534.2, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.492, loss_scale=32, train_wall=146, gb_free=21, wall=26133
2022-03-17 16:42:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 16:43:23 | INFO | train_inner | epoch 042:    241 / 407 loss=5.616, ppl=49.03, wps=43147.1, ups=0.66, wpb=65525.8, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.489, loss_scale=16, train_wall=147, gb_free=21, wall=26284
2022-03-17 16:45:53 | INFO | train_inner | epoch 042:    341 / 407 loss=5.627, ppl=49.4, wps=43675.9, ups=0.67, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.49, loss_scale=16, train_wall=145, gb_free=21, wall=26435
2022-03-17 16:47:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 16:47:51 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 5.802 | ppl 55.78 | wps 70380.6 | wpb 2047.5 | bsz 4 | num_updates 17066 | best_loss 5.802
2022-03-17 16:47:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 17066 updates
2022-03-17 16:47:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 16:47:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 16:47:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 42 @ 17066 updates, score 5.802) (writing took 1.8187728133052588 seconds)
2022-03-17 16:47:53 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-17 16:47:53 | INFO | train | epoch 042 | loss 5.614 | ppl 48.96 | wps 42069 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 17066 | lr 0.000242066 | gnorm 0.49 | loss_scale 16 | train_wall 593 | gb_free 21 | wall 26554
KL Stats: Epoch 42 Divergences: Uniform: 4.685244523922878 Unigram: 4.24736104852346
2022-03-17 16:47:53 | INFO | fairseq.trainer | begin training epoch 43
2022-03-17 16:47:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 16:48:44 | INFO | train_inner | epoch 043:     34 / 407 loss=5.615, ppl=48.99, wps=38302.6, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=17100, lr=0.000241825, gnorm=0.495, loss_scale=16, train_wall=145, gb_free=21, wall=26605
2022-03-17 16:51:14 | INFO | train_inner | epoch 043:    134 / 407 loss=5.588, ppl=48.1, wps=43664.5, ups=0.67, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.492, loss_scale=16, train_wall=146, gb_free=21, wall=26755
2022-03-17 16:53:44 | INFO | train_inner | epoch 043:    234 / 407 loss=5.596, ppl=48.37, wps=43627.4, ups=0.67, wpb=65525.8, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.491, loss_scale=16, train_wall=146, gb_free=21, wall=26905
2022-03-17 16:56:14 | INFO | train_inner | epoch 043:    334 / 407 loss=5.621, ppl=49.22, wps=43654.5, ups=0.67, wpb=65534.2, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.492, loss_scale=32, train_wall=146, gb_free=21, wall=27056
2022-03-17 16:58:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 16:58:22 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 5.801 | ppl 55.77 | wps 70420.5 | wpb 2047.5 | bsz 4 | num_updates 17473 | best_loss 5.801
2022-03-17 16:58:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 17473 updates
2022-03-17 16:58:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 16:58:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 16:58:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 43 @ 17473 updates, score 5.801) (writing took 1.812039096839726 seconds)
2022-03-17 16:58:24 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-17 16:58:24 | INFO | train | epoch 043 | loss 5.604 | ppl 48.65 | wps 42204.4 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 17473 | lr 0.00023923 | gnorm 0.492 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 27186
KL Stats: Epoch 43 Divergences: Uniform: 4.692834397750863 Unigram: 4.2554838446566015
2022-03-17 16:58:24 | INFO | fairseq.trainer | begin training epoch 44
2022-03-17 16:58:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 16:59:05 | INFO | train_inner | epoch 044:     27 / 407 loss=5.609, ppl=48.82, wps=38301.1, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=17500, lr=0.000239046, gnorm=0.489, loss_scale=32, train_wall=145, gb_free=21, wall=27226
2022-03-17 16:59:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 17:01:37 | INFO | train_inner | epoch 044:    128 / 407 loss=5.571, ppl=47.54, wps=43224.5, ups=0.66, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.494, loss_scale=16, train_wall=147, gb_free=21, wall=27378
2022-03-17 17:04:07 | INFO | train_inner | epoch 044:    228 / 407 loss=5.603, ppl=48.61, wps=43617.8, ups=0.67, wpb=65525.8, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.495, loss_scale=16, train_wall=146, gb_free=21, wall=27528
2022-03-17 17:06:37 | INFO | train_inner | epoch 044:    328 / 407 loss=5.6, ppl=48.51, wps=43678.2, ups=0.67, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.497, loss_scale=16, train_wall=146, gb_free=21, wall=27678
2022-03-17 17:08:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 17:08:54 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 5.795 | ppl 55.53 | wps 70287.2 | wpb 2047.5 | bsz 4 | num_updates 17879 | best_loss 5.795
2022-03-17 17:08:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17879 updates
2022-03-17 17:08:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 17:08:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 17:08:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 44 @ 17879 updates, score 5.795) (writing took 1.8576151067391038 seconds)
2022-03-17 17:08:56 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-17 17:08:56 | INFO | train | epoch 044 | loss 5.595 | ppl 48.33 | wps 42099 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 17879 | lr 0.000236498 | gnorm 0.495 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 27817
KL Stats: Epoch 44 Divergences: Uniform: 4.701756313816599 Unigram: 4.263749380933316
2022-03-17 17:08:56 | INFO | fairseq.trainer | begin training epoch 45
2022-03-17 17:08:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 17:09:28 | INFO | train_inner | epoch 045:     21 / 407 loss=5.603, ppl=48.6, wps=38294.9, ups=0.59, wpb=65368.5, bsz=127.7, num_updates=17900, lr=0.00023636, gnorm=0.497, loss_scale=16, train_wall=145, gb_free=21, wall=27849
2022-03-17 17:11:58 | INFO | train_inner | epoch 045:    121 / 407 loss=5.567, ppl=47.39, wps=43665.9, ups=0.67, wpb=65525.8, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.493, loss_scale=16, train_wall=146, gb_free=21, wall=27999
2022-03-17 17:14:28 | INFO | train_inner | epoch 045:    221 / 407 loss=5.581, ppl=47.87, wps=43629.5, ups=0.67, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.494, loss_scale=32, train_wall=146, gb_free=21, wall=28149
2022-03-17 17:16:58 | INFO | train_inner | epoch 045:    321 / 407 loss=5.599, ppl=48.48, wps=43663.3, ups=0.67, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.491, loss_scale=32, train_wall=146, gb_free=21, wall=28299
2022-03-17 17:19:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 17:19:26 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 5.79 | ppl 55.33 | wps 70450.7 | wpb 2047.5 | bsz 4 | num_updates 18286 | best_loss 5.79
2022-03-17 17:19:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 18286 updates
2022-03-17 17:19:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 17:19:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 17:19:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 45 @ 18286 updates, score 5.79) (writing took 1.8382710227742791 seconds)
2022-03-17 17:19:27 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-17 17:19:27 | INFO | train | epoch 045 | loss 5.586 | ppl 48.04 | wps 42210.3 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 18286 | lr 0.000233852 | gnorm 0.494 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 28449
KL Stats: Epoch 45 Divergences: Uniform: 4.708503935021763 Unigram: 4.2702451283957465
2022-03-17 17:19:27 | INFO | fairseq.trainer | begin training epoch 46
2022-03-17 17:19:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 17:19:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 17:19:50 | INFO | train_inner | epoch 046:     15 / 407 loss=5.601, ppl=48.55, wps=37982.3, ups=0.58, wpb=65372.2, bsz=127.7, num_updates=18300, lr=0.000233762, gnorm=0.498, loss_scale=16, train_wall=147, gb_free=21, wall=28471
2022-03-17 17:22:20 | INFO | train_inner | epoch 046:    115 / 407 loss=5.556, ppl=47.05, wps=43667.6, ups=0.67, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.493, loss_scale=16, train_wall=146, gb_free=21, wall=28621
2022-03-17 17:24:50 | INFO | train_inner | epoch 046:    215 / 407 loss=5.571, ppl=47.53, wps=43643.3, ups=0.67, wpb=65525.8, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.501, loss_scale=16, train_wall=146, gb_free=21, wall=28772
2022-03-17 17:27:20 | INFO | train_inner | epoch 046:    315 / 407 loss=5.591, ppl=48.21, wps=43666.1, ups=0.67, wpb=65534.2, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.502, loss_scale=16, train_wall=146, gb_free=21, wall=28922
2022-03-17 17:29:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 17:29:57 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 5.787 | ppl 55.22 | wps 70513.3 | wpb 2047.5 | bsz 4 | num_updates 18692 | best_loss 5.787
2022-03-17 17:29:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18692 updates
2022-03-17 17:29:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 17:29:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 17:29:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 46 @ 18692 updates, score 5.787) (writing took 1.8851369684562087 seconds)
2022-03-17 17:29:59 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-17 17:29:59 | INFO | train | epoch 046 | loss 5.577 | ppl 47.73 | wps 42106.6 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 18692 | lr 0.000231298 | gnorm 0.498 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 29080
KL Stats: Epoch 46 Divergences: Uniform: 4.714385770525949 Unigram: 4.275627523177674
2022-03-17 17:29:59 | INFO | fairseq.trainer | begin training epoch 47
2022-03-17 17:29:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 17:30:11 | INFO | train_inner | epoch 047:      8 / 407 loss=5.59, ppl=48.15, wps=38298.7, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=0.497, loss_scale=16, train_wall=145, gb_free=21, wall=29092
2022-03-17 17:32:41 | INFO | train_inner | epoch 047:    108 / 407 loss=5.536, ppl=46.39, wps=43666.7, ups=0.67, wpb=65534.2, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.501, loss_scale=16, train_wall=146, gb_free=21, wall=29242
2022-03-17 17:33:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 17:35:13 | INFO | train_inner | epoch 047:    209 / 407 loss=5.567, ppl=47.41, wps=43196.6, ups=0.66, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.501, loss_scale=16, train_wall=147, gb_free=21, wall=29394
2022-03-17 17:37:43 | INFO | train_inner | epoch 047:    309 / 407 loss=5.584, ppl=47.97, wps=43680.9, ups=0.67, wpb=65525.8, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.502, loss_scale=16, train_wall=145, gb_free=21, wall=29544
2022-03-17 17:40:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 17:40:29 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 5.78 | ppl 54.94 | wps 70359.1 | wpb 2047.5 | bsz 4 | num_updates 19098 | best_loss 5.78
2022-03-17 17:40:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 19098 updates
2022-03-17 17:40:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 17:40:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 17:40:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 47 @ 19098 updates, score 5.78) (writing took 1.875622820109129 seconds)
2022-03-17 17:40:30 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-17 17:40:30 | INFO | train | epoch 047 | loss 5.569 | ppl 47.46 | wps 42100.7 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 19098 | lr 0.000228826 | gnorm 0.501 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 29712
KL Stats: Epoch 47 Divergences: Uniform: 4.720414331344401 Unigram: 4.283700746763379
2022-03-17 17:40:30 | INFO | fairseq.trainer | begin training epoch 48
2022-03-17 17:40:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 17:40:34 | INFO | train_inner | epoch 048:      2 / 407 loss=5.586, ppl=48.04, wps=38282.8, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=0.5, loss_scale=16, train_wall=145, gb_free=21, wall=29715
2022-03-17 17:43:04 | INFO | train_inner | epoch 048:    102 / 407 loss=5.536, ppl=46.39, wps=43643.1, ups=0.67, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.501, loss_scale=16, train_wall=146, gb_free=21, wall=29865
2022-03-17 17:45:34 | INFO | train_inner | epoch 048:    202 / 407 loss=5.552, ppl=46.92, wps=43610, ups=0.67, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.498, loss_scale=16, train_wall=146, gb_free=21, wall=30015
2022-03-17 17:48:04 | INFO | train_inner | epoch 048:    302 / 407 loss=5.574, ppl=47.64, wps=43660.4, ups=0.67, wpb=65523.9, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.497, loss_scale=32, train_wall=146, gb_free=21, wall=30165
2022-03-17 17:48:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 17:50:36 | INFO | train_inner | epoch 048:    403 / 407 loss=5.584, ppl=47.95, wps=43234.6, ups=0.66, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.502, loss_scale=16, train_wall=147, gb_free=21, wall=30317
2022-03-17 17:50:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 17:51:00 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 5.774 | ppl 54.71 | wps 70528.8 | wpb 2047.5 | bsz 4 | num_updates 19504 | best_loss 5.774
2022-03-17 17:51:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 19504 updates
2022-03-17 17:51:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 17:51:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 17:51:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 48 @ 19504 updates, score 5.774) (writing took 1.8553114980459213 seconds)
2022-03-17 17:51:02 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-17 17:51:02 | INFO | train | epoch 048 | loss 5.561 | ppl 47.2 | wps 42095.9 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 19504 | lr 0.000226432 | gnorm 0.5 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 30343
KL Stats: Epoch 48 Divergences: Uniform: 4.725492138249447 Unigram: 4.2879436832907345
2022-03-17 17:51:02 | INFO | fairseq.trainer | begin training epoch 49
2022-03-17 17:51:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 17:53:26 | INFO | train_inner | epoch 049:     96 / 407 loss=5.529, ppl=46.18, wps=38324, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=19600, lr=0.000225877, gnorm=0.496, loss_scale=16, train_wall=145, gb_free=21, wall=30488
2022-03-17 17:55:56 | INFO | train_inner | epoch 049:    196 / 407 loss=5.554, ppl=46.98, wps=43630.4, ups=0.67, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.5, loss_scale=16, train_wall=146, gb_free=21, wall=30638
2022-03-17 17:58:27 | INFO | train_inner | epoch 049:    296 / 407 loss=5.561, ppl=47.2, wps=43669.2, ups=0.67, wpb=65534.2, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.496, loss_scale=16, train_wall=146, gb_free=21, wall=30788
2022-03-17 18:00:57 | INFO | train_inner | epoch 049:    396 / 407 loss=5.566, ppl=47.37, wps=43660.4, ups=0.67, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.499, loss_scale=16, train_wall=146, gb_free=21, wall=30938
2022-03-17 18:01:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 18:01:32 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 5.771 | ppl 54.62 | wps 70290 | wpb 2047.5 | bsz 4 | num_updates 19911 | best_loss 5.771
2022-03-17 18:01:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19911 updates
2022-03-17 18:01:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 18:01:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 18:01:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 49 @ 19911 updates, score 5.771) (writing took 1.8205467257648706 seconds)
2022-03-17 18:01:34 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-17 18:01:34 | INFO | train | epoch 049 | loss 5.553 | ppl 46.94 | wps 42208.4 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 19911 | lr 0.000224106 | gnorm 0.497 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 30975
KL Stats: Epoch 49 Divergences: Uniform: 4.7330733098298206 Unigram: 4.294254418790548
2022-03-17 18:01:34 | INFO | fairseq.trainer | begin training epoch 50
2022-03-17 18:01:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 18:03:47 | INFO | train_inner | epoch 050:     89 / 407 loss=5.527, ppl=46.11, wps=38257.1, ups=0.59, wpb=65360.1, bsz=127.7, num_updates=20000, lr=0.000223607, gnorm=0.498, loss_scale=32, train_wall=145, gb_free=21, wall=31109
2022-03-17 18:06:18 | INFO | train_inner | epoch 050:    189 / 407 loss=5.535, ppl=46.36, wps=43607, ups=0.67, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.491, loss_scale=32, train_wall=146, gb_free=21, wall=31259
2022-03-17 18:08:48 | INFO | train_inner | epoch 050:    289 / 407 loss=5.559, ppl=47.15, wps=43630.9, ups=0.67, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.507, loss_scale=32, train_wall=146, gb_free=21, wall=31409
2022-03-17 18:11:18 | INFO | train_inner | epoch 050:    389 / 407 loss=5.559, ppl=47.13, wps=43658, ups=0.67, wpb=65525.8, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.504, loss_scale=32, train_wall=146, gb_free=21, wall=31559
2022-03-17 18:11:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 18:12:04 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 5.769 | ppl 54.53 | wps 70390.4 | wpb 2047.5 | bsz 4 | num_updates 20318 | best_loss 5.769
2022-03-17 18:12:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 20318 updates
2022-03-17 18:12:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 18:12:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 18:12:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 50 @ 20318 updates, score 5.769) (writing took 1.8049523392692208 seconds)
2022-03-17 18:12:06 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-17 18:12:06 | INFO | train | epoch 050 | loss 5.545 | ppl 46.7 | wps 42185.4 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 20318 | lr 0.00022185 | gnorm 0.5 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 31607
KL Stats: Epoch 50 Divergences: Uniform: 4.7387339101100245 Unigram: 4.301719996138511
2022-03-17 18:12:06 | INFO | fairseq.trainer | begin training epoch 51
2022-03-17 18:12:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 18:13:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 18:14:10 | INFO | train_inner | epoch 051:     83 / 407 loss=5.519, ppl=45.85, wps=37965.1, ups=0.58, wpb=65372.2, bsz=127.7, num_updates=20400, lr=0.000221404, gnorm=0.502, loss_scale=16, train_wall=147, gb_free=21, wall=31732
2022-03-17 18:16:41 | INFO | train_inner | epoch 051:    183 / 407 loss=5.528, ppl=46.14, wps=43601.1, ups=0.67, wpb=65536, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.502, loss_scale=16, train_wall=146, gb_free=21, wall=31882
2022-03-17 18:19:11 | INFO | train_inner | epoch 051:    283 / 407 loss=5.545, ppl=46.68, wps=43625.4, ups=0.67, wpb=65525.8, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.5, loss_scale=16, train_wall=146, gb_free=21, wall=32032
2022-03-17 18:21:41 | INFO | train_inner | epoch 051:    383 / 407 loss=5.554, ppl=46.99, wps=43658.6, ups=0.67, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.5, loss_scale=16, train_wall=146, gb_free=21, wall=32182
2022-03-17 18:22:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 18:22:36 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 5.765 | ppl 54.39 | wps 70312.9 | wpb 2047.5 | bsz 4 | num_updates 20724 | best_loss 5.765
2022-03-17 18:22:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20724 updates
2022-03-17 18:22:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 18:22:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 18:22:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 51 @ 20724 updates, score 5.765) (writing took 1.8170617241412401 seconds)
2022-03-17 18:22:37 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-17 18:22:37 | INFO | train | epoch 051 | loss 5.538 | ppl 46.45 | wps 42079.4 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 20724 | lr 0.000219666 | gnorm 0.501 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 32239
KL Stats: Epoch 51 Divergences: Uniform: 4.745900334765886 Unigram: 4.308162350486265
2022-03-17 18:22:37 | INFO | fairseq.trainer | begin training epoch 52
2022-03-17 18:22:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 18:24:32 | INFO | train_inner | epoch 052:     76 / 407 loss=5.513, ppl=45.66, wps=38296.1, ups=0.59, wpb=65368.5, bsz=127.7, num_updates=20800, lr=0.000219265, gnorm=0.504, loss_scale=16, train_wall=145, gb_free=21, wall=32353
2022-03-17 18:27:02 | INFO | train_inner | epoch 052:    176 / 407 loss=5.531, ppl=46.23, wps=43652, ups=0.67, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.498, loss_scale=32, train_wall=146, gb_free=21, wall=32503
2022-03-17 18:27:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 18:29:33 | INFO | train_inner | epoch 052:    277 / 407 loss=5.54, ppl=46.52, wps=43225.1, ups=0.66, wpb=65525.8, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.497, loss_scale=16, train_wall=147, gb_free=21, wall=32655
2022-03-17 18:32:03 | INFO | train_inner | epoch 052:    377 / 407 loss=5.546, ppl=46.72, wps=43665.3, ups=0.67, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.505, loss_scale=16, train_wall=146, gb_free=21, wall=32805
2022-03-17 18:32:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 18:33:07 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 5.765 | ppl 54.37 | wps 70418.4 | wpb 2047.5 | bsz 4 | num_updates 21130 | best_loss 5.765
2022-03-17 18:33:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 21130 updates
2022-03-17 18:33:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 18:33:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 18:33:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 52 @ 21130 updates, score 5.765) (writing took 1.8203444881364703 seconds)
2022-03-17 18:33:09 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-17 18:33:09 | INFO | train | epoch 052 | loss 5.531 | ppl 46.23 | wps 42111.5 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 21130 | lr 0.000217546 | gnorm 0.502 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 32870
KL Stats: Epoch 52 Divergences: Uniform: 4.751638186782549 Unigram: 4.31515868713591
2022-03-17 18:33:09 | INFO | fairseq.trainer | begin training epoch 53
2022-03-17 18:33:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 18:34:54 | INFO | train_inner | epoch 053:     70 / 407 loss=5.511, ppl=45.61, wps=38304.4, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=21200, lr=0.000217186, gnorm=0.5, loss_scale=16, train_wall=145, gb_free=21, wall=32975
2022-03-17 18:37:24 | INFO | train_inner | epoch 053:    170 / 407 loss=5.515, ppl=45.73, wps=43641.8, ups=0.67, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.503, loss_scale=16, train_wall=146, gb_free=21, wall=33126
2022-03-17 18:39:54 | INFO | train_inner | epoch 053:    270 / 407 loss=5.525, ppl=46.06, wps=43613.6, ups=0.67, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.504, loss_scale=16, train_wall=146, gb_free=21, wall=33276
2022-03-17 18:41:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 18:42:26 | INFO | train_inner | epoch 053:    371 / 407 loss=5.546, ppl=46.72, wps=43225.9, ups=0.66, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.51, loss_scale=16, train_wall=147, gb_free=21, wall=33427
2022-03-17 18:43:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 18:43:39 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 5.756 | ppl 54.06 | wps 70455.4 | wpb 2047.5 | bsz 4 | num_updates 21536 | best_loss 5.756
2022-03-17 18:43:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 21536 updates
2022-03-17 18:43:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 18:43:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt
2022-03-17 18:43:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_best.pt (epoch 53 @ 21536 updates, score 5.756) (writing took 1.8050458561629057 seconds)
2022-03-17 18:43:41 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-17 18:43:41 | INFO | train | epoch 053 | loss 5.525 | ppl 46.03 | wps 42089.7 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 21536 | lr 0.000215485 | gnorm 0.504 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 33502
KL Stats: Epoch 53 Divergences: Uniform: 4.755619069063317 Unigram: 4.316594654143958
2022-03-17 18:43:41 | INFO | fairseq.trainer | begin training epoch 54
2022-03-17 18:43:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 18:45:17 | INFO | train_inner | epoch 054:     64 / 407 loss=5.512, ppl=45.64, wps=38304.8, ups=0.59, wpb=65361.9, bsz=127.7, num_updates=21600, lr=0.000215166, gnorm=0.506, loss_scale=16, train_wall=145, gb_free=21, wall=33598
2022-03-17 18:47:47 | INFO | train_inner | epoch 054:    164 / 407 loss=5.509, ppl=45.53, wps=43651.9, ups=0.67, wpb=65534.2, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.505, loss_scale=16, train_wall=146, gb_free=21, wall=33748
2022-03-17 18:50:17 | INFO | train_inner | epoch 054:    264 / 407 loss=5.519, ppl=45.86, wps=43641, ups=0.67, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.505, loss_scale=16, train_wall=146, gb_free=21, wall=33898
2022-03-17 18:52:47 | INFO | train_inner | epoch 054:    364 / 407 loss=5.531, ppl=46.25, wps=43678.1, ups=0.67, wpb=65525.8, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.501, loss_scale=16, train_wall=145, gb_free=21, wall=34048
2022-03-17 18:53:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 18:54:10 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 5.759 | ppl 54.14 | wps 70317.6 | wpb 2047.5 | bsz 4 | num_updates 21943 | best_loss 5.756
2022-03-17 18:54:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21943 updates
2022-03-17 18:54:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_last.pt
2022-03-17 18:54:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_last.pt
2022-03-17 18:54:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_last.pt (epoch 54 @ 21943 updates, score 5.759) (writing took 0.8847895422950387 seconds)
2022-03-17 18:54:11 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-17 18:54:11 | INFO | train | epoch 054 | loss 5.518 | ppl 45.81 | wps 42271.6 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 21943 | lr 0.000213477 | gnorm 0.504 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 34133
KL Stats: Epoch 54 Divergences: Uniform: 4.762078972326808 Unigram: 4.3244178300804865
2022-03-17 18:54:11 | INFO | fairseq.trainer | begin training epoch 55
2022-03-17 18:54:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 18:55:37 | INFO | train_inner | epoch 055:     57 / 407 loss=5.511, ppl=45.6, wps=38506.8, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=22000, lr=0.000213201, gnorm=0.504, loss_scale=32, train_wall=145, gb_free=21, wall=34218
2022-03-17 18:56:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 18:58:08 | INFO | train_inner | epoch 055:    158 / 407 loss=5.495, ppl=45.08, wps=43236.7, ups=0.66, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.504, loss_scale=16, train_wall=147, gb_free=21, wall=34370
2022-03-17 19:00:39 | INFO | train_inner | epoch 055:    258 / 407 loss=5.516, ppl=45.76, wps=43639.9, ups=0.67, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.508, loss_scale=16, train_wall=146, gb_free=21, wall=34520
2022-03-17 19:03:09 | INFO | train_inner | epoch 055:    358 / 407 loss=5.527, ppl=46.09, wps=43656.5, ups=0.67, wpb=65525.8, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.5, loss_scale=16, train_wall=146, gb_free=21, wall=34670
2022-03-17 19:04:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 19:04:41 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 5.757 | ppl 54.09 | wps 70295.8 | wpb 2047.5 | bsz 4 | num_updates 22349 | best_loss 5.756
2022-03-17 19:04:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 22349 updates
2022-03-17 19:04:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_last.pt
2022-03-17 19:04:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_last.pt
2022-03-17 19:04:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_last.pt (epoch 55 @ 22349 updates, score 5.757) (writing took 0.8697995273396373 seconds)
2022-03-17 19:04:42 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-17 19:04:42 | INFO | train | epoch 055 | loss 5.511 | ppl 45.61 | wps 42168.9 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 22349 | lr 0.00021153 | gnorm 0.504 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 34763
KL Stats: Epoch 55 Divergences: Uniform: 4.7675231472543596 Unigram: 4.329224689058702
2022-03-17 19:04:42 | INFO | fairseq.trainer | begin training epoch 56
2022-03-17 19:04:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 19:05:58 | INFO | train_inner | epoch 056:     51 / 407 loss=5.497, ppl=45.17, wps=38527, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=22400, lr=0.000211289, gnorm=0.502, loss_scale=16, train_wall=145, gb_free=21, wall=34840
2022-03-17 19:08:28 | INFO | train_inner | epoch 056:    151 / 407 loss=5.483, ppl=44.72, wps=43667.3, ups=0.67, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.506, loss_scale=16, train_wall=146, gb_free=21, wall=34990
2022-03-17 19:10:59 | INFO | train_inner | epoch 056:    251 / 407 loss=5.514, ppl=45.69, wps=43625.9, ups=0.67, wpb=65523.9, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.506, loss_scale=32, train_wall=146, gb_free=21, wall=35140
2022-03-17 19:11:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 19:13:30 | INFO | train_inner | epoch 056:    352 / 407 loss=5.526, ppl=46.08, wps=43250, ups=0.66, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.507, loss_scale=16, train_wall=147, gb_free=21, wall=35291
2022-03-17 19:14:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 19:15:11 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 5.757 | ppl 54.06 | wps 70612.4 | wpb 2047.5 | bsz 4 | num_updates 22755 | best_loss 5.756
2022-03-17 19:15:11 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-17 19:15:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 22755 updates
2022-03-17 19:15:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_last.pt
2022-03-17 19:15:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_last.pt
2022-03-17 19:15:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.0225_0.0025_0.975/checkpoint_last.pt (epoch 56 @ 22755 updates, score 5.757) (writing took 0.8673871392384171 seconds)
2022-03-17 19:15:12 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-17 19:15:12 | INFO | train | epoch 056 | loss 5.505 | ppl 45.42 | wps 42174.5 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 22755 | lr 0.000209634 | gnorm 0.506 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 35394
2022-03-17 19:15:12 | INFO | fairseq_cli.train | done training in 35393.5 seconds
