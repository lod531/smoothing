Sender: LSF System <lsfadmin@eu-g3-075>
Subject: Job 207526044: <w103_fp16_size_0.125_jelinek_0.04_0.06_0.9_#4> in cluster <euler> Exited

Job <w103_fp16_size_0.125_jelinek_0.04_0.06_0.9_#4> was submitted from host <eu-login-27> by user <andriusb> in cluster <euler> at Tue Mar  8 08:10:33 2022
Job was executed on host(s) <eu-g3-075>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Tue Mar  8 08:10:45 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar  8 08:10:45 2022
Terminated at Tue Mar  8 16:28:05 2022
Results reported at Tue Mar  8 16:28:05 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.04, 0.06, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --seed 1321664 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   29821.57 sec.
    Max Memory :                                 7174 MB
    Average Memory :                             4487.98 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               12826.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   29840 sec.
    Turnaround time :                            29852 sec.

The output (if any) follows:

2022-03-08 08:10:51 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321664, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321664, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.04, 0.06, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-08 08:10:52 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-08 08:10:54 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
Calculating frequency stats:
  0%|          | 0/225169 [00:00<?, ?it/s]  0%|          | 689/225169 [00:00<00:32, 6874.34it/s]  1%|          | 1377/225169 [00:00<00:36, 6121.16it/s]  1%|          | 1995/225169 [00:00<00:38, 5818.10it/s]  1%|          | 2597/225169 [00:00<00:37, 5889.04it/s]  1%|▏         | 3272/225169 [00:00<00:35, 6178.46it/s]  2%|▏         | 3944/225169 [00:00<00:34, 6354.05it/s]  2%|▏         | 4636/225169 [00:00<00:33, 6527.90it/s]  2%|▏         | 5351/225169 [00:00<00:32, 6719.72it/s]  3%|▎         | 6075/225169 [00:00<00:31, 6868.15it/s]  3%|▎         | 6764/225169 [00:01<00:35, 6223.47it/s]  3%|▎         | 7399/225169 [00:01<00:34, 6237.26it/s]  4%|▎         | 8032/225169 [00:01<00:35, 6068.02it/s]  4%|▍         | 8645/225169 [00:01<00:36, 5993.66it/s]  4%|▍         | 9272/225169 [00:01<00:35, 6072.35it/s]  4%|▍         | 9909/225169 [00:01<00:34, 6158.41it/s]  5%|▍         | 10528/225169 [00:01<00:35, 6009.06it/s]  5%|▍         | 11151/225169 [00:01<00:35, 6068.58it/s]  5%|▌         | 11760/225169 [00:01<00:36, 5926.71it/s]  6%|▌         | 12435/225169 [00:02<00:34, 6162.08it/s]  6%|▌         | 13054/225169 [00:02<00:34, 6120.02it/s]  6%|▌         | 13756/225169 [00:02<00:33, 6378.27it/s]  6%|▋         | 14396/225169 [00:02<00:33, 6341.78it/s]  7%|▋         | 15053/225169 [00:02<00:32, 6408.70it/s]  7%|▋         | 15695/225169 [00:02<00:33, 6236.32it/s]  7%|▋         | 16321/225169 [00:02<00:34, 6058.38it/s]  8%|▊         | 16929/225169 [00:02<00:34, 5965.35it/s]  8%|▊         | 17550/225169 [00:02<00:34, 6034.57it/s]  8%|▊         | 18173/225169 [00:02<00:33, 6090.96it/s]  8%|▊         | 18803/225169 [00:03<00:33, 6142.20it/s]  9%|▊         | 19581/225169 [00:03<00:31, 6625.70it/s]  9%|▉         | 20245/225169 [00:03<00:32, 6224.66it/s]  9%|▉         | 20874/225169 [00:03<00:32, 6233.90it/s] 10%|▉         | 21502/225169 [00:03<00:34, 5911.42it/s] 10%|▉         | 22135/225169 [00:03<00:33, 6024.40it/s] 10%|█         | 22840/225169 [00:03<00:32, 6314.98it/s] 10%|█         | 23497/225169 [00:03<00:31, 6386.22it/s] 11%|█         | 24256/225169 [00:03<00:29, 6737.20it/s] 11%|█         | 24960/225169 [00:03<00:29, 6815.19it/s] 11%|█▏        | 25644/225169 [00:04<00:29, 6742.71it/s] 12%|█▏        | 26321/225169 [00:04<00:31, 6347.12it/s] 12%|█▏        | 26962/225169 [00:04<00:32, 6187.80it/s] 12%|█▏        | 27585/225169 [00:04<00:33, 5975.72it/s] 13%|█▎        | 28234/225169 [00:04<00:32, 6115.42it/s] 13%|█▎        | 28917/225169 [00:04<00:31, 6314.53it/s] 13%|█▎        | 29552/225169 [00:04<00:31, 6155.27it/s] 13%|█▎        | 30184/225169 [00:04<00:31, 6199.64it/s] 14%|█▎        | 30807/225169 [00:04<00:33, 5875.98it/s] 14%|█▍        | 31432/225169 [00:05<00:32, 5980.96it/s] 14%|█▍        | 32034/225169 [00:05<00:33, 5849.88it/s] 14%|█▍        | 32633/225169 [00:05<00:32, 5885.36it/s] 15%|█▍        | 33224/225169 [00:05<00:33, 5755.47it/s] 15%|█▌        | 33854/225169 [00:05<00:32, 5908.03it/s] 15%|█▌        | 34532/225169 [00:05<00:30, 6155.16it/s] 16%|█▌        | 35150/225169 [00:05<00:31, 6053.18it/s] 16%|█▌        | 35761/225169 [00:05<00:31, 6065.91it/s] 16%|█▌        | 36369/225169 [00:05<00:31, 6020.52it/s] 16%|█▋        | 36972/225169 [00:05<00:32, 5815.97it/s] 17%|█▋        | 37556/225169 [00:06<00:33, 5651.03it/s] 17%|█▋        | 38176/225169 [00:06<00:32, 5802.91it/s] 17%|█▋        | 38765/225169 [00:06<00:31, 5827.68it/s] 17%|█▋        | 39354/225169 [00:06<00:31, 5845.58it/s] 18%|█▊        | 39986/225169 [00:06<00:30, 5978.97it/s] 18%|█▊        | 40677/225169 [00:06<00:29, 6251.57it/s] 18%|█▊        | 41319/225169 [00:06<00:29, 6296.23it/s] 19%|█▊        | 41950/225169 [00:06<00:31, 5889.73it/s] 19%|█▉        | 42545/225169 [00:06<00:31, 5790.20it/s] 19%|█▉        | 43128/225169 [00:07<00:31, 5750.45it/s] 19%|█▉        | 43820/225169 [00:07<00:29, 6082.99it/s] 20%|█▉        | 44432/225169 [00:07<00:30, 6010.37it/s] 20%|██        | 45054/225169 [00:07<00:29, 6069.34it/s] 20%|██        | 45703/225169 [00:07<00:29, 6187.68it/s] 21%|██        | 46444/225169 [00:07<00:27, 6545.27it/s] 21%|██        | 47101/225169 [00:07<00:27, 6401.55it/s] 21%|██▏       | 48068/225169 [00:07<00:24, 7358.11it/s] 22%|██▏       | 48808/225169 [00:07<00:24, 7090.28it/s] 22%|██▏       | 49574/225169 [00:07<00:24, 7249.61it/s] 22%|██▏       | 50303/225169 [00:08<00:26, 6626.39it/s] 23%|██▎       | 50978/225169 [00:08<00:27, 6407.12it/s] 23%|██▎       | 51628/225169 [00:08<00:27, 6413.98it/s] 23%|██▎       | 52465/225169 [00:08<00:24, 6961.67it/s] 24%|██▎       | 53170/225169 [00:08<00:25, 6683.73it/s] 24%|██▍       | 53846/225169 [00:08<00:26, 6526.93it/s] 24%|██▍       | 54504/225169 [00:08<00:28, 6090.59it/s] 24%|██▍       | 55141/225169 [00:08<00:27, 6161.25it/s] 25%|██▍       | 55810/225169 [00:08<00:26, 6307.29it/s] 25%|██▌       | 56446/225169 [00:09<00:27, 6164.17it/s] 25%|██▌       | 57067/225169 [00:09<00:28, 5832.47it/s] 26%|██▌       | 57657/225169 [00:09<00:28, 5841.61it/s] 26%|██▌       | 58310/225169 [00:09<00:27, 6029.56it/s] 26%|██▌       | 59027/225169 [00:09<00:26, 6350.01it/s] 26%|██▋       | 59666/225169 [00:09<00:26, 6189.26it/s] 27%|██▋       | 60289/225169 [00:09<00:27, 6089.36it/s] 27%|██▋       | 61017/225169 [00:09<00:25, 6430.35it/s] 27%|██▋       | 61676/225169 [00:09<00:25, 6472.59it/s] 28%|██▊       | 62326/225169 [00:10<00:25, 6284.68it/s] 28%|██▊       | 62964/225169 [00:10<00:25, 6312.01it/s] 28%|██▊       | 63598/225169 [00:10<00:25, 6245.53it/s] 29%|██▊       | 64224/225169 [00:10<00:26, 6105.00it/s] 29%|██▉       | 64903/225169 [00:10<00:25, 6301.35it/s] 29%|██▉       | 65535/225169 [00:10<00:26, 6079.25it/s] 29%|██▉       | 66164/225169 [00:10<00:25, 6135.02it/s] 30%|██▉       | 66914/225169 [00:10<00:24, 6531.68it/s] 30%|███       | 67570/225169 [00:10<00:25, 6065.92it/s] 30%|███       | 68185/225169 [00:10<00:26, 5922.31it/s] 31%|███       | 68943/225169 [00:11<00:24, 6373.21it/s] 31%|███       | 69588/225169 [00:11<00:24, 6345.00it/s] 31%|███       | 70241/225169 [00:11<00:24, 6389.11it/s] 31%|███▏      | 70884/225169 [00:11<00:25, 6164.13it/s] 32%|███▏      | 71530/225169 [00:11<00:24, 6248.68it/s] 32%|███▏      | 72163/225169 [00:11<00:24, 6268.17it/s] 32%|███▏      | 72905/225169 [00:11<00:23, 6602.91it/s] 33%|███▎      | 73611/225169 [00:11<00:22, 6729.09it/s] 33%|███▎      | 74468/225169 [00:11<00:20, 7268.03it/s] 33%|███▎      | 75197/225169 [00:12<00:22, 6815.95it/s] 34%|███▎      | 75886/225169 [00:12<00:22, 6624.98it/s] 34%|███▍      | 76558/225169 [00:12<00:22, 6651.73it/s] 34%|███▍      | 77227/225169 [00:12<00:23, 6370.81it/s] 35%|███▍      | 77941/225169 [00:12<00:22, 6587.56it/s] 35%|███▍      | 78605/225169 [00:12<00:23, 6151.62it/s] 35%|███▌      | 79228/225169 [00:12<00:25, 5812.53it/s] 35%|███▌      | 79825/225169 [00:12<00:24, 5853.83it/s] 36%|███▌      | 80432/225169 [00:12<00:24, 5908.57it/s] 36%|███▌      | 81098/225169 [00:13<00:23, 6119.47it/s] 36%|███▋      | 81747/225169 [00:13<00:23, 6225.73it/s] 37%|███▋      | 82377/225169 [00:13<00:22, 6241.68it/s] 37%|███▋      | 83022/225169 [00:13<00:22, 6300.13it/s] 37%|███▋      | 83714/225169 [00:13<00:21, 6469.66it/s] 37%|███▋      | 84363/225169 [00:13<00:22, 6229.25it/s] 38%|███▊      | 85006/225169 [00:13<00:22, 6286.52it/s] 38%|███▊      | 85671/225169 [00:13<00:21, 6391.51it/s] 38%|███▊      | 86312/225169 [00:13<00:22, 6285.60it/s] 39%|███▊      | 87023/225169 [00:13<00:21, 6525.74it/s] 39%|███▉      | 87678/225169 [00:14<00:21, 6385.72it/s] 39%|███▉      | 88319/225169 [00:14<00:21, 6378.02it/s] 40%|███▉      | 88958/225169 [00:14<00:22, 5976.17it/s] 40%|███▉      | 89561/225169 [00:14<00:22, 5971.45it/s] 40%|████      | 90162/225169 [00:14<00:23, 5836.97it/s] 40%|████      | 90749/225169 [00:14<00:23, 5818.69it/s] 41%|████      | 91434/225169 [00:14<00:21, 6114.93it/s] 41%|████      | 92075/225169 [00:14<00:21, 6200.83it/s] 41%|████      | 92779/225169 [00:14<00:20, 6440.01it/s] 41%|████▏     | 93425/225169 [00:14<00:21, 6170.17it/s] 42%|████▏     | 94046/225169 [00:15<00:21, 6132.21it/s] 42%|████▏     | 94755/225169 [00:15<00:20, 6408.67it/s] 42%|████▏     | 95399/225169 [00:15<00:20, 6318.75it/s] 43%|████▎     | 96033/225169 [00:15<00:20, 6230.96it/s] 43%|████▎     | 96904/225169 [00:15<00:18, 6947.37it/s] 43%|████▎     | 97602/225169 [00:15<00:19, 6429.13it/s] 44%|████▎     | 98292/225169 [00:15<00:19, 6556.85it/s] 44%|████▍     | 98956/225169 [00:15<00:20, 6287.29it/s] 44%|████▍     | 99592/225169 [00:15<00:20, 6121.47it/s] 45%|████▍     | 100209/225169 [00:16<00:20, 6010.36it/s] 45%|████▍     | 100823/225169 [00:16<00:20, 6046.62it/s] 45%|████▌     | 101431/225169 [00:16<00:20, 6011.72it/s] 45%|████▌     | 102137/225169 [00:16<00:19, 6310.90it/s] 46%|████▌     | 102771/225169 [00:16<00:19, 6239.70it/s] 46%|████▌     | 103397/225169 [00:16<00:19, 6170.28it/s] 46%|████▌     | 104016/225169 [00:16<00:20, 5916.93it/s] 46%|████▋     | 104666/225169 [00:16<00:19, 6082.00it/s] 47%|████▋     | 105277/225169 [00:16<00:19, 6042.92it/s] 47%|████▋     | 105961/225169 [00:16<00:19, 6271.76it/s] 47%|████▋     | 106591/225169 [00:17<00:19, 6124.44it/s] 48%|████▊     | 107206/225169 [00:17<00:19, 5959.12it/s] 48%|████▊     | 107843/225169 [00:17<00:19, 6071.90it/s] 48%|████▊     | 108463/225169 [00:17<00:19, 6107.30it/s] 48%|████▊     | 109076/225169 [00:17<00:19, 5856.79it/s] 49%|████▊     | 109665/225169 [00:17<00:19, 5850.39it/s] 49%|████▉     | 110311/225169 [00:17<00:19, 6026.78it/s] 49%|████▉     | 111028/225169 [00:17<00:17, 6361.02it/s] 50%|████▉     | 111667/225169 [00:17<00:18, 6212.31it/s] 50%|████▉     | 112302/225169 [00:18<00:18, 6245.10it/s] 50%|█████     | 112952/225169 [00:18<00:17, 6313.65it/s] 50%|█████     | 113585/225169 [00:18<00:18, 5921.28it/s] 51%|█████     | 114269/225169 [00:18<00:17, 6173.19it/s] 51%|█████     | 114892/225169 [00:18<00:18, 6114.86it/s] 51%|█████▏    | 115508/225169 [00:18<00:18, 5951.70it/s] 52%|█████▏    | 116107/225169 [00:18<00:18, 5906.99it/s] 52%|█████▏    | 116700/225169 [00:18<00:18, 5765.93it/s] 52%|█████▏    | 117297/225169 [00:18<00:18, 5812.63it/s] 52%|█████▏    | 117890/225169 [00:18<00:18, 5841.57it/s] 53%|█████▎    | 118582/225169 [00:19<00:17, 6154.60it/s] 53%|█████▎    | 119199/225169 [00:19<00:17, 6007.02it/s] 53%|█████▎    | 119802/225169 [00:19<00:17, 5984.93it/s] 54%|█████▎    | 120589/225169 [00:19<00:16, 6535.57it/s] 54%|█████▍    | 121245/225169 [00:19<00:16, 6362.46it/s] 54%|█████▍    | 121884/225169 [00:19<00:16, 6097.12it/s] 54%|█████▍    | 122498/225169 [00:19<00:16, 6090.36it/s] 55%|█████▍    | 123110/225169 [00:19<00:16, 6098.43it/s] 55%|█████▍    | 123722/225169 [00:19<00:16, 6019.76it/s] 55%|█████▌    | 124343/225169 [00:20<00:16, 6070.08it/s] 55%|█████▌    | 124951/225169 [00:20<00:16, 6066.14it/s] 56%|█████▌    | 125570/225169 [00:20<00:16, 6101.84it/s] 56%|█████▌    | 126258/225169 [00:20<00:15, 6330.91it/s] 56%|█████▋    | 126961/225169 [00:20<00:15, 6535.32it/s] 57%|█████▋    | 127616/225169 [00:20<00:15, 6182.52it/s] 57%|█████▋    | 128247/225169 [00:20<00:15, 6214.42it/s] 57%|█████▋    | 128872/225169 [00:20<00:15, 6165.30it/s] 58%|█████▊    | 129552/225169 [00:20<00:15, 6341.44it/s] 58%|█████▊    | 130189/225169 [00:20<00:15, 5958.12it/s] 58%|█████▊    | 130791/225169 [00:21<00:15, 5940.52it/s] 58%|█████▊    | 131423/225169 [00:21<00:15, 6045.28it/s] 59%|█████▊    | 132031/225169 [00:21<00:16, 5547.62it/s] 59%|█████▉    | 132748/225169 [00:21<00:15, 5993.54it/s] 59%|█████▉    | 133373/225169 [00:21<00:15, 6059.68it/s] 60%|█████▉    | 134051/225169 [00:21<00:14, 6264.33it/s] 60%|█████▉    | 134779/225169 [00:21<00:13, 6559.31it/s] 60%|██████    | 135529/225169 [00:21<00:13, 6829.77it/s] 60%|██████    | 136217/225169 [00:21<00:13, 6527.33it/s] 61%|██████    | 136981/225169 [00:22<00:12, 6843.65it/s] 61%|██████    | 137671/225169 [00:22<00:13, 6661.49it/s] 61%|██████▏   | 138342/225169 [00:22<00:13, 6308.74it/s] 62%|██████▏   | 138979/225169 [00:22<00:13, 6197.17it/s] 62%|██████▏   | 139603/225169 [00:22<00:23, 3596.12it/s] 62%|██████▏   | 140204/225169 [00:22<00:20, 4050.97it/s] 63%|██████▎   | 140824/225169 [00:22<00:18, 4504.45it/s] 63%|██████▎   | 141376/225169 [00:23<00:17, 4733.51it/s] 63%|██████▎   | 142006/225169 [00:23<00:16, 5123.15it/s] 63%|██████▎   | 142583/225169 [00:23<00:15, 5292.75it/s] 64%|██████▎   | 143158/225169 [00:23<00:15, 5304.76it/s] 64%|██████▍   | 143970/225169 [00:23<00:13, 6075.87it/s] 64%|██████▍   | 144616/225169 [00:23<00:13, 6183.03it/s] 65%|██████▍   | 145320/225169 [00:23<00:12, 6422.63it/s] 65%|██████▍   | 145983/225169 [00:23<00:12, 6482.71it/s] 65%|██████▌   | 146643/225169 [00:23<00:12, 6180.68it/s] 65%|██████▌   | 147272/225169 [00:23<00:12, 6112.65it/s] 66%|██████▌   | 147891/225169 [00:24<00:13, 5829.78it/s] 66%|██████▌   | 148538/225169 [00:24<00:12, 6002.05it/s] 66%|██████▋   | 149190/225169 [00:24<00:12, 6146.33it/s] 67%|██████▋   | 149810/225169 [00:24<00:12, 5986.37it/s] 67%|██████▋   | 150413/225169 [00:24<00:12, 5901.17it/s] 67%|██████▋   | 151006/225169 [00:24<00:12, 5814.57it/s] 67%|██████▋   | 151643/225169 [00:24<00:12, 5964.75it/s] 68%|██████▊   | 152242/225169 [00:24<00:12, 5692.54it/s] 68%|██████▊   | 152918/225169 [00:24<00:12, 5991.89it/s] 68%|██████▊   | 153587/225169 [00:25<00:11, 6179.00it/s] 68%|██████▊   | 154209/225169 [00:25<00:11, 6010.66it/s] 69%|██████▉   | 154815/225169 [00:25<00:11, 6021.72it/s] 69%|██████▉   | 155488/225169 [00:25<00:11, 6221.63it/s] 69%|██████▉   | 156140/225169 [00:25<00:10, 6306.78it/s] 70%|██████▉   | 156773/225169 [00:25<00:11, 6001.72it/s] 70%|██████▉   | 157486/225169 [00:25<00:10, 6322.26it/s] 70%|███████   | 158165/225169 [00:25<00:10, 6456.19it/s] 71%|███████   | 158815/225169 [00:25<00:10, 6354.85it/s] 71%|███████   | 159454/225169 [00:25<00:10, 6170.19it/s] 71%|███████   | 160153/225169 [00:26<00:10, 6404.06it/s] 71%|███████▏  | 160797/225169 [00:26<00:10, 6143.05it/s] 72%|███████▏  | 161467/225169 [00:26<00:10, 6296.63it/s] 72%|███████▏  | 162101/225169 [00:26<00:10, 6300.33it/s] 72%|███████▏  | 162746/225169 [00:26<00:09, 6336.21it/s] 73%|███████▎  | 163396/225169 [00:26<00:09, 6382.33it/s] 73%|███████▎  | 164036/225169 [00:26<00:10, 5996.84it/s] 73%|███████▎  | 164692/225169 [00:26<00:09, 6155.31it/s] 73%|███████▎  | 165382/225169 [00:26<00:09, 6366.99it/s] 74%|███████▎  | 166042/225169 [00:27<00:09, 6434.66it/s] 74%|███████▍  | 166764/225169 [00:27<00:08, 6660.03it/s] 74%|███████▍  | 167433/225169 [00:27<00:08, 6503.62it/s] 75%|███████▍  | 168086/225169 [00:27<00:09, 6088.10it/s] 75%|███████▍  | 168788/225169 [00:27<00:08, 6349.36it/s] 75%|███████▌  | 169430/225169 [00:27<00:08, 6234.72it/s] 76%|███████▌  | 170058/225169 [00:27<00:09, 5964.12it/s] 76%|███████▌  | 170660/225169 [00:27<00:09, 5869.13it/s] 76%|███████▌  | 171250/225169 [00:27<00:09, 5872.03it/s] 76%|███████▋  | 171887/225169 [00:27<00:08, 6007.06it/s] 77%|███████▋  | 172490/225169 [00:28<00:08, 5964.97it/s] 77%|███████▋  | 173095/225169 [00:28<00:08, 5985.61it/s] 77%|███████▋  | 173723/225169 [00:28<00:08, 6070.07it/s] 77%|███████▋  | 174331/225169 [00:28<00:08, 5991.97it/s] 78%|███████▊  | 175042/225169 [00:28<00:07, 6320.03it/s] 78%|███████▊  | 175676/225169 [00:28<00:08, 5987.24it/s] 78%|███████▊  | 176308/225169 [00:28<00:08, 6081.46it/s] 79%|███████▊  | 176920/225169 [00:28<00:08, 5898.32it/s] 79%|███████▉  | 177513/225169 [00:28<00:08, 5581.52it/s] 79%|███████▉  | 178089/225169 [00:29<00:08, 5631.15it/s] 79%|███████▉  | 178768/225169 [00:29<00:07, 5958.88it/s] 80%|███████▉  | 179384/225169 [00:29<00:07, 6013.53it/s] 80%|███████▉  | 179989/225169 [00:29<00:07, 5806.99it/s] 80%|████████  | 180689/225169 [00:29<00:07, 6142.96it/s] 81%|████████  | 181308/225169 [00:29<00:07, 5951.05it/s] 81%|████████  | 181907/225169 [00:29<00:07, 5777.19it/s] 81%|████████  | 182500/225169 [00:29<00:07, 5812.28it/s] 81%|████████▏ | 183137/225169 [00:29<00:07, 5972.76it/s] 82%|████████▏ | 183737/225169 [00:29<00:07, 5591.17it/s] 82%|████████▏ | 184302/225169 [00:30<00:07, 5523.04it/s] 82%|████████▏ | 184939/225169 [00:30<00:06, 5760.86it/s] 82%|████████▏ | 185520/225169 [00:30<00:07, 5595.14it/s] 83%|████████▎ | 186101/225169 [00:30<00:06, 5653.54it/s] 83%|████████▎ | 186705/225169 [00:30<00:06, 5763.67it/s] 83%|████████▎ | 187354/225169 [00:30<00:06, 5969.76it/s] 83%|████████▎ | 187954/225169 [00:30<00:06, 5857.24it/s] 84%|████████▍ | 188696/225169 [00:30<00:05, 6312.47it/s] 84%|████████▍ | 189330/225169 [00:30<00:05, 6045.21it/s] 84%|████████▍ | 189992/225169 [00:31<00:05, 6207.78it/s] 85%|████████▍ | 190643/225169 [00:31<00:05, 6293.30it/s] 85%|████████▍ | 191275/225169 [00:31<00:05, 6264.92it/s] 85%|████████▌ | 191904/225169 [00:31<00:05, 6182.59it/s] 86%|████████▌ | 192541/225169 [00:31<00:05, 6237.31it/s] 86%|████████▌ | 193166/225169 [00:31<00:05, 6209.05it/s] 86%|████████▌ | 193788/225169 [00:31<00:05, 6163.10it/s] 86%|████████▋ | 194444/225169 [00:31<00:04, 6270.77it/s] 87%|████████▋ | 195072/225169 [00:31<00:04, 6175.08it/s] 87%|████████▋ | 195721/225169 [00:31<00:04, 6266.08it/s] 87%|████████▋ | 196349/225169 [00:32<00:04, 6182.91it/s] 88%|████████▊ | 197046/225169 [00:32<00:04, 6413.98it/s] 88%|████████▊ | 197856/225169 [00:32<00:03, 6905.24it/s] 88%|████████▊ | 198548/225169 [00:32<00:04, 6256.74it/s] 88%|████████▊ | 199186/225169 [00:32<00:04, 6198.92it/s] 89%|████████▊ | 199815/225169 [00:32<00:04, 6148.80it/s] 89%|████████▉ | 200436/225169 [00:32<00:04, 6046.26it/s] 89%|████████▉ | 201045/225169 [00:32<00:04, 5949.12it/s] 90%|████████▉ | 201643/225169 [00:32<00:04, 5863.31it/s] 90%|████████▉ | 202273/225169 [00:33<00:03, 5988.00it/s] 90%|█████████ | 202874/225169 [00:33<00:03, 5987.08it/s] 90%|█████████ | 203565/225169 [00:33<00:03, 6255.74it/s] 91%|█████████ | 204193/225169 [00:33<00:03, 6212.47it/s] 91%|█████████ | 204816/225169 [00:33<00:03, 5962.41it/s] 91%|█████████ | 205439/225169 [00:33<00:03, 6036.15it/s] 92%|█████████▏| 206045/225169 [00:33<00:03, 6010.65it/s] 92%|█████████▏| 206671/225169 [00:33<00:03, 6078.95it/s] 92%|█████████▏| 207281/225169 [00:33<00:03, 5863.43it/s] 92%|█████████▏| 207886/225169 [00:33<00:02, 5916.70it/s] 93%|█████████▎| 208480/225169 [00:34<00:02, 5884.19it/s] 93%|█████████▎| 209070/225169 [00:34<00:02, 5551.51it/s] 93%|█████████▎| 209755/225169 [00:34<00:02, 5914.44it/s] 93%|█████████▎| 210465/225169 [00:34<00:02, 6249.24it/s] 94%|█████████▎| 211095/225169 [00:34<00:02, 6206.56it/s] 94%|█████████▍| 211720/225169 [00:34<00:02, 6076.14it/s] 94%|█████████▍| 212371/225169 [00:34<00:02, 6197.72it/s] 95%|█████████▍| 213082/225169 [00:34<00:01, 6463.56it/s] 95%|█████████▍| 213731/225169 [00:34<00:01, 6006.91it/s] 95%|█████████▌| 214340/225169 [00:35<00:01, 5902.91it/s] 95%|█████████▌| 214936/225169 [00:35<00:01, 5593.59it/s] 96%|█████████▌| 215507/225169 [00:35<00:01, 5621.84it/s] 96%|█████████▌| 216120/225169 [00:35<00:01, 5765.15it/s] 96%|█████████▌| 216701/225169 [00:35<00:01, 5757.52it/s] 96%|█████████▋| 217282/225169 [00:35<00:01, 5768.22it/s] 97%|█████████▋| 217959/225169 [00:35<00:01, 6054.82it/s] 97%|█████████▋| 218593/225169 [00:35<00:01, 6137.86it/s] 97%|█████████▋| 219266/225169 [00:35<00:00, 6311.66it/s] 98%|█████████▊| 219899/225169 [00:35<00:00, 6033.43it/s] 98%|█████████▊| 220506/225169 [00:36<00:00, 5874.84it/s] 98%|█████████▊| 221204/225169 [00:36<00:00, 6190.29it/s] 99%|█████████▊| 221827/225169 [00:36<00:00, 5950.07it/s] 99%|█████████▉| 222426/225169 [00:36<00:00, 5630.20it/s] 99%|█████████▉| 223075/225169 [00:36<00:00, 5867.38it/s] 99%|█████████▉| 223668/225169 [00:36<00:00, 5575.88it/s]100%|█████████▉| 224361/225169 [00:36<00:00, 5947.94it/s]100%|█████████▉| 225018/225169 [00:36<00:00, 6118.21it/s]100%|██████████| 225169/225169 [00:36<00:00, 6111.18it/s]

gathering stats for n=1
  0%|          | 0/225169 [00:00<?, ?it/s]  1%|          | 1897/225169 [00:00<00:11, 18965.63it/s]  2%|▏         | 3924/225169 [00:00<00:11, 19732.07it/s]  3%|▎         | 6129/225169 [00:00<00:10, 20782.57it/s]  4%|▎         | 8208/225169 [00:00<00:11, 19704.45it/s]  5%|▍         | 10201/225169 [00:00<00:10, 19781.26it/s]  5%|▌         | 12185/225169 [00:00<00:10, 19633.15it/s]  6%|▋         | 14173/225169 [00:00<00:10, 19710.59it/s]  7%|▋         | 16147/225169 [00:00<00:10, 19569.12it/s]  8%|▊         | 18125/225169 [00:00<00:10, 19631.66it/s]  9%|▉         | 20202/225169 [00:01<00:10, 19976.65it/s] 10%|▉         | 22201/225169 [00:01<00:10, 19700.58it/s] 11%|█         | 24470/225169 [00:01<00:09, 20593.83it/s] 12%|█▏        | 26532/225169 [00:01<00:09, 20396.69it/s] 13%|█▎        | 28574/225169 [00:01<00:09, 20235.15it/s] 14%|█▎        | 30599/225169 [00:01<00:09, 19814.55it/s] 14%|█▍        | 32583/225169 [00:01<00:09, 19443.65it/s] 15%|█▌        | 34538/225169 [00:01<00:09, 19471.83it/s] 16%|█▌        | 36487/225169 [00:01<00:09, 19448.06it/s] 17%|█▋        | 38433/225169 [00:01<00:09, 19022.57it/s] 18%|█▊        | 40438/225169 [00:02<00:09, 19316.52it/s] 19%|█▉        | 42373/225169 [00:02<00:09, 18900.01it/s] 20%|█▉        | 44328/225169 [00:02<00:09, 19077.54it/s] 21%|██        | 46453/225169 [00:02<00:09, 19709.24it/s] 22%|██▏       | 48762/225169 [00:02<00:08, 20707.68it/s] 23%|██▎       | 50837/225169 [00:02<00:08, 20446.30it/s] 24%|██▎       | 53056/225169 [00:02<00:08, 20956.89it/s] 24%|██▍       | 55155/225169 [00:02<00:08, 20298.69it/s] 25%|██▌       | 57191/225169 [00:02<00:08, 19960.84it/s] 26%|██▋       | 59236/225169 [00:02<00:08, 20100.00it/s] 27%|██▋       | 61354/225169 [00:03<00:08, 20413.41it/s] 28%|██▊       | 63399/225169 [00:03<00:08, 20030.70it/s] 29%|██▉       | 65406/225169 [00:03<00:08, 19903.72it/s] 30%|██▉       | 67425/225169 [00:03<00:07, 19983.89it/s] 31%|███       | 69426/225169 [00:03<00:07, 19975.80it/s] 32%|███▏      | 71444/225169 [00:03<00:07, 20033.08it/s] 33%|███▎      | 73618/225169 [00:03<00:07, 20540.52it/s] 34%|███▎      | 75754/225169 [00:03<00:07, 20781.09it/s] 35%|███▍      | 77833/225169 [00:03<00:07, 20753.83it/s] 35%|███▌      | 79910/225169 [00:04<00:07, 19792.78it/s] 36%|███▋      | 81935/225169 [00:04<00:07, 19922.58it/s] 37%|███▋      | 84006/225169 [00:04<00:07, 20150.04it/s] 38%|███▊      | 86027/225169 [00:04<00:06, 20029.75it/s] 39%|███▉      | 88148/225169 [00:04<00:06, 20377.06it/s] 40%|████      | 90190/225169 [00:04<00:06, 19514.15it/s] 41%|████      | 92221/225169 [00:04<00:06, 19734.89it/s] 42%|████▏     | 94216/225169 [00:04<00:06, 19795.85it/s] 43%|████▎     | 96286/225169 [00:04<00:06, 20057.94it/s] 44%|████▎     | 98399/225169 [00:04<00:06, 20364.85it/s] 45%|████▍     | 100439/225169 [00:05<00:06, 19687.13it/s] 46%|████▌     | 102455/225169 [00:05<00:06, 19823.51it/s] 46%|████▋     | 104444/225169 [00:05<00:06, 19837.65it/s] 47%|████▋     | 106432/225169 [00:05<00:06, 19610.65it/s] 48%|████▊     | 108396/225169 [00:05<00:05, 19566.01it/s] 49%|████▉     | 110355/225169 [00:05<00:05, 19429.83it/s] 50%|████▉     | 112371/225169 [00:05<00:05, 19644.59it/s] 51%|█████     | 114339/225169 [00:05<00:05, 19654.54it/s] 52%|█████▏    | 116306/225169 [00:05<00:05, 19269.90it/s] 53%|█████▎    | 118236/225169 [00:05<00:05, 19193.30it/s] 53%|█████▎    | 120321/225169 [00:06<00:05, 19676.19it/s] 54%|█████▍    | 122291/225169 [00:06<00:05, 19520.26it/s] 55%|█████▌    | 124245/225169 [00:06<00:05, 19410.39it/s] 56%|█████▌    | 126295/225169 [00:06<00:05, 19731.59it/s] 57%|█████▋    | 128270/225169 [00:06<00:04, 19733.54it/s] 58%|█████▊    | 130245/225169 [00:06<00:04, 19532.46it/s] 59%|█████▊    | 132200/225169 [00:06<00:04, 19134.90it/s] 60%|█████▉    | 134310/225169 [00:06<00:04, 19710.24it/s] 61%|██████    | 136507/225169 [00:06<00:04, 20375.62it/s] 62%|██████▏   | 138625/225169 [00:06<00:04, 20613.23it/s] 62%|██████▏   | 140689/225169 [00:07<00:04, 20348.29it/s] 63%|██████▎   | 142727/225169 [00:07<00:04, 19877.94it/s] 64%|██████▍   | 144906/225169 [00:07<00:03, 20435.26it/s] 65%|██████▌   | 146954/225169 [00:07<00:03, 20263.70it/s] 66%|██████▌   | 148984/225169 [00:07<00:03, 19947.94it/s] 67%|██████▋   | 150982/225169 [00:07<00:03, 19485.72it/s] 68%|██████▊   | 152969/225169 [00:07<00:03, 19596.54it/s] 69%|██████▉   | 154939/225169 [00:07<00:03, 19626.13it/s] 70%|██████▉   | 156990/225169 [00:07<00:03, 19885.95it/s] 71%|███████   | 159043/225169 [00:07<00:03, 20076.16it/s] 72%|███████▏  | 161147/225169 [00:08<00:03, 20360.58it/s] 72%|███████▏  | 163185/225169 [00:08<00:03, 20314.18it/s] 73%|███████▎  | 165218/225169 [00:08<00:02, 20227.39it/s] 74%|███████▍  | 167365/225169 [00:08<00:02, 20594.33it/s] 75%|███████▌  | 169426/225169 [00:08<00:02, 20172.61it/s] 76%|███████▌  | 171446/225169 [00:08<00:02, 19647.41it/s] 77%|███████▋  | 173415/225169 [00:08<00:02, 19626.08it/s] 78%|███████▊  | 175392/225169 [00:08<00:02, 19664.30it/s] 79%|███████▉  | 177361/225169 [00:08<00:02, 19239.12it/s] 80%|███████▉  | 179288/225169 [00:09<00:02, 19205.09it/s] 80%|████████  | 181221/225169 [00:09<00:02, 19241.39it/s] 81%|████████▏ | 183147/225169 [00:09<00:02, 19128.99it/s] 82%|████████▏ | 185061/225169 [00:09<00:02, 18746.43it/s] 83%|████████▎ | 186938/225169 [00:09<00:02, 18613.56it/s] 84%|████████▍ | 188982/225169 [00:09<00:01, 19146.70it/s] 85%|████████▍ | 191001/225169 [00:09<00:01, 19453.32it/s] 86%|████████▌ | 192990/225169 [00:09<00:01, 19581.84it/s] 87%|████████▋ | 194984/225169 [00:09<00:01, 19687.51it/s] 88%|████████▊ | 197112/225169 [00:09<00:01, 20162.07it/s] 88%|████████▊ | 199143/225169 [00:10<00:01, 20195.78it/s] 89%|████████▉ | 201164/225169 [00:10<00:01, 19785.97it/s] 90%|█████████ | 203145/225169 [00:10<00:01, 19780.05it/s] 91%|█████████ | 205125/225169 [00:10<00:01, 19712.60it/s] 92%|█████████▏| 207098/225169 [00:10<00:00, 19623.40it/s] 93%|█████████▎| 209062/225169 [00:10<00:00, 19043.52it/s] 94%|█████████▍| 211189/225169 [00:10<00:00, 19690.83it/s] 95%|█████████▍| 213236/225169 [00:10<00:00, 19919.04it/s] 96%|█████████▌| 215232/225169 [00:10<00:00, 19423.47it/s] 96%|█████████▋| 217180/225169 [00:10<00:00, 19115.34it/s] 97%|█████████▋| 219292/225169 [00:11<00:00, 19695.72it/s] 98%|█████████▊| 221266/225169 [00:11<00:00, 19615.65it/s] 99%|█████████▉| 223231/225169 [00:11<00:00, 19140.06it/s]100%|██████████| 225169/225169 [00:11<00:00, 19782.22it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 51.49it/s]2022-03-08 08:11:54 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-08 08:11:54 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-08 08:11:54 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-08 08:11:54 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-08 08:11:54 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-08 08:11:54 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-08 08:11:54 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-08 08:11:55 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-08 08:11:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-08 08:11:55 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-08 08:11:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-08 08:11:55 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-08 08:11:55 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-08 08:11:55 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 08:11:55 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 08:11:55 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-08 08:11:55 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-08 08:11:55 | INFO | fairseq.trainer | begin training epoch 1
2022-03-08 08:11:55 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-08 08:11:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-08 08:12:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 08:12:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 08:12:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 08:12:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-08 08:17:42 | INFO | train_inner | epoch 001:    105 / 196 loss=16.397, ppl=86297.2, wps=20314.9, ups=0.31, wpb=65530.9, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.362, loss_scale=4, train_wall=342, gb_free=14.1, wall=347
2022-03-08 08:22:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 08:22:39 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.304 | ppl 10110.6 | wps 46199.3 | wpb 2034.1 | bsz 4 | num_updates 191
2022-03-08 08:22:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-08 08:22:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 08:22:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 08:22:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.304) (writing took 6.373745203949511 seconds)
2022-03-08 08:22:45 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-08 08:22:45 | INFO | train | epoch 001 | loss 15.381 | ppl 42673.9 | wps 19958.6 | ups 0.3 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.482 | loss_scale 4 | train_wall 630 | gb_free 14.1 | wall 651
2022-03-08 08:22:45 | INFO | fairseq.trainer | begin training epoch 2
2022-03-08 08:22:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 08:23:14 | INFO | train_inner | epoch 002:      9 / 196 loss=14.177, ppl=18525.3, wps=19641.9, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.494, loss_scale=4, train_wall=317, gb_free=14.1, wall=680
2022-03-08 08:28:37 | INFO | train_inner | epoch 002:    109 / 196 loss=12.295, ppl=5026.63, wps=20324.6, ups=0.31, wpb=65530.9, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.979, loss_scale=4, train_wall=317, gb_free=14.1, wall=1002
2022-03-08 08:33:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 08:33:21 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.751 | ppl 1723.45 | wps 46219.9 | wpb 2034.1 | bsz 4 | num_updates 387 | best_loss 10.751
2022-03-08 08:33:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-08 08:33:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 08:33:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 08:33:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 2 @ 387 updates, score 10.751) (writing took 6.371304173022509 seconds)
2022-03-08 08:33:28 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-08 08:33:28 | INFO | train | epoch 002 | loss 11.802 | ppl 3570.6 | wps 19972 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.813 | loss_scale 4 | train_wall 621 | gb_free 14.1 | wall 1293
2022-03-08 08:33:28 | INFO | fairseq.trainer | begin training epoch 3
2022-03-08 08:33:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 08:34:10 | INFO | train_inner | epoch 003:     13 / 196 loss=11.032, ppl=2093.69, wps=19645.3, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.562, loss_scale=4, train_wall=317, gb_free=14.1, wall=1335
2022-03-08 08:39:32 | INFO | train_inner | epoch 003:    113 / 196 loss=10.578, ppl=1528.85, wps=20327.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.48, loss_scale=4, train_wall=317, gb_free=14.1, wall=1657
2022-03-08 08:43:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 08:44:04 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.19 | ppl 1168.47 | wps 45791 | wpb 2034.1 | bsz 4 | num_updates 583 | best_loss 10.19
2022-03-08 08:44:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 583 updates
2022-03-08 08:44:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 08:44:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 08:44:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 3 @ 583 updates, score 10.19) (writing took 6.352090957574546 seconds)
2022-03-08 08:44:10 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-08 08:44:10 | INFO | train | epoch 003 | loss 10.482 | ppl 1430.57 | wps 19970.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 583 | lr 7.29604e-05 | gnorm 0.494 | loss_scale 8 | train_wall 621 | gb_free 14.1 | wall 1935
2022-03-08 08:44:10 | INFO | fairseq.trainer | begin training epoch 4
2022-03-08 08:44:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 08:45:05 | INFO | train_inner | epoch 004:     17 / 196 loss=10.302, ppl=1262.74, wps=19640.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.517, loss_scale=8, train_wall=317, gb_free=14.1, wall=1990
2022-03-08 08:50:27 | INFO | train_inner | epoch 004:    117 / 196 loss=10.055, ppl=1063.86, wps=20316.2, ups=0.31, wpb=65530.9, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.558, loss_scale=8, train_wall=318, gb_free=14.1, wall=2313
2022-03-08 08:54:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 08:54:46 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.776 | ppl 877.02 | wps 45796.7 | wpb 2034.1 | bsz 4 | num_updates 779 | best_loss 9.776
2022-03-08 08:54:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 779 updates
2022-03-08 08:54:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 08:54:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 08:54:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 4 @ 779 updates, score 9.776) (writing took 6.32840087544173 seconds)
2022-03-08 08:54:53 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-08 08:54:53 | INFO | train | epoch 004 | loss 9.995 | ppl 1020.14 | wps 19962.1 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 779 | lr 9.74555e-05 | gnorm 0.601 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 2578
2022-03-08 08:54:53 | INFO | fairseq.trainer | begin training epoch 5
2022-03-08 08:54:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 08:56:00 | INFO | train_inner | epoch 005:     21 / 196 loss=9.85, ppl=922.95, wps=19628.7, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.661, loss_scale=8, train_wall=317, gb_free=14.1, wall=2646
2022-03-08 09:01:23 | INFO | train_inner | epoch 005:    121 / 196 loss=9.644, ppl=800.05, wps=20313.4, ups=0.31, wpb=65527.3, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.733, loss_scale=8, train_wall=318, gb_free=14.1, wall=2968
2022-03-08 09:05:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:05:29 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.415 | ppl 682.72 | wps 45778.8 | wpb 2034.1 | bsz 4 | num_updates 975 | best_loss 9.415
2022-03-08 09:05:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 975 updates
2022-03-08 09:05:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 09:05:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 09:05:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 5 @ 975 updates, score 9.415) (writing took 6.2521736063063145 seconds)
2022-03-08 09:05:35 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-08 09:05:35 | INFO | train | epoch 005 | loss 9.601 | ppl 776.5 | wps 19960.3 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 975 | lr 0.000121951 | gnorm 0.719 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 3221
2022-03-08 09:05:35 | INFO | fairseq.trainer | begin training epoch 6
2022-03-08 09:05:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:06:56 | INFO | train_inner | epoch 006:     25 / 196 loss=9.469, ppl=708.78, wps=19632.2, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.755, loss_scale=8, train_wall=317, gb_free=14.1, wall=3301
2022-03-08 09:12:19 | INFO | train_inner | epoch 006:    125 / 196 loss=9.293, ppl=627.49, wps=20309.4, ups=0.31, wpb=65530.9, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.789, loss_scale=16, train_wall=318, gb_free=14.1, wall=3624
2022-03-08 09:16:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:16:12 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.148 | ppl 567.25 | wps 45671.7 | wpb 2034.1 | bsz 4 | num_updates 1171 | best_loss 9.148
2022-03-08 09:16:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1171 updates
2022-03-08 09:16:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 09:16:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 09:16:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 6 @ 1171 updates, score 9.148) (writing took 6.2765140775591135 seconds)
2022-03-08 09:16:18 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-08 09:16:18 | INFO | train | epoch 006 | loss 9.266 | ppl 615.69 | wps 19957.3 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 1171 | lr 0.000146446 | gnorm 0.79 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 3863
2022-03-08 09:16:18 | INFO | fairseq.trainer | begin training epoch 7
2022-03-08 09:16:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:17:52 | INFO | train_inner | epoch 007:     29 / 196 loss=9.152, ppl=568.78, wps=19633.2, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.785, loss_scale=16, train_wall=317, gb_free=14.1, wall=3957
2022-03-08 09:23:15 | INFO | train_inner | epoch 007:    129 / 196 loss=9.01, ppl=515.65, wps=20292, ups=0.31, wpb=65532.4, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.786, loss_scale=16, train_wall=318, gb_free=14.1, wall=4280
2022-03-08 09:26:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:26:55 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.902 | ppl 478.24 | wps 45703.1 | wpb 2034.1 | bsz 4 | num_updates 1367 | best_loss 8.902
2022-03-08 09:26:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1367 updates
2022-03-08 09:26:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 09:26:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 09:27:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 7 @ 1367 updates, score 8.902) (writing took 6.233892613090575 seconds)
2022-03-08 09:27:01 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-08 09:27:01 | INFO | train | epoch 007 | loss 8.986 | ppl 506.94 | wps 19950.6 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 1367 | lr 0.000170941 | gnorm 0.794 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 4506
2022-03-08 09:27:01 | INFO | fairseq.trainer | begin training epoch 8
2022-03-08 09:27:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:28:48 | INFO | train_inner | epoch 008:     33 / 196 loss=8.881, ppl=471.42, wps=19626.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.797, loss_scale=16, train_wall=317, gb_free=14.1, wall=4613
2022-03-08 09:34:10 | INFO | train_inner | epoch 008:    133 / 196 loss=8.745, ppl=428.99, wps=20308.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.816, loss_scale=16, train_wall=318, gb_free=14.1, wall=4936
2022-03-08 09:37:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:37:38 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.701 | ppl 416.27 | wps 45778.6 | wpb 2034.1 | bsz 4 | num_updates 1563 | best_loss 8.701
2022-03-08 09:37:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1563 updates
2022-03-08 09:37:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 09:37:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 09:37:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 8 @ 1563 updates, score 8.701) (writing took 6.180948738008738 seconds)
2022-03-08 09:37:44 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-08 09:37:44 | INFO | train | epoch 008 | loss 8.733 | ppl 425.45 | wps 19955.5 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 1563 | lr 0.000195436 | gnorm 0.813 | loss_scale 32 | train_wall 622 | gb_free 14.1 | wall 5149
2022-03-08 09:37:44 | INFO | fairseq.trainer | begin training epoch 9
2022-03-08 09:37:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:39:43 | INFO | train_inner | epoch 009:     37 / 196 loss=8.626, ppl=395.16, wps=19634.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.809, loss_scale=32, train_wall=317, gb_free=14.1, wall=5269
2022-03-08 09:45:06 | INFO | train_inner | epoch 009:    137 / 196 loss=8.51, ppl=364.64, wps=20288, ups=0.31, wpb=65527.3, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.809, loss_scale=32, train_wall=318, gb_free=14.1, wall=5592
2022-03-08 09:48:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:48:21 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.527 | ppl 368.77 | wps 45734 | wpb 2034.1 | bsz 4 | num_updates 1759 | best_loss 8.527
2022-03-08 09:48:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1759 updates
2022-03-08 09:48:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 09:48:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 09:48:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 9 @ 1759 updates, score 8.527) (writing took 6.202500742860138 seconds)
2022-03-08 09:48:27 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-08 09:48:27 | INFO | train | epoch 009 | loss 8.499 | ppl 361.87 | wps 19949.4 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 1759 | lr 0.000219931 | gnorm 0.809 | loss_scale 32 | train_wall 622 | gb_free 14.1 | wall 5792
2022-03-08 09:48:27 | INFO | fairseq.trainer | begin training epoch 10
2022-03-08 09:48:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:50:39 | INFO | train_inner | epoch 010:     41 / 196 loss=8.403, ppl=338.49, wps=19629.7, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.806, loss_scale=32, train_wall=317, gb_free=14.1, wall=5925
2022-03-08 09:56:02 | INFO | train_inner | epoch 010:    141 / 196 loss=8.281, ppl=311.1, wps=20301.4, ups=0.31, wpb=65530.9, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.792, loss_scale=32, train_wall=318, gb_free=14.1, wall=6247
2022-03-08 09:58:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:59:03 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.372 | ppl 331.3 | wps 46167.2 | wpb 2034.1 | bsz 4 | num_updates 1955 | best_loss 8.372
2022-03-08 09:59:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1955 updates
2022-03-08 09:59:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 09:59:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 09:59:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 10 @ 1955 updates, score 8.372) (writing took 6.244888672605157 seconds)
2022-03-08 09:59:10 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-08 09:59:10 | INFO | train | epoch 010 | loss 8.281 | ppl 311.01 | wps 19953.1 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 1955 | lr 0.000244426 | gnorm 0.78 | loss_scale 32 | train_wall 622 | gb_free 14.1 | wall 6435
2022-03-08 09:59:10 | INFO | fairseq.trainer | begin training epoch 11
2022-03-08 09:59:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:01:35 | INFO | train_inner | epoch 011:     45 / 196 loss=8.183, ppl=290.7, wps=19623.8, ups=0.3, wpb=65365, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.778, loss_scale=32, train_wall=317, gb_free=14.1, wall=6581
2022-03-08 10:04:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 10:07:01 | INFO | train_inner | epoch 011:    146 / 196 loss=8.082, ppl=270.89, wps=20089.3, ups=0.31, wpb=65530.9, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.785, loss_scale=32, train_wall=321, gb_free=14.1, wall=6907
2022-03-08 10:09:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:09:47 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.243 | ppl 302.99 | wps 46063.8 | wpb 2034.1 | bsz 4 | num_updates 2150 | best_loss 8.243
2022-03-08 10:09:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2150 updates
2022-03-08 10:09:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 10:09:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 10:09:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 11 @ 2150 updates, score 8.243) (writing took 6.227989822626114 seconds)
2022-03-08 10:09:53 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-08 10:09:53 | INFO | train | epoch 011 | loss 8.078 | ppl 270.23 | wps 19844.3 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2150 | lr 0.000268796 | gnorm 0.795 | loss_scale 32 | train_wall 622 | gb_free 14.1 | wall 7078
2022-03-08 10:09:53 | INFO | fairseq.trainer | begin training epoch 12
2022-03-08 10:09:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:12:34 | INFO | train_inner | epoch 012:     50 / 196 loss=7.974, ppl=251.42, wps=19627.6, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.776, loss_scale=32, train_wall=317, gb_free=14.1, wall=7240
2022-03-08 10:17:57 | INFO | train_inner | epoch 012:    150 / 196 loss=7.888, ppl=236.8, wps=20290.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.786, loss_scale=32, train_wall=318, gb_free=14.1, wall=7563
2022-03-08 10:20:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:20:30 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.128 | ppl 279.84 | wps 46186.8 | wpb 2034.1 | bsz 4 | num_updates 2346 | best_loss 8.128
2022-03-08 10:20:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2346 updates
2022-03-08 10:20:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 10:20:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 10:20:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 12 @ 2346 updates, score 8.128) (writing took 6.256310899741948 seconds)
2022-03-08 10:20:36 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-08 10:20:36 | INFO | train | epoch 012 | loss 7.888 | ppl 236.84 | wps 19940.1 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 2346 | lr 0.000293291 | gnorm 0.767 | loss_scale 32 | train_wall 623 | gb_free 14.1 | wall 7722
2022-03-08 10:20:36 | INFO | fairseq.trainer | begin training epoch 13
2022-03-08 10:20:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:23:31 | INFO | train_inner | epoch 013:     54 / 196 loss=7.789, ppl=221.21, wps=19614.3, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.752, loss_scale=32, train_wall=317, gb_free=14.1, wall=7896
2022-03-08 10:28:54 | INFO | train_inner | epoch 013:    154 / 196 loss=7.711, ppl=209.54, wps=20287.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.775, loss_scale=32, train_wall=318, gb_free=14.1, wall=8219
2022-03-08 10:31:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:31:13 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.028 | ppl 260.94 | wps 46299.3 | wpb 2034.1 | bsz 4 | num_updates 2542 | best_loss 8.028
2022-03-08 10:31:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2542 updates
2022-03-08 10:31:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 10:31:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 10:31:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 13 @ 2542 updates, score 8.028) (writing took 6.178265846334398 seconds)
2022-03-08 10:31:19 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-08 10:31:19 | INFO | train | epoch 013 | loss 7.712 | ppl 209.6 | wps 19944.1 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 2542 | lr 0.000317786 | gnorm 0.758 | loss_scale 32 | train_wall 623 | gb_free 14.1 | wall 8365
2022-03-08 10:31:19 | INFO | fairseq.trainer | begin training epoch 14
2022-03-08 10:31:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:33:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 10:34:30 | INFO | train_inner | epoch 014:     59 / 196 loss=7.609, ppl=195.27, wps=19439.7, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.729, loss_scale=32, train_wall=320, gb_free=14.1, wall=8555
2022-03-08 10:38:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 10:39:56 | INFO | train_inner | epoch 014:    160 / 196 loss=7.552, ppl=187.66, wps=20100.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.752, loss_scale=16, train_wall=321, gb_free=14.1, wall=8881
2022-03-08 10:41:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:41:56 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.944 | ppl 246.29 | wps 46465.2 | wpb 2034.1 | bsz 4 | num_updates 2736 | best_loss 7.944
2022-03-08 10:41:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2736 updates
2022-03-08 10:41:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 10:41:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 10:42:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 14 @ 2736 updates, score 7.944) (writing took 6.182551959529519 seconds)
2022-03-08 10:42:02 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-08 10:42:02 | INFO | train | epoch 014 | loss 7.55 | ppl 187.42 | wps 19753.3 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 2736 | lr 0.000342032 | gnorm 0.741 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 9008
2022-03-08 10:42:02 | INFO | fairseq.trainer | begin training epoch 15
2022-03-08 10:42:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:45:29 | INFO | train_inner | epoch 015:     64 / 196 loss=7.452, ppl=175.15, wps=19637.6, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.725, loss_scale=16, train_wall=317, gb_free=14.1, wall=9214
2022-03-08 10:50:52 | INFO | train_inner | epoch 015:    164 / 196 loss=7.401, ppl=168.98, wps=20288.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.751, loss_scale=16, train_wall=318, gb_free=14.1, wall=9537
2022-03-08 10:52:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:52:39 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.886 | ppl 236.51 | wps 46501.3 | wpb 2034.1 | bsz 4 | num_updates 2932 | best_loss 7.886
2022-03-08 10:52:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2932 updates
2022-03-08 10:52:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 10:52:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 10:52:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 15 @ 2932 updates, score 7.886) (writing took 6.199241833761334 seconds)
2022-03-08 10:52:45 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-08 10:52:45 | INFO | train | epoch 015 | loss 7.401 | ppl 169.07 | wps 19948.2 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 2932 | lr 0.000366527 | gnorm 0.743 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 9651
2022-03-08 10:52:45 | INFO | fairseq.trainer | begin training epoch 16
2022-03-08 10:52:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:56:25 | INFO | train_inner | epoch 016:     68 / 196 loss=7.298, ppl=157.42, wps=19635.4, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.743, loss_scale=16, train_wall=317, gb_free=14.1, wall=9870
2022-03-08 11:01:47 | INFO | train_inner | epoch 016:    168 / 196 loss=7.273, ppl=154.65, wps=20313.9, ups=0.31, wpb=65527.3, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.737, loss_scale=16, train_wall=318, gb_free=14.1, wall=10193
2022-03-08 11:03:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:03:22 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.865 | ppl 233.16 | wps 46257 | wpb 2034.1 | bsz 4 | num_updates 3128 | best_loss 7.865
2022-03-08 11:03:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3128 updates
2022-03-08 11:03:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 11:03:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 11:03:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 16 @ 3128 updates, score 7.865) (writing took 6.185455051250756 seconds)
2022-03-08 11:03:28 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-08 11:03:28 | INFO | train | epoch 016 | loss 7.265 | ppl 153.84 | wps 19963.2 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 3128 | lr 0.000391022 | gnorm 0.744 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 10293
2022-03-08 11:03:28 | INFO | fairseq.trainer | begin training epoch 17
2022-03-08 11:03:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:07:20 | INFO | train_inner | epoch 017:     72 / 196 loss=7.169, ppl=143.91, wps=19634.5, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.716, loss_scale=32, train_wall=317, gb_free=14.1, wall=10526
2022-03-08 11:12:43 | INFO | train_inner | epoch 017:    172 / 196 loss=7.14, ppl=141.05, wps=20288.5, ups=0.31, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.742, loss_scale=32, train_wall=318, gb_free=14.1, wall=10849
2022-03-08 11:14:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:14:05 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.835 | ppl 228.29 | wps 46435.9 | wpb 2034.1 | bsz 4 | num_updates 3324 | best_loss 7.835
2022-03-08 11:14:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3324 updates
2022-03-08 11:14:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 11:14:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 11:14:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 17 @ 3324 updates, score 7.835) (writing took 6.278337904252112 seconds)
2022-03-08 11:14:11 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-08 11:14:11 | INFO | train | epoch 017 | loss 7.136 | ppl 140.62 | wps 19946.6 | ups 0.3 | wpb 65448 | bsz 127.8 | num_updates 3324 | lr 0.000415517 | gnorm 0.725 | loss_scale 32 | train_wall 622 | gb_free 14.1 | wall 10936
2022-03-08 11:14:11 | INFO | fairseq.trainer | begin training epoch 18
2022-03-08 11:14:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:18:16 | INFO | train_inner | epoch 018:     76 / 196 loss=7.033, ppl=130.95, wps=19630, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.742, loss_scale=32, train_wall=317, gb_free=14.1, wall=11182
2022-03-08 11:22:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 11:23:42 | INFO | train_inner | epoch 018:    177 / 196 loss=7.021, ppl=129.92, wps=20104.5, ups=0.31, wpb=65527.3, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.716, loss_scale=16, train_wall=321, gb_free=14.1, wall=11508
2022-03-08 11:24:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:24:47 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.813 | ppl 224.88 | wps 46544.1 | wpb 2034.1 | bsz 4 | num_updates 3519 | best_loss 7.813
2022-03-08 11:24:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3519 updates
2022-03-08 11:24:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 11:24:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt
2022-03-08 11:24:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_best.pt (epoch 18 @ 3519 updates, score 7.813) (writing took 6.28548847977072 seconds)
2022-03-08 11:24:54 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-08 11:24:54 | INFO | train | epoch 018 | loss 7.015 | ppl 129.33 | wps 19853.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 3519 | lr 0.000439887 | gnorm 0.733 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 11579
2022-03-08 11:24:54 | INFO | fairseq.trainer | begin training epoch 19
2022-03-08 11:24:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:29:15 | INFO | train_inner | epoch 019:     81 / 196 loss=6.908, ppl=120.11, wps=19639.1, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.741, loss_scale=16, train_wall=317, gb_free=14.1, wall=11840
2022-03-08 11:34:38 | INFO | train_inner | epoch 019:    181 / 196 loss=6.92, ppl=121.08, wps=20300.3, ups=0.31, wpb=65527.3, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.72, loss_scale=16, train_wall=318, gb_free=14.1, wall=12163
2022-03-08 11:35:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:35:30 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.845 | ppl 229.97 | wps 46793.7 | wpb 2034.1 | bsz 4 | num_updates 3715 | best_loss 7.813
2022-03-08 11:35:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3715 updates
2022-03-08 11:35:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 11:35:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 11:35:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 19 @ 3715 updates, score 7.845) (writing took 2.8710977379232645 seconds)
2022-03-08 11:35:33 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-08 11:35:33 | INFO | train | epoch 019 | loss 6.903 | ppl 119.68 | wps 20067.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 3715 | lr 0.000464382 | gnorm 0.727 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 12218
2022-03-08 11:35:33 | INFO | fairseq.trainer | begin training epoch 20
2022-03-08 11:35:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:40:07 | INFO | train_inner | epoch 020:     85 / 196 loss=6.793, ppl=110.86, wps=19832.2, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=3800, lr=0.000475005, gnorm=0.734, loss_scale=16, train_wall=317, gb_free=14.1, wall=12493
2022-03-08 11:45:30 | INFO | train_inner | epoch 020:    185 / 196 loss=6.814, ppl=112.53, wps=20302, ups=0.31, wpb=65530.9, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.714, loss_scale=16, train_wall=318, gb_free=14.1, wall=12816
2022-03-08 11:46:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:46:10 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.846 | ppl 230.15 | wps 46349 | wpb 2034.1 | bsz 4 | num_updates 3911 | best_loss 7.813
2022-03-08 11:46:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3911 updates
2022-03-08 11:46:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 11:46:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 11:46:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 20 @ 3911 updates, score 7.846) (writing took 2.8852404300123453 seconds)
2022-03-08 11:46:13 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-08 11:46:13 | INFO | train | epoch 020 | loss 6.796 | ppl 111.12 | wps 20055.2 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 3911 | lr 0.000488877 | gnorm 0.743 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 12858
2022-03-08 11:46:13 | INFO | fairseq.trainer | begin training epoch 21
2022-03-08 11:46:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:50:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 11:51:03 | INFO | train_inner | epoch 021:     90 / 196 loss=6.679, ppl=102.45, wps=19640.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=4000, lr=0.0005, gnorm=0.759, loss_scale=16, train_wall=320, gb_free=14.1, wall=13148
2022-03-08 11:56:26 | INFO | train_inner | epoch 021:    190 / 196 loss=6.724, ppl=105.68, wps=20302.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.72, loss_scale=16, train_wall=318, gb_free=14.1, wall=13471
2022-03-08 11:56:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:56:49 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.842 | ppl 229.5 | wps 46479.3 | wpb 2034.1 | bsz 4 | num_updates 4106 | best_loss 7.813
2022-03-08 11:56:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4106 updates
2022-03-08 11:56:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 11:56:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 11:56:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 21 @ 4106 updates, score 7.842) (writing took 2.921328926458955 seconds)
2022-03-08 11:56:52 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-08 11:56:52 | INFO | train | epoch 021 | loss 6.693 | ppl 103.46 | wps 19959.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4106 | lr 0.000493504 | gnorm 0.72 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 13497
2022-03-08 11:56:52 | INFO | fairseq.trainer | begin training epoch 22
2022-03-08 11:56:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:01:55 | INFO | train_inner | epoch 022:     94 / 196 loss=6.571, ppl=95.07, wps=19828.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=4200, lr=0.00048795, gnorm=0.705, loss_scale=16, train_wall=317, gb_free=14.1, wall=13801
2022-03-08 12:07:18 | INFO | train_inner | epoch 022:    194 / 196 loss=6.603, ppl=97.2, wps=20304.2, ups=0.31, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.666, loss_scale=16, train_wall=318, gb_free=14.1, wall=14124
2022-03-08 12:07:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:07:29 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.853 | ppl 231.26 | wps 46444.6 | wpb 2034.1 | bsz 4 | num_updates 4302 | best_loss 7.813
2022-03-08 12:07:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4302 updates
2022-03-08 12:07:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 12:07:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 12:07:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 22 @ 4302 updates, score 7.853) (writing took 2.885403382591903 seconds)
2022-03-08 12:07:31 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-08 12:07:31 | INFO | train | epoch 022 | loss 6.584 | ppl 95.92 | wps 20059.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 4302 | lr 0.000482131 | gnorm 0.684 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 14137
2022-03-08 12:07:31 | INFO | fairseq.trainer | begin training epoch 23
2022-03-08 12:07:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:12:48 | INFO | train_inner | epoch 023:     98 / 196 loss=6.453, ppl=87.58, wps=19821.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=4400, lr=0.000476731, gnorm=0.699, loss_scale=16, train_wall=317, gb_free=14.1, wall=14453
2022-03-08 12:18:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:18:08 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.871 | ppl 234.13 | wps 46505.1 | wpb 2034.1 | bsz 4 | num_updates 4498 | best_loss 7.813
2022-03-08 12:18:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4498 updates
2022-03-08 12:18:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 12:18:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 12:18:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 23 @ 4498 updates, score 7.871) (writing took 2.922563288360834 seconds)
2022-03-08 12:18:11 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-08 12:18:11 | INFO | train | epoch 023 | loss 6.481 | ppl 89.35 | wps 20052.2 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 4498 | lr 0.000471509 | gnorm 0.691 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 14777
2022-03-08 12:18:11 | INFO | fairseq.trainer | begin training epoch 24
2022-03-08 12:18:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:18:18 | INFO | train_inner | epoch 024:      2 / 196 loss=6.51, ppl=91.13, wps=19824.6, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.68, loss_scale=16, train_wall=317, gb_free=14.1, wall=14783
2022-03-08 12:19:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 12:23:44 | INFO | train_inner | epoch 024:    103 / 196 loss=6.357, ppl=81.98, wps=20106.3, ups=0.31, wpb=65527.3, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.674, loss_scale=16, train_wall=321, gb_free=14.1, wall=15109
2022-03-08 12:28:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:28:48 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.868 | ppl 233.69 | wps 46321.4 | wpb 2034.1 | bsz 4 | num_updates 4693 | best_loss 7.813
2022-03-08 12:28:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4693 updates
2022-03-08 12:28:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 12:28:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 12:28:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 24 @ 4693 updates, score 7.868) (writing took 2.913443824276328 seconds)
2022-03-08 12:28:51 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-08 12:28:51 | INFO | train | epoch 024 | loss 6.385 | ppl 83.58 | wps 19959.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 4693 | lr 0.000461609 | gnorm 0.675 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 15416
2022-03-08 12:28:51 | INFO | fairseq.trainer | begin training epoch 25
2022-03-08 12:28:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:29:13 | INFO | train_inner | epoch 025:      7 / 196 loss=6.402, ppl=84.56, wps=19828.3, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.676, loss_scale=16, train_wall=317, gb_free=14.1, wall=15439
2022-03-08 12:34:36 | INFO | train_inner | epoch 025:    107 / 196 loss=6.267, ppl=77, wps=20305.9, ups=0.31, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.676, loss_scale=16, train_wall=318, gb_free=14.1, wall=15761
2022-03-08 12:39:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:39:27 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.903 | ppl 239.32 | wps 46404.6 | wpb 2034.1 | bsz 4 | num_updates 4889 | best_loss 7.813
2022-03-08 12:39:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4889 updates
2022-03-08 12:39:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 12:39:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 12:39:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 25 @ 4889 updates, score 7.903) (writing took 2.907258614897728 seconds)
2022-03-08 12:39:30 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-08 12:39:30 | INFO | train | epoch 025 | loss 6.295 | ppl 78.54 | wps 20058 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 4889 | lr 0.000452262 | gnorm 0.677 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 16056
2022-03-08 12:39:30 | INFO | fairseq.trainer | begin training epoch 26
2022-03-08 12:39:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:40:06 | INFO | train_inner | epoch 026:     11 / 196 loss=6.312, ppl=79.43, wps=19826.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.674, loss_scale=16, train_wall=317, gb_free=14.1, wall=16091
2022-03-08 12:45:29 | INFO | train_inner | epoch 026:    111 / 196 loss=6.187, ppl=72.84, wps=20298.7, ups=0.31, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.677, loss_scale=16, train_wall=318, gb_free=14.1, wall=16414
2022-03-08 12:47:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 12:50:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:50:07 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.935 | ppl 244.68 | wps 46293.6 | wpb 2034.1 | bsz 4 | num_updates 5084 | best_loss 7.813
2022-03-08 12:50:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5084 updates
2022-03-08 12:50:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 12:50:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 12:50:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 26 @ 5084 updates, score 7.935) (writing took 2.925227612257004 seconds)
2022-03-08 12:50:10 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-08 12:50:10 | INFO | train | epoch 026 | loss 6.211 | ppl 74.09 | wps 19951.4 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 5084 | lr 0.000443504 | gnorm 0.672 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 16695
2022-03-08 12:50:10 | INFO | fairseq.trainer | begin training epoch 27
2022-03-08 12:50:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:51:02 | INFO | train_inner | epoch 027:     16 / 196 loss=6.22, ppl=74.54, wps=19631, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.682, loss_scale=16, train_wall=320, gb_free=14.1, wall=16747
2022-03-08 12:56:24 | INFO | train_inner | epoch 027:    116 / 196 loss=6.114, ppl=69.27, wps=20299.6, ups=0.31, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.671, loss_scale=16, train_wall=318, gb_free=14.1, wall=17070
2022-03-08 13:00:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:00:47 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.968 | ppl 250.35 | wps 46405.2 | wpb 2034.1 | bsz 4 | num_updates 5280 | best_loss 7.813
2022-03-08 13:00:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5280 updates
2022-03-08 13:00:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 13:00:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 13:00:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 27 @ 5280 updates, score 7.968) (writing took 2.8902776846662164 seconds)
2022-03-08 13:00:49 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-08 13:00:49 | INFO | train | epoch 027 | loss 6.133 | ppl 70.18 | wps 20054.6 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 5280 | lr 0.000435194 | gnorm 0.683 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 17335
2022-03-08 13:00:49 | INFO | fairseq.trainer | begin training epoch 28
2022-03-08 13:00:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:01:54 | INFO | train_inner | epoch 028:     20 / 196 loss=6.135, ppl=70.3, wps=19824.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.688, loss_scale=16, train_wall=317, gb_free=14.1, wall=17400
2022-03-08 13:07:17 | INFO | train_inner | epoch 028:    120 / 196 loss=6.038, ppl=65.71, wps=20300.8, ups=0.31, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.672, loss_scale=16, train_wall=318, gb_free=14.1, wall=17722
2022-03-08 13:11:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:11:26 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.001 | ppl 256.21 | wps 46606.2 | wpb 2034.1 | bsz 4 | num_updates 5476 | best_loss 7.813
2022-03-08 13:11:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5476 updates
2022-03-08 13:11:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 13:11:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 13:11:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 28 @ 5476 updates, score 8.001) (writing took 2.929415237158537 seconds)
2022-03-08 13:11:29 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-08 13:11:29 | INFO | train | epoch 028 | loss 6.057 | ppl 66.59 | wps 20054.2 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 5476 | lr 0.000427335 | gnorm 0.678 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 17975
2022-03-08 13:11:29 | INFO | fairseq.trainer | begin training epoch 29
2022-03-08 13:11:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:12:47 | INFO | train_inner | epoch 029:     24 / 196 loss=6.053, ppl=66.41, wps=19822.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.679, loss_scale=16, train_wall=317, gb_free=14.1, wall=18052
2022-03-08 13:15:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 13:17:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 13:18:16 | INFO | train_inner | epoch 029:    126 / 196 loss=5.969, ppl=62.66, wps=19909.1, ups=0.3, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.69, loss_scale=8, train_wall=324, gb_free=14.1, wall=18381
2022-03-08 13:22:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:22:06 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.019 | ppl 259.33 | wps 46586.7 | wpb 2034.1 | bsz 4 | num_updates 5670 | best_loss 7.813
2022-03-08 13:22:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5670 updates
2022-03-08 13:22:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 13:22:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 13:22:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 29 @ 5670 updates, score 8.019) (writing took 2.907293782569468 seconds)
2022-03-08 13:22:08 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-08 13:22:08 | INFO | train | epoch 029 | loss 5.985 | ppl 63.35 | wps 19858.2 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 5670 | lr 0.000419961 | gnorm 0.678 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 18614
2022-03-08 13:22:08 | INFO | fairseq.trainer | begin training epoch 30
2022-03-08 13:22:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:23:45 | INFO | train_inner | epoch 030:     30 / 196 loss=5.981, ppl=63.15, wps=19830.6, ups=0.3, wpb=65358.3, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.666, loss_scale=8, train_wall=317, gb_free=14.1, wall=18711
2022-03-08 13:29:08 | INFO | train_inner | epoch 030:    130 / 196 loss=5.909, ppl=60.09, wps=20313.1, ups=0.31, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.686, loss_scale=8, train_wall=318, gb_free=14.1, wall=19033
2022-03-08 13:32:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:32:45 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.069 | ppl 268.49 | wps 46612 | wpb 2034.1 | bsz 4 | num_updates 5866 | best_loss 7.813
2022-03-08 13:32:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5866 updates
2022-03-08 13:32:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 13:32:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 13:32:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 30 @ 5866 updates, score 8.069) (writing took 2.8900801381096244 seconds)
2022-03-08 13:32:48 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-08 13:32:48 | INFO | train | epoch 030 | loss 5.92 | ppl 60.54 | wps 20063.1 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 5866 | lr 0.000412885 | gnorm 0.682 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 19253
2022-03-08 13:32:48 | INFO | fairseq.trainer | begin training epoch 31
2022-03-08 13:32:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:34:38 | INFO | train_inner | epoch 031:     34 / 196 loss=5.906, ppl=59.97, wps=19836.5, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.682, loss_scale=8, train_wall=317, gb_free=14.1, wall=19363
2022-03-08 13:40:00 | INFO | train_inner | epoch 031:    134 / 196 loss=5.845, ppl=57.5, wps=20307.1, ups=0.31, wpb=65527.3, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.693, loss_scale=8, train_wall=318, gb_free=14.1, wall=19686
2022-03-08 13:43:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:43:24 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.104 | ppl 275.19 | wps 46398.8 | wpb 2034.1 | bsz 4 | num_updates 6062 | best_loss 7.813
2022-03-08 13:43:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6062 updates
2022-03-08 13:43:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 13:43:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 13:43:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 31 @ 6062 updates, score 8.104) (writing took 2.9067460149526596 seconds)
2022-03-08 13:43:27 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-08 13:43:27 | INFO | train | epoch 031 | loss 5.856 | ppl 57.94 | wps 20070.6 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6062 | lr 0.000406155 | gnorm 0.692 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 19892
2022-03-08 13:43:27 | INFO | fairseq.trainer | begin training epoch 32
2022-03-08 13:43:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:45:30 | INFO | train_inner | epoch 032:     38 / 196 loss=5.838, ppl=57.19, wps=19838.9, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.681, loss_scale=16, train_wall=317, gb_free=14.1, wall=20015
2022-03-08 13:50:53 | INFO | train_inner | epoch 032:    138 / 196 loss=5.789, ppl=55.28, wps=20294.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.698, loss_scale=16, train_wall=318, gb_free=14.1, wall=20338
2022-03-08 13:53:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 13:53:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:54:04 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.134 | ppl 280.96 | wps 46618.1 | wpb 2034.1 | bsz 4 | num_updates 6257 | best_loss 7.813
2022-03-08 13:54:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6257 updates
2022-03-08 13:54:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 13:54:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 13:54:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 32 @ 6257 updates, score 8.134) (writing took 2.8958283411338925 seconds)
2022-03-08 13:54:07 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-08 13:54:07 | INFO | train | epoch 032 | loss 5.795 | ppl 55.51 | wps 19955 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 6257 | lr 0.000399776 | gnorm 0.687 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 20532
2022-03-08 13:54:07 | INFO | fairseq.trainer | begin training epoch 33
2022-03-08 13:54:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:56:25 | INFO | train_inner | epoch 033:     43 / 196 loss=5.771, ppl=54.62, wps=19653.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.678, loss_scale=8, train_wall=320, gb_free=14.1, wall=20671
2022-03-08 14:01:48 | INFO | train_inner | epoch 033:    143 / 196 loss=5.741, ppl=53.47, wps=20305.4, ups=0.31, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.691, loss_scale=8, train_wall=318, gb_free=14.1, wall=20993
2022-03-08 14:04:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:04:43 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.169 | ppl 287.76 | wps 46397.9 | wpb 2034.1 | bsz 4 | num_updates 6453 | best_loss 7.813
2022-03-08 14:04:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6453 updates
2022-03-08 14:04:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 14:04:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 14:04:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 33 @ 6453 updates, score 8.169) (writing took 2.9255515225231647 seconds)
2022-03-08 14:04:46 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-08 14:04:46 | INFO | train | epoch 033 | loss 5.738 | ppl 53.36 | wps 20061 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6453 | lr 0.000393658 | gnorm 0.692 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 21171
2022-03-08 14:04:46 | INFO | fairseq.trainer | begin training epoch 34
2022-03-08 14:04:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:07:18 | INFO | train_inner | epoch 034:     47 / 196 loss=5.713, ppl=52.46, wps=19826.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.712, loss_scale=8, train_wall=317, gb_free=14.1, wall=21323
2022-03-08 14:12:40 | INFO | train_inner | epoch 034:    147 / 196 loss=5.686, ppl=51.49, wps=20312.8, ups=0.31, wpb=65527.3, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.711, loss_scale=8, train_wall=318, gb_free=14.1, wall=21646
2022-03-08 14:15:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:15:22 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.181 | ppl 290.27 | wps 46497.3 | wpb 2034.1 | bsz 4 | num_updates 6649 | best_loss 7.813
2022-03-08 14:15:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6649 updates
2022-03-08 14:15:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 14:15:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 14:15:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 34 @ 6649 updates, score 8.181) (writing took 2.9559315275400877 seconds)
2022-03-08 14:15:25 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-08 14:15:25 | INFO | train | epoch 034 | loss 5.682 | ppl 51.35 | wps 20065.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6649 | lr 0.000387813 | gnorm 0.7 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 21811
2022-03-08 14:15:25 | INFO | fairseq.trainer | begin training epoch 35
2022-03-08 14:15:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:18:10 | INFO | train_inner | epoch 035:     51 / 196 loss=5.643, ppl=49.97, wps=19836, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.683, loss_scale=8, train_wall=317, gb_free=14.1, wall=21975
2022-03-08 14:23:33 | INFO | train_inner | epoch 035:    151 / 196 loss=5.635, ppl=49.7, wps=20297, ups=0.31, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.698, loss_scale=16, train_wall=318, gb_free=14.1, wall=22298
2022-03-08 14:25:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:26:02 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.235 | ppl 301.31 | wps 46607.4 | wpb 2034.1 | bsz 4 | num_updates 6845 | best_loss 7.813
2022-03-08 14:26:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6845 updates
2022-03-08 14:26:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 14:26:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 14:26:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 35 @ 6845 updates, score 8.235) (writing took 3.0109130032360554 seconds)
2022-03-08 14:26:05 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-08 14:26:05 | INFO | train | epoch 035 | loss 5.629 | ppl 49.5 | wps 20053.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6845 | lr 0.00038222 | gnorm 0.702 | loss_scale 16 | train_wall 622 | gb_free 14.1 | wall 22450
2022-03-08 14:26:05 | INFO | fairseq.trainer | begin training epoch 36
2022-03-08 14:26:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:29:03 | INFO | train_inner | epoch 036:     55 / 196 loss=5.598, ppl=48.43, wps=19818.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.708, loss_scale=16, train_wall=317, gb_free=14.1, wall=22628
2022-03-08 14:33:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 14:34:29 | INFO | train_inner | epoch 036:    156 / 196 loss=5.587, ppl=48.07, wps=20098.2, ups=0.31, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.705, loss_scale=8, train_wall=321, gb_free=14.1, wall=22954
2022-03-08 14:36:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:36:42 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.268 | ppl 308.35 | wps 46230.9 | wpb 2034.1 | bsz 4 | num_updates 7040 | best_loss 7.813
2022-03-08 14:36:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7040 updates
2022-03-08 14:36:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 14:36:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 14:36:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 36 @ 7040 updates, score 8.268) (writing took 3.079968422651291 seconds)
2022-03-08 14:36:45 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-08 14:36:45 | INFO | train | epoch 036 | loss 5.578 | ppl 47.78 | wps 19950.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 7040 | lr 0.000376889 | gnorm 0.701 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 23090
2022-03-08 14:36:45 | INFO | fairseq.trainer | begin training epoch 37
2022-03-08 14:36:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:39:58 | INFO | train_inner | epoch 037:     60 / 196 loss=5.537, ppl=46.44, wps=19825, ups=0.3, wpb=65367, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.708, loss_scale=8, train_wall=317, gb_free=14.1, wall=23284
2022-03-08 14:45:21 | INFO | train_inner | epoch 037:    160 / 196 loss=5.547, ppl=46.75, wps=20319.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.707, loss_scale=8, train_wall=318, gb_free=14.1, wall=23606
2022-03-08 14:47:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:47:21 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.282 | ppl 311.32 | wps 46115.1 | wpb 2034.1 | bsz 4 | num_updates 7236 | best_loss 7.813
2022-03-08 14:47:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7236 updates
2022-03-08 14:47:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 14:47:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 14:47:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 37 @ 7236 updates, score 8.282) (writing took 3.104524662718177 seconds)
2022-03-08 14:47:24 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-08 14:47:24 | INFO | train | epoch 037 | loss 5.531 | ppl 46.23 | wps 20060.1 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 7236 | lr 0.00037175 | gnorm 0.708 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 23730
2022-03-08 14:47:24 | INFO | fairseq.trainer | begin training epoch 38
2022-03-08 14:47:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:50:51 | INFO | train_inner | epoch 038:     64 / 196 loss=5.481, ppl=44.68, wps=19821.3, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.696, loss_scale=8, train_wall=317, gb_free=14.1, wall=23936
2022-03-08 14:56:13 | INFO | train_inner | epoch 038:    164 / 196 loss=5.504, ppl=45.38, wps=20305.3, ups=0.31, wpb=65527.3, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.723, loss_scale=8, train_wall=318, gb_free=14.1, wall=24259
2022-03-08 14:57:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:58:01 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.337 | ppl 323.31 | wps 46166.2 | wpb 2034.1 | bsz 4 | num_updates 7432 | best_loss 7.813
2022-03-08 14:58:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7432 updates
2022-03-08 14:58:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 14:58:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 14:58:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 38 @ 7432 updates, score 8.337) (writing took 3.037281251512468 seconds)
2022-03-08 14:58:04 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-08 14:58:04 | INFO | train | epoch 038 | loss 5.485 | ppl 44.79 | wps 20055.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 7432 | lr 0.000366815 | gnorm 0.71 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 24369
2022-03-08 14:58:04 | INFO | fairseq.trainer | begin training epoch 39
2022-03-08 14:58:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:01:43 | INFO | train_inner | epoch 039:     68 / 196 loss=5.438, ppl=43.36, wps=19822.6, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=7500, lr=0.000365148, gnorm=0.703, loss_scale=16, train_wall=317, gb_free=14.1, wall=24589
2022-03-08 15:06:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 15:07:09 | INFO | train_inner | epoch 039:    169 / 196 loss=5.462, ppl=44.09, wps=20098.1, ups=0.31, wpb=65530.9, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.733, loss_scale=8, train_wall=321, gb_free=14.1, wall=24915
2022-03-08 15:08:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:08:40 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.369 | ppl 330.66 | wps 46042 | wpb 2034.1 | bsz 4 | num_updates 7627 | best_loss 7.813
2022-03-08 15:08:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7627 updates
2022-03-08 15:08:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 15:08:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 15:08:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 39 @ 7627 updates, score 8.369) (writing took 2.941529782488942 seconds)
2022-03-08 15:08:43 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-08 15:08:43 | INFO | train | epoch 039 | loss 5.441 | ppl 43.44 | wps 19956.6 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 7627 | lr 0.000362095 | gnorm 0.72 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 25009
2022-03-08 15:08:43 | INFO | fairseq.trainer | begin training epoch 40
2022-03-08 15:08:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:12:39 | INFO | train_inner | epoch 040:     73 / 196 loss=5.388, ppl=41.88, wps=19830, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=7700, lr=0.000360375, gnorm=0.704, loss_scale=8, train_wall=317, gb_free=14.1, wall=25244
2022-03-08 15:18:02 | INFO | train_inner | epoch 040:    173 / 196 loss=5.421, ppl=42.85, wps=20311.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.73, loss_scale=8, train_wall=318, gb_free=14.1, wall=25567
2022-03-08 15:19:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:19:20 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.371 | ppl 331.05 | wps 46558.1 | wpb 2034.1 | bsz 4 | num_updates 7823 | best_loss 7.813
2022-03-08 15:19:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7823 updates
2022-03-08 15:19:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 15:19:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 15:19:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 40 @ 7823 updates, score 8.371) (writing took 2.904837666079402 seconds)
2022-03-08 15:19:23 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-08 15:19:23 | INFO | train | epoch 040 | loss 5.398 | ppl 42.18 | wps 20066.2 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 7823 | lr 0.000357531 | gnorm 0.72 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 25648
2022-03-08 15:19:23 | INFO | fairseq.trainer | begin training epoch 41
2022-03-08 15:19:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:23:31 | INFO | train_inner | epoch 041:     77 / 196 loss=5.342, ppl=40.57, wps=19837.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=7900, lr=0.000355784, gnorm=0.731, loss_scale=8, train_wall=317, gb_free=14.1, wall=25896
2022-03-08 15:28:54 | INFO | train_inner | epoch 041:    177 / 196 loss=5.388, ppl=41.86, wps=20298.6, ups=0.31, wpb=65530.9, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.734, loss_scale=8, train_wall=318, gb_free=14.1, wall=26219
2022-03-08 15:29:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:29:59 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.413 | ppl 340.94 | wps 46300.8 | wpb 2034.1 | bsz 4 | num_updates 8019 | best_loss 7.813
2022-03-08 15:29:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 8019 updates
2022-03-08 15:29:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 15:30:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 15:30:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 41 @ 8019 updates, score 8.413) (writing took 2.9250737698748708 seconds)
2022-03-08 15:30:02 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-08 15:30:02 | INFO | train | epoch 041 | loss 5.358 | ppl 41.01 | wps 20058.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 8019 | lr 0.000353134 | gnorm 0.732 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 26287
2022-03-08 15:30:02 | INFO | fairseq.trainer | begin training epoch 42
2022-03-08 15:30:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:34:24 | INFO | train_inner | epoch 042:     81 / 196 loss=5.293, ppl=39.22, wps=19826.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=8100, lr=0.000351364, gnorm=0.725, loss_scale=16, train_wall=317, gb_free=14.1, wall=26549
2022-03-08 15:37:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 15:39:50 | INFO | train_inner | epoch 042:    182 / 196 loss=5.352, ppl=40.85, wps=20101.1, ups=0.31, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.739, loss_scale=8, train_wall=321, gb_free=14.1, wall=26875
2022-03-08 15:40:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:40:39 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.453 | ppl 350.46 | wps 46266.4 | wpb 2034.1 | bsz 4 | num_updates 8214 | best_loss 7.813
2022-03-08 15:40:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8214 updates
2022-03-08 15:40:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 15:40:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 15:40:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 42 @ 8214 updates, score 8.453) (writing took 2.9549349658191204 seconds)
2022-03-08 15:40:42 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-08 15:40:42 | INFO | train | epoch 042 | loss 5.319 | ppl 39.91 | wps 19954.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8214 | lr 0.000348917 | gnorm 0.731 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 26927
2022-03-08 15:40:42 | INFO | fairseq.trainer | begin training epoch 43
2022-03-08 15:40:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:45:19 | INFO | train_inner | epoch 043:     86 / 196 loss=5.257, ppl=38.23, wps=19833.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=8300, lr=0.000347105, gnorm=0.729, loss_scale=8, train_wall=317, gb_free=14.1, wall=27205
2022-03-08 15:50:42 | INFO | train_inner | epoch 043:    186 / 196 loss=5.318, ppl=39.89, wps=20301.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.729, loss_scale=8, train_wall=318, gb_free=14.1, wall=27527
2022-03-08 15:51:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:51:18 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.481 | ppl 357.29 | wps 46181.8 | wpb 2034.1 | bsz 4 | num_updates 8410 | best_loss 7.813
2022-03-08 15:51:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8410 updates
2022-03-08 15:51:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 15:51:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 15:51:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 43 @ 8410 updates, score 8.481) (writing took 2.9327729055657983 seconds)
2022-03-08 15:51:21 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-08 15:51:21 | INFO | train | epoch 043 | loss 5.282 | ppl 38.92 | wps 20058.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 8410 | lr 0.000344828 | gnorm 0.729 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 27567
2022-03-08 15:51:21 | INFO | fairseq.trainer | begin training epoch 44
2022-03-08 15:51:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:56:12 | INFO | train_inner | epoch 044:     90 / 196 loss=5.21, ppl=37, wps=19825.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=8500, lr=0.000342997, gnorm=0.738, loss_scale=8, train_wall=317, gb_free=14.1, wall=27857
2022-03-08 16:01:34 | INFO | train_inner | epoch 044:    190 / 196 loss=5.289, ppl=39.09, wps=20302.6, ups=0.31, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.737, loss_scale=8, train_wall=318, gb_free=14.1, wall=28180
2022-03-08 16:01:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:01:58 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.507 | ppl 363.68 | wps 46345.6 | wpb 2034.1 | bsz 4 | num_updates 8606 | best_loss 7.813
2022-03-08 16:01:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8606 updates
2022-03-08 16:01:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 16:02:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 16:02:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 44 @ 8606 updates, score 8.507) (writing took 2.934055171906948 seconds)
2022-03-08 16:02:01 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-08 16:02:01 | INFO | train | epoch 044 | loss 5.247 | ppl 37.98 | wps 20058.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 8606 | lr 0.000340878 | gnorm 0.741 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 28206
2022-03-08 16:02:01 | INFO | fairseq.trainer | begin training epoch 45
2022-03-08 16:02:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:07:04 | INFO | train_inner | epoch 045:     94 / 196 loss=5.175, ppl=36.14, wps=19827.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=8700, lr=0.000339032, gnorm=0.747, loss_scale=16, train_wall=317, gb_free=14.1, wall=28510
2022-03-08 16:11:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 16:12:30 | INFO | train_inner | epoch 045:    195 / 196 loss=5.255, ppl=38.2, wps=20092.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.741, loss_scale=8, train_wall=321, gb_free=14.1, wall=28836
2022-03-08 16:12:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:12:37 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.577 | ppl 381.94 | wps 46354.2 | wpb 2034.1 | bsz 4 | num_updates 8801 | best_loss 7.813
2022-03-08 16:12:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8801 updates
2022-03-08 16:12:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 16:12:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 16:12:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 45 @ 8801 updates, score 8.577) (writing took 2.9731765082105994 seconds)
2022-03-08 16:12:40 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-08 16:12:40 | INFO | train | epoch 045 | loss 5.212 | ppl 37.06 | wps 19949.5 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 8801 | lr 0.000337081 | gnorm 0.743 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 28846
2022-03-08 16:12:40 | INFO | fairseq.trainer | begin training epoch 46
2022-03-08 16:12:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:18:00 | INFO | train_inner | epoch 046:     99 / 196 loss=5.139, ppl=35.24, wps=19825.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=8900, lr=0.000335201, gnorm=0.757, loss_scale=8, train_wall=317, gb_free=14.1, wall=29165
2022-03-08 16:23:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:23:17 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.586 | ppl 384.39 | wps 46561.5 | wpb 2034.1 | bsz 4 | num_updates 8997 | best_loss 7.813
2022-03-08 16:23:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8997 updates
2022-03-08 16:23:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 16:23:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt
2022-03-08 16:23:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.04_0.06_0.9_#4/checkpoint_last.pt (epoch 46 @ 8997 updates, score 8.586) (writing took 3.0065223714336753 seconds)
2022-03-08 16:23:20 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-08 16:23:20 | INFO | train | epoch 046 | loss 5.179 | ppl 36.23 | wps 20059.9 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 8997 | lr 0.000333389 | gnorm 0.75 | loss_scale 8 | train_wall 622 | gb_free 14.1 | wall 29485
2022-03-08 16:23:20 | INFO | fairseq.trainer | begin training epoch 47
2022-03-08 16:23:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:23:30 | INFO | train_inner | epoch 047:      3 / 196 loss=5.216, ppl=37.16, wps=19829.8, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.744, loss_scale=8, train_wall=317, gb_free=14.1, wall=29495
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
