Sender: LSF System <lsfadmin@eu-g3-074>
Subject: Job 207014272: <w103_fp16_size_0.25_jelinek_0.02_0.08_0.9_#3> in cluster <euler> Exited

Job <w103_fp16_size_0.25_jelinek_0.02_0.08_0.9_#3> was submitted from host <eu-login-19> by user <andriusb> in cluster <euler> at Thu Mar  3 10:48:36 2022
Job was executed on host(s) <eu-g3-074>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Thu Mar  3 10:49:08 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Mar  3 10:49:08 2022
Terminated at Fri Mar  4 13:22:17 2022
Results reported at Fri Mar  4 13:22:17 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.02, 0.08, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321673 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   95540.86 sec.
    Max Memory :                                 8324 MB
    Average Memory :                             2931.18 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11676.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   95588 sec.
    Turnaround time :                            95621 sec.

The output (if any) follows:

2022-03-03 10:49:19 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321673, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321673, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.02, 0.08, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-03 10:49:20 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-03 10:49:23 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
Calculating frequency stats:
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 703/450337 [00:00<01:04, 7007.89it/s]  0%|          | 1404/450337 [00:00<01:09, 6419.70it/s]  0%|          | 2050/450337 [00:00<01:15, 5948.87it/s]  1%|          | 2705/450337 [00:00<01:12, 6166.71it/s]  1%|          | 3401/450337 [00:00<01:09, 6433.75it/s]  1%|          | 4108/450337 [00:00<01:07, 6637.87it/s]  1%|          | 4873/450337 [00:00<01:04, 6959.50it/s]  1%|          | 5584/450337 [00:00<01:03, 6994.59it/s]  1%|▏         | 6290/450337 [00:00<01:03, 7007.90it/s]  2%|▏         | 6993/450337 [00:01<01:08, 6517.56it/s]  2%|▏         | 7653/450337 [00:01<01:07, 6519.81it/s]  2%|▏         | 8311/450337 [00:01<01:08, 6465.14it/s]  2%|▏         | 8962/450337 [00:01<01:08, 6405.29it/s]  2%|▏         | 9605/450337 [00:01<01:09, 6303.19it/s]  2%|▏         | 10310/450337 [00:01<01:07, 6513.79it/s]  2%|▏         | 10964/450337 [00:01<01:08, 6389.51it/s]  3%|▎         | 11605/450337 [00:01<01:09, 6332.02it/s]  3%|▎         | 12270/450337 [00:01<01:08, 6424.28it/s]  3%|▎         | 12920/450337 [00:01<01:07, 6446.03it/s]  3%|▎         | 13659/450337 [00:02<01:04, 6721.69it/s]  3%|▎         | 14333/450337 [00:02<01:05, 6694.61it/s]  3%|▎         | 15019/450337 [00:02<01:04, 6741.33it/s]  3%|▎         | 15694/450337 [00:02<01:06, 6553.14it/s]  4%|▎         | 16351/450337 [00:02<01:08, 6347.02it/s]  4%|▍         | 16988/450337 [00:02<01:08, 6333.89it/s]  4%|▍         | 17623/450337 [00:02<01:08, 6333.48it/s]  4%|▍         | 18282/450337 [00:02<01:07, 6407.29it/s]  4%|▍         | 19040/450337 [00:02<01:03, 6752.30it/s]  4%|▍         | 19743/450337 [00:03<01:03, 6834.21it/s]  5%|▍         | 20428/450337 [00:03<01:05, 6522.38it/s]  5%|▍         | 21084/450337 [00:03<01:05, 6528.66it/s]  5%|▍         | 21740/450337 [00:03<01:07, 6354.68it/s]  5%|▍         | 22424/450337 [00:03<01:05, 6491.50it/s]  5%|▌         | 23112/450337 [00:03<01:04, 6604.43it/s]  5%|▌         | 23876/450337 [00:03<01:01, 6901.93it/s]  5%|▌         | 24721/450337 [00:03<00:57, 7353.87it/s]  6%|▌         | 25459/450337 [00:03<00:59, 7180.12it/s]  6%|▌         | 26180/450337 [00:03<01:01, 6854.73it/s]  6%|▌         | 26870/450337 [00:04<01:05, 6455.24it/s]  6%|▌         | 27522/450337 [00:04<01:07, 6239.08it/s]  6%|▋         | 28202/450337 [00:04<01:06, 6393.55it/s]  6%|▋         | 28913/450337 [00:04<01:03, 6595.72it/s]  7%|▋         | 29577/450337 [00:04<01:04, 6482.34it/s]  7%|▋         | 30229/450337 [00:04<01:04, 6465.48it/s]  7%|▋         | 30878/450337 [00:04<01:07, 6183.45it/s]  7%|▋         | 31544/450337 [00:04<01:06, 6315.76it/s]  7%|▋         | 32179/450337 [00:04<01:08, 6104.43it/s]  7%|▋         | 32829/450337 [00:05<01:07, 6215.48it/s]  7%|▋         | 33454/450337 [00:05<01:08, 6107.83it/s]  8%|▊         | 34108/450337 [00:05<01:06, 6229.63it/s]  8%|▊         | 34861/450337 [00:05<01:02, 6608.48it/s]  8%|▊         | 35525/450337 [00:05<01:05, 6331.06it/s]  8%|▊         | 36181/450337 [00:05<01:04, 6395.50it/s]  8%|▊         | 36824/450337 [00:05<01:04, 6405.44it/s]  8%|▊         | 37467/450337 [00:05<01:08, 6053.34it/s]  8%|▊         | 38081/450337 [00:05<01:07, 6076.91it/s]  9%|▊         | 38715/450337 [00:05<01:06, 6151.36it/s]  9%|▊         | 39351/450337 [00:06<01:06, 6202.99it/s]  9%|▉         | 39982/450337 [00:06<01:05, 6231.99it/s]  9%|▉         | 40715/450337 [00:06<01:02, 6555.72it/s]  9%|▉         | 41373/450337 [00:06<01:02, 6516.47it/s]  9%|▉         | 42026/450337 [00:06<01:06, 6101.73it/s]  9%|▉         | 42642/450337 [00:06<01:06, 6106.13it/s] 10%|▉         | 43257/450337 [00:06<01:07, 6046.30it/s] 10%|▉         | 43976/450337 [00:06<01:03, 6376.51it/s] 10%|▉         | 44678/450337 [00:06<01:01, 6560.88it/s] 10%|█         | 45337/450337 [00:07<01:02, 6429.40it/s] 10%|█         | 46057/450337 [00:07<01:00, 6650.42it/s] 10%|█         | 46745/450337 [00:07<01:00, 6712.49it/s] 11%|█         | 47763/450337 [00:07<00:52, 7734.58it/s] 11%|█         | 48540/450337 [00:07<00:53, 7470.74it/s] 11%|█         | 49313/450337 [00:07<00:53, 7542.70it/s] 11%|█         | 50071/450337 [00:07<00:55, 7198.65it/s] 11%|█▏        | 50796/450337 [00:07<00:59, 6758.82it/s] 11%|█▏        | 51480/450337 [00:07<00:59, 6683.38it/s] 12%|█▏        | 52364/450337 [00:07<00:54, 7286.92it/s] 12%|█▏        | 53101/450337 [00:08<00:56, 7025.66it/s] 12%|█▏        | 53811/450337 [00:08<00:57, 6943.58it/s] 12%|█▏        | 54510/450337 [00:08<01:01, 6445.65it/s] 12%|█▏        | 55184/450337 [00:08<01:00, 6521.32it/s] 12%|█▏        | 55865/450337 [00:08<00:59, 6596.78it/s] 13%|█▎        | 56561/450337 [00:08<00:58, 6696.53it/s] 13%|█▎        | 57235/450337 [00:08<01:02, 6252.66it/s] 13%|█▎        | 57889/450337 [00:08<01:02, 6328.81it/s] 13%|█▎        | 58696/450337 [00:08<00:57, 6820.63it/s] 13%|█▎        | 59385/450337 [00:09<01:01, 6376.55it/s] 13%|█▎        | 60033/450337 [00:09<01:01, 6325.39it/s] 13%|█▎        | 60722/450337 [00:09<01:00, 6484.17it/s] 14%|█▎        | 61529/450337 [00:09<00:56, 6933.29it/s] 14%|█▍        | 62229/450337 [00:09<00:58, 6621.88it/s] 14%|█▍        | 62927/450337 [00:09<00:57, 6713.74it/s] 14%|█▍        | 63604/450337 [00:09<00:59, 6529.00it/s] 14%|█▍        | 64261/450337 [00:09<01:00, 6431.18it/s] 14%|█▍        | 64963/450337 [00:09<00:58, 6595.67it/s] 15%|█▍        | 65626/450337 [00:10<00:59, 6451.33it/s] 15%|█▍        | 66278/450337 [00:10<00:59, 6467.01it/s] 15%|█▍        | 67001/450337 [00:10<00:57, 6687.61it/s] 15%|█▌        | 67672/450337 [00:10<01:00, 6377.62it/s] 15%|█▌        | 68314/450337 [00:10<01:01, 6246.54it/s] 15%|█▌        | 69078/450337 [00:10<00:57, 6635.03it/s] 16%|█▌        | 69812/450337 [00:10<00:55, 6831.78it/s] 16%|█▌        | 70499/450337 [00:10<00:55, 6839.76it/s] 16%|█▌        | 71186/450337 [00:10<00:58, 6513.32it/s] 16%|█▌        | 71844/450337 [00:10<00:57, 6529.44it/s] 16%|█▌        | 72551/450337 [00:11<00:56, 6685.63it/s] 16%|█▋        | 73367/450337 [00:11<00:52, 7115.07it/s] 16%|█▋        | 74219/450337 [00:11<00:49, 7527.66it/s] 17%|█▋        | 74975/450337 [00:11<00:51, 7286.54it/s] 17%|█▋        | 75708/450337 [00:11<00:54, 6855.51it/s] 17%|█▋        | 76449/450337 [00:11<00:53, 7010.48it/s] 17%|█▋        | 77156/450337 [00:11<00:55, 6775.94it/s] 17%|█▋        | 77887/450337 [00:11<00:53, 6917.60it/s] 17%|█▋        | 78584/450337 [00:11<00:56, 6541.71it/s] 18%|█▊        | 79245/450337 [00:12<01:00, 6113.46it/s] 18%|█▊        | 79865/450337 [00:12<01:00, 6079.56it/s] 18%|█▊        | 80507/450337 [00:12<00:59, 6166.63it/s] 18%|█▊        | 81253/450337 [00:12<00:56, 6530.39it/s] 18%|█▊        | 81912/450337 [00:12<00:56, 6485.35it/s] 18%|█▊        | 82577/450337 [00:12<00:56, 6526.39it/s] 19%|█▊        | 83313/450337 [00:12<00:54, 6765.35it/s] 19%|█▊        | 84006/450337 [00:12<00:53, 6811.06it/s] 19%|█▉        | 84689/450337 [00:12<00:54, 6716.40it/s] 19%|█▉        | 85363/450337 [00:12<00:56, 6474.22it/s] 19%|█▉        | 86063/450337 [00:13<00:55, 6617.30it/s] 19%|█▉        | 86752/450337 [00:13<00:54, 6692.21it/s] 19%|█▉        | 87473/450337 [00:13<00:53, 6836.95it/s] 20%|█▉        | 88202/450337 [00:13<00:51, 6968.45it/s] 20%|█▉        | 88901/450337 [00:13<00:56, 6350.53it/s] 20%|█▉        | 89548/450337 [00:13<00:57, 6320.71it/s] 20%|██        | 90188/450337 [00:13<00:59, 6067.28it/s] 20%|██        | 90802/450337 [00:13<00:59, 6028.23it/s] 20%|██        | 91554/450337 [00:13<00:55, 6445.45it/s] 20%|██        | 92209/450337 [00:14<00:55, 6470.13it/s] 21%|██        | 92885/450337 [00:14<00:54, 6554.02it/s] 21%|██        | 93544/450337 [00:14<00:54, 6546.41it/s] 21%|██        | 94201/450337 [00:14<00:54, 6479.28it/s] 21%|██        | 94941/450337 [00:14<00:52, 6746.39it/s] 21%|██        | 95618/450337 [00:14<00:55, 6436.35it/s] 21%|██▏       | 96350/450337 [00:14<00:52, 6689.88it/s] 22%|██▏       | 97189/450337 [00:14<00:49, 7182.46it/s] 22%|██▏       | 97912/450337 [00:14<00:51, 6799.39it/s] 22%|██▏       | 98599/450337 [00:14<00:53, 6628.51it/s] 22%|██▏       | 99285/450337 [00:15<00:52, 6692.07it/s] 22%|██▏       | 99959/450337 [00:15<00:54, 6392.95it/s] 22%|██▏       | 100603/450337 [00:15<00:56, 6213.08it/s] 23%|██▎       | 101327/450337 [00:15<00:53, 6490.59it/s] 23%|██▎       | 102026/450337 [00:15<00:52, 6632.57it/s] 23%|██▎       | 102693/450337 [00:15<00:52, 6628.84it/s] 23%|██▎       | 103359/450337 [00:15<00:53, 6451.14it/s] 23%|██▎       | 104007/450337 [00:15<00:55, 6243.76it/s] 23%|██▎       | 104676/450337 [00:15<00:54, 6365.33it/s] 23%|██▎       | 105315/450337 [00:16<00:54, 6278.63it/s] 24%|██▎       | 105999/450337 [00:16<00:53, 6439.00it/s] 24%|██▎       | 106645/450337 [00:16<00:53, 6395.25it/s] 24%|██▍       | 107286/450337 [00:16<00:55, 6214.77it/s] 24%|██▍       | 107961/450337 [00:16<00:53, 6363.71it/s] 24%|██▍       | 108600/450337 [00:16<00:55, 6194.35it/s] 24%|██▍       | 109222/450337 [00:16<00:55, 6105.27it/s] 24%|██▍       | 109899/450337 [00:16<00:54, 6291.64it/s] 25%|██▍       | 110581/450337 [00:16<00:52, 6443.98it/s] 25%|██▍       | 111296/450337 [00:16<00:50, 6649.52it/s] 25%|██▍       | 111963/450337 [00:17<00:51, 6613.67it/s] 25%|██▌       | 112678/450337 [00:17<00:49, 6771.85it/s] 25%|██▌       | 113357/450337 [00:17<00:52, 6440.04it/s] 25%|██▌       | 114005/450337 [00:17<00:52, 6437.07it/s] 25%|██▌       | 114652/450337 [00:17<00:52, 6360.32it/s] 26%|██▌       | 115318/450337 [00:17<00:51, 6445.63it/s] 26%|██▌       | 115965/450337 [00:17<00:54, 6128.60it/s] 26%|██▌       | 116586/450337 [00:17<00:54, 6148.65it/s] 26%|██▌       | 117204/450337 [00:17<00:54, 6085.02it/s] 26%|██▌       | 117821/450337 [00:18<00:54, 6107.40it/s] 26%|██▋       | 118557/450337 [00:18<00:51, 6466.99it/s] 26%|██▋       | 119206/450337 [00:18<00:52, 6254.48it/s] 27%|██▋       | 119834/450337 [00:18<00:53, 6231.03it/s] 27%|██▋       | 120624/450337 [00:18<00:49, 6713.27it/s] 27%|██▋       | 121298/450337 [00:18<00:50, 6562.93it/s] 27%|██▋       | 121957/450337 [00:18<00:52, 6282.98it/s] 27%|██▋       | 122642/450337 [00:18<00:50, 6442.12it/s] 27%|██▋       | 123290/450337 [00:18<00:52, 6280.52it/s] 28%|██▊       | 123940/450337 [00:18<00:51, 6339.92it/s] 28%|██▊       | 124589/450337 [00:19<00:51, 6381.97it/s] 28%|██▊       | 125229/450337 [00:19<00:51, 6350.65it/s] 28%|██▊       | 125955/450337 [00:19<00:49, 6617.84it/s] 28%|██▊       | 126674/450337 [00:19<00:47, 6781.45it/s] 28%|██▊       | 127354/450337 [00:19<00:49, 6551.80it/s] 28%|██▊       | 128057/450337 [00:19<00:48, 6688.29it/s] 29%|██▊       | 128728/450337 [00:19<00:49, 6452.46it/s] 29%|██▊       | 129421/450337 [00:19<00:48, 6588.17it/s] 29%|██▉       | 130083/450337 [00:19<00:50, 6302.85it/s] 29%|██▉       | 130718/450337 [00:20<00:51, 6217.55it/s] 29%|██▉       | 131370/450337 [00:20<00:50, 6303.59it/s] 29%|██▉       | 132003/450337 [00:20<00:54, 5793.31it/s] 29%|██▉       | 132762/450337 [00:20<00:50, 6286.88it/s] 30%|██▉       | 133402/450337 [00:20<00:50, 6299.91it/s] 30%|██▉       | 134133/450337 [00:20<00:47, 6587.81it/s] 30%|██▉       | 134884/450337 [00:20<00:46, 6855.53it/s] 30%|███       | 135632/450337 [00:20<00:44, 7034.27it/s] 30%|███       | 136340/450337 [00:20<00:45, 6884.89it/s] 30%|███       | 137182/450337 [00:20<00:42, 7325.56it/s] 31%|███       | 137919/450337 [00:21<00:44, 6993.47it/s] 31%|███       | 138624/450337 [00:21<00:44, 6939.33it/s] 31%|███       | 139322/450337 [00:21<00:46, 6707.49it/s] 31%|███       | 139997/450337 [00:21<00:46, 6693.28it/s] 31%|███       | 140680/450337 [00:21<00:46, 6721.61it/s] 31%|███▏      | 141354/450337 [00:21<00:48, 6426.90it/s] 32%|███▏      | 142013/450337 [00:21<00:47, 6464.84it/s] 32%|███▏      | 142662/450337 [00:21<00:47, 6417.99it/s] 32%|███▏      | 143306/450337 [00:21<00:48, 6367.46it/s] 32%|███▏      | 144107/450337 [00:22<00:44, 6837.66it/s] 32%|███▏      | 144812/450337 [00:22<00:44, 6899.92it/s] 32%|███▏      | 145580/450337 [00:22<00:42, 7119.89it/s] 32%|███▏      | 146294/450337 [00:22<00:43, 6973.46it/s] 33%|███▎      | 146993/450337 [00:22<00:46, 6577.94it/s] 33%|███▎      | 147656/450337 [00:22<00:48, 6295.38it/s] 33%|███▎      | 148291/450337 [00:22<00:48, 6285.27it/s] 33%|███▎      | 148987/450337 [00:22<00:46, 6473.37it/s] 33%|███▎      | 149638/450337 [00:22<00:47, 6286.00it/s] 33%|███▎      | 150270/450337 [00:22<00:48, 6164.32it/s] 34%|███▎      | 150908/450337 [00:23<00:48, 6219.08it/s] 34%|███▎      | 151577/450337 [00:23<00:47, 6351.24it/s] 34%|███▍      | 152214/450337 [00:23<00:49, 6047.95it/s] 34%|███▍      | 152927/450337 [00:23<00:46, 6355.37it/s] 34%|███▍      | 153602/450337 [00:23<00:45, 6464.32it/s] 34%|███▍      | 154252/450337 [00:23<00:46, 6388.25it/s] 34%|███▍      | 154904/450337 [00:23<00:45, 6424.46it/s] 35%|███▍      | 155587/450337 [00:23<00:45, 6540.37it/s] 35%|███▍      | 156310/450337 [00:23<00:43, 6738.47it/s] 35%|███▍      | 156986/450337 [00:24<00:44, 6606.87it/s] 35%|███▌      | 157715/450337 [00:24<00:42, 6807.17it/s] 35%|███▌      | 158463/450337 [00:24<00:41, 7004.07it/s] 35%|███▌      | 159165/450337 [00:24<00:45, 6424.00it/s] 36%|███▌      | 159939/450337 [00:24<00:42, 6791.44it/s] 36%|███▌      | 160628/450337 [00:24<00:43, 6657.71it/s] 36%|███▌      | 161335/450337 [00:24<00:42, 6769.32it/s] 36%|███▌      | 162018/450337 [00:24<00:43, 6569.20it/s] 36%|███▌      | 162740/450337 [00:24<00:42, 6749.92it/s] 36%|███▋      | 163420/450337 [00:24<00:42, 6739.15it/s] 36%|███▋      | 164097/450337 [00:25<00:45, 6352.77it/s] 37%|███▋      | 164822/450337 [00:25<00:43, 6605.54it/s] 37%|███▋      | 165545/450337 [00:25<00:41, 6783.41it/s] 37%|███▋      | 166229/450337 [00:25<00:42, 6683.46it/s] 37%|███▋      | 167009/450337 [00:25<00:40, 7001.98it/s] 37%|███▋      | 167713/450337 [00:25<00:41, 6834.18it/s] 37%|███▋      | 168400/450337 [00:25<00:44, 6358.97it/s] 38%|███▊      | 169121/450337 [00:25<00:42, 6591.54it/s] 38%|███▊      | 169788/450337 [00:25<00:44, 6319.55it/s] 38%|███▊      | 170427/450337 [00:26<00:45, 6145.19it/s] 38%|███▊      | 171047/450337 [00:26<00:45, 6126.09it/s] 38%|███▊      | 171749/450337 [00:26<00:43, 6371.35it/s] 38%|███▊      | 172390/450337 [00:26<00:44, 6280.43it/s] 38%|███▊      | 173040/450337 [00:26<00:43, 6343.41it/s] 39%|███▊      | 173696/450337 [00:26<00:43, 6404.54it/s] 39%|███▊      | 174338/450337 [00:26<00:43, 6303.89it/s] 39%|███▉      | 175075/450337 [00:26<00:41, 6604.56it/s] 39%|███▉      | 175737/450337 [00:26<00:43, 6280.08it/s] 39%|███▉      | 176395/450337 [00:26<00:43, 6362.13it/s] 39%|███▉      | 177035/450337 [00:27<00:43, 6234.02it/s] 39%|███▉      | 177661/450337 [00:27<00:47, 5766.59it/s] 40%|███▉      | 178364/450337 [00:27<00:44, 6108.81it/s] 40%|███▉      | 179051/450337 [00:27<00:42, 6317.15it/s] 40%|███▉      | 179690/450337 [00:27<00:43, 6256.38it/s] 40%|████      | 180321/450337 [00:27<00:43, 6226.87it/s] 40%|████      | 180947/450337 [00:27<00:43, 6151.99it/s] 40%|████      | 181565/450337 [00:27<00:44, 6095.13it/s] 40%|████      | 182221/450337 [00:27<00:43, 6223.56it/s] 41%|████      | 182845/450337 [00:28<00:43, 6191.31it/s] 41%|████      | 183481/450337 [00:28<00:42, 6237.91it/s] 41%|████      | 184106/450337 [00:28<00:46, 5743.91it/s] 41%|████      | 184814/450337 [00:28<00:43, 6112.45it/s] 41%|████      | 185434/450337 [00:28<00:44, 5933.17it/s] 41%|████▏     | 186038/450337 [00:28<00:44, 5961.31it/s] 41%|████▏     | 186668/450337 [00:28<00:43, 6058.50it/s] 42%|████▏     | 187349/450337 [00:28<00:41, 6274.27it/s] 42%|████▏     | 187980/450337 [00:28<00:42, 6190.80it/s] 42%|████▏     | 188748/450337 [00:29<00:39, 6623.00it/s] 42%|████▏     | 189413/450337 [00:29<00:40, 6411.52it/s] 42%|████▏     | 190072/450337 [00:29<00:40, 6461.16it/s] 42%|████▏     | 190752/450337 [00:29<00:39, 6557.47it/s] 43%|████▎     | 191410/450337 [00:29<00:39, 6487.73it/s] 43%|████▎     | 192081/450337 [00:29<00:39, 6546.03it/s] 43%|████▎     | 192773/450337 [00:29<00:38, 6654.68it/s] 43%|████▎     | 193440/450337 [00:29<00:38, 6608.88it/s] 43%|████▎     | 194125/450337 [00:29<00:38, 6669.47it/s] 43%|████▎     | 194793/450337 [00:29<00:39, 6430.76it/s] 43%|████▎     | 195462/450337 [00:30<00:39, 6499.95it/s] 44%|████▎     | 196114/450337 [00:30<00:39, 6401.97it/s] 44%|████▎     | 196756/450337 [00:30<00:40, 6284.43it/s] 44%|████▍     | 197795/450337 [00:30<00:33, 7473.33it/s] 44%|████▍     | 198548/450337 [00:30<00:36, 6816.48it/s] 44%|████▍     | 199244/450337 [00:30<00:37, 6784.10it/s] 44%|████▍     | 199932/450337 [00:30<00:37, 6613.08it/s] 45%|████▍     | 200600/450337 [00:30<00:40, 6232.32it/s] 45%|████▍     | 201275/450337 [00:30<00:39, 6370.84it/s] 45%|████▍     | 201925/450337 [00:31<00:38, 6405.02it/s] 45%|████▍     | 202571/450337 [00:31<00:38, 6379.60it/s] 45%|████▌     | 203259/450337 [00:31<00:37, 6520.96it/s] 45%|████▌     | 203993/450337 [00:31<00:36, 6753.27it/s] 45%|████▌     | 204671/450337 [00:31<00:38, 6455.87it/s] 46%|████▌     | 205321/450337 [00:31<00:38, 6387.16it/s] 46%|████▌     | 205963/450337 [00:31<00:38, 6306.13it/s] 46%|████▌     | 206612/450337 [00:31<00:38, 6357.06it/s] 46%|████▌     | 207250/450337 [00:31<00:39, 6209.21it/s] 46%|████▌     | 207879/450337 [00:31<00:38, 6232.15it/s] 46%|████▋     | 208504/450337 [00:32<00:39, 6183.08it/s] 46%|████▋     | 209124/450337 [00:32<00:40, 5960.92it/s] 47%|████▋     | 209817/450337 [00:32<00:38, 6238.69it/s] 47%|████▋     | 210542/450337 [00:32<00:36, 6532.79it/s] 47%|████▋     | 211198/450337 [00:32<00:36, 6539.75it/s] 47%|████▋     | 211854/450337 [00:32<00:36, 6531.48it/s] 47%|████▋     | 212513/450337 [00:32<00:36, 6543.49it/s] 47%|████▋     | 213226/450337 [00:32<00:35, 6712.86it/s] 47%|████▋     | 213899/450337 [00:32<00:37, 6293.79it/s] 48%|████▊     | 214535/450337 [00:32<00:37, 6246.75it/s] 48%|████▊     | 215164/450337 [00:33<00:38, 6130.89it/s] 48%|████▊     | 215780/450337 [00:33<00:39, 5973.17it/s] 48%|████▊     | 216417/450337 [00:33<00:38, 6081.52it/s] 48%|████▊     | 217030/450337 [00:33<00:38, 6092.35it/s] 48%|████▊     | 217687/450337 [00:33<00:37, 6230.85it/s] 48%|████▊     | 218404/450337 [00:33<00:35, 6507.11it/s] 49%|████▊     | 219084/450337 [00:33<00:35, 6592.59it/s] 49%|████▉     | 219745/450337 [00:33<00:36, 6378.01it/s] 49%|████▉     | 220395/450337 [00:33<00:35, 6404.80it/s] 49%|████▉     | 221087/450337 [00:34<00:34, 6553.68it/s] 49%|████▉     | 221744/450337 [00:34<00:35, 6350.93it/s] 49%|████▉     | 222382/450337 [00:34<00:37, 6011.78it/s] 50%|████▉     | 223061/450337 [00:34<00:36, 6229.22it/s] 50%|████▉     | 223689/450337 [00:34<00:38, 5955.78it/s] 50%|████▉     | 224441/450337 [00:34<00:35, 6394.81it/s] 50%|████▉     | 225110/450337 [00:34<00:34, 6477.45it/s] 50%|█████     | 225925/450337 [00:34<00:32, 6962.63it/s] 50%|█████     | 226627/450337 [00:34<00:32, 6925.78it/s] 50%|█████     | 227324/450337 [00:34<00:33, 6649.01it/s] 51%|█████     | 227994/450337 [00:35<00:34, 6407.70it/s] 51%|█████     | 228639/450337 [00:35<00:35, 6270.50it/s] 51%|█████     | 229269/450337 [00:35<00:35, 6254.45it/s] 51%|█████     | 229897/450337 [00:35<00:37, 5833.88it/s] 51%|█████     | 230572/450337 [00:35<00:36, 6081.40it/s] 51%|█████▏    | 231213/450337 [00:35<00:35, 6171.84it/s] 51%|█████▏    | 231836/450337 [00:35<00:36, 6016.91it/s] 52%|█████▏    | 232477/450337 [00:35<00:35, 6129.05it/s] 52%|█████▏    | 233288/450337 [00:35<00:32, 6702.00it/s] 52%|█████▏    | 233963/450337 [00:36<00:32, 6707.96it/s] 52%|█████▏    | 234727/450337 [00:36<00:30, 6978.62it/s] 52%|█████▏    | 235448/450337 [00:36<00:30, 7039.31it/s] 52%|█████▏    | 236154/450337 [00:36<00:32, 6675.69it/s] 53%|█████▎    | 236827/450337 [00:36<00:33, 6376.31it/s] 53%|█████▎    | 237511/450337 [00:36<00:32, 6506.09it/s] 53%|█████▎    | 238167/450337 [00:36<00:33, 6253.38it/s] 53%|█████▎    | 238960/450337 [00:36<00:31, 6724.66it/s] 53%|█████▎    | 239639/450337 [00:36<00:31, 6587.54it/s] 53%|█████▎    | 240303/450337 [00:37<00:32, 6414.97it/s] 54%|█████▎    | 241191/450337 [00:37<00:29, 7110.23it/s] 54%|█████▎    | 241909/450337 [00:37<00:29, 6980.16it/s] 54%|█████▍    | 242612/450337 [00:37<00:30, 6795.68it/s] 54%|█████▍    | 243296/450337 [00:37<00:31, 6650.09it/s] 54%|█████▍    | 243964/450337 [00:37<00:31, 6566.95it/s] 54%|█████▍    | 244663/450337 [00:37<00:30, 6676.77it/s] 54%|█████▍    | 245333/450337 [00:37<00:31, 6582.18it/s] 55%|█████▍    | 246082/450337 [00:37<00:29, 6838.80it/s] 55%|█████▍    | 246902/450337 [00:37<00:28, 7234.88it/s] 55%|█████▍    | 247628/450337 [00:38<00:29, 6959.20it/s] 55%|█████▌    | 248328/450337 [00:38<00:29, 6771.26it/s] 55%|█████▌    | 249064/450337 [00:38<00:29, 6936.85it/s] 55%|█████▌    | 249799/450337 [00:38<00:28, 7047.96it/s] 56%|█████▌    | 250507/450337 [00:38<00:29, 6800.96it/s] 56%|█████▌    | 251191/450337 [00:38<00:29, 6808.99it/s] 56%|█████▌    | 251875/450337 [00:38<00:29, 6786.94it/s] 56%|█████▌    | 252556/450337 [00:38<00:29, 6724.69it/s] 56%|█████▌    | 253230/450337 [00:38<00:30, 6494.99it/s] 56%|█████▋    | 253980/450337 [00:39<00:28, 6780.78it/s] 57%|█████▋    | 254661/450337 [00:39<00:30, 6481.36it/s] 57%|█████▋    | 255314/450337 [00:39<00:30, 6435.43it/s] 57%|█████▋    | 256105/450337 [00:39<00:28, 6852.20it/s] 57%|█████▋    | 256794/450337 [00:39<00:28, 6846.46it/s] 57%|█████▋    | 257482/450337 [00:39<00:28, 6766.34it/s] 57%|█████▋    | 258161/450337 [00:39<00:29, 6597.92it/s] 57%|█████▋    | 258823/450337 [00:39<00:30, 6231.57it/s] 58%|█████▊    | 259451/450337 [00:39<00:31, 6098.49it/s] 58%|█████▊    | 260138/450337 [00:39<00:30, 6306.93it/s] 58%|█████▊    | 260927/450337 [00:40<00:28, 6752.96it/s] 58%|█████▊    | 261607/450337 [00:40<00:28, 6686.02it/s] 58%|█████▊    | 262279/450337 [00:40<00:29, 6356.34it/s] 58%|█████▊    | 262920/450337 [00:40<00:29, 6317.16it/s] 59%|█████▊    | 263555/450337 [00:40<00:29, 6228.55it/s] 59%|█████▊    | 264181/450337 [00:40<00:30, 6202.62it/s] 59%|█████▉    | 264835/450337 [00:40<00:29, 6300.19it/s] 59%|█████▉    | 265467/450337 [00:40<00:29, 6274.89it/s] 59%|█████▉    | 266096/450337 [00:40<00:29, 6143.91it/s] 59%|█████▉    | 266712/450337 [00:41<00:29, 6146.22it/s] 59%|█████▉    | 267388/450337 [00:41<00:28, 6325.91it/s] 60%|█████▉    | 268022/450337 [00:41<00:29, 6206.34it/s] 60%|█████▉    | 268724/450337 [00:41<00:28, 6443.88it/s] 60%|█████▉    | 269412/450337 [00:41<00:27, 6569.21it/s] 60%|█████▉    | 270071/450337 [00:41<00:28, 6395.45it/s] 60%|██████    | 270744/450337 [00:41<00:27, 6485.39it/s] 60%|██████    | 271394/450337 [00:41<00:28, 6303.75it/s] 60%|██████    | 272065/450337 [00:41<00:27, 6421.02it/s] 61%|██████    | 272894/450337 [00:41<00:25, 6967.36it/s] 61%|██████    | 273594/450337 [00:42<00:25, 6941.36it/s] 61%|██████    | 274290/450337 [00:42<00:25, 6771.88it/s] 61%|██████    | 274970/450337 [00:42<00:27, 6441.25it/s] 61%|██████    | 275619/450337 [00:42<00:28, 6231.00it/s] 61%|██████▏   | 276246/450337 [00:42<00:28, 6198.12it/s] 61%|██████▏   | 276869/450337 [00:42<00:28, 6088.53it/s] 62%|██████▏   | 277489/450337 [00:42<00:28, 6119.13it/s] 62%|██████▏   | 278156/450337 [00:42<00:27, 6277.59it/s] 62%|██████▏   | 278879/450337 [00:42<00:26, 6555.49it/s] 62%|██████▏   | 279537/450337 [00:43<00:26, 6402.36it/s] 62%|██████▏   | 280251/450337 [00:43<00:25, 6616.92it/s] 62%|██████▏   | 280915/450337 [00:43<00:27, 6121.25it/s] 63%|██████▎   | 281582/450337 [00:43<00:26, 6273.87it/s] 63%|██████▎   | 282217/450337 [00:43<00:26, 6256.21it/s] 63%|██████▎   | 282848/450337 [00:43<00:27, 6169.59it/s] 63%|██████▎   | 283571/450337 [00:43<00:25, 6470.08it/s] 63%|██████▎   | 284225/450337 [00:43<00:25, 6485.05it/s] 63%|██████▎   | 284876/450337 [00:43<00:25, 6394.38it/s] 63%|██████▎   | 285518/450337 [00:43<00:26, 6263.88it/s] 64%|██████▎   | 286173/450337 [00:44<00:25, 6340.73it/s] 64%|██████▎   | 286809/450337 [00:44<00:25, 6300.69it/s] 64%|██████▍   | 287441/450337 [00:44<00:26, 6205.32it/s] 64%|██████▍   | 288167/450337 [00:44<00:24, 6506.84it/s] 64%|██████▍   | 288819/450337 [00:44<00:24, 6505.93it/s] 64%|██████▍   | 289471/450337 [00:44<00:24, 6506.27it/s] 64%|██████▍   | 290123/450337 [00:44<00:26, 6057.76it/s] 65%|██████▍   | 290801/450337 [00:44<00:25, 6259.96it/s] 65%|██████▍   | 291433/450337 [00:44<00:25, 6189.44it/s] 65%|██████▍   | 292056/450337 [00:45<00:25, 6162.94it/s] 65%|██████▌   | 292780/450337 [00:45<00:24, 6468.20it/s] 65%|██████▌   | 293430/450337 [00:45<00:25, 6241.05it/s] 65%|██████▌   | 294058/450337 [00:45<00:25, 6230.40it/s] 65%|██████▌   | 294765/450337 [00:45<00:24, 6472.24it/s] 66%|██████▌   | 295454/450337 [00:45<00:23, 6591.80it/s] 66%|██████▌   | 296116/450337 [00:45<00:23, 6588.89it/s] 66%|██████▌   | 296777/450337 [00:45<00:23, 6439.89it/s] 66%|██████▌   | 297423/450337 [00:45<00:24, 6348.10it/s] 66%|██████▌   | 298126/450337 [00:45<00:23, 6544.96it/s] 66%|██████▋   | 298782/450337 [00:46<00:23, 6482.05it/s] 66%|██████▋   | 299460/450337 [00:46<00:22, 6564.31it/s] 67%|██████▋   | 300137/450337 [00:46<00:22, 6623.74it/s] 67%|██████▋   | 300801/450337 [00:46<00:23, 6470.24it/s] 67%|██████▋   | 301450/450337 [00:46<00:23, 6292.77it/s] 67%|██████▋   | 302081/450337 [00:46<00:24, 6006.80it/s] 67%|██████▋   | 302820/450337 [00:46<00:23, 6396.10it/s] 67%|██████▋   | 303494/450337 [00:46<00:22, 6490.39it/s] 68%|██████▊   | 304161/450337 [00:46<00:22, 6539.41it/s] 68%|██████▊   | 304818/450337 [00:46<00:22, 6400.86it/s] 68%|██████▊   | 305461/450337 [00:47<00:22, 6332.72it/s] 68%|██████▊   | 306122/450337 [00:47<00:22, 6410.74it/s] 68%|██████▊   | 306765/450337 [00:47<00:23, 6189.89it/s] 68%|██████▊   | 307501/450337 [00:47<00:21, 6526.52it/s] 68%|██████▊   | 308157/450337 [00:47<00:22, 6196.68it/s] 69%|██████▊   | 308782/450337 [00:47<00:23, 6093.31it/s] 69%|██████▉   | 309747/450337 [00:47<00:19, 7103.32it/s] 69%|██████▉   | 310466/450337 [00:47<00:20, 6934.17it/s] 69%|██████▉   | 311166/450337 [00:47<00:20, 6646.96it/s] 69%|██████▉   | 311837/450337 [00:48<00:21, 6381.76it/s] 69%|██████▉   | 312506/450337 [00:48<00:21, 6464.47it/s] 70%|██████▉   | 313201/450337 [00:48<00:20, 6600.50it/s] 70%|██████▉   | 313972/450337 [00:48<00:19, 6914.53it/s] 70%|██████▉   | 314668/450337 [00:48<00:20, 6697.13it/s] 70%|███████   | 315342/450337 [00:48<00:21, 6378.39it/s] 70%|███████   | 316010/450337 [00:48<00:20, 6462.22it/s] 70%|███████   | 316661/450337 [00:48<00:21, 6308.22it/s] 70%|███████   | 317295/450337 [00:48<00:21, 6270.01it/s] 71%|███████   | 317924/450337 [00:49<00:21, 6075.91it/s] 71%|███████   | 318573/450337 [00:49<00:21, 6193.33it/s] 71%|███████   | 319265/450337 [00:49<00:20, 6402.26it/s] 71%|███████   | 319908/450337 [00:49<00:20, 6406.72it/s] 71%|███████   | 320551/450337 [00:49<00:21, 6099.30it/s] 71%|███████▏  | 321378/450337 [00:49<00:19, 6717.18it/s] 72%|███████▏  | 322056/450337 [00:49<00:19, 6444.89it/s] 72%|███████▏  | 322707/450337 [00:49<00:20, 6273.27it/s] 72%|███████▏  | 323373/450337 [00:49<00:19, 6376.91it/s] 72%|███████▏  | 324015/450337 [00:49<00:19, 6380.55it/s] 72%|███████▏  | 324698/450337 [00:50<00:19, 6508.74it/s] 72%|███████▏  | 325352/450337 [00:50<00:20, 6119.10it/s] 72%|███████▏  | 325979/450337 [00:50<00:20, 6153.15it/s] 73%|███████▎  | 326680/450337 [00:50<00:19, 6397.31it/s] 73%|███████▎  | 327419/450337 [00:50<00:18, 6684.43it/s] 73%|███████▎  | 328235/450337 [00:50<00:17, 7105.99it/s] 73%|███████▎  | 328949/450337 [00:50<00:17, 7026.76it/s] 73%|███████▎  | 329655/450337 [00:50<00:18, 6610.29it/s] 73%|███████▎  | 330323/450337 [00:50<00:19, 6282.14it/s] 74%|███████▎  | 331009/450337 [00:51<00:18, 6436.40it/s] 74%|███████▎  | 331659/450337 [00:51<00:18, 6297.98it/s] 74%|███████▍  | 332293/450337 [00:51<00:18, 6233.40it/s] 74%|███████▍  | 332941/450337 [00:51<00:18, 6300.24it/s] 74%|███████▍  | 333574/450337 [00:51<00:18, 6245.32it/s] 74%|███████▍  | 334201/450337 [00:51<00:18, 6237.09it/s] 74%|███████▍  | 334906/450337 [00:51<00:17, 6471.23it/s] 75%|███████▍  | 335569/450337 [00:51<00:17, 6501.43it/s] 75%|███████▍  | 336234/450337 [00:51<00:17, 6541.92it/s] 75%|███████▍  | 336889/450337 [00:51<00:17, 6541.90it/s] 75%|███████▍  | 337626/450337 [00:52<00:16, 6785.94it/s] 75%|███████▌  | 338306/450337 [00:52<00:16, 6781.20it/s] 75%|███████▌  | 339014/450337 [00:52<00:16, 6870.19it/s] 75%|███████▌  | 339702/450337 [00:52<00:16, 6672.61it/s] 76%|███████▌  | 340371/450337 [00:52<00:17, 6150.82it/s] 76%|███████▌  | 341035/450337 [00:52<00:17, 6283.45it/s] 76%|███████▌  | 341708/450337 [00:52<00:16, 6408.31it/s] 76%|███████▌  | 342401/450337 [00:52<00:16, 6553.81it/s] 76%|███████▌  | 343072/450337 [00:52<00:16, 6597.14it/s] 76%|███████▋  | 343838/450337 [00:52<00:15, 6909.25it/s] 77%|███████▋  | 344532/450337 [00:53<00:16, 6370.29it/s] 77%|███████▋  | 345261/450337 [00:53<00:15, 6626.17it/s] 77%|███████▋  | 345933/450337 [00:53<00:17, 6127.81it/s] 77%|███████▋  | 346659/450337 [00:53<00:16, 6432.38it/s] 77%|███████▋  | 347314/450337 [00:53<00:15, 6439.67it/s] 77%|███████▋  | 347967/450337 [00:53<00:15, 6463.11it/s] 77%|███████▋  | 348620/450337 [00:53<00:15, 6382.83it/s] 78%|███████▊  | 349263/450337 [00:53<00:15, 6373.21it/s] 78%|███████▊  | 349973/450337 [00:53<00:15, 6582.78it/s] 78%|███████▊  | 350761/450337 [00:54<00:14, 6958.40it/s] 78%|███████▊  | 351460/450337 [00:54<00:14, 6868.65it/s] 78%|███████▊  | 352149/450337 [00:54<00:15, 6361.03it/s] 78%|███████▊  | 352794/450337 [00:54<00:16, 6066.23it/s] 78%|███████▊  | 353484/450337 [00:54<00:15, 6292.61it/s] 79%|███████▊  | 354121/450337 [00:54<00:15, 6124.20it/s] 79%|███████▉  | 354871/450337 [00:54<00:14, 6510.55it/s] 79%|███████▉  | 355529/450337 [00:54<00:15, 6270.64it/s] 79%|███████▉  | 356203/450337 [00:54<00:14, 6398.63it/s] 79%|███████▉  | 356848/450337 [00:55<00:14, 6250.56it/s] 79%|███████▉  | 357477/450337 [00:55<00:15, 6017.25it/s] 80%|███████▉  | 358138/450337 [00:55<00:14, 6183.54it/s] 80%|███████▉  | 358817/450337 [00:55<00:14, 6357.22it/s] 80%|███████▉  | 359458/450337 [00:55<00:14, 6370.37it/s] 80%|███████▉  | 360098/450337 [00:55<00:14, 6325.75it/s] 80%|████████  | 360834/450337 [00:55<00:13, 6625.40it/s] 80%|████████  | 361525/450337 [00:55<00:13, 6701.65it/s] 80%|████████  | 362308/450337 [00:55<00:12, 7036.02it/s] 81%|████████  | 363013/450337 [00:55<00:12, 6988.78it/s] 81%|████████  | 363713/450337 [00:56<00:12, 6920.67it/s] 81%|████████  | 364446/450337 [00:56<00:12, 7037.24it/s] 81%|████████  | 365163/450337 [00:56<00:12, 7070.79it/s] 81%|████████  | 365871/450337 [00:56<00:12, 6994.19it/s] 81%|████████▏ | 366571/450337 [00:56<00:12, 6741.45it/s] 82%|████████▏ | 367248/450337 [00:56<00:12, 6642.24it/s] 82%|████████▏ | 367914/450337 [00:56<00:13, 6205.55it/s] 82%|████████▏ | 368580/450337 [00:56<00:12, 6330.72it/s] 82%|████████▏ | 369269/450337 [00:56<00:12, 6488.45it/s] 82%|████████▏ | 369955/450337 [00:57<00:12, 6589.44it/s] 82%|████████▏ | 370618/450337 [00:57<00:12, 6419.70it/s] 82%|████████▏ | 371263/450337 [00:57<00:12, 6399.48it/s] 83%|████████▎ | 371905/450337 [00:57<00:12, 6259.35it/s] 83%|████████▎ | 372583/450337 [00:57<00:12, 6406.62it/s] 83%|████████▎ | 373259/450337 [00:57<00:11, 6509.69it/s] 83%|████████▎ | 373912/450337 [00:57<00:12, 6262.61it/s] 83%|████████▎ | 374712/450337 [00:57<00:11, 6752.08it/s] 83%|████████▎ | 375391/450337 [00:57<00:11, 6530.04it/s] 84%|████████▎ | 376048/450337 [00:57<00:11, 6265.63it/s] 84%|████████▎ | 376679/450337 [00:58<00:12, 6105.88it/s] 84%|████████▍ | 377320/450337 [00:58<00:11, 6188.83it/s] 84%|████████▍ | 377987/450337 [00:58<00:11, 6323.31it/s] 84%|████████▍ | 378644/450337 [00:58<00:11, 6393.16it/s] 84%|████████▍ | 379312/450337 [00:58<00:10, 6474.91it/s] 84%|████████▍ | 380075/450337 [00:58<00:10, 6814.59it/s] 85%|████████▍ | 380759/450337 [00:58<00:10, 6782.77it/s] 85%|████████▍ | 381467/450337 [00:58<00:10, 6857.19it/s] 85%|████████▍ | 382154/450337 [00:58<00:10, 6377.90it/s] 85%|████████▌ | 382799/450337 [00:59<00:10, 6227.84it/s] 85%|████████▌ | 383439/450337 [00:59<00:10, 6273.88it/s] 85%|████████▌ | 384146/450337 [00:59<00:10, 6501.46it/s] 85%|████████▌ | 384800/450337 [00:59<00:10, 6487.73it/s] 86%|████████▌ | 385452/450337 [00:59<00:10, 6366.27it/s] 86%|████████▌ | 386091/450337 [00:59<00:10, 6354.11it/s] 86%|████████▌ | 386728/450337 [00:59<00:10, 6296.31it/s] 86%|████████▌ | 387547/450337 [00:59<00:09, 6846.09it/s] 86%|████████▌ | 388234/450337 [00:59<00:09, 6780.23it/s] 86%|████████▋ | 388914/450337 [00:59<00:09, 6526.55it/s] 87%|████████▋ | 389628/450337 [01:00<00:09, 6699.55it/s] 87%|████████▋ | 390335/450337 [01:00<00:08, 6794.00it/s] 87%|████████▋ | 391017/450337 [01:00<00:08, 6661.96it/s] 87%|████████▋ | 391685/450337 [01:00<00:09, 6286.04it/s] 87%|████████▋ | 392319/450337 [01:00<00:09, 6271.07it/s] 87%|████████▋ | 392950/450337 [01:00<00:09, 6197.19it/s] 87%|████████▋ | 393572/450337 [01:00<00:09, 6038.71it/s] 88%|████████▊ | 394303/450337 [01:00<00:08, 6400.13it/s] 88%|████████▊ | 394947/450337 [01:00<00:08, 6307.85it/s] 88%|████████▊ | 395621/450337 [01:01<00:08, 6432.70it/s] 88%|████████▊ | 396267/450337 [01:01<00:08, 6352.27it/s] 88%|████████▊ | 396904/450337 [01:01<00:08, 6172.59it/s] 88%|████████▊ | 397523/450337 [01:01<00:08, 5998.29it/s] 88%|████████▊ | 398292/450337 [01:01<00:08, 6480.91it/s] 89%|████████▊ | 398996/450337 [01:01<00:07, 6640.49it/s] 89%|████████▉ | 399737/450337 [01:01<00:07, 6860.33it/s] 89%|████████▉ | 400426/450337 [01:01<00:07, 6743.48it/s] 89%|████████▉ | 401103/450337 [01:01<00:07, 6686.24it/s] 89%|████████▉ | 401774/450337 [01:01<00:07, 6377.86it/s] 89%|████████▉ | 402416/450337 [01:02<00:08, 5969.03it/s] 89%|████████▉ | 403025/450337 [01:02<00:07, 5993.62it/s] 90%|████████▉ | 403723/450337 [01:02<00:07, 6270.03it/s] 90%|████████▉ | 404356/450337 [01:02<00:07, 6154.54it/s] 90%|████████▉ | 405032/450337 [01:02<00:07, 6324.03it/s] 90%|█████████ | 405668/450337 [01:02<00:07, 6152.07it/s] 90%|█████████ | 406330/450337 [01:02<00:07, 6281.99it/s] 90%|█████████ | 406961/450337 [01:02<00:07, 6156.88it/s] 91%|█████████ | 407579/450337 [01:02<00:07, 6075.62it/s] 91%|█████████ | 408207/450337 [01:03<00:06, 6123.95it/s] 91%|█████████ | 408944/450337 [01:03<00:06, 6477.42it/s] 91%|█████████ | 409594/450337 [01:03<00:06, 6418.64it/s] 91%|█████████ | 410238/450337 [01:03<00:06, 6178.36it/s] 91%|█████████▏| 410963/450337 [01:03<00:06, 6482.99it/s] 91%|█████████▏| 411615/450337 [01:03<00:06, 6313.08it/s] 92%|█████████▏| 412249/450337 [01:03<00:06, 6054.88it/s] 92%|█████████▏| 412858/450337 [01:03<00:06, 5828.68it/s] 92%|█████████▏| 413484/450337 [01:03<00:06, 5947.83it/s] 92%|█████████▏| 414082/450337 [01:03<00:06, 5719.53it/s] 92%|█████████▏| 414658/450337 [01:04<00:06, 5728.65it/s] 92%|█████████▏| 415299/450337 [01:04<00:05, 5919.95it/s] 92%|█████████▏| 415911/450337 [01:04<00:05, 5973.64it/s] 92%|█████████▏| 416511/450337 [01:04<00:05, 5961.76it/s] 93%|█████████▎| 417124/450337 [01:04<00:05, 6010.46it/s] 93%|█████████▎| 417850/450337 [01:04<00:05, 6377.94it/s] 93%|█████████▎| 418489/450337 [01:04<00:05, 6130.51it/s] 93%|█████████▎| 419231/450337 [01:04<00:04, 6502.65it/s] 93%|█████████▎| 419885/450337 [01:04<00:04, 6430.92it/s] 93%|█████████▎| 420531/450337 [01:05<00:04, 6384.80it/s] 94%|█████████▎| 421172/450337 [01:05<00:04, 6193.90it/s] 94%|█████████▎| 421794/450337 [01:05<00:04, 6097.63it/s] 94%|█████████▍| 422487/450337 [01:05<00:04, 6337.49it/s] 94%|█████████▍| 423159/450337 [01:05<00:04, 6444.48it/s] 94%|█████████▍| 423806/450337 [01:05<00:04, 6386.62it/s] 94%|█████████▍| 424446/450337 [01:05<00:04, 6320.67it/s] 94%|█████████▍| 425079/450337 [01:05<00:04, 6230.33it/s] 95%|█████████▍| 425723/450337 [01:05<00:03, 6287.39it/s] 95%|█████████▍| 426482/450337 [01:05<00:03, 6664.93it/s] 95%|█████████▍| 427150/450337 [01:06<00:03, 6663.57it/s] 95%|█████████▍| 427818/450337 [01:06<00:03, 6624.08it/s] 95%|█████████▌| 428594/450337 [01:06<00:03, 6957.84it/s] 95%|█████████▌| 429291/450337 [01:06<00:03, 6819.60it/s] 95%|█████████▌| 429975/450337 [01:06<00:03, 6738.29it/s] 96%|█████████▌| 430650/450337 [01:06<00:02, 6607.61it/s] 96%|█████████▌| 431312/450337 [01:06<00:03, 6242.08it/s] 96%|█████████▌| 431941/450337 [01:06<00:03, 6060.78it/s] 96%|█████████▌| 432887/450337 [01:06<00:02, 7013.99it/s] 96%|█████████▋| 433598/450337 [01:07<00:02, 6694.03it/s] 96%|█████████▋| 434276/450337 [01:07<00:02, 6297.90it/s] 97%|█████████▋| 434915/450337 [01:07<00:02, 6239.72it/s] 97%|█████████▋| 435620/450337 [01:07<00:02, 6462.15it/s] 97%|█████████▋| 436273/450337 [01:07<00:02, 6225.68it/s] 97%|█████████▋| 436958/450337 [01:07<00:02, 6397.28it/s] 97%|█████████▋| 437610/450337 [01:07<00:01, 6427.92it/s] 97%|█████████▋| 438257/450337 [01:07<00:01, 6406.82it/s] 97%|█████████▋| 438901/450337 [01:07<00:01, 6299.07it/s] 98%|█████████▊| 439533/450337 [01:07<00:01, 6140.02it/s] 98%|█████████▊| 440149/450337 [01:08<00:01, 6005.76it/s] 98%|█████████▊| 440765/450337 [01:08<00:01, 6045.83it/s] 98%|█████████▊| 441478/450337 [01:08<00:01, 6351.93it/s] 98%|█████████▊| 442119/450337 [01:08<00:01, 6366.04it/s] 98%|█████████▊| 442757/450337 [01:08<00:01, 6287.96it/s] 98%|█████████▊| 443485/450337 [01:08<00:01, 6573.48it/s] 99%|█████████▊| 444183/450337 [01:08<00:00, 6688.26it/s] 99%|█████████▉| 444853/450337 [01:08<00:00, 6028.16it/s] 99%|█████████▉| 445476/450337 [01:08<00:00, 6083.39it/s] 99%|█████████▉| 446095/450337 [01:09<00:00, 6112.86it/s] 99%|█████████▉| 446714/450337 [01:09<00:00, 6051.92it/s] 99%|█████████▉| 447433/450337 [01:09<00:00, 6376.18it/s] 99%|█████████▉| 448076/450337 [01:09<00:00, 6336.28it/s]100%|█████████▉| 448791/450337 [01:09<00:00, 6570.69it/s]100%|█████████▉| 449451/450337 [01:09<00:00, 6345.48it/s]100%|█████████▉| 450105/450337 [01:09<00:00, 6398.79it/s]100%|██████████| 450337/450337 [01:09<00:00, 6463.82it/s]

gathering stats for n=1
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 1910/450337 [00:00<00:23, 19097.56it/s]  1%|          | 3980/450337 [00:00<00:22, 20035.58it/s]  1%|▏         | 6228/450337 [00:00<00:21, 21145.08it/s]  2%|▏         | 8343/450337 [00:00<00:21, 20365.74it/s]  2%|▏         | 10384/450337 [00:00<00:21, 20299.45it/s]  3%|▎         | 12417/450337 [00:00<00:21, 20208.17it/s]  3%|▎         | 14479/450337 [00:00<00:21, 20334.09it/s]  4%|▎         | 16514/450337 [00:00<00:21, 20198.11it/s]  4%|▍         | 18535/450337 [00:00<00:21, 20093.82it/s]  5%|▍         | 20711/450337 [00:01<00:20, 20600.92it/s]  5%|▌         | 22773/450337 [00:01<00:20, 20457.52it/s]  6%|▌         | 25057/450337 [00:01<00:20, 21173.64it/s]  6%|▌         | 27176/450337 [00:01<00:20, 20410.34it/s]  7%|▋         | 29281/450337 [00:01<00:20, 20594.31it/s]  7%|▋         | 31346/450337 [00:01<00:20, 20051.58it/s]  7%|▋         | 33357/450337 [00:01<00:21, 19559.04it/s]  8%|▊         | 35390/450337 [00:01<00:20, 19778.61it/s]  8%|▊         | 37373/450337 [00:01<00:21, 19427.19it/s]  9%|▊         | 39320/450337 [00:01<00:21, 19326.31it/s]  9%|▉         | 41401/450337 [00:02<00:20, 19758.97it/s] 10%|▉         | 43380/450337 [00:02<00:21, 19257.37it/s] 10%|█         | 45492/450337 [00:02<00:20, 19792.69it/s] 11%|█         | 47880/450337 [00:02<00:19, 20990.03it/s] 11%|█         | 50097/450337 [00:02<00:18, 21336.06it/s] 12%|█▏        | 52236/450337 [00:02<00:18, 21212.78it/s] 12%|█▏        | 54361/450337 [00:02<00:18, 20892.35it/s] 13%|█▎        | 56454/450337 [00:02<00:18, 20861.99it/s] 13%|█▎        | 58543/450337 [00:02<00:19, 20542.27it/s] 13%|█▎        | 60600/450337 [00:02<00:19, 20274.16it/s] 14%|█▍        | 62749/450337 [00:03<00:18, 20628.16it/s] 14%|█▍        | 64814/450337 [00:03<00:18, 20399.64it/s] 15%|█▍        | 66909/450337 [00:03<00:18, 20561.09it/s] 15%|█▌        | 68967/450337 [00:03<00:18, 20217.71it/s] 16%|█▌        | 71008/450337 [00:03<00:18, 20269.38it/s] 16%|█▋        | 73228/450337 [00:03<00:18, 20839.09it/s] 17%|█▋        | 75456/450337 [00:03<00:17, 21266.10it/s] 17%|█▋        | 77605/450337 [00:03<00:17, 21328.67it/s] 18%|█▊        | 79740/450337 [00:03<00:18, 20317.99it/s] 18%|█▊        | 81809/450337 [00:04<00:18, 20424.35it/s] 19%|█▊        | 83946/450337 [00:04<00:17, 20698.53it/s] 19%|█▉        | 86023/450337 [00:04<00:17, 20567.97it/s] 20%|█▉        | 88188/450337 [00:04<00:17, 20882.58it/s] 20%|██        | 90280/450337 [00:04<00:18, 19987.16it/s] 21%|██        | 92431/450337 [00:04<00:17, 20423.29it/s] 21%|██        | 94483/450337 [00:04<00:17, 20182.75it/s] 21%|██▏       | 96758/450337 [00:04<00:16, 20929.21it/s] 22%|██▏       | 98858/450337 [00:04<00:17, 20499.63it/s] 22%|██▏       | 100914/450337 [00:04<00:17, 20172.74it/s] 23%|██▎       | 102966/450337 [00:05<00:17, 20271.80it/s] 23%|██▎       | 104997/450337 [00:05<00:17, 20039.99it/s] 24%|██▍       | 107004/450337 [00:05<00:17, 19935.68it/s] 24%|██▍       | 109000/450337 [00:05<00:17, 19670.61it/s] 25%|██▍       | 111076/450337 [00:05<00:16, 19988.82it/s] 25%|██▌       | 113077/450337 [00:05<00:17, 19624.00it/s] 26%|██▌       | 115059/450337 [00:05<00:17, 19680.69it/s] 26%|██▌       | 117029/450337 [00:05<00:17, 19262.61it/s] 26%|██▋       | 119004/450337 [00:05<00:17, 19402.28it/s] 27%|██▋       | 121076/450337 [00:05<00:16, 19789.28it/s] 27%|██▋       | 123058/450337 [00:06<00:16, 19574.28it/s] 28%|██▊       | 125044/450337 [00:06<00:16, 19656.53it/s] 28%|██▊       | 127152/450337 [00:06<00:16, 20074.94it/s] 29%|██▊       | 129162/450337 [00:06<00:16, 20062.47it/s] 29%|██▉       | 131170/450337 [00:06<00:16, 19940.96it/s] 30%|██▉       | 133165/450337 [00:06<00:16, 19775.38it/s] 30%|███       | 135382/450337 [00:06<00:15, 20484.57it/s] 31%|███       | 137588/450337 [00:06<00:14, 20952.65it/s] 31%|███       | 139685/450337 [00:06<00:15, 20640.23it/s] 31%|███▏      | 141751/450337 [00:06<00:15, 20273.46it/s] 32%|███▏      | 143838/450337 [00:07<00:14, 20447.09it/s] 32%|███▏      | 146000/450337 [00:07<00:14, 20791.41it/s] 33%|███▎      | 148082/450337 [00:07<00:15, 20117.09it/s] 33%|███▎      | 150100/450337 [00:07<00:14, 20054.35it/s] 34%|███▍      | 152110/450337 [00:07<00:15, 19574.16it/s] 34%|███▍      | 154151/450337 [00:07<00:14, 19812.69it/s] 35%|███▍      | 156270/450337 [00:07<00:14, 20215.07it/s] 35%|███▌      | 158443/450337 [00:07<00:14, 20656.20it/s] 36%|███▌      | 160512/450337 [00:07<00:14, 20392.28it/s] 36%|███▌      | 162581/450337 [00:08<00:14, 20479.75it/s] 37%|███▋      | 164632/450337 [00:08<00:14, 20280.67it/s] 37%|███▋      | 166871/450337 [00:08<00:13, 20899.17it/s] 38%|███▊      | 168964/450337 [00:08<00:13, 20375.62it/s] 38%|███▊      | 171006/450337 [00:08<00:14, 19848.73it/s] 38%|███▊      | 172997/450337 [00:08<00:13, 19863.57it/s] 39%|███▉      | 175052/450337 [00:08<00:13, 20060.66it/s] 39%|███▉      | 177061/450337 [00:08<00:14, 19462.46it/s] 40%|███▉      | 179013/450337 [00:08<00:14, 19296.60it/s] 40%|████      | 180946/450337 [00:08<00:13, 19279.67it/s] 41%|████      | 182891/450337 [00:09<00:13, 19322.64it/s] 41%|████      | 184825/450337 [00:09<00:13, 19134.11it/s] 41%|████▏     | 186740/450337 [00:09<00:13, 18961.83it/s] 42%|████▏     | 188840/450337 [00:09<00:13, 19556.20it/s] 42%|████▏     | 190849/450337 [00:09<00:13, 19708.70it/s] 43%|████▎     | 192889/450337 [00:09<00:12, 19912.46it/s] 43%|████▎     | 194922/450337 [00:09<00:12, 20027.90it/s] 44%|████▎     | 197006/450337 [00:09<00:12, 20268.30it/s] 44%|████▍     | 199143/450337 [00:09<00:12, 20583.06it/s] 45%|████▍     | 201202/450337 [00:09<00:12, 20191.31it/s] 45%|████▌     | 203224/450337 [00:10<00:12, 20097.93it/s] 46%|████▌     | 205236/450337 [00:10<00:12, 20048.76it/s] 46%|████▌     | 207242/450337 [00:10<00:12, 19855.16it/s] 46%|████▋     | 209229/450337 [00:10<00:12, 19554.42it/s] 47%|████▋     | 211314/450337 [00:10<00:11, 19934.15it/s] 47%|████▋     | 213365/450337 [00:10<00:11, 20098.24it/s] 48%|████▊     | 215377/450337 [00:10<00:11, 19642.00it/s] 48%|████▊     | 217345/450337 [00:10<00:11, 19566.11it/s] 49%|████▊     | 219448/450337 [00:10<00:11, 19992.96it/s] 49%|████▉     | 221450/450337 [00:10<00:11, 19935.96it/s] 50%|████▉     | 223446/450337 [00:11<00:11, 19265.96it/s] 50%|█████     | 225643/450337 [00:11<00:11, 20040.05it/s] 51%|█████     | 227678/450337 [00:11<00:11, 20127.01it/s] 51%|█████     | 229696/450337 [00:11<00:11, 19604.20it/s] 51%|█████▏    | 231676/450337 [00:11<00:11, 19658.22it/s] 52%|█████▏    | 233834/450337 [00:11<00:10, 20217.45it/s] 52%|█████▏    | 236009/450337 [00:11<00:10, 20667.74it/s] 53%|█████▎    | 238080/450337 [00:11<00:10, 19962.83it/s] 53%|█████▎    | 240167/450337 [00:11<00:10, 20224.50it/s] 54%|█████▍    | 242442/450337 [00:12<00:09, 20958.88it/s] 54%|█████▍    | 244544/450337 [00:12<00:09, 20683.13it/s] 55%|█████▍    | 246747/450337 [00:12<00:09, 21078.31it/s] 55%|█████▌    | 248859/450337 [00:12<00:09, 20984.02it/s] 56%|█████▌    | 251002/450337 [00:12<00:09, 21109.20it/s] 56%|█████▌    | 253116/450337 [00:12<00:09, 20833.57it/s] 57%|█████▋    | 255202/450337 [00:12<00:09, 20745.13it/s] 57%|█████▋    | 257337/450337 [00:12<00:09, 20918.70it/s] 58%|█████▊    | 259431/450337 [00:12<00:09, 20261.20it/s] 58%|█████▊    | 261633/450337 [00:12<00:09, 20767.67it/s] 59%|█████▊    | 263715/450337 [00:13<00:09, 20207.76it/s] 59%|█████▉    | 265742/450337 [00:13<00:09, 19881.57it/s] 59%|█████▉    | 267735/450337 [00:13<00:09, 19672.97it/s] 60%|█████▉    | 269771/450337 [00:13<00:09, 19870.02it/s] 60%|██████    | 271761/450337 [00:13<00:08, 19862.69it/s] 61%|██████    | 273995/450337 [00:13<00:08, 20592.11it/s] 61%|██████▏   | 276057/450337 [00:13<00:08, 20164.18it/s] 62%|██████▏   | 278077/450337 [00:13<00:08, 19922.63it/s] 62%|██████▏   | 280118/450337 [00:13<00:08, 20063.66it/s] 63%|██████▎   | 282127/450337 [00:13<00:08, 19929.50it/s] 63%|██████▎   | 284156/450337 [00:14<00:08, 20030.74it/s] 64%|██████▎   | 286161/450337 [00:14<00:08, 19955.74it/s] 64%|██████▍   | 288195/450337 [00:14<00:08, 20067.29it/s] 64%|██████▍   | 290203/450337 [00:14<00:08, 19736.87it/s] 65%|██████▍   | 292234/450337 [00:14<00:07, 19905.36it/s] 65%|██████▌   | 294239/450337 [00:14<00:07, 19944.42it/s] 66%|██████▌   | 296327/450337 [00:14<00:07, 20220.79it/s] 66%|██████▋   | 298392/450337 [00:14<00:07, 20345.56it/s] 67%|██████▋   | 300461/450337 [00:14<00:07, 20444.64it/s] 67%|██████▋   | 302507/450337 [00:14<00:07, 20096.04it/s] 68%|██████▊   | 304561/450337 [00:15<00:07, 20224.84it/s] 68%|██████▊   | 306585/450337 [00:15<00:07, 20052.07it/s] 69%|██████▊   | 308592/450337 [00:15<00:07, 19691.86it/s] 69%|██████▉   | 310833/450337 [00:15<00:06, 20485.01it/s] 69%|██████▉   | 312920/450337 [00:15<00:06, 20595.88it/s] 70%|██████▉   | 314982/450337 [00:15<00:06, 20371.66it/s] 70%|███████   | 317022/450337 [00:15<00:06, 20150.89it/s] 71%|███████   | 319039/450337 [00:15<00:06, 19897.63it/s] 71%|███████▏  | 321031/450337 [00:15<00:06, 19867.04it/s] 72%|███████▏  | 323019/450337 [00:16<00:06, 19611.90it/s] 72%|███████▏  | 325070/450337 [00:16<00:06, 19868.53it/s] 73%|███████▎  | 327064/450337 [00:16<00:06, 19889.11it/s] 73%|███████▎  | 329249/450337 [00:16<00:05, 20467.84it/s] 74%|███████▎  | 331298/450337 [00:16<00:05, 19866.54it/s] 74%|███████▍  | 333290/450337 [00:16<00:05, 19873.04it/s] 74%|███████▍  | 335340/450337 [00:16<00:05, 20055.55it/s] 75%|███████▍  | 337421/450337 [00:16<00:05, 20278.08it/s] 75%|███████▌  | 339509/450337 [00:16<00:05, 20449.37it/s] 76%|███████▌  | 341556/450337 [00:16<00:05, 19785.76it/s] 76%|███████▋  | 343610/450337 [00:17<00:05, 20003.49it/s] 77%|███████▋  | 345615/450337 [00:17<00:05, 19963.09it/s] 77%|███████▋  | 347615/450337 [00:17<00:05, 19803.68it/s] 78%|███████▊  | 349599/450337 [00:17<00:05, 19809.05it/s] 78%|███████▊  | 351753/450337 [00:17<00:04, 20319.91it/s] 79%|███████▊  | 353787/450337 [00:17<00:04, 19566.46it/s] 79%|███████▉  | 355763/450337 [00:17<00:04, 19622.27it/s] 79%|███████▉  | 357731/450337 [00:17<00:04, 19512.19it/s] 80%|███████▉  | 359729/450337 [00:17<00:04, 19644.61it/s] 80%|████████  | 361939/450337 [00:17<00:04, 20366.47it/s] 81%|████████  | 364026/450337 [00:18<00:04, 20511.99it/s] 81%|████████▏ | 366209/450337 [00:18<00:04, 20895.16it/s] 82%|████████▏ | 368301/450337 [00:18<00:04, 20057.17it/s] 82%|████████▏ | 370376/450337 [00:18<00:03, 20254.05it/s] 83%|████████▎ | 372408/450337 [00:18<00:03, 19911.20it/s] 83%|████████▎ | 374520/450337 [00:18<00:03, 20262.96it/s] 84%|████████▎ | 376551/450337 [00:18<00:03, 19818.14it/s] 84%|████████▍ | 378538/450337 [00:18<00:03, 19688.91it/s] 85%|████████▍ | 380678/450337 [00:18<00:03, 20184.11it/s] 85%|████████▍ | 382700/450337 [00:19<00:03, 19809.25it/s] 85%|████████▌ | 384778/450337 [00:19<00:03, 20088.91it/s] 86%|████████▌ | 386791/450337 [00:19<00:03, 19894.29it/s] 86%|████████▋ | 388894/450337 [00:19<00:03, 20220.28it/s] 87%|████████▋ | 390984/450337 [00:19<00:02, 20417.16it/s] 87%|████████▋ | 393028/450337 [00:19<00:02, 19748.31it/s] 88%|████████▊ | 395009/450337 [00:19<00:02, 19685.21it/s] 88%|████████▊ | 396982/450337 [00:19<00:02, 19628.95it/s] 89%|████████▊ | 399060/450337 [00:19<00:02, 19962.85it/s] 89%|████████▉ | 401141/450337 [00:19<00:02, 20208.42it/s] 90%|████████▉ | 403164/450337 [00:20<00:02, 19475.58it/s] 90%|████████▉ | 405119/450337 [00:20<00:02, 19407.23it/s] 90%|█████████ | 407065/450337 [00:20<00:02, 19416.74it/s] 91%|█████████ | 409110/450337 [00:20<00:02, 19714.23it/s] 91%|█████████▏| 411085/450337 [00:20<00:02, 19468.84it/s] 92%|█████████▏| 413035/450337 [00:20<00:01, 18825.66it/s] 92%|█████████▏| 414923/450337 [00:20<00:01, 18629.51it/s] 93%|█████████▎| 416868/450337 [00:20<00:01, 18865.40it/s] 93%|█████████▎| 418888/450337 [00:20<00:01, 19253.04it/s] 93%|█████████▎| 420879/450337 [00:20<00:01, 19444.41it/s] 94%|█████████▍| 422827/450337 [00:21<00:01, 19334.77it/s] 94%|█████████▍| 424763/450337 [00:21<00:01, 19298.83it/s] 95%|█████████▍| 426892/450337 [00:21<00:01, 19881.80it/s] 95%|█████████▌| 428898/450337 [00:21<00:01, 19931.69it/s] 96%|█████████▌| 430921/450337 [00:21<00:00, 20016.32it/s] 96%|█████████▌| 433077/450337 [00:21<00:00, 20469.76it/s] 97%|█████████▋| 435125/450337 [00:21<00:00, 19814.41it/s] 97%|█████████▋| 437112/450337 [00:21<00:00, 19803.33it/s] 98%|█████████▊| 439096/450337 [00:21<00:00, 19693.04it/s] 98%|█████████▊| 441068/450337 [00:21<00:00, 19413.62it/s] 98%|█████████▊| 443128/450337 [00:22<00:00, 19760.67it/s] 99%|█████████▉| 445107/450337 [00:22<00:00, 19427.38it/s] 99%|█████████▉| 447053/450337 [00:22<00:00, 19308.77it/s]100%|█████████▉| 449150/450337 [00:22<00:00, 19785.16it/s]100%|██████████| 450337/450337 [00:22<00:00, 20063.96it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 19.96it/s]2022-03-03 10:51:03 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-03 10:51:03 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-03 10:51:03 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-03 10:51:03 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-03 10:51:03 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-03 10:51:03 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-03 10:51:03 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-03 10:51:03 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-03 10:51:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 10:51:03 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-03 10:51:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 10:51:03 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-03 10:51:03 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-03 10:51:03 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-03 10:51:03 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-03 10:51:03 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-03 10:51:03 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-03 10:51:03 | INFO | fairseq.trainer | begin training epoch 1
2022-03-03 10:51:03 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-03 10:51:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-03 10:51:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 10:51:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 10:51:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 10:53:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 10:59:03 | INFO | train_inner | epoch 001:    105 / 393 loss=16.959, ppl=127391, wps=14554.7, ups=0.22, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.61, loss_scale=4, train_wall=475, gb_free=10.1, wall=480
2022-03-03 11:06:29 | INFO | train_inner | epoch 001:    205 / 393 loss=14.48, ppl=22848.1, wps=14698.7, ups=0.22, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.557, loss_scale=4, train_wall=441, gb_free=10.1, wall=926
2022-03-03 11:13:55 | INFO | train_inner | epoch 001:    305 / 393 loss=12.454, ppl=5610.21, wps=14702.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.002, loss_scale=4, train_wall=441, gb_free=10.1, wall=1372
2022-03-03 11:20:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 11:20:31 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.667 | ppl 1625.98 | wps 34147.8 | wpb 2034.1 | bsz 4 | num_updates 388
2022-03-03 11:20:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-03 11:20:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 11:20:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 11:20:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 1 @ 388 updates, score 10.667) (writing took 4.559888226911426 seconds)
2022-03-03 11:20:36 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-03 11:20:36 | INFO | train | epoch 001 | loss 13.825 | ppl 14513.8 | wps 14573.1 | ups 0.22 | wpb 65460.6 | bsz 127.9 | num_updates 388 | lr 4.85903e-05 | gnorm 1.721 | loss_scale 4 | train_wall 1742 | gb_free 10.1 | wall 1773
2022-03-03 11:20:36 | INFO | fairseq.trainer | begin training epoch 2
2022-03-03 11:20:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 11:21:29 | INFO | train_inner | epoch 002:     12 / 393 loss=11.016, ppl=2071.22, wps=14355.9, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.573, loss_scale=4, train_wall=439, gb_free=10.1, wall=1826
2022-03-03 11:28:55 | INFO | train_inner | epoch 002:    112 / 393 loss=10.458, ppl=1406.57, wps=14712.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.461, loss_scale=4, train_wall=441, gb_free=10.1, wall=2272
2022-03-03 11:36:20 | INFO | train_inner | epoch 002:    212 / 393 loss=10.185, ppl=1163.88, wps=14711.2, ups=0.22, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.511, loss_scale=8, train_wall=441, gb_free=10.1, wall=2717
2022-03-03 11:43:46 | INFO | train_inner | epoch 002:    312 / 393 loss=9.954, ppl=992.01, wps=14709.2, ups=0.22, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.562, loss_scale=8, train_wall=441, gb_free=10.1, wall=3163
2022-03-03 11:49:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 11:49:51 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.651 | ppl 804.18 | wps 34129.4 | wpb 2034.1 | bsz 4 | num_updates 781 | best_loss 9.651
2022-03-03 11:49:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 781 updates
2022-03-03 11:49:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 11:49:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 11:49:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 2 @ 781 updates, score 9.651) (writing took 4.666310087777674 seconds)
2022-03-03 11:49:56 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-03 11:49:56 | INFO | train | epoch 002 | loss 10.125 | ppl 1116.67 | wps 14617.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 0.529 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 3533
2022-03-03 11:49:56 | INFO | fairseq.trainer | begin training epoch 3
2022-03-03 11:49:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 11:51:21 | INFO | train_inner | epoch 003:     19 / 393 loss=9.746, ppl=858.68, wps=14348.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.626, loss_scale=8, train_wall=439, gb_free=10.1, wall=3617
2022-03-03 11:58:46 | INFO | train_inner | epoch 003:    119 / 393 loss=9.542, ppl=745.6, wps=14711.1, ups=0.22, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.693, loss_scale=8, train_wall=441, gb_free=10.1, wall=4063
2022-03-03 12:06:12 | INFO | train_inner | epoch 003:    219 / 393 loss=9.376, ppl=664.62, wps=14707.4, ups=0.22, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.803, loss_scale=8, train_wall=441, gb_free=10.1, wall=4508
2022-03-03 12:13:37 | INFO | train_inner | epoch 003:    319 / 393 loss=9.232, ppl=601.26, wps=14706.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.775, loss_scale=16, train_wall=441, gb_free=10.1, wall=4954
2022-03-03 12:19:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:19:12 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.014 | ppl 517.03 | wps 34072.8 | wpb 2034.1 | bsz 4 | num_updates 1174 | best_loss 9.014
2022-03-03 12:19:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1174 updates
2022-03-03 12:19:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 12:19:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 12:19:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 3 @ 1174 updates, score 9.014) (writing took 4.563507203944027 seconds)
2022-03-03 12:19:16 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-03 12:19:16 | INFO | train | epoch 003 | loss 9.344 | ppl 649.83 | wps 14615.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1174 | lr 0.000146821 | gnorm 0.771 | loss_scale 16 | train_wall 1730 | gb_free 10.1 | wall 5293
2022-03-03 12:19:16 | INFO | fairseq.trainer | begin training epoch 4
2022-03-03 12:19:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:21:12 | INFO | train_inner | epoch 004:     26 / 393 loss=9.074, ppl=538.9, wps=14344.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.825, loss_scale=16, train_wall=439, gb_free=10.1, wall=5409
2022-03-03 12:28:38 | INFO | train_inner | epoch 004:    126 / 393 loss=8.93, ppl=487.66, wps=14697.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.793, loss_scale=16, train_wall=441, gb_free=10.1, wall=5855
2022-03-03 12:36:04 | INFO | train_inner | epoch 004:    226 / 393 loss=8.814, ppl=450.08, wps=14696.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.787, loss_scale=16, train_wall=441, gb_free=10.1, wall=6301
2022-03-03 12:43:30 | INFO | train_inner | epoch 004:    326 / 393 loss=8.715, ppl=420.34, wps=14696.8, ups=0.22, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.824, loss_scale=16, train_wall=441, gb_free=10.1, wall=6747
2022-03-03 12:48:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:48:33 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.544 | ppl 373.28 | wps 34145.9 | wpb 2034.1 | bsz 4 | num_updates 1567 | best_loss 8.544
2022-03-03 12:48:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1567 updates
2022-03-03 12:48:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 12:48:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 12:48:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 4 @ 1567 updates, score 8.544) (writing took 4.623363072052598 seconds)
2022-03-03 12:48:38 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-03 12:48:38 | INFO | train | epoch 004 | loss 8.8 | ppl 445.68 | wps 14604.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1567 | lr 0.000195936 | gnorm 0.807 | loss_scale 32 | train_wall 1731 | gb_free 10.1 | wall 7054
2022-03-03 12:48:38 | INFO | fairseq.trainer | begin training epoch 5
2022-03-03 12:48:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:51:05 | INFO | train_inner | epoch 005:     33 / 393 loss=8.596, ppl=386.86, wps=14339.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.819, loss_scale=32, train_wall=439, gb_free=10.1, wall=7202
2022-03-03 12:58:31 | INFO | train_inner | epoch 005:    133 / 393 loss=8.467, ppl=353.91, wps=14697.3, ups=0.22, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.828, loss_scale=32, train_wall=441, gb_free=10.1, wall=7647
2022-03-03 13:05:57 | INFO | train_inner | epoch 005:    233 / 393 loss=8.381, ppl=333.46, wps=14702.5, ups=0.22, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.784, loss_scale=32, train_wall=441, gb_free=10.1, wall=8093
2022-03-03 13:13:22 | INFO | train_inner | epoch 005:    333 / 393 loss=8.291, ppl=313.23, wps=14700.2, ups=0.22, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.794, loss_scale=32, train_wall=441, gb_free=10.1, wall=8539
2022-03-03 13:17:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 13:17:54 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.182 | ppl 290.43 | wps 34098.2 | wpb 2034.1 | bsz 4 | num_updates 1960 | best_loss 8.182
2022-03-03 13:17:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1960 updates
2022-03-03 13:17:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 13:17:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 13:17:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 5 @ 1960 updates, score 8.182) (writing took 4.755588668398559 seconds)
2022-03-03 13:17:59 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-03 13:17:59 | INFO | train | epoch 005 | loss 8.368 | ppl 330.48 | wps 14605.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1960 | lr 0.000245051 | gnorm 0.798 | loss_scale 32 | train_wall 1731 | gb_free 10.1 | wall 8816
2022-03-03 13:17:59 | INFO | fairseq.trainer | begin training epoch 6
2022-03-03 13:17:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 13:20:57 | INFO | train_inner | epoch 006:     40 / 393 loss=8.182, ppl=290.45, wps=14333.5, ups=0.22, wpb=65238.4, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.766, loss_scale=32, train_wall=439, gb_free=10.1, wall=8994
2022-03-03 13:26:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 13:28:28 | INFO | train_inner | epoch 006:    141 / 393 loss=8.076, ppl=269.77, wps=14552.5, ups=0.22, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.759, loss_scale=32, train_wall=445, gb_free=10.1, wall=9445
2022-03-03 13:35:54 | INFO | train_inner | epoch 006:    241 / 393 loss=8.012, ppl=258.19, wps=14700.8, ups=0.22, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.747, loss_scale=32, train_wall=441, gb_free=10.1, wall=9890
2022-03-03 13:43:20 | INFO | train_inner | epoch 006:    341 / 393 loss=7.944, ppl=246.32, wps=14697.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.755, loss_scale=32, train_wall=441, gb_free=10.1, wall=10336
2022-03-03 13:47:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 13:47:16 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.909 | ppl 240.36 | wps 34051.3 | wpb 2034.1 | bsz 4 | num_updates 2352 | best_loss 7.909
2022-03-03 13:47:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2352 updates
2022-03-03 13:47:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 13:47:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 13:47:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 6 @ 2352 updates, score 7.909) (writing took 4.71732263546437 seconds)
2022-03-03 13:47:21 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-03 13:47:21 | INFO | train | epoch 006 | loss 8.006 | ppl 257.08 | wps 14567.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 2352 | lr 0.000294041 | gnorm 0.754 | loss_scale 32 | train_wall 1731 | gb_free 10.1 | wall 10577
2022-03-03 13:47:21 | INFO | fairseq.trainer | begin training epoch 7
2022-03-03 13:47:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 13:50:55 | INFO | train_inner | epoch 007:     48 / 393 loss=7.837, ppl=228.7, wps=14334.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.734, loss_scale=32, train_wall=439, gb_free=10.1, wall=10791
2022-03-03 13:58:21 | INFO | train_inner | epoch 007:    148 / 393 loss=7.747, ppl=214.87, wps=14694.6, ups=0.22, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.736, loss_scale=32, train_wall=441, gb_free=10.1, wall=11237
2022-03-03 14:04:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 14:05:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 14:05:55 | INFO | train_inner | epoch 007:    250 / 393 loss=7.698, ppl=207.63, wps=14412.9, ups=0.22, wpb=65530.2, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.732, loss_scale=16, train_wall=450, gb_free=10.1, wall=11692
2022-03-03 14:13:21 | INFO | train_inner | epoch 007:    350 / 393 loss=7.661, ppl=202.32, wps=14703.1, ups=0.22, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.717, loss_scale=16, train_wall=441, gb_free=10.1, wall=12138
2022-03-03 14:16:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:16:37 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.692 | ppl 206.82 | wps 34061.6 | wpb 2034.1 | bsz 4 | num_updates 2743 | best_loss 7.692
2022-03-03 14:16:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2743 updates
2022-03-03 14:16:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 14:16:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 14:16:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 7 @ 2743 updates, score 7.692) (writing took 4.579729158431292 seconds)
2022-03-03 14:16:42 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-03 14:16:42 | INFO | train | epoch 007 | loss 7.703 | ppl 208.39 | wps 14532.2 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 2743 | lr 0.000342906 | gnorm 0.721 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 12339
2022-03-03 14:16:42 | INFO | fairseq.trainer | begin training epoch 8
2022-03-03 14:16:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:20:56 | INFO | train_inner | epoch 008:     57 / 393 loss=7.549, ppl=187.27, wps=14340.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.695, loss_scale=16, train_wall=439, gb_free=10.1, wall=12593
2022-03-03 14:28:22 | INFO | train_inner | epoch 008:    157 / 393 loss=7.478, ppl=178.24, wps=14706.1, ups=0.22, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.708, loss_scale=16, train_wall=441, gb_free=10.1, wall=13038
2022-03-03 14:35:47 | INFO | train_inner | epoch 008:    257 / 393 loss=7.453, ppl=175.23, wps=14705.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.702, loss_scale=16, train_wall=441, gb_free=10.1, wall=13484
2022-03-03 14:43:13 | INFO | train_inner | epoch 008:    357 / 393 loss=7.42, ppl=171.26, wps=14706.1, ups=0.22, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.692, loss_scale=16, train_wall=441, gb_free=10.1, wall=13930
2022-03-03 14:45:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 14:45:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:45:58 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.52 | ppl 183.56 | wps 34219.6 | wpb 2034.1 | bsz 4 | num_updates 3135 | best_loss 7.52
2022-03-03 14:45:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3135 updates
2022-03-03 14:45:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 14:46:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 14:46:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 8 @ 3135 updates, score 7.52) (writing took 4.745448163710535 seconds)
2022-03-03 14:46:03 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-03 14:46:03 | INFO | train | epoch 008 | loss 7.45 | ppl 174.87 | wps 14573.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3135 | lr 0.000391897 | gnorm 0.702 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 14099
2022-03-03 14:46:03 | INFO | fairseq.trainer | begin training epoch 9
2022-03-03 14:46:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:50:52 | INFO | train_inner | epoch 009:     65 / 393 loss=7.3, ppl=157.56, wps=14200.6, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.69, loss_scale=16, train_wall=443, gb_free=10.1, wall=14389
2022-03-03 14:58:18 | INFO | train_inner | epoch 009:    165 / 393 loss=7.257, ppl=152.97, wps=14702.6, ups=0.22, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.675, loss_scale=16, train_wall=441, gb_free=10.1, wall=14835
2022-03-03 15:05:44 | INFO | train_inner | epoch 009:    265 / 393 loss=7.241, ppl=151.3, wps=14702.6, ups=0.22, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.675, loss_scale=16, train_wall=441, gb_free=10.1, wall=15281
2022-03-03 15:13:10 | INFO | train_inner | epoch 009:    365 / 393 loss=7.217, ppl=148.74, wps=14704.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.663, loss_scale=16, train_wall=441, gb_free=10.1, wall=15726
2022-03-03 15:15:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 15:15:19 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.384 | ppl 166.99 | wps 34148.2 | wpb 2034.1 | bsz 4 | num_updates 3528 | best_loss 7.384
2022-03-03 15:15:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3528 updates
2022-03-03 15:15:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 15:15:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 15:15:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 9 @ 3528 updates, score 7.384) (writing took 4.684952259995043 seconds)
2022-03-03 15:15:24 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-03 15:15:24 | INFO | train | epoch 009 | loss 7.239 | ppl 151.06 | wps 14610.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 3528 | lr 0.000441012 | gnorm 0.673 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 15860
2022-03-03 15:15:24 | INFO | fairseq.trainer | begin training epoch 10
2022-03-03 15:15:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 15:20:45 | INFO | train_inner | epoch 010:     72 / 393 loss=7.1, ppl=137.14, wps=14340.1, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.682, loss_scale=16, train_wall=439, gb_free=10.1, wall=16181
2022-03-03 15:24:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 15:28:15 | INFO | train_inner | epoch 010:    173 / 393 loss=7.061, ppl=133.57, wps=14548.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.653, loss_scale=16, train_wall=445, gb_free=10.1, wall=16632
2022-03-03 15:33:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 15:35:45 | INFO | train_inner | epoch 010:    274 / 393 loss=7.058, ppl=133.25, wps=14559.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.649, loss_scale=8, train_wall=445, gb_free=10.1, wall=17082
2022-03-03 15:43:11 | INFO | train_inner | epoch 010:    374 / 393 loss=7.062, ppl=133.64, wps=14702.7, ups=0.22, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.647, loss_scale=8, train_wall=441, gb_free=10.1, wall=17528
2022-03-03 15:44:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 15:44:40 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.279 | ppl 155.33 | wps 33973.2 | wpb 2034.1 | bsz 4 | num_updates 3919 | best_loss 7.279
2022-03-03 15:44:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3919 updates
2022-03-03 15:44:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 15:44:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 15:44:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 10 @ 3919 updates, score 7.279) (writing took 4.509405460208654 seconds)
2022-03-03 15:44:45 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-03 15:44:45 | INFO | train | epoch 010 | loss 7.06 | ppl 133.47 | wps 14534.3 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 3919 | lr 0.000489877 | gnorm 0.652 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 17621
2022-03-03 15:44:45 | INFO | fairseq.trainer | begin training epoch 11
2022-03-03 15:44:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 15:50:46 | INFO | train_inner | epoch 011:     81 / 393 loss=6.929, ppl=121.84, wps=14352.1, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.646, loss_scale=8, train_wall=439, gb_free=10.1, wall=17982
2022-03-03 15:58:11 | INFO | train_inner | epoch 011:    181 / 393 loss=6.902, ppl=119.58, wps=14702.3, ups=0.22, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.635, loss_scale=8, train_wall=441, gb_free=10.1, wall=18428
2022-03-03 16:05:37 | INFO | train_inner | epoch 011:    281 / 393 loss=6.908, ppl=120.05, wps=14708.9, ups=0.22, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.628, loss_scale=8, train_wall=441, gb_free=10.1, wall=18874
2022-03-03 16:13:02 | INFO | train_inner | epoch 011:    381 / 393 loss=6.899, ppl=119.38, wps=14707.7, ups=0.22, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.599, loss_scale=16, train_wall=441, gb_free=10.1, wall=19319
2022-03-03 16:13:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:14:00 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.201 | ppl 147.12 | wps 34177.4 | wpb 2034.1 | bsz 4 | num_updates 4312 | best_loss 7.201
2022-03-03 16:14:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4312 updates
2022-03-03 16:14:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 16:14:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 16:14:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 11 @ 4312 updates, score 7.201) (writing took 4.537517128512263 seconds)
2022-03-03 16:14:05 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-03 16:14:05 | INFO | train | epoch 011 | loss 6.901 | ppl 119.55 | wps 14614.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 4312 | lr 0.000481571 | gnorm 0.628 | loss_scale 16 | train_wall 1730 | gb_free 10.1 | wall 19382
2022-03-03 16:14:05 | INFO | fairseq.trainer | begin training epoch 12
2022-03-03 16:14:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:20:37 | INFO | train_inner | epoch 012:     88 / 393 loss=6.752, ppl=107.75, wps=14347.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.594, loss_scale=16, train_wall=439, gb_free=10.1, wall=19774
2022-03-03 16:28:03 | INFO | train_inner | epoch 012:    188 / 393 loss=6.742, ppl=107.07, wps=14700.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.592, loss_scale=16, train_wall=441, gb_free=10.1, wall=20220
2022-03-03 16:35:29 | INFO | train_inner | epoch 012:    288 / 393 loss=6.764, ppl=108.71, wps=14705.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.601, loss_scale=16, train_wall=441, gb_free=10.1, wall=20665
2022-03-03 16:42:54 | INFO | train_inner | epoch 012:    388 / 393 loss=6.765, ppl=108.75, wps=14702.4, ups=0.22, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.604, loss_scale=16, train_wall=441, gb_free=10.1, wall=21111
2022-03-03 16:43:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:43:21 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.136 | ppl 140.65 | wps 34082.6 | wpb 2034.1 | bsz 4 | num_updates 4705 | best_loss 7.136
2022-03-03 16:43:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4705 updates
2022-03-03 16:43:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 16:43:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 16:43:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 12 @ 4705 updates, score 7.136) (writing took 4.5955550810322165 seconds)
2022-03-03 16:43:26 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-03 16:43:26 | INFO | train | epoch 012 | loss 6.751 | ppl 107.74 | wps 14610.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 4705 | lr 0.00046102 | gnorm 0.599 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 21142
2022-03-03 16:43:26 | INFO | fairseq.trainer | begin training epoch 13
2022-03-03 16:43:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:50:29 | INFO | train_inner | epoch 013:     95 / 393 loss=6.594, ppl=96.61, wps=14347.1, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.563, loss_scale=32, train_wall=439, gb_free=10.1, wall=21566
2022-03-03 16:51:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 16:57:59 | INFO | train_inner | epoch 013:    196 / 393 loss=6.623, ppl=98.54, wps=14561.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.592, loss_scale=16, train_wall=445, gb_free=10.1, wall=22016
2022-03-03 17:05:25 | INFO | train_inner | epoch 013:    296 / 393 loss=6.631, ppl=99.12, wps=14703, ups=0.22, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.576, loss_scale=16, train_wall=441, gb_free=10.1, wall=22462
2022-03-03 17:12:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:12:42 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.104 | ppl 137.61 | wps 33960.9 | wpb 2034.1 | bsz 4 | num_updates 5097 | best_loss 7.104
2022-03-03 17:12:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5097 updates
2022-03-03 17:12:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 17:12:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 17:12:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 13 @ 5097 updates, score 7.104) (writing took 4.733628121204674 seconds)
2022-03-03 17:12:47 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-03 17:12:47 | INFO | train | epoch 013 | loss 6.625 | ppl 98.72 | wps 14573.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5097 | lr 0.000442938 | gnorm 0.576 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 22903
2022-03-03 17:12:47 | INFO | fairseq.trainer | begin training epoch 14
2022-03-03 17:12:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 17:13:00 | INFO | train_inner | epoch 014:      3 / 393 loss=6.655, ppl=100.78, wps=14339.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.579, loss_scale=16, train_wall=439, gb_free=10.1, wall=22917
2022-03-03 17:17:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 17:20:30 | INFO | train_inner | epoch 014:    104 / 393 loss=6.486, ppl=89.63, wps=14561, ups=0.22, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.548, loss_scale=8, train_wall=445, gb_free=10.1, wall=23367
2022-03-03 17:27:56 | INFO | train_inner | epoch 014:    204 / 393 loss=6.509, ppl=91.1, wps=14710.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.578, loss_scale=8, train_wall=441, gb_free=10.1, wall=23812
2022-03-03 17:35:21 | INFO | train_inner | epoch 014:    304 / 393 loss=6.539, ppl=92.97, wps=14709.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.556, loss_scale=8, train_wall=441, gb_free=10.1, wall=24258
2022-03-03 17:41:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:42:02 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.067 | ppl 134.04 | wps 34036.8 | wpb 2034.1 | bsz 4 | num_updates 5489 | best_loss 7.067
2022-03-03 17:42:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5489 updates
2022-03-03 17:42:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 17:42:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 17:42:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 14 @ 5489 updates, score 7.067) (writing took 4.567251779139042 seconds)
2022-03-03 17:42:07 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-03 17:42:07 | INFO | train | epoch 014 | loss 6.518 | ppl 91.65 | wps 14579.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5489 | lr 0.000426828 | gnorm 0.563 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 24663
2022-03-03 17:42:07 | INFO | fairseq.trainer | begin training epoch 15
2022-03-03 17:42:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 17:42:56 | INFO | train_inner | epoch 015:     11 / 393 loss=6.524, ppl=92.01, wps=14351.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.569, loss_scale=8, train_wall=439, gb_free=10.1, wall=24712
2022-03-03 17:50:21 | INFO | train_inner | epoch 015:    111 / 393 loss=6.386, ppl=83.65, wps=14712.2, ups=0.22, wpb=65530.2, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.563, loss_scale=8, train_wall=441, gb_free=10.1, wall=25158
2022-03-03 17:57:47 | INFO | train_inner | epoch 015:    211 / 393 loss=6.418, ppl=85.51, wps=14710.3, ups=0.22, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.57, loss_scale=16, train_wall=441, gb_free=10.1, wall=25603
2022-03-03 18:05:12 | INFO | train_inner | epoch 015:    311 / 393 loss=6.441, ppl=86.89, wps=14707.4, ups=0.22, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.565, loss_scale=16, train_wall=441, gb_free=10.1, wall=26049
2022-03-03 18:11:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:11:22 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.039 | ppl 131.51 | wps 33880.2 | wpb 2034.1 | bsz 4 | num_updates 5882 | best_loss 7.039
2022-03-03 18:11:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5882 updates
2022-03-03 18:11:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 18:11:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 18:11:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 15 @ 5882 updates, score 7.039) (writing took 4.60629600007087 seconds)
2022-03-03 18:11:27 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-03 18:11:27 | INFO | train | epoch 015 | loss 6.425 | ppl 85.94 | wps 14614.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 5882 | lr 0.000412323 | gnorm 0.567 | loss_scale 16 | train_wall 1730 | gb_free 10.1 | wall 26424
2022-03-03 18:11:27 | INFO | fairseq.trainer | begin training epoch 16
2022-03-03 18:11:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:12:47 | INFO | train_inner | epoch 016:     18 / 393 loss=6.437, ppl=86.62, wps=14340.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.575, loss_scale=16, train_wall=439, gb_free=10.1, wall=26504
2022-03-03 18:20:13 | INFO | train_inner | epoch 016:    118 / 393 loss=6.297, ppl=78.62, wps=14703.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.565, loss_scale=16, train_wall=441, gb_free=10.1, wall=26950
2022-03-03 18:27:39 | INFO | train_inner | epoch 016:    218 / 393 loss=6.339, ppl=80.98, wps=14700.8, ups=0.22, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.56, loss_scale=16, train_wall=441, gb_free=10.1, wall=27395
2022-03-03 18:34:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 18:35:09 | INFO | train_inner | epoch 016:    319 / 393 loss=6.37, ppl=82.74, wps=14560.8, ups=0.22, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.571, loss_scale=16, train_wall=445, gb_free=10.1, wall=27846
2022-03-03 18:40:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:40:43 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.037 | ppl 131.33 | wps 33834.9 | wpb 2034.1 | bsz 4 | num_updates 6274 | best_loss 7.037
2022-03-03 18:40:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6274 updates
2022-03-03 18:40:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 18:40:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 18:40:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 16 @ 6274 updates, score 7.037) (writing took 4.60249927546829 seconds)
2022-03-03 18:40:48 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-03 18:40:48 | INFO | train | epoch 016 | loss 6.343 | ppl 81.19 | wps 14573.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6274 | lr 0.000399234 | gnorm 0.564 | loss_scale 16 | train_wall 1731 | gb_free 10.1 | wall 28184
2022-03-03 18:40:48 | INFO | fairseq.trainer | begin training epoch 17
2022-03-03 18:40:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:42:44 | INFO | train_inner | epoch 017:     26 / 393 loss=6.339, ppl=80.97, wps=14346.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.554, loss_scale=16, train_wall=439, gb_free=10.1, wall=28300
2022-03-03 18:50:09 | INFO | train_inner | epoch 017:    126 / 393 loss=6.228, ppl=74.96, wps=14704.6, ups=0.22, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.596, loss_scale=16, train_wall=441, gb_free=10.1, wall=28746
2022-03-03 18:57:35 | INFO | train_inner | epoch 017:    226 / 393 loss=6.265, ppl=76.88, wps=14699, ups=0.22, wpb=65530.9, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.568, loss_scale=16, train_wall=441, gb_free=10.1, wall=29192
2022-03-03 19:05:01 | INFO | train_inner | epoch 017:    326 / 393 loss=6.3, ppl=78.81, wps=14701.1, ups=0.22, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.552, loss_scale=16, train_wall=441, gb_free=10.1, wall=29638
2022-03-03 19:05:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 19:09:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:10:04 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.027 | ppl 130.45 | wps 33825.1 | wpb 2034.1 | bsz 4 | num_updates 6666 | best_loss 7.027
2022-03-03 19:10:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6666 updates
2022-03-03 19:10:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 19:10:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-03 19:10:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 17 @ 6666 updates, score 7.027) (writing took 4.55206567235291 seconds)
2022-03-03 19:10:08 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-03 19:10:08 | INFO | train | epoch 017 | loss 6.271 | ppl 77.2 | wps 14573.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 6666 | lr 0.000387318 | gnorm 0.577 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 29945
2022-03-03 19:10:09 | INFO | fairseq.trainer | begin training epoch 18
2022-03-03 19:10:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 19:12:40 | INFO | train_inner | epoch 018:     34 / 393 loss=6.267, ppl=77.03, wps=14214.7, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.585, loss_scale=8, train_wall=443, gb_free=10.1, wall=30097
2022-03-03 19:20:06 | INFO | train_inner | epoch 018:    134 / 393 loss=6.162, ppl=71.59, wps=14707.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.568, loss_scale=8, train_wall=441, gb_free=10.1, wall=30542
2022-03-03 19:27:31 | INFO | train_inner | epoch 018:    234 / 393 loss=6.201, ppl=73.58, wps=14713.6, ups=0.22, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.569, loss_scale=8, train_wall=441, gb_free=10.1, wall=30988
2022-03-03 19:34:56 | INFO | train_inner | epoch 018:    334 / 393 loss=6.24, ppl=75.59, wps=14710.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.567, loss_scale=8, train_wall=441, gb_free=10.1, wall=31433
2022-03-03 19:39:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:39:24 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.038 | ppl 131.39 | wps 34134.1 | wpb 2034.1 | bsz 4 | num_updates 7059 | best_loss 7.027
2022-03-03 19:39:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7059 updates
2022-03-03 19:39:24 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-03 19:39:24 | INFO | train | epoch 018 | loss 6.204 | ppl 73.75 | wps 14656.7 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 7059 | lr 0.000376382 | gnorm 0.566 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 31700
2022-03-03 19:39:24 | INFO | fairseq.trainer | begin training epoch 19
2022-03-03 19:39:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 19:42:26 | INFO | train_inner | epoch 019:     41 / 393 loss=6.177, ppl=72.37, wps=14500.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.586, loss_scale=8, train_wall=439, gb_free=10.1, wall=31883
2022-03-03 19:49:52 | INFO | train_inner | epoch 019:    141 / 393 loss=6.11, ppl=69.05, wps=14707.8, ups=0.22, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.552, loss_scale=16, train_wall=441, gb_free=10.1, wall=32329
2022-03-03 19:53:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 19:57:22 | INFO | train_inner | epoch 019:    242 / 393 loss=6.145, ppl=70.78, wps=14560.1, ups=0.22, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.588, loss_scale=8, train_wall=445, gb_free=10.1, wall=32779
2022-03-03 20:04:48 | INFO | train_inner | epoch 019:    342 / 393 loss=6.179, ppl=72.46, wps=14711.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.586, loss_scale=8, train_wall=441, gb_free=10.1, wall=33224
2022-03-03 20:08:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:08:39 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.042 | ppl 131.74 | wps 33581.5 | wpb 2034.1 | bsz 4 | num_updates 7451 | best_loss 7.027
2022-03-03 20:08:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7451 updates
2022-03-03 20:08:39 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-03 20:08:39 | INFO | train | epoch 019 | loss 6.145 | ppl 70.77 | wps 14616.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7451 | lr 0.000366347 | gnorm 0.579 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 33456
2022-03-03 20:08:39 | INFO | fairseq.trainer | begin training epoch 20
2022-03-03 20:08:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:12:18 | INFO | train_inner | epoch 020:     49 / 393 loss=6.117, ppl=69.4, wps=14493.9, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.555, loss_scale=8, train_wall=439, gb_free=10.1, wall=33674
2022-03-03 20:19:43 | INFO | train_inner | epoch 020:    149 / 393 loss=6.053, ppl=66.42, wps=14710.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.585, loss_scale=8, train_wall=441, gb_free=10.1, wall=34120
2022-03-03 20:27:09 | INFO | train_inner | epoch 020:    249 / 393 loss=6.092, ppl=68.23, wps=14707.1, ups=0.22, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.606, loss_scale=8, train_wall=441, gb_free=10.1, wall=34566
2022-03-03 20:32:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:34:39 | INFO | train_inner | epoch 020:    350 / 393 loss=6.131, ppl=70.08, wps=14566.4, ups=0.22, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.558, loss_scale=8, train_wall=445, gb_free=10.1, wall=35016
2022-03-03 20:37:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:37:55 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.039 | ppl 131.52 | wps 33970.9 | wpb 2034.1 | bsz 4 | num_updates 7843 | best_loss 7.027
2022-03-03 20:37:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7843 updates
2022-03-03 20:37:55 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-03 20:37:55 | INFO | train | epoch 020 | loss 6.089 | ppl 68.07 | wps 14618 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7843 | lr 0.000357075 | gnorm 0.576 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 35212
2022-03-03 20:37:55 | INFO | fairseq.trainer | begin training epoch 21
2022-03-03 20:37:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:42:09 | INFO | train_inner | epoch 021:     57 / 393 loss=6.044, ppl=65.99, wps=14498.2, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.578, loss_scale=8, train_wall=439, gb_free=10.1, wall=35466
2022-03-03 20:49:34 | INFO | train_inner | epoch 021:    157 / 393 loss=6.011, ppl=64.5, wps=14704.7, ups=0.22, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.575, loss_scale=8, train_wall=441, gb_free=10.1, wall=35911
2022-03-03 20:57:00 | INFO | train_inner | epoch 021:    257 / 393 loss=6.047, ppl=66.14, wps=14707.3, ups=0.22, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.579, loss_scale=8, train_wall=441, gb_free=10.1, wall=36357
2022-03-03 21:04:26 | INFO | train_inner | epoch 021:    357 / 393 loss=6.081, ppl=67.71, wps=14705.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.59, loss_scale=8, train_wall=441, gb_free=10.1, wall=36802
2022-03-03 21:07:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:07:11 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.049 | ppl 132.45 | wps 34221.5 | wpb 2034.1 | bsz 4 | num_updates 8236 | best_loss 7.027
2022-03-03 21:07:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8236 updates
2022-03-03 21:07:11 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-03 21:07:11 | INFO | train | epoch 021 | loss 6.039 | ppl 65.75 | wps 14652.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 8236 | lr 0.000348451 | gnorm 0.58 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 36967
2022-03-03 21:07:11 | INFO | fairseq.trainer | begin training epoch 22
2022-03-03 21:07:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:11:56 | INFO | train_inner | epoch 022:     64 / 393 loss=5.984, ppl=63.32, wps=14500.7, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.573, loss_scale=16, train_wall=439, gb_free=10.1, wall=37252
2022-03-03 21:19:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 21:19:26 | INFO | train_inner | epoch 022:    165 / 393 loss=5.966, ppl=62.52, wps=14555, ups=0.22, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.58, loss_scale=8, train_wall=445, gb_free=10.1, wall=37703
2022-03-03 21:26:51 | INFO | train_inner | epoch 022:    265 / 393 loss=5.995, ppl=63.76, wps=14708.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.596, loss_scale=8, train_wall=441, gb_free=10.1, wall=38148
2022-03-03 21:34:17 | INFO | train_inner | epoch 022:    365 / 393 loss=6.039, ppl=65.75, wps=14707.5, ups=0.22, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.613, loss_scale=8, train_wall=441, gb_free=10.1, wall=38594
2022-03-03 21:36:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:36:26 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.057 | ppl 133.19 | wps 34088.7 | wpb 2034.1 | bsz 4 | num_updates 8628 | best_loss 7.027
2022-03-03 21:36:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8628 updates
2022-03-03 21:36:26 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-03 21:36:26 | INFO | train | epoch 022 | loss 5.991 | ppl 63.62 | wps 14615.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8628 | lr 0.000340443 | gnorm 0.59 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 38723
2022-03-03 21:36:26 | INFO | fairseq.trainer | begin training epoch 23
2022-03-03 21:36:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:41:47 | INFO | train_inner | epoch 023:     72 / 393 loss=5.928, ppl=60.9, wps=14492.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.571, loss_scale=8, train_wall=439, gb_free=10.1, wall=39044
2022-03-03 21:49:13 | INFO | train_inner | epoch 023:    172 / 393 loss=5.922, ppl=60.64, wps=14710.2, ups=0.22, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.599, loss_scale=8, train_wall=441, gb_free=10.1, wall=39490
2022-03-03 21:56:38 | INFO | train_inner | epoch 023:    272 / 393 loss=5.962, ppl=62.35, wps=14709.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.619, loss_scale=8, train_wall=441, gb_free=10.1, wall=39935
2022-03-03 22:01:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 22:04:09 | INFO | train_inner | epoch 023:    373 / 393 loss=5.99, ppl=63.57, wps=14556.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.595, loss_scale=8, train_wall=445, gb_free=10.1, wall=40385
2022-03-03 22:05:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:05:42 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.069 | ppl 134.23 | wps 34152.8 | wpb 2034.1 | bsz 4 | num_updates 9020 | best_loss 7.027
2022-03-03 22:05:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9020 updates
2022-03-03 22:05:42 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-03 22:05:42 | INFO | train | epoch 023 | loss 5.947 | ppl 61.69 | wps 14614.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 9020 | lr 0.000332964 | gnorm 0.596 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 40479
2022-03-03 22:05:42 | INFO | fairseq.trainer | begin training epoch 24
2022-03-03 22:05:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:11:39 | INFO | train_inner | epoch 024:     80 / 393 loss=5.87, ppl=58.5, wps=14498.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.582, loss_scale=8, train_wall=439, gb_free=10.1, wall=40835
2022-03-03 22:19:04 | INFO | train_inner | epoch 024:    180 / 393 loss=5.886, ppl=59.12, wps=14707.6, ups=0.22, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.597, loss_scale=8, train_wall=441, gb_free=10.1, wall=41281
2022-03-03 22:26:30 | INFO | train_inner | epoch 024:    280 / 393 loss=5.924, ppl=60.7, wps=14703.3, ups=0.22, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.611, loss_scale=8, train_wall=441, gb_free=10.1, wall=41727
2022-03-03 22:33:55 | INFO | train_inner | epoch 024:    380 / 393 loss=5.954, ppl=61.98, wps=14713.5, ups=0.22, wpb=65530.2, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.603, loss_scale=8, train_wall=441, gb_free=10.1, wall=42172
2022-03-03 22:34:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:34:58 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.069 | ppl 134.28 | wps 33977.7 | wpb 2034.1 | bsz 4 | num_updates 9413 | best_loss 7.027
2022-03-03 22:34:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9413 updates
2022-03-03 22:34:58 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-03 22:34:58 | INFO | train | epoch 024 | loss 5.906 | ppl 59.96 | wps 14654.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9413 | lr 0.000325939 | gnorm 0.601 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 42234
2022-03-03 22:34:58 | INFO | fairseq.trainer | begin training epoch 25
2022-03-03 22:34:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:41:25 | INFO | train_inner | epoch 025:     87 / 393 loss=5.826, ppl=56.73, wps=14495.8, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.584, loss_scale=16, train_wall=439, gb_free=10.1, wall=42622
2022-03-03 22:42:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 22:48:55 | INFO | train_inner | epoch 025:    188 / 393 loss=5.843, ppl=57.38, wps=14563.4, ups=0.22, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.615, loss_scale=8, train_wall=445, gb_free=10.1, wall=43072
2022-03-03 22:56:21 | INFO | train_inner | epoch 025:    288 / 393 loss=5.881, ppl=58.92, wps=14711.2, ups=0.22, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.594, loss_scale=8, train_wall=441, gb_free=10.1, wall=43518
2022-03-03 23:03:46 | INFO | train_inner | epoch 025:    388 / 393 loss=5.928, ppl=60.88, wps=14707.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.589, loss_scale=8, train_wall=441, gb_free=10.1, wall=43963
2022-03-03 23:04:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:04:13 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.09 | ppl 136.26 | wps 34036.9 | wpb 2034.1 | bsz 4 | num_updates 9805 | best_loss 7.027
2022-03-03 23:04:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9805 updates
2022-03-03 23:04:13 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-03 23:04:13 | INFO | train | epoch 025 | loss 5.867 | ppl 58.37 | wps 14617.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 9805 | lr 0.000319357 | gnorm 0.599 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 43990
2022-03-03 23:04:13 | INFO | fairseq.trainer | begin training epoch 26
2022-03-03 23:04:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:11:16 | INFO | train_inner | epoch 026:     95 / 393 loss=5.775, ppl=54.75, wps=14501, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.605, loss_scale=8, train_wall=439, gb_free=10.1, wall=44413
2022-03-03 23:18:42 | INFO | train_inner | epoch 026:    195 / 393 loss=5.815, ppl=56.31, wps=14715.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.614, loss_scale=8, train_wall=440, gb_free=10.1, wall=44858
2022-03-03 23:24:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 23:26:12 | INFO | train_inner | epoch 026:    296 / 393 loss=5.849, ppl=57.64, wps=14563.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.597, loss_scale=8, train_wall=445, gb_free=10.1, wall=45308
2022-03-03 23:33:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:33:28 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.114 | ppl 138.49 | wps 34043.2 | wpb 2034.1 | bsz 4 | num_updates 10197 | best_loss 7.027
2022-03-03 23:33:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10197 updates
2022-03-03 23:33:28 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-03 23:33:28 | INFO | train | epoch 026 | loss 5.831 | ppl 56.91 | wps 14620.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10197 | lr 0.000313158 | gnorm 0.6 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 45745
2022-03-03 23:33:28 | INFO | fairseq.trainer | begin training epoch 27
2022-03-03 23:33:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:33:42 | INFO | train_inner | epoch 027:      3 / 393 loss=5.885, ppl=59.11, wps=14496.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.595, loss_scale=8, train_wall=439, gb_free=10.1, wall=45759
2022-03-03 23:41:07 | INFO | train_inner | epoch 027:    103 / 393 loss=5.733, ppl=53.19, wps=14708.4, ups=0.22, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.59, loss_scale=8, train_wall=441, gb_free=10.1, wall=46204
2022-03-03 23:48:33 | INFO | train_inner | epoch 027:    203 / 393 loss=5.78, ppl=54.95, wps=14705, ups=0.22, wpb=65530.9, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.602, loss_scale=8, train_wall=441, gb_free=10.1, wall=46650
2022-03-03 23:55:59 | INFO | train_inner | epoch 027:    303 / 393 loss=5.823, ppl=56.6, wps=14707.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.617, loss_scale=8, train_wall=441, gb_free=10.1, wall=47095
2022-03-04 00:02:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:02:44 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.124 | ppl 139.5 | wps 34144.3 | wpb 2034.1 | bsz 4 | num_updates 10590 | best_loss 7.027
2022-03-04 00:02:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10590 updates
2022-03-04 00:02:44 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 00:02:44 | INFO | train | epoch 027 | loss 5.797 | ppl 55.6 | wps 14653.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 10590 | lr 0.000307293 | gnorm 0.604 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 47501
2022-03-04 00:02:44 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 00:02:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:03:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 00:03:33 | INFO | train_inner | epoch 028:     11 / 393 loss=5.843, ppl=57.41, wps=14356.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.615, loss_scale=8, train_wall=443, gb_free=10.1, wall=47550
2022-03-04 00:10:59 | INFO | train_inner | epoch 028:    111 / 393 loss=5.704, ppl=52.11, wps=14712.2, ups=0.22, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.602, loss_scale=8, train_wall=441, gb_free=10.1, wall=47995
2022-03-04 00:18:24 | INFO | train_inner | epoch 028:    211 / 393 loss=5.751, ppl=53.87, wps=14708.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.61, loss_scale=8, train_wall=441, gb_free=10.1, wall=48441
2022-03-04 00:25:50 | INFO | train_inner | epoch 028:    311 / 393 loss=5.794, ppl=55.5, wps=14706.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.606, loss_scale=8, train_wall=441, gb_free=10.1, wall=48886
2022-03-04 00:31:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:31:59 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.14 | ppl 141.04 | wps 34064.1 | wpb 2034.1 | bsz 4 | num_updates 10982 | best_loss 7.027
2022-03-04 00:32:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10982 updates
2022-03-04 00:32:00 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 00:32:00 | INFO | train | epoch 028 | loss 5.764 | ppl 54.32 | wps 14617.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10982 | lr 0.000301758 | gnorm 0.608 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 49256
2022-03-04 00:32:00 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 00:32:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:33:20 | INFO | train_inner | epoch 029:     18 / 393 loss=5.789, ppl=55.29, wps=14495.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.601, loss_scale=8, train_wall=439, gb_free=10.1, wall=49337
2022-03-04 00:40:45 | INFO | train_inner | epoch 029:    118 / 393 loss=5.684, ppl=51.41, wps=14707.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.611, loss_scale=8, train_wall=441, gb_free=10.1, wall=49782
2022-03-04 00:48:11 | INFO | train_inner | epoch 029:    218 / 393 loss=5.723, ppl=52.83, wps=14707.1, ups=0.22, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.616, loss_scale=16, train_wall=441, gb_free=10.1, wall=50228
2022-03-04 00:48:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 00:55:41 | INFO | train_inner | epoch 029:    319 / 393 loss=5.766, ppl=54.4, wps=14567.7, ups=0.22, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.601, loss_scale=8, train_wall=445, gb_free=10.1, wall=50678
2022-03-04 01:01:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:01:15 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.152 | ppl 142.21 | wps 34056.3 | wpb 2034.1 | bsz 4 | num_updates 11374 | best_loss 7.027
2022-03-04 01:01:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11374 updates
2022-03-04 01:01:15 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 01:01:15 | INFO | train | epoch 029 | loss 5.733 | ppl 53.17 | wps 14617.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 11374 | lr 0.000296513 | gnorm 0.61 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 51012
2022-03-04 01:01:15 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 01:01:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:03:11 | INFO | train_inner | epoch 030:     26 / 393 loss=5.746, ppl=53.65, wps=14496.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.618, loss_scale=8, train_wall=439, gb_free=10.1, wall=51128
2022-03-04 01:10:36 | INFO | train_inner | epoch 030:    126 / 393 loss=5.655, ppl=50.38, wps=14710.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.632, loss_scale=8, train_wall=441, gb_free=10.1, wall=51573
2022-03-04 01:18:02 | INFO | train_inner | epoch 030:    226 / 393 loss=5.699, ppl=51.96, wps=14712.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.628, loss_scale=8, train_wall=441, gb_free=10.1, wall=52019
2022-03-04 01:25:28 | INFO | train_inner | epoch 030:    326 / 393 loss=5.732, ppl=53.16, wps=14707.5, ups=0.22, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.619, loss_scale=8, train_wall=441, gb_free=10.1, wall=52464
2022-03-04 01:27:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 01:30:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:30:30 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.153 | ppl 142.32 | wps 34058.6 | wpb 2034.1 | bsz 4 | num_updates 11766 | best_loss 7.027
2022-03-04 01:30:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11766 updates
2022-03-04 01:30:31 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 01:30:31 | INFO | train | epoch 030 | loss 5.703 | ppl 52.11 | wps 14617.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 11766 | lr 0.000291532 | gnorm 0.63 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 52767
2022-03-04 01:30:31 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 01:30:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:33:02 | INFO | train_inner | epoch 031:     34 / 393 loss=5.714, ppl=52.48, wps=14353.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.629, loss_scale=8, train_wall=443, gb_free=10.1, wall=52919
2022-03-04 01:40:28 | INFO | train_inner | epoch 031:    134 / 393 loss=5.626, ppl=49.39, wps=14706.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.604, loss_scale=8, train_wall=441, gb_free=10.1, wall=53364
2022-03-04 01:47:53 | INFO | train_inner | epoch 031:    234 / 393 loss=5.672, ppl=50.98, wps=14709.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.624, loss_scale=8, train_wall=441, gb_free=10.1, wall=53810
2022-03-04 01:55:19 | INFO | train_inner | epoch 031:    334 / 393 loss=5.714, ppl=52.49, wps=14712.8, ups=0.22, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.635, loss_scale=8, train_wall=441, gb_free=10.1, wall=54255
2022-03-04 01:59:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:59:46 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.169 | ppl 143.95 | wps 34154 | wpb 2034.1 | bsz 4 | num_updates 12159 | best_loss 7.027
2022-03-04 01:59:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12159 updates
2022-03-04 01:59:46 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 01:59:46 | INFO | train | epoch 031 | loss 5.676 | ppl 51.12 | wps 14655.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 12159 | lr 0.000286781 | gnorm 0.614 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 54523
2022-03-04 01:59:46 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 01:59:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:02:49 | INFO | train_inner | epoch 032:     41 / 393 loss=5.678, ppl=51.19, wps=14497, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.639, loss_scale=8, train_wall=439, gb_free=10.1, wall=54705
2022-03-04 02:09:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 02:10:19 | INFO | train_inner | epoch 032:    142 / 393 loss=5.601, ppl=48.53, wps=14561.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.626, loss_scale=8, train_wall=445, gb_free=10.1, wall=55155
2022-03-04 02:17:44 | INFO | train_inner | epoch 032:    242 / 393 loss=5.651, ppl=50.26, wps=14708.9, ups=0.22, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.621, loss_scale=8, train_wall=441, gb_free=10.1, wall=55601
2022-03-04 02:25:11 | INFO | train_inner | epoch 032:    342 / 393 loss=5.697, ppl=51.89, wps=14680.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.664, loss_scale=8, train_wall=441, gb_free=10.1, wall=56047
2022-03-04 02:28:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:29:02 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.188 | ppl 145.85 | wps 34053.9 | wpb 2034.1 | bsz 4 | num_updates 12551 | best_loss 7.027
2022-03-04 02:29:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12551 updates
2022-03-04 02:29:02 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 02:29:02 | INFO | train | epoch 032 | loss 5.649 | ppl 50.18 | wps 14608.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12551 | lr 0.000282267 | gnorm 0.643 | loss_scale 8 | train_wall 1731 | gb_free 10.1 | wall 56279
2022-03-04 02:29:02 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 02:29:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:32:41 | INFO | train_inner | epoch 033:     49 / 393 loss=5.626, ppl=49.4, wps=14497.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.655, loss_scale=8, train_wall=439, gb_free=10.1, wall=56498
2022-03-04 02:40:07 | INFO | train_inner | epoch 033:    149 / 393 loss=5.584, ppl=47.97, wps=14701.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.631, loss_scale=8, train_wall=441, gb_free=10.1, wall=56943
2022-03-04 02:47:32 | INFO | train_inner | epoch 033:    249 / 393 loss=5.629, ppl=49.5, wps=14713.2, ups=0.22, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.627, loss_scale=16, train_wall=441, gb_free=10.1, wall=57389
2022-03-04 02:49:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 02:55:02 | INFO | train_inner | epoch 033:    350 / 393 loss=5.668, ppl=50.84, wps=14561.5, ups=0.22, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.631, loss_scale=8, train_wall=445, gb_free=10.1, wall=57839
2022-03-04 02:58:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:58:18 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.212 | ppl 148.24 | wps 34155.2 | wpb 2034.1 | bsz 4 | num_updates 12943 | best_loss 7.027
2022-03-04 02:58:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12943 updates
2022-03-04 02:58:18 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 02:58:18 | INFO | train | epoch 033 | loss 5.624 | ppl 49.31 | wps 14616.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12943 | lr 0.00027796 | gnorm 0.637 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 58035
2022-03-04 02:58:18 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 02:58:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:02:32 | INFO | train_inner | epoch 034:     57 / 393 loss=5.591, ppl=48.2, wps=14496.1, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.648, loss_scale=8, train_wall=439, gb_free=10.1, wall=58289
2022-03-04 03:09:58 | INFO | train_inner | epoch 034:    157 / 393 loss=5.565, ppl=47.35, wps=14704.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.634, loss_scale=8, train_wall=441, gb_free=10.1, wall=58734
2022-03-04 03:17:23 | INFO | train_inner | epoch 034:    257 / 393 loss=5.606, ppl=48.71, wps=14706, ups=0.22, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.644, loss_scale=8, train_wall=441, gb_free=10.1, wall=59180
2022-03-04 03:24:49 | INFO | train_inner | epoch 034:    357 / 393 loss=5.649, ppl=50.18, wps=14704.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.632, loss_scale=8, train_wall=441, gb_free=10.1, wall=59626
2022-03-04 03:27:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:27:34 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.222 | ppl 149.33 | wps 34115.4 | wpb 2034.1 | bsz 4 | num_updates 13336 | best_loss 7.027
2022-03-04 03:27:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13336 updates
2022-03-04 03:27:34 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 03:27:34 | INFO | train | epoch 034 | loss 5.6 | ppl 48.5 | wps 14650.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 13336 | lr 0.000273834 | gnorm 0.639 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 59791
2022-03-04 03:27:34 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 03:27:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:30:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 03:32:24 | INFO | train_inner | epoch 035:     65 / 393 loss=5.564, ppl=47.32, wps=14347.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.645, loss_scale=8, train_wall=443, gb_free=10.1, wall=60081
2022-03-04 03:39:49 | INFO | train_inner | epoch 035:    165 / 393 loss=5.542, ppl=46.59, wps=14707.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.633, loss_scale=8, train_wall=441, gb_free=10.1, wall=60526
2022-03-04 03:47:15 | INFO | train_inner | epoch 035:    265 / 393 loss=5.59, ppl=48.17, wps=14712.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.641, loss_scale=8, train_wall=441, gb_free=10.1, wall=60972
2022-03-04 03:54:40 | INFO | train_inner | epoch 035:    365 / 393 loss=5.625, ppl=49.36, wps=14706.9, ups=0.22, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.639, loss_scale=8, train_wall=441, gb_free=10.1, wall=61417
2022-03-04 03:56:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:56:50 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.242 | ppl 151.41 | wps 34147.2 | wpb 2034.1 | bsz 4 | num_updates 13728 | best_loss 7.027
2022-03-04 03:56:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13728 updates
2022-03-04 03:56:50 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 03:56:50 | INFO | train | epoch 035 | loss 5.576 | ppl 47.72 | wps 14615.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 13728 | lr 0.000269896 | gnorm 0.638 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 61546
2022-03-04 03:56:50 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 03:56:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:02:11 | INFO | train_inner | epoch 036:     72 / 393 loss=5.533, ppl=46.29, wps=14498.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.647, loss_scale=8, train_wall=439, gb_free=10.1, wall=61867
2022-03-04 04:09:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 04:09:41 | INFO | train_inner | epoch 036:    173 / 393 loss=5.521, ppl=45.92, wps=14562, ups=0.22, wpb=65535.4, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.659, loss_scale=8, train_wall=445, gb_free=10.1, wall=62317
2022-03-04 04:17:06 | INFO | train_inner | epoch 036:    273 / 393 loss=5.571, ppl=47.53, wps=14708.1, ups=0.22, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.649, loss_scale=8, train_wall=441, gb_free=10.1, wall=62763
2022-03-04 04:24:32 | INFO | train_inner | epoch 036:    373 / 393 loss=5.605, ppl=48.66, wps=14708.9, ups=0.22, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.646, loss_scale=8, train_wall=441, gb_free=10.1, wall=63208
2022-03-04 04:25:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:26:05 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.257 | ppl 152.99 | wps 34105 | wpb 2034.1 | bsz 4 | num_updates 14120 | best_loss 7.027
2022-03-04 04:26:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14120 updates
2022-03-04 04:26:05 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 04:26:05 | INFO | train | epoch 036 | loss 5.555 | ppl 47 | wps 14616.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14120 | lr 0.000266123 | gnorm 0.655 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 63302
2022-03-04 04:26:05 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 04:26:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:32:02 | INFO | train_inner | epoch 037:     80 / 393 loss=5.499, ppl=45.23, wps=14496.2, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.634, loss_scale=8, train_wall=439, gb_free=10.1, wall=63658
2022-03-04 04:39:27 | INFO | train_inner | epoch 037:    180 / 393 loss=5.505, ppl=45.41, wps=14709.3, ups=0.22, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.659, loss_scale=8, train_wall=441, gb_free=10.1, wall=64104
2022-03-04 04:46:53 | INFO | train_inner | epoch 037:    280 / 393 loss=5.549, ppl=46.82, wps=14711.3, ups=0.22, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.637, loss_scale=8, train_wall=441, gb_free=10.1, wall=64549
2022-03-04 04:47:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 04:54:23 | INFO | train_inner | epoch 037:    381 / 393 loss=5.587, ppl=48.08, wps=14565.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.656, loss_scale=8, train_wall=445, gb_free=10.1, wall=64999
2022-03-04 04:55:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:55:21 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.256 | ppl 152.86 | wps 34076.3 | wpb 2034.1 | bsz 4 | num_updates 14512 | best_loss 7.027
2022-03-04 04:55:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14512 updates
2022-03-04 04:55:21 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 04:55:21 | INFO | train | epoch 037 | loss 5.533 | ppl 46.3 | wps 14618.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14512 | lr 0.000262504 | gnorm 0.642 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 65057
2022-03-04 04:55:21 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 04:55:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:01:53 | INFO | train_inner | epoch 038:     88 / 393 loss=5.465, ppl=44.17, wps=14497.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.641, loss_scale=8, train_wall=439, gb_free=10.1, wall=65449
2022-03-04 05:09:18 | INFO | train_inner | epoch 038:    188 / 393 loss=5.492, ppl=45, wps=14711.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.65, loss_scale=8, train_wall=441, gb_free=10.1, wall=65895
2022-03-04 05:16:44 | INFO | train_inner | epoch 038:    288 / 393 loss=5.53, ppl=46.21, wps=14712.3, ups=0.22, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.651, loss_scale=8, train_wall=441, gb_free=10.1, wall=66340
2022-03-04 05:24:09 | INFO | train_inner | epoch 038:    388 / 393 loss=5.575, ppl=47.67, wps=14712.4, ups=0.22, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.657, loss_scale=8, train_wall=441, gb_free=10.1, wall=66786
2022-03-04 05:24:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:24:36 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.276 | ppl 154.98 | wps 34092.6 | wpb 2034.1 | bsz 4 | num_updates 14905 | best_loss 7.027
2022-03-04 05:24:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14905 updates
2022-03-04 05:24:36 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 05:24:36 | INFO | train | epoch 038 | loss 5.513 | ppl 45.67 | wps 14657.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 14905 | lr 0.00025902 | gnorm 0.651 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 66813
2022-03-04 05:24:36 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 05:24:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:27:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 05:31:44 | INFO | train_inner | epoch 039:     96 / 393 loss=5.429, ppl=43.07, wps=14357.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.675, loss_scale=8, train_wall=443, gb_free=10.1, wall=67240
2022-03-04 05:39:09 | INFO | train_inner | epoch 039:    196 / 393 loss=5.475, ppl=44.47, wps=14704.3, ups=0.22, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.634, loss_scale=8, train_wall=441, gb_free=10.1, wall=67686
2022-03-04 05:46:35 | INFO | train_inner | epoch 039:    296 / 393 loss=5.517, ppl=45.81, wps=14709, ups=0.22, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.649, loss_scale=8, train_wall=441, gb_free=10.1, wall=68132
2022-03-04 05:53:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:53:51 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.284 | ppl 155.9 | wps 34074.2 | wpb 2034.1 | bsz 4 | num_updates 15297 | best_loss 7.027
2022-03-04 05:53:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15297 updates
2022-03-04 05:53:51 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 05:53:51 | INFO | train | epoch 039 | loss 5.493 | ppl 45.03 | wps 14617.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 15297 | lr 0.00025568 | gnorm 0.653 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 68568
2022-03-04 05:53:51 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 05:53:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:54:05 | INFO | train_inner | epoch 040:      3 / 393 loss=5.55, ppl=46.86, wps=14497.4, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.652, loss_scale=8, train_wall=439, gb_free=10.1, wall=68582
2022-03-04 06:01:30 | INFO | train_inner | epoch 040:    103 / 393 loss=5.414, ppl=42.64, wps=14711.5, ups=0.22, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.657, loss_scale=8, train_wall=441, gb_free=10.1, wall=69027
2022-03-04 06:06:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 06:09:00 | INFO | train_inner | epoch 040:    204 / 393 loss=5.451, ppl=43.74, wps=14567.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.658, loss_scale=8, train_wall=445, gb_free=10.1, wall=69477
2022-03-04 06:16:26 | INFO | train_inner | epoch 040:    304 / 393 loss=5.503, ppl=45.35, wps=14702.4, ups=0.22, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.663, loss_scale=8, train_wall=441, gb_free=10.1, wall=69923
2022-03-04 06:23:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:23:07 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.295 | ppl 157.05 | wps 34144.2 | wpb 2034.1 | bsz 4 | num_updates 15689 | best_loss 7.027
2022-03-04 06:23:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15689 updates
2022-03-04 06:23:07 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 06:23:07 | INFO | train | epoch 040 | loss 5.474 | ppl 44.46 | wps 14617 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 15689 | lr 0.000252466 | gnorm 0.662 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 70324
2022-03-04 06:23:07 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 06:23:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:23:56 | INFO | train_inner | epoch 041:     11 / 393 loss=5.522, ppl=45.95, wps=14495.2, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.666, loss_scale=8, train_wall=439, gb_free=10.1, wall=70373
2022-03-04 06:31:22 | INFO | train_inner | epoch 041:    111 / 393 loss=5.398, ppl=42.15, wps=14710.9, ups=0.22, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.674, loss_scale=8, train_wall=441, gb_free=10.1, wall=70818
2022-03-04 06:38:47 | INFO | train_inner | epoch 041:    211 / 393 loss=5.446, ppl=43.6, wps=14707.1, ups=0.22, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.664, loss_scale=8, train_wall=441, gb_free=10.1, wall=71264
2022-03-04 06:46:13 | INFO | train_inner | epoch 041:    311 / 393 loss=5.486, ppl=44.8, wps=14703.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.663, loss_scale=16, train_wall=441, gb_free=10.1, wall=71710
2022-03-04 06:52:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:52:23 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.313 | ppl 158.96 | wps 34190.9 | wpb 2034.1 | bsz 4 | num_updates 16082 | best_loss 7.027
2022-03-04 06:52:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16082 updates
2022-03-04 06:52:23 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 06:52:23 | INFO | train | epoch 041 | loss 5.456 | ppl 43.91 | wps 14651.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 16082 | lr 0.000249362 | gnorm 0.663 | loss_scale 16 | train_wall 1730 | gb_free 10.1 | wall 72080
2022-03-04 06:52:23 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 06:52:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:53:43 | INFO | train_inner | epoch 042:     18 / 393 loss=5.486, ppl=44.83, wps=14489, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.657, loss_scale=16, train_wall=439, gb_free=10.1, wall=72160
2022-03-04 06:57:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 07:01:13 | INFO | train_inner | epoch 042:    119 / 393 loss=5.388, ppl=41.86, wps=14563.6, ups=0.22, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.664, loss_scale=8, train_wall=445, gb_free=10.1, wall=72610
2022-03-04 07:08:39 | INFO | train_inner | epoch 042:    219 / 393 loss=5.429, ppl=43.08, wps=14706.1, ups=0.22, wpb=65530.2, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.659, loss_scale=8, train_wall=441, gb_free=10.1, wall=73055
2022-03-04 07:16:04 | INFO | train_inner | epoch 042:    319 / 393 loss=5.468, ppl=44.26, wps=14709.7, ups=0.22, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.662, loss_scale=8, train_wall=441, gb_free=10.1, wall=73501
2022-03-04 07:21:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:21:38 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.318 | ppl 159.53 | wps 34102.8 | wpb 2034.1 | bsz 4 | num_updates 16474 | best_loss 7.027
2022-03-04 07:21:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16474 updates
2022-03-04 07:21:38 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 07:21:38 | INFO | train | epoch 042 | loss 5.439 | ppl 43.38 | wps 14616.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16474 | lr 0.000246377 | gnorm 0.664 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 73835
2022-03-04 07:21:38 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 07:21:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:23:34 | INFO | train_inner | epoch 043:     26 / 393 loss=5.462, ppl=44.07, wps=14495.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.68, loss_scale=8, train_wall=439, gb_free=10.1, wall=73951
2022-03-04 07:31:00 | INFO | train_inner | epoch 043:    126 / 393 loss=5.371, ppl=41.38, wps=14707.8, ups=0.22, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.675, loss_scale=8, train_wall=441, gb_free=10.1, wall=74397
2022-03-04 07:35:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 07:38:30 | INFO | train_inner | epoch 043:    227 / 393 loss=5.415, ppl=42.67, wps=14563.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.676, loss_scale=8, train_wall=445, gb_free=10.1, wall=74847
2022-03-04 07:45:56 | INFO | train_inner | epoch 043:    327 / 393 loss=5.455, ppl=43.88, wps=14709.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.698, loss_scale=8, train_wall=441, gb_free=10.1, wall=75292
2022-03-04 07:50:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:50:54 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.34 | ppl 162.04 | wps 34058 | wpb 2034.1 | bsz 4 | num_updates 16866 | best_loss 7.027
2022-03-04 07:50:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16866 updates
2022-03-04 07:50:54 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 07:50:54 | INFO | train | epoch 043 | loss 5.422 | ppl 42.88 | wps 14616.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16866 | lr 0.000243497 | gnorm 0.683 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 75591
2022-03-04 07:50:54 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 07:50:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:53:26 | INFO | train_inner | epoch 044:     34 / 393 loss=5.439, ppl=43.38, wps=14498.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.666, loss_scale=8, train_wall=439, gb_free=10.1, wall=75742
2022-03-04 08:00:51 | INFO | train_inner | epoch 044:    134 / 393 loss=5.37, ppl=41.35, wps=14708.2, ups=0.22, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.673, loss_scale=8, train_wall=441, gb_free=10.1, wall=76188
2022-03-04 08:08:17 | INFO | train_inner | epoch 044:    234 / 393 loss=5.393, ppl=42.02, wps=14706.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.668, loss_scale=8, train_wall=441, gb_free=10.1, wall=76633
2022-03-04 08:15:42 | INFO | train_inner | epoch 044:    334 / 393 loss=5.439, ppl=43.39, wps=14710.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.69, loss_scale=16, train_wall=441, gb_free=10.1, wall=77079
2022-03-04 08:18:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 08:20:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:20:10 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.325 | ppl 160.39 | wps 34149.6 | wpb 2034.1 | bsz 4 | num_updates 17258 | best_loss 7.027
2022-03-04 08:20:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17258 updates
2022-03-04 08:20:10 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 08:20:10 | INFO | train | epoch 044 | loss 5.406 | ppl 42.41 | wps 14616.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 17258 | lr 0.000240716 | gnorm 0.681 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 77346
2022-03-04 08:20:10 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 08:20:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:23:17 | INFO | train_inner | epoch 045:     42 / 393 loss=5.409, ppl=42.49, wps=14355, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.697, loss_scale=8, train_wall=443, gb_free=10.1, wall=77533
2022-03-04 08:30:42 | INFO | train_inner | epoch 045:    142 / 393 loss=5.353, ppl=40.86, wps=14710.2, ups=0.22, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.693, loss_scale=8, train_wall=441, gb_free=10.1, wall=77979
2022-03-04 08:38:08 | INFO | train_inner | epoch 045:    242 / 393 loss=5.386, ppl=41.81, wps=14708.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.682, loss_scale=8, train_wall=441, gb_free=10.1, wall=78425
2022-03-04 08:45:33 | INFO | train_inner | epoch 045:    342 / 393 loss=5.43, ppl=43.1, wps=14707.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.687, loss_scale=8, train_wall=441, gb_free=10.1, wall=78870
2022-03-04 08:49:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:49:25 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.352 | ppl 163.35 | wps 34089.1 | wpb 2034.1 | bsz 4 | num_updates 17651 | best_loss 7.027
2022-03-04 08:49:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17651 updates
2022-03-04 08:49:25 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 08:49:25 | INFO | train | epoch 045 | loss 5.391 | ppl 41.95 | wps 14654.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 17651 | lr 0.000238021 | gnorm 0.685 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 79102
2022-03-04 08:49:25 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 08:49:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:53:04 | INFO | train_inner | epoch 046:     49 / 393 loss=5.381, ppl=41.66, wps=14497.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.676, loss_scale=8, train_wall=439, gb_free=10.1, wall=79320
2022-03-04 09:00:29 | INFO | train_inner | epoch 046:    149 / 393 loss=5.337, ppl=40.41, wps=14709.2, ups=0.22, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.689, loss_scale=16, train_wall=441, gb_free=10.1, wall=79766
2022-03-04 09:00:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:07:59 | INFO | train_inner | epoch 046:    250 / 393 loss=5.38, ppl=41.65, wps=14564.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.685, loss_scale=8, train_wall=445, gb_free=10.1, wall=80216
2022-03-04 09:15:24 | INFO | train_inner | epoch 046:    350 / 393 loss=5.422, ppl=42.87, wps=14712.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.689, loss_scale=8, train_wall=441, gb_free=10.1, wall=80661
2022-03-04 09:18:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:18:40 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.364 | ppl 164.78 | wps 34104.8 | wpb 2034.1 | bsz 4 | num_updates 18043 | best_loss 7.027
2022-03-04 09:18:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18043 updates
2022-03-04 09:18:40 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 09:18:40 | INFO | train | epoch 046 | loss 5.376 | ppl 41.52 | wps 14619 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18043 | lr 0.000235421 | gnorm 0.683 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 80857
2022-03-04 09:18:40 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 09:18:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:22:54 | INFO | train_inner | epoch 047:     57 / 393 loss=5.354, ppl=40.9, wps=14499.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.693, loss_scale=8, train_wall=439, gb_free=10.1, wall=81111
2022-03-04 09:30:20 | INFO | train_inner | epoch 047:    157 / 393 loss=5.32, ppl=39.95, wps=14708.9, ups=0.22, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.676, loss_scale=8, train_wall=441, gb_free=10.1, wall=81557
2022-03-04 09:37:46 | INFO | train_inner | epoch 047:    257 / 393 loss=5.373, ppl=41.44, wps=14708.2, ups=0.22, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.723, loss_scale=8, train_wall=441, gb_free=10.1, wall=82002
2022-03-04 09:42:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:45:15 | INFO | train_inner | epoch 047:    358 / 393 loss=5.406, ppl=42.4, wps=14564.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.682, loss_scale=8, train_wall=445, gb_free=10.1, wall=82452
2022-03-04 09:47:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:47:56 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.371 | ppl 165.58 | wps 34145.4 | wpb 2034.1 | bsz 4 | num_updates 18435 | best_loss 7.027
2022-03-04 09:47:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18435 updates
2022-03-04 09:47:56 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 09:47:56 | INFO | train | epoch 047 | loss 5.362 | ppl 41.13 | wps 14618.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18435 | lr 0.000232905 | gnorm 0.698 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 82613
2022-03-04 09:47:56 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 09:47:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:52:46 | INFO | train_inner | epoch 048:     65 / 393 loss=5.333, ppl=40.31, wps=14497.8, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.697, loss_scale=8, train_wall=439, gb_free=10.1, wall=82902
2022-03-04 10:00:11 | INFO | train_inner | epoch 048:    165 / 393 loss=5.318, ppl=39.89, wps=14712.8, ups=0.22, wpb=65530.2, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.69, loss_scale=8, train_wall=440, gb_free=10.1, wall=83348
2022-03-04 10:07:36 | INFO | train_inner | epoch 048:    265 / 393 loss=5.361, ppl=41.09, wps=14709.1, ups=0.22, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.701, loss_scale=8, train_wall=441, gb_free=10.1, wall=83793
2022-03-04 10:15:02 | INFO | train_inner | epoch 048:    365 / 393 loss=5.396, ppl=42.1, wps=14710.7, ups=0.22, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.716, loss_scale=8, train_wall=441, gb_free=10.1, wall=84239
2022-03-04 10:17:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:17:11 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.39 | ppl 167.76 | wps 34149.4 | wpb 2034.1 | bsz 4 | num_updates 18828 | best_loss 7.027
2022-03-04 10:17:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18828 updates
2022-03-04 10:17:11 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 10:17:11 | INFO | train | epoch 048 | loss 5.348 | ppl 40.74 | wps 14655.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 18828 | lr 0.000230461 | gnorm 0.699 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 84368
2022-03-04 10:17:11 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 10:17:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:22:32 | INFO | train_inner | epoch 049:     72 / 393 loss=5.306, ppl=39.57, wps=14488.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.68, loss_scale=16, train_wall=439, gb_free=10.1, wall=84689
2022-03-04 10:25:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 10:30:02 | INFO | train_inner | epoch 049:    173 / 393 loss=5.3, ppl=39.41, wps=14564.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.714, loss_scale=8, train_wall=445, gb_free=10.1, wall=85139
2022-03-04 10:37:28 | INFO | train_inner | epoch 049:    273 / 393 loss=5.347, ppl=40.7, wps=14712.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.698, loss_scale=8, train_wall=441, gb_free=10.1, wall=85584
2022-03-04 10:44:53 | INFO | train_inner | epoch 049:    373 / 393 loss=5.391, ppl=41.96, wps=14712.5, ups=0.22, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.699, loss_scale=8, train_wall=441, gb_free=10.1, wall=86030
2022-03-04 10:46:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:46:27 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.391 | ppl 167.87 | wps 34132.1 | wpb 2034.1 | bsz 4 | num_updates 19220 | best_loss 7.027
2022-03-04 10:46:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19220 updates
2022-03-04 10:46:27 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 10:46:27 | INFO | train | epoch 049 | loss 5.335 | ppl 40.35 | wps 14617.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 19220 | lr 0.000228099 | gnorm 0.698 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 86123
2022-03-04 10:46:27 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 10:46:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:52:23 | INFO | train_inner | epoch 050:     80 / 393 loss=5.284, ppl=38.97, wps=14499.4, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=19300, lr=0.000227626, gnorm=0.693, loss_scale=8, train_wall=439, gb_free=10.1, wall=86480
2022-03-04 10:59:49 | INFO | train_inner | epoch 050:    180 / 393 loss=5.295, ppl=39.26, wps=14709.4, ups=0.22, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.683, loss_scale=8, train_wall=441, gb_free=10.1, wall=86925
2022-03-04 11:03:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 11:07:19 | INFO | train_inner | epoch 050:    281 / 393 loss=5.337, ppl=40.43, wps=14567.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.716, loss_scale=8, train_wall=445, gb_free=10.1, wall=87375
2022-03-04 11:14:44 | INFO | train_inner | epoch 050:    381 / 393 loss=5.381, ppl=41.68, wps=14706.7, ups=0.22, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.694, loss_scale=8, train_wall=441, gb_free=10.1, wall=87821
2022-03-04 11:15:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:15:42 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.405 | ppl 169.43 | wps 34109.7 | wpb 2034.1 | bsz 4 | num_updates 19612 | best_loss 7.027
2022-03-04 11:15:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19612 updates
2022-03-04 11:15:42 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 11:15:42 | INFO | train | epoch 050 | loss 5.322 | ppl 39.99 | wps 14618.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 19612 | lr 0.000225808 | gnorm 0.698 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 87879
2022-03-04 11:15:42 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 11:15:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:22:14 | INFO | train_inner | epoch 051:     88 / 393 loss=5.257, ppl=38.24, wps=14497, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.702, loss_scale=8, train_wall=439, gb_free=10.1, wall=88271
2022-03-04 11:29:40 | INFO | train_inner | epoch 051:    188 / 393 loss=5.29, ppl=39.12, wps=14706.7, ups=0.22, wpb=65530.2, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.709, loss_scale=8, train_wall=441, gb_free=10.1, wall=88717
2022-03-04 11:37:05 | INFO | train_inner | epoch 051:    288 / 393 loss=5.327, ppl=40.14, wps=14709.1, ups=0.22, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.696, loss_scale=8, train_wall=441, gb_free=10.1, wall=89162
2022-03-04 11:44:31 | INFO | train_inner | epoch 051:    388 / 393 loss=5.371, ppl=41.38, wps=14710.2, ups=0.22, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.729, loss_scale=16, train_wall=441, gb_free=10.1, wall=89608
2022-03-04 11:44:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:44:58 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.42 | ppl 171.3 | wps 34097.5 | wpb 2034.1 | bsz 4 | num_updates 20005 | best_loss 7.027
2022-03-04 11:44:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20005 updates
2022-03-04 11:44:58 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 11:44:58 | INFO | train | epoch 051 | loss 5.31 | ppl 39.66 | wps 14653.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 20005 | lr 0.000223579 | gnorm 0.707 | loss_scale 16 | train_wall 1730 | gb_free 10.1 | wall 89634
2022-03-04 11:44:58 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 11:44:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:46:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 11:52:05 | INFO | train_inner | epoch 052:     96 / 393 loss=5.239, ppl=37.78, wps=14354.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=20100, lr=0.00022305, gnorm=0.684, loss_scale=8, train_wall=443, gb_free=10.1, wall=90062
2022-03-04 11:59:31 | INFO | train_inner | epoch 052:    196 / 393 loss=5.277, ppl=38.78, wps=14709.5, ups=0.22, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.716, loss_scale=8, train_wall=441, gb_free=10.1, wall=90508
2022-03-04 12:06:57 | INFO | train_inner | epoch 052:    296 / 393 loss=5.321, ppl=39.98, wps=14704.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.711, loss_scale=8, train_wall=441, gb_free=10.1, wall=90953
2022-03-04 12:14:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:14:13 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.432 | ppl 172.7 | wps 34132.8 | wpb 2034.1 | bsz 4 | num_updates 20397 | best_loss 7.027
2022-03-04 12:14:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20397 updates
2022-03-04 12:14:13 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 12:14:13 | INFO | train | epoch 052 | loss 5.298 | ppl 39.33 | wps 14616.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20397 | lr 0.00022142 | gnorm 0.711 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 91390
2022-03-04 12:14:13 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 12:14:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:14:27 | INFO | train_inner | epoch 053:      3 / 393 loss=5.354, ppl=40.9, wps=14498.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.731, loss_scale=8, train_wall=439, gb_free=10.1, wall=91403
2022-03-04 12:21:52 | INFO | train_inner | epoch 053:    103 / 393 loss=5.226, ppl=37.43, wps=14706.6, ups=0.22, wpb=65530.9, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.687, loss_scale=8, train_wall=441, gb_free=10.1, wall=91849
2022-03-04 12:29:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 12:29:22 | INFO | train_inner | epoch 053:    204 / 393 loss=5.271, ppl=38.61, wps=14563.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.697, loss_scale=8, train_wall=445, gb_free=10.1, wall=92299
2022-03-04 12:36:48 | INFO | train_inner | epoch 053:    304 / 393 loss=5.311, ppl=39.69, wps=14713.3, ups=0.22, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.715, loss_scale=8, train_wall=441, gb_free=10.1, wall=92744
2022-03-04 12:43:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:43:29 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.438 | ppl 173.44 | wps 34129.7 | wpb 2034.1 | bsz 4 | num_updates 20789 | best_loss 7.027
2022-03-04 12:43:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20789 updates
2022-03-04 12:43:29 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 12:43:29 | INFO | train | epoch 053 | loss 5.285 | ppl 39 | wps 14618.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20789 | lr 0.000219323 | gnorm 0.7 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 93145
2022-03-04 12:43:29 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 12:43:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:44:18 | INFO | train_inner | epoch 054:     11 / 393 loss=5.323, ppl=40.04, wps=14501.2, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=20800, lr=0.000219265, gnorm=0.702, loss_scale=8, train_wall=439, gb_free=10.1, wall=93194
2022-03-04 12:51:43 | INFO | train_inner | epoch 054:    111 / 393 loss=5.223, ppl=37.34, wps=14710.9, ups=0.22, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.707, loss_scale=8, train_wall=441, gb_free=10.1, wall=93640
2022-03-04 12:59:09 | INFO | train_inner | epoch 054:    211 / 393 loss=5.262, ppl=38.36, wps=14704.2, ups=0.22, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.724, loss_scale=8, train_wall=441, gb_free=10.1, wall=94086
2022-03-04 13:06:34 | INFO | train_inner | epoch 054:    311 / 393 loss=5.299, ppl=39.37, wps=14708.2, ups=0.22, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.704, loss_scale=8, train_wall=441, gb_free=10.1, wall=94531
2022-03-04 13:07:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 13:12:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:12:44 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.46 | ppl 176.08 | wps 34138.1 | wpb 2034.1 | bsz 4 | num_updates 21181 | best_loss 7.027
2022-03-04 13:12:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21181 updates
2022-03-04 13:12:44 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 13:12:44 | INFO | train | epoch 054 | loss 5.275 | ppl 38.72 | wps 14615.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 21181 | lr 0.000217284 | gnorm 0.717 | loss_scale 8 | train_wall 1730 | gb_free 10.1 | wall 94901
2022-03-04 13:12:44 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 13:12:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:14:09 | INFO | train_inner | epoch 055:     19 / 393 loss=5.31, ppl=39.66, wps=14349.4, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.725, loss_scale=8, train_wall=443, gb_free=10.1, wall=94986
2022-03-04 13:21:35 | INFO | train_inner | epoch 055:    119 / 393 loss=5.211, ppl=37.03, wps=14706.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.696, loss_scale=8, train_wall=441, gb_free=10.1, wall=95431
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
