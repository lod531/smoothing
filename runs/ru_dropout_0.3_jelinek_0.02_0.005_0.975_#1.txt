Sender: LSF System <lsfadmin@eu-g2-05>
Subject: Job 208727636: <ru_dropout_0.3_jelinek_0.02_0.005_0.975_#1> in cluster <euler> Done

Job <ru_dropout_0.3_jelinek_0.02_0.005_0.975_#1> was submitted from host <eu-login-24> by user <andriusb> in cluster <euler> at Tue Mar 15 16:41:42 2022
Job was executed on host(s) <eu-g2-05>, in queue <gpu.120h>, as user <andriusb> in cluster <euler> at Tue Mar 15 16:42:04 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar 15 16:42:04 2022
Terminated at Wed Mar 16 15:41:06 2022
Results reported at Wed Mar 16 15:41:06 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/ru --save-dir /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.3 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas \(0.02,0.005,0.975\) --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --patience 3 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   86542.56 sec.
    Max Memory :                                 3887 MB
    Average Memory :                             2954.13 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               16113.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   82742 sec.
    Turnaround time :                            82764 sec.

The output (if any) follows:

2022-03-15 16:42:13 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.3, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/ru', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.02,0.005,0.975)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-15 16:42:13 | INFO | fairseq.tasks.language_modeling | dictionary: 35920 types
2022-03-15 16:42:14 | INFO | fairseq.data.data_utils | loaded 53,136 examples from: data-bin/ru/train
Calculating frequency stats:
  0%|          | 0/53136 [00:00<?, ?it/s]  0%|          | 65/53136 [00:00<01:22, 645.82it/s]  0%|          | 137/53136 [00:00<01:17, 688.18it/s]  0%|          | 223/53136 [00:00<01:09, 765.27it/s]  1%|          | 300/53136 [00:00<01:14, 713.67it/s]  1%|          | 372/53136 [00:00<01:13, 714.99it/s]  1%|          | 444/53136 [00:00<01:19, 662.03it/s]  1%|          | 511/53136 [00:00<01:30, 584.48it/s]  1%|          | 572/53136 [00:00<01:34, 557.65it/s]  1%|          | 629/53136 [00:01<01:35, 551.32it/s]  1%|▏         | 687/53136 [00:01<01:33, 558.30it/s]  1%|▏         | 762/53136 [00:01<01:26, 608.15it/s]  2%|▏         | 831/53136 [00:01<01:23, 625.14it/s]  2%|▏         | 909/53136 [00:01<01:18, 669.19it/s]  2%|▏         | 1006/53136 [00:01<01:08, 756.56it/s]  2%|▏         | 1083/53136 [00:01<01:17, 670.96it/s]  2%|▏         | 1172/53136 [00:01<01:11, 728.57it/s]  2%|▏         | 1266/53136 [00:01<01:06, 785.45it/s]  3%|▎         | 1347/53136 [00:01<01:07, 766.86it/s]  3%|▎         | 1441/53136 [00:02<01:03, 815.44it/s]  3%|▎         | 1536/53136 [00:02<01:00, 848.09it/s]  3%|▎         | 1649/53136 [00:02<00:55, 928.97it/s]  3%|▎         | 1754/53136 [00:02<00:53, 951.63it/s]  3%|▎         | 1852/53136 [00:02<00:53, 956.80it/s]  4%|▎         | 1949/53136 [00:02<00:58, 869.74it/s]  4%|▍         | 2053/53136 [00:02<00:55, 914.82it/s]  4%|▍         | 2148/53136 [00:02<00:55, 923.44it/s]  4%|▍         | 2249/53136 [00:02<00:54, 937.41it/s]  4%|▍         | 2373/53136 [00:03<00:49, 1023.77it/s]  5%|▍         | 2477/53136 [00:03<01:01, 821.95it/s]   5%|▍         | 2567/53136 [00:03<01:00, 834.31it/s]  5%|▌         | 2678/53136 [00:03<00:56, 893.98it/s]  5%|▌         | 2790/53136 [00:03<00:52, 953.99it/s]  5%|▌         | 2889/53136 [00:03<00:57, 877.91it/s]  6%|▌         | 2984/53136 [00:03<00:55, 896.23it/s]  6%|▌         | 3077/53136 [00:03<01:06, 752.48it/s]  6%|▌         | 3158/53136 [00:04<01:05, 764.12it/s]  6%|▌         | 3239/53136 [00:04<01:09, 715.86it/s]  6%|▋         | 3332/53136 [00:04<01:04, 769.17it/s]  6%|▋         | 3412/53136 [00:04<01:08, 728.46it/s]  7%|▋         | 3518/53136 [00:04<01:00, 815.27it/s]  7%|▋         | 3603/53136 [00:04<01:10, 701.44it/s]  7%|▋         | 3681/53136 [00:04<01:08, 719.18it/s]  7%|▋         | 3758/53136 [00:04<01:07, 731.37it/s]  7%|▋         | 3851/53136 [00:04<01:03, 777.27it/s]  7%|▋         | 3947/53136 [00:05<00:59, 827.49it/s]  8%|▊         | 4042/53136 [00:05<00:57, 853.54it/s]  8%|▊         | 4151/53136 [00:05<00:53, 909.35it/s]  8%|▊         | 4243/53136 [00:05<00:53, 908.48it/s]  8%|▊         | 4335/53136 [00:05<01:02, 783.33it/s]  8%|▊         | 4417/53136 [00:05<01:02, 783.49it/s]  8%|▊         | 4499/53136 [00:05<01:01, 793.34it/s]  9%|▊         | 4582/53136 [00:05<01:00, 802.96it/s]  9%|▉         | 4676/53136 [00:05<00:57, 840.13it/s]  9%|▉         | 4762/53136 [00:06<00:58, 832.67it/s]  9%|▉         | 4847/53136 [00:06<01:10, 682.25it/s]  9%|▉         | 4941/53136 [00:06<01:04, 742.85it/s]  9%|▉         | 5042/53136 [00:06<00:59, 809.16it/s] 10%|▉         | 5127/53136 [00:06<01:07, 707.93it/s] 10%|▉         | 5212/53136 [00:06<01:04, 743.60it/s] 10%|▉         | 5294/53136 [00:06<01:06, 720.41it/s] 10%|█         | 5369/53136 [00:06<01:09, 691.05it/s] 10%|█         | 5483/53136 [00:07<00:59, 805.62it/s] 10%|█         | 5569/53136 [00:07<00:57, 820.24it/s] 11%|█         | 5665/53136 [00:07<00:55, 858.66it/s] 11%|█         | 5754/53136 [00:07<00:54, 866.93it/s] 11%|█         | 5853/53136 [00:07<00:52, 900.17it/s] 11%|█         | 5945/53136 [00:07<00:56, 839.17it/s] 11%|█▏        | 6061/53136 [00:07<00:51, 914.86it/s] 12%|█▏        | 6154/53136 [00:07<00:51, 915.27it/s] 12%|█▏        | 6247/53136 [00:07<01:00, 771.73it/s] 12%|█▏        | 6333/53136 [00:08<00:59, 792.85it/s] 12%|█▏        | 6439/53136 [00:08<00:54, 861.65it/s] 12%|█▏        | 6557/53136 [00:08<00:49, 948.68it/s] 13%|█▎        | 6672/53136 [00:08<00:46, 994.39it/s] 13%|█▎        | 6775/53136 [00:08<00:46, 1002.27it/s] 13%|█▎        | 6877/53136 [00:08<00:52, 878.59it/s]  13%|█▎        | 6969/53136 [00:08<01:06, 690.49it/s] 13%|█▎        | 7058/53136 [00:08<01:02, 735.32it/s] 13%|█▎        | 7139/53136 [00:09<01:07, 684.62it/s] 14%|█▎        | 7228/53136 [00:09<01:02, 733.00it/s] 14%|█▍        | 7314/53136 [00:09<01:00, 757.35it/s] 14%|█▍        | 7394/53136 [00:09<01:22, 556.77it/s] 14%|█▍        | 7473/53136 [00:09<01:20, 565.73it/s] 14%|█▍        | 7561/53136 [00:09<01:11, 635.38it/s] 14%|█▍        | 7633/53136 [00:09<01:12, 625.15it/s] 15%|█▍        | 7724/53136 [00:09<01:05, 694.46it/s] 15%|█▍        | 7822/53136 [00:10<00:59, 766.83it/s] 15%|█▍        | 7921/53136 [00:10<00:54, 823.31it/s] 15%|█▌        | 8007/53136 [00:10<01:00, 741.61it/s] 15%|█▌        | 8086/53136 [00:10<01:12, 625.08it/s] 15%|█▌        | 8154/53136 [00:10<01:16, 584.75it/s] 15%|█▌        | 8217/53136 [00:10<01:25, 524.98it/s] 16%|█▌        | 8273/53136 [00:10<01:24, 531.21it/s] 16%|█▌        | 8329/53136 [00:10<01:23, 535.55it/s] 16%|█▌        | 8385/53136 [00:11<01:40, 446.23it/s] 16%|█▌        | 8445/53136 [00:11<01:32, 480.68it/s] 16%|█▌        | 8497/53136 [00:11<01:31, 488.45it/s] 16%|█▌        | 8591/53136 [00:11<01:17, 577.85it/s] 16%|█▋        | 8654/53136 [00:11<01:15, 590.70it/s] 16%|█▋        | 8729/53136 [00:11<01:10, 631.61it/s] 17%|█▋        | 8797/53136 [00:11<01:09, 638.54it/s] 17%|█▋        | 8862/53136 [00:11<01:16, 582.14it/s] 17%|█▋        | 8922/53136 [00:12<01:17, 568.42it/s] 17%|█▋        | 8998/53136 [00:12<01:11, 620.27it/s] 17%|█▋        | 9062/53136 [00:12<01:21, 539.12it/s] 17%|█▋        | 9119/53136 [00:12<01:21, 538.84it/s] 17%|█▋        | 9192/53136 [00:12<01:16, 576.19it/s] 17%|█▋        | 9259/53136 [00:12<01:13, 600.51it/s] 18%|█▊        | 9321/53136 [00:12<01:28, 497.82it/s] 18%|█▊        | 9388/53136 [00:12<01:21, 539.80it/s] 18%|█▊        | 9446/53136 [00:13<01:26, 502.42it/s] 18%|█▊        | 9514/53136 [00:13<01:20, 544.73it/s] 18%|█▊        | 9572/53136 [00:13<01:21, 536.64it/s] 18%|█▊        | 9653/53136 [00:13<01:13, 594.59it/s] 18%|█▊        | 9714/53136 [00:13<01:12, 596.11it/s] 18%|█▊        | 9775/53136 [00:13<01:16, 568.76it/s] 19%|█▊        | 9833/53136 [00:13<01:45, 412.05it/s] 19%|█▊        | 9915/53136 [00:13<01:26, 501.64it/s] 19%|█▉        | 9977/53136 [00:14<01:21, 529.01it/s] 19%|█▉        | 10044/53136 [00:14<01:16, 564.00it/s] 19%|█▉        | 10142/53136 [00:14<01:03, 671.88it/s] 19%|█▉        | 10214/53136 [00:14<01:03, 676.14it/s] 19%|█▉        | 10285/53136 [00:14<01:05, 651.61it/s] 19%|█▉        | 10353/53136 [00:14<01:24, 505.82it/s] 20%|█▉        | 10410/53136 [00:14<01:26, 491.79it/s] 20%|█▉        | 10464/53136 [00:14<01:37, 439.92it/s] 20%|█▉        | 10512/53136 [00:15<01:37, 437.45it/s] 20%|█▉        | 10559/53136 [00:15<01:36, 439.79it/s] 20%|██        | 10640/53136 [00:15<01:19, 532.22it/s] 20%|██        | 10714/53136 [00:15<01:14, 570.43it/s] 20%|██        | 10774/53136 [00:15<01:22, 512.77it/s] 20%|██        | 10828/53136 [00:15<01:29, 471.27it/s] 21%|██        | 10896/53136 [00:15<01:20, 522.17it/s] 21%|██        | 10974/53136 [00:15<01:11, 588.23it/s] 21%|██        | 11036/53136 [00:15<01:11, 586.76it/s] 21%|██        | 11097/53136 [00:16<01:40, 417.83it/s] 21%|██        | 11147/53136 [00:16<01:43, 404.54it/s] 21%|██        | 11201/53136 [00:16<01:36, 432.58it/s] 21%|██        | 11249/53136 [00:16<01:45, 397.01it/s] 21%|██▏       | 11324/53136 [00:16<01:27, 480.01it/s] 21%|██▏       | 11379/53136 [00:16<01:24, 496.83it/s] 22%|██▏       | 11438/53136 [00:16<01:20, 515.86it/s] 22%|██▏       | 11512/53136 [00:16<01:12, 573.76it/s] 22%|██▏       | 11572/53136 [00:17<01:29, 462.40it/s] 22%|██▏       | 11624/53136 [00:17<01:29, 464.75it/s] 22%|██▏       | 11674/53136 [00:17<01:28, 470.26it/s] 22%|██▏       | 11730/53136 [00:17<01:24, 492.46it/s] 22%|██▏       | 11782/53136 [00:17<01:24, 488.72it/s] 22%|██▏       | 11841/53136 [00:17<01:20, 515.63it/s] 22%|██▏       | 11906/53136 [00:17<01:14, 553.50it/s] 23%|██▎       | 11976/53136 [00:17<01:09, 590.48it/s] 23%|██▎       | 12042/53136 [00:18<01:10, 580.28it/s] 23%|██▎       | 12101/53136 [00:18<01:25, 479.66it/s] 23%|██▎       | 12176/53136 [00:18<01:14, 546.51it/s] 23%|██▎       | 12268/53136 [00:18<01:04, 630.68it/s] 23%|██▎       | 12361/53136 [00:18<00:57, 709.74it/s] 23%|██▎       | 12475/53136 [00:18<00:49, 827.57it/s] 24%|██▎       | 12561/53136 [00:18<00:58, 688.02it/s] 24%|██▍       | 12636/53136 [00:18<01:10, 574.75it/s] 24%|██▍       | 12708/53136 [00:19<01:07, 601.08it/s] 24%|██▍       | 12781/53136 [00:19<01:09, 582.02it/s] 24%|██▍       | 12877/53136 [00:19<01:00, 670.75it/s] 24%|██▍       | 12976/53136 [00:19<00:53, 751.39it/s] 25%|██▍       | 13056/53136 [00:19<00:52, 763.68it/s] 25%|██▍       | 13136/53136 [00:19<00:51, 770.86it/s] 25%|██▍       | 13221/53136 [00:19<00:50, 791.22it/s] 25%|██▌       | 13302/53136 [00:19<00:50, 790.26it/s] 25%|██▌       | 13383/53136 [00:19<00:54, 733.23it/s] 25%|██▌       | 13469/53136 [00:20<00:51, 766.40it/s] 25%|██▌       | 13548/53136 [00:20<00:53, 736.58it/s] 26%|██▌       | 13648/53136 [00:20<00:48, 807.72it/s] 26%|██▌       | 13731/53136 [00:20<00:49, 788.29it/s] 26%|██▌       | 13820/53136 [00:20<00:48, 816.84it/s] 26%|██▌       | 13903/53136 [00:20<00:59, 658.48it/s] 26%|██▋       | 13995/53136 [00:20<00:54, 722.47it/s] 26%|██▋       | 14077/53136 [00:20<00:52, 747.24it/s] 27%|██▋       | 14156/53136 [00:21<01:00, 647.33it/s] 27%|██▋       | 14258/53136 [00:21<00:52, 738.95it/s] 27%|██▋       | 14338/53136 [00:21<00:56, 682.53it/s] 27%|██▋       | 14426/53136 [00:21<00:53, 729.07it/s] 27%|██▋       | 14508/53136 [00:21<00:51, 751.17it/s] 27%|██▋       | 14610/53136 [00:21<00:46, 824.49it/s] 28%|██▊       | 14718/53136 [00:21<00:45, 835.53it/s] 28%|██▊       | 14807/53136 [00:21<00:45, 850.15it/s] 28%|██▊       | 14894/53136 [00:21<00:45, 835.07it/s] 28%|██▊       | 14979/53136 [00:22<00:47, 805.38it/s] 28%|██▊       | 15061/53136 [00:22<00:47, 795.75it/s] 28%|██▊       | 15142/53136 [00:22<00:51, 734.72it/s] 29%|██▊       | 15230/53136 [00:22<00:49, 770.90it/s] 29%|██▉       | 15309/53136 [00:22<01:12, 522.28it/s] 29%|██▉       | 15376/53136 [00:22<01:08, 551.85it/s] 29%|██▉       | 15441/53136 [00:22<01:13, 509.43it/s] 29%|██▉       | 15524/53136 [00:23<01:04, 581.43it/s] 29%|██▉       | 15595/53136 [00:23<01:01, 610.40it/s] 29%|██▉       | 15671/53136 [00:23<00:57, 648.86it/s] 30%|██▉       | 15753/53136 [00:23<00:53, 694.03it/s] 30%|██▉       | 15827/53136 [00:23<01:00, 613.23it/s] 30%|██▉       | 15908/53136 [00:23<00:56, 662.60it/s] 30%|███       | 15986/53136 [00:23<00:53, 693.37it/s] 30%|███       | 16078/53136 [00:23<00:49, 755.01it/s] 30%|███       | 16157/53136 [00:23<00:48, 758.33it/s] 31%|███       | 16235/53136 [00:23<00:51, 718.15it/s] 31%|███       | 16337/53136 [00:24<00:46, 796.22it/s] 31%|███       | 16423/53136 [00:24<00:45, 814.14it/s] 31%|███       | 16516/53136 [00:24<00:45, 799.15it/s] 31%|███▏      | 16633/53136 [00:24<00:40, 901.62it/s] 32%|███▏      | 16738/53136 [00:24<00:38, 942.68it/s] 32%|███▏      | 16842/53136 [00:24<00:37, 970.24it/s] 32%|███▏      | 16940/53136 [00:24<00:37, 957.75it/s] 32%|███▏      | 17037/53136 [00:24<00:38, 939.67it/s] 32%|███▏      | 17155/53136 [00:24<00:35, 1008.47it/s] 32%|███▏      | 17257/53136 [00:25<00:37, 947.23it/s]  33%|███▎      | 17353/53136 [00:25<00:40, 889.87it/s] 33%|███▎      | 17444/53136 [00:25<00:41, 857.35it/s] 33%|███▎      | 17542/53136 [00:25<00:40, 888.07it/s] 33%|███▎      | 17635/53136 [00:25<00:39, 899.67it/s] 33%|███▎      | 17727/53136 [00:25<00:39, 902.90it/s] 34%|███▎      | 17824/53136 [00:25<00:40, 866.96it/s] 34%|███▎      | 17912/53136 [00:25<00:42, 833.66it/s] 34%|███▍      | 18004/53136 [00:25<00:40, 857.44it/s] 34%|███▍      | 18098/53136 [00:26<00:39, 878.14it/s] 34%|███▍      | 18192/53136 [00:26<00:39, 895.17it/s] 34%|███▍      | 18282/53136 [00:26<00:43, 795.45it/s] 35%|███▍      | 18364/53136 [00:26<00:47, 732.44it/s] 35%|███▍      | 18440/53136 [00:26<00:51, 667.23it/s] 35%|███▍      | 18510/53136 [00:26<00:51, 674.14it/s] 35%|███▌      | 18603/53136 [00:26<00:47, 729.16it/s] 35%|███▌      | 18678/53136 [00:26<00:48, 714.68it/s] 35%|███▌      | 18792/53136 [00:26<00:41, 828.38it/s] 36%|███▌      | 18914/53136 [00:27<00:36, 931.43it/s] 36%|███▌      | 19009/53136 [00:27<00:39, 870.39it/s] 36%|███▌      | 19117/53136 [00:27<00:37, 917.94it/s] 36%|███▌      | 19211/53136 [00:27<00:36, 917.12it/s] 36%|███▋      | 19304/53136 [00:27<00:38, 869.02it/s] 37%|███▋      | 19397/53136 [00:27<00:38, 884.15it/s] 37%|███▋      | 19487/53136 [00:27<00:42, 793.53it/s] 37%|███▋      | 19569/53136 [00:27<00:50, 668.60it/s] 37%|███▋      | 19641/53136 [00:28<00:51, 650.42it/s] 37%|███▋      | 19709/53136 [00:28<00:53, 628.16it/s] 37%|███▋      | 19774/53136 [00:28<01:01, 544.88it/s] 37%|███▋      | 19832/53136 [00:28<01:04, 515.56it/s] 37%|███▋      | 19892/53136 [00:28<01:02, 534.85it/s] 38%|███▊      | 19948/53136 [00:28<01:02, 532.28it/s] 38%|███▊      | 20004/53136 [00:28<01:01, 537.53it/s] 38%|███▊      | 20068/53136 [00:28<00:58, 563.45it/s] 38%|███▊      | 20126/53136 [00:29<00:59, 553.98it/s] 38%|███▊      | 20182/53136 [00:29<01:03, 521.62it/s] 38%|███▊      | 20235/53136 [00:29<01:29, 368.29it/s] 38%|███▊      | 20295/53136 [00:29<01:19, 415.33it/s] 38%|███▊      | 20343/53136 [00:29<01:18, 417.50it/s] 38%|███▊      | 20390/53136 [00:29<01:18, 419.11it/s] 38%|███▊      | 20443/53136 [00:29<01:13, 442.77it/s] 39%|███▊      | 20541/53136 [00:30<01:09, 467.36it/s] 39%|███▉      | 20614/53136 [00:30<01:01, 527.21it/s] 39%|███▉      | 20701/53136 [00:30<00:53, 610.80it/s] 39%|███▉      | 20793/53136 [00:30<00:47, 677.00it/s] 39%|███▉      | 20866/53136 [00:30<00:47, 686.51it/s] 39%|███▉      | 20950/53136 [00:30<00:46, 698.81it/s] 40%|███▉      | 21030/53136 [00:30<00:44, 723.19it/s] 40%|███▉      | 21105/53136 [00:30<00:44, 724.19it/s] 40%|███▉      | 21179/53136 [00:30<00:54, 589.01it/s] 40%|████      | 21268/53136 [00:31<00:48, 661.74it/s] 40%|████      | 21351/53136 [00:31<00:45, 705.14it/s] 40%|████      | 21426/53136 [00:31<00:45, 696.50it/s] 40%|████      | 21515/53136 [00:31<00:42, 749.26it/s] 41%|████      | 21593/53136 [00:31<00:56, 554.40it/s] 41%|████      | 21658/53136 [00:31<01:13, 429.87it/s] 41%|████      | 21711/53136 [00:31<01:15, 414.39it/s] 41%|████      | 21770/53136 [00:32<01:10, 448.02it/s] 41%|████      | 21821/53136 [00:32<01:10, 446.20it/s] 41%|████      | 21896/53136 [00:32<01:00, 514.64it/s] 41%|████▏     | 21966/53136 [00:32<00:55, 557.01it/s] 41%|████▏     | 22026/53136 [00:32<00:55, 562.80it/s] 42%|████▏     | 22094/53136 [00:32<00:52, 593.36it/s] 42%|████▏     | 22156/53136 [00:32<00:55, 558.01it/s] 42%|████▏     | 22218/53136 [00:32<00:53, 572.84it/s] 42%|████▏     | 22277/53136 [00:32<00:55, 551.66it/s] 42%|████▏     | 22334/53136 [00:33<01:01, 504.50it/s] 42%|████▏     | 22414/53136 [00:33<00:52, 580.18it/s] 42%|████▏     | 22474/53136 [00:33<00:56, 540.28it/s] 42%|████▏     | 22540/53136 [00:33<00:53, 571.27it/s] 43%|████▎     | 22627/53136 [00:33<00:46, 652.20it/s] 43%|████▎     | 22720/53136 [00:33<00:41, 729.71it/s] 43%|████▎     | 22803/53136 [00:33<00:40, 758.03it/s] 43%|████▎     | 22881/53136 [00:33<00:46, 648.42it/s] 43%|████▎     | 22966/53136 [00:33<00:44, 682.67it/s] 43%|████▎     | 23038/53136 [00:34<00:47, 637.48it/s] 44%|████▎     | 23139/53136 [00:34<00:41, 731.28it/s] 44%|████▎     | 23216/53136 [00:34<00:41, 727.15it/s] 44%|████▍     | 23301/53136 [00:34<00:48, 618.17it/s] 44%|████▍     | 23387/53136 [00:34<00:44, 666.30it/s] 44%|████▍     | 23458/53136 [00:34<00:44, 670.91it/s] 44%|████▍     | 23531/53136 [00:34<00:43, 685.84it/s] 44%|████▍     | 23602/53136 [00:34<00:51, 576.89it/s] 45%|████▍     | 23668/53136 [00:35<00:49, 597.04it/s] 45%|████▍     | 23775/53136 [00:35<00:40, 719.16it/s] 45%|████▍     | 23852/53136 [00:35<00:48, 604.55it/s] 45%|████▌     | 23930/53136 [00:35<00:45, 646.00it/s] 45%|████▌     | 24000/53136 [00:35<00:45, 647.04it/s] 45%|████▌     | 24090/53136 [00:35<00:40, 713.36it/s] 46%|████▌     | 24179/53136 [00:35<00:38, 754.59it/s] 46%|████▌     | 24261/53136 [00:35<00:37, 767.50it/s] 46%|████▌     | 24341/53136 [00:35<00:37, 776.56it/s] 46%|████▌     | 24421/53136 [00:36<00:37, 767.84it/s] 46%|████▌     | 24517/53136 [00:36<00:34, 821.50it/s] 46%|████▋     | 24613/53136 [00:36<00:33, 857.57it/s] 46%|████▋     | 24700/53136 [00:36<00:36, 776.44it/s] 47%|████▋     | 24780/53136 [00:36<00:39, 725.34it/s] 47%|████▋     | 24872/53136 [00:36<00:36, 776.63it/s] 47%|████▋     | 24952/53136 [00:36<00:38, 737.36it/s] 47%|████▋     | 25048/53136 [00:36<00:35, 796.51it/s] 47%|████▋     | 25130/53136 [00:37<00:38, 723.72it/s] 47%|████▋     | 25205/53136 [00:37<00:44, 626.83it/s] 48%|████▊     | 25272/53136 [00:37<00:50, 555.36it/s] 48%|████▊     | 25371/53136 [00:37<00:42, 651.20it/s] 48%|████▊     | 25441/53136 [00:37<00:41, 662.70it/s] 48%|████▊     | 25511/53136 [00:37<00:43, 641.15it/s] 48%|████▊     | 25586/53136 [00:37<00:41, 660.77it/s] 48%|████▊     | 25664/53136 [00:37<00:39, 691.39it/s] 48%|████▊     | 25740/53136 [00:38<00:41, 658.85it/s] 49%|████▊     | 25827/53136 [00:38<00:38, 707.46it/s] 49%|████▉     | 25904/53136 [00:38<00:37, 723.63it/s] 49%|████▉     | 25990/53136 [00:38<00:35, 759.75it/s] 49%|████▉     | 26077/53136 [00:38<00:34, 791.21it/s] 49%|████▉     | 26157/53136 [00:38<00:46, 581.52it/s] 49%|████▉     | 26224/53136 [00:38<00:44, 601.42it/s] 49%|████▉     | 26291/53136 [00:38<00:44, 598.33it/s] 50%|████▉     | 26360/53136 [00:38<00:43, 619.79it/s] 50%|████▉     | 26448/53136 [00:39<00:38, 688.23it/s] 50%|████▉     | 26525/53136 [00:39<00:37, 701.49it/s] 50%|█████     | 26604/53136 [00:39<00:37, 716.23it/s] 50%|█████     | 26679/53136 [00:39<00:36, 719.82it/s] 50%|█████     | 26758/53136 [00:39<00:35, 738.18it/s] 51%|█████     | 26844/53136 [00:39<00:34, 769.41it/s] 51%|█████     | 26922/53136 [00:39<00:34, 759.90it/s] 51%|█████     | 27008/53136 [00:39<00:33, 787.95it/s] 51%|█████     | 27119/53136 [00:39<00:30, 852.63it/s] 51%|█████     | 27205/53136 [00:40<00:33, 772.77it/s] 51%|█████▏    | 27286/53136 [00:40<00:33, 779.58it/s] 51%|█████▏    | 27365/53136 [00:40<00:35, 722.44it/s] 52%|█████▏    | 27462/53136 [00:40<00:32, 788.39it/s] 52%|█████▏    | 27543/53136 [00:40<00:36, 704.30it/s] 52%|█████▏    | 27616/53136 [00:40<00:36, 694.33it/s] 52%|█████▏    | 27688/53136 [00:40<00:36, 698.50it/s] 52%|█████▏    | 27800/53136 [00:40<00:31, 804.90it/s] 52%|█████▏    | 27882/53136 [00:40<00:32, 781.80it/s] 53%|█████▎    | 27962/53136 [00:41<00:32, 775.74it/s] 53%|█████▎    | 28041/53136 [00:41<00:34, 725.93it/s] 53%|█████▎    | 28115/53136 [00:41<00:38, 652.06it/s] 53%|█████▎    | 28211/53136 [00:41<00:34, 716.93it/s] 53%|█████▎    | 28296/53136 [00:41<00:34, 729.15it/s] 53%|█████▎    | 28381/53136 [00:41<00:32, 756.46it/s] 54%|█████▎    | 28474/53136 [00:41<00:30, 803.64it/s] 54%|█████▍    | 28582/53136 [00:41<00:28, 870.47it/s] 54%|█████▍    | 28671/53136 [00:41<00:28, 872.18it/s] 54%|█████▍    | 28759/53136 [00:42<00:29, 830.68it/s] 54%|█████▍    | 28843/53136 [00:42<00:33, 715.68it/s] 54%|█████▍    | 28951/53136 [00:42<00:29, 808.97it/s] 55%|█████▍    | 29053/53136 [00:42<00:28, 842.58it/s] 55%|█████▍    | 29141/53136 [00:42<00:28, 844.27it/s] 55%|█████▌    | 29228/53136 [00:42<00:29, 819.83it/s] 55%|█████▌    | 29312/53136 [00:42<00:35, 676.91it/s] 55%|█████▌    | 29405/53136 [00:42<00:32, 738.00it/s] 56%|█████▌    | 29514/53136 [00:43<00:28, 828.57it/s] 56%|█████▌    | 29622/53136 [00:43<00:26, 884.09it/s] 56%|█████▌    | 29715/53136 [00:43<00:27, 839.08it/s] 56%|█████▌    | 29802/53136 [00:43<00:31, 731.92it/s] 56%|█████▋    | 29902/53136 [00:43<00:29, 797.27it/s] 56%|█████▋    | 29997/53136 [00:43<00:27, 834.42it/s] 57%|█████▋    | 30084/53136 [00:43<00:33, 684.90it/s] 57%|█████▋    | 30170/53136 [00:43<00:31, 723.86it/s] 57%|█████▋    | 30262/53136 [00:43<00:29, 771.05it/s] 57%|█████▋    | 30372/53136 [00:44<00:26, 852.16it/s] 57%|█████▋    | 30462/53136 [00:44<00:31, 728.42it/s] 57%|█████▋    | 30544/53136 [00:44<00:31, 714.63it/s] 58%|█████▊    | 30620/53136 [00:44<00:33, 670.66it/s] 58%|█████▊    | 30708/53136 [00:44<00:31, 715.55it/s] 58%|█████▊    | 30793/53136 [00:44<00:30, 741.13it/s] 58%|█████▊    | 30875/53136 [00:44<00:29, 760.44it/s] 58%|█████▊    | 30965/53136 [00:44<00:27, 798.93it/s] 58%|█████▊    | 31047/53136 [00:45<00:31, 710.66it/s] 59%|█████▊    | 31121/53136 [00:45<00:31, 700.73it/s] 59%|█████▊    | 31193/53136 [00:45<00:31, 703.00it/s] 59%|█████▉    | 31307/53136 [00:45<00:26, 817.09it/s] 59%|█████▉    | 31404/53136 [00:45<00:25, 859.42it/s] 59%|█████▉    | 31492/53136 [00:45<00:25, 852.40it/s] 59%|█████▉    | 31580/53136 [00:45<00:25, 858.94it/s] 60%|█████▉    | 31667/53136 [00:45<00:25, 851.44it/s] 60%|█████▉    | 31753/53136 [00:45<00:27, 787.42it/s] 60%|█████▉    | 31852/53136 [00:46<00:25, 833.42it/s] 60%|██████    | 31937/53136 [00:46<00:27, 777.10it/s] 60%|██████    | 32017/53136 [00:46<00:28, 746.90it/s] 60%|██████    | 32113/53136 [00:46<00:26, 804.35it/s] 61%|██████    | 32208/53136 [00:46<00:25, 829.11it/s] 61%|██████    | 32292/53136 [00:46<00:27, 769.93it/s] 61%|██████    | 32371/53136 [00:46<00:27, 766.50it/s] 61%|██████    | 32450/53136 [00:46<00:26, 771.24it/s] 61%|██████    | 32528/53136 [00:46<00:29, 702.20it/s] 61%|██████▏   | 32627/53136 [00:47<00:27, 751.66it/s] 62%|██████▏   | 32704/53136 [00:47<00:27, 756.46it/s] 62%|██████▏   | 32781/53136 [00:47<00:26, 754.36it/s] 62%|██████▏   | 32885/53136 [00:47<00:24, 834.85it/s] 62%|██████▏   | 32970/53136 [00:47<00:24, 807.58it/s] 62%|██████▏   | 33055/53136 [00:47<00:24, 818.30it/s] 62%|██████▏   | 33138/53136 [00:47<00:25, 786.80it/s] 63%|██████▎   | 33218/53136 [00:47<00:26, 763.48it/s] 63%|██████▎   | 33295/53136 [00:47<00:31, 620.50it/s] 63%|██████▎   | 33362/53136 [00:48<00:32, 599.97it/s] 63%|██████▎   | 33427/53136 [00:48<00:32, 609.98it/s] 63%|██████▎   | 33519/53136 [00:48<00:28, 690.05it/s] 63%|██████▎   | 33591/53136 [00:48<00:35, 558.36it/s] 63%|██████▎   | 33653/53136 [00:48<00:40, 480.59it/s] 63%|██████▎   | 33724/53136 [00:48<00:36, 529.38it/s] 64%|██████▎   | 33783/53136 [00:48<00:38, 501.55it/s] 64%|██████▎   | 33863/53136 [00:49<00:33, 572.81it/s] 64%|██████▍   | 33944/53136 [00:49<00:30, 630.96it/s] 64%|██████▍   | 34030/53136 [00:49<00:27, 690.21it/s] 64%|██████▍   | 34105/53136 [00:49<00:27, 686.58it/s] 64%|██████▍   | 34212/53136 [00:49<00:24, 784.83it/s] 65%|██████▍   | 34293/53136 [00:49<00:24, 765.98it/s] 65%|██████▍   | 34372/53136 [00:49<00:31, 602.55it/s] 65%|██████▍   | 34450/53136 [00:49<00:29, 630.52it/s] 65%|██████▍   | 34528/53136 [00:49<00:27, 665.76it/s] 65%|██████▌   | 34607/53136 [00:50<00:26, 696.96it/s] 65%|██████▌   | 34690/53136 [00:50<00:25, 731.71it/s] 65%|██████▌   | 34777/53136 [00:50<00:25, 731.81it/s] 66%|██████▌   | 34853/53136 [00:50<00:30, 607.37it/s] 66%|██████▌   | 34922/53136 [00:50<00:29, 624.95it/s] 66%|██████▌   | 34989/53136 [00:50<00:29, 623.91it/s] 66%|██████▌   | 35054/53136 [00:50<00:35, 506.19it/s] 66%|██████▌   | 35132/53136 [00:50<00:31, 569.75it/s] 66%|██████▌   | 35195/53136 [00:51<00:31, 563.58it/s] 66%|██████▋   | 35273/53136 [00:51<00:28, 618.07it/s] 67%|██████▋   | 35342/53136 [00:51<00:27, 636.59it/s] 67%|██████▋   | 35425/53136 [00:51<00:25, 689.29it/s] 67%|██████▋   | 35497/53136 [00:51<00:30, 579.11it/s] 67%|██████▋   | 35592/53136 [00:51<00:26, 670.44it/s] 67%|██████▋   | 35672/53136 [00:51<00:25, 696.17it/s] 67%|██████▋   | 35759/53136 [00:51<00:23, 739.96it/s] 67%|██████▋   | 35849/53136 [00:51<00:22, 783.87it/s] 68%|██████▊   | 35930/53136 [00:52<00:26, 646.15it/s] 68%|██████▊   | 36001/53136 [00:52<00:27, 623.13it/s] 68%|██████▊   | 36087/53136 [00:52<00:24, 682.41it/s] 68%|██████▊   | 36160/53136 [00:52<00:26, 644.75it/s] 68%|██████▊   | 36228/53136 [00:52<00:32, 523.00it/s] 68%|██████▊   | 36291/53136 [00:52<00:30, 546.14it/s] 68%|██████▊   | 36373/53136 [00:52<00:27, 610.72it/s] 69%|██████▊   | 36443/53136 [00:53<00:26, 630.97it/s] 69%|██████▊   | 36525/53136 [00:53<00:24, 679.54it/s] 69%|██████▉   | 36596/53136 [00:53<00:28, 582.91it/s] 69%|██████▉   | 36663/53136 [00:53<00:27, 602.06it/s] 69%|██████▉   | 36727/53136 [00:53<00:27, 605.32it/s] 69%|██████▉   | 36790/53136 [00:53<00:28, 579.69it/s] 69%|██████▉   | 36855/53136 [00:53<00:27, 597.80it/s] 70%|██████▉   | 36931/53136 [00:53<00:25, 642.59it/s] 70%|██████▉   | 37002/53136 [00:53<00:24, 660.52it/s] 70%|██████▉   | 37070/53136 [00:54<00:27, 579.81it/s] 70%|██████▉   | 37131/53136 [00:54<00:32, 489.11it/s] 70%|██████▉   | 37185/53136 [00:54<00:31, 501.11it/s] 70%|███████   | 37239/53136 [00:54<00:31, 506.49it/s] 70%|███████   | 37292/53136 [00:54<00:30, 511.48it/s] 70%|███████   | 37379/53136 [00:54<00:25, 609.59it/s] 70%|███████   | 37449/53136 [00:54<00:24, 634.12it/s] 71%|███████   | 37535/53136 [00:54<00:22, 694.10it/s] 71%|███████   | 37606/53136 [00:54<00:22, 678.77it/s] 71%|███████   | 37675/53136 [00:55<00:25, 612.77it/s] 71%|███████   | 37739/53136 [00:55<00:27, 561.16it/s] 71%|███████   | 37797/53136 [00:55<00:27, 565.93it/s] 71%|███████   | 37855/53136 [00:55<00:30, 509.21it/s] 71%|███████▏  | 37908/53136 [00:55<00:31, 477.53it/s] 71%|███████▏  | 37971/53136 [00:55<00:29, 516.10it/s] 72%|███████▏  | 38025/53136 [00:55<00:31, 482.72it/s] 72%|███████▏  | 38082/53136 [00:55<00:29, 504.04it/s] 72%|███████▏  | 38144/53136 [00:56<00:28, 532.62it/s] 72%|███████▏  | 38199/53136 [00:56<00:41, 363.88it/s] 72%|███████▏  | 38247/53136 [00:56<00:38, 386.60it/s] 72%|███████▏  | 38292/53136 [00:56<00:37, 398.60it/s] 72%|███████▏  | 38337/53136 [00:56<00:36, 409.92it/s] 72%|███████▏  | 38389/53136 [00:56<00:33, 436.07it/s] 72%|███████▏  | 38436/53136 [00:56<00:36, 403.04it/s] 72%|███████▏  | 38492/53136 [00:56<00:33, 440.45it/s] 73%|███████▎  | 38543/53136 [00:57<00:31, 458.98it/s] 73%|███████▎  | 38591/53136 [00:57<00:32, 442.44it/s] 73%|███████▎  | 38648/53136 [00:57<00:30, 476.30it/s] 73%|███████▎  | 38735/53136 [00:57<00:24, 583.92it/s] 73%|███████▎  | 38795/53136 [00:57<00:26, 537.29it/s] 73%|███████▎  | 38873/53136 [00:57<00:23, 602.74it/s] 73%|███████▎  | 38972/53136 [00:57<00:19, 709.00it/s] 74%|███████▎  | 39055/53136 [00:57<00:19, 731.91it/s] 74%|███████▎  | 39136/53136 [00:57<00:18, 752.95it/s] 74%|███████▍  | 39237/53136 [00:58<00:16, 826.20it/s] 74%|███████▍  | 39327/53136 [00:58<00:16, 847.21it/s] 74%|███████▍  | 39413/53136 [00:58<00:16, 816.47it/s] 74%|███████▍  | 39496/53136 [00:58<00:19, 682.49it/s] 74%|███████▍  | 39569/53136 [00:58<00:21, 630.63it/s] 75%|███████▍  | 39643/53136 [00:58<00:20, 657.01it/s] 75%|███████▍  | 39727/53136 [00:58<00:20, 657.59it/s] 75%|███████▍  | 39796/53136 [00:58<00:20, 663.28it/s] 75%|███████▌  | 39902/53136 [00:58<00:17, 768.86it/s] 75%|███████▌  | 39983/53136 [00:59<00:16, 776.30it/s] 75%|███████▌  | 40063/53136 [00:59<00:16, 778.64it/s] 76%|███████▌  | 40144/53136 [00:59<00:16, 786.35it/s] 76%|███████▌  | 40252/53136 [00:59<00:14, 870.89it/s] 76%|███████▌  | 40340/53136 [00:59<00:16, 787.69it/s] 76%|███████▌  | 40421/53136 [00:59<00:18, 696.86it/s] 76%|███████▌  | 40511/53136 [00:59<00:16, 746.68it/s] 76%|███████▋  | 40604/53136 [00:59<00:15, 793.74it/s] 77%|███████▋  | 40701/53136 [00:59<00:14, 838.12it/s] 77%|███████▋  | 40787/53136 [01:00<00:15, 778.53it/s] 77%|███████▋  | 40868/53136 [01:00<00:15, 767.28it/s] 77%|███████▋  | 40975/53136 [01:00<00:14, 848.35it/s] 77%|███████▋  | 41074/53136 [01:00<00:13, 867.37it/s] 77%|███████▋  | 41162/53136 [01:00<00:16, 731.98it/s] 78%|███████▊  | 41240/53136 [01:00<00:17, 690.24it/s] 78%|███████▊  | 41323/53136 [01:00<00:16, 723.34it/s] 78%|███████▊  | 41409/53136 [01:00<00:15, 758.35it/s] 78%|███████▊  | 41488/53136 [01:01<00:16, 726.92it/s] 78%|███████▊  | 41592/53136 [01:01<00:14, 811.19it/s] 78%|███████▊  | 41676/53136 [01:01<00:14, 784.23it/s] 79%|███████▊  | 41757/53136 [01:01<00:14, 774.73it/s] 79%|███████▉  | 41850/53136 [01:01<00:13, 814.07it/s] 79%|███████▉  | 41933/53136 [01:01<00:14, 779.41it/s] 79%|███████▉  | 42012/53136 [01:01<00:15, 701.19it/s] 79%|███████▉  | 42093/53136 [01:01<00:15, 728.82it/s] 79%|███████▉  | 42168/53136 [01:01<00:14, 733.46it/s] 80%|███████▉  | 42254/53136 [01:02<00:14, 766.65it/s] 80%|███████▉  | 42332/53136 [01:02<00:14, 767.47it/s] 80%|███████▉  | 42442/53136 [01:02<00:12, 863.00it/s] 80%|████████  | 42530/53136 [01:02<00:12, 861.87it/s] 80%|████████  | 42617/53136 [01:02<00:12, 858.61it/s] 80%|████████  | 42712/53136 [01:02<00:11, 885.19it/s] 81%|████████  | 42810/53136 [01:02<00:11, 913.07it/s] 81%|████████  | 42902/53136 [01:02<00:12, 816.82it/s] 81%|████████  | 42986/53136 [01:02<00:12, 818.68it/s] 81%|████████  | 43070/53136 [01:02<00:12, 791.05it/s] 81%|████████  | 43151/53136 [01:03<00:14, 695.67it/s] 81%|████████▏ | 43224/53136 [01:03<00:15, 654.18it/s] 82%|████████▏ | 43319/53136 [01:03<00:13, 727.92it/s] 82%|████████▏ | 43395/53136 [01:03<00:13, 715.38it/s] 82%|████████▏ | 43472/53136 [01:03<00:13, 698.13it/s] 82%|████████▏ | 43544/53136 [01:03<00:13, 690.10it/s] 82%|████████▏ | 43614/53136 [01:03<00:14, 653.63it/s] 82%|████████▏ | 43689/53136 [01:03<00:13, 678.94it/s] 82%|████████▏ | 43758/53136 [01:04<00:15, 621.80it/s] 83%|████████▎ | 43845/53136 [01:04<00:13, 687.01it/s] 83%|████████▎ | 43917/53136 [01:04<00:13, 682.67it/s] 83%|████████▎ | 43993/53136 [01:04<00:13, 701.57it/s] 83%|████████▎ | 44073/53136 [01:04<00:13, 670.80it/s] 83%|████████▎ | 44142/53136 [01:04<00:15, 597.37it/s] 83%|████████▎ | 44224/53136 [01:04<00:13, 650.78it/s] 83%|████████▎ | 44316/53136 [01:04<00:12, 719.32it/s] 84%|████████▎ | 44391/53136 [01:04<00:13, 653.41it/s] 84%|████████▎ | 44459/53136 [01:05<00:18, 475.73it/s] 84%|████████▍ | 44515/53136 [01:05<00:17, 488.21it/s] 84%|████████▍ | 44572/53136 [01:05<00:16, 507.01it/s] 84%|████████▍ | 44641/53136 [01:05<00:15, 552.22it/s] 84%|████████▍ | 44701/53136 [01:05<00:15, 557.18it/s] 84%|████████▍ | 44760/53136 [01:05<00:15, 526.71it/s] 84%|████████▍ | 44824/53136 [01:05<00:14, 555.31it/s] 84%|████████▍ | 44882/53136 [01:05<00:14, 556.68it/s] 85%|████████▍ | 44940/53136 [01:06<00:14, 560.08it/s] 85%|████████▍ | 44998/53136 [01:06<00:14, 547.82it/s] 85%|████████▍ | 45056/53136 [01:06<00:14, 555.48it/s] 85%|████████▍ | 45153/53136 [01:06<00:11, 671.90it/s] 85%|████████▌ | 45236/53136 [01:06<00:11, 716.99it/s] 85%|████████▌ | 45309/53136 [01:06<00:11, 697.20it/s] 85%|████████▌ | 45380/53136 [01:06<00:11, 677.83it/s] 86%|████████▌ | 45452/53136 [01:06<00:11, 680.25it/s] 86%|████████▌ | 45535/53136 [01:06<00:10, 707.52it/s] 86%|████████▌ | 45606/53136 [01:07<00:13, 578.14it/s] 86%|████████▌ | 45668/53136 [01:07<00:13, 572.35it/s] 86%|████████▌ | 45734/53136 [01:07<00:12, 591.64it/s] 86%|████████▌ | 45810/53136 [01:07<00:11, 634.20it/s] 86%|████████▋ | 45899/53136 [01:07<00:10, 704.17it/s] 87%|████████▋ | 45972/53136 [01:07<00:10, 695.73it/s] 87%|████████▋ | 46043/53136 [01:07<00:12, 586.28it/s] 87%|████████▋ | 46111/53136 [01:07<00:11, 609.77it/s] 87%|████████▋ | 46175/53136 [01:08<00:12, 572.84it/s] 87%|████████▋ | 46235/53136 [01:08<00:14, 477.37it/s] 87%|████████▋ | 46287/53136 [01:08<00:15, 444.06it/s] 87%|████████▋ | 46344/53136 [01:08<00:14, 470.47it/s] 87%|████████▋ | 46406/53136 [01:08<00:13, 507.06it/s] 87%|████████▋ | 46462/53136 [01:08<00:12, 520.92it/s] 88%|████████▊ | 46520/53136 [01:08<00:12, 534.98it/s] 88%|████████▊ | 46576/53136 [01:08<00:12, 537.40it/s] 88%|████████▊ | 46631/53136 [01:09<00:13, 496.16it/s] 88%|████████▊ | 46684/53136 [01:09<00:12, 504.83it/s] 88%|████████▊ | 46736/53136 [01:09<00:14, 438.58it/s] 88%|████████▊ | 46820/53136 [01:09<00:11, 538.78it/s] 88%|████████▊ | 46877/53136 [01:09<00:12, 498.59it/s] 88%|████████▊ | 46953/53136 [01:09<00:10, 563.63it/s] 89%|████████▊ | 47029/53136 [01:09<00:09, 613.13it/s] 89%|████████▊ | 47101/53136 [01:09<00:09, 641.50it/s] 89%|████████▉ | 47189/53136 [01:09<00:08, 706.72it/s] 89%|████████▉ | 47262/53136 [01:10<00:09, 647.97it/s] 89%|████████▉ | 47344/53136 [01:10<00:08, 676.21it/s] 89%|████████▉ | 47414/53136 [01:10<00:08, 673.93it/s] 89%|████████▉ | 47483/53136 [01:10<00:09, 610.07it/s] 89%|████████▉ | 47546/53136 [01:10<00:09, 614.27it/s] 90%|████████▉ | 47629/53136 [01:10<00:08, 669.15it/s] 90%|████████▉ | 47698/53136 [01:10<00:08, 663.60it/s] 90%|████████▉ | 47779/53136 [01:10<00:07, 703.56it/s] 90%|█████████ | 47851/53136 [01:11<00:10, 513.88it/s] 90%|█████████ | 47911/53136 [01:11<00:11, 474.63it/s] 90%|█████████ | 47976/53136 [01:11<00:10, 513.77it/s] 90%|█████████ | 48033/53136 [01:11<00:10, 504.47it/s] 90%|█████████ | 48088/53136 [01:11<00:11, 455.42it/s] 91%|█████████ | 48137/53136 [01:11<00:11, 453.81it/s] 91%|█████████ | 48185/53136 [01:11<00:11, 440.71it/s] 91%|█████████ | 48251/53136 [01:11<00:10, 478.77it/s] 91%|█████████ | 48303/53136 [01:12<00:09, 488.52it/s] 91%|█████████ | 48353/53136 [01:12<00:10, 458.74it/s] 91%|█████████ | 48409/53136 [01:12<00:09, 478.86it/s] 91%|█████████ | 48458/53136 [01:12<00:09, 479.74it/s] 91%|█████████▏| 48507/53136 [01:12<00:11, 411.64it/s] 91%|█████████▏| 48558/53136 [01:12<00:10, 432.71it/s] 91%|█████████▏| 48603/53136 [01:12<00:10, 432.63it/s] 92%|█████████▏| 48670/53136 [01:12<00:08, 496.97it/s] 92%|█████████▏| 48722/53136 [01:12<00:09, 465.65it/s] 92%|█████████▏| 48770/53136 [01:13<00:12, 345.02it/s] 92%|█████████▏| 48810/53136 [01:13<00:13, 318.91it/s] 92%|█████████▏| 48858/53136 [01:13<00:12, 352.98it/s] 92%|█████████▏| 48912/53136 [01:13<00:10, 396.67it/s] 92%|█████████▏| 48975/53136 [01:13<00:09, 454.61it/s] 92%|█████████▏| 49032/53136 [01:13<00:08, 481.62it/s] 92%|█████████▏| 49083/53136 [01:13<00:09, 439.33it/s] 93%|█████████▎| 49154/53136 [01:13<00:07, 507.50it/s] 93%|█████████▎| 49208/53136 [01:14<00:09, 430.61it/s] 93%|█████████▎| 49255/53136 [01:14<00:09, 397.53it/s] 93%|█████████▎| 49298/53136 [01:14<00:10, 377.36it/s] 93%|█████████▎| 49344/53136 [01:14<00:09, 396.49it/s] 93%|█████████▎| 49396/53136 [01:14<00:08, 427.95it/s] 93%|█████████▎| 49451/53136 [01:14<00:08, 454.95it/s] 93%|█████████▎| 49498/53136 [01:14<00:08, 453.98it/s] 93%|█████████▎| 49549/53136 [01:14<00:07, 468.34it/s] 93%|█████████▎| 49600/53136 [01:15<00:07, 477.59it/s] 93%|█████████▎| 49649/53136 [01:15<00:08, 407.44it/s] 94%|█████████▎| 49702/53136 [01:15<00:07, 438.85it/s] 94%|█████████▎| 49751/53136 [01:15<00:07, 447.66it/s] 94%|█████████▎| 49798/53136 [01:15<00:07, 440.56it/s] 94%|█████████▍| 49865/53136 [01:15<00:06, 503.37it/s] 94%|█████████▍| 49917/53136 [01:15<00:06, 504.16it/s] 94%|█████████▍| 49977/53136 [01:15<00:05, 529.16it/s] 94%|█████████▍| 50034/53136 [01:15<00:05, 538.48it/s] 94%|█████████▍| 50118/53136 [01:16<00:04, 625.92it/s] 94%|█████████▍| 50184/53136 [01:16<00:04, 609.69it/s] 95%|█████████▍| 50273/53136 [01:16<00:04, 689.08it/s] 95%|█████████▍| 50343/53136 [01:16<00:04, 661.21it/s] 95%|█████████▍| 50437/53136 [01:16<00:03, 739.86it/s] 95%|█████████▌| 50513/53136 [01:16<00:03, 743.88it/s] 95%|█████████▌| 50589/53136 [01:16<00:04, 551.97it/s] 95%|█████████▌| 50675/53136 [01:16<00:03, 622.79it/s] 96%|█████████▌| 50777/53136 [01:16<00:03, 722.18it/s] 96%|█████████▌| 50857/53136 [01:17<00:03, 708.46it/s] 96%|█████████▌| 50937/53136 [01:17<00:03, 728.47it/s] 96%|█████████▌| 51014/53136 [01:17<00:02, 711.32it/s] 96%|█████████▌| 51088/53136 [01:17<00:03, 632.22it/s] 96%|█████████▋| 51196/53136 [01:17<00:02, 745.17it/s] 96%|█████████▋| 51275/53136 [01:17<00:02, 753.41it/s] 97%|█████████▋| 51354/53136 [01:17<00:02, 761.25it/s] 97%|█████████▋| 51433/53136 [01:17<00:02, 623.14it/s] 97%|█████████▋| 51509/53136 [01:18<00:02, 650.25it/s] 97%|█████████▋| 51579/53136 [01:18<00:02, 625.70it/s] 97%|█████████▋| 51645/53136 [01:18<00:02, 502.22it/s] 97%|█████████▋| 51747/53136 [01:18<00:02, 618.17it/s] 98%|█████████▊| 51831/53136 [01:18<00:01, 670.66it/s] 98%|█████████▊| 51911/53136 [01:18<00:01, 703.22it/s] 98%|█████████▊| 52016/53136 [01:18<00:01, 792.66it/s] 98%|█████████▊| 52100/53136 [01:18<00:01, 776.79it/s] 98%|█████████▊| 52181/53136 [01:19<00:01, 684.84it/s] 98%|█████████▊| 52254/53136 [01:19<00:01, 503.60it/s] 98%|█████████▊| 52337/53136 [01:19<00:01, 571.35it/s] 99%|█████████▊| 52430/53136 [01:19<00:01, 648.81it/s] 99%|█████████▉| 52504/53136 [01:19<00:01, 582.87it/s] 99%|█████████▉| 52587/53136 [01:19<00:00, 632.23it/s] 99%|█████████▉| 52657/53136 [01:19<00:00, 631.97it/s] 99%|█████████▉| 52748/53136 [01:19<00:00, 702.27it/s] 99%|█████████▉| 52852/53136 [01:20<00:00, 792.22it/s]100%|█████████▉| 52936/53136 [01:20<00:00, 742.27it/s]100%|█████████▉| 53014/53136 [01:20<00:00, 733.92it/s]100%|█████████▉| 53090/53136 [01:20<00:00, 618.55it/s]100%|██████████| 53136/53136 [01:20<00:00, 659.57it/s]

gathering stats for n=1
  0%|          | 0/53136 [00:00<?, ?it/s]  0%|          | 198/53136 [00:00<00:26, 1969.97it/s]  1%|          | 417/53136 [00:00<00:25, 2098.64it/s]  1%|          | 640/53136 [00:00<00:24, 2155.32it/s]  2%|▏         | 856/53136 [00:00<00:26, 1976.24it/s]  2%|▏         | 1056/53136 [00:00<00:26, 1940.01it/s]  2%|▏         | 1261/53136 [00:00<00:26, 1973.38it/s]  3%|▎         | 1465/53136 [00:00<00:25, 1994.27it/s]  3%|▎         | 1706/53136 [00:00<00:24, 2122.59it/s]  4%|▎         | 1920/53136 [00:00<00:24, 2098.42it/s]  4%|▍         | 2131/53136 [00:01<00:24, 2066.78it/s]  4%|▍         | 2353/53136 [00:01<00:24, 2107.36it/s]  5%|▍         | 2565/53136 [00:01<00:25, 1964.61it/s]  5%|▌         | 2899/53136 [00:01<00:21, 2333.10it/s]  6%|▌         | 3188/53136 [00:01<00:20, 2442.30it/s]  7%|▋         | 3525/53136 [00:01<00:18, 2706.81it/s]  7%|▋         | 3815/53136 [00:01<00:17, 2762.70it/s]  8%|▊         | 4155/53136 [00:01<00:16, 2945.05it/s]  8%|▊         | 4469/53136 [00:01<00:16, 3001.73it/s]  9%|▉         | 4791/53136 [00:01<00:16, 2965.27it/s] 10%|▉         | 5089/53136 [00:02<00:17, 2825.52it/s] 10%|█         | 5374/53136 [00:02<00:22, 2158.42it/s] 11%|█         | 5614/53136 [00:02<00:23, 2046.49it/s] 11%|█         | 5892/53136 [00:02<00:21, 2219.95it/s] 12%|█▏        | 6215/53136 [00:02<00:18, 2474.70it/s] 12%|█▏        | 6557/53136 [00:02<00:17, 2722.66it/s] 13%|█▎        | 6879/53136 [00:02<00:16, 2859.22it/s] 14%|█▎        | 7177/53136 [00:02<00:17, 2605.59it/s] 14%|█▍        | 7450/53136 [00:03<00:18, 2529.39it/s] 15%|█▍        | 7750/53136 [00:03<00:17, 2654.37it/s] 15%|█▌        | 8076/53136 [00:03<00:16, 2813.74it/s] 16%|█▌        | 8364/53136 [00:03<00:16, 2782.15it/s] 16%|█▋        | 8647/53136 [00:03<00:16, 2766.20it/s] 17%|█▋        | 8950/53136 [00:03<00:15, 2835.04it/s] 17%|█▋        | 9263/53136 [00:03<00:15, 2875.09it/s] 18%|█▊        | 9632/53136 [00:03<00:13, 3110.82it/s] 19%|█▊        | 9946/53136 [00:03<00:15, 2806.02it/s] 19%|█▉        | 10252/53136 [00:04<00:14, 2874.20it/s] 20%|█▉        | 10545/53136 [00:04<00:15, 2787.62it/s] 20%|██        | 10828/53136 [00:04<00:15, 2789.53it/s] 21%|██        | 11128/53136 [00:04<00:14, 2848.39it/s] 21%|██▏       | 11416/53136 [00:04<00:14, 2796.17it/s] 22%|██▏       | 11725/53136 [00:04<00:14, 2880.17it/s] 23%|██▎       | 12051/53136 [00:04<00:13, 2990.44it/s] 23%|██▎       | 12352/53136 [00:04<00:16, 2499.20it/s] 24%|██▎       | 12617/53136 [00:05<00:19, 2105.90it/s] 24%|██▍       | 12847/53136 [00:05<00:20, 2003.08it/s] 25%|██▍       | 13060/53136 [00:05<00:19, 2029.16it/s] 25%|██▍       | 13273/53136 [00:05<00:19, 1994.98it/s] 25%|██▌       | 13479/53136 [00:05<00:19, 1997.81it/s] 26%|██▌       | 13684/53136 [00:05<00:20, 1937.48it/s] 26%|██▌       | 13881/53136 [00:05<00:20, 1878.19it/s] 26%|██▋       | 14071/53136 [00:05<00:21, 1814.04it/s] 27%|██▋       | 14254/53136 [00:05<00:21, 1796.89it/s] 27%|██▋       | 14435/53136 [00:06<00:22, 1729.81it/s] 28%|██▊       | 14663/53136 [00:06<00:20, 1880.44it/s] 28%|██▊       | 14868/53136 [00:06<00:19, 1926.76it/s] 28%|██▊       | 15063/53136 [00:06<00:20, 1866.57it/s] 29%|██▊       | 15251/53136 [00:06<00:21, 1798.86it/s] 29%|██▉       | 15433/53136 [00:06<00:22, 1696.52it/s] 29%|██▉       | 15605/53136 [00:06<00:22, 1700.61it/s] 30%|██▉       | 15799/53136 [00:06<00:21, 1767.78it/s] 30%|███       | 16114/53136 [00:06<00:17, 2162.24it/s] 31%|███       | 16412/53136 [00:06<00:15, 2398.95it/s] 32%|███▏      | 16767/53136 [00:07<00:13, 2735.84it/s] 32%|███▏      | 17044/53136 [00:07<00:13, 2668.44it/s] 33%|███▎      | 17314/53136 [00:07<00:15, 2385.94it/s] 33%|███▎      | 17560/53136 [00:07<00:16, 2207.46it/s] 33%|███▎      | 17788/53136 [00:07<00:16, 2163.18it/s] 34%|███▍      | 18009/53136 [00:07<00:17, 2008.14it/s] 34%|███▍      | 18214/53136 [00:07<00:17, 1946.48it/s] 35%|███▍      | 18412/53136 [00:07<00:19, 1808.84it/s] 35%|███▍      | 18596/53136 [00:08<00:19, 1736.06it/s] 35%|███▌      | 18772/53136 [00:08<00:19, 1740.20it/s] 36%|███▌      | 18983/53136 [00:08<00:18, 1839.46it/s] 36%|███▌      | 19189/53136 [00:08<00:17, 1900.91it/s] 36%|███▋      | 19381/53136 [00:08<00:17, 1889.44it/s] 37%|███▋      | 19572/53136 [00:08<00:18, 1803.52it/s] 37%|███▋      | 19761/53136 [00:08<00:18, 1815.45it/s] 38%|███▊      | 19948/53136 [00:08<00:18, 1830.16it/s] 38%|███▊      | 20143/53136 [00:08<00:17, 1864.67it/s] 38%|███▊      | 20331/53136 [00:09<00:18, 1752.51it/s] 39%|███▊      | 20517/53136 [00:09<00:18, 1781.64it/s] 39%|███▉      | 20697/53136 [00:09<00:18, 1717.94it/s] 39%|███▉      | 20891/53136 [00:09<00:18, 1757.04it/s] 40%|███▉      | 21086/53136 [00:09<00:17, 1806.68it/s] 40%|████      | 21268/53136 [00:09<00:18, 1763.79it/s] 40%|████      | 21501/53136 [00:09<00:16, 1925.09it/s] 41%|████      | 21736/53136 [00:09<00:15, 2045.97it/s] 42%|████▏     | 22057/53136 [00:09<00:13, 2386.71it/s] 42%|████▏     | 22386/53136 [00:09<00:11, 2645.94it/s] 43%|████▎     | 22740/53136 [00:10<00:10, 2904.97it/s] 43%|████▎     | 23058/53136 [00:10<00:10, 2984.66it/s] 44%|████▍     | 23404/53136 [00:10<00:09, 3118.44it/s] 45%|████▍     | 23717/53136 [00:10<00:09, 3033.69it/s] 45%|████▌     | 24057/53136 [00:10<00:09, 3140.27it/s] 46%|████▌     | 24374/53136 [00:10<00:09, 3145.76it/s] 47%|████▋     | 24739/53136 [00:10<00:08, 3273.01it/s] 47%|████▋     | 25080/53136 [00:10<00:08, 3308.61it/s] 48%|████▊     | 25412/53136 [00:10<00:08, 3263.04it/s] 48%|████▊     | 25739/53136 [00:10<00:08, 3206.29it/s] 49%|████▉     | 26078/53136 [00:11<00:08, 3206.69it/s] 50%|████▉     | 26399/53136 [00:11<00:08, 3044.86it/s] 50%|█████     | 26706/53136 [00:11<00:08, 3033.66it/s] 51%|█████     | 27039/53136 [00:11<00:08, 3114.87it/s] 51%|█████▏    | 27352/53136 [00:11<00:08, 3058.06it/s] 52%|█████▏    | 27659/53136 [00:11<00:08, 3029.41it/s] 53%|█████▎    | 28008/53136 [00:11<00:07, 3162.79it/s] 53%|█████▎    | 28326/53136 [00:11<00:07, 3108.62it/s] 54%|█████▍    | 28711/53136 [00:11<00:07, 3322.53it/s] 55%|█████▍    | 29059/53136 [00:12<00:07, 3367.62it/s] 55%|█████▌    | 29397/53136 [00:12<00:07, 3233.03it/s] 56%|█████▌    | 29727/53136 [00:12<00:07, 3248.33it/s] 57%|█████▋    | 30054/53136 [00:12<00:07, 3024.46it/s] 57%|█████▋    | 30401/53136 [00:12<00:07, 3084.59it/s] 58%|█████▊    | 30729/53136 [00:12<00:07, 3138.82it/s] 58%|█████▊    | 31060/53136 [00:12<00:06, 3171.29it/s] 59%|█████▉    | 31422/53136 [00:12<00:06, 3299.13it/s] 60%|█████▉    | 31772/53136 [00:12<00:06, 3357.51it/s] 60%|██████    | 32110/53136 [00:12<00:06, 3307.93it/s] 61%|██████    | 32450/53136 [00:13<00:06, 3329.38it/s] 62%|██████▏   | 32784/53136 [00:13<00:06, 3316.35it/s] 62%|██████▏   | 33137/53136 [00:13<00:05, 3379.34it/s] 63%|██████▎   | 33476/53136 [00:13<00:05, 3285.63it/s] 64%|██████▎   | 33806/53136 [00:13<00:06, 3051.30it/s] 64%|██████▍   | 34167/53136 [00:13<00:05, 3205.49it/s] 65%|██████▍   | 34492/53136 [00:13<00:05, 3132.29it/s] 66%|██████▌   | 34823/53136 [00:13<00:05, 3176.54it/s] 66%|██████▌   | 35143/53136 [00:13<00:06, 2885.52it/s] 67%|██████▋   | 35448/53136 [00:14<00:06, 2902.19it/s] 67%|██████▋   | 35798/53136 [00:14<00:05, 3067.50it/s] 68%|██████▊   | 36110/53136 [00:14<00:05, 2990.14it/s] 69%|██████▊   | 36413/53136 [00:14<00:05, 2999.80it/s] 69%|██████▉   | 36716/53136 [00:14<00:05, 2929.11it/s] 70%|██████▉   | 37011/53136 [00:14<00:05, 2879.48it/s] 70%|███████   | 37301/53136 [00:14<00:05, 2821.68it/s] 71%|███████   | 37633/53136 [00:14<00:05, 2948.38it/s] 71%|███████▏  | 37966/53136 [00:14<00:04, 3057.83it/s] 72%|███████▏  | 38273/53136 [00:15<00:05, 2939.21it/s] 73%|███████▎  | 38569/53136 [00:15<00:04, 2928.24it/s] 73%|███████▎  | 38863/53136 [00:15<00:04, 2907.49it/s] 74%|███████▍  | 39223/53136 [00:15<00:04, 3106.65it/s] 74%|███████▍  | 39535/53136 [00:15<00:04, 3015.26it/s] 75%|███████▍  | 39848/53136 [00:15<00:04, 3046.66it/s] 76%|███████▌  | 40200/53136 [00:15<00:04, 3180.70it/s] 76%|███████▋  | 40520/53136 [00:15<00:03, 3180.76it/s] 77%|███████▋  | 40849/53136 [00:15<00:03, 3205.21it/s] 77%|███████▋  | 41171/53136 [00:15<00:03, 3185.10it/s] 78%|███████▊  | 41509/53136 [00:16<00:03, 3242.70it/s] 79%|███████▊  | 41837/53136 [00:16<00:03, 3253.20it/s] 79%|███████▉  | 42163/53136 [00:16<00:03, 3167.45it/s] 80%|████████  | 42526/53136 [00:16<00:03, 3298.33it/s] 81%|████████  | 42865/53136 [00:16<00:03, 3321.15it/s] 81%|████████▏ | 43198/53136 [00:16<00:03, 3170.60it/s] 82%|████████▏ | 43517/53136 [00:16<00:03, 3102.34it/s] 82%|████████▏ | 43829/53136 [00:16<00:03, 3030.95it/s] 83%|████████▎ | 44134/53136 [00:16<00:02, 3035.58it/s] 84%|████████▎ | 44439/53136 [00:17<00:02, 2911.12it/s] 84%|████████▍ | 44803/53136 [00:17<00:02, 3117.02it/s] 85%|████████▍ | 45117/53136 [00:17<00:02, 2828.08it/s] 85%|████████▌ | 45406/53136 [00:17<00:03, 2420.87it/s] 86%|████████▌ | 45661/53136 [00:17<00:03, 2153.52it/s] 86%|████████▋ | 45889/53136 [00:17<00:03, 2076.82it/s] 87%|████████▋ | 46105/53136 [00:17<00:03, 2039.40it/s] 87%|████████▋ | 46314/53136 [00:17<00:03, 1938.99it/s] 88%|████████▊ | 46537/53136 [00:18<00:03, 2003.79it/s] 88%|████████▊ | 46741/53136 [00:18<00:03, 1847.33it/s] 88%|████████▊ | 46951/53136 [00:18<00:03, 1911.38it/s] 89%|████████▊ | 47148/53136 [00:18<00:03, 1926.53it/s] 89%|████████▉ | 47344/53136 [00:18<00:03, 1858.78it/s] 89%|████████▉ | 47532/53136 [00:18<00:03, 1809.96it/s] 90%|████████▉ | 47729/53136 [00:18<00:02, 1849.90it/s] 90%|█████████ | 47916/53136 [00:18<00:03, 1695.59it/s] 91%|█████████ | 48092/53136 [00:18<00:02, 1711.55it/s] 91%|█████████ | 48274/53136 [00:19<00:02, 1741.11it/s] 91%|█████████ | 48456/53136 [00:19<00:02, 1761.15it/s] 92%|█████████▏| 48647/53136 [00:19<00:02, 1801.37it/s] 92%|█████████▏| 48829/53136 [00:19<00:02, 1592.63it/s] 92%|█████████▏| 49032/53136 [00:19<00:02, 1706.86it/s] 93%|█████████▎| 49208/53136 [00:19<00:02, 1656.97it/s] 93%|█████████▎| 49378/53136 [00:19<00:02, 1654.34it/s] 93%|█████████▎| 49561/53136 [00:19<00:02, 1702.65it/s] 94%|█████████▎| 49735/53136 [00:19<00:02, 1699.45it/s] 94%|█████████▍| 49930/53136 [00:19<00:01, 1769.05it/s] 94%|█████████▍| 50136/53136 [00:20<00:01, 1853.04it/s] 95%|█████████▍| 50323/53136 [00:20<00:01, 1819.73it/s] 95%|█████████▌| 50513/53136 [00:20<00:01, 1821.50it/s] 95%|█████████▌| 50696/53136 [00:20<00:01, 1679.30it/s] 96%|█████████▌| 50893/53136 [00:20<00:01, 1759.62it/s] 96%|█████████▌| 51072/53136 [00:20<00:01, 1739.83it/s] 97%|█████████▋| 51368/53136 [00:20<00:00, 2073.48it/s] 97%|█████████▋| 51641/53136 [00:20<00:00, 2095.73it/s] 98%|█████████▊| 51852/53136 [00:20<00:00, 2093.88it/s] 98%|█████████▊| 52063/53136 [00:21<00:00, 2030.25it/s] 98%|█████████▊| 52267/53136 [00:21<00:00, 1693.17it/s] 99%|█████████▊| 52457/53136 [00:21<00:00, 1743.29it/s] 99%|█████████▉| 52639/53136 [00:21<00:00, 1714.53it/s] 99%|█████████▉| 52831/53136 [00:21<00:00, 1768.74it/s]100%|█████████▉| 53012/53136 [00:21<00:00, 1718.62it/s]100%|██████████| 53136/53136 [00:21<00:00, 2441.83it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 49.97it/s]2022-03-15 16:44:01 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(35920, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=35920, bias=False)
  )
)
2022-03-15 16:44:01 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-15 16:44:01 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-15 16:44:01 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-15 16:44:01 | INFO | fairseq_cli.train | num. shared model params: 37,305,344 (num. trained: 37,305,344)
2022-03-15 16:44:01 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-15 16:44:01 | INFO | fairseq.data.data_utils | loaded 2,558 examples from: data-bin/ru/valid
2022-03-15 16:44:01 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-15 16:44:01 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 16:44:01 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-15 16:44:01 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 16:44:01 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-15 16:44:01 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-15 16:44:01 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-15 16:44:01 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-15 16:44:01 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-15 16:44:01 | INFO | fairseq.data.data_utils | loaded 53,136 examples from: data-bin/ru/train
2022-03-15 16:44:01 | INFO | fairseq.trainer | begin training epoch 1
2022-03-15 16:44:01 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-15 16:44:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-15 16:44:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:44:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 16:44:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-15 16:50:31 | INFO | train_inner | epoch 001:    104 / 407 loss=14.835, ppl=29234.8, wps=18106, ups=0.28, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.2, loss_scale=8, train_wall=361, gb_free=9.6, wall=390
2022-03-15 16:56:30 | INFO | train_inner | epoch 001:    204 / 407 loss=13.369, ppl=10579.2, wps=18237.1, ups=0.28, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=0.648, loss_scale=16, train_wall=332, gb_free=9.6, wall=749
2022-03-15 17:02:27 | INFO | train_inner | epoch 001:    304 / 407 loss=12.496, ppl=5776.59, wps=18372.9, ups=0.28, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.404, loss_scale=32, train_wall=329, gb_free=9.6, wall=1106
2022-03-15 17:08:26 | INFO | train_inner | epoch 001:    404 / 407 loss=12.053, ppl=4249.59, wps=18285.4, ups=0.28, wpb=65534.2, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.394, loss_scale=64, train_wall=331, gb_free=9.6, wall=1465
2022-03-15 17:08:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:09:22 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.871 | ppl 3745.31 | wps 28398.7 | wpb 511.9 | bsz 1 | num_updates 403
2022-03-15 17:09:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 403 updates
2022-03-15 17:09:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 17:09:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 17:09:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 1 @ 403 updates, score 11.871) (writing took 2.41132354689762 seconds)
2022-03-15 17:09:25 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-15 17:09:25 | INFO | train | epoch 001 | loss 13.18 | ppl 9281.16 | wps 17648.3 | ups 0.27 | wpb 65492.3 | bsz 127.9 | num_updates 403 | lr 5.04649e-05 | gnorm 0.908 | loss_scale 64 | train_wall 1361 | gb_free 9.6 | wall 1524
KL Stats: Epoch 1 Divergences: Uniform: 0.6974651440399324 Unigram: 0.8214340907282871
2022-03-15 17:09:25 | INFO | fairseq.trainer | begin training epoch 2
2022-03-15 17:09:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:13:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:15:13 | INFO | train_inner | epoch 002:     98 / 407 loss=11.864, ppl=3727.13, wps=16022.3, ups=0.25, wpb=65360.1, bsz=127.7, num_updates=500, lr=6.25875e-05, gnorm=0.408, loss_scale=32, train_wall=330, gb_free=9.6, wall=1872
2022-03-15 17:21:16 | INFO | train_inner | epoch 002:    198 / 407 loss=11.633, ppl=3176.35, wps=18097.6, ups=0.28, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.453, loss_scale=64, train_wall=334, gb_free=9.6, wall=2235
2022-03-15 17:25:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:27:21 | INFO | train_inner | epoch 002:    299 / 407 loss=11.336, ppl=2585.52, wps=17918.9, ups=0.27, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.451, loss_scale=32, train_wall=338, gb_free=9.6, wall=2600
2022-03-15 17:33:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:33:29 | INFO | train_inner | epoch 002:    400 / 407 loss=10.991, ppl=2034.57, wps=17846, ups=0.27, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.522, loss_scale=32, train_wall=339, gb_free=9.6, wall=2968
2022-03-15 17:33:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:34:39 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.585 | ppl 1536.05 | wps 28722.6 | wpb 511.9 | bsz 1 | num_updates 807 | best_loss 10.585
2022-03-15 17:34:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 807 updates
2022-03-15 17:34:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 17:34:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 17:34:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 2 @ 807 updates, score 10.585) (writing took 2.714164206990972 seconds)
2022-03-15 17:34:42 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-15 17:34:42 | INFO | train | epoch 002 | loss 11.441 | ppl 2780.54 | wps 17434.6 | ups 0.27 | wpb 65492.5 | bsz 127.9 | num_updates 807 | lr 0.000100955 | gnorm 0.459 | loss_scale 32 | train_wall 1354 | gb_free 9.6 | wall 3041
KL Stats: Epoch 2 Divergences: Uniform: 1.394995149331882 Unigram: 0.5680976558692535
2022-03-15 17:34:42 | INFO | fairseq.trainer | begin training epoch 3
2022-03-15 17:34:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:40:12 | INFO | train_inner | epoch 003:     93 / 407 loss=10.637, ppl=1592.57, wps=16202, ups=0.25, wpb=65361.9, bsz=127.7, num_updates=900, lr=0.000112578, gnorm=0.542, loss_scale=32, train_wall=326, gb_free=9.6, wall=3371
2022-03-15 17:41:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:46:14 | INFO | train_inner | epoch 003:    194 / 407 loss=10.375, ppl=1328.19, wps=18083.1, ups=0.28, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.531, loss_scale=32, train_wall=334, gb_free=9.6, wall=3733
2022-03-15 17:50:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:51:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 17:52:17 | INFO | train_inner | epoch 003:    296 / 407 loss=10.15, ppl=1136.34, wps=18054.9, ups=0.28, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.598, loss_scale=16, train_wall=335, gb_free=9.6, wall=4096
2022-03-15 17:58:19 | INFO | train_inner | epoch 003:    396 / 407 loss=9.941, ppl=982.97, wps=18103.8, ups=0.28, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.6, loss_scale=16, train_wall=334, gb_free=9.6, wall=4458
2022-03-15 17:58:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:59:45 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.623 | ppl 788.42 | wps 28823.8 | wpb 511.9 | bsz 1 | num_updates 1211 | best_loss 9.623
2022-03-15 17:59:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1211 updates
2022-03-15 17:59:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 17:59:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 17:59:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 3 @ 1211 updates, score 9.623) (writing took 2.52461415308062 seconds)
2022-03-15 17:59:48 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-15 17:59:48 | INFO | train | epoch 003 | loss 10.254 | ppl 1221.48 | wps 17574.3 | ups 0.27 | wpb 65492.5 | bsz 127.9 | num_updates 1211 | lr 0.000151445 | gnorm 0.569 | loss_scale 32 | train_wall 1344 | gb_free 9.6 | wall 4547
KL Stats: Epoch 3 Divergences: Uniform: 1.9690660041333214 Unigram: 1.5443546405359434
2022-03-15 17:59:48 | INFO | fairseq.trainer | begin training epoch 4
2022-03-15 17:59:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:05:07 | INFO | train_inner | epoch 004:     89 / 407 loss=9.729, ppl=848.54, wps=16048, ups=0.25, wpb=65360.1, bsz=127.7, num_updates=1300, lr=0.000162568, gnorm=0.672, loss_scale=32, train_wall=331, gb_free=9.6, wall=4866
2022-03-15 18:07:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:11:11 | INFO | train_inner | epoch 004:    190 / 407 loss=9.539, ppl=743.77, wps=17975.5, ups=0.27, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.688, loss_scale=32, train_wall=336, gb_free=9.6, wall=5230
2022-03-15 18:16:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:17:20 | INFO | train_inner | epoch 004:    291 / 407 loss=9.35, ppl=652.49, wps=17781.2, ups=0.27, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.677, loss_scale=32, train_wall=340, gb_free=9.6, wall=5599
2022-03-15 18:23:17 | INFO | train_inner | epoch 004:    391 / 407 loss=9.171, ppl=576.48, wps=18351, ups=0.28, wpb=65536, bsz=128, num_updates=1600, lr=0.00020006, gnorm=0.702, loss_scale=32, train_wall=330, gb_free=9.6, wall=5956
2022-03-15 18:24:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:24:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:24:54 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.856 | ppl 463.47 | wps 31413.9 | wpb 511.9 | bsz 1 | num_updates 1615 | best_loss 8.856
2022-03-15 18:24:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1615 updates
2022-03-15 18:24:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 18:24:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 18:24:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 4 @ 1615 updates, score 8.856) (writing took 2.35200633702334 seconds)
2022-03-15 18:24:57 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-15 18:24:57 | INFO | train | epoch 004 | loss 9.424 | ppl 687.02 | wps 17533.2 | ups 0.27 | wpb 65492.5 | bsz 127.9 | num_updates 1615 | lr 0.000201935 | gnorm 0.689 | loss_scale 32 | train_wall 1350 | gb_free 9.6 | wall 6056
KL Stats: Epoch 4 Divergences: Uniform: 2.4942749074279584 Unigram: 2.114995564135863
2022-03-15 18:24:57 | INFO | fairseq.trainer | begin training epoch 5
2022-03-15 18:24:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:30:05 | INFO | train_inner | epoch 005:     85 / 407 loss=8.996, ppl=510.56, wps=16030.2, ups=0.25, wpb=65360.1, bsz=127.7, num_updates=1700, lr=0.000212558, gnorm=0.709, loss_scale=32, train_wall=334, gb_free=9.6, wall=6364
2022-03-15 18:34:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:36:09 | INFO | train_inner | epoch 005:    186 / 407 loss=8.841, ppl=458.42, wps=17990.7, ups=0.27, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.746, loss_scale=32, train_wall=337, gb_free=9.6, wall=6728
2022-03-15 18:42:06 | INFO | train_inner | epoch 005:    286 / 407 loss=8.688, ppl=412.38, wps=18358, ups=0.28, wpb=65534.2, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.718, loss_scale=32, train_wall=329, gb_free=9.6, wall=7085
2022-03-15 18:42:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:48:12 | INFO | train_inner | epoch 005:    387 / 407 loss=8.565, ppl=378.75, wps=17901.9, ups=0.27, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.724, loss_scale=32, train_wall=338, gb_free=9.6, wall=7451
2022-03-15 18:49:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:50:13 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.25 | ppl 304.53 | wps 27977.3 | wpb 511.9 | bsz 1 | num_updates 2020 | best_loss 8.25
2022-03-15 18:50:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 2020 updates
2022-03-15 18:50:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 18:50:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 18:50:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 5 @ 2020 updates, score 8.25) (writing took 3.0638465299271047 seconds)
2022-03-15 18:50:16 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-15 18:50:16 | INFO | train | epoch 005 | loss 8.747 | ppl 429.51 | wps 17456.5 | ups 0.27 | wpb 65492.6 | bsz 127.9 | num_updates 2020 | lr 0.00025255 | gnorm 0.727 | loss_scale 32 | train_wall 1355 | gb_free 9.6 | wall 7575
KL Stats: Epoch 5 Divergences: Uniform: 2.9457631286354373 Unigram: 2.502101240667987
2022-03-15 18:50:16 | INFO | fairseq.trainer | begin training epoch 6
2022-03-15 18:50:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:51:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:55:10 | INFO | train_inner | epoch 006:     81 / 407 loss=8.417, ppl=341.76, wps=15652.4, ups=0.24, wpb=65360.1, bsz=127.7, num_updates=2100, lr=0.000262548, gnorm=0.732, loss_scale=32, train_wall=338, gb_free=9.6, wall=7869
2022-03-15 19:01:12 | INFO | train_inner | epoch 006:    181 / 407 loss=8.286, ppl=312.15, wps=18083, ups=0.28, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.719, loss_scale=64, train_wall=334, gb_free=9.6, wall=8231
2022-03-15 19:01:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:07:19 | INFO | train_inner | epoch 006:    282 / 407 loss=8.186, ppl=291.26, wps=17832.8, ups=0.27, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.702, loss_scale=32, train_wall=339, gb_free=9.6, wall=8599
2022-03-15 19:09:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:13:21 | INFO | train_inner | epoch 006:    383 / 407 loss=8.086, ppl=271.72, wps=18140.8, ups=0.28, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.702, loss_scale=32, train_wall=333, gb_free=9.6, wall=8960
2022-03-15 19:14:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:15:30 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.819 | ppl 225.82 | wps 27749.4 | wpb 511.9 | bsz 1 | num_updates 2424 | best_loss 7.819
2022-03-15 19:15:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2424 updates
2022-03-15 19:15:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 19:15:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 19:15:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 6 @ 2424 updates, score 7.819) (writing took 2.3897096660220996 seconds)
2022-03-15 19:15:33 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-15 19:15:33 | INFO | train | epoch 006 | loss 8.22 | ppl 298.24 | wps 17446.7 | ups 0.27 | wpb 65492.5 | bsz 127.9 | num_updates 2424 | lr 0.000303039 | gnorm 0.706 | loss_scale 32 | train_wall 1352 | gb_free 9.6 | wall 9092
KL Stats: Epoch 6 Divergences: Uniform: 3.315336598658064 Unigram: 2.8081357368876945
2022-03-15 19:15:33 | INFO | fairseq.trainer | begin training epoch 7
2022-03-15 19:15:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:17:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:20:07 | INFO | train_inner | epoch 007:     77 / 407 loss=7.959, ppl=248.79, wps=16078.9, ups=0.25, wpb=65361.9, bsz=127.7, num_updates=2500, lr=0.000312538, gnorm=0.679, loss_scale=32, train_wall=328, gb_free=9.6, wall=9366
2022-03-15 19:25:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:26:14 | INFO | train_inner | epoch 007:    178 / 407 loss=7.887, ppl=236.64, wps=17849.7, ups=0.27, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.693, loss_scale=32, train_wall=339, gb_free=9.6, wall=9733
2022-03-15 19:32:08 | INFO | train_inner | epoch 007:    278 / 407 loss=7.812, ppl=224.74, wps=18515.8, ups=0.28, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.665, loss_scale=32, train_wall=326, gb_free=9.6, wall=10087
2022-03-15 19:33:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:36:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:37:52 | INFO | train_inner | epoch 007:    380 / 407 loss=7.733, ppl=212.72, wps=19092.2, ups=0.29, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.658, loss_scale=16, train_wall=317, gb_free=9.6, wall=10431
2022-03-15 19:39:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:40:02 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.499 | ppl 180.95 | wps 31214.6 | wpb 511.9 | bsz 1 | num_updates 2827 | best_loss 7.499
2022-03-15 19:40:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2827 updates
2022-03-15 19:40:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 19:40:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 19:40:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 7 @ 2827 updates, score 7.499) (writing took 2.4197219419293106 seconds)
2022-03-15 19:40:05 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-15 19:40:05 | INFO | train | epoch 007 | loss 7.826 | ppl 226.96 | wps 17930.6 | ups 0.27 | wpb 65492.3 | bsz 127.9 | num_updates 2827 | lr 0.000353404 | gnorm 0.673 | loss_scale 16 | train_wall 1315 | gb_free 9.6 | wall 10564
KL Stats: Epoch 7 Divergences: Uniform: 3.5885161329985062 Unigram: 3.033675207703915
2022-03-15 19:40:05 | INFO | fairseq.trainer | begin training epoch 8
2022-03-15 19:40:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:44:05 | INFO | train_inner | epoch 008:     73 / 407 loss=7.63, ppl=198.06, wps=17506.7, ups=0.27, wpb=65360.1, bsz=127.7, num_updates=2900, lr=0.000362528, gnorm=0.644, loss_scale=32, train_wall=303, gb_free=9.6, wall=10804
2022-03-15 19:49:31 | INFO | train_inner | epoch 008:    173 / 407 loss=7.566, ppl=189.55, wps=20087.7, ups=0.31, wpb=65534.2, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.635, loss_scale=32, train_wall=301, gb_free=9.6, wall=11130
2022-03-15 19:50:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:55:00 | INFO | train_inner | epoch 008:    274 / 407 loss=7.515, ppl=182.87, wps=19911.3, ups=0.3, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.622, loss_scale=32, train_wall=304, gb_free=9.6, wall=11459
2022-03-15 19:57:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 19:59:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:00:36 | INFO | train_inner | epoch 008:    376 / 407 loss=7.459, ppl=175.9, wps=19510.7, ups=0.3, wpb=65536, bsz=128, num_updates=3200, lr=0.00040002, gnorm=0.616, loss_scale=16, train_wall=310, gb_free=9.6, wall=11795
2022-03-15 20:02:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:03:00 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.234 | ppl 150.54 | wps 31051.2 | wpb 511.9 | bsz 1 | num_updates 3231 | best_loss 7.234
2022-03-15 20:03:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3231 updates
2022-03-15 20:03:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 20:03:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 20:03:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 8 @ 3231 updates, score 7.234) (writing took 2.361754307989031 seconds)
2022-03-15 20:03:03 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-15 20:03:03 | INFO | train | epoch 008 | loss 7.521 | ppl 183.73 | wps 19203.4 | ups 0.29 | wpb 65492.5 | bsz 127.9 | num_updates 3231 | lr 0.000403894 | gnorm 0.627 | loss_scale 16 | train_wall 1230 | gb_free 9.6 | wall 11942
KL Stats: Epoch 8 Divergences: Uniform: 3.7719428535956125 Unigram: 3.1965350987457977
2022-03-15 20:03:03 | INFO | fairseq.trainer | begin training epoch 9
2022-03-15 20:03:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:06:50 | INFO | train_inner | epoch 009:     69 / 407 loss=7.36, ppl=164.24, wps=17470.6, ups=0.27, wpb=65361.9, bsz=127.7, num_updates=3300, lr=0.000412518, gnorm=0.61, loss_scale=16, train_wall=303, gb_free=9.6, wall=12169
2022-03-15 20:09:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:12:21 | INFO | train_inner | epoch 009:    170 / 407 loss=7.303, ppl=157.87, wps=19793.9, ups=0.3, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.598, loss_scale=16, train_wall=306, gb_free=9.6, wall=12501
2022-03-15 20:17:51 | INFO | train_inner | epoch 009:    270 / 407 loss=7.255, ppl=152.77, wps=19888.7, ups=0.3, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.59, loss_scale=32, train_wall=304, gb_free=9.6, wall=12830
2022-03-15 20:23:21 | INFO | train_inner | epoch 009:    370 / 407 loss=7.205, ppl=147.58, wps=19842.5, ups=0.3, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.582, loss_scale=32, train_wall=305, gb_free=9.6, wall=13160
2022-03-15 20:24:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 20:24:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:25:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:26:07 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.98 | ppl 126.26 | wps 30809.9 | wpb 511.9 | bsz 1 | num_updates 3635 | best_loss 6.98
2022-03-15 20:26:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3635 updates
2022-03-15 20:26:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 20:26:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 20:26:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 9 @ 3635 updates, score 6.98) (writing took 2.4702814699849114 seconds)
2022-03-15 20:26:09 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-15 20:26:09 | INFO | train | epoch 009 | loss 7.261 | ppl 153.37 | wps 19080.4 | ups 0.29 | wpb 65492.5 | bsz 127.9 | num_updates 3635 | lr 0.000454384 | gnorm 0.591 | loss_scale 16 | train_wall 1238 | gb_free 9.6 | wall 13328
KL Stats: Epoch 9 Divergences: Uniform: 3.9114741146411403 Unigram: 3.32222341141048
2022-03-15 20:26:09 | INFO | fairseq.trainer | begin training epoch 10
2022-03-15 20:26:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:29:46 | INFO | train_inner | epoch 010:     65 / 407 loss=7.109, ppl=138.01, wps=17001.9, ups=0.26, wpb=65360.1, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.578, loss_scale=16, train_wall=313, gb_free=9.6, wall=13545
2022-03-15 20:34:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:35:20 | INFO | train_inner | epoch 010:    166 / 407 loss=7.06, ppl=133.4, wps=19611.9, ups=0.3, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.565, loss_scale=16, train_wall=309, gb_free=9.6, wall=13879
2022-03-15 20:40:49 | INFO | train_inner | epoch 010:    266 / 407 loss=7.028, ppl=130.52, wps=19902.4, ups=0.3, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.561, loss_scale=16, train_wall=304, gb_free=9.6, wall=14208
2022-03-15 20:43:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:46:21 | INFO | train_inner | epoch 010:    367 / 407 loss=6.984, ppl=126.56, wps=19725.8, ups=0.3, wpb=65534.2, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.556, loss_scale=16, train_wall=307, gb_free=9.6, wall=14540
2022-03-15 20:48:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:49:17 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.773 | ppl 109.39 | wps 30840.9 | wpb 511.9 | bsz 1 | num_updates 4040 | best_loss 6.773
2022-03-15 20:49:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 4040 updates
2022-03-15 20:49:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 20:49:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 20:49:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 10 @ 4040 updates, score 6.773) (writing took 2.3803383290069178 seconds)
2022-03-15 20:49:20 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-15 20:49:20 | INFO | train | epoch 010 | loss 7.026 | ppl 130.32 | wps 19079 | ups 0.29 | wpb 65492.6 | bsz 127.9 | num_updates 4040 | lr 0.000497519 | gnorm 0.564 | loss_scale 16 | train_wall 1242 | gb_free 9.6 | wall 14719
KL Stats: Epoch 10 Divergences: Uniform: 4.024319723899199 Unigram: 3.4250782584411503
2022-03-15 20:49:20 | INFO | fairseq.trainer | begin training epoch 11
2022-03-15 20:49:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:52:40 | INFO | train_inner | epoch 011:     60 / 407 loss=6.894, ppl=118.97, wps=17246.3, ups=0.26, wpb=65361.9, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.55, loss_scale=32, train_wall=308, gb_free=9.6, wall=14919
2022-03-15 20:53:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:58:15 | INFO | train_inner | epoch 011:    161 / 407 loss=6.847, ppl=115.12, wps=19597.8, ups=0.3, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.535, loss_scale=16, train_wall=309, gb_free=9.6, wall=15254
2022-03-15 21:02:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:03:46 | INFO | train_inner | epoch 011:    262 / 407 loss=6.802, ppl=111.55, wps=19798.8, ups=0.3, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.532, loss_scale=16, train_wall=306, gb_free=9.6, wall=15585
2022-03-15 21:09:16 | INFO | train_inner | epoch 011:    362 / 407 loss=6.785, ppl=110.31, wps=19820.7, ups=0.3, wpb=65534.2, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.529, loss_scale=32, train_wall=305, gb_free=9.6, wall=15915
2022-03-15 21:11:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:12:29 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.598 | ppl 96.87 | wps 30881.5 | wpb 511.9 | bsz 1 | num_updates 4445 | best_loss 6.598
2022-03-15 21:12:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4445 updates
2022-03-15 21:12:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 21:12:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 21:12:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 11 @ 4445 updates, score 6.598) (writing took 2.3903679000213742 seconds)
2022-03-15 21:12:31 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-15 21:12:31 | INFO | train | epoch 011 | loss 6.812 | ppl 112.4 | wps 19061.9 | ups 0.29 | wpb 65492.6 | bsz 127.9 | num_updates 4445 | lr 0.000474312 | gnorm 0.533 | loss_scale 32 | train_wall 1243 | gb_free 9.6 | wall 16110
KL Stats: Epoch 11 Divergences: Uniform: 4.122089189178363 Unigram: 3.5179235004346254
2022-03-15 21:12:31 | INFO | fairseq.trainer | begin training epoch 12
2022-03-15 21:12:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:13:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:15:38 | INFO | train_inner | epoch 012:     56 / 407 loss=6.706, ppl=104.4, wps=17135.1, ups=0.26, wpb=65361.9, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.519, loss_scale=16, train_wall=310, gb_free=9.6, wall=16297
2022-03-15 21:21:10 | INFO | train_inner | epoch 012:    156 / 407 loss=6.653, ppl=100.62, wps=19749.7, ups=0.3, wpb=65534.2, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.504, loss_scale=32, train_wall=306, gb_free=9.6, wall=16629
2022-03-15 21:22:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:26:42 | INFO | train_inner | epoch 012:    257 / 407 loss=6.647, ppl=100.19, wps=19721.5, ups=0.3, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.509, loss_scale=16, train_wall=307, gb_free=9.6, wall=16961
2022-03-15 21:31:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:32:14 | INFO | train_inner | epoch 012:    358 / 407 loss=6.625, ppl=98.73, wps=19726.7, ups=0.3, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.507, loss_scale=16, train_wall=307, gb_free=9.6, wall=17293
2022-03-15 21:34:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:35:38 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.481 | ppl 89.35 | wps 30970.3 | wpb 511.9 | bsz 1 | num_updates 4849 | best_loss 6.481
2022-03-15 21:35:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4849 updates
2022-03-15 21:35:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 21:35:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 21:35:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 12 @ 4849 updates, score 6.481) (writing took 2.3555749889928848 seconds)
2022-03-15 21:35:40 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-15 21:35:40 | INFO | train | epoch 012 | loss 6.642 | ppl 99.85 | wps 19042.7 | ups 0.29 | wpb 65492.5 | bsz 127.9 | num_updates 4849 | lr 0.000454123 | gnorm 0.508 | loss_scale 16 | train_wall 1241 | gb_free 9.6 | wall 17500
KL Stats: Epoch 12 Divergences: Uniform: 4.20082932928626 Unigram: 3.595509126549374
2022-03-15 21:35:41 | INFO | fairseq.trainer | begin training epoch 13
2022-03-15 21:35:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:38:30 | INFO | train_inner | epoch 013:     51 / 407 loss=6.567, ppl=94.8, wps=17404.9, ups=0.27, wpb=65361.9, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.498, loss_scale=16, train_wall=305, gb_free=9.6, wall=17669
2022-03-15 21:42:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:44:04 | INFO | train_inner | epoch 013:    152 / 407 loss=6.52, ppl=91.79, wps=19590.8, ups=0.3, wpb=65534.2, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.492, loss_scale=16, train_wall=309, gb_free=9.6, wall=18003
2022-03-15 21:49:35 | INFO | train_inner | epoch 013:    252 / 407 loss=6.521, ppl=91.82, wps=19834.4, ups=0.3, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.486, loss_scale=32, train_wall=305, gb_free=9.6, wall=18334
2022-03-15 21:55:05 | INFO | train_inner | epoch 013:    352 / 407 loss=6.502, ppl=90.64, wps=19857.9, ups=0.3, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.487, loss_scale=32, train_wall=305, gb_free=9.6, wall=18664
2022-03-15 21:56:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 21:58:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:58:47 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.39 | ppl 83.85 | wps 31232.6 | wpb 511.9 | bsz 1 | num_updates 5254 | best_loss 6.39
2022-03-15 21:58:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5254 updates
2022-03-15 21:58:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 21:58:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 21:58:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 13 @ 5254 updates, score 6.39) (writing took 2.476867160992697 seconds)
2022-03-15 21:58:49 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-15 21:58:49 | INFO | train | epoch 013 | loss 6.512 | ppl 91.26 | wps 19098.1 | ups 0.29 | wpb 65492.6 | bsz 127.9 | num_updates 5254 | lr 0.00043627 | gnorm 0.489 | loss_scale 32 | train_wall 1241 | gb_free 9.6 | wall 18888
KL Stats: Epoch 13 Divergences: Uniform: 4.259179312174823 Unigram: 3.655744864533796
2022-03-15 21:58:49 | INFO | fairseq.trainer | begin training epoch 14
2022-03-15 21:58:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:01:22 | INFO | train_inner | epoch 014:     46 / 407 loss=6.449, ppl=87.37, wps=17325.6, ups=0.27, wpb=65360.1, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.492, loss_scale=32, train_wall=306, gb_free=9.6, wall=19041
2022-03-15 22:04:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:06:56 | INFO | train_inner | epoch 014:    147 / 407 loss=6.412, ppl=85.14, wps=19632.4, ups=0.3, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.486, loss_scale=32, train_wall=308, gb_free=9.6, wall=19375
2022-03-15 22:11:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:12:31 | INFO | train_inner | epoch 014:    248 / 407 loss=6.411, ppl=85.12, wps=19521.1, ups=0.3, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.481, loss_scale=32, train_wall=310, gb_free=9.6, wall=19711
2022-03-15 22:17:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:18:04 | INFO | train_inner | epoch 014:    349 / 407 loss=6.409, ppl=84.96, wps=19697.9, ups=0.3, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.485, loss_scale=16, train_wall=307, gb_free=9.6, wall=20043
2022-03-15 22:21:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:21:56 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.322 | ppl 79.99 | wps 31447 | wpb 511.9 | bsz 1 | num_updates 5658 | best_loss 6.322
2022-03-15 22:21:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5658 updates
2022-03-15 22:21:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 22:21:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 22:21:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 14 @ 5658 updates, score 6.322) (writing took 2.3342148339143023 seconds)
2022-03-15 22:21:58 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-15 22:21:58 | INFO | train | epoch 014 | loss 6.408 | ppl 84.91 | wps 19053 | ups 0.29 | wpb 65492.5 | bsz 127.9 | num_updates 5658 | lr 0.000420406 | gnorm 0.485 | loss_scale 16 | train_wall 1241 | gb_free 9.6 | wall 20277
KL Stats: Epoch 14 Divergences: Uniform: 4.3080250577009 Unigram: 3.707177339858659
2022-03-15 22:21:58 | INFO | fairseq.trainer | begin training epoch 15
2022-03-15 22:21:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:24:16 | INFO | train_inner | epoch 015:     42 / 407 loss=6.366, ppl=82.5, wps=17602.9, ups=0.27, wpb=65361.9, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.481, loss_scale=16, train_wall=301, gb_free=9.6, wall=20415
2022-03-15 22:29:48 | INFO | train_inner | epoch 015:    142 / 407 loss=6.334, ppl=80.66, wps=19714, ups=0.3, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.485, loss_scale=32, train_wall=307, gb_free=9.6, wall=20747
2022-03-15 22:32:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:35:24 | INFO | train_inner | epoch 015:    243 / 407 loss=6.325, ppl=80.19, wps=19487.7, ups=0.3, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.483, loss_scale=32, train_wall=311, gb_free=9.6, wall=21083
2022-03-15 22:39:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 22:39:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:41:02 | INFO | train_inner | epoch 015:    345 / 407 loss=6.319, ppl=79.83, wps=19394.8, ups=0.3, wpb=65534.2, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.484, loss_scale=16, train_wall=312, gb_free=9.6, wall=21421
2022-03-15 22:44:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:45:08 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.258 | ppl 76.52 | wps 30966.4 | wpb 511.9 | bsz 1 | num_updates 6062 | best_loss 6.258
2022-03-15 22:45:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 6062 updates
2022-03-15 22:45:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 22:45:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 22:45:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 15 @ 6062 updates, score 6.258) (writing took 2.3801449979655445 seconds)
2022-03-15 22:45:11 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-15 22:45:11 | INFO | train | epoch 015 | loss 6.323 | ppl 80.04 | wps 19000.6 | ups 0.29 | wpb 65492.5 | bsz 127.9 | num_updates 6062 | lr 0.000406155 | gnorm 0.483 | loss_scale 16 | train_wall 1244 | gb_free 9.6 | wall 21670
KL Stats: Epoch 15 Divergences: Uniform: 4.350143008871554 Unigram: 3.7491314440626273
2022-03-15 22:45:11 | INFO | fairseq.trainer | begin training epoch 16
2022-03-15 22:45:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:47:17 | INFO | train_inner | epoch 016:     38 / 407 loss=6.278, ppl=77.61, wps=17440.1, ups=0.27, wpb=65361.9, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.482, loss_scale=16, train_wall=304, gb_free=9.6, wall=21796
2022-03-15 22:48:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:52:51 | INFO | train_inner | epoch 016:    139 / 407 loss=6.252, ppl=76.2, wps=19620.8, ups=0.3, wpb=65534.2, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.484, loss_scale=16, train_wall=308, gb_free=9.6, wall=22130
2022-03-15 22:58:23 | INFO | train_inner | epoch 016:    239 / 407 loss=6.256, ppl=76.41, wps=19758.9, ups=0.3, wpb=65536, bsz=128, num_updates=6300, lr=0.00039841, gnorm=0.472, loss_scale=32, train_wall=306, gb_free=9.6, wall=22462
2022-03-15 22:58:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:03:56 | INFO | train_inner | epoch 016:    340 / 407 loss=6.246, ppl=75.92, wps=19634.8, ups=0.3, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.475, loss_scale=16, train_wall=308, gb_free=9.6, wall=22795
2022-03-15 23:07:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:08:20 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.205 | ppl 73.76 | wps 30889.6 | wpb 511.9 | bsz 1 | num_updates 6467 | best_loss 6.205
2022-03-15 23:08:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6467 updates
2022-03-15 23:08:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 23:08:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 23:08:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 16 @ 6467 updates, score 6.205) (writing took 2.3567174579948187 seconds)
2022-03-15 23:08:23 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-15 23:08:23 | INFO | train | epoch 016 | loss 6.249 | ppl 76.05 | wps 19054.8 | ups 0.29 | wpb 65492.6 | bsz 127.9 | num_updates 6467 | lr 0.000393232 | gnorm 0.476 | loss_scale 32 | train_wall 1243 | gb_free 9.6 | wall 23062
KL Stats: Epoch 16 Divergences: Uniform: 4.386902157568766 Unigram: 3.7886228536166424
2022-03-15 23:08:23 | INFO | fairseq.trainer | begin training epoch 17
2022-03-15 23:08:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:10:12 | INFO | train_inner | epoch 017:     33 / 407 loss=6.227, ppl=74.89, wps=17404.2, ups=0.27, wpb=65361.9, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.474, loss_scale=32, train_wall=305, gb_free=9.6, wall=23171
2022-03-15 23:13:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:15:45 | INFO | train_inner | epoch 017:    134 / 407 loss=6.18, ppl=72.51, wps=19664.3, ups=0.3, wpb=65534.2, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.474, loss_scale=16, train_wall=308, gb_free=9.6, wall=23504
2022-03-15 23:21:17 | INFO | train_inner | epoch 017:    234 / 407 loss=6.186, ppl=72.81, wps=19778.8, ups=0.3, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.48, loss_scale=32, train_wall=306, gb_free=9.6, wall=23836
2022-03-15 23:26:48 | INFO | train_inner | epoch 017:    334 / 407 loss=6.187, ppl=72.86, wps=19765.4, ups=0.3, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.484, loss_scale=32, train_wall=306, gb_free=9.6, wall=24167
2022-03-15 23:27:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:28:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:30:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:31:32 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.162 | ppl 71.63 | wps 30959.9 | wpb 511.9 | bsz 1 | num_updates 6871 | best_loss 6.162
2022-03-15 23:31:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6871 updates
2022-03-15 23:31:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 23:31:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 23:31:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 17 @ 6871 updates, score 6.162) (writing took 2.36905898200348 seconds)
2022-03-15 23:31:35 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-15 23:31:35 | INFO | train | epoch 017 | loss 6.186 | ppl 72.81 | wps 19007.6 | ups 0.29 | wpb 65492.5 | bsz 127.9 | num_updates 6871 | lr 0.000381496 | gnorm 0.479 | loss_scale 16 | train_wall 1244 | gb_free 9.6 | wall 24454
KL Stats: Epoch 17 Divergences: Uniform: 4.417925942378429 Unigram: 3.821450338565937
2022-03-15 23:31:35 | INFO | fairseq.trainer | begin training epoch 18
2022-03-15 23:31:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:33:11 | INFO | train_inner | epoch 018:     29 / 407 loss=6.169, ppl=71.96, wps=17084.1, ups=0.26, wpb=65361.9, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.48, loss_scale=16, train_wall=311, gb_free=9.6, wall=24550
2022-03-15 23:37:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:38:44 | INFO | train_inner | epoch 018:    130 / 407 loss=6.123, ppl=69.72, wps=19636, ups=0.3, wpb=65534.2, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.477, loss_scale=16, train_wall=308, gb_free=9.6, wall=24883
2022-03-15 23:44:15 | INFO | train_inner | epoch 018:    230 / 407 loss=6.134, ppl=70.24, wps=19807.5, ups=0.3, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.471, loss_scale=32, train_wall=306, gb_free=9.6, wall=25214
2022-03-15 23:49:48 | INFO | train_inner | epoch 018:    330 / 407 loss=6.135, ppl=70.28, wps=19688, ups=0.3, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.481, loss_scale=32, train_wall=307, gb_free=9.6, wall=25547
2022-03-15 23:51:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 23:54:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:54:46 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.118 | ppl 69.44 | wps 31029.2 | wpb 511.9 | bsz 1 | num_updates 7276 | best_loss 6.118
2022-03-15 23:54:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7276 updates
2022-03-15 23:54:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 23:54:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-15 23:54:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 18 @ 7276 updates, score 6.118) (writing took 2.5426305889850482 seconds)
2022-03-15 23:54:49 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-15 23:54:49 | INFO | train | epoch 018 | loss 6.132 | ppl 70.14 | wps 19023.4 | ups 0.29 | wpb 65492.6 | bsz 127.9 | num_updates 7276 | lr 0.000370727 | gnorm 0.479 | loss_scale 32 | train_wall 1245 | gb_free 9.6 | wall 25848
KL Stats: Epoch 18 Divergences: Uniform: 4.442769271190534 Unigram: 3.8477721660086512
2022-03-15 23:54:49 | INFO | fairseq.trainer | begin training epoch 19
2022-03-15 23:54:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:56:09 | INFO | train_inner | epoch 019:     24 / 407 loss=6.13, ppl=70.02, wps=17181, ups=0.26, wpb=65361.9, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.486, loss_scale=32, train_wall=309, gb_free=9.6, wall=25928
2022-03-15 23:59:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:01:41 | INFO | train_inner | epoch 019:    125 / 407 loss=6.073, ppl=67.32, wps=19694.9, ups=0.3, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.481, loss_scale=32, train_wall=307, gb_free=9.6, wall=26260
2022-03-16 00:06:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:07:16 | INFO | train_inner | epoch 019:    226 / 407 loss=6.078, ppl=67.55, wps=19604, ups=0.3, wpb=65534.2, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.476, loss_scale=32, train_wall=309, gb_free=9.6, wall=26595
2022-03-16 00:12:47 | INFO | train_inner | epoch 019:    326 / 407 loss=6.093, ppl=68.25, wps=19796.5, ups=0.3, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.475, loss_scale=32, train_wall=306, gb_free=9.6, wall=26926
2022-03-16 00:13:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:17:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:17:53 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.089 | ppl 68.07 | wps 31047.3 | wpb 511.9 | bsz 1 | num_updates 7680 | best_loss 6.089
2022-03-16 00:17:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7680 updates
2022-03-16 00:17:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 00:17:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 00:17:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 19 @ 7680 updates, score 6.089) (writing took 4.739174562972039 seconds)
2022-03-16 00:17:58 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-16 00:17:58 | INFO | train | epoch 019 | loss 6.083 | ppl 67.79 | wps 19054.2 | ups 0.29 | wpb 65492.5 | bsz 127.9 | num_updates 7680 | lr 0.000360844 | gnorm 0.476 | loss_scale 32 | train_wall 1239 | gb_free 9.6 | wall 27237
KL Stats: Epoch 19 Divergences: Uniform: 4.470705106801139 Unigram: 3.875562228409351
2022-03-16 00:17:58 | INFO | fairseq.trainer | begin training epoch 20
2022-03-16 00:17:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:19:04 | INFO | train_inner | epoch 020:     20 / 407 loss=6.079, ppl=67.61, wps=17327.9, ups=0.27, wpb=65361.9, bsz=127.7, num_updates=7700, lr=0.000360375, gnorm=0.47, loss_scale=32, train_wall=304, gb_free=9.6, wall=27303
2022-03-16 00:21:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:24:38 | INFO | train_inner | epoch 020:    121 / 407 loss=6.023, ppl=65.02, wps=19628.9, ups=0.3, wpb=65534.2, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.479, loss_scale=32, train_wall=308, gb_free=9.6, wall=27637
2022-03-16 00:26:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:30:11 | INFO | train_inner | epoch 020:    222 / 407 loss=6.046, ppl=66.09, wps=19667, ups=0.3, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.481, loss_scale=16, train_wall=308, gb_free=9.6, wall=27970
2022-03-16 00:35:00 | INFO | train_inner | epoch 020:    322 / 407 loss=6.048, ppl=66.15, wps=22689.6, ups=0.35, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.474, loss_scale=32, train_wall=265, gb_free=9.6, wall=28259
2022-03-16 00:38:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:39:28 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.058 | ppl 66.64 | wps 35779.8 | wpb 511.9 | bsz 1 | num_updates 8085 | best_loss 6.058
2022-03-16 00:39:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 8085 updates
2022-03-16 00:39:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 00:39:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 00:39:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 20 @ 8085 updates, score 6.058) (writing took 2.288124318001792 seconds)
2022-03-16 00:39:31 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-16 00:39:31 | INFO | train | epoch 020 | loss 6.041 | ppl 65.83 | wps 20510.8 | ups 0.31 | wpb 65492.6 | bsz 127.9 | num_updates 8085 | lr 0.00035169 | gnorm 0.478 | loss_scale 32 | train_wall 1154 | gb_free 9.6 | wall 28530
KL Stats: Epoch 20 Divergences: Uniform: 4.491943157349893 Unigram: 3.9013495668278253
2022-03-16 00:39:31 | INFO | fairseq.trainer | begin training epoch 21
2022-03-16 00:39:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:39:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:40:15 | INFO | train_inner | epoch 021:     16 / 407 loss=6.039, ppl=65.74, wps=20770.5, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=8100, lr=0.000351364, gnorm=0.48, loss_scale=32, train_wall=251, gb_free=9.6, wall=28574
2022-03-16 00:41:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:44:50 | INFO | train_inner | epoch 021:    117 / 407 loss=5.984, ppl=63.31, wps=23779.1, ups=0.36, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.479, loss_scale=16, train_wall=252, gb_free=9.6, wall=28849
2022-03-16 00:49:23 | INFO | train_inner | epoch 021:    217 / 407 loss=6.006, ppl=64.27, wps=24031.5, ups=0.37, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.482, loss_scale=32, train_wall=249, gb_free=9.6, wall=29122
2022-03-16 00:53:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 00:53:58 | INFO | train_inner | epoch 021:    318 / 407 loss=6.007, ppl=64.29, wps=23797.7, ups=0.36, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.48, loss_scale=32, train_wall=252, gb_free=9.6, wall=29397
2022-03-16 00:58:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:58:37 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.031 | ppl 65.37 | wps 35623.3 | wpb 511.9 | bsz 1 | num_updates 8489 | best_loss 6.031
2022-03-16 00:58:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8489 updates
2022-03-16 00:58:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 00:58:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 00:58:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 21 @ 8489 updates, score 6.031) (writing took 2.212134887930006 seconds)
2022-03-16 00:58:40 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-16 00:58:40 | INFO | train | epoch 021 | loss 6.002 | ppl 64.09 | wps 23027.8 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 8489 | lr 0.000343219 | gnorm 0.479 | loss_scale 32 | train_wall 1014 | gb_free 9.6 | wall 29679
KL Stats: Epoch 21 Divergences: Uniform: 4.511427581825582 Unigram: 3.9196236717953665
2022-03-16 00:58:40 | INFO | fairseq.trainer | begin training epoch 22
2022-03-16 00:58:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:59:10 | INFO | train_inner | epoch 022:     11 / 407 loss=6.011, ppl=64.5, wps=20972.8, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=8500, lr=0.000342997, gnorm=0.477, loss_scale=32, train_wall=248, gb_free=9.6, wall=29709
2022-03-16 00:59:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:03:45 | INFO | train_inner | epoch 022:    112 / 407 loss=5.95, ppl=61.83, wps=23786.3, ups=0.36, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.484, loss_scale=32, train_wall=252, gb_free=9.6, wall=29984
2022-03-16 01:05:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:08:20 | INFO | train_inner | epoch 022:    213 / 407 loss=5.964, ppl=62.42, wps=23831.7, ups=0.36, wpb=65534.2, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.482, loss_scale=32, train_wall=251, gb_free=9.6, wall=30259
2022-03-16 01:11:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:12:56 | INFO | train_inner | epoch 022:    314 / 407 loss=5.978, ppl=63.02, wps=23797.2, ups=0.36, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.475, loss_scale=32, train_wall=252, gb_free=9.6, wall=30535
2022-03-16 01:13:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:17:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:17:46 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.014 | ppl 64.63 | wps 35769.5 | wpb 511.9 | bsz 1 | num_updates 8892 | best_loss 6.014
2022-03-16 01:17:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8892 updates
2022-03-16 01:17:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 01:17:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 01:17:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 22 @ 8892 updates, score 6.014) (writing took 2.22282742196694 seconds)
2022-03-16 01:17:48 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-16 01:17:48 | INFO | train | epoch 022 | loss 5.967 | ppl 62.54 | wps 22978.1 | ups 0.35 | wpb 65492.3 | bsz 127.9 | num_updates 8892 | lr 0.000335352 | gnorm 0.48 | loss_scale 16 | train_wall 1014 | gb_free 9.6 | wall 30827
KL Stats: Epoch 22 Divergences: Uniform: 4.532254534525368 Unigram: 3.9416611426986816
2022-03-16 01:17:48 | INFO | fairseq.trainer | begin training epoch 23
2022-03-16 01:17:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:18:10 | INFO | train_inner | epoch 023:      8 / 407 loss=5.973, ppl=62.8, wps=20781.9, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=8900, lr=0.000335201, gnorm=0.478, loss_scale=16, train_wall=251, gb_free=9.6, wall=30849
2022-03-16 01:22:43 | INFO | train_inner | epoch 023:    108 / 407 loss=5.914, ppl=60.3, wps=24023.8, ups=0.37, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.486, loss_scale=32, train_wall=249, gb_free=9.6, wall=31122
2022-03-16 01:26:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:27:18 | INFO | train_inner | epoch 023:    209 / 407 loss=5.929, ppl=60.92, wps=23803.4, ups=0.36, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.483, loss_scale=32, train_wall=252, gb_free=9.6, wall=31397
2022-03-16 01:31:51 | INFO | train_inner | epoch 023:    309 / 407 loss=5.956, ppl=62.07, wps=24068.7, ups=0.37, wpb=65534.2, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.476, loss_scale=32, train_wall=249, gb_free=9.6, wall=31670
2022-03-16 01:32:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:33:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:36:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:36:54 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.993 | ppl 63.7 | wps 35759.2 | wpb 511.9 | bsz 1 | num_updates 9296 | best_loss 5.993
2022-03-16 01:36:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9296 updates
2022-03-16 01:36:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 01:36:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 01:36:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 23 @ 9296 updates, score 5.993) (writing took 2.2299784659408033 seconds)
2022-03-16 01:36:57 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-16 01:36:57 | INFO | train | epoch 023 | loss 5.935 | ppl 61.19 | wps 23043.5 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 9296 | lr 0.000327983 | gnorm 0.481 | loss_scale 16 | train_wall 1013 | gb_free 9.6 | wall 31976
KL Stats: Epoch 23 Divergences: Uniform: 4.5499105561264 Unigram: 3.959803593716867
2022-03-16 01:36:57 | INFO | fairseq.trainer | begin training epoch 24
2022-03-16 01:36:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:37:08 | INFO | train_inner | epoch 024:      4 / 407 loss=5.941, ppl=61.42, wps=20624.9, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=9300, lr=0.000327913, gnorm=0.478, loss_scale=16, train_wall=253, gb_free=9.6, wall=31987
2022-03-16 01:41:40 | INFO | train_inner | epoch 024:    104 / 407 loss=5.883, ppl=59.01, wps=24035.8, ups=0.37, wpb=65534.2, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.488, loss_scale=32, train_wall=249, gb_free=9.6, wall=32259
2022-03-16 01:45:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:46:15 | INFO | train_inner | epoch 024:    205 / 407 loss=5.903, ppl=59.82, wps=23836.7, ups=0.36, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.479, loss_scale=32, train_wall=251, gb_free=9.6, wall=32534
2022-03-16 01:50:48 | INFO | train_inner | epoch 024:    305 / 407 loss=5.915, ppl=60.35, wps=24033.2, ups=0.37, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.48, loss_scale=32, train_wall=249, gb_free=9.6, wall=32807
2022-03-16 01:51:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 01:55:23 | INFO | train_inner | epoch 024:    406 / 407 loss=5.925, ppl=60.75, wps=23805.8, ups=0.36, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.475, loss_scale=32, train_wall=252, gb_free=9.6, wall=33082
2022-03-16 01:55:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:56:03 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.977 | ppl 63.01 | wps 35624.1 | wpb 511.9 | bsz 1 | num_updates 9701 | best_loss 5.977
2022-03-16 01:56:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9701 updates
2022-03-16 01:56:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 01:56:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 01:56:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 24 @ 9701 updates, score 5.977) (writing took 2.20009483397007 seconds)
2022-03-16 01:56:05 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-16 01:56:05 | INFO | train | epoch 024 | loss 5.906 | ppl 59.96 | wps 23097.2 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 9701 | lr 0.000321064 | gnorm 0.481 | loss_scale 32 | train_wall 1013 | gb_free 9.6 | wall 33124
KL Stats: Epoch 24 Divergences: Uniform: 4.566926544808905 Unigram: 3.9765623756989985
2022-03-16 01:56:05 | INFO | fairseq.trainer | begin training epoch 25
2022-03-16 01:56:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:58:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:00:38 | INFO | train_inner | epoch 025:    100 / 407 loss=5.856, ppl=57.94, wps=20770.3, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.486, loss_scale=32, train_wall=251, gb_free=9.6, wall=33397
2022-03-16 02:04:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:05:14 | INFO | train_inner | epoch 025:    201 / 407 loss=5.867, ppl=58.37, wps=23771.2, ups=0.36, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.485, loss_scale=32, train_wall=252, gb_free=9.6, wall=33673
2022-03-16 02:09:47 | INFO | train_inner | epoch 025:    301 / 407 loss=5.895, ppl=59.53, wps=24003.8, ups=0.37, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.48, loss_scale=32, train_wall=250, gb_free=9.6, wall=33946
2022-03-16 02:09:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:14:22 | INFO | train_inner | epoch 025:    402 / 407 loss=5.894, ppl=59.48, wps=23797.1, ups=0.36, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.481, loss_scale=32, train_wall=252, gb_free=9.6, wall=34221
2022-03-16 02:14:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:15:12 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.952 | ppl 61.89 | wps 35695.6 | wpb 511.9 | bsz 1 | num_updates 10105 | best_loss 5.952
2022-03-16 02:15:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 10105 updates
2022-03-16 02:15:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 02:15:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 02:15:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 25 @ 10105 updates, score 5.952) (writing took 2.224946844042279 seconds)
2022-03-16 02:15:15 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-16 02:15:15 | INFO | train | epoch 025 | loss 5.878 | ppl 58.82 | wps 23015.4 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 10105 | lr 0.000314581 | gnorm 0.483 | loss_scale 32 | train_wall 1014 | gb_free 9.6 | wall 34274
KL Stats: Epoch 25 Divergences: Uniform: 4.581549538630536 Unigram: 3.991664420392055
2022-03-16 02:15:15 | INFO | fairseq.trainer | begin training epoch 26
2022-03-16 02:15:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:16:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:19:37 | INFO | train_inner | epoch 026:     96 / 407 loss=5.833, ppl=57.02, wps=20754.1, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.488, loss_scale=32, train_wall=251, gb_free=9.6, wall=34536
2022-03-16 02:22:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:24:11 | INFO | train_inner | epoch 026:    197 / 407 loss=5.853, ppl=57.82, wps=23872, ups=0.36, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.479, loss_scale=32, train_wall=251, gb_free=9.6, wall=34811
2022-03-16 02:28:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:28:47 | INFO | train_inner | epoch 026:    298 / 407 loss=5.852, ppl=57.75, wps=23803.6, ups=0.36, wpb=65534.2, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.475, loss_scale=32, train_wall=252, gb_free=9.6, wall=35086
2022-03-16 02:33:19 | INFO | train_inner | epoch 026:    398 / 407 loss=5.875, ppl=58.67, wps=24051.4, ups=0.37, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.479, loss_scale=32, train_wall=249, gb_free=9.6, wall=35358
2022-03-16 02:33:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:34:20 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.945 | ppl 61.61 | wps 35690.2 | wpb 511.9 | bsz 1 | num_updates 10509 | best_loss 5.945
2022-03-16 02:34:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10509 updates
2022-03-16 02:34:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 02:34:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 02:34:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 26 @ 10509 updates, score 5.945) (writing took 2.218819117057137 seconds)
2022-03-16 02:34:23 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-16 02:34:23 | INFO | train | epoch 026 | loss 5.854 | ppl 57.84 | wps 23045.1 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 10509 | lr 0.000308475 | gnorm 0.48 | loss_scale 32 | train_wall 1013 | gb_free 9.6 | wall 35422
KL Stats: Epoch 26 Divergences: Uniform: 4.596408085285313 Unigram: 4.0072285171142745
2022-03-16 02:34:23 | INFO | fairseq.trainer | begin training epoch 27
2022-03-16 02:34:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:34:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 02:38:34 | INFO | train_inner | epoch 027:     92 / 407 loss=5.819, ppl=56.45, wps=20777.7, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.49, loss_scale=32, train_wall=251, gb_free=9.6, wall=35673
2022-03-16 02:38:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:43:09 | INFO | train_inner | epoch 027:    193 / 407 loss=5.831, ppl=56.93, wps=23813.8, ups=0.36, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.48, loss_scale=16, train_wall=251, gb_free=9.6, wall=35948
2022-03-16 02:47:42 | INFO | train_inner | epoch 027:    293 / 407 loss=5.827, ppl=56.77, wps=24007.6, ups=0.37, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.485, loss_scale=32, train_wall=250, gb_free=9.6, wall=36221
2022-03-16 02:47:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:52:18 | INFO | train_inner | epoch 027:    394 / 407 loss=5.856, ppl=57.93, wps=23774.9, ups=0.36, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.481, loss_scale=16, train_wall=252, gb_free=9.6, wall=36497
2022-03-16 02:52:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:53:30 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.926 | ppl 60.8 | wps 35728.7 | wpb 511.9 | bsz 1 | num_updates 10913 | best_loss 5.926
2022-03-16 02:53:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10913 updates
2022-03-16 02:53:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 02:53:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 02:53:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 27 @ 10913 updates, score 5.926) (writing took 2.224260677001439 seconds)
2022-03-16 02:53:32 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-16 02:53:32 | INFO | train | epoch 027 | loss 5.831 | ppl 56.94 | wps 23021.1 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 10913 | lr 0.000302711 | gnorm 0.484 | loss_scale 16 | train_wall 1014 | gb_free 9.6 | wall 36571
KL Stats: Epoch 27 Divergences: Uniform: 4.611241787221853 Unigram: 4.021865584609534
2022-03-16 02:53:32 | INFO | fairseq.trainer | begin training epoch 28
2022-03-16 02:53:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:57:30 | INFO | train_inner | epoch 028:     87 / 407 loss=5.782, ppl=55.02, wps=20938.2, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=11000, lr=0.000301511, gnorm=0.485, loss_scale=32, train_wall=249, gb_free=9.6, wall=36809
2022-03-16 03:00:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:02:06 | INFO | train_inner | epoch 028:    188 / 407 loss=5.799, ppl=55.68, wps=23718.9, ups=0.36, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.487, loss_scale=32, train_wall=253, gb_free=9.6, wall=37085
2022-03-16 03:06:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:06:41 | INFO | train_inner | epoch 028:    289 / 407 loss=5.826, ppl=56.74, wps=23810.4, ups=0.36, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.488, loss_scale=32, train_wall=252, gb_free=9.6, wall=37360
2022-03-16 03:11:14 | INFO | train_inner | epoch 028:    389 / 407 loss=5.824, ppl=56.66, wps=24025.9, ups=0.37, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.485, loss_scale=32, train_wall=249, gb_free=9.6, wall=37633
2022-03-16 03:11:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:12:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:12:40 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.915 | ppl 60.33 | wps 35701.3 | wpb 511.9 | bsz 1 | num_updates 11317 | best_loss 5.915
2022-03-16 03:12:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 11317 updates
2022-03-16 03:12:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 03:12:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 03:12:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 28 @ 11317 updates, score 5.915) (writing took 2.2178113639820367 seconds)
2022-03-16 03:12:42 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-16 03:12:42 | INFO | train | epoch 028 | loss 5.809 | ppl 56.07 | wps 23004.4 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 11317 | lr 0.000297259 | gnorm 0.487 | loss_scale 32 | train_wall 1015 | gb_free 9.6 | wall 37721
KL Stats: Epoch 28 Divergences: Uniform: 4.6228922949669 Unigram: 4.033943715751719
2022-03-16 03:12:42 | INFO | fairseq.trainer | begin training epoch 29
2022-03-16 03:12:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:16:29 | INFO | train_inner | epoch 029:     83 / 407 loss=5.778, ppl=54.87, wps=20771.8, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=11400, lr=0.000296174, gnorm=0.491, loss_scale=32, train_wall=251, gb_free=9.6, wall=37948
2022-03-16 03:18:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:21:04 | INFO | train_inner | epoch 029:    184 / 407 loss=5.796, ppl=55.55, wps=23799.5, ups=0.36, wpb=65534.2, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.487, loss_scale=32, train_wall=252, gb_free=9.6, wall=38223
2022-03-16 03:24:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:25:39 | INFO | train_inner | epoch 029:    285 / 407 loss=5.78, ppl=54.93, wps=23821.7, ups=0.36, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.488, loss_scale=32, train_wall=251, gb_free=9.6, wall=38498
2022-03-16 03:28:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:30:15 | INFO | train_inner | epoch 029:    386 / 407 loss=5.805, ppl=55.89, wps=23791.9, ups=0.36, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.489, loss_scale=16, train_wall=252, gb_free=9.6, wall=38774
2022-03-16 03:31:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:31:49 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.9 | ppl 59.73 | wps 35673.1 | wpb 511.9 | bsz 1 | num_updates 11721 | best_loss 5.9
2022-03-16 03:31:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11721 updates
2022-03-16 03:31:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 03:31:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 03:31:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 29 @ 11721 updates, score 5.9) (writing took 2.235880105989054 seconds)
2022-03-16 03:31:51 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-16 03:31:51 | INFO | train | epoch 029 | loss 5.789 | ppl 55.29 | wps 23035 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 11721 | lr 0.000292091 | gnorm 0.488 | loss_scale 16 | train_wall 1013 | gb_free 9.6 | wall 38870
KL Stats: Epoch 29 Divergences: Uniform: 4.636643040162437 Unigram: 4.047159065742898
2022-03-16 03:31:51 | INFO | fairseq.trainer | begin training epoch 30
2022-03-16 03:31:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:35:27 | INFO | train_inner | epoch 030:     79 / 407 loss=5.765, ppl=54.39, wps=20958.5, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=11800, lr=0.000291111, gnorm=0.486, loss_scale=32, train_wall=249, gb_free=9.6, wall=39086
2022-03-16 03:39:59 | INFO | train_inner | epoch 030:    179 / 407 loss=5.755, ppl=54.02, wps=24041.3, ups=0.37, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.487, loss_scale=32, train_wall=249, gb_free=9.6, wall=39358
2022-03-16 03:40:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:44:34 | INFO | train_inner | epoch 030:    280 / 407 loss=5.778, ppl=54.88, wps=23814.4, ups=0.36, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.485, loss_scale=32, train_wall=252, gb_free=9.6, wall=39633
2022-03-16 03:46:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:49:10 | INFO | train_inner | epoch 030:    381 / 407 loss=5.786, ppl=55.18, wps=23798.6, ups=0.36, wpb=65534.2, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.496, loss_scale=32, train_wall=252, gb_free=9.6, wall=39909
2022-03-16 03:50:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:50:58 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.891 | ppl 59.34 | wps 35619.6 | wpb 511.9 | bsz 1 | num_updates 12126 | best_loss 5.891
2022-03-16 03:50:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 12126 updates
2022-03-16 03:50:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 03:50:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 03:51:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 30 @ 12126 updates, score 5.891) (writing took 2.2869725179625675 seconds)
2022-03-16 03:51:00 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-16 03:51:00 | INFO | train | epoch 030 | loss 5.771 | ppl 54.59 | wps 23085.5 | ups 0.35 | wpb 65492.6 | bsz 127.9 | num_updates 12126 | lr 0.000287171 | gnorm 0.489 | loss_scale 32 | train_wall 1014 | gb_free 9.6 | wall 40019
KL Stats: Epoch 30 Divergences: Uniform: 4.648570145134489 Unigram: 4.059579330026955
2022-03-16 03:51:00 | INFO | fairseq.trainer | begin training epoch 31
2022-03-16 03:51:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:53:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:54:24 | INFO | train_inner | epoch 031:     75 / 407 loss=5.734, ppl=53.21, wps=20766.8, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=12200, lr=0.000286299, gnorm=0.489, loss_scale=32, train_wall=251, gb_free=9.6, wall=40224
2022-03-16 03:58:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 03:59:00 | INFO | train_inner | epoch 031:    176 / 407 loss=5.745, ppl=53.62, wps=23798.8, ups=0.36, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.493, loss_scale=32, train_wall=252, gb_free=9.6, wall=40499
2022-03-16 04:03:32 | INFO | train_inner | epoch 031:    276 / 407 loss=5.762, ppl=54.25, wps=24048.1, ups=0.37, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.489, loss_scale=32, train_wall=249, gb_free=9.6, wall=40771
2022-03-16 04:04:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:08:07 | INFO | train_inner | epoch 031:    377 / 407 loss=5.777, ppl=54.85, wps=23833.9, ups=0.36, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.493, loss_scale=32, train_wall=251, gb_free=9.6, wall=41046
2022-03-16 04:09:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:10:06 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.881 | ppl 58.94 | wps 35694.2 | wpb 511.9 | bsz 1 | num_updates 12530 | best_loss 5.881
2022-03-16 04:10:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12530 updates
2022-03-16 04:10:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 04:10:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 04:10:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 31 @ 12530 updates, score 5.881) (writing took 2.22044808906503 seconds)
2022-03-16 04:10:08 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-16 04:10:08 | INFO | train | epoch 031 | loss 5.753 | ppl 53.93 | wps 23044.3 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 12530 | lr 0.000282504 | gnorm 0.492 | loss_scale 32 | train_wall 1013 | gb_free 9.6 | wall 41167
KL Stats: Epoch 31 Divergences: Uniform: 4.6591572265724785 Unigram: 4.070637151749009
2022-03-16 04:10:08 | INFO | fairseq.trainer | begin training epoch 32
2022-03-16 04:10:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:11:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:13:22 | INFO | train_inner | epoch 032:     71 / 407 loss=5.723, ppl=52.83, wps=20782.7, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=12600, lr=0.000281718, gnorm=0.49, loss_scale=32, train_wall=251, gb_free=9.6, wall=41361
2022-03-16 04:17:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:17:57 | INFO | train_inner | epoch 032:    172 / 407 loss=5.724, ppl=52.86, wps=23813.9, ups=0.36, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.489, loss_scale=32, train_wall=251, gb_free=9.6, wall=41636
2022-03-16 04:22:29 | INFO | train_inner | epoch 032:    272 / 407 loss=5.742, ppl=53.5, wps=24080, ups=0.37, wpb=65534.2, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.489, loss_scale=32, train_wall=249, gb_free=9.6, wall=41908
2022-03-16 04:22:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:27:04 | INFO | train_inner | epoch 032:    373 / 407 loss=5.758, ppl=54.13, wps=23816.4, ups=0.36, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.489, loss_scale=32, train_wall=252, gb_free=9.6, wall=42183
2022-03-16 04:28:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:29:14 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.871 | ppl 58.51 | wps 35739.2 | wpb 511.9 | bsz 1 | num_updates 12934 | best_loss 5.871
2022-03-16 04:29:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12934 updates
2022-03-16 04:29:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 04:29:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 04:29:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 32 @ 12934 updates, score 5.871) (writing took 2.200581460027024 seconds)
2022-03-16 04:29:16 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-16 04:29:16 | INFO | train | epoch 032 | loss 5.735 | ppl 53.27 | wps 23049 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 12934 | lr 0.000278057 | gnorm 0.49 | loss_scale 32 | train_wall 1013 | gb_free 9.6 | wall 42315
KL Stats: Epoch 32 Divergences: Uniform: 4.671650027525299 Unigram: 4.081201533685899
2022-03-16 04:29:16 | INFO | fairseq.trainer | begin training epoch 33
2022-03-16 04:29:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:29:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:32:19 | INFO | train_inner | epoch 033:     67 / 407 loss=5.709, ppl=52.32, wps=20786.4, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=13000, lr=0.00027735, gnorm=0.497, loss_scale=32, train_wall=251, gb_free=9.6, wall=42498
2022-03-16 04:35:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:36:54 | INFO | train_inner | epoch 033:    168 / 407 loss=5.71, ppl=52.34, wps=23778.4, ups=0.36, wpb=65534.2, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.491, loss_scale=32, train_wall=252, gb_free=9.6, wall=42773
2022-03-16 04:41:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:41:30 | INFO | train_inner | epoch 033:    269 / 407 loss=5.728, ppl=52.99, wps=23807.8, ups=0.36, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.487, loss_scale=32, train_wall=252, gb_free=9.6, wall=43049
2022-03-16 04:46:02 | INFO | train_inner | epoch 033:    369 / 407 loss=5.733, ppl=53.2, wps=24066.4, ups=0.37, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.492, loss_scale=32, train_wall=249, gb_free=9.6, wall=43321
2022-03-16 04:47:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:47:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:48:22 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.859 | ppl 58.04 | wps 35617.2 | wpb 511.9 | bsz 1 | num_updates 13337 | best_loss 5.859
2022-03-16 04:48:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 13337 updates
2022-03-16 04:48:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 04:48:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 04:48:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 33 @ 13337 updates, score 5.859) (writing took 2.205396046047099 seconds)
2022-03-16 04:48:25 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-16 04:48:25 | INFO | train | epoch 033 | loss 5.72 | ppl 52.71 | wps 22977.2 | ups 0.35 | wpb 65492.3 | bsz 127.9 | num_updates 13337 | lr 0.000273824 | gnorm 0.491 | loss_scale 32 | train_wall 1013 | gb_free 9.6 | wall 43464
KL Stats: Epoch 33 Divergences: Uniform: 4.683445006721421 Unigram: 4.091720986079069
2022-03-16 04:48:25 | INFO | fairseq.trainer | begin training epoch 34
2022-03-16 04:48:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:51:17 | INFO | train_inner | epoch 034:     63 / 407 loss=5.699, ppl=51.93, wps=20782.7, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=13400, lr=0.000273179, gnorm=0.492, loss_scale=32, train_wall=251, gb_free=9.6, wall=43636
2022-03-16 04:53:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 04:55:52 | INFO | train_inner | epoch 034:    164 / 407 loss=5.688, ppl=51.55, wps=23795.2, ups=0.36, wpb=65534.2, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.491, loss_scale=32, train_wall=252, gb_free=9.6, wall=43911
2022-03-16 04:59:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:00:27 | INFO | train_inner | epoch 034:    265 / 407 loss=5.716, ppl=52.56, wps=23819.2, ups=0.36, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.492, loss_scale=32, train_wall=252, gb_free=9.6, wall=44186
2022-03-16 05:05:00 | INFO | train_inner | epoch 034:    365 / 407 loss=5.725, ppl=52.88, wps=24003.9, ups=0.37, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.486, loss_scale=32, train_wall=250, gb_free=9.6, wall=44459
2022-03-16 05:05:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:06:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:07:31 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.854 | ppl 57.84 | wps 35657.1 | wpb 511.9 | bsz 1 | num_updates 13741 | best_loss 5.854
2022-03-16 05:07:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13741 updates
2022-03-16 05:07:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 05:07:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 05:07:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 34 @ 13741 updates, score 5.854) (writing took 2.2433793409727514 seconds)
2022-03-16 05:07:34 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-16 05:07:34 | INFO | train | epoch 034 | loss 5.704 | ppl 52.14 | wps 23028.7 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 13741 | lr 0.000269768 | gnorm 0.49 | loss_scale 32 | train_wall 1014 | gb_free 9.6 | wall 44613
KL Stats: Epoch 34 Divergences: Uniform: 4.693767865046495 Unigram: 4.103012995209446
2022-03-16 05:07:34 | INFO | fairseq.trainer | begin training epoch 35
2022-03-16 05:07:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:10:15 | INFO | train_inner | epoch 035:     59 / 407 loss=5.679, ppl=51.23, wps=20768.6, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=13800, lr=0.000269191, gnorm=0.489, loss_scale=32, train_wall=251, gb_free=9.6, wall=44774
2022-03-16 05:11:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:14:50 | INFO | train_inner | epoch 035:    160 / 407 loss=5.682, ppl=51.36, wps=23777.5, ups=0.36, wpb=65534.2, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.488, loss_scale=32, train_wall=252, gb_free=9.6, wall=45049
2022-03-16 05:17:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:19:26 | INFO | train_inner | epoch 035:    261 / 407 loss=5.698, ppl=51.92, wps=23798.8, ups=0.36, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.494, loss_scale=32, train_wall=252, gb_free=9.6, wall=45325
2022-03-16 05:23:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:24:01 | INFO | train_inner | epoch 035:    362 / 407 loss=5.709, ppl=52.3, wps=23778, ups=0.36, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.492, loss_scale=32, train_wall=252, gb_free=9.6, wall=45600
2022-03-16 05:26:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:26:41 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.845 | ppl 57.48 | wps 35626 | wpb 511.9 | bsz 1 | num_updates 14145 | best_loss 5.845
2022-03-16 05:26:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 14145 updates
2022-03-16 05:26:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 05:26:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 05:26:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 35 @ 14145 updates, score 5.845) (writing took 2.2402924690395594 seconds)
2022-03-16 05:26:43 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-16 05:26:43 | INFO | train | epoch 035 | loss 5.691 | ppl 51.66 | wps 23010.3 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 14145 | lr 0.000265888 | gnorm 0.492 | loss_scale 32 | train_wall 1015 | gb_free 9.6 | wall 45763
KL Stats: Epoch 35 Divergences: Uniform: 4.702314641724272 Unigram: 4.111770084111599
2022-03-16 05:26:44 | INFO | fairseq.trainer | begin training epoch 36
2022-03-16 05:26:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:29:14 | INFO | train_inner | epoch 036:     55 / 407 loss=5.678, ppl=51.2, wps=20937, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=14200, lr=0.000265372, gnorm=0.499, loss_scale=32, train_wall=249, gb_free=9.6, wall=45913
2022-03-16 05:30:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:33:49 | INFO | train_inner | epoch 036:    156 / 407 loss=5.667, ppl=50.8, wps=23785.5, ups=0.36, wpb=65534.2, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.491, loss_scale=32, train_wall=252, gb_free=9.6, wall=46188
2022-03-16 05:36:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:38:25 | INFO | train_inner | epoch 036:    257 / 407 loss=5.675, ppl=51.08, wps=23768.4, ups=0.36, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.496, loss_scale=32, train_wall=252, gb_free=9.6, wall=46464
2022-03-16 05:41:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:43:00 | INFO | train_inner | epoch 036:    358 / 407 loss=5.693, ppl=51.75, wps=23808.4, ups=0.36, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.498, loss_scale=32, train_wall=252, gb_free=9.6, wall=46739
2022-03-16 05:45:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:45:51 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.837 | ppl 57.16 | wps 35715.7 | wpb 511.9 | bsz 1 | num_updates 14549 | best_loss 5.837
2022-03-16 05:45:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14549 updates
2022-03-16 05:45:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 05:45:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 05:45:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 36 @ 14549 updates, score 5.837) (writing took 2.2259994839550927 seconds)
2022-03-16 05:45:53 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-16 05:45:53 | INFO | train | epoch 036 | loss 5.677 | ppl 51.15 | wps 23021.5 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 14549 | lr 0.00026217 | gnorm 0.495 | loss_scale 32 | train_wall 1014 | gb_free 9.6 | wall 46912
KL Stats: Epoch 36 Divergences: Uniform: 4.712811884657187 Unigram: 4.120188342034259
2022-03-16 05:45:53 | INFO | fairseq.trainer | begin training epoch 37
2022-03-16 05:45:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:48:12 | INFO | train_inner | epoch 037:     51 / 407 loss=5.656, ppl=50.41, wps=20964.9, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=14600, lr=0.000261712, gnorm=0.498, loss_scale=32, train_wall=248, gb_free=9.6, wall=47051
2022-03-16 05:48:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:52:47 | INFO | train_inner | epoch 037:    152 / 407 loss=5.65, ppl=50.22, wps=23813.7, ups=0.36, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.489, loss_scale=32, train_wall=251, gb_free=9.6, wall=47326
2022-03-16 05:54:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 05:57:22 | INFO | train_inner | epoch 037:    253 / 407 loss=5.666, ppl=50.78, wps=23808, ups=0.36, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.494, loss_scale=32, train_wall=252, gb_free=9.6, wall=47601
2022-03-16 06:00:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:01:58 | INFO | train_inner | epoch 037:    354 / 407 loss=5.685, ppl=51.43, wps=23810, ups=0.36, wpb=65534.2, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.491, loss_scale=32, train_wall=252, gb_free=9.6, wall=47877
2022-03-16 06:04:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:04:59 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.825 | ppl 56.68 | wps 35661.6 | wpb 511.9 | bsz 1 | num_updates 14953 | best_loss 5.825
2022-03-16 06:04:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14953 updates
2022-03-16 06:04:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 06:05:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 06:05:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 37 @ 14953 updates, score 5.825) (writing took 2.2204390399856493 seconds)
2022-03-16 06:05:01 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-16 06:05:01 | INFO | train | epoch 037 | loss 5.664 | ppl 50.69 | wps 23041.9 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 14953 | lr 0.000258604 | gnorm 0.493 | loss_scale 32 | train_wall 1013 | gb_free 9.6 | wall 48060
KL Stats: Epoch 37 Divergences: Uniform: 4.719503581861404 Unigram: 4.127527591427756
2022-03-16 06:05:01 | INFO | fairseq.trainer | begin training epoch 38
2022-03-16 06:05:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:06:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:07:12 | INFO | train_inner | epoch 038:     48 / 407 loss=5.651, ppl=50.24, wps=20768, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=0.496, loss_scale=32, train_wall=251, gb_free=9.6, wall=48191
2022-03-16 06:11:45 | INFO | train_inner | epoch 038:    148 / 407 loss=5.629, ppl=49.49, wps=24047.1, ups=0.37, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.491, loss_scale=32, train_wall=249, gb_free=9.6, wall=48464
2022-03-16 06:12:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:16:20 | INFO | train_inner | epoch 038:    249 / 407 loss=5.666, ppl=50.76, wps=23798.3, ups=0.36, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.496, loss_scale=32, train_wall=252, gb_free=9.6, wall=48739
2022-03-16 06:18:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:20:55 | INFO | train_inner | epoch 038:    350 / 407 loss=5.663, ppl=50.68, wps=23829, ups=0.36, wpb=65534.2, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.494, loss_scale=32, train_wall=251, gb_free=9.6, wall=49014
2022-03-16 06:23:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:24:07 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.823 | ppl 56.59 | wps 35657.7 | wpb 511.9 | bsz 1 | num_updates 15357 | best_loss 5.823
2022-03-16 06:24:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 15357 updates
2022-03-16 06:24:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 06:24:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 06:24:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 38 @ 15357 updates, score 5.823) (writing took 2.2197718349052593 seconds)
2022-03-16 06:24:09 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-16 06:24:09 | INFO | train | epoch 038 | loss 5.651 | ppl 50.27 | wps 23039.9 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 15357 | lr 0.00025518 | gnorm 0.494 | loss_scale 32 | train_wall 1013 | gb_free 9.6 | wall 49209
KL Stats: Epoch 38 Divergences: Uniform: 4.730181662000934 Unigram: 4.138424874422743
2022-03-16 06:24:10 | INFO | fairseq.trainer | begin training epoch 39
2022-03-16 06:24:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:24:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:26:10 | INFO | train_inner | epoch 039:     44 / 407 loss=5.652, ppl=50.27, wps=20793, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=0.498, loss_scale=32, train_wall=251, gb_free=9.6, wall=49329
2022-03-16 06:30:42 | INFO | train_inner | epoch 039:    144 / 407 loss=5.626, ppl=49.4, wps=24052, ups=0.37, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.501, loss_scale=32, train_wall=249, gb_free=9.6, wall=49601
2022-03-16 06:30:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:35:17 | INFO | train_inner | epoch 039:    245 / 407 loss=5.64, ppl=49.87, wps=23832.7, ups=0.36, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.496, loss_scale=32, train_wall=251, gb_free=9.6, wall=49876
2022-03-16 06:36:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:39:52 | INFO | train_inner | epoch 039:    346 / 407 loss=5.654, ppl=50.36, wps=23846.3, ups=0.36, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.501, loss_scale=32, train_wall=251, gb_free=9.6, wall=50151
2022-03-16 06:42:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:42:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:43:15 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.815 | ppl 56.3 | wps 35718.9 | wpb 511.9 | bsz 1 | num_updates 15760 | best_loss 5.815
2022-03-16 06:43:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15760 updates
2022-03-16 06:43:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 06:43:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 06:43:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 39 @ 15760 updates, score 5.815) (writing took 2.202663647942245 seconds)
2022-03-16 06:43:18 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-16 06:43:18 | INFO | train | epoch 039 | loss 5.64 | ppl 49.87 | wps 22988.2 | ups 0.35 | wpb 65492.3 | bsz 127.9 | num_updates 15760 | lr 0.000251896 | gnorm 0.498 | loss_scale 32 | train_wall 1013 | gb_free 9.6 | wall 50357
KL Stats: Epoch 39 Divergences: Uniform: 4.738774324570838 Unigram: 4.14549957118923
2022-03-16 06:43:18 | INFO | fairseq.trainer | begin training epoch 40
2022-03-16 06:43:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:45:07 | INFO | train_inner | epoch 040:     40 / 407 loss=5.635, ppl=49.69, wps=20754.3, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=0.493, loss_scale=32, train_wall=252, gb_free=9.6, wall=50466
2022-03-16 06:49:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:49:42 | INFO | train_inner | epoch 040:    141 / 407 loss=5.607, ppl=48.74, wps=23827.3, ups=0.36, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.505, loss_scale=32, train_wall=251, gb_free=9.6, wall=50741
2022-03-16 06:54:14 | INFO | train_inner | epoch 040:    241 / 407 loss=5.623, ppl=49.28, wps=24047.3, ups=0.37, wpb=65534.2, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.498, loss_scale=32, train_wall=249, gb_free=9.6, wall=51013
2022-03-16 06:54:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 06:58:49 | INFO | train_inner | epoch 040:    342 / 407 loss=5.649, ppl=50.2, wps=23856.7, ups=0.36, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.496, loss_scale=32, train_wall=251, gb_free=9.6, wall=51288
2022-03-16 07:00:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:01:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:02:23 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.811 | ppl 56.15 | wps 35617.9 | wpb 511.9 | bsz 1 | num_updates 16164 | best_loss 5.811
2022-03-16 07:02:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 16164 updates
2022-03-16 07:02:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 07:02:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 07:02:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 40 @ 16164 updates, score 5.811) (writing took 2.225128803984262 seconds)
2022-03-16 07:02:25 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-16 07:02:25 | INFO | train | epoch 040 | loss 5.628 | ppl 49.46 | wps 23056.2 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 16164 | lr 0.000248729 | gnorm 0.499 | loss_scale 32 | train_wall 1012 | gb_free 9.6 | wall 51504
KL Stats: Epoch 40 Divergences: Uniform: 4.746837923714343 Unigram: 4.152685067897912
2022-03-16 07:02:25 | INFO | fairseq.trainer | begin training epoch 41
2022-03-16 07:02:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:04:04 | INFO | train_inner | epoch 041:     36 / 407 loss=5.624, ppl=49.32, wps=20784.6, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=0.503, loss_scale=32, train_wall=251, gb_free=9.6, wall=51603
2022-03-16 07:07:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:08:39 | INFO | train_inner | epoch 041:    137 / 407 loss=5.608, ppl=48.78, wps=23761.7, ups=0.36, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.493, loss_scale=32, train_wall=252, gb_free=9.6, wall=51878
2022-03-16 07:13:14 | INFO | train_inner | epoch 041:    237 / 407 loss=5.619, ppl=49.15, wps=23828.7, ups=0.36, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.496, loss_scale=64, train_wall=252, gb_free=9.6, wall=52153
2022-03-16 07:13:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:17:53 | INFO | train_inner | epoch 041:    338 / 407 loss=5.631, ppl=49.56, wps=23539.3, ups=0.36, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.502, loss_scale=32, train_wall=255, gb_free=9.6, wall=52432
2022-03-16 07:19:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:21:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:21:40 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 5.808 | ppl 56.04 | wps 35480.8 | wpb 511.9 | bsz 1 | num_updates 16568 | best_loss 5.808
2022-03-16 07:21:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16568 updates
2022-03-16 07:21:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 07:21:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 07:21:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 41 @ 16568 updates, score 5.808) (writing took 2.21573996799998 seconds)
2022-03-16 07:21:42 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-16 07:21:42 | INFO | train | epoch 041 | loss 5.618 | ppl 49.1 | wps 22865.4 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 16568 | lr 0.000245677 | gnorm 0.498 | loss_scale 32 | train_wall 1021 | gb_free 9.6 | wall 52661
KL Stats: Epoch 41 Divergences: Uniform: 4.7554670931747625 Unigram: 4.162335007218118
2022-03-16 07:21:42 | INFO | fairseq.trainer | begin training epoch 42
2022-03-16 07:21:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:23:11 | INFO | train_inner | epoch 042:     32 / 407 loss=5.615, ppl=49.02, wps=20551.2, ups=0.31, wpb=65361.9, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=0.497, loss_scale=32, train_wall=254, gb_free=9.6, wall=52750
2022-03-16 07:25:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:27:50 | INFO | train_inner | epoch 042:    133 / 407 loss=5.589, ppl=48.12, wps=23495.9, ups=0.36, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.5, loss_scale=32, train_wall=255, gb_free=9.6, wall=53029
2022-03-16 07:31:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:32:28 | INFO | train_inner | epoch 042:    234 / 407 loss=5.601, ppl=48.55, wps=23551.5, ups=0.36, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.501, loss_scale=32, train_wall=255, gb_free=9.6, wall=53307
2022-03-16 07:37:03 | INFO | train_inner | epoch 042:    334 / 407 loss=5.629, ppl=49.47, wps=23801.7, ups=0.36, wpb=65534.2, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.498, loss_scale=32, train_wall=252, gb_free=9.6, wall=53582
2022-03-16 07:37:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:40:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:41:01 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 5.801 | ppl 55.77 | wps 35509.1 | wpb 511.9 | bsz 1 | num_updates 16972 | best_loss 5.801
2022-03-16 07:41:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16972 updates
2022-03-16 07:41:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 07:41:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 07:41:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 42 @ 16972 updates, score 5.801) (writing took 2.2119513700017706 seconds)
2022-03-16 07:41:03 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-16 07:41:03 | INFO | train | epoch 042 | loss 5.607 | ppl 48.74 | wps 22792.8 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 16972 | lr 0.000242736 | gnorm 0.499 | loss_scale 32 | train_wall 1025 | gb_free 9.6 | wall 53822
KL Stats: Epoch 42 Divergences: Uniform: 4.761334134646825 Unigram: 4.167652785258609
2022-03-16 07:41:03 | INFO | fairseq.trainer | begin training epoch 43
2022-03-16 07:41:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:42:20 | INFO | train_inner | epoch 043:     28 / 407 loss=5.607, ppl=48.73, wps=20617.8, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=0.496, loss_scale=32, train_wall=253, gb_free=9.6, wall=53899
2022-03-16 07:44:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:46:59 | INFO | train_inner | epoch 043:    129 / 407 loss=5.58, ppl=47.84, wps=23549, ups=0.36, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.499, loss_scale=32, train_wall=254, gb_free=9.6, wall=54178
2022-03-16 07:50:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:51:37 | INFO | train_inner | epoch 043:    230 / 407 loss=5.601, ppl=48.55, wps=23567.9, ups=0.36, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.501, loss_scale=32, train_wall=254, gb_free=9.6, wall=54456
2022-03-16 07:56:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 07:56:15 | INFO | train_inner | epoch 043:    331 / 407 loss=5.605, ppl=48.66, wps=23560.7, ups=0.36, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.501, loss_scale=32, train_wall=254, gb_free=9.6, wall=54734
2022-03-16 07:59:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:00:21 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 5.8 | ppl 55.71 | wps 35529.2 | wpb 511.9 | bsz 1 | num_updates 17376 | best_loss 5.8
2022-03-16 08:00:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 17376 updates
2022-03-16 08:00:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 08:00:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 08:00:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 43 @ 17376 updates, score 5.8) (writing took 2.2199938049307093 seconds)
2022-03-16 08:00:23 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-16 08:00:23 | INFO | train | epoch 043 | loss 5.597 | ppl 48.4 | wps 22805.3 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 17376 | lr 0.000239897 | gnorm 0.499 | loss_scale 32 | train_wall 1025 | gb_free 9.6 | wall 54982
KL Stats: Epoch 43 Divergences: Uniform: 4.771666999294088 Unigram: 4.177728220826985
2022-03-16 08:00:23 | INFO | fairseq.trainer | begin training epoch 44
2022-03-16 08:00:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:01:30 | INFO | train_inner | epoch 044:     24 / 407 loss=5.6, ppl=48.49, wps=20763.7, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=0.494, loss_scale=32, train_wall=251, gb_free=9.6, wall=55049
2022-03-16 08:02:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:06:08 | INFO | train_inner | epoch 044:    125 / 407 loss=5.569, ppl=47.46, wps=23539.4, ups=0.36, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.498, loss_scale=32, train_wall=255, gb_free=9.6, wall=55327
2022-03-16 08:08:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:10:47 | INFO | train_inner | epoch 044:    226 / 407 loss=5.589, ppl=48.14, wps=23536, ups=0.36, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.501, loss_scale=32, train_wall=255, gb_free=9.6, wall=55606
2022-03-16 08:14:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:15:25 | INFO | train_inner | epoch 044:    327 / 407 loss=5.6, ppl=48.51, wps=23548.5, ups=0.36, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.502, loss_scale=32, train_wall=255, gb_free=9.6, wall=55884
2022-03-16 08:19:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:19:42 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 5.788 | ppl 55.27 | wps 35521.5 | wpb 511.9 | bsz 1 | num_updates 17780 | best_loss 5.788
2022-03-16 08:19:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17780 updates
2022-03-16 08:19:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 08:19:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 08:19:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 44 @ 17780 updates, score 5.788) (writing took 2.2479668180458248 seconds)
2022-03-16 08:19:44 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-16 08:19:44 | INFO | train | epoch 044 | loss 5.587 | ppl 48.07 | wps 22795.1 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 17780 | lr 0.000237156 | gnorm 0.501 | loss_scale 32 | train_wall 1025 | gb_free 9.6 | wall 56143
KL Stats: Epoch 44 Divergences: Uniform: 4.777900940843282 Unigram: 4.18207690454102
2022-03-16 08:19:44 | INFO | fairseq.trainer | begin training epoch 45
2022-03-16 08:19:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:20:39 | INFO | train_inner | epoch 045:     20 / 407 loss=5.587, ppl=48.08, wps=20788.1, ups=0.32, wpb=65360.1, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=0.503, loss_scale=32, train_wall=251, gb_free=9.6, wall=56198
2022-03-16 08:21:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:25:17 | INFO | train_inner | epoch 045:    121 / 407 loss=5.551, ppl=46.87, wps=23562.9, ups=0.36, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.503, loss_scale=32, train_wall=254, gb_free=9.6, wall=56476
2022-03-16 08:27:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:29:56 | INFO | train_inner | epoch 045:    222 / 407 loss=5.569, ppl=47.49, wps=23543.6, ups=0.36, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.503, loss_scale=32, train_wall=255, gb_free=9.6, wall=56755
2022-03-16 08:33:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:34:34 | INFO | train_inner | epoch 045:    323 / 407 loss=5.605, ppl=48.67, wps=23575.2, ups=0.36, wpb=65534.2, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.501, loss_scale=32, train_wall=254, gb_free=9.6, wall=57033
2022-03-16 08:38:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:39:02 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 5.792 | ppl 55.42 | wps 35500.2 | wpb 511.9 | bsz 1 | num_updates 18184 | best_loss 5.788
2022-03-16 08:39:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 18184 updates
2022-03-16 08:39:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 08:39:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 08:39:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt (epoch 45 @ 18184 updates, score 5.792) (writing took 1.2581286020576954 seconds)
2022-03-16 08:39:03 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-16 08:39:03 | INFO | train | epoch 045 | loss 5.578 | ppl 47.77 | wps 22825.5 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 18184 | lr 0.000234507 | gnorm 0.503 | loss_scale 32 | train_wall 1025 | gb_free 9.6 | wall 57302
KL Stats: Epoch 45 Divergences: Uniform: 4.786703036749412 Unigram: 4.192730985546673
2022-03-16 08:39:03 | INFO | fairseq.trainer | begin training epoch 46
2022-03-16 08:39:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:39:47 | INFO | train_inner | epoch 046:     16 / 407 loss=5.59, ppl=48.17, wps=20833.8, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=0.504, loss_scale=64, train_wall=251, gb_free=9.6, wall=57347
2022-03-16 08:39:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:44:25 | INFO | train_inner | epoch 046:    117 / 407 loss=5.549, ppl=46.81, wps=23599.8, ups=0.36, wpb=65534.2, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.507, loss_scale=32, train_wall=254, gb_free=9.6, wall=57624
2022-03-16 08:45:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:49:03 | INFO | train_inner | epoch 046:    218 / 407 loss=5.569, ppl=47.49, wps=23580.3, ups=0.36, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.501, loss_scale=32, train_wall=254, gb_free=9.6, wall=57902
2022-03-16 08:51:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:53:41 | INFO | train_inner | epoch 046:    319 / 407 loss=5.58, ppl=47.82, wps=23569.4, ups=0.36, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.507, loss_scale=32, train_wall=254, gb_free=9.6, wall=58180
2022-03-16 08:57:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 08:57:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:58:20 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 5.777 | ppl 54.82 | wps 35411.5 | wpb 511.9 | bsz 1 | num_updates 18587 | best_loss 5.777
2022-03-16 08:58:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18587 updates
2022-03-16 08:58:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 08:58:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 08:58:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 46 @ 18587 updates, score 5.777) (writing took 3.205875421059318 seconds)
2022-03-16 08:58:24 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-16 08:58:24 | INFO | train | epoch 046 | loss 5.569 | ppl 47.46 | wps 22747.4 | ups 0.35 | wpb 65492.3 | bsz 127.9 | num_updates 18587 | lr 0.000231951 | gnorm 0.504 | loss_scale 32 | train_wall 1024 | gb_free 9.6 | wall 58463
KL Stats: Epoch 46 Divergences: Uniform: 4.790734076210642 Unigram: 4.194822543431815
2022-03-16 08:58:24 | INFO | fairseq.trainer | begin training epoch 47
2022-03-16 08:58:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:59:00 | INFO | train_inner | epoch 047:     13 / 407 loss=5.581, ppl=47.87, wps=20527.3, ups=0.31, wpb=65361.9, bsz=127.7, num_updates=18600, lr=0.000231869, gnorm=0.501, loss_scale=32, train_wall=254, gb_free=9.6, wall=58499
2022-03-16 09:03:35 | INFO | train_inner | epoch 047:    113 / 407 loss=5.533, ppl=46.29, wps=23755.8, ups=0.36, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.502, loss_scale=32, train_wall=252, gb_free=9.6, wall=58775
2022-03-16 09:04:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:08:13 | INFO | train_inner | epoch 047:    214 / 407 loss=5.547, ppl=46.74, wps=23578.6, ups=0.36, wpb=65534.2, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.505, loss_scale=32, train_wall=254, gb_free=9.6, wall=59052
2022-03-16 09:10:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:12:52 | INFO | train_inner | epoch 047:    315 / 407 loss=5.58, ppl=47.84, wps=23545.5, ups=0.36, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.504, loss_scale=32, train_wall=255, gb_free=9.6, wall=59331
2022-03-16 09:16:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:17:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:17:42 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 5.778 | ppl 54.86 | wps 35592.5 | wpb 511.9 | bsz 1 | num_updates 18991 | best_loss 5.777
2022-03-16 09:17:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18991 updates
2022-03-16 09:17:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 09:17:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 09:17:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt (epoch 47 @ 18991 updates, score 5.778) (writing took 1.2324478190857917 seconds)
2022-03-16 09:17:43 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-16 09:17:43 | INFO | train | epoch 047 | loss 5.561 | ppl 47.21 | wps 22818.8 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 18991 | lr 0.00022947 | gnorm 0.502 | loss_scale 32 | train_wall 1025 | gb_free 9.6 | wall 59622
KL Stats: Epoch 47 Divergences: Uniform: 4.800997289582066 Unigram: 4.204804606553803
2022-03-16 09:17:43 | INFO | fairseq.trainer | begin training epoch 48
2022-03-16 09:17:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:18:08 | INFO | train_inner | epoch 048:      9 / 407 loss=5.585, ppl=48, wps=20656.7, ups=0.32, wpb=65361.9, bsz=127.7, num_updates=19000, lr=0.000229416, gnorm=0.498, loss_scale=32, train_wall=254, gb_free=9.6, wall=59647
2022-03-16 09:22:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:22:46 | INFO | train_inner | epoch 048:    110 / 407 loss=5.531, ppl=46.24, wps=23578.3, ups=0.36, wpb=65534.2, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.496, loss_scale=32, train_wall=254, gb_free=9.6, wall=59925
2022-03-16 09:27:23 | INFO | train_inner | epoch 048:    210 / 407 loss=5.548, ppl=46.77, wps=23686, ups=0.36, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.499, loss_scale=32, train_wall=253, gb_free=9.6, wall=60202
2022-03-16 09:28:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:31:56 | INFO | train_inner | epoch 048:    311 / 407 loss=5.563, ppl=47.27, wps=23963, ups=0.37, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.507, loss_scale=32, train_wall=250, gb_free=9.6, wall=60475
2022-03-16 09:34:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:36:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:36:47 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 5.77 | ppl 54.59 | wps 36117 | wpb 511.9 | bsz 1 | num_updates 19395 | best_loss 5.77
2022-03-16 09:36:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 19395 updates
2022-03-16 09:36:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 09:36:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 09:36:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 48 @ 19395 updates, score 5.77) (writing took 2.305171604035422 seconds)
2022-03-16 09:36:49 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-16 09:36:49 | INFO | train | epoch 048 | loss 5.553 | ppl 46.94 | wps 23093.5 | ups 0.35 | wpb 65492.5 | bsz 127.9 | num_updates 19395 | lr 0.000227068 | gnorm 0.5 | loss_scale 32 | train_wall 1011 | gb_free 9.6 | wall 60768
KL Stats: Epoch 48 Divergences: Uniform: 4.805171136041947 Unigram: 4.207618989591512
2022-03-16 09:36:49 | INFO | fairseq.trainer | begin training epoch 49
2022-03-16 09:36:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:37:02 | INFO | train_inner | epoch 049:      5 / 407 loss=5.57, ppl=47.49, wps=21363.1, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=19400, lr=0.000227038, gnorm=0.499, loss_scale=32, train_wall=243, gb_free=9.6, wall=60781
2022-03-16 09:40:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:41:29 | INFO | train_inner | epoch 049:    106 / 407 loss=5.518, ppl=45.83, wps=24606, ups=0.38, wpb=65534.2, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.505, loss_scale=32, train_wall=243, gb_free=9.6, wall=61048
2022-03-16 09:45:50 | INFO | train_inner | epoch 049:    206 / 407 loss=5.549, ppl=46.81, wps=25035.9, ups=0.38, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.506, loss_scale=32, train_wall=239, gb_free=9.6, wall=61309
2022-03-16 09:46:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:50:15 | INFO | train_inner | epoch 049:    307 / 407 loss=5.554, ppl=46.97, wps=24789.8, ups=0.38, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.503, loss_scale=32, train_wall=241, gb_free=9.6, wall=61574
2022-03-16 09:51:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:54:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:55:12 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 5.768 | ppl 54.5 | wps 36682.6 | wpb 511.9 | bsz 1 | num_updates 19799 | best_loss 5.768
2022-03-16 09:55:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19799 updates
2022-03-16 09:55:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 09:55:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 09:55:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 49 @ 19799 updates, score 5.768) (writing took 2.3331532799638808 seconds)
2022-03-16 09:55:14 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-16 09:55:14 | INFO | train | epoch 049 | loss 5.545 | ppl 46.69 | wps 23935.9 | ups 0.37 | wpb 65492.5 | bsz 127.9 | num_updates 19799 | lr 0.000224739 | gnorm 0.504 | loss_scale 32 | train_wall 972 | gb_free 9.6 | wall 61873
KL Stats: Epoch 49 Divergences: Uniform: 4.8119728690612344 Unigram: 4.214861008938032
2022-03-16 09:55:14 | INFO | fairseq.trainer | begin training epoch 50
2022-03-16 09:55:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:55:17 | INFO | train_inner | epoch 050:      1 / 407 loss=5.559, ppl=47.14, wps=21616.1, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=19800, lr=0.000224733, gnorm=0.502, loss_scale=32, train_wall=240, gb_free=9.6, wall=61876
2022-03-16 09:58:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 09:59:42 | INFO | train_inner | epoch 050:    102 / 407 loss=5.511, ppl=45.59, wps=24775.2, ups=0.38, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.506, loss_scale=32, train_wall=241, gb_free=9.6, wall=62141
2022-03-16 10:03:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:04:06 | INFO | train_inner | epoch 050:    203 / 407 loss=5.535, ppl=46.37, wps=24757.8, ups=0.38, wpb=65534.2, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.511, loss_scale=32, train_wall=241, gb_free=9.6, wall=62405
2022-03-16 10:08:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:08:30 | INFO | train_inner | epoch 050:    304 / 407 loss=5.543, ppl=46.62, wps=24816, ups=0.38, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.505, loss_scale=16, train_wall=241, gb_free=9.6, wall=62669
2022-03-16 10:12:52 | INFO | train_inner | epoch 050:    404 / 407 loss=5.562, ppl=47.23, wps=25027.4, ups=0.38, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.506, loss_scale=16, train_wall=239, gb_free=9.6, wall=62931
2022-03-16 10:12:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:13:36 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 5.759 | ppl 54.15 | wps 36810.3 | wpb 511.9 | bsz 1 | num_updates 20203 | best_loss 5.759
2022-03-16 10:13:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 20203 updates
2022-03-16 10:13:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 10:13:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 10:13:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 50 @ 20203 updates, score 5.759) (writing took 2.2857555060181767 seconds)
2022-03-16 10:13:38 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-16 10:13:38 | INFO | train | epoch 050 | loss 5.538 | ppl 46.46 | wps 23970.8 | ups 0.37 | wpb 65492.5 | bsz 127.9 | num_updates 20203 | lr 0.000222481 | gnorm 0.507 | loss_scale 16 | train_wall 971 | gb_free 9.6 | wall 62977
KL Stats: Epoch 50 Divergences: Uniform: 4.815497144548292 Unigram: 4.217457432544605
2022-03-16 10:13:38 | INFO | fairseq.trainer | begin training epoch 51
2022-03-16 10:13:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:17:52 | INFO | train_inner | epoch 051:     97 / 407 loss=5.501, ppl=45.28, wps=21782.7, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=0.508, loss_scale=32, train_wall=238, gb_free=9.6, wall=63231
2022-03-16 10:20:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:22:17 | INFO | train_inner | epoch 051:    198 / 407 loss=5.526, ppl=46.08, wps=24756.5, ups=0.38, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.506, loss_scale=32, train_wall=241, gb_free=9.6, wall=63496
2022-03-16 10:25:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:26:44 | INFO | train_inner | epoch 051:    299 / 407 loss=5.537, ppl=46.44, wps=24562.4, ups=0.37, wpb=65536, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.508, loss_scale=32, train_wall=243, gb_free=9.6, wall=63763
2022-03-16 10:31:08 | INFO | train_inner | epoch 051:    399 / 407 loss=5.552, ppl=46.91, wps=24826.7, ups=0.38, wpb=65534.2, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.508, loss_scale=32, train_wall=241, gb_free=9.6, wall=64027
2022-03-16 10:31:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:31:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:32:05 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 5.758 | ppl 54.11 | wps 36632.7 | wpb 511.9 | bsz 1 | num_updates 20607 | best_loss 5.758
2022-03-16 10:32:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20607 updates
2022-03-16 10:32:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 10:32:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 10:32:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 51 @ 20607 updates, score 5.758) (writing took 2.1873859509360045 seconds)
2022-03-16 10:32:07 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-16 10:32:07 | INFO | train | epoch 051 | loss 5.529 | ppl 46.17 | wps 23862.1 | ups 0.36 | wpb 65492.5 | bsz 127.9 | num_updates 20607 | lr 0.000220289 | gnorm 0.507 | loss_scale 32 | train_wall 975 | gb_free 9.6 | wall 64086
KL Stats: Epoch 51 Divergences: Uniform: 4.823718152902822 Unigram: 4.2258935169282035
2022-03-16 10:32:07 | INFO | fairseq.trainer | begin training epoch 52
2022-03-16 10:32:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:36:13 | INFO | train_inner | epoch 052:     93 / 407 loss=5.505, ppl=45.4, wps=21442.2, ups=0.33, wpb=65360.1, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=0.51, loss_scale=32, train_wall=242, gb_free=9.6, wall=64332
2022-03-16 10:37:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:40:39 | INFO | train_inner | epoch 052:    194 / 407 loss=5.521, ppl=45.9, wps=24600.5, ups=0.38, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.511, loss_scale=32, train_wall=243, gb_free=9.6, wall=64598
2022-03-16 10:43:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:45:06 | INFO | train_inner | epoch 052:    295 / 407 loss=5.53, ppl=46.19, wps=24592.6, ups=0.38, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.507, loss_scale=32, train_wall=243, gb_free=9.6, wall=64865
2022-03-16 10:49:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:49:32 | INFO | train_inner | epoch 052:    396 / 407 loss=5.533, ppl=46.31, wps=24607, ups=0.38, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.51, loss_scale=32, train_wall=243, gb_free=9.6, wall=65131
2022-03-16 10:50:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:50:37 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 5.755 | ppl 54.01 | wps 36601.6 | wpb 511.9 | bsz 1 | num_updates 21011 | best_loss 5.755
2022-03-16 10:50:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 21011 updates
2022-03-16 10:50:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 10:50:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 10:50:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 52 @ 21011 updates, score 5.755) (writing took 2.1948385869618505 seconds)
2022-03-16 10:50:39 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-16 10:50:39 | INFO | train | epoch 052 | loss 5.522 | ppl 45.96 | wps 23787.9 | ups 0.36 | wpb 65492.5 | bsz 127.9 | num_updates 21011 | lr 0.000218161 | gnorm 0.51 | loss_scale 32 | train_wall 978 | gb_free 9.6 | wall 65198
KL Stats: Epoch 52 Divergences: Uniform: 4.82972137896316 Unigram: 4.23135975140904
2022-03-16 10:50:39 | INFO | fairseq.trainer | begin training epoch 53
2022-03-16 10:50:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:54:34 | INFO | train_inner | epoch 053:     89 / 407 loss=5.493, ppl=45.02, wps=21600.7, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=0.507, loss_scale=32, train_wall=240, gb_free=9.6, wall=65434
2022-03-16 10:55:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 10:59:01 | INFO | train_inner | epoch 053:    190 / 407 loss=5.5, ppl=45.26, wps=24571.7, ups=0.37, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.505, loss_scale=32, train_wall=243, gb_free=9.6, wall=65700
2022-03-16 11:01:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:03:28 | INFO | train_inner | epoch 053:    291 / 407 loss=5.538, ppl=46.47, wps=24577.1, ups=0.38, wpb=65534.2, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.514, loss_scale=32, train_wall=243, gb_free=9.6, wall=65967
2022-03-16 11:06:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:07:54 | INFO | train_inner | epoch 053:    392 / 407 loss=5.535, ppl=46.35, wps=24604.2, ups=0.38, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.504, loss_scale=32, train_wall=243, gb_free=9.6, wall=66233
2022-03-16 11:08:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:08:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:09:10 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 5.759 | ppl 54.15 | wps 36604.5 | wpb 511.9 | bsz 1 | num_updates 21414 | best_loss 5.755
2022-03-16 11:09:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 21414 updates
2022-03-16 11:09:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 11:09:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 11:09:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt (epoch 53 @ 21414 updates, score 5.759) (writing took 1.224434373085387 seconds)
2022-03-16 11:09:11 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-16 11:09:11 | INFO | train | epoch 053 | loss 5.515 | ppl 45.74 | wps 23739.3 | ups 0.36 | wpb 65492.3 | bsz 127.9 | num_updates 21414 | lr 0.000216098 | gnorm 0.508 | loss_scale 16 | train_wall 979 | gb_free 9.6 | wall 66310
KL Stats: Epoch 53 Divergences: Uniform: 4.836791113776894 Unigram: 4.23772921386717
2022-03-16 11:09:11 | INFO | fairseq.trainer | begin training epoch 54
2022-03-16 11:09:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:12:58 | INFO | train_inner | epoch 054:     86 / 407 loss=5.472, ppl=44.4, wps=21508.3, ups=0.33, wpb=65360.1, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=0.515, loss_scale=16, train_wall=242, gb_free=9.6, wall=66537
2022-03-16 11:17:24 | INFO | train_inner | epoch 054:    186 / 407 loss=5.507, ppl=45.48, wps=24598.8, ups=0.38, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.516, loss_scale=32, train_wall=243, gb_free=9.6, wall=66804
2022-03-16 11:20:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:21:53 | INFO | train_inner | epoch 054:    287 / 407 loss=5.521, ppl=45.91, wps=24438.2, ups=0.37, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.504, loss_scale=32, train_wall=244, gb_free=9.6, wall=67072
2022-03-16 11:26:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:26:20 | INFO | train_inner | epoch 054:    388 / 407 loss=5.531, ppl=46.25, wps=24504.8, ups=0.37, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.514, loss_scale=32, train_wall=244, gb_free=9.6, wall=67339
2022-03-16 11:27:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:27:46 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 5.752 | ppl 53.89 | wps 36333.4 | wpb 511.9 | bsz 1 | num_updates 21819 | best_loss 5.752
2022-03-16 11:27:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21819 updates
2022-03-16 11:27:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 11:27:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 11:27:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 54 @ 21819 updates, score 5.752) (writing took 2.212004726054147 seconds)
2022-03-16 11:27:49 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-16 11:27:49 | INFO | train | epoch 054 | loss 5.509 | ppl 45.53 | wps 23733.9 | ups 0.36 | wpb 65492.6 | bsz 127.9 | num_updates 21819 | lr 0.000214083 | gnorm 0.512 | loss_scale 32 | train_wall 983 | gb_free 9.6 | wall 67428
KL Stats: Epoch 54 Divergences: Uniform: 4.842986298263418 Unigram: 4.242977988630709
2022-03-16 11:27:49 | INFO | fairseq.trainer | begin training epoch 55
2022-03-16 11:27:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:31:23 | INFO | train_inner | epoch 055:     81 / 407 loss=5.493, ppl=45.05, wps=21601, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=21900, lr=0.000213687, gnorm=0.509, loss_scale=32, train_wall=240, gb_free=9.6, wall=67642
2022-03-16 11:32:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:34:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:35:52 | INFO | train_inner | epoch 055:    183 / 407 loss=5.494, ppl=45.08, wps=24299.9, ups=0.37, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.515, loss_scale=16, train_wall=246, gb_free=9.6, wall=67911
2022-03-16 11:40:17 | INFO | train_inner | epoch 055:    283 / 407 loss=5.499, ppl=45.21, wps=24790.4, ups=0.38, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.508, loss_scale=32, train_wall=241, gb_free=9.6, wall=68176
2022-03-16 11:44:42 | INFO | train_inner | epoch 055:    383 / 407 loss=5.52, ppl=45.89, wps=24742.2, ups=0.38, wpb=65534.2, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.507, loss_scale=32, train_wall=242, gb_free=9.6, wall=68441
2022-03-16 11:45:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:45:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:46:21 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 5.749 | ppl 53.78 | wps 36279.5 | wpb 511.9 | bsz 1 | num_updates 22223 | best_loss 5.749
2022-03-16 11:46:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 22223 updates
2022-03-16 11:46:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 11:46:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 11:46:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 55 @ 22223 updates, score 5.749) (writing took 2.2482108959229663 seconds)
2022-03-16 11:46:23 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-16 11:46:23 | INFO | train | epoch 055 | loss 5.502 | ppl 45.32 | wps 23736.4 | ups 0.36 | wpb 65492.5 | bsz 127.9 | num_updates 22223 | lr 0.000212128 | gnorm 0.51 | loss_scale 32 | train_wall 980 | gb_free 9.6 | wall 68542
KL Stats: Epoch 55 Divergences: Uniform: 4.847389731505249 Unigram: 4.2462168482358145
2022-03-16 11:46:23 | INFO | fairseq.trainer | begin training epoch 56
2022-03-16 11:46:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:49:47 | INFO | train_inner | epoch 056:     77 / 407 loss=5.479, ppl=44.6, wps=21378.6, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=22300, lr=0.000211762, gnorm=0.515, loss_scale=32, train_wall=243, gb_free=9.6, wall=68746
2022-03-16 11:51:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:54:15 | INFO | train_inner | epoch 056:    178 / 407 loss=5.488, ppl=44.88, wps=24523.1, ups=0.37, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.51, loss_scale=32, train_wall=244, gb_free=9.6, wall=69014
2022-03-16 11:57:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 11:58:42 | INFO | train_inner | epoch 056:    279 / 407 loss=5.505, ppl=45.43, wps=24494, ups=0.37, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.511, loss_scale=32, train_wall=244, gb_free=9.6, wall=69281
2022-03-16 12:03:07 | INFO | train_inner | epoch 056:    379 / 407 loss=5.515, ppl=45.72, wps=24750.6, ups=0.38, wpb=65534.2, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.511, loss_scale=64, train_wall=241, gb_free=9.6, wall=69546
2022-03-16 12:03:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:04:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:04:57 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 5.741 | ppl 53.5 | wps 36309.4 | wpb 511.9 | bsz 1 | num_updates 22627 | best_loss 5.741
2022-03-16 12:04:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 22627 updates
2022-03-16 12:04:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 12:04:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 12:04:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 56 @ 22627 updates, score 5.741) (writing took 2.2258184080710635 seconds)
2022-03-16 12:04:59 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-16 12:04:59 | INFO | train | epoch 056 | loss 5.496 | ppl 45.14 | wps 23705 | ups 0.36 | wpb 65492.5 | bsz 127.9 | num_updates 22627 | lr 0.000210226 | gnorm 0.511 | loss_scale 32 | train_wall 982 | gb_free 9.6 | wall 69659
KL Stats: Epoch 56 Divergences: Uniform: 4.850794435342756 Unigram: 4.248969585209819
2022-03-16 12:04:59 | INFO | fairseq.trainer | begin training epoch 57
2022-03-16 12:04:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:08:12 | INFO | train_inner | epoch 057:     73 / 407 loss=5.474, ppl=44.46, wps=21421.4, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=22700, lr=0.000209888, gnorm=0.514, loss_scale=32, train_wall=242, gb_free=9.6, wall=69851
2022-03-16 12:09:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:12:39 | INFO | train_inner | epoch 057:    174 / 407 loss=5.48, ppl=44.63, wps=24573.5, ups=0.37, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.511, loss_scale=32, train_wall=243, gb_free=9.6, wall=70118
2022-03-16 12:15:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:17:06 | INFO | train_inner | epoch 057:    275 / 407 loss=5.491, ppl=44.98, wps=24567.5, ups=0.37, wpb=65534.2, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.506, loss_scale=32, train_wall=243, gb_free=9.6, wall=70385
2022-03-16 12:20:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:21:32 | INFO | train_inner | epoch 057:    376 / 407 loss=5.512, ppl=45.65, wps=24589.4, ups=0.38, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.513, loss_scale=32, train_wall=243, gb_free=9.6, wall=70651
2022-03-16 12:22:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:23:30 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 5.743 | ppl 53.55 | wps 36522.5 | wpb 511.9 | bsz 1 | num_updates 23031 | best_loss 5.741
2022-03-16 12:23:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 23031 updates
2022-03-16 12:23:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 12:23:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 12:23:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt (epoch 57 @ 23031 updates, score 5.743) (writing took 1.2174260229803622 seconds)
2022-03-16 12:23:31 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-16 12:23:31 | INFO | train | epoch 057 | loss 5.49 | ppl 44.93 | wps 23800 | ups 0.36 | wpb 65492.5 | bsz 127.9 | num_updates 23031 | lr 0.000208374 | gnorm 0.512 | loss_scale 32 | train_wall 979 | gb_free 9.6 | wall 70770
KL Stats: Epoch 57 Divergences: Uniform: 4.85767654913616 Unigram: 4.255451306901806
2022-03-16 12:23:31 | INFO | fairseq.trainer | begin training epoch 58
2022-03-16 12:23:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:26:34 | INFO | train_inner | epoch 058:     69 / 407 loss=5.476, ppl=44.5, wps=21677, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=23100, lr=0.000208063, gnorm=0.516, loss_scale=32, train_wall=240, gb_free=9.6, wall=70953
2022-03-16 12:27:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:31:00 | INFO | train_inner | epoch 058:    170 / 407 loss=5.468, ppl=44.27, wps=24591.6, ups=0.38, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.512, loss_scale=32, train_wall=243, gb_free=9.6, wall=71219
2022-03-16 12:32:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:35:27 | INFO | train_inner | epoch 058:    271 / 407 loss=5.489, ppl=44.92, wps=24594.3, ups=0.38, wpb=65534.2, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.515, loss_scale=32, train_wall=243, gb_free=9.6, wall=71486
2022-03-16 12:38:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:39:53 | INFO | train_inner | epoch 058:    372 / 407 loss=5.5, ppl=45.24, wps=24583.9, ups=0.38, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.514, loss_scale=32, train_wall=243, gb_free=9.6, wall=71752
2022-03-16 12:41:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:42:01 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 5.745 | ppl 53.62 | wps 36516.4 | wpb 511.9 | bsz 1 | num_updates 23435 | best_loss 5.741
2022-03-16 12:42:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 23435 updates
2022-03-16 12:42:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 12:42:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 12:42:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt (epoch 58 @ 23435 updates, score 5.745) (writing took 1.2159170339582488 seconds)
2022-03-16 12:42:03 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-16 12:42:03 | INFO | train | epoch 058 | loss 5.484 | ppl 44.75 | wps 23804.7 | ups 0.36 | wpb 65492.5 | bsz 127.9 | num_updates 23435 | lr 0.00020657 | gnorm 0.513 | loss_scale 32 | train_wall 979 | gb_free 9.6 | wall 71882
KL Stats: Epoch 58 Divergences: Uniform: 4.863713200443122 Unigram: 4.26084919700741
2022-03-16 12:42:03 | INFO | fairseq.trainer | begin training epoch 59
2022-03-16 12:42:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:44:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 12:44:57 | INFO | train_inner | epoch 059:     66 / 407 loss=5.471, ppl=44.35, wps=21491, ups=0.33, wpb=65361.9, bsz=127.7, num_updates=23500, lr=0.000206284, gnorm=0.507, loss_scale=32, train_wall=243, gb_free=9.6, wall=72056
2022-03-16 12:49:21 | INFO | train_inner | epoch 059:    166 / 407 loss=5.462, ppl=44.08, wps=24837.3, ups=0.38, wpb=65534.2, bsz=128, num_updates=23600, lr=0.000205847, gnorm=0.512, loss_scale=32, train_wall=240, gb_free=9.6, wall=72320
2022-03-16 12:49:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:53:48 | INFO | train_inner | epoch 059:    267 / 407 loss=5.485, ppl=44.78, wps=24591.6, ups=0.38, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.51, loss_scale=16, train_wall=243, gb_free=9.6, wall=72587
2022-03-16 12:58:11 | INFO | train_inner | epoch 059:    367 / 407 loss=5.493, ppl=45.05, wps=24888.1, ups=0.38, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.509, loss_scale=32, train_wall=240, gb_free=9.6, wall=72850
2022-03-16 12:59:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:00:32 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 5.735 | ppl 53.24 | wps 36325.4 | wpb 511.9 | bsz 1 | num_updates 23840 | best_loss 5.735
2022-03-16 13:00:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 23840 updates
2022-03-16 13:00:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 13:00:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 13:00:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 59 @ 23840 updates, score 5.735) (writing took 2.23195847007446 seconds)
2022-03-16 13:00:34 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-16 13:00:34 | INFO | train | epoch 059 | loss 5.478 | ppl 44.56 | wps 23867 | ups 0.36 | wpb 65492.6 | bsz 127.9 | num_updates 23840 | lr 0.000204808 | gnorm 0.51 | loss_scale 32 | train_wall 977 | gb_free 9.6 | wall 72993
KL Stats: Epoch 59 Divergences: Uniform: 4.868434118780404 Unigram: 4.265116702254744
2022-03-16 13:00:34 | INFO | fairseq.trainer | begin training epoch 60
2022-03-16 13:00:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:01:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:03:14 | INFO | train_inner | epoch 060:     61 / 407 loss=5.469, ppl=44.29, wps=21549.2, ups=0.33, wpb=65360.1, bsz=127.7, num_updates=23900, lr=0.000204551, gnorm=0.514, loss_scale=32, train_wall=241, gb_free=9.6, wall=73153
2022-03-16 13:07:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:07:38 | INFO | train_inner | epoch 060:    162 / 407 loss=5.455, ppl=43.88, wps=24838.3, ups=0.38, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.515, loss_scale=32, train_wall=241, gb_free=9.6, wall=73417
2022-03-16 13:11:52 | INFO | train_inner | epoch 060:    262 / 407 loss=5.472, ppl=44.38, wps=25776.2, ups=0.39, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.512, loss_scale=32, train_wall=232, gb_free=9.6, wall=73671
2022-03-16 13:12:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:16:09 | INFO | train_inner | epoch 060:    363 / 407 loss=5.496, ppl=45.12, wps=25577.5, ups=0.39, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.516, loss_scale=32, train_wall=234, gb_free=9.6, wall=73928
2022-03-16 13:17:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:18:35 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 5.732 | ppl 53.15 | wps 37412.6 | wpb 511.9 | bsz 1 | num_updates 24244 | best_loss 5.732
2022-03-16 13:18:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 24244 updates
2022-03-16 13:18:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 13:18:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 13:18:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 60 @ 24244 updates, score 5.732) (writing took 2.3059835400199518 seconds)
2022-03-16 13:18:38 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-16 13:18:38 | INFO | train | epoch 060 | loss 5.472 | ppl 44.4 | wps 24418.9 | ups 0.37 | wpb 65492.5 | bsz 127.9 | num_updates 24244 | lr 0.000203094 | gnorm 0.515 | loss_scale 64 | train_wall 953 | gb_free 9.6 | wall 74077
KL Stats: Epoch 60 Divergences: Uniform: 4.873300542026351 Unigram: 4.270012581013285
2022-03-16 13:18:38 | INFO | fairseq.trainer | begin training epoch 61
2022-03-16 13:18:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:18:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:20:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:21:05 | INFO | train_inner | epoch 061:     58 / 407 loss=5.464, ppl=44.15, wps=22018.9, ups=0.34, wpb=65361.9, bsz=127.7, num_updates=24300, lr=0.00020286, gnorm=0.52, loss_scale=16, train_wall=236, gb_free=9.6, wall=74224
2022-03-16 13:25:20 | INFO | train_inner | epoch 061:    158 / 407 loss=5.455, ppl=43.85, wps=25745.6, ups=0.39, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.517, loss_scale=16, train_wall=232, gb_free=9.6, wall=74479
2022-03-16 13:29:34 | INFO | train_inner | epoch 061:    258 / 407 loss=5.465, ppl=44.18, wps=25822.2, ups=0.39, wpb=65534.2, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.512, loss_scale=32, train_wall=232, gb_free=9.6, wall=74733
2022-03-16 13:31:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:33:50 | INFO | train_inner | epoch 061:    359 / 407 loss=5.485, ppl=44.78, wps=25595.4, ups=0.39, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.511, loss_scale=32, train_wall=234, gb_free=9.6, wall=74989
2022-03-16 13:35:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:36:27 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 5.731 | ppl 53.13 | wps 37401.5 | wpb 511.9 | bsz 1 | num_updates 24648 | best_loss 5.731
2022-03-16 13:36:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 24648 updates
2022-03-16 13:36:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 13:36:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 13:36:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 61 @ 24648 updates, score 5.731) (writing took 2.223118658992462 seconds)
2022-03-16 13:36:29 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-16 13:36:29 | INFO | train | epoch 061 | loss 5.466 | ppl 44.22 | wps 24694.2 | ups 0.38 | wpb 65492.5 | bsz 127.9 | num_updates 24648 | lr 0.000201423 | gnorm 0.515 | loss_scale 32 | train_wall 943 | gb_free 9.6 | wall 75148
KL Stats: Epoch 61 Divergences: Uniform: 4.879769649159744 Unigram: 4.274759823491455
2022-03-16 13:36:29 | INFO | fairseq.trainer | begin training epoch 62
2022-03-16 13:36:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:37:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:38:44 | INFO | train_inner | epoch 062:     53 / 407 loss=5.462, ppl=44.08, wps=22239.4, ups=0.34, wpb=65361.9, bsz=127.7, num_updates=24700, lr=0.000201211, gnorm=0.512, loss_scale=16, train_wall=233, gb_free=9.6, wall=75283
2022-03-16 13:42:58 | INFO | train_inner | epoch 062:    153 / 407 loss=5.447, ppl=43.62, wps=25808.7, ups=0.39, wpb=65534.2, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.516, loss_scale=32, train_wall=232, gb_free=9.6, wall=75537
2022-03-16 13:47:11 | INFO | train_inner | epoch 062:    253 / 407 loss=5.461, ppl=44.03, wps=25837.4, ups=0.39, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.511, loss_scale=32, train_wall=231, gb_free=9.6, wall=75790
2022-03-16 13:48:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:51:28 | INFO | train_inner | epoch 062:    354 / 407 loss=5.48, ppl=44.64, wps=25502.7, ups=0.39, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.514, loss_scale=32, train_wall=234, gb_free=9.6, wall=76047
2022-03-16 13:53:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 13:53:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:54:18 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 5.728 | ppl 53 | wps 37419.6 | wpb 511.9 | bsz 1 | num_updates 25052 | best_loss 5.728
2022-03-16 13:54:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 25052 updates
2022-03-16 13:54:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 13:54:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 13:54:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 62 @ 25052 updates, score 5.728) (writing took 2.2881223569856957 seconds)
2022-03-16 13:54:20 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-16 13:54:20 | INFO | train | epoch 062 | loss 5.461 | ppl 44.04 | wps 24702.7 | ups 0.38 | wpb 65492.5 | bsz 127.9 | num_updates 25052 | lr 0.000199792 | gnorm 0.514 | loss_scale 32 | train_wall 942 | gb_free 9.6 | wall 76219
KL Stats: Epoch 62 Divergences: Uniform: 4.884270903783244 Unigram: 4.279048988220974
2022-03-16 13:54:20 | INFO | fairseq.trainer | begin training epoch 63
2022-03-16 13:54:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 13:56:22 | INFO | train_inner | epoch 063:     48 / 407 loss=5.449, ppl=43.67, wps=22224.5, ups=0.34, wpb=65361.9, bsz=127.7, num_updates=25100, lr=0.000199601, gnorm=0.51, loss_scale=32, train_wall=233, gb_free=9.6, wall=76341
2022-03-16 13:59:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 14:00:41 | INFO | train_inner | epoch 063:    149 / 407 loss=5.445, ppl=43.57, wps=25340.7, ups=0.39, wpb=65536, bsz=128, num_updates=25200, lr=0.000199205, gnorm=0.516, loss_scale=32, train_wall=236, gb_free=9.6, wall=76600
2022-03-16 14:04:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 14:05:01 | INFO | train_inner | epoch 063:    250 / 407 loss=5.461, ppl=44.04, wps=25217.8, ups=0.38, wpb=65534.2, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.518, loss_scale=16, train_wall=237, gb_free=9.6, wall=76860
2022-03-16 14:09:15 | INFO | train_inner | epoch 063:    350 / 407 loss=5.466, ppl=44.2, wps=25759.7, ups=0.39, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.518, loss_scale=16, train_wall=232, gb_free=9.6, wall=77114
2022-03-16 14:11:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 14:12:14 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 5.73 | ppl 53.08 | wps 37337 | wpb 511.9 | bsz 1 | num_updates 25457 | best_loss 5.728
2022-03-16 14:12:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 25457 updates
2022-03-16 14:12:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 14:12:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 14:12:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt (epoch 63 @ 25457 updates, score 5.73) (writing took 1.3224083230597898 seconds)
2022-03-16 14:12:16 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-16 14:12:16 | INFO | train | epoch 063 | loss 5.456 | ppl 43.91 | wps 24662.2 | ups 0.38 | wpb 65492.6 | bsz 127.9 | num_updates 25457 | lr 0.000198197 | gnorm 0.515 | loss_scale 32 | train_wall 947 | gb_free 9.6 | wall 77295
KL Stats: Epoch 63 Divergences: Uniform: 4.887971240718361 Unigram: 4.282210144663085
2022-03-16 14:12:16 | INFO | fairseq.trainer | begin training epoch 64
2022-03-16 14:12:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 14:14:05 | INFO | train_inner | epoch 064:     43 / 407 loss=5.455, ppl=43.88, wps=22564.6, ups=0.35, wpb=65360.1, bsz=127.7, num_updates=25500, lr=0.00019803, gnorm=0.515, loss_scale=32, train_wall=230, gb_free=9.6, wall=77404
2022-03-16 14:15:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 14:18:21 | INFO | train_inner | epoch 064:    144 / 407 loss=5.44, ppl=43.42, wps=25548.1, ups=0.39, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.524, loss_scale=32, train_wall=234, gb_free=9.6, wall=77660
2022-03-16 14:21:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 14:22:38 | INFO | train_inner | epoch 064:    245 / 407 loss=5.451, ppl=43.74, wps=25589.5, ups=0.39, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.515, loss_scale=32, train_wall=234, gb_free=9.6, wall=77917
2022-03-16 14:26:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 14:26:54 | INFO | train_inner | epoch 064:    346 / 407 loss=5.457, ppl=43.93, wps=25584.8, ups=0.39, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.512, loss_scale=32, train_wall=234, gb_free=9.6, wall=78173
2022-03-16 14:29:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 14:30:05 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 5.727 | ppl 52.97 | wps 37366.4 | wpb 511.9 | bsz 1 | num_updates 25861 | best_loss 5.727
2022-03-16 14:30:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 25861 updates
2022-03-16 14:30:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 14:30:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 14:30:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 64 @ 25861 updates, score 5.727) (writing took 2.2776171539444476 seconds)
2022-03-16 14:30:08 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-16 14:30:08 | INFO | train | epoch 064 | loss 5.451 | ppl 43.75 | wps 24682.2 | ups 0.38 | wpb 65492.5 | bsz 127.9 | num_updates 25861 | lr 0.000196642 | gnorm 0.517 | loss_scale 32 | train_wall 943 | gb_free 9.6 | wall 78367
KL Stats: Epoch 64 Divergences: Uniform: 4.894131148134137 Unigram: 4.288752041171069
2022-03-16 14:30:08 | INFO | fairseq.trainer | begin training epoch 65
2022-03-16 14:30:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 14:31:47 | INFO | train_inner | epoch 065:     39 / 407 loss=5.46, ppl=44.01, wps=22284.3, ups=0.34, wpb=65361.9, bsz=127.7, num_updates=25900, lr=0.000196494, gnorm=0.517, loss_scale=32, train_wall=233, gb_free=9.6, wall=78466
2022-03-16 14:32:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 14:36:05 | INFO | train_inner | epoch 065:    140 / 407 loss=5.436, ppl=43.3, wps=25355.9, ups=0.39, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.518, loss_scale=32, train_wall=236, gb_free=9.6, wall=78725
2022-03-16 14:38:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 14:40:20 | INFO | train_inner | epoch 065:    241 / 407 loss=5.448, ppl=43.67, wps=25707.8, ups=0.39, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.518, loss_scale=32, train_wall=233, gb_free=9.6, wall=78979
2022-03-16 14:43:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 14:44:35 | INFO | train_inner | epoch 065:    342 / 407 loss=5.457, ppl=43.94, wps=25719.6, ups=0.39, wpb=65534.2, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.516, loss_scale=32, train_wall=233, gb_free=9.6, wall=79234
2022-03-16 14:47:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 14:47:54 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 5.715 | ppl 52.54 | wps 37468.6 | wpb 511.9 | bsz 1 | num_updates 26265 | best_loss 5.715
2022-03-16 14:47:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 26265 updates
2022-03-16 14:47:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 14:47:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-16 14:47:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 65 @ 26265 updates, score 5.715) (writing took 2.268630026956089 seconds)
2022-03-16 14:47:57 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-16 14:47:57 | INFO | train | epoch 065 | loss 5.447 | ppl 43.61 | wps 24753.8 | ups 0.38 | wpb 65492.5 | bsz 127.9 | num_updates 26265 | lr 0.000195124 | gnorm 0.517 | loss_scale 32 | train_wall 941 | gb_free 9.6 | wall 79436
KL Stats: Epoch 65 Divergences: Uniform: 4.8962591608684 Unigram: 4.290554289779674
2022-03-16 14:47:57 | INFO | fairseq.trainer | begin training epoch 66
2022-03-16 14:47:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 14:49:25 | INFO | train_inner | epoch 066:     35 / 407 loss=5.441, ppl=43.45, wps=22555.5, ups=0.35, wpb=65361.9, bsz=127.7, num_updates=26300, lr=0.000194994, gnorm=0.52, loss_scale=32, train_wall=230, gb_free=9.6, wall=79524
2022-03-16 14:49:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 14:53:40 | INFO | train_inner | epoch 066:    136 / 407 loss=5.425, ppl=42.97, wps=25722.3, ups=0.39, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.519, loss_scale=32, train_wall=232, gb_free=9.6, wall=79779
2022-03-16 14:55:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 14:57:54 | INFO | train_inner | epoch 066:    237 / 407 loss=5.446, ppl=43.59, wps=25739.4, ups=0.39, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.521, loss_scale=32, train_wall=232, gb_free=9.6, wall=80033
2022-03-16 15:00:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 15:02:09 | INFO | train_inner | epoch 066:    338 / 407 loss=5.451, ppl=43.73, wps=25749, ups=0.39, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.518, loss_scale=32, train_wall=232, gb_free=9.6, wall=80288
2022-03-16 15:04:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 15:05:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 15:05:38 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 5.724 | ppl 52.85 | wps 37567.9 | wpb 511.9 | bsz 1 | num_updates 26668 | best_loss 5.715
2022-03-16 15:05:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 26668 updates
2022-03-16 15:05:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 15:05:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 15:05:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt (epoch 66 @ 26668 updates, score 5.724) (writing took 1.2965228189714253 seconds)
2022-03-16 15:05:39 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-16 15:05:39 | INFO | train | epoch 066 | loss 5.442 | ppl 43.46 | wps 24837.9 | ups 0.38 | wpb 65492.3 | bsz 127.9 | num_updates 26668 | lr 0.000193644 | gnorm 0.519 | loss_scale 16 | train_wall 936 | gb_free 9.6 | wall 80498
KL Stats: Epoch 66 Divergences: Uniform: 4.901405176951 Unigram: 4.296343225915997
2022-03-16 15:05:39 | INFO | fairseq.trainer | begin training epoch 67
2022-03-16 15:05:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 15:07:00 | INFO | train_inner | epoch 067:     32 / 407 loss=5.443, ppl=43.5, wps=22447.9, ups=0.34, wpb=65360.1, bsz=127.7, num_updates=26700, lr=0.000193528, gnorm=0.522, loss_scale=16, train_wall=232, gb_free=9.6, wall=80579
2022-03-16 15:11:12 | INFO | train_inner | epoch 067:    132 / 407 loss=5.414, ppl=42.63, wps=25974.9, ups=0.4, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.515, loss_scale=32, train_wall=230, gb_free=9.6, wall=80831
2022-03-16 15:15:25 | INFO | train_inner | epoch 067:    232 / 407 loss=5.438, ppl=43.35, wps=25985, ups=0.4, wpb=65534.2, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.524, loss_scale=32, train_wall=230, gb_free=9.6, wall=81084
2022-03-16 15:16:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 15:19:39 | INFO | train_inner | epoch 067:    333 / 407 loss=5.453, ppl=43.81, wps=25725.8, ups=0.39, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.516, loss_scale=32, train_wall=232, gb_free=9.6, wall=81338
2022-03-16 15:21:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 15:22:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 15:23:21 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 5.723 | ppl 52.81 | wps 37493.1 | wpb 511.9 | bsz 1 | num_updates 27073 | best_loss 5.715
2022-03-16 15:23:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 27073 updates
2022-03-16 15:23:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 15:23:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 15:23:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt (epoch 67 @ 27073 updates, score 5.723) (writing took 1.3997297700261697 seconds)
2022-03-16 15:23:22 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-16 15:23:22 | INFO | train | epoch 067 | loss 5.437 | ppl 43.32 | wps 24950.5 | ups 0.38 | wpb 65492.6 | bsz 127.9 | num_updates 27073 | lr 0.00019219 | gnorm 0.52 | loss_scale 32 | train_wall 936 | gb_free 9.6 | wall 81561
KL Stats: Epoch 67 Divergences: Uniform: 4.907034985720888 Unigram: 4.3009028767678705
2022-03-16 15:23:22 | INFO | fairseq.trainer | begin training epoch 68
2022-03-16 15:23:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 15:24:30 | INFO | train_inner | epoch 068:     27 / 407 loss=5.442, ppl=43.47, wps=22445.4, ups=0.34, wpb=65360.1, bsz=127.7, num_updates=27100, lr=0.000192095, gnorm=0.521, loss_scale=32, train_wall=232, gb_free=9.6, wall=81630
2022-03-16 15:27:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 15:28:45 | INFO | train_inner | epoch 068:    128 / 407 loss=5.413, ppl=42.6, wps=25723, ups=0.39, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.526, loss_scale=32, train_wall=232, gb_free=9.6, wall=81884
2022-03-16 15:32:57 | INFO | train_inner | epoch 068:    228 / 407 loss=5.429, ppl=43.07, wps=25986.9, ups=0.4, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.52, loss_scale=32, train_wall=230, gb_free=9.6, wall=82137
2022-03-16 15:33:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 15:37:12 | INFO | train_inner | epoch 068:    329 / 407 loss=5.446, ppl=43.58, wps=25749.5, ups=0.39, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.515, loss_scale=32, train_wall=232, gb_free=9.6, wall=82391
2022-03-16 15:38:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-16 15:40:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 15:41:03 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 5.72 | ppl 52.7 | wps 37617.4 | wpb 511.9 | bsz 1 | num_updates 27477 | best_loss 5.715
2022-03-16 15:41:03 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-16 15:41:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 27477 updates
2022-03-16 15:41:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 15:41:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-16 15:41:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt (epoch 68 @ 27477 updates, score 5.72) (writing took 1.2640988510102034 seconds)
2022-03-16 15:41:05 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-16 15:41:05 | INFO | train | epoch 068 | loss 5.432 | ppl 43.16 | wps 24904.2 | ups 0.38 | wpb 65492.5 | bsz 127.9 | num_updates 27477 | lr 0.000190772 | gnorm 0.52 | loss_scale 32 | train_wall 936 | gb_free 9.6 | wall 82624
2022-03-16 15:41:05 | INFO | fairseq_cli.train | done training in 82623.5 seconds
Sender: LSF System <lsfadmin@eu-g3-073>
Subject: Job 209212401: <ru_dropout_0.3_jelinek_0.02_0.005_0.975_#1> in cluster <euler> Done

Job <ru_dropout_0.3_jelinek_0.02_0.005_0.975_#1> was submitted from host <eu-login-02> by user <andriusb> in cluster <euler> at Thu Mar 17 09:18:02 2022
Job was executed on host(s) <eu-g3-073>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Thu Mar 17 09:18:34 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Mar 17 09:18:34 2022
Terminated at Thu Mar 17 20:12:06 2022
Results reported at Thu Mar 17 20:12:06 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/ru --save-dir /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.3 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas \(0.02,0.005,0.975\) --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --seed 66575611 --fp16 --no-epoch-checkpoints --patience 3 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   39191.83 sec.
    Max Memory :                                 3844 MB
    Average Memory :                             3002.68 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               16156.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   39211 sec.
    Turnaround time :                            39244 sec.

The output (if any) follows:

2022-03-17 09:18:41 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.3, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/ru', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.02,0.005,0.975)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-17 09:18:41 | INFO | fairseq.tasks.language_modeling | dictionary: 35920 types
2022-03-17 09:18:42 | INFO | fairseq.data.data_utils | loaded 53,136 examples from: data-bin/ru/train
Calculating frequency stats:
  0%|          | 0/53136 [00:00<?, ?it/s]  0%|          | 76/53136 [00:00<01:09, 758.33it/s]  0%|          | 152/53136 [00:00<01:13, 722.12it/s]  0%|          | 238/53136 [00:00<01:07, 780.73it/s]  1%|          | 317/53136 [00:00<01:10, 746.81it/s]  1%|          | 392/53136 [00:00<01:12, 729.55it/s]  1%|          | 486/53136 [00:00<01:05, 797.80it/s]  1%|          | 581/53136 [00:00<01:02, 841.35it/s]  1%|▏         | 666/53136 [00:00<01:03, 821.89it/s]  1%|▏         | 753/53136 [00:00<01:02, 836.09it/s]  2%|▏         | 837/53136 [00:01<01:10, 742.51it/s]  2%|▏         | 914/53136 [00:01<01:09, 746.42it/s]  2%|▏         | 1000/53136 [00:01<01:07, 777.67it/s]  2%|▏         | 1079/53136 [00:01<01:15, 689.09it/s]  2%|▏         | 1169/53136 [00:01<01:09, 744.84it/s]  2%|▏         | 1246/53136 [00:01<01:10, 740.99it/s]  3%|▎         | 1329/53136 [00:01<01:10, 734.20it/s]  3%|▎         | 1415/53136 [00:01<01:07, 768.66it/s]  3%|▎         | 1501/53136 [00:01<01:05, 785.65it/s]  3%|▎         | 1604/53136 [00:02<01:00, 852.99it/s]  3%|▎         | 1707/53136 [00:02<00:56, 902.49it/s]  3%|▎         | 1804/53136 [00:02<00:55, 920.30it/s]  4%|▎         | 1897/53136 [00:02<00:58, 868.62it/s]  4%|▎         | 1985/53136 [00:02<01:02, 821.33it/s]  4%|▍         | 2081/53136 [00:02<00:59, 858.04it/s]  4%|▍         | 2168/53136 [00:02<01:01, 830.63it/s]  4%|▍         | 2267/53136 [00:02<00:58, 873.42it/s]  4%|▍         | 2369/53136 [00:02<00:55, 912.82it/s]  5%|▍         | 2462/53136 [00:03<01:06, 764.54it/s]  5%|▍         | 2543/53136 [00:03<01:09, 730.19it/s]  5%|▍         | 2643/53136 [00:03<01:03, 796.96it/s]  5%|▌         | 2739/53136 [00:03<01:00, 839.57it/s]  5%|▌         | 2831/53136 [00:03<00:58, 857.03it/s]  5%|▌         | 2919/53136 [00:03<01:06, 757.60it/s]  6%|▌         | 3001/53136 [00:03<01:05, 771.12it/s]  6%|▌         | 3081/53136 [00:03<01:14, 676.39it/s]  6%|▌         | 3153/53136 [00:04<01:13, 677.31it/s]  6%|▌         | 3224/53136 [00:04<01:19, 630.53it/s]  6%|▌         | 3314/53136 [00:04<01:11, 696.89it/s]  6%|▋         | 3387/53136 [00:04<01:13, 677.62it/s]  7%|▋         | 3462/53136 [00:04<01:11, 694.26it/s]  7%|▋         | 3552/53136 [00:04<01:07, 735.18it/s]  7%|▋         | 3627/53136 [00:04<01:16, 647.45it/s]  7%|▋         | 3695/53136 [00:04<01:19, 625.27it/s]  7%|▋         | 3781/53136 [00:04<01:12, 683.74it/s]  7%|▋         | 3861/53136 [00:05<01:09, 713.94it/s]  7%|▋         | 3941/53136 [00:05<01:06, 737.69it/s]  8%|▊         | 4020/53136 [00:05<01:05, 747.43it/s]  8%|▊         | 4103/53136 [00:05<01:03, 770.42it/s]  8%|▊         | 4192/53136 [00:05<01:01, 791.52it/s]  8%|▊         | 4272/53136 [00:05<01:01, 791.09it/s]  8%|▊         | 4352/53136 [00:05<01:09, 702.74it/s]  8%|▊         | 4425/53136 [00:05<01:09, 702.44it/s]  8%|▊         | 4497/53136 [00:05<01:09, 697.94it/s]  9%|▊         | 4573/53136 [00:06<01:08, 708.80it/s]  9%|▉         | 4664/53136 [00:06<01:03, 764.90it/s]  9%|▉         | 4742/53136 [00:06<01:04, 748.92it/s]  9%|▉         | 4818/53136 [00:06<01:19, 611.20it/s]  9%|▉         | 4893/53136 [00:06<01:15, 639.75it/s]  9%|▉         | 4967/53136 [00:06<01:12, 665.01it/s] 10%|▉         | 5061/53136 [00:06<01:05, 739.48it/s] 10%|▉         | 5138/53136 [00:06<01:15, 638.06it/s] 10%|▉         | 5212/53136 [00:06<01:12, 663.75it/s] 10%|▉         | 5282/53136 [00:07<01:11, 667.29it/s] 10%|█         | 5352/53136 [00:07<01:18, 606.21it/s] 10%|█         | 5452/53136 [00:07<01:09, 685.40it/s] 10%|█         | 5539/53136 [00:07<01:04, 732.68it/s] 11%|█         | 5623/53136 [00:07<01:02, 761.89it/s] 11%|█         | 5701/53136 [00:07<01:02, 760.15it/s] 11%|█         | 5782/53136 [00:07<01:01, 772.47it/s] 11%|█         | 5865/53136 [00:07<00:59, 788.52it/s] 11%|█         | 5945/53136 [00:07<01:04, 728.02it/s] 11%|█▏        | 6052/53136 [00:08<00:57, 822.48it/s] 12%|█▏        | 6136/53136 [00:08<00:59, 785.48it/s] 12%|█▏        | 6216/53136 [00:08<01:08, 689.47it/s] 12%|█▏        | 6291/53136 [00:08<01:07, 695.98it/s] 12%|█▏        | 6392/53136 [00:08<01:00, 778.70it/s] 12%|█▏        | 6480/53136 [00:08<00:58, 798.70it/s] 12%|█▏        | 6580/53136 [00:08<00:54, 847.54it/s] 13%|█▎        | 6680/53136 [00:08<00:52, 889.48it/s] 13%|█▎        | 6775/53136 [00:08<00:51, 900.93it/s] 13%|█▎        | 6867/53136 [00:09<00:57, 798.42it/s] 13%|█▎        | 6950/53136 [00:09<01:11, 647.29it/s] 13%|█▎        | 7037/53136 [00:09<01:06, 698.40it/s] 13%|█▎        | 7121/53136 [00:09<01:02, 731.95it/s] 14%|█▎        | 7219/53136 [00:09<00:57, 794.58it/s] 14%|█▎        | 7303/53136 [00:09<00:58, 780.23it/s] 14%|█▍        | 7384/53136 [00:09<01:16, 600.47it/s] 14%|█▍        | 7452/53136 [00:10<01:19, 576.08it/s] 14%|█▍        | 7515/53136 [00:10<01:17, 586.86it/s] 14%|█▍        | 7602/53136 [00:10<01:09, 655.94it/s] 14%|█▍        | 7672/53136 [00:10<01:12, 630.46it/s] 15%|█▍        | 7767/53136 [00:10<01:03, 711.33it/s] 15%|█▍        | 7845/53136 [00:10<01:02, 728.89it/s] 15%|█▍        | 7947/53136 [00:10<00:55, 808.37it/s] 15%|█▌        | 8031/53136 [00:10<00:56, 794.43it/s] 15%|█▌        | 8113/53136 [00:10<00:57, 782.71it/s] 15%|█▌        | 8193/53136 [00:11<01:02, 716.26it/s] 16%|█▌        | 8267/53136 [00:11<01:03, 706.62it/s] 16%|█▌        | 8339/53136 [00:11<01:03, 706.71it/s] 16%|█▌        | 8416/53136 [00:11<01:02, 716.64it/s] 16%|█▌        | 8497/53136 [00:11<01:00, 742.85it/s] 16%|█▌        | 8572/53136 [00:11<01:03, 706.18it/s] 16%|█▋        | 8644/53136 [00:11<01:05, 677.48it/s] 16%|█▋        | 8720/53136 [00:11<01:03, 699.77it/s] 17%|█▋        | 8816/53136 [00:11<00:57, 771.39it/s] 17%|█▋        | 8894/53136 [00:12<00:58, 750.86it/s] 17%|█▋        | 8970/53136 [00:12<01:06, 667.86it/s] 17%|█▋        | 9039/53136 [00:12<01:06, 659.22it/s] 17%|█▋        | 9121/53136 [00:12<01:02, 700.96it/s] 17%|█▋        | 9217/53136 [00:12<00:57, 769.90it/s] 17%|█▋        | 9296/53136 [00:12<01:00, 724.44it/s] 18%|█▊        | 9384/53136 [00:12<00:57, 766.06it/s] 18%|█▊        | 9462/53136 [00:12<00:59, 739.51it/s] 18%|█▊        | 9555/53136 [00:12<00:55, 788.64it/s] 18%|█▊        | 9653/53136 [00:13<00:51, 842.86it/s] 18%|█▊        | 9739/53136 [00:13<00:57, 755.28it/s] 18%|█▊        | 9817/53136 [00:13<01:15, 574.50it/s] 19%|█▊        | 9890/53136 [00:13<01:11, 606.80it/s] 19%|█▊        | 9958/53136 [00:13<01:11, 601.85it/s] 19%|█▉        | 10023/53136 [00:13<01:11, 607.19it/s] 19%|█▉        | 10124/53136 [00:13<01:00, 706.38it/s] 19%|█▉        | 10205/53136 [00:13<00:58, 733.96it/s] 19%|█▉        | 10282/53136 [00:13<00:58, 736.74it/s] 19%|█▉        | 10358/53136 [00:14<01:04, 665.43it/s] 20%|█▉        | 10428/53136 [00:14<01:10, 604.79it/s] 20%|█▉        | 10503/53136 [00:14<01:07, 627.40it/s] 20%|█▉        | 10568/53136 [00:14<01:13, 577.79it/s] 20%|██        | 10664/53136 [00:14<01:03, 671.92it/s] 20%|██        | 10734/53136 [00:14<01:03, 663.63it/s] 20%|██        | 10803/53136 [00:14<01:04, 658.92it/s] 20%|██        | 10888/53136 [00:14<00:59, 709.61it/s] 21%|██        | 10991/53136 [00:15<00:52, 799.78it/s] 21%|██        | 11073/53136 [00:15<01:06, 637.27it/s] 21%|██        | 11143/53136 [00:15<01:06, 634.48it/s] 21%|██        | 11232/53136 [00:15<01:00, 689.27it/s] 21%|██▏       | 11305/53136 [00:15<01:00, 685.78it/s] 21%|██▏       | 11377/53136 [00:15<01:00, 688.55it/s] 22%|██▏       | 11448/53136 [00:15<01:01, 677.28it/s] 22%|██▏       | 11534/53136 [00:15<00:57, 727.33it/s] 22%|██▏       | 11609/53136 [00:16<01:04, 643.18it/s] 22%|██▏       | 11679/53136 [00:16<01:03, 656.48it/s] 22%|██▏       | 11767/53136 [00:16<00:57, 715.31it/s] 22%|██▏       | 11841/53136 [00:16<00:57, 721.83it/s] 22%|██▏       | 11929/53136 [00:16<00:54, 754.24it/s] 23%|██▎       | 12011/53136 [00:16<00:53, 771.14it/s] 23%|██▎       | 12089/53136 [00:16<01:05, 628.42it/s] 23%|██▎       | 12162/53136 [00:16<01:02, 653.38it/s] 23%|██▎       | 12234/53136 [00:16<01:00, 670.78it/s] 23%|██▎       | 12324/53136 [00:17<00:55, 731.75it/s] 23%|██▎       | 12427/53136 [00:17<00:50, 810.58it/s] 24%|██▎       | 12511/53136 [00:17<00:51, 792.79it/s] 24%|██▎       | 12592/53136 [00:17<01:03, 641.19it/s] 24%|██▍       | 12662/53136 [00:17<01:08, 588.71it/s] 24%|██▍       | 12734/53136 [00:17<01:05, 617.21it/s] 24%|██▍       | 12800/53136 [00:17<01:10, 571.41it/s] 24%|██▍       | 12880/53136 [00:17<01:04, 626.75it/s] 24%|██▍       | 12976/53136 [00:18<00:56, 713.00it/s] 25%|██▍       | 13056/53136 [00:18<00:55, 726.92it/s] 25%|██▍       | 13134/53136 [00:18<00:54, 732.05it/s] 25%|██▍       | 13212/53136 [00:18<00:53, 745.43it/s] 25%|██▌       | 13290/53136 [00:18<00:53, 750.82it/s] 25%|██▌       | 13367/53136 [00:18<00:53, 750.22it/s] 25%|██▌       | 13447/53136 [00:18<00:52, 762.23it/s] 25%|██▌       | 13524/53136 [00:18<00:53, 745.15it/s] 26%|██▌       | 13599/53136 [00:18<00:58, 673.60it/s] 26%|██▌       | 13693/53136 [00:18<00:53, 737.21it/s] 26%|██▌       | 13782/53136 [00:19<00:51, 763.37it/s] 26%|██▌       | 13860/53136 [00:19<00:51, 758.07it/s] 26%|██▌       | 13937/53136 [00:19<01:02, 628.42it/s] 26%|██▋       | 14007/53136 [00:19<01:00, 642.99it/s] 27%|██▋       | 14094/53136 [00:19<00:55, 700.31it/s] 27%|██▋       | 14167/53136 [00:19<00:59, 651.62it/s] 27%|██▋       | 14259/53136 [00:19<00:54, 709.44it/s] 27%|██▋       | 14333/53136 [00:19<01:02, 618.60it/s] 27%|██▋       | 14411/53136 [00:20<00:58, 657.99it/s] 27%|██▋       | 14484/53136 [00:20<00:57, 672.41it/s] 27%|██▋       | 14565/53136 [00:20<00:54, 701.76it/s] 28%|██▊       | 14684/53136 [00:20<00:46, 831.14it/s] 28%|██▊       | 14770/53136 [00:20<00:45, 835.92it/s] 28%|██▊       | 14856/53136 [00:20<00:46, 819.95it/s] 28%|██▊       | 14940/53136 [00:20<00:46, 814.37it/s] 28%|██▊       | 15023/53136 [00:20<00:56, 672.28it/s] 28%|██▊       | 15095/53136 [00:20<00:57, 656.61it/s] 29%|██▊       | 15179/53136 [00:21<00:54, 702.42it/s] 29%|██▊       | 15253/53136 [00:21<00:54, 693.25it/s] 29%|██▉       | 15325/53136 [00:21<00:58, 642.32it/s] 29%|██▉       | 15392/53136 [00:21<00:59, 637.26it/s] 29%|██▉       | 15457/53136 [00:21<01:01, 613.60it/s] 29%|██▉       | 15536/53136 [00:21<00:59, 636.64it/s] 29%|██▉       | 15602/53136 [00:21<00:59, 629.00it/s] 30%|██▉       | 15680/53136 [00:21<00:55, 669.03it/s] 30%|██▉       | 15758/53136 [00:21<00:53, 697.92it/s] 30%|██▉       | 15829/53136 [00:22<01:00, 618.72it/s] 30%|██▉       | 15897/53136 [00:22<00:58, 632.00it/s] 30%|███       | 15967/53136 [00:22<00:57, 649.07it/s] 30%|███       | 16050/53136 [00:22<00:53, 697.59it/s] 30%|███       | 16128/53136 [00:22<00:53, 693.32it/s] 30%|███       | 16205/53136 [00:22<00:53, 687.84it/s] 31%|███       | 16276/53136 [00:22<00:53, 692.79it/s] 31%|███       | 16346/53136 [00:22<00:55, 656.98it/s] 31%|███       | 16430/53136 [00:22<00:51, 706.93it/s] 31%|███       | 16512/53136 [00:23<00:49, 738.57it/s] 31%|███       | 16600/53136 [00:23<00:47, 773.24it/s] 31%|███▏      | 16688/53136 [00:23<00:45, 798.03it/s] 32%|███▏      | 16783/53136 [00:23<00:43, 840.87it/s] 32%|███▏      | 16869/53136 [00:23<00:43, 840.76it/s] 32%|███▏      | 16954/53136 [00:23<00:44, 811.85it/s] 32%|███▏      | 17036/53136 [00:23<00:46, 775.28it/s] 32%|███▏      | 17128/53136 [00:23<00:44, 812.33it/s] 32%|███▏      | 17211/53136 [00:23<00:44, 808.89it/s] 33%|███▎      | 17293/53136 [00:24<00:50, 706.55it/s] 33%|███▎      | 17373/53136 [00:24<00:49, 728.24it/s] 33%|███▎      | 17448/53136 [00:24<00:49, 714.14it/s] 33%|███▎      | 17521/53136 [00:24<00:51, 697.38it/s] 33%|███▎      | 17612/53136 [00:24<00:47, 752.56it/s] 33%|███▎      | 17698/53136 [00:24<00:45, 781.13it/s] 33%|███▎      | 17778/53136 [00:24<00:45, 771.59it/s] 34%|███▎      | 17856/53136 [00:24<00:53, 663.78it/s] 34%|███▎      | 17926/53136 [00:24<00:55, 638.81it/s] 34%|███▍      | 18022/53136 [00:25<00:48, 721.66it/s] 34%|███▍      | 18097/53136 [00:25<00:48, 718.00it/s] 34%|███▍      | 18171/53136 [00:25<00:49, 702.37it/s] 34%|███▍      | 18248/53136 [00:25<00:49, 704.95it/s] 34%|███▍      | 18320/53136 [00:25<00:57, 609.25it/s] 35%|███▍      | 18395/53136 [00:25<00:53, 643.50it/s] 35%|███▍      | 18462/53136 [00:25<00:56, 613.44it/s] 35%|███▍      | 18526/53136 [00:25<00:58, 592.82it/s] 35%|███▌      | 18602/53136 [00:26<00:54, 630.16it/s] 35%|███▌      | 18667/53136 [00:26<01:01, 564.24it/s] 35%|███▌      | 18746/53136 [00:26<00:55, 621.46it/s] 35%|███▌      | 18839/53136 [00:26<00:49, 696.59it/s] 36%|███▌      | 18930/53136 [00:26<00:45, 753.75it/s] 36%|███▌      | 19016/53136 [00:26<00:43, 783.60it/s] 36%|███▌      | 19101/53136 [00:26<00:42, 801.92it/s] 36%|███▌      | 19183/53136 [00:26<00:44, 771.08it/s] 36%|███▋      | 19265/53136 [00:26<00:45, 746.06it/s] 36%|███▋      | 19341/53136 [00:27<00:47, 713.99it/s] 37%|███▋      | 19421/53136 [00:27<00:45, 735.01it/s] 37%|███▋      | 19496/53136 [00:27<00:50, 667.39it/s] 37%|███▋      | 19566/53136 [00:27<00:49, 671.76it/s] 37%|███▋      | 19650/53136 [00:27<00:46, 716.04it/s] 37%|███▋      | 19723/53136 [00:27<00:50, 658.70it/s] 37%|███▋      | 19795/53136 [00:27<00:49, 674.36it/s] 37%|███▋      | 19864/53136 [00:27<00:49, 670.27it/s] 38%|███▊      | 19943/53136 [00:27<00:47, 699.17it/s] 38%|███▊      | 20025/53136 [00:28<00:45, 726.08it/s] 38%|███▊      | 20109/53136 [00:28<00:43, 756.72it/s] 38%|███▊      | 20186/53136 [00:28<00:54, 600.71it/s] 38%|███▊      | 20258/53136 [00:28<00:52, 629.99it/s] 38%|███▊      | 20326/53136 [00:28<00:52, 626.05it/s] 38%|███▊      | 20396/53136 [00:28<00:51, 637.56it/s] 39%|███▊      | 20480/53136 [00:28<00:47, 692.00it/s] 39%|███▊      | 20552/53136 [00:28<00:58, 557.32it/s] 39%|███▉      | 20619/53136 [00:29<00:56, 572.24it/s] 39%|███▉      | 20713/53136 [00:29<00:48, 664.56it/s] 39%|███▉      | 20793/53136 [00:29<00:46, 696.31it/s] 39%|███▉      | 20867/53136 [00:29<00:46, 695.30it/s] 39%|███▉      | 20949/53136 [00:29<00:44, 729.78it/s] 40%|███▉      | 21024/53136 [00:29<00:45, 711.81it/s] 40%|███▉      | 21105/53136 [00:29<00:44, 719.12it/s] 40%|███▉      | 21178/53136 [00:29<00:48, 659.45it/s] 40%|████      | 21261/53136 [00:29<00:45, 704.62it/s] 40%|████      | 21334/53136 [00:29<00:46, 679.79it/s] 40%|████      | 21404/53136 [00:30<00:46, 680.24it/s] 40%|████      | 21489/53136 [00:30<00:43, 725.85it/s] 41%|████      | 21570/53136 [00:30<00:44, 705.28it/s] 41%|████      | 21642/53136 [00:30<00:47, 666.05it/s] 41%|████      | 21710/53136 [00:30<00:58, 533.40it/s] 41%|████      | 21773/53136 [00:30<00:56, 554.70it/s] 41%|████      | 21865/53136 [00:30<00:48, 646.19it/s] 41%|████▏     | 21934/53136 [00:30<00:47, 655.43it/s] 41%|████▏     | 22014/53136 [00:31<00:45, 691.38it/s] 42%|████▏     | 22086/53136 [00:31<00:47, 653.59it/s] 42%|████▏     | 22155/53136 [00:31<00:46, 662.78it/s] 42%|████▏     | 22223/53136 [00:31<00:48, 638.52it/s] 42%|████▏     | 22298/53136 [00:31<00:46, 669.23it/s] 42%|████▏     | 22381/53136 [00:31<00:43, 706.78it/s] 42%|████▏     | 22463/53136 [00:31<00:43, 712.79it/s] 42%|████▏     | 22548/53136 [00:31<00:40, 750.99it/s] 43%|████▎     | 22636/53136 [00:31<00:38, 786.59it/s] 43%|████▎     | 22723/53136 [00:32<00:37, 809.99it/s] 43%|████▎     | 22805/53136 [00:32<00:38, 790.53it/s] 43%|████▎     | 22885/53136 [00:32<00:40, 739.10it/s] 43%|████▎     | 22966/53136 [00:32<00:40, 737.57it/s] 43%|████▎     | 23041/53136 [00:32<00:47, 637.76it/s] 44%|████▎     | 23137/53136 [00:32<00:41, 716.08it/s] 44%|████▎     | 23212/53136 [00:32<00:42, 705.74it/s] 44%|████▍     | 23301/53136 [00:32<00:44, 676.68it/s] 44%|████▍     | 23381/53136 [00:32<00:42, 695.77it/s] 44%|████▍     | 23453/53136 [00:33<00:44, 662.75it/s] 44%|████▍     | 23523/53136 [00:33<00:44, 672.21it/s] 44%|████▍     | 23592/53136 [00:33<00:50, 584.46it/s] 45%|████▍     | 23653/53136 [00:33<00:50, 582.68it/s] 45%|████▍     | 23736/53136 [00:33<00:45, 646.45it/s] 45%|████▍     | 23812/53136 [00:33<00:44, 652.10it/s] 45%|████▍     | 23886/53136 [00:33<00:43, 675.70it/s] 45%|████▌     | 23956/53136 [00:33<00:42, 681.49it/s] 45%|████▌     | 24044/53136 [00:33<00:39, 732.95it/s] 45%|████▌     | 24119/53136 [00:34<00:39, 729.52it/s] 46%|████▌     | 24204/53136 [00:34<00:37, 763.29it/s] 46%|████▌     | 24281/53136 [00:34<00:41, 699.90it/s] 46%|████▌     | 24356/53136 [00:34<00:40, 710.98it/s] 46%|████▌     | 24439/53136 [00:34<00:38, 743.86it/s] 46%|████▌     | 24529/53136 [00:34<00:36, 788.03it/s] 46%|████▋     | 24621/53136 [00:34<00:34, 824.35it/s] 46%|████▋     | 24705/53136 [00:34<00:34, 827.12it/s] 47%|████▋     | 24789/53136 [00:34<00:37, 747.54it/s] 47%|████▋     | 24882/53136 [00:35<00:35, 795.19it/s] 47%|████▋     | 24964/53136 [00:35<00:40, 688.11it/s] 47%|████▋     | 25059/53136 [00:35<00:37, 754.05it/s] 47%|████▋     | 25144/53136 [00:35<00:36, 768.06it/s] 47%|████▋     | 25224/53136 [00:35<00:37, 747.35it/s] 48%|████▊     | 25325/53136 [00:35<00:33, 818.88it/s] 48%|████▊     | 25409/53136 [00:35<00:37, 747.67it/s] 48%|████▊     | 25487/53136 [00:35<00:42, 654.35it/s] 48%|████▊     | 25566/53136 [00:36<00:40, 687.10it/s] 48%|████▊     | 25638/53136 [00:36<00:41, 658.62it/s] 48%|████▊     | 25737/53136 [00:36<00:37, 727.03it/s] 49%|████▊     | 25820/53136 [00:36<00:36, 754.03it/s] 49%|████▊     | 25898/53136 [00:36<00:36, 740.05it/s] 49%|████▉     | 25979/53136 [00:36<00:36, 754.12it/s] 49%|████▉     | 26060/53136 [00:36<00:35, 768.54it/s] 49%|████▉     | 26138/53136 [00:36<00:45, 598.58it/s] 49%|████▉     | 26211/53136 [00:36<00:43, 615.39it/s] 49%|████▉     | 26279/53136 [00:37<00:42, 630.82it/s] 50%|████▉     | 26346/53136 [00:37<00:42, 637.41it/s] 50%|████▉     | 26426/53136 [00:37<00:39, 679.74it/s] 50%|████▉     | 26516/53136 [00:37<00:35, 740.62it/s] 50%|█████     | 26592/53136 [00:37<00:42, 629.89it/s] 50%|█████     | 26671/53136 [00:37<00:40, 655.60it/s] 50%|█████     | 26757/53136 [00:37<00:37, 708.37it/s] 50%|█████     | 26832/53136 [00:37<00:36, 717.25it/s] 51%|█████     | 26906/53136 [00:37<00:36, 723.08it/s] 51%|█████     | 26980/53136 [00:38<00:36, 719.51it/s] 51%|█████     | 27065/53136 [00:38<00:34, 756.85it/s] 51%|█████     | 27142/53136 [00:38<00:40, 642.85it/s] 51%|█████     | 27214/53136 [00:38<00:39, 662.05it/s] 51%|█████▏    | 27293/53136 [00:38<00:37, 690.61it/s] 51%|█████▏    | 27365/53136 [00:38<00:39, 652.41it/s] 52%|█████▏    | 27461/53136 [00:38<00:35, 722.08it/s] 52%|█████▏    | 27535/53136 [00:38<00:36, 703.37it/s] 52%|█████▏    | 27607/53136 [00:39<00:40, 623.93it/s] 52%|█████▏    | 27672/53136 [00:39<00:41, 619.56it/s] 52%|█████▏    | 27779/53136 [00:39<00:34, 736.70it/s] 52%|█████▏    | 27859/53136 [00:39<00:33, 750.01it/s] 53%|█████▎    | 27936/53136 [00:39<00:34, 740.63it/s] 53%|█████▎    | 28012/53136 [00:39<00:38, 658.96it/s] 53%|█████▎    | 28081/53136 [00:39<00:38, 648.88it/s] 53%|█████▎    | 28148/53136 [00:39<00:40, 624.23it/s] 53%|█████▎    | 28236/53136 [00:39<00:35, 692.43it/s] 53%|█████▎    | 28307/53136 [00:40<00:39, 621.37it/s] 53%|█████▎    | 28393/53136 [00:40<00:36, 675.05it/s] 54%|█████▎    | 28498/53136 [00:40<00:31, 774.88it/s] 54%|█████▍    | 28586/53136 [00:40<00:30, 797.44it/s] 54%|█████▍    | 28678/53136 [00:40<00:29, 830.69it/s] 54%|█████▍    | 28763/53136 [00:40<00:31, 781.46it/s] 54%|█████▍    | 28843/53136 [00:40<00:32, 740.48it/s] 54%|█████▍    | 28936/53136 [00:40<00:30, 789.68it/s] 55%|█████▍    | 29034/53136 [00:40<00:28, 842.32it/s] 55%|█████▍    | 29120/53136 [00:41<00:34, 689.72it/s] 55%|█████▍    | 29195/53136 [00:41<00:34, 685.24it/s] 55%|█████▌    | 29268/53136 [00:41<00:35, 679.42it/s] 55%|█████▌    | 29339/53136 [00:41<00:35, 673.93it/s] 55%|█████▌    | 29409/53136 [00:41<00:35, 669.06it/s] 56%|█████▌    | 29507/53136 [00:41<00:31, 748.01it/s] 56%|█████▌    | 29591/53136 [00:41<00:30, 773.43it/s] 56%|█████▌    | 29670/53136 [00:41<00:30, 765.06it/s] 56%|█████▌    | 29748/53136 [00:42<00:40, 578.99it/s] 56%|█████▌    | 29813/53136 [00:42<00:40, 581.12it/s] 56%|█████▋    | 29908/53136 [00:42<00:34, 665.86it/s] 56%|█████▋    | 29983/53136 [00:42<00:33, 685.90it/s] 57%|█████▋    | 30056/53136 [00:42<00:41, 551.08it/s] 57%|█████▋    | 30129/53136 [00:42<00:38, 592.36it/s] 57%|█████▋    | 30204/53136 [00:42<00:36, 631.22it/s] 57%|█████▋    | 30301/53136 [00:42<00:31, 720.16it/s] 57%|█████▋    | 30383/53136 [00:42<00:30, 744.12it/s] 57%|█████▋    | 30461/53136 [00:43<00:38, 594.93it/s] 57%|█████▋    | 30537/53136 [00:43<00:35, 633.07it/s] 58%|█████▊    | 30607/53136 [00:43<00:36, 625.05it/s] 58%|█████▊    | 30681/53136 [00:43<00:34, 649.94it/s] 58%|█████▊    | 30767/53136 [00:43<00:31, 705.33it/s] 58%|█████▊    | 30841/53136 [00:43<00:32, 688.72it/s] 58%|█████▊    | 30920/53136 [00:43<00:31, 716.36it/s] 58%|█████▊    | 30994/53136 [00:43<00:34, 642.63it/s] 58%|█████▊    | 31074/53136 [00:44<00:32, 680.12it/s] 59%|█████▊    | 31145/53136 [00:44<00:32, 678.54it/s] 59%|█████▉    | 31222/53136 [00:44<00:31, 703.55it/s] 59%|█████▉    | 31320/53136 [00:44<00:28, 765.18it/s] 59%|█████▉    | 31418/53136 [00:44<00:26, 825.01it/s] 59%|█████▉    | 31502/53136 [00:44<00:29, 729.11it/s] 59%|█████▉    | 31583/53136 [00:44<00:28, 749.82it/s] 60%|█████▉    | 31661/53136 [00:44<00:28, 749.04it/s] 60%|█████▉    | 31748/53136 [00:44<00:27, 782.17it/s] 60%|█████▉    | 31845/53136 [00:45<00:25, 829.85it/s] 60%|██████    | 31930/53136 [00:45<00:27, 764.51it/s] 60%|██████    | 32009/53136 [00:45<00:32, 648.96it/s] 60%|██████    | 32099/53136 [00:45<00:29, 709.91it/s] 61%|██████    | 32204/53136 [00:45<00:26, 796.79it/s] 61%|██████    | 32288/53136 [00:45<00:28, 738.10it/s] 61%|██████    | 32366/53136 [00:45<00:28, 723.21it/s] 61%|██████    | 32450/53136 [00:45<00:27, 743.37it/s] 61%|██████    | 32527/53136 [00:45<00:27, 750.43it/s] 61%|██████▏   | 32616/53136 [00:46<00:26, 789.21it/s] 62%|██████▏   | 32697/53136 [00:46<00:27, 739.77it/s] 62%|██████▏   | 32773/53136 [00:46<00:27, 741.20it/s] 62%|██████▏   | 32849/53136 [00:46<00:27, 742.60it/s] 62%|██████▏   | 32934/53136 [00:46<00:26, 761.35it/s] 62%|██████▏   | 33013/53136 [00:46<00:26, 768.81it/s] 62%|██████▏   | 33093/53136 [00:46<00:26, 769.54it/s] 62%|██████▏   | 33171/53136 [00:46<00:26, 762.56it/s] 63%|██████▎   | 33248/53136 [00:46<00:26, 741.58it/s] 63%|██████▎   | 33323/53136 [00:47<00:33, 589.55it/s] 63%|██████▎   | 33416/53136 [00:47<00:29, 672.91it/s] 63%|██████▎   | 33506/53136 [00:47<00:26, 730.41it/s] 63%|██████▎   | 33584/53136 [00:47<00:28, 676.45it/s] 63%|██████▎   | 33657/53136 [00:47<00:28, 688.66it/s] 63%|██████▎   | 33729/53136 [00:47<00:31, 623.93it/s] 64%|██████▎   | 33795/53136 [00:47<00:33, 575.88it/s] 64%|██████▎   | 33870/53136 [00:47<00:31, 617.05it/s] 64%|██████▍   | 33967/53136 [00:48<00:27, 705.27it/s] 64%|██████▍   | 34053/53136 [00:48<00:25, 741.80it/s] 64%|██████▍   | 34130/53136 [00:48<00:25, 748.12it/s] 64%|██████▍   | 34207/53136 [00:48<00:25, 743.38it/s] 65%|██████▍   | 34283/53136 [00:48<00:25, 735.00it/s] 65%|██████▍   | 34358/53136 [00:48<00:29, 637.45it/s] 65%|██████▍   | 34450/53136 [00:48<00:27, 670.67it/s] 65%|██████▍   | 34519/53136 [00:48<00:28, 644.37it/s] 65%|██████▌   | 34585/53136 [00:48<00:29, 635.29it/s] 65%|██████▌   | 34665/53136 [00:49<00:27, 677.07it/s] 65%|██████▌   | 34751/53136 [00:49<00:25, 726.49it/s] 66%|██████▌   | 34825/53136 [00:49<00:25, 728.65it/s] 66%|██████▌   | 34899/53136 [00:49<00:29, 615.29it/s] 66%|██████▌   | 34964/53136 [00:49<00:31, 584.66it/s] 66%|██████▌   | 35025/53136 [00:49<00:33, 534.87it/s] 66%|██████▌   | 35085/53136 [00:49<00:32, 550.98it/s] 66%|██████▌   | 35152/53136 [00:49<00:30, 581.58it/s] 66%|██████▋   | 35212/53136 [00:50<00:33, 541.69it/s] 66%|██████▋   | 35289/53136 [00:50<00:29, 601.32it/s] 67%|██████▋   | 35365/53136 [00:50<00:27, 642.20it/s] 67%|██████▋   | 35444/53136 [00:50<00:25, 682.54it/s] 67%|██████▋   | 35514/53136 [00:50<00:28, 615.10it/s] 67%|██████▋   | 35604/53136 [00:50<00:25, 690.03it/s] 67%|██████▋   | 35681/53136 [00:50<00:24, 711.27it/s] 67%|██████▋   | 35763/53136 [00:50<00:23, 740.97it/s] 67%|██████▋   | 35839/53136 [00:50<00:23, 739.55it/s] 68%|██████▊   | 35914/53136 [00:51<00:26, 650.72it/s] 68%|██████▊   | 35982/53136 [00:51<00:27, 624.37it/s] 68%|██████▊   | 36068/53136 [00:51<00:25, 680.57it/s] 68%|██████▊   | 36138/53136 [00:51<00:27, 614.71it/s] 68%|██████▊   | 36202/53136 [00:51<00:27, 611.39it/s] 68%|██████▊   | 36265/53136 [00:51<00:30, 552.36it/s] 68%|██████▊   | 36342/53136 [00:51<00:27, 605.62it/s] 69%|██████▊   | 36405/53136 [00:51<00:27, 608.44it/s] 69%|██████▊   | 36468/53136 [00:51<00:27, 609.56it/s] 69%|██████▉   | 36568/53136 [00:52<00:24, 675.75it/s] 69%|██████▉   | 36636/53136 [00:52<00:24, 667.35it/s] 69%|██████▉   | 36703/53136 [00:52<00:26, 622.41it/s] 69%|██████▉   | 36766/53136 [00:52<00:27, 605.65it/s] 69%|██████▉   | 36827/53136 [00:52<00:28, 573.32it/s] 69%|██████▉   | 36907/53136 [00:52<00:25, 632.38it/s] 70%|██████▉   | 36972/53136 [00:52<00:25, 630.48it/s] 70%|██████▉   | 37046/53136 [00:52<00:24, 659.78it/s] 70%|██████▉   | 37113/53136 [00:52<00:24, 653.90it/s] 70%|██████▉   | 37179/53136 [00:53<00:24, 641.50it/s] 70%|███████   | 37248/53136 [00:53<00:24, 651.64it/s] 70%|███████   | 37314/53136 [00:53<00:24, 643.62it/s] 70%|███████   | 37387/53136 [00:53<00:25, 613.69it/s] 71%|███████   | 37462/53136 [00:53<00:24, 650.72it/s] 71%|███████   | 37546/53136 [00:53<00:22, 697.78it/s] 71%|███████   | 37630/53136 [00:53<00:21, 737.90it/s] 71%|███████   | 37707/53136 [00:53<00:20, 745.99it/s] 71%|███████   | 37783/53136 [00:53<00:21, 725.35it/s] 71%|███████▏  | 37868/53136 [00:54<00:20, 760.41it/s] 71%|███████▏  | 37952/53136 [00:54<00:19, 779.37it/s] 72%|███████▏  | 38031/53136 [00:54<00:20, 747.52it/s] 72%|███████▏  | 38107/53136 [00:54<00:21, 709.54it/s] 72%|███████▏  | 38179/53136 [00:54<00:25, 594.34it/s] 72%|███████▏  | 38260/53136 [00:54<00:23, 644.77it/s] 72%|███████▏  | 38328/53136 [00:54<00:23, 633.05it/s] 72%|███████▏  | 38394/53136 [00:54<00:24, 594.36it/s] 72%|███████▏  | 38460/53136 [00:54<00:24, 609.04it/s] 73%|███████▎  | 38540/53136 [00:55<00:22, 660.09it/s] 73%|███████▎  | 38608/53136 [00:55<00:22, 648.02it/s] 73%|███████▎  | 38691/53136 [00:55<00:20, 694.61it/s] 73%|███████▎  | 38762/53136 [00:55<00:21, 667.95it/s] 73%|███████▎  | 38831/53136 [00:55<00:21, 673.44it/s] 73%|███████▎  | 38899/53136 [00:55<00:22, 639.81it/s] 73%|███████▎  | 38992/53136 [00:55<00:19, 720.39it/s] 74%|███████▎  | 39066/53136 [00:55<00:19, 709.34it/s] 74%|███████▎  | 39141/53136 [00:55<00:19, 719.72it/s] 74%|███████▍  | 39230/53136 [00:56<00:18, 767.40it/s] 74%|███████▍  | 39310/53136 [00:56<00:17, 774.88it/s] 74%|███████▍  | 39388/53136 [00:56<00:17, 766.99it/s] 74%|███████▍  | 39466/53136 [00:56<00:18, 753.00it/s] 74%|███████▍  | 39542/53136 [00:56<00:22, 605.13it/s] 75%|███████▍  | 39608/53136 [00:56<00:22, 608.90it/s] 75%|███████▍  | 39679/53136 [00:56<00:21, 624.04it/s] 75%|███████▍  | 39763/53136 [00:56<00:19, 679.23it/s] 75%|███████▍  | 39849/53136 [00:56<00:18, 726.26it/s] 75%|███████▌  | 39939/53136 [00:57<00:17, 774.21it/s] 75%|███████▌  | 40028/53136 [00:57<00:16, 801.04it/s] 75%|███████▌  | 40110/53136 [00:57<00:18, 691.79it/s] 76%|███████▌  | 40205/53136 [00:57<00:17, 755.45it/s] 76%|███████▌  | 40284/53136 [00:57<00:17, 734.01it/s] 76%|███████▌  | 40360/53136 [00:57<00:17, 713.98it/s] 76%|███████▌  | 40437/53136 [00:57<00:17, 727.71it/s] 76%|███████▋  | 40520/53136 [00:57<00:16, 747.05it/s] 76%|███████▋  | 40607/53136 [00:57<00:16, 780.09it/s] 77%|███████▋  | 40691/53136 [00:58<00:15, 797.26it/s] 77%|███████▋  | 40772/53136 [00:58<00:18, 679.45it/s] 77%|███████▋  | 40845/53136 [00:58<00:17, 690.98it/s] 77%|███████▋  | 40935/53136 [00:58<00:16, 747.01it/s] 77%|███████▋  | 41016/53136 [00:58<00:15, 757.61it/s] 77%|███████▋  | 41106/53136 [00:58<00:15, 785.71it/s] 78%|███████▊  | 41186/53136 [00:58<00:19, 621.71it/s] 78%|███████▊  | 41255/53136 [00:58<00:19, 624.41it/s] 78%|███████▊  | 41342/53136 [00:59<00:17, 683.63it/s] 78%|███████▊  | 41418/53136 [00:59<00:16, 703.37it/s] 78%|███████▊  | 41502/53136 [00:59<00:15, 738.68it/s] 78%|███████▊  | 41597/53136 [00:59<00:14, 797.66it/s] 78%|███████▊  | 41679/53136 [00:59<00:15, 724.05it/s] 79%|███████▊  | 41755/53136 [00:59<00:15, 719.87it/s] 79%|███████▊  | 41834/53136 [00:59<00:15, 739.03it/s] 79%|███████▉  | 41910/53136 [00:59<00:17, 656.74it/s] 79%|███████▉  | 41985/53136 [00:59<00:16, 677.43it/s] 79%|███████▉  | 42055/53136 [01:00<00:16, 658.34it/s] 79%|███████▉  | 42133/53136 [01:00<00:15, 690.23it/s] 79%|███████▉  | 42210/53136 [01:00<00:15, 705.84it/s] 80%|███████▉  | 42282/53136 [01:00<00:15, 704.85it/s] 80%|███████▉  | 42373/53136 [01:00<00:14, 762.68it/s] 80%|███████▉  | 42455/53136 [01:00<00:13, 778.79it/s] 80%|████████  | 42534/53136 [01:00<00:13, 759.55it/s] 80%|████████  | 42613/53136 [01:00<00:13, 768.01it/s] 80%|████████  | 42705/53136 [01:00<00:12, 810.92it/s] 81%|████████  | 42787/53136 [01:00<00:12, 803.74it/s] 81%|████████  | 42868/53136 [01:01<00:13, 772.21it/s] 81%|████████  | 42957/53136 [01:01<00:12, 805.44it/s] 81%|████████  | 43038/53136 [01:01<00:13, 768.13it/s] 81%|████████  | 43116/53136 [01:01<00:16, 613.71it/s] 81%|████████▏ | 43183/53136 [01:01<00:15, 623.94it/s] 81%|████████▏ | 43274/53136 [01:01<00:14, 688.82it/s] 82%|████████▏ | 43361/53136 [01:01<00:13, 732.56it/s] 82%|████████▏ | 43438/53136 [01:01<00:13, 713.01it/s] 82%|████████▏ | 43512/53136 [01:02<00:14, 661.72it/s] 82%|████████▏ | 43581/53136 [01:02<00:16, 583.90it/s] 82%|████████▏ | 43663/53136 [01:02<00:14, 638.98it/s] 82%|████████▏ | 43730/53136 [01:02<00:15, 598.11it/s] 82%|████████▏ | 43808/53136 [01:02<00:14, 643.92it/s] 83%|████████▎ | 43885/53136 [01:02<00:13, 677.34it/s] 83%|████████▎ | 43955/53136 [01:02<00:15, 598.72it/s] 83%|████████▎ | 44043/53136 [01:02<00:13, 667.66it/s] 83%|████████▎ | 44115/53136 [01:02<00:13, 676.03it/s] 83%|████████▎ | 44185/53136 [01:03<00:15, 581.01it/s] 83%|████████▎ | 44274/53136 [01:03<00:14, 604.53it/s] 83%|████████▎ | 44345/53136 [01:03<00:14, 621.99it/s] 84%|████████▎ | 44410/53136 [01:03<00:13, 628.14it/s] 84%|████████▎ | 44487/53136 [01:03<00:12, 665.61it/s] 84%|████████▍ | 44564/53136 [01:03<00:12, 694.14it/s] 84%|████████▍ | 44663/53136 [01:03<00:10, 774.38it/s] 84%|████████▍ | 44742/53136 [01:03<00:11, 746.19it/s] 84%|████████▍ | 44830/53136 [01:04<00:10, 776.66it/s] 85%|████████▍ | 44921/53136 [01:04<00:10, 813.59it/s] 85%|████████▍ | 45004/53136 [01:04<00:11, 730.87it/s] 85%|████████▍ | 45102/53136 [01:04<00:10, 797.49it/s] 85%|████████▌ | 45192/53136 [01:04<00:09, 825.24it/s] 85%|████████▌ | 45277/53136 [01:04<00:10, 774.80it/s] 85%|████████▌ | 45357/53136 [01:04<00:10, 748.85it/s] 86%|████████▌ | 45434/53136 [01:04<00:10, 711.00it/s] 86%|████████▌ | 45512/53136 [01:04<00:10, 729.09it/s] 86%|████████▌ | 45586/53136 [01:05<00:11, 684.27it/s] 86%|████████▌ | 45656/53136 [01:05<00:11, 659.28it/s] 86%|████████▌ | 45723/53136 [01:05<00:12, 612.65it/s] 86%|████████▌ | 45792/53136 [01:05<00:11, 626.77it/s] 86%|████████▋ | 45876/53136 [01:05<00:10, 683.25it/s] 86%|████████▋ | 45961/53136 [01:05<00:09, 729.59it/s] 87%|████████▋ | 46036/53136 [01:05<00:10, 709.86it/s] 87%|████████▋ | 46122/53136 [01:05<00:09, 751.85it/s] 87%|████████▋ | 46199/53136 [01:05<00:09, 703.95it/s] 87%|████████▋ | 46277/53136 [01:06<00:09, 716.19it/s] 87%|████████▋ | 46363/53136 [01:06<00:08, 756.21it/s] 87%|████████▋ | 46457/53136 [01:06<00:08, 808.74it/s] 88%|████████▊ | 46543/53136 [01:06<00:08, 821.92it/s] 88%|████████▊ | 46626/53136 [01:06<00:09, 697.09it/s] 88%|████████▊ | 46706/53136 [01:06<00:08, 718.51it/s] 88%|████████▊ | 46781/53136 [01:06<00:09, 651.71it/s] 88%|████████▊ | 46864/53136 [01:06<00:09, 691.53it/s] 88%|████████▊ | 46936/53136 [01:06<00:09, 654.53it/s] 88%|████████▊ | 47022/53136 [01:07<00:08, 707.98it/s] 89%|████████▊ | 47095/53136 [01:07<00:08, 684.63it/s] 89%|████████▉ | 47187/53136 [01:07<00:07, 747.41it/s] 89%|████████▉ | 47264/53136 [01:07<00:08, 689.39it/s] 89%|████████▉ | 47344/53136 [01:07<00:08, 709.22it/s] 89%|████████▉ | 47417/53136 [01:07<00:08, 651.33it/s] 89%|████████▉ | 47487/53136 [01:07<00:08, 662.58it/s] 90%|████████▉ | 47561/53136 [01:07<00:08, 677.24it/s] 90%|████████▉ | 47650/53136 [01:07<00:07, 735.71it/s] 90%|████████▉ | 47725/53136 [01:08<00:07, 716.71it/s] 90%|████████▉ | 47799/53136 [01:08<00:07, 720.98it/s] 90%|█████████ | 47872/53136 [01:08<00:09, 541.35it/s] 90%|█████████ | 47955/53136 [01:08<00:08, 608.49it/s] 90%|█████████ | 48033/53136 [01:08<00:07, 647.90it/s] 91%|█████████ | 48104/53136 [01:08<00:07, 631.74it/s] 91%|█████████ | 48171/53136 [01:08<00:07, 634.12it/s] 91%|█████████ | 48251/53136 [01:08<00:07, 674.71it/s] 91%|█████████ | 48321/53136 [01:09<00:07, 671.92it/s] 91%|█████████ | 48400/53136 [01:09<00:06, 704.14it/s] 91%|█████████ | 48472/53136 [01:09<00:06, 705.96it/s] 91%|█████████▏| 48544/53136 [01:09<00:06, 678.52it/s] 91%|█████████▏| 48613/53136 [01:09<00:06, 656.90it/s] 92%|█████████▏| 48689/53136 [01:09<00:06, 683.38it/s] 92%|█████████▏| 48758/53136 [01:09<00:07, 594.24it/s] 92%|█████████▏| 48820/53136 [01:09<00:07, 565.02it/s] 92%|█████████▏| 48879/53136 [01:09<00:07, 563.41it/s] 92%|█████████▏| 48956/53136 [01:10<00:06, 609.98it/s] 92%|█████████▏| 49048/53136 [01:10<00:05, 692.68it/s] 92%|█████████▏| 49119/53136 [01:10<00:05, 684.00it/s] 93%|█████████▎| 49194/53136 [01:10<00:05, 675.76it/s] 93%|█████████▎| 49263/53136 [01:10<00:06, 640.92it/s] 93%|█████████▎| 49328/53136 [01:10<00:06, 576.32it/s] 93%|█████████▎| 49408/53136 [01:10<00:05, 629.33it/s] 93%|█████████▎| 49493/53136 [01:10<00:05, 659.20it/s] 93%|█████████▎| 49562/53136 [01:10<00:05, 662.74it/s] 93%|█████████▎| 49632/53136 [01:11<00:05, 670.09it/s] 94%|█████████▎| 49710/53136 [01:11<00:04, 697.42it/s] 94%|█████████▎| 49781/53136 [01:11<00:05, 660.22it/s] 94%|█████████▍| 49885/53136 [01:11<00:04, 765.64it/s] 94%|█████████▍| 49963/53136 [01:11<00:04, 699.99it/s] 94%|█████████▍| 50056/53136 [01:11<00:04, 745.58it/s] 94%|█████████▍| 50145/53136 [01:11<00:03, 782.60it/s] 95%|█████████▍| 50225/53136 [01:11<00:03, 747.84it/s] 95%|█████████▍| 50301/53136 [01:11<00:03, 731.29it/s] 95%|█████████▍| 50386/53136 [01:12<00:03, 763.30it/s] 95%|█████████▍| 50464/53136 [01:12<00:03, 733.17it/s] 95%|█████████▌| 50539/53136 [01:12<00:03, 681.47it/s] 95%|█████████▌| 50609/53136 [01:12<00:04, 590.43it/s] 95%|█████████▌| 50683/53136 [01:12<00:03, 627.04it/s] 96%|█████████▌| 50778/53136 [01:12<00:03, 710.73it/s] 96%|█████████▌| 50852/53136 [01:12<00:03, 688.07it/s] 96%|█████████▌| 50931/53136 [01:12<00:03, 713.53it/s] 96%|█████████▌| 51005/53136 [01:13<00:03, 647.84it/s] 96%|█████████▌| 51072/53136 [01:13<00:03, 620.59it/s] 96%|█████████▋| 51159/53136 [01:13<00:02, 682.72it/s] 96%|█████████▋| 51242/53136 [01:13<00:02, 721.71it/s] 97%|█████████▋| 51316/53136 [01:13<00:02, 683.97it/s] 97%|█████████▋| 51386/53136 [01:13<00:02, 633.98it/s] 97%|█████████▋| 51451/53136 [01:13<00:02, 596.48it/s] 97%|█████████▋| 51524/53136 [01:13<00:02, 630.69it/s] 97%|█████████▋| 51589/53136 [01:13<00:02, 588.99it/s] 97%|█████████▋| 51650/53136 [01:14<00:03, 477.90it/s] 97%|█████████▋| 51740/53136 [01:14<00:02, 575.37it/s] 98%|█████████▊| 51817/53136 [01:14<00:02, 622.90it/s] 98%|█████████▊| 51897/53136 [01:14<00:01, 664.38it/s] 98%|█████████▊| 51976/53136 [01:14<00:01, 697.99it/s] 98%|█████████▊| 52060/53136 [01:14<00:01, 724.89it/s] 98%|█████████▊| 52135/53136 [01:14<00:01, 683.75it/s] 98%|█████████▊| 52206/53136 [01:14<00:01, 552.41it/s] 98%|█████████▊| 52267/53136 [01:15<00:01, 500.90it/s] 99%|█████████▊| 52346/53136 [01:15<00:01, 565.21it/s] 99%|█████████▊| 52408/53136 [01:15<00:01, 569.86it/s] 99%|█████████▉| 52473/53136 [01:15<00:01, 563.15it/s] 99%|█████████▉| 52544/53136 [01:15<00:00, 593.94it/s] 99%|█████████▉| 52618/53136 [01:15<00:00, 624.01it/s] 99%|█████████▉| 52682/53136 [01:15<00:00, 611.65it/s] 99%|█████████▉| 52768/53136 [01:15<00:00, 679.07it/s]100%|█████████▉| 52871/53136 [01:15<00:00, 773.76it/s]100%|█████████▉| 52950/53136 [01:16<00:00, 649.25it/s]100%|█████████▉| 53020/53136 [01:16<00:00, 642.11it/s]100%|█████████▉| 53088/53136 [01:16<00:00, 602.15it/s]100%|██████████| 53136/53136 [01:16<00:00, 694.83it/s]

gathering stats for n=1
  0%|          | 0/53136 [00:00<?, ?it/s]  1%|          | 274/53136 [00:00<00:19, 2737.07it/s]  1%|          | 580/53136 [00:00<00:17, 2924.44it/s]  2%|▏         | 873/53136 [00:00<00:19, 2727.30it/s]  2%|▏         | 1149/53136 [00:00<00:18, 2738.59it/s]  3%|▎         | 1429/53136 [00:00<00:18, 2755.95it/s]  3%|▎         | 1768/53136 [00:00<00:17, 2965.53it/s]  4%|▍         | 2066/53136 [00:00<00:17, 2924.22it/s]  4%|▍         | 2380/53136 [00:00<00:16, 2989.85it/s]  5%|▌         | 2680/53136 [00:00<00:17, 2894.23it/s]  6%|▌         | 2971/53136 [00:01<00:17, 2850.94it/s]  6%|▌         | 3257/53136 [00:01<00:19, 2607.28it/s]  7%|▋         | 3526/53136 [00:01<00:18, 2628.73it/s]  7%|▋         | 3792/53136 [00:01<00:19, 2559.89it/s]  8%|▊         | 4071/53136 [00:01<00:18, 2616.58it/s]  8%|▊         | 4350/53136 [00:01<00:18, 2665.16it/s]  9%|▊         | 4619/53136 [00:01<00:18, 2631.89it/s]  9%|▉         | 4884/53136 [00:01<00:18, 2583.36it/s] 10%|▉         | 5144/53136 [00:01<00:18, 2527.47it/s] 10%|█         | 5398/53136 [00:02<00:18, 2513.53it/s] 11%|█         | 5683/53136 [00:02<00:18, 2610.04it/s] 11%|█         | 5945/53136 [00:02<00:18, 2602.85it/s] 12%|█▏        | 6224/53136 [00:02<00:17, 2653.59it/s] 12%|█▏        | 6532/53136 [00:02<00:16, 2774.90it/s] 13%|█▎        | 6846/53136 [00:02<00:16, 2882.89it/s] 13%|█▎        | 7135/53136 [00:02<00:16, 2708.89it/s] 14%|█▍        | 7409/53136 [00:02<00:17, 2557.16it/s] 14%|█▍        | 7668/53136 [00:02<00:17, 2556.98it/s] 15%|█▌        | 7984/53136 [00:02<00:16, 2727.42it/s] 16%|█▌        | 8260/53136 [00:03<00:17, 2603.99it/s] 16%|█▌        | 8523/53136 [00:03<00:17, 2604.14it/s] 17%|█▋        | 8804/53136 [00:03<00:16, 2661.05it/s] 17%|█▋        | 9072/53136 [00:03<00:17, 2574.67it/s] 18%|█▊        | 9352/53136 [00:03<00:16, 2636.83it/s] 18%|█▊        | 9674/53136 [00:03<00:15, 2804.55it/s] 19%|█▊        | 9956/53136 [00:03<00:17, 2474.50it/s] 19%|█▉        | 10234/53136 [00:03<00:16, 2551.20it/s] 20%|█▉        | 10496/53136 [00:03<00:17, 2485.94it/s] 20%|██        | 10750/53136 [00:04<00:17, 2469.97it/s] 21%|██        | 11042/53136 [00:04<00:16, 2499.38it/s] 21%|██▏       | 11294/53136 [00:04<00:16, 2473.98it/s] 22%|██▏       | 11550/53136 [00:04<00:16, 2496.81it/s] 22%|██▏       | 11803/53136 [00:04<00:16, 2506.26it/s] 23%|██▎       | 12073/53136 [00:04<00:16, 2522.63it/s] 23%|██▎       | 12338/53136 [00:04<00:15, 2558.92it/s] 24%|██▎       | 12604/53136 [00:04<00:15, 2582.87it/s] 24%|██▍       | 12863/53136 [00:04<00:16, 2475.94it/s] 25%|██▍       | 13142/53136 [00:04<00:15, 2562.95it/s] 25%|██▌       | 13423/53136 [00:05<00:15, 2604.77it/s] 26%|██▌       | 13696/53136 [00:05<00:15, 2626.23it/s] 26%|██▋       | 13960/53136 [00:05<00:15, 2578.28it/s] 27%|██▋       | 14219/53136 [00:05<00:15, 2556.54it/s] 27%|██▋       | 14475/53136 [00:05<00:15, 2524.75it/s] 28%|██▊       | 14789/53136 [00:05<00:14, 2702.83it/s] 28%|██▊       | 15060/53136 [00:05<00:14, 2658.26it/s] 29%|██▉       | 15327/53136 [00:05<00:15, 2519.92it/s] 29%|██▉       | 15581/53136 [00:05<00:15, 2467.20it/s] 30%|██▉       | 15829/53136 [00:06<00:15, 2463.82it/s] 30%|███       | 16089/53136 [00:06<00:14, 2501.76it/s] 31%|███       | 16340/53136 [00:06<00:14, 2472.23it/s] 31%|███▏      | 16640/53136 [00:06<00:13, 2618.30it/s] 32%|███▏      | 16931/53136 [00:06<00:13, 2703.47it/s] 32%|███▏      | 17215/53136 [00:06<00:13, 2738.45it/s] 33%|███▎      | 17490/53136 [00:06<00:13, 2636.47it/s] 33%|███▎      | 17776/53136 [00:06<00:13, 2692.37it/s] 34%|███▍      | 18047/53136 [00:06<00:13, 2623.80it/s] 34%|███▍      | 18311/53136 [00:06<00:13, 2506.69it/s] 35%|███▍      | 18564/53136 [00:07<00:14, 2393.07it/s] 35%|███▌      | 18830/53136 [00:07<00:13, 2466.50it/s] 36%|███▌      | 19141/53136 [00:07<00:12, 2648.51it/s] 37%|███▋      | 19409/53136 [00:07<00:12, 2622.66it/s] 37%|███▋      | 19673/53136 [00:07<00:12, 2577.26it/s] 38%|███▊      | 19932/53136 [00:07<00:13, 2532.27it/s] 38%|███▊      | 20187/53136 [00:07<00:13, 2452.06it/s] 38%|███▊      | 20434/53136 [00:07<00:13, 2445.00it/s] 39%|███▉      | 20680/53136 [00:07<00:13, 2438.67it/s] 39%|███▉      | 20950/53136 [00:08<00:12, 2503.47it/s] 40%|███▉      | 21201/53136 [00:08<00:12, 2470.25it/s] 40%|████      | 21476/53136 [00:08<00:12, 2546.52it/s] 41%|████      | 21732/53136 [00:08<00:13, 2343.16it/s] 41%|████▏     | 22007/53136 [00:08<00:12, 2453.34it/s] 42%|████▏     | 22256/53136 [00:08<00:12, 2422.73it/s] 42%|████▏     | 22538/53136 [00:08<00:12, 2535.58it/s] 43%|████▎     | 22812/53136 [00:08<00:11, 2594.74it/s] 43%|████▎     | 23074/53136 [00:08<00:11, 2536.21it/s] 44%|████▍     | 23338/53136 [00:08<00:11, 2565.49it/s] 44%|████▍     | 23596/53136 [00:09<00:12, 2429.74it/s] 45%|████▍     | 23864/53136 [00:09<00:11, 2497.51it/s] 45%|████▌     | 24137/53136 [00:09<00:11, 2563.45it/s] 46%|████▌     | 24395/53136 [00:09<00:11, 2560.92it/s] 47%|████▋     | 24723/53136 [00:09<00:10, 2771.68it/s] 47%|████▋     | 25002/53136 [00:09<00:10, 2700.94it/s] 48%|████▊     | 25287/53136 [00:09<00:10, 2743.77it/s] 48%|████▊     | 25563/53136 [00:09<00:10, 2659.28it/s] 49%|████▊     | 25836/53136 [00:09<00:10, 2679.00it/s] 49%|████▉     | 26105/53136 [00:10<00:10, 2618.44it/s] 50%|████▉     | 26368/53136 [00:10<00:11, 2430.67it/s] 50%|█████     | 26635/53136 [00:10<00:10, 2496.39it/s] 51%|█████     | 26900/53136 [00:10<00:10, 2538.88it/s] 51%|█████     | 27156/53136 [00:10<00:10, 2513.93it/s] 52%|█████▏    | 27409/53136 [00:10<00:10, 2508.82it/s] 52%|█████▏    | 27661/53136 [00:10<00:10, 2430.39it/s] 53%|█████▎    | 27952/53136 [00:10<00:09, 2562.88it/s] 53%|█████▎    | 28210/53136 [00:10<00:09, 2501.78it/s] 54%|█████▎    | 28496/53136 [00:11<00:09, 2604.57it/s] 54%|█████▍    | 28779/53136 [00:11<00:09, 2667.18it/s] 55%|█████▍    | 29065/53136 [00:11<00:08, 2719.08it/s] 55%|█████▌    | 29338/53136 [00:11<00:09, 2582.25it/s] 56%|█████▌    | 29622/53136 [00:11<00:08, 2655.79it/s] 56%|█████▋    | 29890/53136 [00:11<00:09, 2476.65it/s] 57%|█████▋    | 30141/53136 [00:11<00:09, 2390.50it/s] 57%|█████▋    | 30401/53136 [00:11<00:09, 2409.54it/s] 58%|█████▊    | 30644/53136 [00:11<00:09, 2409.64it/s] 58%|█████▊    | 30920/53136 [00:11<00:08, 2499.06it/s] 59%|█████▊    | 31172/53136 [00:12<00:08, 2456.42it/s] 59%|█████▉    | 31474/53136 [00:12<00:08, 2615.89it/s] 60%|█████▉    | 31756/53136 [00:12<00:07, 2675.30it/s] 60%|██████    | 32025/53136 [00:12<00:08, 2604.03it/s] 61%|██████    | 32310/53136 [00:12<00:07, 2669.13it/s] 61%|██████▏   | 32578/53136 [00:12<00:07, 2656.00it/s] 62%|██████▏   | 32856/53136 [00:12<00:07, 2690.53it/s] 62%|██████▏   | 33126/53136 [00:12<00:07, 2687.69it/s] 63%|██████▎   | 33396/53136 [00:12<00:07, 2561.96it/s] 63%|██████▎   | 33654/53136 [00:13<00:07, 2560.14it/s] 64%|██████▍   | 33911/53136 [00:13<00:07, 2453.67it/s] 64%|██████▍   | 34212/53136 [00:13<00:07, 2602.86it/s] 65%|██████▍   | 34474/53136 [00:13<00:07, 2509.33it/s] 65%|██████▌   | 34729/53136 [00:13<00:07, 2517.77it/s] 66%|██████▌   | 34982/53136 [00:13<00:07, 2393.81it/s] 66%|██████▋   | 35224/53136 [00:13<00:07, 2264.88it/s] 67%|██████▋   | 35456/53136 [00:13<00:07, 2277.21it/s] 67%|██████▋   | 35737/53136 [00:13<00:07, 2427.05it/s] 68%|██████▊   | 35982/53136 [00:13<00:07, 2376.92it/s] 68%|██████▊   | 36222/53136 [00:14<00:07, 2328.01it/s] 69%|██████▊   | 36468/53136 [00:14<00:07, 2364.35it/s] 69%|██████▉   | 36706/53136 [00:14<00:07, 2345.02it/s] 70%|██████▉   | 36945/53136 [00:14<00:06, 2356.48it/s] 70%|██████▉   | 37182/53136 [00:14<00:06, 2321.51it/s] 70%|███████   | 37430/53136 [00:14<00:06, 2365.07it/s] 71%|███████   | 37714/53136 [00:14<00:06, 2502.47it/s] 71%|███████▏  | 37980/53136 [00:14<00:05, 2545.03it/s] 72%|███████▏  | 38235/53136 [00:14<00:06, 2445.60it/s] 72%|███████▏  | 38481/53136 [00:15<00:06, 2422.69it/s] 73%|███████▎  | 38739/53136 [00:15<00:05, 2415.95it/s] 73%|███████▎  | 39016/53136 [00:15<00:05, 2486.30it/s] 74%|███████▍  | 39302/53136 [00:15<00:05, 2593.76it/s] 74%|███████▍  | 39563/53136 [00:15<00:05, 2454.72it/s] 75%|███████▍  | 39832/53136 [00:15<00:05, 2514.72it/s] 75%|███████▌  | 40112/53136 [00:15<00:05, 2596.47it/s] 76%|███████▌  | 40380/53136 [00:15<00:04, 2604.79it/s] 77%|███████▋  | 40673/53136 [00:15<00:04, 2699.00it/s] 77%|███████▋  | 40944/53136 [00:15<00:04, 2661.09it/s] 78%|███████▊  | 41211/53136 [00:16<00:04, 2585.05it/s] 78%|███████▊  | 41477/53136 [00:16<00:04, 2606.26it/s] 79%|███████▊  | 41743/53136 [00:16<00:04, 2612.03it/s] 79%|███████▉  | 42005/53136 [00:16<00:04, 2556.24it/s] 80%|███████▉  | 42269/53136 [00:16<00:04, 2561.73it/s] 80%|████████  | 42569/53136 [00:16<00:03, 2689.42it/s] 81%|████████  | 42860/53136 [00:16<00:03, 2754.11it/s] 81%|████████  | 43136/53136 [00:16<00:03, 2578.83it/s] 82%|████████▏ | 43397/53136 [00:16<00:03, 2579.57it/s] 82%|████████▏ | 43657/53136 [00:17<00:03, 2500.29it/s] 83%|████████▎ | 43909/53136 [00:17<00:03, 2490.14it/s] 83%|████████▎ | 44160/53136 [00:17<00:03, 2372.25it/s] 84%|████████▎ | 44409/53136 [00:17<00:03, 2404.58it/s] 84%|████████▍ | 44698/53136 [00:17<00:03, 2542.63it/s] 85%|████████▍ | 44985/53136 [00:17<00:03, 2637.43it/s] 85%|████████▌ | 45275/53136 [00:17<00:02, 2711.38it/s] 86%|████████▌ | 45548/53136 [00:17<00:02, 2625.43it/s] 86%|████████▌ | 45812/53136 [00:17<00:02, 2511.34it/s] 87%|████████▋ | 46087/53136 [00:17<00:02, 2577.80it/s] 87%|████████▋ | 46347/53136 [00:18<00:02, 2572.57it/s] 88%|████████▊ | 46629/53136 [00:18<00:02, 2642.56it/s] 88%|████████▊ | 46895/53136 [00:18<00:02, 2573.52it/s] 89%|████████▉ | 47176/53136 [00:18<00:02, 2637.64it/s] 89%|████████▉ | 47441/53136 [00:18<00:02, 2583.58it/s] 90%|████████▉ | 47701/53136 [00:18<00:02, 2562.57it/s] 90%|█████████ | 47958/53136 [00:18<00:02, 2443.86it/s] 91%|█████████ | 48204/53136 [00:18<00:02, 2415.01it/s] 91%|█████████ | 48460/53136 [00:18<00:01, 2455.59it/s] 92%|█████████▏| 48714/53136 [00:19<00:01, 2479.17it/s] 92%|█████████▏| 48963/53136 [00:19<00:01, 2324.92it/s] 93%|█████████▎| 49205/53136 [00:19<00:01, 2344.57it/s] 93%|█████████▎| 49444/53136 [00:19<00:01, 2356.50it/s] 94%|█████████▎| 49683/53136 [00:19<00:01, 2363.56it/s] 94%|█████████▍| 49960/53136 [00:19<00:01, 2482.32it/s] 95%|█████████▍| 50234/53136 [00:19<00:01, 2557.46it/s] 95%|█████████▌| 50513/53136 [00:19<00:01, 2607.15it/s] 96%|█████████▌| 50775/53136 [00:19<00:00, 2513.86it/s] 96%|█████████▌| 51028/53136 [00:19<00:00, 2481.59it/s] 97%|█████████▋| 51281/53136 [00:20<00:00, 2494.27it/s] 97%|█████████▋| 51531/53136 [00:20<00:00, 2337.99it/s] 97%|█████████▋| 51767/53136 [00:20<00:00, 2301.75it/s] 98%|█████████▊| 52056/53136 [00:20<00:00, 2467.70it/s] 98%|█████████▊| 52305/53136 [00:20<00:00, 2159.29it/s] 99%|█████████▉| 52573/53136 [00:20<00:00, 2293.03it/s] 99%|█████████▉| 52838/53136 [00:20<00:00, 2388.46it/s]100%|█████████▉| 53083/53136 [00:20<00:00, 2296.26it/s]100%|██████████| 53136/53136 [00:20<00:00, 2544.05it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 63.56it/s]2022-03-17 09:20:22 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(35920, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=35920, bias=False)
  )
)
2022-03-17 09:20:22 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-17 09:20:22 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-17 09:20:22 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-17 09:20:22 | INFO | fairseq_cli.train | num. shared model params: 37,305,344 (num. trained: 37,305,344)
2022-03-17 09:20:22 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-17 09:20:22 | INFO | fairseq.data.data_utils | loaded 2,558 examples from: data-bin/ru/valid
2022-03-17 09:20:23 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-17 09:20:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-17 09:20:23 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-17 09:20:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-17 09:20:23 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-17 09:20:23 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-17 09:20:23 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-17 09:20:23 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-17 09:20:23 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-17 09:20:23 | INFO | fairseq.data.data_utils | loaded 53,136 examples from: data-bin/ru/train
2022-03-17 09:20:23 | INFO | fairseq.trainer | begin training epoch 1
2022-03-17 09:20:23 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-17 09:20:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-17 09:20:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 09:20:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 09:20:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-17 09:23:02 | INFO | train_inner | epoch 001:    104 / 407 loss=14.836, ppl=29243.2, wps=43886.6, ups=0.67, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.196, loss_scale=8, train_wall=155, gb_free=21, wall=160
2022-03-17 09:25:32 | INFO | train_inner | epoch 001:    204 / 407 loss=13.367, ppl=10568, wps=43625.2, ups=0.67, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=0.648, loss_scale=8, train_wall=146, gb_free=21, wall=310
2022-03-17 09:28:02 | INFO | train_inner | epoch 001:    304 / 407 loss=12.492, ppl=5762.18, wps=43739.5, ups=0.67, wpb=65525.8, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.417, loss_scale=8, train_wall=145, gb_free=21, wall=460
2022-03-17 09:30:32 | INFO | train_inner | epoch 001:    404 / 407 loss=12.056, ppl=4256.67, wps=43718, ups=0.67, wpb=65534.2, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.399, loss_scale=8, train_wall=145, gb_free=21, wall=610
2022-03-17 09:30:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 09:30:55 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.874 | ppl 3753.72 | wps 70268 | wpb 2047.5 | bsz 4 | num_updates 403
2022-03-17 09:30:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 403 updates
2022-03-17 09:30:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 09:30:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 09:30:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 1 @ 403 updates, score 11.874) (writing took 1.8050196785479784 seconds)
2022-03-17 09:30:57 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-17 09:30:57 | INFO | train | epoch 001 | loss 13.179 | ppl 9275.56 | wps 42277.8 | ups 0.65 | wpb 65492.3 | bsz 127.9 | num_updates 403 | lr 5.04649e-05 | gnorm 0.911 | loss_scale 8 | train_wall 595 | gb_free 21 | wall 635
KL Stats: Epoch 1 Divergences: Uniform: 0.6979871760736943 Unigram: 0.8213783584811776
2022-03-17 09:30:57 | INFO | fairseq.trainer | begin training epoch 2
2022-03-17 09:30:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 09:33:23 | INFO | train_inner | epoch 002:     97 / 407 loss=11.863, ppl=3724.38, wps=38152.4, ups=0.58, wpb=65361.9, bsz=127.7, num_updates=500, lr=6.25875e-05, gnorm=0.401, loss_scale=8, train_wall=145, gb_free=21, wall=781
2022-03-17 09:35:54 | INFO | train_inner | epoch 002:    197 / 407 loss=11.636, ppl=3183.47, wps=43430.2, ups=0.66, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.459, loss_scale=16, train_wall=146, gb_free=21, wall=932
2022-03-17 09:38:25 | INFO | train_inner | epoch 002:    297 / 407 loss=11.341, ppl=2594.05, wps=43438.4, ups=0.66, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.48, loss_scale=16, train_wall=146, gb_free=21, wall=1083
2022-03-17 09:40:56 | INFO | train_inner | epoch 002:    397 / 407 loss=11, ppl=2048.15, wps=43445.9, ups=0.66, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.518, loss_scale=16, train_wall=146, gb_free=21, wall=1234
2022-03-17 09:41:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 09:41:30 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.585 | ppl 1536.38 | wps 70529 | wpb 2047.5 | bsz 4 | num_updates 810 | best_loss 10.585
2022-03-17 09:41:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 810 updates
2022-03-17 09:41:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 09:41:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 09:41:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 2 @ 810 updates, score 10.585) (writing took 1.8012410989031196 seconds)
2022-03-17 09:41:32 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-17 09:41:32 | INFO | train | epoch 002 | loss 11.441 | ppl 2779.45 | wps 42011.2 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 810 | lr 0.00010133 | gnorm 0.468 | loss_scale 16 | train_wall 593 | gb_free 21 | wall 1269
KL Stats: Epoch 2 Divergences: Uniform: 1.398156453565769 Unigram: 0.5670753403099839
2022-03-17 09:41:32 | INFO | fairseq.trainer | begin training epoch 3
2022-03-17 09:41:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 09:43:47 | INFO | train_inner | epoch 003:     90 / 407 loss=10.651, ppl=1607.66, wps=38327.3, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=900, lr=0.000112578, gnorm=0.504, loss_scale=16, train_wall=145, gb_free=21, wall=1404
2022-03-17 09:46:17 | INFO | train_inner | epoch 003:    190 / 407 loss=10.374, ppl=1327.25, wps=43683.3, ups=0.67, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.537, loss_scale=16, train_wall=145, gb_free=21, wall=1554
2022-03-17 09:48:47 | INFO | train_inner | epoch 003:    290 / 407 loss=10.148, ppl=1134.53, wps=43684.9, ups=0.67, wpb=65525.8, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.603, loss_scale=32, train_wall=145, gb_free=21, wall=1704
2022-03-17 09:51:17 | INFO | train_inner | epoch 003:    390 / 407 loss=9.945, ppl=985.6, wps=43699.5, ups=0.67, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.62, loss_scale=32, train_wall=145, gb_free=21, wall=1854
2022-03-17 09:51:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 09:52:01 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.616 | ppl 784.73 | wps 70311.5 | wpb 2047.5 | bsz 4 | num_updates 1217 | best_loss 9.616
2022-03-17 09:52:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1217 updates
2022-03-17 09:52:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 09:52:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 09:52:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 3 @ 1217 updates, score 9.616) (writing took 1.796569717116654 seconds)
2022-03-17 09:52:03 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-17 09:52:03 | INFO | train | epoch 003 | loss 10.249 | ppl 1216.8 | wps 42243.5 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 1217 | lr 0.000152195 | gnorm 0.569 | loss_scale 32 | train_wall 592 | gb_free 21 | wall 1900
KL Stats: Epoch 3 Divergences: Uniform: 1.9706765809921758 Unigram: 1.5481285319111129
2022-03-17 09:52:03 | INFO | fairseq.trainer | begin training epoch 4
2022-03-17 09:52:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 09:54:07 | INFO | train_inner | epoch 004:     83 / 407 loss=9.739, ppl=854.65, wps=38353, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=1300, lr=0.000162568, gnorm=0.647, loss_scale=32, train_wall=145, gb_free=21, wall=2025
2022-03-17 09:56:37 | INFO | train_inner | epoch 004:    183 / 407 loss=9.546, ppl=747.67, wps=43714.3, ups=0.67, wpb=65525.8, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.688, loss_scale=32, train_wall=145, gb_free=21, wall=2174
2022-03-17 09:59:07 | INFO | train_inner | epoch 004:    283 / 407 loss=9.35, ppl=652.8, wps=43689.8, ups=0.67, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.701, loss_scale=32, train_wall=145, gb_free=21, wall=2324
2022-03-17 10:00:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 10:01:38 | INFO | train_inner | epoch 004:    384 / 407 loss=9.177, ppl=578.9, wps=43289.1, ups=0.66, wpb=65534.2, bsz=128, num_updates=1600, lr=0.00020006, gnorm=0.71, loss_scale=32, train_wall=147, gb_free=21, wall=2476
2022-03-17 10:02:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 10:02:31 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.846 | ppl 460.18 | wps 70534.8 | wpb 2047.5 | bsz 4 | num_updates 1623 | best_loss 8.846
2022-03-17 10:02:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1623 updates
2022-03-17 10:02:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 10:02:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 10:02:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 4 @ 1623 updates, score 8.846) (writing took 1.7858587335795164 seconds)
2022-03-17 10:02:33 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-17 10:02:33 | INFO | train | epoch 004 | loss 9.416 | ppl 683.02 | wps 42160.6 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 1623 | lr 0.000202934 | gnorm 0.69 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 2531
KL Stats: Epoch 4 Divergences: Uniform: 2.5006685947399374 Unigram: 2.1181223742809685
2022-03-17 10:02:33 | INFO | fairseq.trainer | begin training epoch 5
2022-03-17 10:02:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 10:04:29 | INFO | train_inner | epoch 005:     77 / 407 loss=9.002, ppl=512.56, wps=38360.4, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=1700, lr=0.000212558, gnorm=0.761, loss_scale=32, train_wall=145, gb_free=21, wall=2646
2022-03-17 10:06:59 | INFO | train_inner | epoch 005:    177 / 407 loss=8.845, ppl=459.71, wps=43703.5, ups=0.67, wpb=65534.2, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.744, loss_scale=32, train_wall=145, gb_free=21, wall=2796
2022-03-17 10:09:29 | INFO | train_inner | epoch 005:    277 / 407 loss=8.701, ppl=416.06, wps=43704.3, ups=0.67, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.722, loss_scale=32, train_wall=145, gb_free=21, wall=2946
2022-03-17 10:11:59 | INFO | train_inner | epoch 005:    377 / 407 loss=8.556, ppl=376.47, wps=43720.9, ups=0.67, wpb=65525.8, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.729, loss_scale=32, train_wall=145, gb_free=21, wall=3096
2022-03-17 10:12:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 10:13:02 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.249 | ppl 304.25 | wps 70599.4 | wpb 2047.5 | bsz 4 | num_updates 2030 | best_loss 8.249
2022-03-17 10:13:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 2030 updates
2022-03-17 10:13:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 10:13:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 10:13:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 5 @ 2030 updates, score 8.249) (writing took 1.8539646938443184 seconds)
2022-03-17 10:13:04 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-17 10:13:04 | INFO | train | epoch 005 | loss 8.736 | ppl 426.44 | wps 42265.5 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 2030 | lr 0.000253799 | gnorm 0.742 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 3161
KL Stats: Epoch 5 Divergences: Uniform: 2.947656212590917 Unigram: 2.5083477450960925
2022-03-17 10:13:04 | INFO | fairseq.trainer | begin training epoch 6
2022-03-17 10:13:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 10:13:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 10:14:50 | INFO | train_inner | epoch 006:     71 / 407 loss=8.409, ppl=339.85, wps=38041.9, ups=0.58, wpb=65370.3, bsz=127.7, num_updates=2100, lr=0.000262548, gnorm=0.73, loss_scale=32, train_wall=146, gb_free=21, wall=3268
2022-03-17 10:17:20 | INFO | train_inner | epoch 006:    171 / 407 loss=8.292, ppl=313.43, wps=43706.1, ups=0.67, wpb=65525.8, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.734, loss_scale=32, train_wall=145, gb_free=21, wall=3418
2022-03-17 10:19:50 | INFO | train_inner | epoch 006:    271 / 407 loss=8.185, ppl=291.12, wps=43679.9, ups=0.67, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.708, loss_scale=32, train_wall=145, gb_free=21, wall=3568
2022-03-17 10:22:20 | INFO | train_inner | epoch 006:    371 / 407 loss=8.091, ppl=272.57, wps=43728.9, ups=0.67, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.695, loss_scale=32, train_wall=145, gb_free=21, wall=3718
2022-03-17 10:23:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 10:23:33 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.807 | ppl 223.95 | wps 70344.9 | wpb 2047.5 | bsz 4 | num_updates 2436 | best_loss 7.807
2022-03-17 10:23:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2436 updates
2022-03-17 10:23:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 10:23:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 10:23:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 6 @ 2436 updates, score 7.807) (writing took 1.8563021924346685 seconds)
2022-03-17 10:23:35 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-17 10:23:35 | INFO | train | epoch 006 | loss 8.209 | ppl 295.89 | wps 42152.7 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 2436 | lr 0.000304539 | gnorm 0.713 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 3792
KL Stats: Epoch 6 Divergences: Uniform: 3.320522553318485 Unigram: 2.813568893088618
2022-03-17 10:23:35 | INFO | fairseq.trainer | begin training epoch 7
2022-03-17 10:23:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 10:25:11 | INFO | train_inner | epoch 007:     64 / 407 loss=7.965, ppl=249.84, wps=38332.6, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=2500, lr=0.000312538, gnorm=0.698, loss_scale=32, train_wall=145, gb_free=21, wall=3888
2022-03-17 10:27:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 10:27:42 | INFO | train_inner | epoch 007:    165 / 407 loss=7.881, ppl=235.79, wps=43289.6, ups=0.66, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.677, loss_scale=32, train_wall=147, gb_free=21, wall=4040
2022-03-17 10:30:12 | INFO | train_inner | epoch 007:    265 / 407 loss=7.808, ppl=224.1, wps=43682.3, ups=0.67, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.659, loss_scale=32, train_wall=145, gb_free=21, wall=4190
2022-03-17 10:32:42 | INFO | train_inner | epoch 007:    365 / 407 loss=7.737, ppl=213.35, wps=43733.8, ups=0.67, wpb=65534.2, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.656, loss_scale=32, train_wall=145, gb_free=21, wall=4339
2022-03-17 10:33:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 10:34:04 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.485 | ppl 179.1 | wps 70604.2 | wpb 2047.5 | bsz 4 | num_updates 2842 | best_loss 7.485
2022-03-17 10:34:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2842 updates
2022-03-17 10:34:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 10:34:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 10:34:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 7 @ 2842 updates, score 7.485) (writing took 1.8179637007415295 seconds)
2022-03-17 10:34:05 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-17 10:34:05 | INFO | train | epoch 007 | loss 7.817 | ppl 225.46 | wps 42165.6 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 2842 | lr 0.000355279 | gnorm 0.666 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 4423
KL Stats: Epoch 7 Divergences: Uniform: 3.5879991844495427 Unigram: 3.0382552353450927
2022-03-17 10:34:05 | INFO | fairseq.trainer | begin training epoch 8
2022-03-17 10:34:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 10:34:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 10:35:34 | INFO | train_inner | epoch 008:     59 / 407 loss=7.634, ppl=198.69, wps=38024.8, ups=0.58, wpb=65349.8, bsz=127.6, num_updates=2900, lr=0.000362528, gnorm=0.646, loss_scale=16, train_wall=146, gb_free=21, wall=4511
2022-03-17 10:38:04 | INFO | train_inner | epoch 008:    159 / 407 loss=7.578, ppl=191.07, wps=43722.3, ups=0.67, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.628, loss_scale=16, train_wall=145, gb_free=21, wall=4661
2022-03-17 10:40:34 | INFO | train_inner | epoch 008:    259 / 407 loss=7.512, ppl=182.57, wps=43691.1, ups=0.67, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.622, loss_scale=16, train_wall=145, gb_free=21, wall=4811
2022-03-17 10:43:04 | INFO | train_inner | epoch 008:    359 / 407 loss=7.447, ppl=174.48, wps=43713.7, ups=0.67, wpb=65536, bsz=128, num_updates=3200, lr=0.00040002, gnorm=0.608, loss_scale=16, train_wall=145, gb_free=21, wall=4961
2022-03-17 10:44:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 10:44:34 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.218 | ppl 148.83 | wps 70435.1 | wpb 2047.5 | bsz 4 | num_updates 3248 | best_loss 7.218
2022-03-17 10:44:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3248 updates
2022-03-17 10:44:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 10:44:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 10:44:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 8 @ 3248 updates, score 7.218) (writing took 1.844180602580309 seconds)
2022-03-17 10:44:36 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-17 10:44:36 | INFO | train | epoch 008 | loss 7.51 | ppl 182.33 | wps 42151.8 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 3248 | lr 0.000406019 | gnorm 0.622 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 5054
KL Stats: Epoch 8 Divergences: Uniform: 3.775016666570747 Unigram: 3.200433156536989
2022-03-17 10:44:36 | INFO | fairseq.trainer | begin training epoch 9
2022-03-17 10:44:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 10:45:54 | INFO | train_inner | epoch 009:     52 / 407 loss=7.368, ppl=165.14, wps=38323.2, ups=0.59, wpb=65361.9, bsz=127.7, num_updates=3300, lr=0.000412518, gnorm=0.605, loss_scale=16, train_wall=145, gb_free=21, wall=5132
2022-03-17 10:48:24 | INFO | train_inner | epoch 009:    152 / 407 loss=7.293, ppl=156.86, wps=43668.8, ups=0.67, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.588, loss_scale=32, train_wall=145, gb_free=21, wall=5282
2022-03-17 10:49:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 10:50:56 | INFO | train_inner | epoch 009:    253 / 407 loss=7.252, ppl=152.43, wps=43207.9, ups=0.66, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.587, loss_scale=16, train_wall=147, gb_free=21, wall=5433
2022-03-17 10:53:26 | INFO | train_inner | epoch 009:    353 / 407 loss=7.203, ppl=147.29, wps=43669.8, ups=0.67, wpb=65534.2, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.579, loss_scale=16, train_wall=145, gb_free=21, wall=5584
2022-03-17 10:54:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 10:55:06 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.965 | ppl 124.95 | wps 70603.5 | wpb 2047.5 | bsz 4 | num_updates 3654 | best_loss 6.965
2022-03-17 10:55:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3654 updates
2022-03-17 10:55:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 10:55:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 10:55:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 9 @ 3654 updates, score 6.965) (writing took 1.8504497846588492 seconds)
2022-03-17 10:55:08 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-17 10:55:08 | INFO | train | epoch 009 | loss 7.247 | ppl 151.95 | wps 42111.7 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 3654 | lr 0.000456759 | gnorm 0.585 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 5685
KL Stats: Epoch 9 Divergences: Uniform: 3.9159033291950855 Unigram: 3.326444803506207
2022-03-17 10:55:08 | INFO | fairseq.trainer | begin training epoch 10
2022-03-17 10:55:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 10:56:17 | INFO | train_inner | epoch 010:     46 / 407 loss=7.107, ppl=137.85, wps=38340.5, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.562, loss_scale=16, train_wall=145, gb_free=21, wall=5754
2022-03-17 10:58:47 | INFO | train_inner | epoch 010:    146 / 407 loss=7.051, ppl=132.65, wps=43707.1, ups=0.67, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.559, loss_scale=16, train_wall=145, gb_free=21, wall=5904
2022-03-17 11:01:16 | INFO | train_inner | epoch 010:    246 / 407 loss=7.01, ppl=128.93, wps=43717, ups=0.67, wpb=65525.8, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.559, loss_scale=16, train_wall=145, gb_free=21, wall=6054
2022-03-17 11:03:46 | INFO | train_inner | epoch 010:    346 / 407 loss=6.973, ppl=125.63, wps=43747, ups=0.67, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.561, loss_scale=32, train_wall=145, gb_free=21, wall=6204
2022-03-17 11:05:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 11:05:36 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.75 | ppl 107.6 | wps 70556.4 | wpb 2047.5 | bsz 4 | num_updates 4061 | best_loss 6.75
2022-03-17 11:05:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 4061 updates
2022-03-17 11:05:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 11:05:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 11:05:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 10 @ 4061 updates, score 6.75) (writing took 1.8437994355335832 seconds)
2022-03-17 11:05:38 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-17 11:05:38 | INFO | train | epoch 010 | loss 7.006 | ppl 128.56 | wps 42277.6 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 4061 | lr 0.000496231 | gnorm 0.556 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 6316
KL Stats: Epoch 10 Divergences: Uniform: 4.031439167318908 Unigram: 3.433720160795344
2022-03-17 11:05:38 | INFO | fairseq.trainer | begin training epoch 11
2022-03-17 11:05:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 11:06:37 | INFO | train_inner | epoch 011:     39 / 407 loss=6.899, ppl=119.34, wps=38381.8, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.546, loss_scale=32, train_wall=145, gb_free=21, wall=6374
2022-03-17 11:09:06 | INFO | train_inner | epoch 011:    139 / 407 loss=6.827, ppl=113.52, wps=43739.2, ups=0.67, wpb=65534.2, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.526, loss_scale=32, train_wall=145, gb_free=21, wall=6524
2022-03-17 11:11:36 | INFO | train_inner | epoch 011:    239 / 407 loss=6.796, ppl=111.12, wps=43715, ups=0.67, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.527, loss_scale=32, train_wall=145, gb_free=21, wall=6674
2022-03-17 11:14:06 | INFO | train_inner | epoch 011:    339 / 407 loss=6.774, ppl=109.41, wps=43754.1, ups=0.67, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.514, loss_scale=32, train_wall=145, gb_free=21, wall=6824
2022-03-17 11:15:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 11:16:07 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.586 | ppl 96.06 | wps 70599.9 | wpb 2047.5 | bsz 4 | num_updates 4468 | best_loss 6.586
2022-03-17 11:16:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4468 updates
2022-03-17 11:16:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 11:16:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 11:16:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 11 @ 4468 updates, score 6.586) (writing took 1.8168937554582953 seconds)
2022-03-17 11:16:08 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-17 11:16:08 | INFO | train | epoch 011 | loss 6.792 | ppl 110.79 | wps 42292.8 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 4468 | lr 0.00047309 | gnorm 0.524 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 6946
KL Stats: Epoch 11 Divergences: Uniform: 4.128167771071997 Unigram: 3.5300090375756876
2022-03-17 11:16:08 | INFO | fairseq.trainer | begin training epoch 12
2022-03-17 11:16:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 11:16:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 11:16:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 11:16:59 | INFO | train_inner | epoch 012:     34 / 407 loss=6.706, ppl=104.43, wps=37726.2, ups=0.58, wpb=65361.9, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.513, loss_scale=16, train_wall=148, gb_free=21, wall=6997
2022-03-17 11:19:29 | INFO | train_inner | epoch 012:    134 / 407 loss=6.642, ppl=99.91, wps=43729.5, ups=0.67, wpb=65525.8, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.503, loss_scale=16, train_wall=145, gb_free=21, wall=7147
2022-03-17 11:21:59 | INFO | train_inner | epoch 012:    234 / 407 loss=6.63, ppl=99.07, wps=43737.1, ups=0.67, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.494, loss_scale=16, train_wall=145, gb_free=21, wall=7296
2022-03-17 11:24:29 | INFO | train_inner | epoch 012:    334 / 407 loss=6.619, ppl=98.29, wps=43736.5, ups=0.67, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.495, loss_scale=16, train_wall=145, gb_free=21, wall=7446
2022-03-17 11:26:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 11:26:37 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.471 | ppl 88.7 | wps 70651.5 | wpb 2047.5 | bsz 4 | num_updates 4873 | best_loss 6.471
2022-03-17 11:26:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4873 updates
2022-03-17 11:26:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 11:26:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 11:26:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 12 @ 4873 updates, score 6.471) (writing took 1.880166171118617 seconds)
2022-03-17 11:26:39 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-17 11:26:39 | INFO | train | epoch 012 | loss 6.625 | ppl 98.71 | wps 42080.2 | ups 0.64 | wpb 65492.6 | bsz 127.9 | num_updates 4873 | lr 0.000453004 | gnorm 0.498 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 7576
KL Stats: Epoch 12 Divergences: Uniform: 4.204753204566895 Unigram: 3.6042094708710106
2022-03-17 11:26:39 | INFO | fairseq.trainer | begin training epoch 13
2022-03-17 11:26:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 11:27:19 | INFO | train_inner | epoch 013:     27 / 407 loss=6.57, ppl=95.03, wps=38370.2, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.492, loss_scale=16, train_wall=145, gb_free=21, wall=7617
2022-03-17 11:29:49 | INFO | train_inner | epoch 013:    127 / 407 loss=6.507, ppl=90.95, wps=43741.1, ups=0.67, wpb=65534.2, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.481, loss_scale=32, train_wall=145, gb_free=21, wall=7767
2022-03-17 11:31:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 11:32:21 | INFO | train_inner | epoch 013:    228 / 407 loss=6.503, ppl=90.7, wps=43259.4, ups=0.66, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.486, loss_scale=16, train_wall=147, gb_free=21, wall=7918
2022-03-17 11:34:50 | INFO | train_inner | epoch 013:    328 / 407 loss=6.491, ppl=89.96, wps=43745.6, ups=0.67, wpb=65525.8, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.491, loss_scale=16, train_wall=145, gb_free=21, wall=8068
2022-03-17 11:36:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 11:37:07 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.377 | ppl 83.12 | wps 70468.9 | wpb 2047.5 | bsz 4 | num_updates 5279 | best_loss 6.377
2022-03-17 11:37:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5279 updates
2022-03-17 11:37:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 11:37:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 11:37:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 13 @ 5279 updates, score 6.377) (writing took 1.8453109478577971 seconds)
2022-03-17 11:37:09 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-17 11:37:09 | INFO | train | epoch 013 | loss 6.498 | ppl 90.41 | wps 42172.2 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 5279 | lr 0.000435235 | gnorm 0.484 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 8207
KL Stats: Epoch 13 Divergences: Uniform: 4.261953442847247 Unigram: 3.663469595723338
2022-03-17 11:37:09 | INFO | fairseq.trainer | begin training epoch 14
2022-03-17 11:37:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 11:37:41 | INFO | train_inner | epoch 014:     21 / 407 loss=6.467, ppl=88.44, wps=38366.1, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.474, loss_scale=16, train_wall=145, gb_free=21, wall=8238
2022-03-17 11:40:11 | INFO | train_inner | epoch 014:    121 / 407 loss=6.399, ppl=84.37, wps=43732, ups=0.67, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.479, loss_scale=16, train_wall=145, gb_free=21, wall=8388
2022-03-17 11:42:41 | INFO | train_inner | epoch 014:    221 / 407 loss=6.401, ppl=84.53, wps=43706.9, ups=0.67, wpb=65523.9, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.478, loss_scale=16, train_wall=145, gb_free=21, wall=8538
2022-03-17 11:45:10 | INFO | train_inner | epoch 014:    321 / 407 loss=6.397, ppl=84.28, wps=43708.4, ups=0.67, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.47, loss_scale=32, train_wall=145, gb_free=21, wall=8688
2022-03-17 11:47:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 11:47:38 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.306 | ppl 79.11 | wps 70434.2 | wpb 2047.5 | bsz 4 | num_updates 5686 | best_loss 6.306
2022-03-17 11:47:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5686 updates
2022-03-17 11:47:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 11:47:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 11:47:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 14 @ 5686 updates, score 6.306) (writing took 1.9017621418461204 seconds)
2022-03-17 11:47:40 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-17 11:47:40 | INFO | train | epoch 014 | loss 6.396 | ppl 84.23 | wps 42265.7 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 5686 | lr 0.000419369 | gnorm 0.475 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 8837
KL Stats: Epoch 14 Divergences: Uniform: 4.30920891056162 Unigram: 3.712611269868007
2022-03-17 11:47:40 | INFO | fairseq.trainer | begin training epoch 15
2022-03-17 11:47:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 11:48:01 | INFO | train_inner | epoch 015:     14 / 407 loss=6.377, ppl=83.12, wps=38347.2, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.474, loss_scale=32, train_wall=145, gb_free=21, wall=8858
2022-03-17 11:50:31 | INFO | train_inner | epoch 015:    114 / 407 loss=6.301, ppl=78.87, wps=43741.9, ups=0.67, wpb=65523.9, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.47, loss_scale=32, train_wall=145, gb_free=21, wall=9008
2022-03-17 11:53:01 | INFO | train_inner | epoch 015:    214 / 407 loss=6.315, ppl=79.63, wps=43718.7, ups=0.67, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.472, loss_scale=32, train_wall=145, gb_free=21, wall=9158
2022-03-17 11:55:31 | INFO | train_inner | epoch 015:    314 / 407 loss=6.321, ppl=79.94, wps=43725.4, ups=0.67, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.472, loss_scale=32, train_wall=145, gb_free=21, wall=9308
2022-03-17 11:57:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 11:58:08 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.243 | ppl 75.72 | wps 70418 | wpb 2047.5 | bsz 4 | num_updates 6093 | best_loss 6.243
2022-03-17 11:58:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 6093 updates
2022-03-17 11:58:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 11:58:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 11:58:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 15 @ 6093 updates, score 6.243) (writing took 1.8216941691935062 seconds)
2022-03-17 11:58:10 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-17 11:58:10 | INFO | train | epoch 015 | loss 6.311 | ppl 79.42 | wps 42284.6 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 6093 | lr 0.000405121 | gnorm 0.471 | loss_scale 64 | train_wall 591 | gb_free 21 | wall 9468
KL Stats: Epoch 15 Divergences: Uniform: 4.348421297881031 Unigram: 3.753886033491306
2022-03-17 11:58:10 | INFO | fairseq.trainer | begin training epoch 16
2022-03-17 11:58:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 11:58:21 | INFO | train_inner | epoch 016:      7 / 407 loss=6.306, ppl=79.13, wps=38381.2, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.468, loss_scale=64, train_wall=145, gb_free=21, wall=9478
2022-03-17 11:58:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 12:00:52 | INFO | train_inner | epoch 016:    108 / 407 loss=6.237, ppl=75.43, wps=43279.3, ups=0.66, wpb=65534.2, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.464, loss_scale=32, train_wall=147, gb_free=21, wall=9630
2022-03-17 12:03:22 | INFO | train_inner | epoch 016:    208 / 407 loss=6.242, ppl=75.68, wps=43699.1, ups=0.67, wpb=65536, bsz=128, num_updates=6300, lr=0.00039841, gnorm=0.465, loss_scale=32, train_wall=145, gb_free=21, wall=9780
2022-03-17 12:05:52 | INFO | train_inner | epoch 016:    308 / 407 loss=6.232, ppl=75.16, wps=43718.4, ups=0.67, wpb=65525.8, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.466, loss_scale=32, train_wall=145, gb_free=21, wall=9930
2022-03-17 12:08:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 12:08:39 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.2 | ppl 73.53 | wps 70494.2 | wpb 2047.5 | bsz 4 | num_updates 6499 | best_loss 6.2
2022-03-17 12:08:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6499 updates
2022-03-17 12:08:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 12:08:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 12:08:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 16 @ 6499 updates, score 6.2) (writing took 1.8428997658193111 seconds)
2022-03-17 12:08:41 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-17 12:08:41 | INFO | train | epoch 016 | loss 6.24 | ppl 75.58 | wps 42158.8 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 6499 | lr 0.000392262 | gnorm 0.465 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 10098
KL Stats: Epoch 16 Divergences: Uniform: 4.385643507266843 Unigram: 3.7892400837869125
2022-03-17 12:08:41 | INFO | fairseq.trainer | begin training epoch 17
2022-03-17 12:08:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 12:08:43 | INFO | train_inner | epoch 017:      1 / 407 loss=6.249, ppl=76.06, wps=38356.4, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.467, loss_scale=32, train_wall=145, gb_free=21, wall=10100
2022-03-17 12:11:12 | INFO | train_inner | epoch 017:    101 / 407 loss=6.164, ppl=71.73, wps=43721.6, ups=0.67, wpb=65525.8, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.474, loss_scale=32, train_wall=145, gb_free=21, wall=10250
2022-03-17 12:11:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 12:13:44 | INFO | train_inner | epoch 017:    202 / 407 loss=6.179, ppl=72.45, wps=43275.9, ups=0.66, wpb=65534.2, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.47, loss_scale=32, train_wall=147, gb_free=21, wall=10401
2022-03-17 12:16:14 | INFO | train_inner | epoch 017:    302 / 407 loss=6.185, ppl=72.73, wps=43705.5, ups=0.67, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.465, loss_scale=32, train_wall=145, gb_free=21, wall=10551
2022-03-17 12:18:44 | INFO | train_inner | epoch 017:    402 / 407 loss=6.183, ppl=72.64, wps=43723.4, ups=0.67, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.465, loss_scale=32, train_wall=145, gb_free=21, wall=10701
2022-03-17 12:18:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 12:19:10 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.147 | ppl 70.89 | wps 70615.4 | wpb 2047.5 | bsz 4 | num_updates 6905 | best_loss 6.147
2022-03-17 12:19:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6905 updates
2022-03-17 12:19:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 12:19:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 12:19:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 17 @ 6905 updates, score 6.147) (writing took 1.8201699489727616 seconds)
2022-03-17 12:19:12 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-17 12:19:12 | INFO | train | epoch 017 | loss 6.178 | ppl 72.39 | wps 42163 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 6905 | lr 0.000380556 | gnorm 0.469 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 10729
KL Stats: Epoch 17 Divergences: Uniform: 4.412889479003399 Unigram: 3.819759615452782
2022-03-17 12:19:12 | INFO | fairseq.trainer | begin training epoch 18
2022-03-17 12:19:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 12:21:34 | INFO | train_inner | epoch 018:     95 / 407 loss=6.107, ppl=68.92, wps=38370.6, ups=0.59, wpb=65360.1, bsz=127.7, num_updates=7000, lr=0.000377964, gnorm=0.468, loss_scale=32, train_wall=145, gb_free=21, wall=10871
2022-03-17 12:24:04 | INFO | train_inner | epoch 018:    195 / 407 loss=6.126, ppl=69.85, wps=43723.6, ups=0.67, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.472, loss_scale=32, train_wall=145, gb_free=21, wall=11021
2022-03-17 12:24:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 12:26:35 | INFO | train_inner | epoch 018:    296 / 407 loss=6.129, ppl=70, wps=43277.9, ups=0.66, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.458, loss_scale=32, train_wall=147, gb_free=21, wall=11173
2022-03-17 12:29:05 | INFO | train_inner | epoch 018:    396 / 407 loss=6.135, ppl=70.26, wps=43753.1, ups=0.67, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.466, loss_scale=32, train_wall=145, gb_free=21, wall=11323
2022-03-17 12:29:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 12:29:40 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.113 | ppl 69.24 | wps 70494.8 | wpb 2047.5 | bsz 4 | num_updates 7311 | best_loss 6.113
2022-03-17 12:29:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7311 updates
2022-03-17 12:29:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 12:29:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 12:29:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 18 @ 7311 updates, score 6.113) (writing took 1.8489146456122398 seconds)
2022-03-17 12:29:42 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-17 12:29:42 | INFO | train | epoch 018 | loss 6.124 | ppl 69.72 | wps 42173.1 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 7311 | lr 0.000369838 | gnorm 0.466 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 11360
KL Stats: Epoch 18 Divergences: Uniform: 4.442550954684405 Unigram: 3.8492682486974235
2022-03-17 12:29:42 | INFO | fairseq.trainer | begin training epoch 19
2022-03-17 12:29:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 12:31:56 | INFO | train_inner | epoch 019:     89 / 407 loss=6.063, ppl=66.85, wps=38358.7, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.473, loss_scale=32, train_wall=145, gb_free=21, wall=11493
2022-03-17 12:34:25 | INFO | train_inner | epoch 019:    189 / 407 loss=6.078, ppl=67.55, wps=43726, ups=0.67, wpb=65523.9, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.471, loss_scale=32, train_wall=145, gb_free=21, wall=11643
2022-03-17 12:36:55 | INFO | train_inner | epoch 019:    289 / 407 loss=6.084, ppl=67.85, wps=43703.7, ups=0.67, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.466, loss_scale=32, train_wall=145, gb_free=21, wall=11793
2022-03-17 12:38:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 12:39:27 | INFO | train_inner | epoch 019:    390 / 407 loss=6.081, ppl=67.72, wps=43319, ups=0.66, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.468, loss_scale=32, train_wall=147, gb_free=21, wall=11944
2022-03-17 12:39:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 12:40:11 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.079 | ppl 67.59 | wps 70604.7 | wpb 2047.5 | bsz 4 | num_updates 7717 | best_loss 6.079
2022-03-17 12:40:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7717 updates
2022-03-17 12:40:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 12:40:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 12:40:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 19 @ 7717 updates, score 6.079) (writing took 1.8654095688834786 seconds)
2022-03-17 12:40:13 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-17 12:40:13 | INFO | train | epoch 019 | loss 6.076 | ppl 67.47 | wps 42169.7 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 7717 | lr 0.000359978 | gnorm 0.47 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 11990
KL Stats: Epoch 19 Divergences: Uniform: 4.465416926737566 Unigram: 3.874802986411567
2022-03-17 12:40:13 | INFO | fairseq.trainer | begin training epoch 20
2022-03-17 12:40:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 12:42:17 | INFO | train_inner | epoch 020:     83 / 407 loss=6.012, ppl=64.54, wps=38335.8, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.47, loss_scale=32, train_wall=145, gb_free=21, wall=12115
2022-03-17 12:44:47 | INFO | train_inner | epoch 020:    183 / 407 loss=6.041, ppl=65.85, wps=43725.9, ups=0.67, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.477, loss_scale=32, train_wall=145, gb_free=21, wall=12264
2022-03-17 12:47:17 | INFO | train_inner | epoch 020:    283 / 407 loss=6.039, ppl=65.75, wps=43685.2, ups=0.67, wpb=65525.8, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.467, loss_scale=32, train_wall=145, gb_free=21, wall=12414
2022-03-17 12:49:47 | INFO | train_inner | epoch 020:    383 / 407 loss=6.052, ppl=66.36, wps=43726.7, ups=0.67, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.46, loss_scale=32, train_wall=145, gb_free=21, wall=12564
2022-03-17 12:50:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 12:50:42 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.056 | ppl 66.52 | wps 70725.4 | wpb 2047.5 | bsz 4 | num_updates 8124 | best_loss 6.056
2022-03-17 12:50:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 8124 updates
2022-03-17 12:50:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 12:50:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 12:50:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 20 @ 8124 updates, score 6.056) (writing took 1.8482232643291354 seconds)
2022-03-17 12:50:43 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-17 12:50:43 | INFO | train | epoch 020 | loss 6.033 | ppl 65.5 | wps 42264.1 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 8124 | lr 0.000350845 | gnorm 0.468 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 12621
KL Stats: Epoch 20 Divergences: Uniform: 4.488248726356525 Unigram: 3.8994398797214718
2022-03-17 12:50:43 | INFO | fairseq.trainer | begin training epoch 21
2022-03-17 12:50:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 12:51:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 12:52:39 | INFO | train_inner | epoch 021:     77 / 407 loss=5.986, ppl=63.37, wps=38042.4, ups=0.58, wpb=65372.2, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.469, loss_scale=32, train_wall=146, gb_free=21, wall=12736
2022-03-17 12:55:09 | INFO | train_inner | epoch 021:    177 / 407 loss=5.995, ppl=63.78, wps=43725.1, ups=0.67, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.467, loss_scale=32, train_wall=145, gb_free=21, wall=12886
2022-03-17 12:57:39 | INFO | train_inner | epoch 021:    277 / 407 loss=5.999, ppl=63.94, wps=43702.7, ups=0.67, wpb=65525.8, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.473, loss_scale=32, train_wall=145, gb_free=21, wall=13036
2022-03-17 13:00:08 | INFO | train_inner | epoch 021:    377 / 407 loss=6.009, ppl=64.42, wps=43747.9, ups=0.67, wpb=65534.2, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.462, loss_scale=32, train_wall=145, gb_free=21, wall=13186
2022-03-17 13:00:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 13:01:12 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.024 | ppl 65.06 | wps 70527.1 | wpb 2047.5 | bsz 4 | num_updates 8530 | best_loss 6.024
2022-03-17 13:01:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8530 updates
2022-03-17 13:01:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 13:01:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 13:01:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 21 @ 8530 updates, score 6.024) (writing took 1.8826896557584405 seconds)
2022-03-17 13:01:14 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-17 13:01:14 | INFO | train | epoch 021 | loss 5.995 | ppl 63.78 | wps 42169.6 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 8530 | lr 0.000342393 | gnorm 0.467 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 13251
KL Stats: Epoch 21 Divergences: Uniform: 4.506811180728782 Unigram: 3.919686350212774
2022-03-17 13:01:14 | INFO | fairseq.trainer | begin training epoch 22
2022-03-17 13:01:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 13:02:59 | INFO | train_inner | epoch 022:     70 / 407 loss=5.969, ppl=62.66, wps=38355.2, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.467, loss_scale=32, train_wall=145, gb_free=21, wall=13356
2022-03-17 13:04:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 13:05:30 | INFO | train_inner | epoch 022:    171 / 407 loss=5.94, ppl=61.4, wps=43287.9, ups=0.66, wpb=65534.2, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.471, loss_scale=32, train_wall=147, gb_free=21, wall=13508
2022-03-17 13:08:00 | INFO | train_inner | epoch 022:    271 / 407 loss=5.965, ppl=62.48, wps=43699.7, ups=0.67, wpb=65525.8, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.472, loss_scale=32, train_wall=145, gb_free=21, wall=13658
2022-03-17 13:10:30 | INFO | train_inner | epoch 022:    371 / 407 loss=5.971, ppl=62.72, wps=43729.5, ups=0.67, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.467, loss_scale=32, train_wall=145, gb_free=21, wall=13807
2022-03-17 13:11:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 13:11:43 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.006 | ppl 64.26 | wps 70404.8 | wpb 2047.5 | bsz 4 | num_updates 8936 | best_loss 6.006
2022-03-17 13:11:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8936 updates
2022-03-17 13:11:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 13:11:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 13:11:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 22 @ 8936 updates, score 6.006) (writing took 1.9096668269485235 seconds)
2022-03-17 13:11:45 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-17 13:11:45 | INFO | train | epoch 022 | loss 5.961 | ppl 62.28 | wps 42164.1 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 8936 | lr 0.000334525 | gnorm 0.469 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 13882
KL Stats: Epoch 22 Divergences: Uniform: 4.525360432634062 Unigram: 3.9382732404440572
2022-03-17 13:11:45 | INFO | fairseq.trainer | begin training epoch 23
2022-03-17 13:11:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 13:13:20 | INFO | train_inner | epoch 023:     64 / 407 loss=5.936, ppl=61.22, wps=38356, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.471, loss_scale=32, train_wall=145, gb_free=21, wall=13978
2022-03-17 13:15:50 | INFO | train_inner | epoch 023:    164 / 407 loss=5.923, ppl=60.69, wps=43724.2, ups=0.67, wpb=65534.2, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.473, loss_scale=32, train_wall=145, gb_free=21, wall=14128
2022-03-17 13:17:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 13:18:22 | INFO | train_inner | epoch 023:    265 / 407 loss=5.924, ppl=60.73, wps=43291.9, ups=0.66, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.47, loss_scale=32, train_wall=147, gb_free=21, wall=14279
2022-03-17 13:20:51 | INFO | train_inner | epoch 023:    365 / 407 loss=5.945, ppl=61.61, wps=43766.3, ups=0.67, wpb=65525.8, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.472, loss_scale=32, train_wall=145, gb_free=21, wall=14429
2022-03-17 13:21:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 13:22:13 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.985 | ppl 63.35 | wps 70684.9 | wpb 2047.5 | bsz 4 | num_updates 9342 | best_loss 5.985
2022-03-17 13:22:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9342 updates
2022-03-17 13:22:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 13:22:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 13:22:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 23 @ 9342 updates, score 5.985) (writing took 1.865232658572495 seconds)
2022-03-17 13:22:15 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-17 13:22:15 | INFO | train | epoch 023 | loss 5.929 | ppl 60.93 | wps 42187.1 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 9342 | lr 0.000327175 | gnorm 0.472 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 14512
KL Stats: Epoch 23 Divergences: Uniform: 4.544337334413949 Unigram: 3.9573107723782437
2022-03-17 13:22:15 | INFO | fairseq.trainer | begin training epoch 24
2022-03-17 13:22:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 13:23:42 | INFO | train_inner | epoch 024:     58 / 407 loss=5.892, ppl=59.37, wps=38373.5, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.47, loss_scale=32, train_wall=145, gb_free=21, wall=14599
2022-03-17 13:26:12 | INFO | train_inner | epoch 024:    158 / 407 loss=5.89, ppl=59.31, wps=43732.4, ups=0.67, wpb=65534.2, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.472, loss_scale=32, train_wall=145, gb_free=21, wall=14749
2022-03-17 13:28:42 | INFO | train_inner | epoch 024:    258 / 407 loss=5.91, ppl=60.11, wps=43679.4, ups=0.67, wpb=65525.8, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.473, loss_scale=32, train_wall=145, gb_free=21, wall=14899
2022-03-17 13:31:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 13:31:13 | INFO | train_inner | epoch 024:    359 / 407 loss=5.922, ppl=60.64, wps=43293, ups=0.66, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.478, loss_scale=32, train_wall=147, gb_free=21, wall=15050
2022-03-17 13:32:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 13:32:44 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.967 | ppl 62.57 | wps 70237.8 | wpb 2047.5 | bsz 4 | num_updates 9748 | best_loss 5.967
2022-03-17 13:32:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9748 updates
2022-03-17 13:32:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 13:32:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 13:32:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 24 @ 9748 updates, score 5.967) (writing took 1.8764926483854651 seconds)
2022-03-17 13:32:46 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-17 13:32:46 | INFO | train | epoch 024 | loss 5.9 | ppl 59.7 | wps 42155.1 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 9748 | lr 0.000320289 | gnorm 0.473 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 15143
KL Stats: Epoch 24 Divergences: Uniform: 4.558545051677279 Unigram: 3.974524901417812
2022-03-17 13:32:46 | INFO | fairseq.trainer | begin training epoch 25
2022-03-17 13:32:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 13:34:04 | INFO | train_inner | epoch 025:     52 / 407 loss=5.882, ppl=58.96, wps=38341.8, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.468, loss_scale=32, train_wall=145, gb_free=21, wall=15221
2022-03-17 13:36:33 | INFO | train_inner | epoch 025:    152 / 407 loss=5.859, ppl=58.04, wps=43735.6, ups=0.67, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.472, loss_scale=32, train_wall=145, gb_free=21, wall=15371
2022-03-17 13:39:03 | INFO | train_inner | epoch 025:    252 / 407 loss=5.878, ppl=58.81, wps=43719.2, ups=0.67, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.475, loss_scale=32, train_wall=145, gb_free=21, wall=15521
2022-03-17 13:41:33 | INFO | train_inner | epoch 025:    352 / 407 loss=5.878, ppl=58.81, wps=43736.2, ups=0.67, wpb=65525.8, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.469, loss_scale=32, train_wall=145, gb_free=21, wall=15671
2022-03-17 13:42:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 13:43:14 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.958 | ppl 62.16 | wps 70520.6 | wpb 2047.5 | bsz 4 | num_updates 10155 | best_loss 5.958
2022-03-17 13:43:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 10155 updates
2022-03-17 13:43:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 13:43:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 13:43:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 25 @ 10155 updates, score 5.958) (writing took 1.858170055784285 seconds)
2022-03-17 13:43:16 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-17 13:43:16 | INFO | train | epoch 025 | loss 5.873 | ppl 58.62 | wps 42282.3 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 10155 | lr 0.000313805 | gnorm 0.471 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 15773
KL Stats: Epoch 25 Divergences: Uniform: 4.576891726777653 Unigram: 3.9916953823870402
2022-03-17 13:43:16 | INFO | fairseq.trainer | begin training epoch 26
2022-03-17 13:43:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 13:44:24 | INFO | train_inner | epoch 026:     45 / 407 loss=5.857, ppl=57.97, wps=38360.6, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.475, loss_scale=32, train_wall=145, gb_free=21, wall=15841
2022-03-17 13:44:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 13:45:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 13:46:56 | INFO | train_inner | epoch 026:    147 / 407 loss=5.832, ppl=56.96, wps=42895.2, ups=0.65, wpb=65525.8, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.474, loss_scale=16, train_wall=148, gb_free=21, wall=15994
2022-03-17 13:49:26 | INFO | train_inner | epoch 026:    247 / 407 loss=5.851, ppl=57.73, wps=43704.6, ups=0.67, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.481, loss_scale=16, train_wall=145, gb_free=21, wall=16144
2022-03-17 13:51:56 | INFO | train_inner | epoch 026:    347 / 407 loss=5.872, ppl=58.58, wps=43750.1, ups=0.67, wpb=65534.2, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.477, loss_scale=16, train_wall=145, gb_free=21, wall=16293
2022-03-17 13:53:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 13:53:45 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.935 | ppl 61.17 | wps 70469.7 | wpb 2047.5 | bsz 4 | num_updates 10560 | best_loss 5.935
2022-03-17 13:53:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10560 updates
2022-03-17 13:53:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 13:53:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 13:53:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 26 @ 10560 updates, score 5.935) (writing took 2.037695549428463 seconds)
2022-03-17 13:53:47 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-17 13:53:47 | INFO | train | epoch 026 | loss 5.849 | ppl 57.62 | wps 42063.4 | ups 0.64 | wpb 65492.6 | bsz 127.9 | num_updates 10560 | lr 0.000307729 | gnorm 0.476 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 16404
KL Stats: Epoch 26 Divergences: Uniform: 4.588462499580502 Unigram: 4.003706688102037
2022-03-17 13:53:47 | INFO | fairseq.trainer | begin training epoch 27
2022-03-17 13:53:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 13:54:47 | INFO | train_inner | epoch 027:     40 / 407 loss=5.833, ppl=57, wps=38328.4, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.469, loss_scale=16, train_wall=145, gb_free=21, wall=16464
2022-03-17 13:57:16 | INFO | train_inner | epoch 027:    140 / 407 loss=5.809, ppl=56.08, wps=43755.1, ups=0.67, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.475, loss_scale=16, train_wall=145, gb_free=21, wall=16614
2022-03-17 13:59:46 | INFO | train_inner | epoch 027:    240 / 407 loss=5.835, ppl=57.08, wps=43710.1, ups=0.67, wpb=65534.2, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.468, loss_scale=32, train_wall=145, gb_free=21, wall=16764
2022-03-17 14:02:16 | INFO | train_inner | epoch 027:    340 / 407 loss=5.835, ppl=57.09, wps=43723.4, ups=0.67, wpb=65525.8, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.473, loss_scale=32, train_wall=145, gb_free=21, wall=16914
2022-03-17 14:03:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 14:04:15 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.918 | ppl 60.48 | wps 70490.1 | wpb 2047.5 | bsz 4 | num_updates 10967 | best_loss 5.918
2022-03-17 14:04:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10967 updates
2022-03-17 14:04:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 14:04:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 14:04:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 27 @ 10967 updates, score 5.918) (writing took 1.8345003053545952 seconds)
2022-03-17 14:04:17 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-17 14:04:17 | INFO | train | epoch 027 | loss 5.826 | ppl 56.73 | wps 42282.8 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 10967 | lr 0.000301965 | gnorm 0.472 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 17034
KL Stats: Epoch 27 Divergences: Uniform: 4.6034544178090675 Unigram: 4.018410097390484
2022-03-17 14:04:17 | INFO | fairseq.trainer | begin training epoch 28
2022-03-17 14:04:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 14:05:07 | INFO | train_inner | epoch 028:     33 / 407 loss=5.822, ppl=56.57, wps=38374.7, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=11000, lr=0.000301511, gnorm=0.477, loss_scale=32, train_wall=145, gb_free=21, wall=17084
2022-03-17 14:07:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 14:07:38 | INFO | train_inner | epoch 028:    134 / 407 loss=5.786, ppl=55.17, wps=43309.4, ups=0.66, wpb=65534.2, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.48, loss_scale=16, train_wall=147, gb_free=21, wall=17235
2022-03-17 14:10:08 | INFO | train_inner | epoch 028:    234 / 407 loss=5.807, ppl=55.99, wps=43713.1, ups=0.67, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.473, loss_scale=16, train_wall=145, gb_free=21, wall=17385
2022-03-17 14:12:38 | INFO | train_inner | epoch 028:    334 / 407 loss=5.818, ppl=56.41, wps=43754, ups=0.67, wpb=65525.8, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.471, loss_scale=16, train_wall=145, gb_free=21, wall=17535
2022-03-17 14:14:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 14:14:46 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.909 | ppl 60.07 | wps 70546.3 | wpb 2047.5 | bsz 4 | num_updates 11373 | best_loss 5.909
2022-03-17 14:14:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 11373 updates
2022-03-17 14:14:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 14:14:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 14:14:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 28 @ 11373 updates, score 5.909) (writing took 1.8466850584372878 seconds)
2022-03-17 14:14:47 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-17 14:14:47 | INFO | train | epoch 028 | loss 5.804 | ppl 55.88 | wps 42182.7 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 11373 | lr 0.000296526 | gnorm 0.477 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 17665
KL Stats: Epoch 28 Divergences: Uniform: 4.615467983760104 Unigram: 4.031997203146581
2022-03-17 14:14:47 | INFO | fairseq.trainer | begin training epoch 29
2022-03-17 14:14:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 14:15:28 | INFO | train_inner | epoch 029:     27 / 407 loss=5.805, ppl=55.89, wps=38375.3, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=11400, lr=0.000296174, gnorm=0.48, loss_scale=16, train_wall=145, gb_free=21, wall=17705
2022-03-17 14:17:58 | INFO | train_inner | epoch 029:    127 / 407 loss=5.769, ppl=54.53, wps=43733.4, ups=0.67, wpb=65534.2, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.471, loss_scale=16, train_wall=145, gb_free=21, wall=17855
2022-03-17 14:20:28 | INFO | train_inner | epoch 029:    227 / 407 loss=5.783, ppl=55.04, wps=43714.3, ups=0.67, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.48, loss_scale=32, train_wall=145, gb_free=21, wall=18005
2022-03-17 14:22:58 | INFO | train_inner | epoch 029:    327 / 407 loss=5.8, ppl=55.71, wps=43730.2, ups=0.67, wpb=65525.8, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.478, loss_scale=32, train_wall=145, gb_free=21, wall=18155
2022-03-17 14:24:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 14:25:16 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.894 | ppl 59.48 | wps 70450.7 | wpb 2047.5 | bsz 4 | num_updates 11780 | best_loss 5.894
2022-03-17 14:25:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11780 updates
2022-03-17 14:25:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 14:25:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 14:25:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 29 @ 11780 updates, score 5.894) (writing took 1.857187144458294 seconds)
2022-03-17 14:25:18 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-17 14:25:18 | INFO | train | epoch 029 | loss 5.784 | ppl 55.12 | wps 42272.4 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 11780 | lr 0.000291358 | gnorm 0.476 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 18295
KL Stats: Epoch 29 Divergences: Uniform: 4.62944387631637 Unigram: 4.044825232999295
2022-03-17 14:25:18 | INFO | fairseq.trainer | begin training epoch 30
2022-03-17 14:25:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 14:25:48 | INFO | train_inner | epoch 030:     20 / 407 loss=5.787, ppl=55.21, wps=38350.9, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=11800, lr=0.000291111, gnorm=0.476, loss_scale=32, train_wall=145, gb_free=21, wall=18325
2022-03-17 14:28:18 | INFO | train_inner | epoch 030:    120 / 407 loss=5.747, ppl=53.71, wps=43762.4, ups=0.67, wpb=65525.8, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.476, loss_scale=32, train_wall=145, gb_free=21, wall=18475
2022-03-17 14:30:48 | INFO | train_inner | epoch 030:    220 / 407 loss=5.757, ppl=54.08, wps=43713.1, ups=0.67, wpb=65534.2, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.474, loss_scale=32, train_wall=145, gb_free=21, wall=18625
2022-03-17 14:33:17 | INFO | train_inner | epoch 030:    320 / 407 loss=5.78, ppl=54.95, wps=43726.4, ups=0.67, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.48, loss_scale=32, train_wall=145, gb_free=21, wall=18775
2022-03-17 14:33:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 14:35:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 14:35:47 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.888 | ppl 59.23 | wps 70377.1 | wpb 2047.5 | bsz 4 | num_updates 12186 | best_loss 5.888
2022-03-17 14:35:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 12186 updates
2022-03-17 14:35:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 14:35:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 14:35:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 30 @ 12186 updates, score 5.888) (writing took 1.7583689792081714 seconds)
2022-03-17 14:35:48 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-17 14:35:48 | INFO | train | epoch 030 | loss 5.765 | ppl 54.38 | wps 42181.7 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 12186 | lr 0.000286464 | gnorm 0.477 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 18926
KL Stats: Epoch 30 Divergences: Uniform: 4.641409733711049 Unigram: 4.059306559737998
2022-03-17 14:35:48 | INFO | fairseq.trainer | begin training epoch 31
2022-03-17 14:35:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 14:36:09 | INFO | train_inner | epoch 031:     14 / 407 loss=5.774, ppl=54.72, wps=38034, ups=0.58, wpb=65361.9, bsz=127.7, num_updates=12200, lr=0.000286299, gnorm=0.478, loss_scale=32, train_wall=146, gb_free=21, wall=18947
2022-03-17 14:38:39 | INFO | train_inner | epoch 031:    114 / 407 loss=5.724, ppl=52.85, wps=43744.9, ups=0.67, wpb=65534.2, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.477, loss_scale=32, train_wall=145, gb_free=21, wall=19097
2022-03-17 14:39:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 14:41:10 | INFO | train_inner | epoch 031:    215 / 407 loss=5.745, ppl=53.64, wps=43330.9, ups=0.66, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.48, loss_scale=16, train_wall=147, gb_free=21, wall=19248
2022-03-17 14:43:40 | INFO | train_inner | epoch 031:    315 / 407 loss=5.76, ppl=54.21, wps=43762.5, ups=0.67, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.474, loss_scale=16, train_wall=145, gb_free=21, wall=19398
2022-03-17 14:45:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 14:46:17 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.874 | ppl 58.63 | wps 70387.1 | wpb 2047.5 | bsz 4 | num_updates 12592 | best_loss 5.874
2022-03-17 14:46:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12592 updates
2022-03-17 14:46:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 14:46:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 14:46:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 31 @ 12592 updates, score 5.874) (writing took 1.7724057547748089 seconds)
2022-03-17 14:46:18 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-17 14:46:18 | INFO | train | epoch 031 | loss 5.748 | ppl 53.73 | wps 42197.5 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 12592 | lr 0.000281808 | gnorm 0.478 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 19556
KL Stats: Epoch 31 Divergences: Uniform: 4.651535321782183 Unigram: 4.0687240101856315
2022-03-17 14:46:18 | INFO | fairseq.trainer | begin training epoch 32
2022-03-17 14:46:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 14:46:30 | INFO | train_inner | epoch 032:      8 / 407 loss=5.76, ppl=54.2, wps=38377.8, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=12600, lr=0.000281718, gnorm=0.48, loss_scale=16, train_wall=145, gb_free=21, wall=19568
2022-03-17 14:49:00 | INFO | train_inner | epoch 032:    108 / 407 loss=5.705, ppl=52.16, wps=43763.8, ups=0.67, wpb=65534.2, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.479, loss_scale=16, train_wall=145, gb_free=21, wall=19718
2022-03-17 14:51:30 | INFO | train_inner | epoch 032:    208 / 407 loss=5.729, ppl=53.03, wps=43752.8, ups=0.67, wpb=65525.8, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.478, loss_scale=16, train_wall=145, gb_free=21, wall=19867
2022-03-17 14:54:00 | INFO | train_inner | epoch 032:    308 / 407 loss=5.736, ppl=53.29, wps=43708.4, ups=0.67, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.481, loss_scale=32, train_wall=145, gb_free=21, wall=20017
2022-03-17 14:56:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 14:56:47 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.867 | ppl 58.38 | wps 70525.9 | wpb 2047.5 | bsz 4 | num_updates 12999 | best_loss 5.867
2022-03-17 14:56:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12999 updates
2022-03-17 14:56:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 14:56:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 14:56:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 32 @ 12999 updates, score 5.867) (writing took 1.7589378030970693 seconds)
2022-03-17 14:56:49 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-17 14:56:49 | INFO | train | epoch 032 | loss 5.731 | ppl 53.11 | wps 42292.8 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 12999 | lr 0.000277361 | gnorm 0.479 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 20186
KL Stats: Epoch 32 Divergences: Uniform: 4.665281354510983 Unigram: 4.08005215849108
2022-03-17 14:56:49 | INFO | fairseq.trainer | begin training epoch 33
2022-03-17 14:56:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 14:56:50 | INFO | train_inner | epoch 033:      1 / 407 loss=5.758, ppl=54.11, wps=38376.1, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=13000, lr=0.00027735, gnorm=0.479, loss_scale=32, train_wall=145, gb_free=21, wall=20188
2022-03-17 14:59:20 | INFO | train_inner | epoch 033:    101 / 407 loss=5.685, ppl=51.46, wps=43756.6, ups=0.67, wpb=65525.8, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.483, loss_scale=32, train_wall=145, gb_free=21, wall=20337
2022-03-17 15:01:50 | INFO | train_inner | epoch 033:    201 / 407 loss=5.706, ppl=52.21, wps=43762, ups=0.67, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.479, loss_scale=32, train_wall=145, gb_free=21, wall=20487
2022-03-17 15:04:20 | INFO | train_inner | epoch 033:    301 / 407 loss=5.736, ppl=53.31, wps=43731.4, ups=0.67, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.488, loss_scale=32, train_wall=145, gb_free=21, wall=20637
2022-03-17 15:06:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 15:06:51 | INFO | train_inner | epoch 033:    402 / 407 loss=5.733, ppl=53.2, wps=43340.4, ups=0.66, wpb=65534.2, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.48, loss_scale=32, train_wall=147, gb_free=21, wall=20788
2022-03-17 15:06:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 15:07:17 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.855 | ppl 57.89 | wps 70498 | wpb 2047.5 | bsz 4 | num_updates 13405 | best_loss 5.855
2022-03-17 15:07:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 13405 updates
2022-03-17 15:07:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 15:07:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 15:07:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 33 @ 13405 updates, score 5.855) (writing took 1.796469003893435 seconds)
2022-03-17 15:07:19 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-17 15:07:19 | INFO | train | epoch 033 | loss 5.715 | ppl 52.52 | wps 42201.8 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 13405 | lr 0.000273128 | gnorm 0.483 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 20816
KL Stats: Epoch 33 Divergences: Uniform: 4.675005188672094 Unigram: 4.090659877903745
2022-03-17 15:07:19 | INFO | fairseq.trainer | begin training epoch 34
2022-03-17 15:07:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 15:09:41 | INFO | train_inner | epoch 034:     95 / 407 loss=5.672, ppl=50.98, wps=38389.5, ups=0.59, wpb=65361.9, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=0.485, loss_scale=32, train_wall=145, gb_free=21, wall=20959
2022-03-17 15:12:11 | INFO | train_inner | epoch 034:    195 / 407 loss=5.701, ppl=52.02, wps=43753.3, ups=0.67, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.478, loss_scale=32, train_wall=145, gb_free=21, wall=21108
2022-03-17 15:13:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 15:14:42 | INFO | train_inner | epoch 034:    296 / 407 loss=5.705, ppl=52.15, wps=43303, ups=0.66, wpb=65534.2, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.48, loss_scale=16, train_wall=147, gb_free=21, wall=21260
2022-03-17 15:17:12 | INFO | train_inner | epoch 034:    396 / 407 loss=5.719, ppl=52.66, wps=43736.5, ups=0.67, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.479, loss_scale=16, train_wall=145, gb_free=21, wall=21410
2022-03-17 15:17:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 15:17:47 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.846 | ppl 57.52 | wps 70337.6 | wpb 2047.5 | bsz 4 | num_updates 13811 | best_loss 5.846
2022-03-17 15:17:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13811 updates
2022-03-17 15:17:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 15:17:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 15:17:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 34 @ 13811 updates, score 5.846) (writing took 1.7841744804754853 seconds)
2022-03-17 15:17:49 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-17 15:17:49 | INFO | train | epoch 034 | loss 5.7 | ppl 51.97 | wps 42186.5 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 13811 | lr 0.000269084 | gnorm 0.48 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 21446
KL Stats: Epoch 34 Divergences: Uniform: 4.684689176917752 Unigram: 4.099930516042896
2022-03-17 15:17:49 | INFO | fairseq.trainer | begin training epoch 35
2022-03-17 15:17:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 15:20:02 | INFO | train_inner | epoch 035:     89 / 407 loss=5.661, ppl=50.61, wps=38382.8, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=0.485, loss_scale=16, train_wall=145, gb_free=21, wall=21580
2022-03-17 15:22:32 | INFO | train_inner | epoch 035:    189 / 407 loss=5.674, ppl=51.07, wps=43724.2, ups=0.67, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.482, loss_scale=16, train_wall=145, gb_free=21, wall=21730
2022-03-17 15:25:02 | INFO | train_inner | epoch 035:    289 / 407 loss=5.701, ppl=52.01, wps=43696.9, ups=0.67, wpb=65525.8, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.484, loss_scale=16, train_wall=145, gb_free=21, wall=21880
2022-03-17 15:27:32 | INFO | train_inner | epoch 035:    389 / 407 loss=5.709, ppl=52.31, wps=43724.3, ups=0.67, wpb=65534.2, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.478, loss_scale=32, train_wall=145, gb_free=21, wall=22030
2022-03-17 15:27:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 15:28:18 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.842 | ppl 57.35 | wps 70356.9 | wpb 2047.5 | bsz 4 | num_updates 14218 | best_loss 5.842
2022-03-17 15:28:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 14218 updates
2022-03-17 15:28:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 15:28:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 15:28:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 35 @ 14218 updates, score 5.842) (writing took 1.7816716264933348 seconds)
2022-03-17 15:28:20 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-17 15:28:20 | INFO | train | epoch 035 | loss 5.686 | ppl 51.5 | wps 42277.2 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 14218 | lr 0.000265204 | gnorm 0.482 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 22077
KL Stats: Epoch 35 Divergences: Uniform: 4.694845075138051 Unigram: 4.1097501429339225
2022-03-17 15:28:20 | INFO | fairseq.trainer | begin training epoch 36
2022-03-17 15:28:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 15:30:23 | INFO | train_inner | epoch 036:     82 / 407 loss=5.655, ppl=50.37, wps=38362.2, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=0.487, loss_scale=32, train_wall=145, gb_free=21, wall=22200
2022-03-17 15:32:52 | INFO | train_inner | epoch 036:    182 / 407 loss=5.662, ppl=50.64, wps=43750.3, ups=0.67, wpb=65534.2, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.484, loss_scale=32, train_wall=145, gb_free=21, wall=22350
2022-03-17 15:35:22 | INFO | train_inner | epoch 036:    282 / 407 loss=5.677, ppl=51.18, wps=43719.2, ups=0.67, wpb=65525.8, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.483, loss_scale=32, train_wall=145, gb_free=21, wall=22500
2022-03-17 15:37:52 | INFO | train_inner | epoch 036:    382 / 407 loss=5.695, ppl=51.81, wps=43747.7, ups=0.67, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.483, loss_scale=32, train_wall=145, gb_free=21, wall=22649
2022-03-17 15:38:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 15:38:48 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.83 | ppl 56.88 | wps 70603.3 | wpb 2047.5 | bsz 4 | num_updates 14625 | best_loss 5.83
2022-03-17 15:38:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14625 updates
2022-03-17 15:38:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 15:38:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 15:38:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 36 @ 14625 updates, score 5.83) (writing took 1.7847952507436275 seconds)
2022-03-17 15:38:50 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-17 15:38:50 | INFO | train | epoch 036 | loss 5.673 | ppl 51.01 | wps 42290.7 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 14625 | lr 0.000261488 | gnorm 0.484 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 22707
KL Stats: Epoch 36 Divergences: Uniform: 4.703470176409297 Unigram: 4.117995390735903
2022-03-17 15:38:50 | INFO | fairseq.trainer | begin training epoch 37
2022-03-17 15:38:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 15:40:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 15:40:44 | INFO | train_inner | epoch 037:     76 / 407 loss=5.641, ppl=49.91, wps=38064.9, ups=0.58, wpb=65372.2, bsz=127.7, num_updates=14700, lr=0.00026082, gnorm=0.485, loss_scale=32, train_wall=146, gb_free=21, wall=22821
2022-03-17 15:43:14 | INFO | train_inner | epoch 037:    176 / 407 loss=5.648, ppl=50.13, wps=43741, ups=0.67, wpb=65525.8, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.482, loss_scale=32, train_wall=145, gb_free=21, wall=22971
2022-03-17 15:45:44 | INFO | train_inner | epoch 037:    276 / 407 loss=5.667, ppl=50.8, wps=43704.1, ups=0.67, wpb=65534.2, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.495, loss_scale=32, train_wall=145, gb_free=21, wall=23121
2022-03-17 15:48:13 | INFO | train_inner | epoch 037:    376 / 407 loss=5.687, ppl=51.53, wps=43708.2, ups=0.67, wpb=65536, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.482, loss_scale=32, train_wall=145, gb_free=21, wall=23271
2022-03-17 15:49:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 15:49:19 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.823 | ppl 56.62 | wps 69625.9 | wpb 2047.5 | bsz 4 | num_updates 15031 | best_loss 5.823
2022-03-17 15:49:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 15031 updates
2022-03-17 15:49:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 15:49:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 15:49:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 37 @ 15031 updates, score 5.823) (writing took 1.7908265916630626 seconds)
2022-03-17 15:49:21 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-17 15:49:21 | INFO | train | epoch 037 | loss 5.66 | ppl 50.55 | wps 42152.4 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 15031 | lr 0.000257932 | gnorm 0.486 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 23338
KL Stats: Epoch 37 Divergences: Uniform: 4.7120140939345845 Unigram: 4.127830344648974
2022-03-17 15:49:21 | INFO | fairseq.trainer | begin training epoch 38
2022-03-17 15:49:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 15:51:05 | INFO | train_inner | epoch 038:     69 / 407 loss=5.631, ppl=49.56, wps=38181.6, ups=0.58, wpb=65372.2, bsz=127.7, num_updates=15100, lr=0.000257343, gnorm=0.488, loss_scale=32, train_wall=146, gb_free=21, wall=23442
2022-03-17 15:51:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 15:53:37 | INFO | train_inner | epoch 038:    170 / 407 loss=5.637, ppl=49.75, wps=43059.6, ups=0.66, wpb=65525.8, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.494, loss_scale=16, train_wall=148, gb_free=21, wall=23594
2022-03-17 15:56:08 | INFO | train_inner | epoch 038:    270 / 407 loss=5.663, ppl=50.66, wps=43444.6, ups=0.66, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.485, loss_scale=16, train_wall=146, gb_free=21, wall=23745
2022-03-17 15:58:38 | INFO | train_inner | epoch 038:    370 / 407 loss=5.66, ppl=50.58, wps=43475.1, ups=0.66, wpb=65534.2, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.481, loss_scale=16, train_wall=146, gb_free=21, wall=23896
2022-03-17 15:59:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 15:59:53 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.816 | ppl 56.35 | wps 69962.4 | wpb 2047.5 | bsz 4 | num_updates 15437 | best_loss 5.816
2022-03-17 15:59:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 15437 updates
2022-03-17 15:59:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 15:59:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 15:59:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 38 @ 15437 updates, score 5.816) (writing took 1.796943198889494 seconds)
2022-03-17 15:59:55 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-17 15:59:55 | INFO | train | epoch 038 | loss 5.648 | ppl 50.14 | wps 41930.4 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 15437 | lr 0.000254518 | gnorm 0.488 | loss_scale 16 | train_wall 595 | gb_free 21 | wall 23972
KL Stats: Epoch 38 Divergences: Uniform: 4.722120090897698 Unigram: 4.135143827864342
2022-03-17 15:59:55 | INFO | fairseq.trainer | begin training epoch 39
2022-03-17 15:59:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 16:01:30 | INFO | train_inner | epoch 039:     63 / 407 loss=5.628, ppl=49.45, wps=38139.4, ups=0.58, wpb=65361.9, bsz=127.7, num_updates=15500, lr=0.000254, gnorm=0.487, loss_scale=16, train_wall=146, gb_free=21, wall=24067
2022-03-17 16:04:01 | INFO | train_inner | epoch 039:    163 / 407 loss=5.627, ppl=49.43, wps=43449.4, ups=0.66, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.486, loss_scale=16, train_wall=146, gb_free=21, wall=24218
2022-03-17 16:06:31 | INFO | train_inner | epoch 039:    263 / 407 loss=5.635, ppl=49.7, wps=43466.9, ups=0.66, wpb=65534.2, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.486, loss_scale=32, train_wall=146, gb_free=21, wall=24369
2022-03-17 16:09:02 | INFO | train_inner | epoch 039:    363 / 407 loss=5.655, ppl=50.39, wps=43500.2, ups=0.66, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.486, loss_scale=32, train_wall=146, gb_free=21, wall=24520
2022-03-17 16:10:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 16:10:27 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.81 | ppl 56.1 | wps 69834.8 | wpb 2047.5 | bsz 4 | num_updates 15844 | best_loss 5.81
2022-03-17 16:10:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15844 updates
2022-03-17 16:10:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 16:10:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 16:10:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 39 @ 15844 updates, score 5.81) (writing took 1.8005050346255302 seconds)
2022-03-17 16:10:29 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-17 16:10:29 | INFO | train | epoch 039 | loss 5.636 | ppl 49.72 | wps 42033.2 | ups 0.64 | wpb 65492.8 | bsz 127.9 | num_updates 15844 | lr 0.000251228 | gnorm 0.486 | loss_scale 32 | train_wall 595 | gb_free 21 | wall 24606
KL Stats: Epoch 39 Divergences: Uniform: 4.730811035486992 Unigram: 4.144070770929343
2022-03-17 16:10:29 | INFO | fairseq.trainer | begin training epoch 40
2022-03-17 16:10:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 16:11:53 | INFO | train_inner | epoch 040:     56 / 407 loss=5.626, ppl=49.39, wps=38149, ups=0.58, wpb=65361.9, bsz=127.7, num_updates=15900, lr=0.000250785, gnorm=0.486, loss_scale=32, train_wall=146, gb_free=21, wall=24691
2022-03-17 16:14:24 | INFO | train_inner | epoch 040:    156 / 407 loss=5.614, ppl=48.98, wps=43477.9, ups=0.66, wpb=65534.2, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.49, loss_scale=32, train_wall=146, gb_free=21, wall=24842
2022-03-17 16:16:55 | INFO | train_inner | epoch 040:    256 / 407 loss=5.62, ppl=49.19, wps=43466.1, ups=0.66, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.485, loss_scale=32, train_wall=146, gb_free=21, wall=24992
2022-03-17 16:18:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 16:19:27 | INFO | train_inner | epoch 040:    357 / 407 loss=5.643, ppl=49.96, wps=43122, ups=0.66, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.487, loss_scale=32, train_wall=147, gb_free=21, wall=25144
2022-03-17 16:20:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 16:21:01 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.805 | ppl 55.91 | wps 70582.2 | wpb 2047.5 | bsz 4 | num_updates 16250 | best_loss 5.805
2022-03-17 16:21:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 16250 updates
2022-03-17 16:21:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 16:21:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 16:21:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 40 @ 16250 updates, score 5.805) (writing took 1.7822221368551254 seconds)
2022-03-17 16:21:03 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-17 16:21:03 | INFO | train | epoch 040 | loss 5.625 | ppl 49.34 | wps 41968.3 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 16250 | lr 0.000248069 | gnorm 0.487 | loss_scale 32 | train_wall 594 | gb_free 21 | wall 25240
KL Stats: Epoch 40 Divergences: Uniform: 4.738263123733635 Unigram: 4.152871179310304
2022-03-17 16:21:03 | INFO | fairseq.trainer | begin training epoch 41
2022-03-17 16:21:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 16:22:18 | INFO | train_inner | epoch 041:     50 / 407 loss=5.613, ppl=48.95, wps=38300, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=16300, lr=0.000247689, gnorm=0.484, loss_scale=32, train_wall=145, gb_free=21, wall=25315
2022-03-17 16:24:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 16:24:49 | INFO | train_inner | epoch 041:    151 / 407 loss=5.604, ppl=48.63, wps=43258.9, ups=0.66, wpb=65523.9, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.492, loss_scale=16, train_wall=147, gb_free=21, wall=25466
2022-03-17 16:27:19 | INFO | train_inner | epoch 041:    251 / 407 loss=5.623, ppl=49.27, wps=43673, ups=0.67, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.491, loss_scale=16, train_wall=146, gb_free=21, wall=25617
2022-03-17 16:29:49 | INFO | train_inner | epoch 041:    351 / 407 loss=5.624, ppl=49.33, wps=43671.2, ups=0.67, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.49, loss_scale=16, train_wall=146, gb_free=21, wall=25767
2022-03-17 16:31:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 16:31:32 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 5.799 | ppl 55.68 | wps 70307.2 | wpb 2047.5 | bsz 4 | num_updates 16656 | best_loss 5.799
2022-03-17 16:31:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16656 updates
2022-03-17 16:31:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 16:31:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 16:31:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 41 @ 16656 updates, score 5.799) (writing took 1.7957548750564456 seconds)
2022-03-17 16:31:34 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-17 16:31:34 | INFO | train | epoch 041 | loss 5.614 | ppl 48.98 | wps 42132 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 16656 | lr 0.000245027 | gnorm 0.49 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 25871
KL Stats: Epoch 41 Divergences: Uniform: 4.74591895253998 Unigram: 4.157916433030124
2022-03-17 16:31:34 | INFO | fairseq.trainer | begin training epoch 42
2022-03-17 16:31:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 16:32:40 | INFO | train_inner | epoch 042:     44 / 407 loss=5.602, ppl=48.58, wps=38344.1, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=16700, lr=0.000244704, gnorm=0.488, loss_scale=16, train_wall=145, gb_free=21, wall=25937
2022-03-17 16:35:10 | INFO | train_inner | epoch 042:    144 / 407 loss=5.592, ppl=48.25, wps=43664, ups=0.67, wpb=65523.9, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.491, loss_scale=16, train_wall=146, gb_free=21, wall=26087
2022-03-17 16:37:40 | INFO | train_inner | epoch 042:    244 / 407 loss=5.606, ppl=48.69, wps=43658.1, ups=0.67, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.493, loss_scale=16, train_wall=146, gb_free=21, wall=26237
2022-03-17 16:39:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 16:40:11 | INFO | train_inner | epoch 042:    345 / 407 loss=5.617, ppl=49.08, wps=43245.7, ups=0.66, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.487, loss_scale=16, train_wall=147, gb_free=21, wall=26389
2022-03-17 16:41:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 16:42:03 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 5.794 | ppl 55.48 | wps 70319 | wpb 2047.5 | bsz 4 | num_updates 17062 | best_loss 5.794
2022-03-17 16:42:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 17062 updates
2022-03-17 16:42:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 16:42:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 16:42:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 42 @ 17062 updates, score 5.794) (writing took 1.8719056388363242 seconds)
2022-03-17 16:42:05 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-17 16:42:05 | INFO | train | epoch 042 | loss 5.604 | ppl 48.63 | wps 42115.1 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 17062 | lr 0.000242095 | gnorm 0.49 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 26502
KL Stats: Epoch 42 Divergences: Uniform: 4.753492406597734 Unigram: 4.16587231519869
2022-03-17 16:42:05 | INFO | fairseq.trainer | begin training epoch 43
2022-03-17 16:42:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 16:43:02 | INFO | train_inner | epoch 043:     38 / 407 loss=5.602, ppl=48.57, wps=38289.8, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=17100, lr=0.000241825, gnorm=0.492, loss_scale=16, train_wall=145, gb_free=21, wall=26560
2022-03-17 16:45:32 | INFO | train_inner | epoch 043:    138 / 407 loss=5.58, ppl=47.84, wps=43694.3, ups=0.67, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.491, loss_scale=16, train_wall=145, gb_free=21, wall=26710
2022-03-17 16:48:02 | INFO | train_inner | epoch 043:    238 / 407 loss=5.583, ppl=47.93, wps=43697.4, ups=0.67, wpb=65525.8, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.486, loss_scale=16, train_wall=145, gb_free=21, wall=26859
2022-03-17 16:50:32 | INFO | train_inner | epoch 043:    338 / 407 loss=5.613, ppl=48.94, wps=43730.7, ups=0.67, wpb=65534.2, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.494, loss_scale=16, train_wall=145, gb_free=21, wall=27009
2022-03-17 16:52:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 16:52:34 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 5.792 | ppl 55.42 | wps 70586.1 | wpb 2047.5 | bsz 4 | num_updates 17469 | best_loss 5.792
2022-03-17 16:52:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 17469 updates
2022-03-17 16:52:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 16:52:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 16:52:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 43 @ 17469 updates, score 5.792) (writing took 1.8060523392632604 seconds)
2022-03-17 16:52:36 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-17 16:52:36 | INFO | train | epoch 043 | loss 5.594 | ppl 48.3 | wps 42256.8 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 17469 | lr 0.000239258 | gnorm 0.49 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 27133
KL Stats: Epoch 43 Divergences: Uniform: 4.762095866439677 Unigram: 4.174491377820562
2022-03-17 16:52:36 | INFO | fairseq.trainer | begin training epoch 44
2022-03-17 16:52:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 16:53:22 | INFO | train_inner | epoch 044:     31 / 407 loss=5.597, ppl=48.41, wps=38353.4, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=17500, lr=0.000239046, gnorm=0.486, loss_scale=32, train_wall=145, gb_free=21, wall=27180
2022-03-17 16:55:52 | INFO | train_inner | epoch 044:    131 / 407 loss=5.562, ppl=47.23, wps=43684.2, ups=0.67, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.489, loss_scale=32, train_wall=145, gb_free=21, wall=27330
2022-03-17 16:58:22 | INFO | train_inner | epoch 044:    231 / 407 loss=5.592, ppl=48.23, wps=43711.3, ups=0.67, wpb=65525.8, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.492, loss_scale=32, train_wall=145, gb_free=21, wall=27480
2022-03-17 17:00:52 | INFO | train_inner | epoch 044:    331 / 407 loss=5.589, ppl=48.14, wps=43711.7, ups=0.67, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.496, loss_scale=32, train_wall=145, gb_free=21, wall=27630
2022-03-17 17:02:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 17:03:05 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 5.785 | ppl 55.12 | wps 70642 | wpb 2047.5 | bsz 4 | num_updates 17876 | best_loss 5.785
2022-03-17 17:03:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17876 updates
2022-03-17 17:03:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 17:03:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 17:03:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 44 @ 17876 updates, score 5.785) (writing took 1.8457950027659535 seconds)
2022-03-17 17:03:07 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-17 17:03:07 | INFO | train | epoch 044 | loss 5.584 | ppl 47.98 | wps 42258.3 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 17876 | lr 0.000236518 | gnorm 0.493 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 27764
KL Stats: Epoch 44 Divergences: Uniform: 4.770102498752447 Unigram: 4.181284103145847
2022-03-17 17:03:07 | INFO | fairseq.trainer | begin training epoch 45
2022-03-17 17:03:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 17:03:43 | INFO | train_inner | epoch 045:     24 / 407 loss=5.593, ppl=48.27, wps=38350, ups=0.59, wpb=65368.5, bsz=127.7, num_updates=17900, lr=0.00023636, gnorm=0.497, loss_scale=32, train_wall=145, gb_free=21, wall=27800
2022-03-17 17:06:13 | INFO | train_inner | epoch 045:    124 / 407 loss=5.555, ppl=47.02, wps=43707, ups=0.67, wpb=65525.8, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.491, loss_scale=32, train_wall=145, gb_free=21, wall=27950
2022-03-17 17:06:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 17:08:44 | INFO | train_inner | epoch 045:    225 / 407 loss=5.571, ppl=47.55, wps=43263.1, ups=0.66, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.496, loss_scale=32, train_wall=147, gb_free=21, wall=28102
2022-03-17 17:09:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 17:11:16 | INFO | train_inner | epoch 045:    326 / 407 loss=5.59, ppl=48.17, wps=43272.8, ups=0.66, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.486, loss_scale=16, train_wall=147, gb_free=21, wall=28253
2022-03-17 17:13:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 17:13:36 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 5.78 | ppl 54.95 | wps 70677.4 | wpb 2047.5 | bsz 4 | num_updates 18281 | best_loss 5.78
2022-03-17 17:13:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 18281 updates
2022-03-17 17:13:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 17:13:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 17:13:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 45 @ 18281 updates, score 5.78) (writing took 1.8448187578469515 seconds)
2022-03-17 17:13:37 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-17 17:13:37 | INFO | train | epoch 045 | loss 5.575 | ppl 47.68 | wps 42050.9 | ups 0.64 | wpb 65492.6 | bsz 127.9 | num_updates 18281 | lr 0.000233884 | gnorm 0.492 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 28395
KL Stats: Epoch 45 Divergences: Uniform: 4.777021698195515 Unigram: 4.187762556831384
2022-03-17 17:13:37 | INFO | fairseq.trainer | begin training epoch 46
2022-03-17 17:13:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 17:14:06 | INFO | train_inner | epoch 046:     19 / 407 loss=5.588, ppl=48.11, wps=38371.4, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=18300, lr=0.000233762, gnorm=0.496, loss_scale=16, train_wall=145, gb_free=21, wall=28423
2022-03-17 17:16:36 | INFO | train_inner | epoch 046:    119 / 407 loss=5.548, ppl=46.79, wps=43718, ups=0.67, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.493, loss_scale=16, train_wall=145, gb_free=21, wall=28573
2022-03-17 17:19:06 | INFO | train_inner | epoch 046:    219 / 407 loss=5.561, ppl=47.22, wps=43710.4, ups=0.67, wpb=65525.8, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.499, loss_scale=16, train_wall=145, gb_free=21, wall=28723
2022-03-17 17:21:36 | INFO | train_inner | epoch 046:    319 / 407 loss=5.58, ppl=47.82, wps=43691.8, ups=0.67, wpb=65534.2, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.493, loss_scale=16, train_wall=145, gb_free=21, wall=28873
2022-03-17 17:23:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 17:24:06 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 5.778 | ppl 54.86 | wps 70517.3 | wpb 2047.5 | bsz 4 | num_updates 18688 | best_loss 5.778
2022-03-17 17:24:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18688 updates
2022-03-17 17:24:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 17:24:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 17:24:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 46 @ 18688 updates, score 5.778) (writing took 1.7991344816982746 seconds)
2022-03-17 17:24:08 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-17 17:24:08 | INFO | train | epoch 046 | loss 5.567 | ppl 47.4 | wps 42268.6 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 18688 | lr 0.000231323 | gnorm 0.494 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 29025
KL Stats: Epoch 46 Divergences: Uniform: 4.785211568518053 Unigram: 4.195020106306716
2022-03-17 17:24:08 | INFO | fairseq.trainer | begin training epoch 47
2022-03-17 17:24:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 17:24:26 | INFO | train_inner | epoch 047:     12 / 407 loss=5.575, ppl=47.68, wps=38375, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=0.495, loss_scale=32, train_wall=145, gb_free=21, wall=29043
2022-03-17 17:25:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 17:26:57 | INFO | train_inner | epoch 047:    113 / 407 loss=5.531, ppl=46.25, wps=43295.4, ups=0.66, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.495, loss_scale=16, train_wall=147, gb_free=21, wall=29195
2022-03-17 17:29:27 | INFO | train_inner | epoch 047:    213 / 407 loss=5.556, ppl=47.03, wps=43733.2, ups=0.67, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.494, loss_scale=16, train_wall=145, gb_free=21, wall=29345
2022-03-17 17:31:57 | INFO | train_inner | epoch 047:    313 / 407 loss=5.573, ppl=47.61, wps=43702.4, ups=0.67, wpb=65525.8, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.494, loss_scale=16, train_wall=145, gb_free=21, wall=29495
2022-03-17 17:34:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 17:34:37 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 5.769 | ppl 54.52 | wps 70586.5 | wpb 2047.5 | bsz 4 | num_updates 19094 | best_loss 5.769
2022-03-17 17:34:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 19094 updates
2022-03-17 17:34:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 17:34:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 17:34:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 47 @ 19094 updates, score 5.769) (writing took 1.8498049285262823 seconds)
2022-03-17 17:34:39 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-17 17:34:39 | INFO | train | epoch 047 | loss 5.558 | ppl 47.12 | wps 42160.2 | ups 0.64 | wpb 65493.1 | bsz 127.9 | num_updates 19094 | lr 0.00022885 | gnorm 0.495 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 29656
KL Stats: Epoch 47 Divergences: Uniform: 4.789917507646113 Unigram: 4.200629019802659
2022-03-17 17:34:39 | INFO | fairseq.trainer | begin training epoch 48
2022-03-17 17:34:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 17:34:48 | INFO | train_inner | epoch 048:      6 / 407 loss=5.572, ppl=47.58, wps=38336.2, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=0.496, loss_scale=16, train_wall=145, gb_free=21, wall=29665
2022-03-17 17:37:18 | INFO | train_inner | epoch 048:    106 / 407 loss=5.528, ppl=46.15, wps=43709.7, ups=0.67, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.501, loss_scale=16, train_wall=145, gb_free=21, wall=29815
2022-03-17 17:39:48 | INFO | train_inner | epoch 048:    206 / 407 loss=5.542, ppl=46.6, wps=43714.8, ups=0.67, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.499, loss_scale=32, train_wall=145, gb_free=21, wall=29965
2022-03-17 17:42:18 | INFO | train_inner | epoch 048:    306 / 407 loss=5.561, ppl=47.21, wps=43673.6, ups=0.67, wpb=65523.9, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.496, loss_scale=32, train_wall=146, gb_free=21, wall=30115
2022-03-17 17:44:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 17:44:49 | INFO | train_inner | epoch 048:    407 / 407 loss=5.574, ppl=47.65, wps=43270.5, ups=0.66, wpb=65372.2, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=0.499, loss_scale=16, train_wall=147, gb_free=21, wall=30266
2022-03-17 17:44:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 17:45:08 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 5.768 | ppl 54.51 | wps 70557.2 | wpb 2047.5 | bsz 4 | num_updates 19500 | best_loss 5.768
2022-03-17 17:45:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 19500 updates
2022-03-17 17:45:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 17:45:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 17:45:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 48 @ 19500 updates, score 5.768) (writing took 1.8248872915282845 seconds)
2022-03-17 17:45:10 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-17 17:45:10 | INFO | train | epoch 048 | loss 5.55 | ppl 46.85 | wps 42150.1 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 19500 | lr 0.000226455 | gnorm 0.499 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 30287
KL Stats: Epoch 48 Divergences: Uniform: 4.79942768556404 Unigram: 4.20870845909635
2022-03-17 17:45:10 | INFO | fairseq.trainer | begin training epoch 49
2022-03-17 17:45:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 17:47:39 | INFO | train_inner | epoch 049:    100 / 407 loss=5.516, ppl=45.77, wps=38387.4, ups=0.59, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.491, loss_scale=16, train_wall=145, gb_free=21, wall=30437
2022-03-17 17:50:09 | INFO | train_inner | epoch 049:    200 / 407 loss=5.543, ppl=46.63, wps=43715.5, ups=0.67, wpb=65534.2, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.503, loss_scale=16, train_wall=145, gb_free=21, wall=30587
2022-03-17 17:52:39 | INFO | train_inner | epoch 049:    300 / 407 loss=5.553, ppl=46.94, wps=43697.2, ups=0.67, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.496, loss_scale=16, train_wall=145, gb_free=21, wall=30737
2022-03-17 17:55:09 | INFO | train_inner | epoch 049:    400 / 407 loss=5.555, ppl=47.01, wps=43723.9, ups=0.67, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.496, loss_scale=16, train_wall=145, gb_free=21, wall=30887
2022-03-17 17:55:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 17:55:38 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 5.764 | ppl 54.36 | wps 70612.1 | wpb 2047.5 | bsz 4 | num_updates 19907 | best_loss 5.764
2022-03-17 17:55:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19907 updates
2022-03-17 17:55:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 17:55:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 17:55:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 49 @ 19907 updates, score 5.764) (writing took 1.8637417769059539 seconds)
2022-03-17 17:55:40 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-17 17:55:40 | INFO | train | epoch 049 | loss 5.542 | ppl 46.6 | wps 42267.2 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 19907 | lr 0.000224129 | gnorm 0.497 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 30918
KL Stats: Epoch 49 Divergences: Uniform: 4.8036547774912695 Unigram: 4.2124638787959165
2022-03-17 17:55:40 | INFO | fairseq.trainer | begin training epoch 50
2022-03-17 17:55:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 17:58:00 | INFO | train_inner | epoch 050:     93 / 407 loss=5.515, ppl=45.73, wps=38343.7, ups=0.59, wpb=65360.1, bsz=127.7, num_updates=20000, lr=0.000223607, gnorm=0.498, loss_scale=32, train_wall=145, gb_free=21, wall=31057
2022-03-17 18:00:30 | INFO | train_inner | epoch 050:    193 / 407 loss=5.525, ppl=46.04, wps=43719, ups=0.67, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.492, loss_scale=32, train_wall=145, gb_free=21, wall=31207
2022-03-17 18:01:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 18:03:01 | INFO | train_inner | epoch 050:    294 / 407 loss=5.545, ppl=46.7, wps=43264, ups=0.66, wpb=65525.8, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.505, loss_scale=16, train_wall=147, gb_free=21, wall=31358
2022-03-17 18:05:31 | INFO | train_inner | epoch 050:    394 / 407 loss=5.551, ppl=46.87, wps=43710.3, ups=0.67, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.495, loss_scale=16, train_wall=145, gb_free=21, wall=31508
2022-03-17 18:05:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 18:06:09 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 5.76 | ppl 54.19 | wps 70498.9 | wpb 2047.5 | bsz 4 | num_updates 20313 | best_loss 5.76
2022-03-17 18:06:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 20313 updates
2022-03-17 18:06:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 18:06:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 18:06:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 50 @ 20313 updates, score 5.76) (writing took 1.783594991080463 seconds)
2022-03-17 18:06:11 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-17 18:06:11 | INFO | train | epoch 050 | loss 5.534 | ppl 46.34 | wps 42160.6 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 20313 | lr 0.000221877 | gnorm 0.498 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 31548
KL Stats: Epoch 50 Divergences: Uniform: 4.809639360282372 Unigram: 4.218808737345406
2022-03-17 18:06:11 | INFO | fairseq.trainer | begin training epoch 51
2022-03-17 18:06:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 18:08:21 | INFO | train_inner | epoch 051:     87 / 407 loss=5.504, ppl=45.38, wps=38373.3, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=20400, lr=0.000221404, gnorm=0.505, loss_scale=16, train_wall=145, gb_free=21, wall=31679
2022-03-17 18:10:51 | INFO | train_inner | epoch 051:    187 / 407 loss=5.521, ppl=45.92, wps=43705.3, ups=0.67, wpb=65536, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.496, loss_scale=16, train_wall=145, gb_free=21, wall=31829
2022-03-17 18:13:21 | INFO | train_inner | epoch 051:    287 / 407 loss=5.532, ppl=46.27, wps=43689.8, ups=0.67, wpb=65525.8, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.503, loss_scale=16, train_wall=145, gb_free=21, wall=31979
2022-03-17 18:15:51 | INFO | train_inner | epoch 051:    387 / 407 loss=5.545, ppl=46.7, wps=43731.9, ups=0.67, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.494, loss_scale=32, train_wall=145, gb_free=21, wall=32129
2022-03-17 18:16:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 18:16:40 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 5.755 | ppl 54 | wps 70599.5 | wpb 2047.5 | bsz 4 | num_updates 20720 | best_loss 5.755
2022-03-17 18:16:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20720 updates
2022-03-17 18:16:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 18:16:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 18:16:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 51 @ 20720 updates, score 5.755) (writing took 1.8067549066618085 seconds)
2022-03-17 18:16:42 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-17 18:16:42 | INFO | train | epoch 051 | loss 5.527 | ppl 46.11 | wps 42266 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 20720 | lr 0.000219687 | gnorm 0.499 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 32179
KL Stats: Epoch 51 Divergences: Uniform: 4.816681221186929 Unigram: 4.223553647126274
2022-03-17 18:16:42 | INFO | fairseq.trainer | begin training epoch 52
2022-03-17 18:16:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 18:18:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 18:18:43 | INFO | train_inner | epoch 052:     81 / 407 loss=5.5, ppl=45.24, wps=38015.7, ups=0.58, wpb=65368.5, bsz=127.7, num_updates=20800, lr=0.000219265, gnorm=0.499, loss_scale=16, train_wall=147, gb_free=21, wall=32300
2022-03-17 18:21:13 | INFO | train_inner | epoch 052:    181 / 407 loss=5.521, ppl=45.92, wps=43713.1, ups=0.67, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.501, loss_scale=16, train_wall=145, gb_free=21, wall=32450
2022-03-17 18:23:43 | INFO | train_inner | epoch 052:    281 / 407 loss=5.529, ppl=46.16, wps=43685.8, ups=0.67, wpb=65525.8, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.499, loss_scale=16, train_wall=145, gb_free=21, wall=32600
2022-03-17 18:26:13 | INFO | train_inner | epoch 052:    381 / 407 loss=5.535, ppl=46.37, wps=43707.9, ups=0.67, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.499, loss_scale=16, train_wall=145, gb_free=21, wall=32750
2022-03-17 18:26:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 18:27:11 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 5.755 | ppl 54.02 | wps 70382.3 | wpb 2047.5 | bsz 4 | num_updates 21126 | best_loss 5.755
2022-03-17 18:27:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 21126 updates
2022-03-17 18:27:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 18:27:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 18:27:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 52 @ 21126 updates, score 5.755) (writing took 1.7974711004644632 seconds)
2022-03-17 18:27:12 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-17 18:27:12 | INFO | train | epoch 052 | loss 5.52 | ppl 45.88 | wps 42147.8 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 21126 | lr 0.000217566 | gnorm 0.5 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 32810
KL Stats: Epoch 52 Divergences: Uniform: 4.8226591652744055 Unigram: 4.230577735593861
2022-03-17 18:27:12 | INFO | fairseq.trainer | begin training epoch 53
2022-03-17 18:27:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 18:29:03 | INFO | train_inner | epoch 053:     74 / 407 loss=5.496, ppl=45.13, wps=38339, ups=0.59, wpb=65370.3, bsz=127.7, num_updates=21200, lr=0.000217186, gnorm=0.502, loss_scale=16, train_wall=145, gb_free=21, wall=32921
2022-03-17 18:31:33 | INFO | train_inner | epoch 053:    174 / 407 loss=5.508, ppl=45.5, wps=43710.2, ups=0.67, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.502, loss_scale=32, train_wall=145, gb_free=21, wall=33071
2022-03-17 18:34:03 | INFO | train_inner | epoch 053:    274 / 407 loss=5.511, ppl=45.62, wps=43684.8, ups=0.67, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.501, loss_scale=32, train_wall=145, gb_free=21, wall=33221
2022-03-17 18:35:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 18:36:35 | INFO | train_inner | epoch 053:    375 / 407 loss=5.533, ppl=46.3, wps=43284.8, ups=0.66, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.503, loss_scale=16, train_wall=147, gb_free=21, wall=33372
2022-03-17 18:37:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 18:37:41 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 5.75 | ppl 53.83 | wps 70586.7 | wpb 2047.5 | bsz 4 | num_updates 21532 | best_loss 5.75
2022-03-17 18:37:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 21532 updates
2022-03-17 18:37:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 18:37:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 18:37:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 53 @ 21532 updates, score 5.75) (writing took 1.8108046185225248 seconds)
2022-03-17 18:37:43 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-17 18:37:43 | INFO | train | epoch 053 | loss 5.513 | ppl 45.66 | wps 42152.8 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 21532 | lr 0.000215505 | gnorm 0.503 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 33441
KL Stats: Epoch 53 Divergences: Uniform: 4.829470749278162 Unigram: 4.235541368773722
2022-03-17 18:37:43 | INFO | fairseq.trainer | begin training epoch 54
2022-03-17 18:37:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 18:39:25 | INFO | train_inner | epoch 054:     68 / 407 loss=5.5, ppl=45.25, wps=38357.5, ups=0.59, wpb=65361.9, bsz=127.7, num_updates=21600, lr=0.000215166, gnorm=0.506, loss_scale=16, train_wall=145, gb_free=21, wall=33543
2022-03-17 18:41:55 | INFO | train_inner | epoch 054:    168 / 407 loss=5.497, ppl=45.15, wps=43717.9, ups=0.67, wpb=65534.2, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.503, loss_scale=16, train_wall=145, gb_free=21, wall=33693
2022-03-17 18:44:25 | INFO | train_inner | epoch 054:    268 / 407 loss=5.509, ppl=45.55, wps=43683.8, ups=0.67, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.502, loss_scale=16, train_wall=145, gb_free=21, wall=33843
2022-03-17 18:46:55 | INFO | train_inner | epoch 054:    368 / 407 loss=5.521, ppl=45.92, wps=43707.2, ups=0.67, wpb=65525.8, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.505, loss_scale=16, train_wall=145, gb_free=21, wall=33992
2022-03-17 18:47:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 18:48:12 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 5.746 | ppl 53.67 | wps 70446 | wpb 2047.5 | bsz 4 | num_updates 21939 | best_loss 5.746
2022-03-17 18:48:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21939 updates
2022-03-17 18:48:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 18:48:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 18:48:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 54 @ 21939 updates, score 5.746) (writing took 1.8566411947831511 seconds)
2022-03-17 18:48:14 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-17 18:48:14 | INFO | train | epoch 054 | loss 5.506 | ppl 45.45 | wps 42256.6 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 21939 | lr 0.000213497 | gnorm 0.503 | loss_scale 16 | train_wall 592 | gb_free 21 | wall 34071
KL Stats: Epoch 54 Divergences: Uniform: 4.835454379145048 Unigram: 4.240792900477595
2022-03-17 18:48:14 | INFO | fairseq.trainer | begin training epoch 55
2022-03-17 18:48:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 18:49:45 | INFO | train_inner | epoch 055:     61 / 407 loss=5.495, ppl=45.09, wps=38351.7, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=22000, lr=0.000213201, gnorm=0.504, loss_scale=32, train_wall=145, gb_free=21, wall=34163
2022-03-17 18:49:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 18:52:17 | INFO | train_inner | epoch 055:    162 / 407 loss=5.484, ppl=44.77, wps=43283.1, ups=0.66, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.502, loss_scale=16, train_wall=147, gb_free=21, wall=34314
2022-03-17 18:54:47 | INFO | train_inner | epoch 055:    262 / 407 loss=5.507, ppl=45.47, wps=43687.6, ups=0.67, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.505, loss_scale=16, train_wall=145, gb_free=21, wall=34464
2022-03-17 18:57:17 | INFO | train_inner | epoch 055:    362 / 407 loss=5.515, ppl=45.73, wps=43732.8, ups=0.67, wpb=65523.9, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.503, loss_scale=16, train_wall=145, gb_free=21, wall=34614
2022-03-17 18:58:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 18:58:43 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 5.746 | ppl 53.68 | wps 70637.7 | wpb 2047.5 | bsz 4 | num_updates 22345 | best_loss 5.746
2022-03-17 18:58:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 22345 updates
2022-03-17 18:58:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 18:58:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 18:58:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 55 @ 22345 updates, score 5.746) (writing took 1.861540305428207 seconds)
2022-03-17 18:58:45 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-17 18:58:45 | INFO | train | epoch 055 | loss 5.5 | ppl 45.24 | wps 42160 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 22345 | lr 0.000211548 | gnorm 0.503 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 34702
KL Stats: Epoch 55 Divergences: Uniform: 4.8400167722686325 Unigram: 4.246913537836083
2022-03-17 18:58:45 | INFO | fairseq.trainer | begin training epoch 56
2022-03-17 18:58:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 19:00:07 | INFO | train_inner | epoch 056:     55 / 407 loss=5.483, ppl=44.74, wps=38357.1, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=22400, lr=0.000211289, gnorm=0.498, loss_scale=16, train_wall=145, gb_free=21, wall=34785
2022-03-17 19:02:37 | INFO | train_inner | epoch 056:    155 / 407 loss=5.471, ppl=44.37, wps=43722.4, ups=0.67, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.506, loss_scale=16, train_wall=145, gb_free=21, wall=34934
2022-03-17 19:05:07 | INFO | train_inner | epoch 056:    255 / 407 loss=5.501, ppl=45.3, wps=43696.3, ups=0.67, wpb=65523.9, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.503, loss_scale=32, train_wall=145, gb_free=21, wall=35084
2022-03-17 19:07:37 | INFO | train_inner | epoch 056:    355 / 407 loss=5.514, ppl=45.7, wps=43727.4, ups=0.67, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.501, loss_scale=32, train_wall=145, gb_free=21, wall=35234
2022-03-17 19:08:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 19:09:14 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 5.746 | ppl 53.69 | wps 70394.7 | wpb 2047.5 | bsz 4 | num_updates 22752 | best_loss 5.746
2022-03-17 19:09:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 22752 updates
2022-03-17 19:09:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 19:09:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 19:09:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 56 @ 22752 updates, score 5.746) (writing took 1.8387035205960274 seconds)
2022-03-17 19:09:15 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-17 19:09:15 | INFO | train | epoch 056 | loss 5.493 | ppl 45.05 | wps 42265.5 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 22752 | lr 0.000209648 | gnorm 0.504 | loss_scale 32 | train_wall 591 | gb_free 21 | wall 35333
KL Stats: Epoch 56 Divergences: Uniform: 4.846158528105528 Unigram: 4.251944267202474
2022-03-17 19:09:15 | INFO | fairseq.trainer | begin training epoch 57
2022-03-17 19:09:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 19:09:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 19:10:29 | INFO | train_inner | epoch 057:     49 / 407 loss=5.496, ppl=45.14, wps=38011.9, ups=0.58, wpb=65370.3, bsz=127.7, num_updates=22800, lr=0.000209427, gnorm=0.507, loss_scale=16, train_wall=147, gb_free=21, wall=35406
2022-03-17 19:12:59 | INFO | train_inner | epoch 057:    149 / 407 loss=5.472, ppl=44.38, wps=43734.2, ups=0.67, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.501, loss_scale=16, train_wall=145, gb_free=21, wall=35556
2022-03-17 19:15:29 | INFO | train_inner | epoch 057:    249 / 407 loss=5.484, ppl=44.74, wps=43713.9, ups=0.67, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.505, loss_scale=16, train_wall=145, gb_free=21, wall=35706
2022-03-17 19:17:58 | INFO | train_inner | epoch 057:    349 / 407 loss=5.503, ppl=45.34, wps=43865.9, ups=0.67, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.505, loss_scale=16, train_wall=145, gb_free=21, wall=35855
2022-03-17 19:19:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 19:19:43 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 5.737 | ppl 53.35 | wps 71021.1 | wpb 2047.5 | bsz 4 | num_updates 23158 | best_loss 5.737
2022-03-17 19:19:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 23158 updates
2022-03-17 19:19:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 19:19:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 19:19:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 57 @ 23158 updates, score 5.737) (writing took 1.853760153055191 seconds)
2022-03-17 19:19:45 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-17 19:19:45 | INFO | train | epoch 057 | loss 5.488 | ppl 44.87 | wps 42226.9 | ups 0.64 | wpb 65492.7 | bsz 127.9 | num_updates 23158 | lr 0.000207802 | gnorm 0.504 | loss_scale 16 | train_wall 591 | gb_free 21 | wall 35962
KL Stats: Epoch 57 Divergences: Uniform: 4.850326509791448 Unigram: 4.254890945979527
2022-03-17 19:19:45 | INFO | fairseq.trainer | begin training epoch 58
2022-03-17 19:19:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 19:20:48 | INFO | train_inner | epoch 058:     42 / 407 loss=5.487, ppl=44.84, wps=38476.3, ups=0.59, wpb=65361.9, bsz=127.7, num_updates=23200, lr=0.000207614, gnorm=0.507, loss_scale=16, train_wall=145, gb_free=21, wall=36025
2022-03-17 19:23:17 | INFO | train_inner | epoch 058:    142 / 407 loss=5.464, ppl=44.14, wps=43852.3, ups=0.67, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.507, loss_scale=32, train_wall=145, gb_free=21, wall=36175
2022-03-17 19:24:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 19:25:48 | INFO | train_inner | epoch 058:    243 / 407 loss=5.473, ppl=44.4, wps=43444.2, ups=0.66, wpb=65523.9, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.508, loss_scale=16, train_wall=146, gb_free=21, wall=36326
2022-03-17 19:28:18 | INFO | train_inner | epoch 058:    343 / 407 loss=5.497, ppl=45.16, wps=43883.8, ups=0.67, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.505, loss_scale=16, train_wall=145, gb_free=21, wall=36475
2022-03-17 19:29:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 19:30:12 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 5.735 | ppl 53.27 | wps 71044.1 | wpb 2047.5 | bsz 4 | num_updates 23564 | best_loss 5.735
2022-03-17 19:30:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 23564 updates
2022-03-17 19:30:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 19:30:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 19:30:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 58 @ 23564 updates, score 5.735) (writing took 1.8487997222691774 seconds)
2022-03-17 19:30:13 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-17 19:30:13 | INFO | train | epoch 058 | loss 5.481 | ppl 44.67 | wps 42310.8 | ups 0.65 | wpb 65492.7 | bsz 127.9 | num_updates 23564 | lr 0.000206004 | gnorm 0.508 | loss_scale 16 | train_wall 589 | gb_free 21 | wall 36591
KL Stats: Epoch 58 Divergences: Uniform: 4.856735192615153 Unigram: 4.261411476370751
2022-03-17 19:30:14 | INFO | fairseq.trainer | begin training epoch 59
2022-03-17 19:30:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 19:31:07 | INFO | train_inner | epoch 059:     36 / 407 loss=5.486, ppl=44.82, wps=38499.4, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=23600, lr=0.000205847, gnorm=0.51, loss_scale=16, train_wall=145, gb_free=21, wall=36645
2022-03-17 19:33:37 | INFO | train_inner | epoch 059:    136 / 407 loss=5.463, ppl=44.1, wps=43874.6, ups=0.67, wpb=65525.8, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.502, loss_scale=16, train_wall=145, gb_free=21, wall=36794
2022-03-17 19:36:06 | INFO | train_inner | epoch 059:    236 / 407 loss=5.475, ppl=44.49, wps=43850.6, ups=0.67, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.507, loss_scale=16, train_wall=145, gb_free=21, wall=36944
2022-03-17 19:38:36 | INFO | train_inner | epoch 059:    336 / 407 loss=5.489, ppl=44.91, wps=43873.2, ups=0.67, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.515, loss_scale=32, train_wall=145, gb_free=21, wall=37093
2022-03-17 19:40:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 19:40:40 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 5.725 | ppl 52.89 | wps 70984.2 | wpb 2047.5 | bsz 4 | num_updates 23971 | best_loss 5.725
2022-03-17 19:40:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 23971 updates
2022-03-17 19:40:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 19:40:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt
2022-03-17 19:40:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_best.pt (epoch 59 @ 23971 updates, score 5.725) (writing took 1.9062838787212968 seconds)
2022-03-17 19:40:42 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-17 19:40:42 | INFO | train | epoch 059 | loss 5.475 | ppl 44.49 | wps 42407.8 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 23971 | lr 0.000204248 | gnorm 0.508 | loss_scale 32 | train_wall 589 | gb_free 21 | wall 37219
KL Stats: Epoch 59 Divergences: Uniform: 4.860159917328679 Unigram: 4.2643464400254985
2022-03-17 19:40:42 | INFO | fairseq.trainer | begin training epoch 60
2022-03-17 19:40:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 19:41:25 | INFO | train_inner | epoch 060:     29 / 407 loss=5.483, ppl=44.74, wps=38463.8, ups=0.59, wpb=65368.5, bsz=127.7, num_updates=24000, lr=0.000204124, gnorm=0.511, loss_scale=32, train_wall=145, gb_free=21, wall=37263
2022-03-17 19:41:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-17 19:43:56 | INFO | train_inner | epoch 060:    130 / 407 loss=5.454, ppl=43.83, wps=43419.2, ups=0.66, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.509, loss_scale=16, train_wall=146, gb_free=21, wall=37414
2022-03-17 19:46:26 | INFO | train_inner | epoch 060:    230 / 407 loss=5.473, ppl=44.43, wps=43856.9, ups=0.67, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.507, loss_scale=16, train_wall=145, gb_free=21, wall=37563
2022-03-17 19:48:55 | INFO | train_inner | epoch 060:    330 / 407 loss=5.478, ppl=44.58, wps=43839.6, ups=0.67, wpb=65525.8, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.505, loss_scale=16, train_wall=145, gb_free=21, wall=37713
2022-03-17 19:50:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 19:51:09 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 5.732 | ppl 53.17 | wps 70903.7 | wpb 2047.5 | bsz 4 | num_updates 24377 | best_loss 5.725
2022-03-17 19:51:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 24377 updates
2022-03-17 19:51:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-17 19:51:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-17 19:51:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt (epoch 60 @ 24377 updates, score 5.732) (writing took 0.8752791974693537 seconds)
2022-03-17 19:51:10 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-17 19:51:10 | INFO | train | epoch 060 | loss 5.47 | ppl 44.33 | wps 42364.5 | ups 0.65 | wpb 65492.7 | bsz 127.9 | num_updates 24377 | lr 0.00020254 | gnorm 0.508 | loss_scale 16 | train_wall 589 | gb_free 21 | wall 37847
KL Stats: Epoch 60 Divergences: Uniform: 4.865444760490908 Unigram: 4.269983561958716
2022-03-17 19:51:10 | INFO | fairseq.trainer | begin training epoch 61
2022-03-17 19:51:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 19:51:44 | INFO | train_inner | epoch 061:     23 / 407 loss=5.467, ppl=44.24, wps=38722, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=24400, lr=0.000202444, gnorm=0.505, loss_scale=16, train_wall=145, gb_free=21, wall=37882
2022-03-17 19:54:13 | INFO | train_inner | epoch 061:    123 / 407 loss=5.441, ppl=43.43, wps=43878.1, ups=0.67, wpb=65525.8, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.511, loss_scale=16, train_wall=145, gb_free=21, wall=38031
2022-03-17 19:56:43 | INFO | train_inner | epoch 061:    223 / 407 loss=5.468, ppl=44.26, wps=43869.3, ups=0.67, wpb=65534.2, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.516, loss_scale=32, train_wall=145, gb_free=21, wall=38180
2022-03-17 19:59:12 | INFO | train_inner | epoch 061:    323 / 407 loss=5.473, ppl=44.41, wps=43870.5, ups=0.67, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.506, loss_scale=32, train_wall=145, gb_free=21, wall=38330
2022-03-17 20:01:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 20:01:36 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 5.728 | ppl 53.02 | wps 71022.6 | wpb 2047.5 | bsz 4 | num_updates 24784 | best_loss 5.725
2022-03-17 20:01:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 24784 updates
2022-03-17 20:01:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-17 20:01:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-17 20:01:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt (epoch 61 @ 24784 updates, score 5.728) (writing took 0.8715578503906727 seconds)
2022-03-17 20:01:37 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-17 20:01:37 | INFO | train | epoch 061 | loss 5.464 | ppl 44.15 | wps 42482.3 | ups 0.65 | wpb 65492.8 | bsz 127.9 | num_updates 24784 | lr 0.00020087 | gnorm 0.51 | loss_scale 32 | train_wall 589 | gb_free 21 | wall 38475
KL Stats: Epoch 61 Divergences: Uniform: 4.87125568349771 Unigram: 4.274866129158401
2022-03-17 20:01:37 | INFO | fairseq.trainer | begin training epoch 62
2022-03-17 20:01:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-17 20:02:01 | INFO | train_inner | epoch 062:     16 / 407 loss=5.484, ppl=44.76, wps=38709.9, ups=0.59, wpb=65372.2, bsz=127.7, num_updates=24800, lr=0.000200805, gnorm=0.508, loss_scale=32, train_wall=145, gb_free=21, wall=38499
2022-03-17 20:04:30 | INFO | train_inner | epoch 062:    116 / 407 loss=5.437, ppl=43.32, wps=43889.7, ups=0.67, wpb=65525.8, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.509, loss_scale=32, train_wall=145, gb_free=21, wall=38648
2022-03-17 20:07:00 | INFO | train_inner | epoch 062:    216 / 407 loss=5.462, ppl=44.07, wps=43865.4, ups=0.67, wpb=65534.2, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.513, loss_scale=32, train_wall=145, gb_free=21, wall=38797
2022-03-17 20:07:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-17 20:09:31 | INFO | train_inner | epoch 062:    317 / 407 loss=5.467, ppl=44.22, wps=43451.7, ups=0.66, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.514, loss_scale=32, train_wall=146, gb_free=21, wall=38948
2022-03-17 20:11:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-17 20:12:04 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 5.728 | ppl 52.99 | wps 70821.4 | wpb 2047.5 | bsz 4 | num_updates 25190 | best_loss 5.725
2022-03-17 20:12:04 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-17 20:12:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 25190 updates
2022-03-17 20:12:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-17 20:12:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt
2022-03-17 20:12:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/ru_dropout_0.3_#1_jelinek_0.02_0.005_0.975/checkpoint_last.pt (epoch 62 @ 25190 updates, score 5.728) (writing took 0.8860532054677606 seconds)
2022-03-17 20:12:05 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-17 20:12:05 | INFO | train | epoch 062 | loss 5.459 | ppl 44 | wps 42382.1 | ups 0.65 | wpb 65492.7 | bsz 127.9 | num_updates 25190 | lr 0.000199244 | gnorm 0.511 | loss_scale 32 | train_wall 589 | gb_free 21 | wall 39102
2022-03-17 20:12:05 | INFO | fairseq_cli.train | done training in 39101.9 seconds
