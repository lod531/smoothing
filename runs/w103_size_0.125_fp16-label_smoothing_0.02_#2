Sender: LSF System <lsfadmin@eu-g3-059>
Subject: Job 207345371: <w103_size_0.125_fp16_label_smoothing_0.02_#2> in cluster <euler> Exited

Job <w103_size_0.125_fp16_label_smoothing_0.02_#2> was submitted from host <eu-login-10> by user <andriusb> in cluster <euler> at Sun Mar  6 12:41:06 2022
Job was executed on host(s) <eu-g3-059>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Sun Mar  6 12:41:16 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Sun Mar  6 12:41:16 2022
Terminated at Tue Mar  8 06:20:46 2022
Results reported at Tue Mar  8 06:20:46 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.02 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575612 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   149893.23 sec.
    Max Memory :                                 6809 MB
    Average Memory :                             3743.46 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               13191.00 MB
    Max Swap :                                   2 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   149969 sec.
    Turnaround time :                            149980 sec.

The output (if any) follows:

2022-03-06 12:41:24 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575612, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575612, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.02, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-06 12:41:24 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-06 12:41:27 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-06 12:41:27 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-06 12:41:27 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-06 12:41:27 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-06 12:41:27 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-06 12:41:27 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-06 12:41:27 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-06 12:41:29 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-06 12:41:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 12:41:29 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-06 12:41:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 12:41:29 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-06 12:41:29 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-06 12:41:29 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 12:41:29 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 12:41:29 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-06 12:41:30 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-06 12:41:30 | INFO | fairseq.trainer | begin training epoch 1
2022-03-06 12:41:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 12:41:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-06 12:41:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 12:41:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 12:41:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 12:42:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-06 12:47:21 | INFO | train_inner | epoch 001:    105 / 196 loss=16.447, nll_loss=16.408, ppl=86926.8, wps=20959.7, ups=0.32, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.551, loss_scale=4, train_wall=327, gb_free=19.9, wall=352
2022-03-06 12:52:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 12:52:10 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.115 | nll_loss 13.006 | ppl 8223.37 | wps 40673.7 | wpb 510.9 | bsz 1 | num_updates 191
2022-03-06 12:52:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-06 12:52:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 12:52:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 12:52:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.115) (writing took 6.712473631137982 seconds)
2022-03-06 12:52:17 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-06 12:52:17 | INFO | train | epoch 001 | loss 15.356 | nll_loss 15.294 | ppl 40188.5 | wps 20540.4 | ups 0.31 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.573 | loss_scale 8 | train_wall 591 | gb_free 19.9 | wall 647
2022-03-06 12:52:17 | INFO | fairseq.trainer | begin training epoch 2
2022-03-06 12:52:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 12:52:45 | INFO | train_inner | epoch 002:      9 / 196 loss=14.065, nll_loss=13.977, ppl=16121.2, wps=20166.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.476, loss_scale=8, train_wall=290, gb_free=19.9, wall=676
2022-03-06 12:57:58 | INFO | train_inner | epoch 002:    109 / 196 loss=12.143, nll_loss=12.009, ppl=4122.64, wps=20977.1, ups=0.32, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.966, loss_scale=16, train_wall=290, gb_free=19.9, wall=988
2022-03-06 13:02:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:02:34 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.472 | nll_loss 10.278 | ppl 1241.79 | wps 40681.9 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 10.472
2022-03-06 13:02:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-06 13:02:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 13:02:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 13:02:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 2 @ 387 updates, score 10.472) (writing took 6.830683910986409 seconds)
2022-03-06 13:02:41 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-06 13:02:41 | INFO | train | epoch 002 | loss 11.628 | nll_loss 11.477 | ppl 2850.66 | wps 20558.1 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.813 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 1271
2022-03-06 13:02:41 | INFO | fairseq.trainer | begin training epoch 3
2022-03-06 13:02:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:03:22 | INFO | train_inner | epoch 003:     13 / 196 loss=10.825, nll_loss=10.648, ppl=1604.38, wps=20170, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.577, loss_scale=32, train_wall=289, gb_free=19.9, wall=1312
2022-03-06 13:08:34 | INFO | train_inner | epoch 003:    113 / 196 loss=10.259, nll_loss=10.051, ppl=1061.08, wps=20969.3, ups=0.32, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.519, loss_scale=32, train_wall=290, gb_free=19.9, wall=1625
2022-03-06 13:09:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:12:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:12:58 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.784 | nll_loss 9.554 | ppl 751.44 | wps 40918.5 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 9.784
2022-03-06 13:12:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 582 updates
2022-03-06 13:12:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 13:13:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 13:13:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 3 @ 582 updates, score 9.784) (writing took 6.805658101802692 seconds)
2022-03-06 13:13:05 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-06 13:13:05 | INFO | train | epoch 003 | loss 10.152 | nll_loss 9.94 | ppl 982.02 | wps 20445.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 582 | lr 7.28355e-05 | gnorm 0.532 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 1896
2022-03-06 13:13:05 | INFO | fairseq.trainer | begin training epoch 4
2022-03-06 13:13:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:14:01 | INFO | train_inner | epoch 004:     18 / 196 loss=9.935, nll_loss=9.711, ppl=838.4, wps=19968.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.578, loss_scale=32, train_wall=292, gb_free=19.9, wall=1952
2022-03-06 13:14:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 13:19:17 | INFO | train_inner | epoch 004:    119 / 196 loss=9.638, nll_loss=9.403, ppl=677.02, wps=20769.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.694, loss_scale=16, train_wall=293, gb_free=19.9, wall=2268
2022-03-06 13:23:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:23:22 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.315 | nll_loss 9.066 | ppl 535.96 | wps 40516.5 | wpb 510.9 | bsz 1 | num_updates 777 | best_loss 9.315
2022-03-06 13:23:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 777 updates
2022-03-06 13:23:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 13:23:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 13:23:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 4 @ 777 updates, score 9.315) (writing took 6.9237324250862 seconds)
2022-03-06 13:23:29 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-06 13:23:29 | INFO | train | epoch 004 | loss 9.57 | nll_loss 9.333 | ppl 644.97 | wps 20448.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 777 | lr 9.72056e-05 | gnorm 0.716 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 2520
2022-03-06 13:23:29 | INFO | fairseq.trainer | begin training epoch 5
2022-03-06 13:23:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:24:41 | INFO | train_inner | epoch 005:     23 / 196 loss=9.394, nll_loss=9.15, ppl=568.21, wps=20162.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.732, loss_scale=32, train_wall=289, gb_free=19.9, wall=2592
2022-03-06 13:28:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:29:57 | INFO | train_inner | epoch 005:    124 / 196 loss=9.163, nll_loss=8.91, ppl=481.15, wps=20751.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.841, loss_scale=32, train_wall=293, gb_free=19.9, wall=2908
2022-03-06 13:33:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:33:47 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.918 | nll_loss 8.655 | ppl 403.12 | wps 40680.8 | wpb 510.9 | bsz 1 | num_updates 972 | best_loss 8.918
2022-03-06 13:33:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 972 updates
2022-03-06 13:33:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 13:33:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 13:33:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 5 @ 972 updates, score 8.918) (writing took 6.903217667946592 seconds)
2022-03-06 13:33:54 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-06 13:33:54 | INFO | train | epoch 005 | loss 9.111 | nll_loss 8.857 | ppl 463.56 | wps 20437.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 972 | lr 0.000121576 | gnorm 0.823 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 3144
2022-03-06 13:33:54 | INFO | fairseq.trainer | begin training epoch 6
2022-03-06 13:33:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:35:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:35:24 | INFO | train_inner | epoch 006:     29 / 196 loss=8.954, nll_loss=8.694, ppl=414.14, wps=19963.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.851, loss_scale=32, train_wall=292, gb_free=19.9, wall=3235
2022-03-06 13:40:37 | INFO | train_inner | epoch 006:    129 / 196 loss=8.757, nll_loss=8.49, ppl=359.58, wps=20950.1, ups=0.32, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.907, loss_scale=32, train_wall=290, gb_free=19.9, wall=3548
2022-03-06 13:42:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:44:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:44:11 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.593 | nll_loss 8.316 | ppl 318.68 | wps 40627.7 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 8.593
2022-03-06 13:44:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1166 updates
2022-03-06 13:44:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 13:44:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 13:44:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 6 @ 1166 updates, score 8.593) (writing took 6.844051475869492 seconds)
2022-03-06 13:44:18 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-06 13:44:18 | INFO | train | epoch 006 | loss 8.73 | nll_loss 8.462 | ppl 352.6 | wps 20328.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1166 | lr 0.000145821 | gnorm 0.902 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 3769
2022-03-06 13:44:18 | INFO | fairseq.trainer | begin training epoch 7
2022-03-06 13:44:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:46:05 | INFO | train_inner | epoch 007:     34 / 196 loss=8.59, nll_loss=8.317, ppl=318.92, wps=19964.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.912, loss_scale=32, train_wall=292, gb_free=19.9, wall=3875
2022-03-06 13:49:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:51:21 | INFO | train_inner | epoch 007:    135 / 196 loss=8.428, nll_loss=8.15, ppl=283.96, wps=20739, ups=0.32, wpb=65532.4, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.932, loss_scale=32, train_wall=293, gb_free=19.9, wall=4191
2022-03-06 13:54:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:54:36 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.336 | nll_loss 8.049 | ppl 264.84 | wps 40776.3 | wpb 510.9 | bsz 1 | num_updates 1361 | best_loss 8.336
2022-03-06 13:54:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1361 updates
2022-03-06 13:54:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 13:54:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 13:54:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 7 @ 1361 updates, score 8.336) (writing took 6.8951479298993945 seconds)
2022-03-06 13:54:43 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-06 13:54:43 | INFO | train | epoch 007 | loss 8.41 | nll_loss 8.132 | ppl 280.46 | wps 20420.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1361 | lr 0.000170191 | gnorm 0.919 | loss_scale 32 | train_wall 569 | gb_free 19.9 | wall 4394
2022-03-06 13:54:43 | INFO | fairseq.trainer | begin training epoch 8
2022-03-06 13:54:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:56:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:56:48 | INFO | train_inner | epoch 008:     40 / 196 loss=8.275, nll_loss=7.992, ppl=254.57, wps=19938, ups=0.31, wpb=65367, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.91, loss_scale=32, train_wall=293, gb_free=19.9, wall=4519
2022-03-06 14:02:01 | INFO | train_inner | epoch 008:    140 / 196 loss=8.13, nll_loss=7.842, ppl=229.51, wps=20941.8, ups=0.32, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.936, loss_scale=32, train_wall=290, gb_free=19.9, wall=4832
2022-03-06 14:03:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:04:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:05:01 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.1 | nll_loss 7.806 | ppl 223.87 | wps 40535.3 | wpb 510.9 | bsz 1 | num_updates 1555 | best_loss 8.1
2022-03-06 14:05:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1555 updates
2022-03-06 14:05:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 14:05:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 14:05:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 8 @ 1555 updates, score 8.1) (writing took 6.985079745994881 seconds)
2022-03-06 14:05:08 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-06 14:05:08 | INFO | train | epoch 008 | loss 8.123 | nll_loss 7.835 | ppl 228.27 | wps 20306.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1555 | lr 0.000194436 | gnorm 0.941 | loss_scale 32 | train_wall 569 | gb_free 19.9 | wall 5019
2022-03-06 14:05:08 | INFO | fairseq.trainer | begin training epoch 9
2022-03-06 14:05:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:07:29 | INFO | train_inner | epoch 009:     45 / 196 loss=7.995, nll_loss=7.702, ppl=208.26, wps=19935.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.969, loss_scale=32, train_wall=293, gb_free=19.9, wall=5160
2022-03-06 14:09:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:11:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 14:12:48 | INFO | train_inner | epoch 009:    147 / 196 loss=7.856, nll_loss=7.559, ppl=188.57, wps=20536.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.928, loss_scale=16, train_wall=296, gb_free=19.9, wall=5479
2022-03-06 14:15:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:15:26 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.894 | nll_loss 7.595 | ppl 193.34 | wps 40618.3 | wpb 510.9 | bsz 1 | num_updates 1749 | best_loss 7.894
2022-03-06 14:15:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1749 updates
2022-03-06 14:15:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 14:15:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 14:15:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 9 @ 1749 updates, score 7.894) (writing took 6.959848230937496 seconds)
2022-03-06 14:15:33 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-06 14:15:33 | INFO | train | epoch 009 | loss 7.855 | nll_loss 7.558 | ppl 188.4 | wps 20316.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1749 | lr 0.000218681 | gnorm 0.946 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 5644
2022-03-06 14:15:33 | INFO | fairseq.trainer | begin training epoch 10
2022-03-06 14:15:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:18:13 | INFO | train_inner | epoch 010:     51 / 196 loss=7.724, nll_loss=7.423, ppl=171.61, wps=20132.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.934, loss_scale=16, train_wall=290, gb_free=19.9, wall=5804
2022-03-06 14:23:26 | INFO | train_inner | epoch 010:    151 / 196 loss=7.6, nll_loss=7.295, ppl=157.01, wps=20939.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.935, loss_scale=32, train_wall=290, gb_free=19.9, wall=6117
2022-03-06 14:25:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:25:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:25:51 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.706 | nll_loss 7.398 | ppl 168.68 | wps 40844.6 | wpb 510.9 | bsz 1 | num_updates 1944 | best_loss 7.706
2022-03-06 14:25:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1944 updates
2022-03-06 14:25:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 14:25:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 14:25:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 10 @ 1944 updates, score 7.706) (writing took 6.860968881985173 seconds)
2022-03-06 14:25:58 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-06 14:25:58 | INFO | train | epoch 010 | loss 7.604 | nll_loss 7.298 | ppl 157.37 | wps 20420.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1944 | lr 0.000243051 | gnorm 0.923 | loss_scale 32 | train_wall 569 | gb_free 19.9 | wall 6269
2022-03-06 14:25:58 | INFO | fairseq.trainer | begin training epoch 11
2022-03-06 14:25:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:28:54 | INFO | train_inner | epoch 011:     56 / 196 loss=7.472, nll_loss=7.162, ppl=143.24, wps=19948.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.919, loss_scale=32, train_wall=293, gb_free=19.9, wall=6444
2022-03-06 14:32:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:34:10 | INFO | train_inner | epoch 011:    157 / 196 loss=7.366, nll_loss=7.052, ppl=132.71, wps=20737.3, ups=0.32, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.901, loss_scale=32, train_wall=293, gb_free=19.9, wall=6760
2022-03-06 14:36:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:36:16 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.565 | nll_loss 7.248 | ppl 152.06 | wps 40683.1 | wpb 510.9 | bsz 1 | num_updates 2139 | best_loss 7.565
2022-03-06 14:36:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2139 updates
2022-03-06 14:36:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 14:36:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 14:36:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 11 @ 2139 updates, score 7.565) (writing took 6.834477900993079 seconds)
2022-03-06 14:36:23 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-06 14:36:23 | INFO | train | epoch 011 | loss 7.369 | nll_loss 7.056 | ppl 133.04 | wps 20419.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2139 | lr 0.000267422 | gnorm 0.912 | loss_scale 32 | train_wall 569 | gb_free 19.9 | wall 6894
2022-03-06 14:36:23 | INFO | fairseq.trainer | begin training epoch 12
2022-03-06 14:36:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:39:34 | INFO | train_inner | epoch 012:     61 / 196 loss=7.235, nll_loss=6.917, ppl=120.81, wps=20135.1, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.896, loss_scale=32, train_wall=290, gb_free=19.9, wall=7085
2022-03-06 14:40:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:44:50 | INFO | train_inner | epoch 012:    162 / 196 loss=7.142, nll_loss=6.821, ppl=113.03, wps=20743.7, ups=0.32, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.892, loss_scale=32, train_wall=293, gb_free=19.9, wall=7401
2022-03-06 14:46:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:46:41 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.437 | nll_loss 7.121 | ppl 139.16 | wps 40807.7 | wpb 510.9 | bsz 1 | num_updates 2334 | best_loss 7.437
2022-03-06 14:46:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2334 updates
2022-03-06 14:46:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 14:46:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 14:46:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 12 @ 2334 updates, score 7.437) (writing took 6.851993020158261 seconds)
2022-03-06 14:46:48 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-06 14:46:48 | INFO | train | epoch 012 | loss 7.151 | nll_loss 6.83 | ppl 113.76 | wps 20424.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2334 | lr 0.000291792 | gnorm 0.89 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 7519
2022-03-06 14:46:48 | INFO | fairseq.trainer | begin training epoch 13
2022-03-06 14:46:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:47:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:50:18 | INFO | train_inner | epoch 013:     67 / 196 loss=7.019, nll_loss=6.693, ppl=103.47, wps=19945.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.881, loss_scale=32, train_wall=293, gb_free=19.9, wall=7729
2022-03-06 14:54:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:55:34 | INFO | train_inner | epoch 013:    168 / 196 loss=6.947, nll_loss=6.618, ppl=98.24, wps=20730.5, ups=0.32, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.881, loss_scale=32, train_wall=293, gb_free=19.9, wall=8045
2022-03-06 14:57:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:57:06 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.338 | nll_loss 7.015 | ppl 129.34 | wps 40906.7 | wpb 510.9 | bsz 1 | num_updates 2528 | best_loss 7.338
2022-03-06 14:57:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2528 updates
2022-03-06 14:57:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 14:57:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 14:57:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 13 @ 2528 updates, score 7.338) (writing took 6.8413418820127845 seconds)
2022-03-06 14:57:13 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-06 14:57:13 | INFO | train | epoch 013 | loss 6.95 | nll_loss 6.622 | ppl 98.51 | wps 20313.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 2528 | lr 0.000316037 | gnorm 0.871 | loss_scale 32 | train_wall 569 | gb_free 19.9 | wall 8144
2022-03-06 14:57:13 | INFO | fairseq.trainer | begin training epoch 14
2022-03-06 14:57:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:00:59 | INFO | train_inner | epoch 014:     72 / 196 loss=6.818, nll_loss=6.486, ppl=89.61, wps=20133.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.875, loss_scale=32, train_wall=290, gb_free=19.9, wall=8369
2022-03-06 15:02:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:02:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:06:18 | INFO | train_inner | epoch 014:    174 / 196 loss=6.759, nll_loss=6.424, ppl=85.86, wps=20533, ups=0.31, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.86, loss_scale=16, train_wall=296, gb_free=19.9, wall=8689
2022-03-06 15:07:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:07:31 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.244 | nll_loss 6.911 | ppl 120.33 | wps 40406.6 | wpb 510.9 | bsz 1 | num_updates 2722 | best_loss 7.244
2022-03-06 15:07:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2722 updates
2022-03-06 15:07:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 15:07:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 15:07:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 14 @ 2722 updates, score 7.244) (writing took 7.048874523025006 seconds)
2022-03-06 15:07:39 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-06 15:07:39 | INFO | train | epoch 014 | loss 6.768 | nll_loss 6.433 | ppl 86.42 | wps 20304.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 2722 | lr 0.000340282 | gnorm 0.866 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 8769
2022-03-06 15:07:39 | INFO | fairseq.trainer | begin training epoch 15
2022-03-06 15:07:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:11:43 | INFO | train_inner | epoch 015:     78 / 196 loss=6.64, nll_loss=6.3, ppl=78.82, wps=20117.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.841, loss_scale=32, train_wall=290, gb_free=19.9, wall=9013
2022-03-06 15:16:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:16:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:17:02 | INFO | train_inner | epoch 015:    180 / 196 loss=6.597, nll_loss=6.256, ppl=76.41, wps=20526.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.875, loss_scale=16, train_wall=296, gb_free=19.9, wall=9333
2022-03-06 15:17:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:17:57 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.163 | nll_loss 6.826 | ppl 113.42 | wps 40558.9 | wpb 510.9 | bsz 1 | num_updates 2916 | best_loss 7.163
2022-03-06 15:17:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2916 updates
2022-03-06 15:17:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 15:18:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 15:18:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 15 @ 2916 updates, score 7.163) (writing took 7.010699091013521 seconds)
2022-03-06 15:18:04 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-06 15:18:04 | INFO | train | epoch 015 | loss 6.6 | nll_loss 6.259 | ppl 76.57 | wps 20303.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 2916 | lr 0.000364527 | gnorm 0.854 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 9394
2022-03-06 15:18:04 | INFO | fairseq.trainer | begin training epoch 16
2022-03-06 15:18:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:22:27 | INFO | train_inner | epoch 016:     84 / 196 loss=6.456, nll_loss=6.11, ppl=69.08, wps=20135.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.808, loss_scale=16, train_wall=290, gb_free=19.9, wall=9657
2022-03-06 15:27:40 | INFO | train_inner | epoch 016:    184 / 196 loss=6.448, nll_loss=6.101, ppl=68.65, wps=20943.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.859, loss_scale=32, train_wall=290, gb_free=19.9, wall=9970
2022-03-06 15:28:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:28:22 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.141 | nll_loss 6.797 | ppl 111.23 | wps 40642.6 | wpb 510.9 | bsz 1 | num_updates 3112 | best_loss 7.141
2022-03-06 15:28:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3112 updates
2022-03-06 15:28:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 15:28:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 15:28:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 16 @ 3112 updates, score 7.141) (writing took 7.05317404307425 seconds)
2022-03-06 15:28:29 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-06 15:28:29 | INFO | train | epoch 016 | loss 6.44 | nll_loss 6.093 | ppl 68.26 | wps 20522.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 3112 | lr 0.000389022 | gnorm 0.837 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 10020
2022-03-06 15:28:29 | INFO | fairseq.trainer | begin training epoch 17
2022-03-06 15:28:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:30:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:33:08 | INFO | train_inner | epoch 017:     89 / 196 loss=6.297, nll_loss=5.945, ppl=61.6, wps=19936.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.836, loss_scale=32, train_wall=293, gb_free=19.9, wall=10298
2022-03-06 15:36:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:38:24 | INFO | train_inner | epoch 017:    190 / 196 loss=6.301, nll_loss=5.948, ppl=61.72, wps=20739.1, ups=0.32, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.84, loss_scale=16, train_wall=293, gb_free=19.9, wall=10614
2022-03-06 15:38:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:38:47 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.087 | nll_loss 6.748 | ppl 107.47 | wps 40394.1 | wpb 510.9 | bsz 1 | num_updates 3306 | best_loss 7.087
2022-03-06 15:38:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3306 updates
2022-03-06 15:38:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 15:38:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 15:38:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 17 @ 3306 updates, score 7.087) (writing took 6.962901233928278 seconds)
2022-03-06 15:38:54 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-06 15:38:54 | INFO | train | epoch 017 | loss 6.291 | nll_loss 5.939 | ppl 61.34 | wps 20313.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 3306 | lr 0.000413267 | gnorm 0.84 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 10645
2022-03-06 15:38:54 | INFO | fairseq.trainer | begin training epoch 18
2022-03-06 15:38:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:43:48 | INFO | train_inner | epoch 018:     94 / 196 loss=6.147, nll_loss=5.789, ppl=55.29, wps=20138.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.827, loss_scale=16, train_wall=290, gb_free=19.9, wall=10939
2022-03-06 15:48:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:49:04 | INFO | train_inner | epoch 018:    195 / 196 loss=6.171, nll_loss=5.813, ppl=56.22, wps=20738.1, ups=0.32, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.857, loss_scale=16, train_wall=293, gb_free=19.9, wall=11255
2022-03-06 15:49:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:49:12 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.101 | nll_loss 6.763 | ppl 108.63 | wps 39415.4 | wpb 510.9 | bsz 1 | num_updates 3501 | best_loss 7.087
2022-03-06 15:49:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3501 updates
2022-03-06 15:49:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 15:49:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 15:49:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 18 @ 3501 updates, score 7.101) (writing took 3.3973706108517945 seconds)
2022-03-06 15:49:15 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-06 15:49:15 | INFO | train | epoch 018 | loss 6.154 | nll_loss 5.795 | ppl 55.54 | wps 20534.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3501 | lr 0.000437637 | gnorm 0.843 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 11266
2022-03-06 15:49:16 | INFO | fairseq.trainer | begin training epoch 19
2022-03-06 15:49:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:54:25 | INFO | train_inner | epoch 019:     99 / 196 loss=6.002, nll_loss=5.638, ppl=49.81, wps=20344.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.817, loss_scale=16, train_wall=290, gb_free=19.9, wall=11576
2022-03-06 15:57:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:59:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:59:34 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.054 | nll_loss 6.709 | ppl 104.63 | wps 40258.6 | wpb 510.9 | bsz 1 | num_updates 3696 | best_loss 7.054
2022-03-06 15:59:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3696 updates
2022-03-06 15:59:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 15:59:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt
2022-03-06 15:59:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_best.pt (epoch 19 @ 3696 updates, score 7.054) (writing took 6.989686594111845 seconds)
2022-03-06 15:59:41 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-06 15:59:41 | INFO | train | epoch 019 | loss 6.021 | nll_loss 5.658 | ppl 50.5 | wps 20408.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3696 | lr 0.000462008 | gnorm 0.82 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 11891
2022-03-06 15:59:41 | INFO | fairseq.trainer | begin training epoch 20
2022-03-06 15:59:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:59:53 | INFO | train_inner | epoch 020:      4 / 196 loss=6.034, nll_loss=5.671, ppl=50.93, wps=19928, ups=0.3, wpb=65367, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.819, loss_scale=16, train_wall=293, gb_free=19.9, wall=11904
2022-03-06 16:05:07 | INFO | train_inner | epoch 020:    104 / 196 loss=5.87, nll_loss=5.501, ppl=45.29, wps=20918.2, ups=0.32, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.846, loss_scale=32, train_wall=291, gb_free=19.9, wall=12217
2022-03-06 16:09:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:09:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:09:59 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.116 | nll_loss 6.779 | ppl 109.83 | wps 39494 | wpb 510.9 | bsz 1 | num_updates 3891 | best_loss 7.054
2022-03-06 16:09:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3891 updates
2022-03-06 16:09:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 16:10:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 16:10:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 20 @ 3891 updates, score 7.116) (writing took 3.4892675671726465 seconds)
2022-03-06 16:10:03 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-06 16:10:03 | INFO | train | epoch 020 | loss 5.895 | nll_loss 5.527 | ppl 46.11 | wps 20515.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3891 | lr 0.000486378 | gnorm 0.819 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 12513
2022-03-06 16:10:03 | INFO | fairseq.trainer | begin training epoch 21
2022-03-06 16:10:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:10:31 | INFO | train_inner | epoch 021:      9 / 196 loss=5.909, nll_loss=5.541, ppl=46.56, wps=20151.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3900, lr=0.000487503, gnorm=0.81, loss_scale=16, train_wall=293, gb_free=19.9, wall=12542
2022-03-06 16:15:44 | INFO | train_inner | epoch 021:    109 / 196 loss=5.747, nll_loss=5.373, ppl=41.45, wps=20938.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.831, loss_scale=16, train_wall=290, gb_free=19.9, wall=12855
2022-03-06 16:18:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:20:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:20:21 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.109 | nll_loss 6.766 | ppl 108.8 | wps 39128.1 | wpb 510.9 | bsz 1 | num_updates 4086 | best_loss 7.054
2022-03-06 16:20:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4086 updates
2022-03-06 16:20:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 16:20:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 16:20:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 21 @ 4086 updates, score 7.109) (writing took 3.378734849160537 seconds)
2022-03-06 16:20:25 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-06 16:20:25 | INFO | train | epoch 021 | loss 5.775 | nll_loss 5.402 | ppl 42.29 | wps 20530.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4086 | lr 0.00049471 | gnorm 0.836 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 13135
2022-03-06 16:20:25 | INFO | fairseq.trainer | begin training epoch 22
2022-03-06 16:20:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:21:08 | INFO | train_inner | epoch 022:     14 / 196 loss=5.777, nll_loss=5.404, ppl=42.33, wps=20156.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.823, loss_scale=16, train_wall=293, gb_free=19.9, wall=13179
2022-03-06 16:26:21 | INFO | train_inner | epoch 022:    114 / 196 loss=5.629, nll_loss=5.251, ppl=38.08, wps=20950.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.805, loss_scale=32, train_wall=290, gb_free=19.9, wall=13492
2022-03-06 16:27:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:30:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:30:42 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.129 | nll_loss 6.776 | ppl 109.61 | wps 39454.8 | wpb 510.9 | bsz 1 | num_updates 4281 | best_loss 7.054
2022-03-06 16:30:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4281 updates
2022-03-06 16:30:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 16:30:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 16:30:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 22 @ 4281 updates, score 7.129) (writing took 3.420036382973194 seconds)
2022-03-06 16:30:46 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-06 16:30:46 | INFO | train | epoch 022 | loss 5.647 | nll_loss 5.269 | ppl 38.56 | wps 20539.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4281 | lr 0.000483312 | gnorm 0.797 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 13756
2022-03-06 16:30:46 | INFO | fairseq.trainer | begin training epoch 23
2022-03-06 16:30:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:31:45 | INFO | train_inner | epoch 023:     19 / 196 loss=5.643, nll_loss=5.264, ppl=38.42, wps=20162.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=4300, lr=0.000482243, gnorm=0.768, loss_scale=16, train_wall=292, gb_free=19.9, wall=13816
2022-03-06 16:36:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:37:01 | INFO | train_inner | epoch 023:    120 / 196 loss=5.508, nll_loss=5.125, ppl=34.89, wps=20732.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.817, loss_scale=16, train_wall=293, gb_free=19.9, wall=14132
2022-03-06 16:40:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:41:04 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.141 | nll_loss 6.788 | ppl 110.5 | wps 40212.8 | wpb 510.9 | bsz 1 | num_updates 4476 | best_loss 7.054
2022-03-06 16:41:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4476 updates
2022-03-06 16:41:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 16:41:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 16:41:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 23 @ 4476 updates, score 7.141) (writing took 3.3286654471885413 seconds)
2022-03-06 16:41:07 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-06 16:41:07 | INFO | train | epoch 023 | loss 5.526 | nll_loss 5.143 | ppl 35.33 | wps 20537.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4476 | lr 0.000472667 | gnorm 0.785 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 14378
2022-03-06 16:41:07 | INFO | fairseq.trainer | begin training epoch 24
2022-03-06 16:41:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:42:22 | INFO | train_inner | epoch 024:     24 / 196 loss=5.505, nll_loss=5.12, ppl=34.79, wps=20367.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.753, loss_scale=16, train_wall=290, gb_free=19.9, wall=14453
2022-03-06 16:47:35 | INFO | train_inner | epoch 024:    124 / 196 loss=5.396, nll_loss=5.008, ppl=32.18, wps=20955.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.785, loss_scale=32, train_wall=290, gb_free=19.9, wall=14766
2022-03-06 16:50:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 16:51:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:51:25 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.181 | nll_loss 6.817 | ppl 112.75 | wps 40638.7 | wpb 510.9 | bsz 1 | num_updates 4671 | best_loss 7.054
2022-03-06 16:51:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4671 updates
2022-03-06 16:51:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 16:51:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 16:51:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 24 @ 4671 updates, score 7.181) (writing took 3.399060247000307 seconds)
2022-03-06 16:51:28 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-06 16:51:28 | INFO | train | epoch 024 | loss 5.41 | nll_loss 5.022 | ppl 32.5 | wps 20546.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4671 | lr 0.000462695 | gnorm 0.771 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 14999
2022-03-06 16:51:28 | INFO | fairseq.trainer | begin training epoch 25
2022-03-06 16:51:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:52:59 | INFO | train_inner | epoch 025:     29 / 196 loss=5.394, nll_loss=5.006, ppl=32.12, wps=20168, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.758, loss_scale=32, train_wall=293, gb_free=19.9, wall=15090
2022-03-06 16:53:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:58:15 | INFO | train_inner | epoch 025:    130 / 196 loss=5.288, nll_loss=4.895, ppl=29.75, wps=20751.2, ups=0.32, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.771, loss_scale=16, train_wall=293, gb_free=19.9, wall=15406
2022-03-06 17:01:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:01:46 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.219 | nll_loss 6.868 | ppl 116.83 | wps 40570.6 | wpb 510.9 | bsz 1 | num_updates 4866 | best_loss 7.054
2022-03-06 17:01:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4866 updates
2022-03-06 17:01:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 17:01:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 17:01:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 25 @ 4866 updates, score 7.219) (writing took 3.6531768061686307 seconds)
2022-03-06 17:01:50 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-06 17:01:50 | INFO | train | epoch 025 | loss 5.303 | nll_loss 4.911 | ppl 30.08 | wps 20534.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4866 | lr 0.000453329 | gnorm 0.766 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 15620
2022-03-06 17:01:50 | INFO | fairseq.trainer | begin training epoch 26
2022-03-06 17:01:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:02:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:03:40 | INFO | train_inner | epoch 026:     35 / 196 loss=5.281, nll_loss=4.888, ppl=29.61, wps=20140.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.767, loss_scale=16, train_wall=293, gb_free=19.9, wall=15730
2022-03-06 17:08:53 | INFO | train_inner | epoch 026:    135 / 196 loss=5.198, nll_loss=4.801, ppl=27.87, wps=20914.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.782, loss_scale=32, train_wall=291, gb_free=19.9, wall=16043
2022-03-06 17:12:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:12:09 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.265 | nll_loss 6.907 | ppl 120.02 | wps 40512.1 | wpb 510.9 | bsz 1 | num_updates 5061 | best_loss 7.054
2022-03-06 17:12:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5061 updates
2022-03-06 17:12:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 17:12:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 17:12:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 26 @ 5061 updates, score 7.265) (writing took 3.4178987571503967 seconds)
2022-03-06 17:12:12 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-06 17:12:12 | INFO | train | epoch 026 | loss 5.202 | nll_loss 4.805 | ppl 27.96 | wps 20505.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5061 | lr 0.00044451 | gnorm 0.767 | loss_scale 32 | train_wall 569 | gb_free 19.9 | wall 16243
2022-03-06 17:12:12 | INFO | fairseq.trainer | begin training epoch 27
2022-03-06 17:12:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:12:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:14:17 | INFO | train_inner | epoch 027:     40 / 196 loss=5.164, nll_loss=4.765, ppl=27.19, wps=20139.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.76, loss_scale=16, train_wall=293, gb_free=19.9, wall=16368
2022-03-06 17:19:30 | INFO | train_inner | epoch 027:    140 / 196 loss=5.107, nll_loss=4.706, ppl=26.09, wps=20947.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.793, loss_scale=32, train_wall=290, gb_free=19.9, wall=16681
2022-03-06 17:19:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:22:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:22:30 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.298 | nll_loss 6.945 | ppl 123.19 | wps 40606.2 | wpb 510.9 | bsz 1 | num_updates 5255 | best_loss 7.054
2022-03-06 17:22:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5255 updates
2022-03-06 17:22:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 17:22:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 17:22:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 27 @ 5255 updates, score 7.298) (writing took 3.3710110769607127 seconds)
2022-03-06 17:22:34 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-06 17:22:34 | INFO | train | epoch 027 | loss 5.107 | nll_loss 4.706 | ppl 26.1 | wps 20434.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 5255 | lr 0.000436228 | gnorm 0.773 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 16864
2022-03-06 17:22:34 | INFO | fairseq.trainer | begin training epoch 28
2022-03-06 17:22:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:24:55 | INFO | train_inner | epoch 028:     45 / 196 loss=5.064, nll_loss=4.661, ppl=25.3, wps=20161.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.765, loss_scale=16, train_wall=293, gb_free=19.9, wall=17005
2022-03-06 17:29:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:30:11 | INFO | train_inner | epoch 028:    146 / 196 loss=5.015, nll_loss=4.609, ppl=24.41, wps=20733, ups=0.32, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.77, loss_scale=16, train_wall=293, gb_free=19.9, wall=17321
2022-03-06 17:32:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:32:52 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.331 | nll_loss 6.973 | ppl 125.66 | wps 40985.4 | wpb 510.9 | bsz 1 | num_updates 5450 | best_loss 7.054
2022-03-06 17:32:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5450 updates
2022-03-06 17:32:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 17:32:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 17:32:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 28 @ 5450 updates, score 7.331) (writing took 3.4834709849674255 seconds)
2022-03-06 17:32:55 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-06 17:32:55 | INFO | train | epoch 028 | loss 5.018 | nll_loss 4.613 | ppl 24.47 | wps 20536.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5450 | lr 0.000428353 | gnorm 0.775 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 17486
2022-03-06 17:32:55 | INFO | fairseq.trainer | begin training epoch 29
2022-03-06 17:32:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:35:32 | INFO | train_inner | epoch 029:     50 / 196 loss=4.977, nll_loss=4.57, ppl=23.75, wps=20369.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.772, loss_scale=16, train_wall=290, gb_free=19.9, wall=17642
2022-03-06 17:38:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:40:47 | INFO | train_inner | epoch 029:    151 / 196 loss=4.934, nll_loss=4.524, ppl=23.01, wps=20748.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.779, loss_scale=16, train_wall=293, gb_free=19.9, wall=17958
2022-03-06 17:43:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:43:13 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.409 | nll_loss 7.051 | ppl 132.58 | wps 40668.5 | wpb 510.9 | bsz 1 | num_updates 5645 | best_loss 7.054
2022-03-06 17:43:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5645 updates
2022-03-06 17:43:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 17:43:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 17:43:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 29 @ 5645 updates, score 7.409) (writing took 3.3939550509676337 seconds)
2022-03-06 17:43:16 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-06 17:43:16 | INFO | train | epoch 029 | loss 4.932 | nll_loss 4.523 | ppl 23 | wps 20548.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5645 | lr 0.000420889 | gnorm 0.777 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 18107
2022-03-06 17:43:16 | INFO | fairseq.trainer | begin training epoch 30
2022-03-06 17:43:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:46:08 | INFO | train_inner | epoch 030:     55 / 196 loss=4.886, nll_loss=4.475, ppl=22.24, wps=20367.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.773, loss_scale=32, train_wall=290, gb_free=19.9, wall=18279
2022-03-06 17:51:21 | INFO | train_inner | epoch 030:    155 / 196 loss=4.86, nll_loss=4.447, ppl=21.81, wps=20932.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.787, loss_scale=32, train_wall=291, gb_free=19.9, wall=18592
2022-03-06 17:51:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 17:52:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:53:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:53:34 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.454 | nll_loss 7.095 | ppl 136.68 | wps 40557.9 | wpb 510.9 | bsz 1 | num_updates 5839 | best_loss 7.054
2022-03-06 17:53:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5839 updates
2022-03-06 17:53:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 17:53:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 17:53:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 30 @ 5839 updates, score 7.454) (writing took 3.419566380092874 seconds)
2022-03-06 17:53:38 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-06 17:53:38 | INFO | train | epoch 030 | loss 4.852 | nll_loss 4.439 | ppl 21.69 | wps 20426.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 5839 | lr 0.000413838 | gnorm 0.783 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 18728
2022-03-06 17:53:38 | INFO | fairseq.trainer | begin training epoch 31
2022-03-06 17:53:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:56:49 | INFO | train_inner | epoch 031:     61 / 196 loss=4.788, nll_loss=4.373, ppl=20.72, wps=19970.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.787, loss_scale=16, train_wall=296, gb_free=19.9, wall=18919
2022-03-06 17:59:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:02:05 | INFO | train_inner | epoch 031:    162 / 196 loss=4.794, nll_loss=4.378, ppl=20.79, wps=20745.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.795, loss_scale=16, train_wall=293, gb_free=19.9, wall=19235
2022-03-06 18:03:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:03:56 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.487 | nll_loss 7.123 | ppl 139.37 | wps 40564.5 | wpb 510.9 | bsz 1 | num_updates 6034 | best_loss 7.054
2022-03-06 18:03:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6034 updates
2022-03-06 18:03:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 18:03:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 18:03:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 31 @ 6034 updates, score 7.487) (writing took 3.6159007251262665 seconds)
2022-03-06 18:03:59 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-06 18:03:59 | INFO | train | epoch 031 | loss 4.775 | nll_loss 4.358 | ppl 20.51 | wps 20528.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6034 | lr 0.000407096 | gnorm 0.794 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 19350
2022-03-06 18:03:59 | INFO | fairseq.trainer | begin training epoch 32
2022-03-06 18:03:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:07:26 | INFO | train_inner | epoch 032:     66 / 196 loss=4.705, nll_loss=4.286, ppl=19.51, wps=20309.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.789, loss_scale=32, train_wall=290, gb_free=19.9, wall=19557
2022-03-06 18:08:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:12:43 | INFO | train_inner | epoch 032:    167 / 196 loss=4.723, nll_loss=4.304, ppl=19.75, wps=20712.6, ups=0.32, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.788, loss_scale=16, train_wall=294, gb_free=19.9, wall=19873
2022-03-06 18:14:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:14:18 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.54 | nll_loss 7.181 | ppl 145.07 | wps 40789.2 | wpb 510.9 | bsz 1 | num_updates 6229 | best_loss 7.054
2022-03-06 18:14:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6229 updates
2022-03-06 18:14:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 18:14:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 18:14:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 32 @ 6229 updates, score 7.54) (writing took 3.2645339579321444 seconds)
2022-03-06 18:14:22 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-06 18:14:22 | INFO | train | epoch 032 | loss 4.701 | nll_loss 4.281 | ppl 19.45 | wps 20512.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6229 | lr 0.000400674 | gnorm 0.789 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 19972
2022-03-06 18:14:22 | INFO | fairseq.trainer | begin training epoch 33
2022-03-06 18:14:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:17:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:18:07 | INFO | train_inner | epoch 033:     72 / 196 loss=4.632, nll_loss=4.209, ppl=18.49, wps=20160.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.789, loss_scale=16, train_wall=293, gb_free=19.9, wall=20198
2022-03-06 18:23:20 | INFO | train_inner | epoch 033:    172 / 196 loss=4.658, nll_loss=4.236, ppl=18.84, wps=20939.4, ups=0.32, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.813, loss_scale=16, train_wall=290, gb_free=19.9, wall=20511
2022-03-06 18:24:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:24:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:24:40 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.589 | nll_loss 7.226 | ppl 149.67 | wps 40833.7 | wpb 510.9 | bsz 1 | num_updates 6423 | best_loss 7.054
2022-03-06 18:24:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6423 updates
2022-03-06 18:24:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 18:24:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 18:24:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 33 @ 6423 updates, score 7.589) (writing took 3.597171620000154 seconds)
2022-03-06 18:24:43 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-06 18:24:43 | INFO | train | epoch 033 | loss 4.631 | nll_loss 4.208 | ppl 18.48 | wps 20419.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 6423 | lr 0.000394576 | gnorm 0.795 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 20594
2022-03-06 18:24:43 | INFO | fairseq.trainer | begin training epoch 34
2022-03-06 18:24:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:28:44 | INFO | train_inner | epoch 034:     77 / 196 loss=4.549, nll_loss=4.123, ppl=17.42, wps=20145.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.788, loss_scale=16, train_wall=293, gb_free=19.9, wall=20835
2022-03-06 18:33:57 | INFO | train_inner | epoch 034:    177 / 196 loss=4.597, nll_loss=4.171, ppl=18.02, wps=20941.2, ups=0.32, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.814, loss_scale=32, train_wall=291, gb_free=19.9, wall=21148
2022-03-06 18:34:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:34:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:35:02 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.658 | nll_loss 7.297 | ppl 157.22 | wps 40809.1 | wpb 510.9 | bsz 1 | num_updates 6618 | best_loss 7.054
2022-03-06 18:35:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6618 updates
2022-03-06 18:35:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 18:35:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 18:35:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 34 @ 6618 updates, score 7.658) (writing took 3.368734481977299 seconds)
2022-03-06 18:35:05 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-06 18:35:05 | INFO | train | epoch 034 | loss 4.566 | nll_loss 4.139 | ppl 17.62 | wps 20534.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6618 | lr 0.00038872 | gnorm 0.802 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 21215
2022-03-06 18:35:05 | INFO | fairseq.trainer | begin training epoch 35
2022-03-06 18:35:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:39:22 | INFO | train_inner | epoch 035:     82 / 196 loss=4.48, nll_loss=4.05, ppl=16.56, wps=20166.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.794, loss_scale=16, train_wall=293, gb_free=19.9, wall=21472
2022-03-06 18:41:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:44:38 | INFO | train_inner | epoch 035:    183 / 196 loss=4.544, nll_loss=4.115, ppl=17.33, wps=20723.4, ups=0.32, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.789, loss_scale=16, train_wall=293, gb_free=19.9, wall=21788
2022-03-06 18:45:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:45:23 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.701 | nll_loss 7.339 | ppl 161.87 | wps 40794.7 | wpb 510.9 | bsz 1 | num_updates 6813 | best_loss 7.054
2022-03-06 18:45:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6813 updates
2022-03-06 18:45:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 18:45:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 18:45:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 35 @ 6813 updates, score 7.701) (writing took 3.7941080620512366 seconds)
2022-03-06 18:45:27 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-06 18:45:27 | INFO | train | epoch 035 | loss 4.503 | nll_loss 4.073 | ppl 16.83 | wps 20513.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6813 | lr 0.000383116 | gnorm 0.797 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 21838
2022-03-06 18:45:27 | INFO | fairseq.trainer | begin training epoch 36
2022-03-06 18:45:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:50:00 | INFO | train_inner | epoch 036:     87 / 196 loss=4.409, nll_loss=3.976, ppl=15.74, wps=20310, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.807, loss_scale=32, train_wall=290, gb_free=19.9, wall=22110
2022-03-06 18:50:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:55:16 | INFO | train_inner | epoch 036:    188 / 196 loss=4.488, nll_loss=4.056, ppl=16.64, wps=20742.4, ups=0.32, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.824, loss_scale=16, train_wall=293, gb_free=19.9, wall=22426
2022-03-06 18:55:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:55:45 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.745 | nll_loss 7.377 | ppl 166.22 | wps 40551.5 | wpb 510.9 | bsz 1 | num_updates 7008 | best_loss 7.054
2022-03-06 18:55:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7008 updates
2022-03-06 18:55:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 18:55:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 18:55:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 36 @ 7008 updates, score 7.745) (writing took 3.339783410774544 seconds)
2022-03-06 18:55:49 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-06 18:55:49 | INFO | train | epoch 036 | loss 4.442 | nll_loss 4.009 | ppl 16.11 | wps 20530.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7008 | lr 0.000377749 | gnorm 0.815 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 22459
2022-03-06 18:55:49 | INFO | fairseq.trainer | begin training epoch 37
2022-03-06 18:55:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:56:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:00:40 | INFO | train_inner | epoch 037:     93 / 196 loss=4.347, nll_loss=3.911, ppl=15.04, wps=20170.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.805, loss_scale=16, train_wall=292, gb_free=19.9, wall=22750
2022-03-06 19:04:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:05:55 | INFO | train_inner | epoch 037:    194 / 196 loss=4.426, nll_loss=3.991, ppl=15.91, wps=20752.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.828, loss_scale=16, train_wall=293, gb_free=19.9, wall=23066
2022-03-06 19:06:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:06:06 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.8 | nll_loss 7.433 | ppl 172.76 | wps 40643.7 | wpb 510.9 | bsz 1 | num_updates 7202 | best_loss 7.054
2022-03-06 19:06:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7202 updates
2022-03-06 19:06:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 19:06:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 19:06:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 37 @ 7202 updates, score 7.8) (writing took 3.3387995939701796 seconds)
2022-03-06 19:06:10 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-06 19:06:10 | INFO | train | epoch 037 | loss 4.382 | nll_loss 3.946 | ppl 15.42 | wps 20445.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7202 | lr 0.000372626 | gnorm 0.815 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 23080
2022-03-06 19:06:10 | INFO | fairseq.trainer | begin training epoch 38
2022-03-06 19:06:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:11:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:11:20 | INFO | train_inner | epoch 038:     99 / 196 loss=4.283, nll_loss=3.844, ppl=14.36, wps=20165.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.815, loss_scale=16, train_wall=293, gb_free=19.9, wall=23390
2022-03-06 19:16:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:16:28 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.859 | nll_loss 7.5 | ppl 180.97 | wps 40932.5 | wpb 510.9 | bsz 1 | num_updates 7397 | best_loss 7.054
2022-03-06 19:16:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7397 updates
2022-03-06 19:16:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 19:16:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 19:16:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 38 @ 7397 updates, score 7.859) (writing took 3.3907009621616453 seconds)
2022-03-06 19:16:31 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-06 19:16:31 | INFO | train | epoch 038 | loss 4.327 | nll_loss 3.889 | ppl 14.81 | wps 20541.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7397 | lr 0.000367682 | gnorm 0.82 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 23701
2022-03-06 19:16:31 | INFO | fairseq.trainer | begin training epoch 39
2022-03-06 19:16:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:16:40 | INFO | train_inner | epoch 039:      3 / 196 loss=4.369, nll_loss=3.932, ppl=15.26, wps=20372.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.827, loss_scale=16, train_wall=290, gb_free=19.9, wall=23711
2022-03-06 19:18:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:21:56 | INFO | train_inner | epoch 039:    104 / 196 loss=4.225, nll_loss=3.782, ppl=13.75, wps=20750.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.816, loss_scale=16, train_wall=293, gb_free=19.9, wall=24027
2022-03-06 19:25:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:26:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:26:48 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.922 | nll_loss 7.557 | ppl 188.26 | wps 40670.1 | wpb 510.9 | bsz 1 | num_updates 7591 | best_loss 7.054
2022-03-06 19:26:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7591 updates
2022-03-06 19:26:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 19:26:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 19:26:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 39 @ 7591 updates, score 7.922) (writing took 3.5647702189162374 seconds)
2022-03-06 19:26:52 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-06 19:26:52 | INFO | train | epoch 039 | loss 4.274 | nll_loss 3.832 | ppl 14.24 | wps 20441.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7591 | lr 0.000362953 | gnorm 0.819 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 24323
2022-03-06 19:26:52 | INFO | fairseq.trainer | begin training epoch 40
2022-03-06 19:26:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:27:20 | INFO | train_inner | epoch 040:      9 / 196 loss=4.31, nll_loss=3.87, ppl=14.62, wps=20169.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7600, lr=0.000362738, gnorm=0.819, loss_scale=16, train_wall=292, gb_free=19.9, wall=24351
2022-03-06 19:32:33 | INFO | train_inner | epoch 040:    109 / 196 loss=4.189, nll_loss=3.744, ppl=13.4, wps=20949.8, ups=0.32, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.855, loss_scale=32, train_wall=290, gb_free=19.9, wall=24664
2022-03-06 19:33:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:37:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:37:10 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.974 | nll_loss 7.608 | ppl 195.11 | wps 40691 | wpb 510.9 | bsz 1 | num_updates 7786 | best_loss 7.054
2022-03-06 19:37:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7786 updates
2022-03-06 19:37:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 19:37:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 19:37:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 40 @ 7786 updates, score 7.974) (writing took 3.64168188883923 seconds)
2022-03-06 19:37:14 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-06 19:37:14 | INFO | train | epoch 040 | loss 4.225 | nll_loss 3.781 | ppl 13.74 | wps 20516.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7786 | lr 0.000358379 | gnorm 0.852 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 24945
2022-03-06 19:37:14 | INFO | fairseq.trainer | begin training epoch 41
2022-03-06 19:37:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:37:58 | INFO | train_inner | epoch 041:     14 / 196 loss=4.252, nll_loss=3.809, ppl=14.01, wps=20117.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.847, loss_scale=16, train_wall=293, gb_free=19.9, wall=24989
2022-03-06 19:40:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:43:14 | INFO | train_inner | epoch 041:    115 / 196 loss=4.138, nll_loss=3.691, ppl=12.91, wps=20717, ups=0.32, wpb=65532.4, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.832, loss_scale=16, train_wall=293, gb_free=19.9, wall=25305
2022-03-06 19:47:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:47:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:47:32 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8 | nll_loss 7.639 | ppl 199.28 | wps 40516.4 | wpb 510.9 | bsz 1 | num_updates 7980 | best_loss 7.054
2022-03-06 19:47:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 7980 updates
2022-03-06 19:47:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 19:47:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 19:47:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 41 @ 7980 updates, score 8.0) (writing took 3.2999281741213053 seconds)
2022-03-06 19:47:36 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-06 19:47:36 | INFO | train | epoch 041 | loss 4.173 | nll_loss 3.726 | ppl 13.24 | wps 20429.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7980 | lr 0.000353996 | gnorm 0.836 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 25566
2022-03-06 19:47:36 | INFO | fairseq.trainer | begin training epoch 42
2022-03-06 19:47:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:48:38 | INFO | train_inner | epoch 042:     20 / 196 loss=4.193, nll_loss=3.747, ppl=13.42, wps=20185.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=8000, lr=0.000353553, gnorm=0.842, loss_scale=16, train_wall=292, gb_free=19.9, wall=25629
2022-03-06 19:53:51 | INFO | train_inner | epoch 042:    120 / 196 loss=4.1, nll_loss=3.651, ppl=12.56, wps=20948.8, ups=0.32, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.821, loss_scale=16, train_wall=290, gb_free=19.9, wall=25942
2022-03-06 19:54:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:57:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:57:53 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.063 | nll_loss 7.699 | ppl 207.86 | wps 40784.5 | wpb 510.9 | bsz 1 | num_updates 8175 | best_loss 7.054
2022-03-06 19:57:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8175 updates
2022-03-06 19:57:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 19:57:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 19:57:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 42 @ 8175 updates, score 8.063) (writing took 3.409960675984621 seconds)
2022-03-06 19:57:57 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-06 19:57:57 | INFO | train | epoch 042 | loss 4.127 | nll_loss 3.679 | ppl 12.81 | wps 20542.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 8175 | lr 0.000349749 | gnorm 0.83 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 26187
2022-03-06 19:57:57 | INFO | fairseq.trainer | begin training epoch 43
2022-03-06 19:57:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:59:15 | INFO | train_inner | epoch 043:     25 / 196 loss=4.139, nll_loss=3.691, ppl=12.91, wps=20166.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.841, loss_scale=16, train_wall=293, gb_free=19.9, wall=26266
2022-03-06 20:01:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:04:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:04:34 | INFO | train_inner | epoch 043:    127 / 196 loss=4.063, nll_loss=3.612, ppl=12.22, wps=20557.3, ups=0.31, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.839, loss_scale=8, train_wall=296, gb_free=19.9, wall=26585
2022-03-06 20:08:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:08:14 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.126 | nll_loss 7.762 | ppl 217.1 | wps 40612 | wpb 510.9 | bsz 1 | num_updates 8369 | best_loss 7.054
2022-03-06 20:08:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8369 updates
2022-03-06 20:08:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 20:08:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 20:08:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 43 @ 8369 updates, score 8.126) (writing took 3.4091174190398306 seconds)
2022-03-06 20:08:18 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-06 20:08:18 | INFO | train | epoch 043 | loss 4.081 | nll_loss 3.63 | ppl 12.38 | wps 20451.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 8369 | lr 0.000345671 | gnorm 0.845 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 26808
2022-03-06 20:08:18 | INFO | fairseq.trainer | begin training epoch 44
2022-03-06 20:08:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:09:55 | INFO | train_inner | epoch 044:     31 / 196 loss=4.078, nll_loss=3.626, ppl=12.35, wps=20384, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=8400, lr=0.000345033, gnorm=0.857, loss_scale=8, train_wall=289, gb_free=19.9, wall=26905
2022-03-06 20:15:07 | INFO | train_inner | epoch 044:    131 / 196 loss=4.026, nll_loss=3.572, ppl=11.89, wps=20965.5, ups=0.32, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.864, loss_scale=16, train_wall=290, gb_free=19.9, wall=27218
2022-03-06 20:18:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:18:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:18:35 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.129 | nll_loss 7.765 | ppl 217.59 | wps 40699.8 | wpb 510.9 | bsz 1 | num_updates 8564 | best_loss 7.054
2022-03-06 20:18:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8564 updates
2022-03-06 20:18:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 20:18:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 20:18:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 44 @ 8564 updates, score 8.129) (writing took 3.517626760993153 seconds)
2022-03-06 20:18:39 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-06 20:18:39 | INFO | train | epoch 044 | loss 4.037 | nll_loss 3.584 | ppl 11.99 | wps 20549.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 8564 | lr 0.000341713 | gnorm 0.863 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 27429
2022-03-06 20:18:39 | INFO | fairseq.trainer | begin training epoch 45
2022-03-06 20:18:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:20:32 | INFO | train_inner | epoch 045:     36 / 196 loss=4.032, nll_loss=3.578, ppl=11.94, wps=20156.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.863, loss_scale=16, train_wall=293, gb_free=19.9, wall=27542
2022-03-06 20:25:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:25:47 | INFO | train_inner | epoch 045:    137 / 196 loss=3.986, nll_loss=3.531, ppl=11.56, wps=20744.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.853, loss_scale=16, train_wall=293, gb_free=19.9, wall=27858
2022-03-06 20:28:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:28:57 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.225 | nll_loss 7.859 | ppl 232.19 | wps 40763 | wpb 510.9 | bsz 1 | num_updates 8759 | best_loss 7.054
2022-03-06 20:28:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8759 updates
2022-03-06 20:28:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 20:29:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 20:29:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 45 @ 8759 updates, score 8.225) (writing took 3.5422401381656528 seconds)
2022-03-06 20:29:00 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-06 20:29:00 | INFO | train | epoch 045 | loss 3.995 | nll_loss 3.54 | ppl 11.63 | wps 20537.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 8759 | lr 0.000337888 | gnorm 0.864 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 28051
2022-03-06 20:29:00 | INFO | fairseq.trainer | begin training epoch 46
2022-03-06 20:29:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:31:09 | INFO | train_inner | epoch 046:     41 / 196 loss=3.989, nll_loss=3.533, ppl=11.58, wps=20352.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=8800, lr=0.0003371, gnorm=0.873, loss_scale=16, train_wall=290, gb_free=19.9, wall=28179
2022-03-06 20:32:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:36:25 | INFO | train_inner | epoch 046:    142 / 196 loss=3.947, nll_loss=3.489, ppl=11.23, wps=20741.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.86, loss_scale=16, train_wall=293, gb_free=19.9, wall=28495
2022-03-06 20:39:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:39:18 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.265 | nll_loss 7.904 | ppl 239.45 | wps 40834.8 | wpb 510.9 | bsz 1 | num_updates 8954 | best_loss 7.054
2022-03-06 20:39:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8954 updates
2022-03-06 20:39:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 20:39:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 20:39:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 46 @ 8954 updates, score 8.265) (writing took 3.794910311931744 seconds)
2022-03-06 20:39:22 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-06 20:39:22 | INFO | train | epoch 046 | loss 3.954 | nll_loss 3.497 | ppl 11.29 | wps 20522.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 8954 | lr 0.000334188 | gnorm 0.86 | loss_scale 32 | train_wall 569 | gb_free 19.9 | wall 28673
2022-03-06 20:39:22 | INFO | fairseq.trainer | begin training epoch 47
2022-03-06 20:39:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:39:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:41:49 | INFO | train_inner | epoch 047:     47 / 196 loss=3.934, nll_loss=3.475, ppl=11.12, wps=20126.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.865, loss_scale=16, train_wall=293, gb_free=19.9, wall=28820
2022-03-06 20:46:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:47:06 | INFO | train_inner | epoch 047:    148 / 196 loss=3.919, nll_loss=3.459, ppl=11, wps=20711.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.878, loss_scale=16, train_wall=294, gb_free=19.9, wall=29136
2022-03-06 20:48:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:49:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:49:41 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.306 | nll_loss 7.944 | ppl 246.21 | wps 40675.2 | wpb 510.9 | bsz 1 | num_updates 9147 | best_loss 7.054
2022-03-06 20:49:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 9147 updates
2022-03-06 20:49:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 20:49:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 20:49:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 47 @ 9147 updates, score 8.306) (writing took 3.62204470904544 seconds)
2022-03-06 20:49:44 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-06 20:49:44 | INFO | train | epoch 047 | loss 3.916 | nll_loss 3.457 | ppl 10.98 | wps 20296.6 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 9147 | lr 0.000330644 | gnorm 0.881 | loss_scale 8 | train_wall 569 | gb_free 19.9 | wall 29295
2022-03-06 20:49:44 | INFO | fairseq.trainer | begin training epoch 48
2022-03-06 20:49:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:52:30 | INFO | train_inner | epoch 048:     53 / 196 loss=3.885, nll_loss=3.424, ppl=10.73, wps=20146.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=9200, lr=0.00032969, gnorm=0.885, loss_scale=8, train_wall=293, gb_free=19.9, wall=29461
2022-03-06 20:57:43 | INFO | train_inner | epoch 048:    153 / 196 loss=3.894, nll_loss=3.433, ppl=10.8, wps=20958.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.863, loss_scale=16, train_wall=290, gb_free=19.9, wall=29773
2022-03-06 20:58:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:59:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:00:02 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.377 | nll_loss 8.011 | ppl 257.98 | wps 40755.7 | wpb 510.9 | bsz 1 | num_updates 9342 | best_loss 7.054
2022-03-06 21:00:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 9342 updates
2022-03-06 21:00:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 21:00:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 21:00:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 48 @ 9342 updates, score 8.377) (writing took 3.4662333268206567 seconds)
2022-03-06 21:00:05 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-06 21:00:05 | INFO | train | epoch 048 | loss 3.878 | nll_loss 3.417 | ppl 10.68 | wps 20548.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 9342 | lr 0.000327175 | gnorm 0.876 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 29916
2022-03-06 21:00:05 | INFO | fairseq.trainer | begin training epoch 49
2022-03-06 21:00:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:03:07 | INFO | train_inner | epoch 049:     58 / 196 loss=3.844, nll_loss=3.382, ppl=10.42, wps=20174.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.9, loss_scale=8, train_wall=292, gb_free=19.9, wall=30097
2022-03-06 21:08:20 | INFO | train_inner | epoch 049:    158 / 196 loss=3.857, nll_loss=3.394, ppl=10.51, wps=20958.7, ups=0.32, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.874, loss_scale=16, train_wall=290, gb_free=19.9, wall=30410
2022-03-06 21:10:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:10:23 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.452 | nll_loss 8.085 | ppl 271.51 | wps 40752.7 | wpb 510.9 | bsz 1 | num_updates 9538 | best_loss 7.054
2022-03-06 21:10:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 9538 updates
2022-03-06 21:10:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 21:10:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 21:10:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 49 @ 9538 updates, score 8.452) (writing took 3.329743256093934 seconds)
2022-03-06 21:10:26 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-06 21:10:26 | INFO | train | epoch 049 | loss 3.841 | nll_loss 3.378 | ppl 10.4 | wps 20658.6 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 9538 | lr 0.000323796 | gnorm 0.876 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 30537
2022-03-06 21:10:26 | INFO | fairseq.trainer | begin training epoch 50
2022-03-06 21:10:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:12:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 21:13:44 | INFO | train_inner | epoch 050:     63 / 196 loss=3.806, nll_loss=3.341, ppl=10.13, wps=20173.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=9600, lr=0.000322749, gnorm=0.866, loss_scale=16, train_wall=293, gb_free=19.9, wall=30734
2022-03-06 21:18:56 | INFO | train_inner | epoch 050:    163 / 196 loss=3.826, nll_loss=3.361, ppl=10.28, wps=20958.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.896, loss_scale=16, train_wall=290, gb_free=19.9, wall=31047
2022-03-06 21:19:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 21:20:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:20:44 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.485 | nll_loss 8.121 | ppl 278.43 | wps 40596 | wpb 510.9 | bsz 1 | num_updates 9732 | best_loss 7.054
2022-03-06 21:20:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 9732 updates
2022-03-06 21:20:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 21:20:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 21:20:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 50 @ 9732 updates, score 8.485) (writing took 3.404065133072436 seconds)
2022-03-06 21:20:48 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-06 21:20:48 | INFO | train | epoch 050 | loss 3.807 | nll_loss 3.342 | ppl 10.14 | wps 20439.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 9732 | lr 0.000320552 | gnorm 0.888 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 31158
2022-03-06 21:20:48 | INFO | fairseq.trainer | begin training epoch 51
2022-03-06 21:20:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:24:20 | INFO | train_inner | epoch 051:     68 / 196 loss=3.754, nll_loss=3.287, ppl=9.76, wps=20170.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.89, loss_scale=16, train_wall=292, gb_free=19.9, wall=31371
2022-03-06 21:26:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 21:29:36 | INFO | train_inner | epoch 051:    169 / 196 loss=3.803, nll_loss=3.337, ppl=10.1, wps=20754, ups=0.32, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.916, loss_scale=16, train_wall=293, gb_free=19.9, wall=31687
2022-03-06 21:31:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:31:05 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 8.514 | nll_loss 8.152 | ppl 284.45 | wps 40704.4 | wpb 510.9 | bsz 1 | num_updates 9927 | best_loss 7.054
2022-03-06 21:31:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 9927 updates
2022-03-06 21:31:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 21:31:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 21:31:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 51 @ 9927 updates, score 8.514) (writing took 3.453934296965599 seconds)
2022-03-06 21:31:09 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-06 21:31:09 | INFO | train | epoch 051 | loss 3.773 | nll_loss 3.306 | ppl 9.89 | wps 20545.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 9927 | lr 0.000317388 | gnorm 0.903 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 31779
2022-03-06 21:31:09 | INFO | fairseq.trainer | begin training epoch 52
2022-03-06 21:31:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:33:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 21:35:00 | INFO | train_inner | epoch 052:     74 / 196 loss=3.727, nll_loss=3.259, ppl=9.57, wps=20163.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10000, lr=0.000316228, gnorm=0.88, loss_scale=16, train_wall=293, gb_free=19.9, wall=32011
2022-03-06 21:36:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:40:16 | INFO | train_inner | epoch 052:    175 / 196 loss=3.77, nll_loss=3.303, ppl=9.87, wps=20761.2, ups=0.32, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.909, loss_scale=8, train_wall=293, gb_free=19.9, wall=32326
2022-03-06 21:41:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:41:26 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 8.601 | nll_loss 8.236 | ppl 301.6 | wps 40282.6 | wpb 510.9 | bsz 1 | num_updates 10121 | best_loss 7.054
2022-03-06 21:41:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 10121 updates
2022-03-06 21:41:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 21:41:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 21:41:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 52 @ 10121 updates, score 8.601) (writing took 3.4115963021758944 seconds)
2022-03-06 21:41:30 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-06 21:41:30 | INFO | train | epoch 052 | loss 3.739 | nll_loss 3.271 | ppl 9.65 | wps 20444.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 10121 | lr 0.000314332 | gnorm 0.896 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 32400
2022-03-06 21:41:30 | INFO | fairseq.trainer | begin training epoch 53
2022-03-06 21:41:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:45:37 | INFO | train_inner | epoch 053:     79 / 196 loss=3.682, nll_loss=3.211, ppl=9.26, wps=20340.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.889, loss_scale=16, train_wall=290, gb_free=19.9, wall=32648
2022-03-06 21:50:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 21:50:54 | INFO | train_inner | epoch 053:    180 / 196 loss=3.748, nll_loss=3.279, ppl=9.71, wps=20723.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.912, loss_scale=16, train_wall=293, gb_free=19.9, wall=32964
2022-03-06 21:51:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:51:48 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 8.621 | nll_loss 8.254 | ppl 305.37 | wps 40519.1 | wpb 510.9 | bsz 1 | num_updates 10316 | best_loss 7.054
2022-03-06 21:51:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 10316 updates
2022-03-06 21:51:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 21:51:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 21:51:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 53 @ 10316 updates, score 8.621) (writing took 3.8103770189918578 seconds)
2022-03-06 21:51:52 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-06 21:51:52 | INFO | train | epoch 053 | loss 3.708 | nll_loss 3.238 | ppl 9.44 | wps 20502.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10316 | lr 0.000311347 | gnorm 0.901 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 33023
2022-03-06 21:51:52 | INFO | fairseq.trainer | begin training epoch 54
2022-03-06 21:51:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:53:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:56:18 | INFO | train_inner | epoch 054:     85 / 196 loss=3.638, nll_loss=3.165, ppl=8.97, wps=20135.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=10400, lr=0.000310087, gnorm=0.895, loss_scale=8, train_wall=293, gb_free=19.9, wall=33289
2022-03-06 22:01:31 | INFO | train_inner | epoch 054:    185 / 196 loss=3.72, nll_loss=3.25, ppl=9.51, wps=20960.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.912, loss_scale=16, train_wall=290, gb_free=19.9, wall=33601
2022-03-06 22:02:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:02:10 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 8.69 | nll_loss 8.328 | ppl 321.35 | wps 40689.2 | wpb 510.9 | bsz 1 | num_updates 10511 | best_loss 7.054
2022-03-06 22:02:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 10511 updates
2022-03-06 22:02:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 22:02:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 22:02:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 54 @ 10511 updates, score 8.69) (writing took 3.3697310120332986 seconds)
2022-03-06 22:02:13 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-06 22:02:13 | INFO | train | epoch 054 | loss 3.677 | nll_loss 3.205 | ppl 9.22 | wps 20552 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10511 | lr 0.000308445 | gnorm 0.901 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 33644
2022-03-06 22:02:13 | INFO | fairseq.trainer | begin training epoch 55
2022-03-06 22:02:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:05:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:06:55 | INFO | train_inner | epoch 055:     90 / 196 loss=3.61, nll_loss=3.135, ppl=8.79, wps=20170.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.897, loss_scale=8, train_wall=293, gb_free=19.9, wall=33925
2022-03-06 22:12:07 | INFO | train_inner | epoch 055:    190 / 196 loss=3.695, nll_loss=3.224, ppl=9.34, wps=20962.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.919, loss_scale=16, train_wall=290, gb_free=19.9, wall=34238
2022-03-06 22:12:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:12:31 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 8.732 | nll_loss 8.365 | ppl 329.73 | wps 40755.4 | wpb 510.9 | bsz 1 | num_updates 10706 | best_loss 7.054
2022-03-06 22:12:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 10706 updates
2022-03-06 22:12:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 22:12:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 22:12:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 55 @ 10706 updates, score 8.732) (writing took 3.4632547218352556 seconds)
2022-03-06 22:12:34 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-06 22:12:34 | INFO | train | epoch 055 | loss 3.648 | nll_loss 3.175 | ppl 9.03 | wps 20545.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10706 | lr 0.000305623 | gnorm 0.912 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 34265
2022-03-06 22:12:34 | INFO | fairseq.trainer | begin training epoch 56
2022-03-06 22:12:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:17:28 | INFO | train_inner | epoch 056:     94 / 196 loss=3.574, nll_loss=3.098, ppl=8.56, wps=20363, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10800, lr=0.00030429, gnorm=0.907, loss_scale=16, train_wall=290, gb_free=19.9, wall=34559
2022-03-06 22:18:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 22:22:44 | INFO | train_inner | epoch 056:    195 / 196 loss=3.667, nll_loss=3.194, ppl=9.15, wps=20750.4, ups=0.32, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.926, loss_scale=16, train_wall=293, gb_free=19.9, wall=34875
2022-03-06 22:22:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:22:52 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 8.792 | nll_loss 8.427 | ppl 344.07 | wps 40469.7 | wpb 510.9 | bsz 1 | num_updates 10901 | best_loss 7.054
2022-03-06 22:22:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 10901 updates
2022-03-06 22:22:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 22:22:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 22:22:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 56 @ 10901 updates, score 8.792) (writing took 3.431722026085481 seconds)
2022-03-06 22:22:56 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-06 22:22:56 | INFO | train | epoch 056 | loss 3.618 | nll_loss 3.144 | ppl 8.84 | wps 20545.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10901 | lr 0.000302877 | gnorm 0.915 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 34886
2022-03-06 22:22:56 | INFO | fairseq.trainer | begin training epoch 57
2022-03-06 22:22:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:25:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 22:28:08 | INFO | train_inner | epoch 057:    100 / 196 loss=3.545, nll_loss=3.067, ppl=8.38, wps=20170.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=11000, lr=0.000301511, gnorm=0.9, loss_scale=16, train_wall=292, gb_free=19.9, wall=35199
2022-03-06 22:32:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 22:33:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:33:13 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 8.844 | nll_loss 8.477 | ppl 356.39 | wps 40575.1 | wpb 510.9 | bsz 1 | num_updates 11095 | best_loss 7.054
2022-03-06 22:33:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 11095 updates
2022-03-06 22:33:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 22:33:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 22:33:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 57 @ 11095 updates, score 8.844) (writing took 3.545925454935059 seconds)
2022-03-06 22:33:17 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-06 22:33:17 | INFO | train | epoch 057 | loss 3.589 | nll_loss 3.113 | ppl 8.65 | wps 20439.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 11095 | lr 0.000300218 | gnorm 0.907 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 35507
2022-03-06 22:33:17 | INFO | fairseq.trainer | begin training epoch 58
2022-03-06 22:33:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:33:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:33:36 | INFO | train_inner | epoch 058:      6 / 196 loss=3.629, nll_loss=3.155, ppl=8.91, wps=19978.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=11100, lr=0.00030015, gnorm=0.915, loss_scale=8, train_wall=295, gb_free=19.9, wall=35526
2022-03-06 22:38:48 | INFO | train_inner | epoch 058:    106 / 196 loss=3.523, nll_loss=3.043, ppl=8.24, wps=20972.2, ups=0.32, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.914, loss_scale=8, train_wall=290, gb_free=19.9, wall=35839
2022-03-06 22:40:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:43:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:43:34 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 8.867 | nll_loss 8.502 | ppl 362.61 | wps 40472.6 | wpb 510.9 | bsz 1 | num_updates 11289 | best_loss 7.054
2022-03-06 22:43:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 11289 updates
2022-03-06 22:43:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 22:43:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 22:43:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 58 @ 11289 updates, score 8.867) (writing took 3.56508443900384 seconds)
2022-03-06 22:43:38 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-06 22:43:38 | INFO | train | epoch 058 | loss 3.563 | nll_loss 3.086 | ppl 8.49 | wps 20448.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 11289 | lr 0.000297627 | gnorm 0.924 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 36128
2022-03-06 22:43:38 | INFO | fairseq.trainer | begin training epoch 59
2022-03-06 22:43:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:44:12 | INFO | train_inner | epoch 059:     11 / 196 loss=3.593, nll_loss=3.117, ppl=8.67, wps=20170.1, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=11300, lr=0.000297482, gnorm=0.929, loss_scale=8, train_wall=292, gb_free=19.9, wall=36163
2022-03-06 22:49:25 | INFO | train_inner | epoch 059:    111 / 196 loss=3.501, nll_loss=3.02, ppl=8.11, wps=20964.2, ups=0.32, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.926, loss_scale=16, train_wall=290, gb_free=19.9, wall=36475
2022-03-06 22:52:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:53:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:53:55 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 8.909 | nll_loss 8.547 | ppl 374.05 | wps 40765.7 | wpb 510.9 | bsz 1 | num_updates 11484 | best_loss 7.054
2022-03-06 22:53:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 11484 updates
2022-03-06 22:53:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 22:53:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 22:53:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 59 @ 11484 updates, score 8.909) (writing took 3.530168408062309 seconds)
2022-03-06 22:53:59 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-06 22:53:59 | INFO | train | epoch 059 | loss 3.538 | nll_loss 3.059 | ppl 8.33 | wps 20549.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11484 | lr 0.000295089 | gnorm 0.932 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 36749
2022-03-06 22:53:59 | INFO | fairseq.trainer | begin training epoch 60
2022-03-06 22:53:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:54:49 | INFO | train_inner | epoch 060:     16 / 196 loss=3.569, nll_loss=3.092, ppl=8.52, wps=20170.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=11500, lr=0.000294884, gnorm=0.94, loss_scale=8, train_wall=292, gb_free=19.9, wall=36799
2022-03-06 23:00:01 | INFO | train_inner | epoch 060:    116 / 196 loss=3.483, nll_loss=3.001, ppl=8.01, wps=20959.9, ups=0.32, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.927, loss_scale=16, train_wall=290, gb_free=19.9, wall=37112
2022-03-06 23:02:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:04:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:04:16 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 8.943 | nll_loss 8.579 | ppl 382.48 | wps 40579 | wpb 510.9 | bsz 1 | num_updates 11679 | best_loss 7.054
2022-03-06 23:04:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 11679 updates
2022-03-06 23:04:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 23:04:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 23:04:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 60 @ 11679 updates, score 8.943) (writing took 3.529181378893554 seconds)
2022-03-06 23:04:20 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-06 23:04:20 | INFO | train | epoch 060 | loss 3.511 | nll_loss 3.031 | ppl 8.17 | wps 20543.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11679 | lr 0.000292615 | gnorm 0.936 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 37370
2022-03-06 23:04:20 | INFO | fairseq.trainer | begin training epoch 61
2022-03-06 23:04:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:05:26 | INFO | train_inner | epoch 061:     21 / 196 loss=3.53, nll_loss=3.051, ppl=8.29, wps=20164.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=11700, lr=0.000292353, gnorm=0.952, loss_scale=8, train_wall=292, gb_free=19.9, wall=37436
2022-03-06 23:10:38 | INFO | train_inner | epoch 061:    121 / 196 loss=3.462, nll_loss=2.979, ppl=7.88, wps=20972, ups=0.32, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.927, loss_scale=16, train_wall=290, gb_free=19.9, wall=37749
2022-03-06 23:13:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:14:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:14:37 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 8.99 | nll_loss 8.629 | ppl 396.01 | wps 40118.5 | wpb 510.9 | bsz 1 | num_updates 11874 | best_loss 7.054
2022-03-06 23:14:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 11874 updates
2022-03-06 23:14:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 23:14:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 23:14:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 61 @ 11874 updates, score 8.99) (writing took 3.539519320940599 seconds)
2022-03-06 23:14:41 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-06 23:14:41 | INFO | train | epoch 061 | loss 3.487 | nll_loss 3.006 | ppl 8.03 | wps 20555.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11874 | lr 0.000290203 | gnorm 0.934 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 37991
2022-03-06 23:14:41 | INFO | fairseq.trainer | begin training epoch 62
2022-03-06 23:14:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:16:02 | INFO | train_inner | epoch 062:     26 / 196 loss=3.503, nll_loss=3.023, ppl=8.13, wps=20169.8, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=11900, lr=0.000289886, gnorm=0.951, loss_scale=8, train_wall=292, gb_free=19.9, wall=38073
2022-03-06 23:21:15 | INFO | train_inner | epoch 062:    126 / 196 loss=3.439, nll_loss=2.956, ppl=7.76, wps=20958.9, ups=0.32, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.941, loss_scale=16, train_wall=290, gb_free=19.9, wall=38385
2022-03-06 23:24:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:24:58 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.021 | nll_loss 8.656 | ppl 403.35 | wps 40717.3 | wpb 510.9 | bsz 1 | num_updates 12070 | best_loss 7.054
2022-03-06 23:24:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 12070 updates
2022-03-06 23:24:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 23:25:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 23:25:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 62 @ 12070 updates, score 9.021) (writing took 3.5397706299554557 seconds)
2022-03-06 23:25:02 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-06 23:25:02 | INFO | train | epoch 062 | loss 3.463 | nll_loss 2.981 | ppl 7.89 | wps 20649.4 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 12070 | lr 0.000287837 | gnorm 0.949 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 38613
2022-03-06 23:25:02 | INFO | fairseq.trainer | begin training epoch 63
2022-03-06 23:25:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:26:36 | INFO | train_inner | epoch 063:     30 / 196 loss=3.478, nll_loss=2.996, ppl=7.98, wps=20361.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=12100, lr=0.00028748, gnorm=0.94, loss_scale=16, train_wall=290, gb_free=19.9, wall=38706
2022-03-06 23:26:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 23:31:52 | INFO | train_inner | epoch 063:    131 / 196 loss=3.42, nll_loss=2.935, ppl=7.65, wps=20754.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.941, loss_scale=16, train_wall=293, gb_free=19.9, wall=39022
2022-03-06 23:33:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 23:35:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:35:20 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.08 | nll_loss 8.716 | ppl 420.61 | wps 40555.5 | wpb 510.9 | bsz 1 | num_updates 12264 | best_loss 7.054
2022-03-06 23:35:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 12264 updates
2022-03-06 23:35:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 23:35:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 23:35:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 63 @ 12264 updates, score 9.08) (writing took 3.5208312270697206 seconds)
2022-03-06 23:35:23 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-06 23:35:23 | INFO | train | epoch 063 | loss 3.438 | nll_loss 2.954 | ppl 7.75 | wps 20441.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 12264 | lr 0.000285551 | gnorm 0.943 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 39234
2022-03-06 23:35:23 | INFO | fairseq.trainer | begin training epoch 64
2022-03-06 23:35:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:37:16 | INFO | train_inner | epoch 064:     36 / 196 loss=3.445, nll_loss=2.962, ppl=7.79, wps=20160.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=12300, lr=0.000285133, gnorm=0.95, loss_scale=16, train_wall=293, gb_free=19.9, wall=39346
2022-03-06 23:41:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 23:42:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:42:35 | INFO | train_inner | epoch 064:    138 / 196 loss=3.405, nll_loss=2.92, ppl=7.57, wps=20545.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.937, loss_scale=8, train_wall=296, gb_free=19.9, wall=39665
2022-03-06 23:45:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:45:41 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.137 | nll_loss 8.775 | ppl 438.09 | wps 40518.7 | wpb 510.9 | bsz 1 | num_updates 12458 | best_loss 7.054
2022-03-06 23:45:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 12458 updates
2022-03-06 23:45:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 23:45:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 23:45:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 64 @ 12458 updates, score 9.137) (writing took 3.5616689738817513 seconds)
2022-03-06 23:45:44 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-06 23:45:44 | INFO | train | epoch 064 | loss 3.417 | nll_loss 2.932 | ppl 7.63 | wps 20437.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 12458 | lr 0.000283319 | gnorm 0.944 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 39855
2022-03-06 23:45:44 | INFO | fairseq.trainer | begin training epoch 65
2022-03-06 23:45:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:47:56 | INFO | train_inner | epoch 065:     42 / 196 loss=3.413, nll_loss=2.928, ppl=7.61, wps=20370.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=12500, lr=0.000282843, gnorm=0.949, loss_scale=8, train_wall=289, gb_free=19.9, wall=39986
2022-03-06 23:53:08 | INFO | train_inner | epoch 065:    142 / 196 loss=3.39, nll_loss=2.904, ppl=7.48, wps=20959.4, ups=0.32, wpb=65536, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.952, loss_scale=16, train_wall=290, gb_free=19.9, wall=40299
2022-03-06 23:55:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:56:02 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.152 | nll_loss 8.785 | ppl 441.26 | wps 40552.3 | wpb 510.9 | bsz 1 | num_updates 12654 | best_loss 7.054
2022-03-06 23:56:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 12654 updates
2022-03-06 23:56:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 23:56:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-06 23:56:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 65 @ 12654 updates, score 9.152) (writing took 3.5608876717742532 seconds)
2022-03-06 23:56:06 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-06 23:56:06 | INFO | train | epoch 065 | loss 3.396 | nll_loss 2.91 | ppl 7.51 | wps 20652.1 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 12654 | lr 0.000281116 | gnorm 0.952 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 40476
2022-03-06 23:56:06 | INFO | fairseq.trainer | begin training epoch 66
2022-03-06 23:56:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:56:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 23:57:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:58:36 | INFO | train_inner | epoch 066:     48 / 196 loss=3.392, nll_loss=2.906, ppl=7.5, wps=19968.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=12700, lr=0.000280607, gnorm=0.957, loss_scale=8, train_wall=295, gb_free=19.9, wall=40626
2022-03-07 00:03:48 | INFO | train_inner | epoch 066:    148 / 196 loss=3.374, nll_loss=2.887, ppl=7.4, wps=20967.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.946, loss_scale=8, train_wall=290, gb_free=19.9, wall=40939
2022-03-07 00:04:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:06:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:06:23 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.168 | nll_loss 8.805 | ppl 447.17 | wps 40523 | wpb 510.9 | bsz 1 | num_updates 12847 | best_loss 7.054
2022-03-07 00:06:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 12847 updates
2022-03-07 00:06:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 00:06:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 00:06:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 66 @ 12847 updates, score 9.168) (writing took 3.552477234043181 seconds)
2022-03-07 00:06:27 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-07 00:06:27 | INFO | train | epoch 066 | loss 3.373 | nll_loss 2.886 | ppl 7.39 | wps 20337.5 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 12847 | lr 0.000278997 | gnorm 0.953 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 41097
2022-03-07 00:06:27 | INFO | fairseq.trainer | begin training epoch 67
2022-03-07 00:06:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:09:12 | INFO | train_inner | epoch 067:     53 / 196 loss=3.356, nll_loss=2.868, ppl=7.3, wps=20167.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=12900, lr=0.000278423, gnorm=0.958, loss_scale=8, train_wall=292, gb_free=19.9, wall=41263
2022-03-07 00:14:25 | INFO | train_inner | epoch 067:    153 / 196 loss=3.36, nll_loss=2.872, ppl=7.32, wps=20947.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.961, loss_scale=16, train_wall=290, gb_free=19.9, wall=41576
2022-03-07 00:16:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:16:44 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.216 | nll_loss 8.855 | ppl 463.12 | wps 40803.1 | wpb 510.9 | bsz 1 | num_updates 13043 | best_loss 7.054
2022-03-07 00:16:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 13043 updates
2022-03-07 00:16:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 00:16:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 00:16:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 67 @ 13043 updates, score 9.216) (writing took 3.5146483900025487 seconds)
2022-03-07 00:16:48 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-07 00:16:48 | INFO | train | epoch 067 | loss 3.353 | nll_loss 2.865 | ppl 7.28 | wps 20645.6 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 13043 | lr 0.000276893 | gnorm 0.959 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 41718
2022-03-07 00:16:48 | INFO | fairseq.trainer | begin training epoch 68
2022-03-07 00:16:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:18:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:19:49 | INFO | train_inner | epoch 068:     58 / 196 loss=3.331, nll_loss=2.842, ppl=7.17, wps=20166.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=13100, lr=0.000276289, gnorm=0.96, loss_scale=16, train_wall=292, gb_free=19.9, wall=41900
2022-03-07 00:21:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:25:05 | INFO | train_inner | epoch 068:    159 / 196 loss=3.343, nll_loss=2.855, ppl=7.23, wps=20762.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.958, loss_scale=8, train_wall=293, gb_free=19.9, wall=42216
2022-03-07 00:27:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:27:05 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.245 | nll_loss 8.885 | ppl 472.61 | wps 40611.1 | wpb 510.9 | bsz 1 | num_updates 13237 | best_loss 7.054
2022-03-07 00:27:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 13237 updates
2022-03-07 00:27:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 00:27:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 00:27:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 68 @ 13237 updates, score 9.245) (writing took 3.527552213985473 seconds)
2022-03-07 00:27:09 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-07 00:27:09 | INFO | train | epoch 068 | loss 3.33 | nll_loss 2.841 | ppl 7.16 | wps 20446.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 13237 | lr 0.000274856 | gnorm 0.954 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 42339
2022-03-07 00:27:09 | INFO | fairseq.trainer | begin training epoch 69
2022-03-07 00:27:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:30:26 | INFO | train_inner | epoch 069:     63 / 196 loss=3.303, nll_loss=2.813, ppl=7.03, wps=20374.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=13300, lr=0.000274204, gnorm=0.954, loss_scale=16, train_wall=289, gb_free=19.9, wall=42536
2022-03-07 00:34:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:35:42 | INFO | train_inner | epoch 069:    164 / 196 loss=3.33, nll_loss=2.84, ppl=7.16, wps=20754.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.945, loss_scale=16, train_wall=293, gb_free=19.9, wall=42852
2022-03-07 00:37:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:37:26 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.267 | nll_loss 8.901 | ppl 478.02 | wps 40624.7 | wpb 510.9 | bsz 1 | num_updates 13432 | best_loss 7.054
2022-03-07 00:37:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 13432 updates
2022-03-07 00:37:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 00:37:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 00:37:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 69 @ 13432 updates, score 9.267) (writing took 3.5582750260364264 seconds)
2022-03-07 00:37:30 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-07 00:37:30 | INFO | train | epoch 069 | loss 3.312 | nll_loss 2.822 | ppl 7.07 | wps 20549.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13432 | lr 0.000272854 | gnorm 0.951 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 42960
2022-03-07 00:37:30 | INFO | fairseq.trainer | begin training epoch 70
2022-03-07 00:37:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:41:03 | INFO | train_inner | epoch 070:     68 / 196 loss=3.28, nll_loss=2.789, ppl=6.91, wps=20351.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=0.961, loss_scale=16, train_wall=290, gb_free=19.9, wall=43173
2022-03-07 00:41:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:46:18 | INFO | train_inner | epoch 070:    169 / 196 loss=3.316, nll_loss=2.826, ppl=7.09, wps=20759.9, ups=0.32, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.984, loss_scale=16, train_wall=293, gb_free=19.9, wall=43489
2022-03-07 00:47:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:47:48 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.322 | nll_loss 8.958 | ppl 497.45 | wps 40428.3 | wpb 510.9 | bsz 1 | num_updates 13627 | best_loss 7.054
2022-03-07 00:47:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 13627 updates
2022-03-07 00:47:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 00:47:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 00:47:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 70 @ 13627 updates, score 9.322) (writing took 3.5398399741388857 seconds)
2022-03-07 00:47:51 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-07 00:47:51 | INFO | train | epoch 070 | loss 3.294 | nll_loss 2.802 | ppl 6.98 | wps 20542 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13627 | lr 0.000270894 | gnorm 0.976 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 43582
2022-03-07 00:47:51 | INFO | fairseq.trainer | begin training epoch 71
2022-03-07 00:47:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:48:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:51:43 | INFO | train_inner | epoch 071:     74 / 196 loss=3.257, nll_loss=2.764, ppl=6.79, wps=20150.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=13700, lr=0.000270172, gnorm=0.964, loss_scale=16, train_wall=293, gb_free=19.9, wall=43813
2022-03-07 00:55:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:56:59 | INFO | train_inner | epoch 071:    175 / 196 loss=3.294, nll_loss=2.802, ppl=6.98, wps=20731.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.968, loss_scale=16, train_wall=293, gb_free=19.9, wall=44129
2022-03-07 00:58:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:58:09 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.404 | nll_loss 9.044 | ppl 527.82 | wps 40631.5 | wpb 510.9 | bsz 1 | num_updates 13821 | best_loss 7.054
2022-03-07 00:58:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 13821 updates
2022-03-07 00:58:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 00:58:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 00:58:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 71 @ 13821 updates, score 9.404) (writing took 3.557008298113942 seconds)
2022-03-07 00:58:13 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-07 00:58:13 | INFO | train | epoch 071 | loss 3.272 | nll_loss 2.78 | ppl 6.87 | wps 20425.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 13821 | lr 0.000268986 | gnorm 0.969 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 44203
2022-03-07 00:58:13 | INFO | fairseq.trainer | begin training epoch 72
2022-03-07 00:58:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:02:20 | INFO | train_inner | epoch 072:     79 / 196 loss=3.231, nll_loss=2.737, ppl=6.67, wps=20363.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=0.974, loss_scale=32, train_wall=289, gb_free=19.9, wall=44450
2022-03-07 01:02:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 01:07:36 | INFO | train_inner | epoch 072:    180 / 196 loss=3.287, nll_loss=2.796, ppl=6.94, wps=20748.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.982, loss_scale=16, train_wall=293, gb_free=19.9, wall=44766
2022-03-07 01:08:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:08:30 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.395 | nll_loss 9.036 | ppl 524.87 | wps 40658 | wpb 510.9 | bsz 1 | num_updates 14016 | best_loss 7.054
2022-03-07 01:08:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 14016 updates
2022-03-07 01:08:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 01:08:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 01:08:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 72 @ 14016 updates, score 9.395) (writing took 3.5519320629537106 seconds)
2022-03-07 01:08:34 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-07 01:08:34 | INFO | train | epoch 072 | loss 3.256 | nll_loss 2.763 | ppl 6.79 | wps 20544.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14016 | lr 0.000267109 | gnorm 0.976 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 44825
2022-03-07 01:08:34 | INFO | fairseq.trainer | begin training epoch 73
2022-03-07 01:08:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:09:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 01:13:00 | INFO | train_inner | epoch 073:     85 / 196 loss=3.211, nll_loss=2.715, ppl=6.57, wps=20170, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14100, lr=0.000266312, gnorm=0.974, loss_scale=16, train_wall=292, gb_free=19.9, wall=45090
2022-03-07 01:16:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 01:18:16 | INFO | train_inner | epoch 073:    186 / 196 loss=3.273, nll_loss=2.78, ppl=6.87, wps=20745.4, ups=0.32, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.973, loss_scale=16, train_wall=293, gb_free=19.9, wall=45406
2022-03-07 01:18:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:18:52 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.442 | nll_loss 9.078 | ppl 540.48 | wps 40672.8 | wpb 510.9 | bsz 1 | num_updates 14210 | best_loss 7.054
2022-03-07 01:18:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 14210 updates
2022-03-07 01:18:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 01:18:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 01:18:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 73 @ 14210 updates, score 9.442) (writing took 3.5427209860645235 seconds)
2022-03-07 01:18:55 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-07 01:18:55 | INFO | train | epoch 073 | loss 3.237 | nll_loss 2.743 | ppl 6.7 | wps 20438.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14210 | lr 0.000265279 | gnorm 0.973 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 45446
2022-03-07 01:18:55 | INFO | fairseq.trainer | begin training epoch 74
2022-03-07 01:18:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:21:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:23:40 | INFO | train_inner | epoch 074:     91 / 196 loss=3.184, nll_loss=2.687, ppl=6.44, wps=20169.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=0.986, loss_scale=8, train_wall=292, gb_free=19.9, wall=45730
2022-03-07 01:28:53 | INFO | train_inner | epoch 074:    191 / 196 loss=3.266, nll_loss=2.773, ppl=6.84, wps=20958.1, ups=0.32, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.986, loss_scale=16, train_wall=290, gb_free=19.9, wall=46043
2022-03-07 01:29:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:29:13 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.518 | nll_loss 9.158 | ppl 571.35 | wps 40743.3 | wpb 510.9 | bsz 1 | num_updates 14405 | best_loss 7.054
2022-03-07 01:29:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 14405 updates
2022-03-07 01:29:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 01:29:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 01:29:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 74 @ 14405 updates, score 9.518) (writing took 3.5456010890193284 seconds)
2022-03-07 01:29:16 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-07 01:29:16 | INFO | train | epoch 074 | loss 3.221 | nll_loss 2.726 | ppl 6.62 | wps 20547.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14405 | lr 0.000263477 | gnorm 0.986 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 46067
2022-03-07 01:29:16 | INFO | fairseq.trainer | begin training epoch 75
2022-03-07 01:29:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:29:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:34:16 | INFO | train_inner | epoch 075:     96 / 196 loss=3.159, nll_loss=2.661, ppl=6.33, wps=20176.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=14500, lr=0.000262613, gnorm=0.971, loss_scale=8, train_wall=292, gb_free=19.9, wall=46367
2022-03-07 01:39:28 | INFO | train_inner | epoch 075:    196 / 196 loss=3.252, nll_loss=2.758, ppl=6.76, wps=20954.7, ups=0.32, wpb=65363.4, bsz=127.7, num_updates=14600, lr=0.000261712, gnorm=0.99, loss_scale=16, train_wall=290, gb_free=19.9, wall=46679
2022-03-07 01:39:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:39:34 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.521 | nll_loss 9.163 | ppl 573.17 | wps 40635.1 | wpb 510.9 | bsz 1 | num_updates 14600 | best_loss 7.054
2022-03-07 01:39:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 14600 updates
2022-03-07 01:39:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 01:39:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 01:39:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 75 @ 14600 updates, score 9.521) (writing took 3.5278345216065645 seconds)
2022-03-07 01:39:37 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-07 01:39:37 | INFO | train | epoch 075 | loss 3.204 | nll_loss 2.708 | ppl 6.53 | wps 20550 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14600 | lr 0.000261712 | gnorm 0.98 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 46688
2022-03-07 01:39:37 | INFO | fairseq.trainer | begin training epoch 76
2022-03-07 01:39:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:43:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 01:44:53 | INFO | train_inner | epoch 076:    101 / 196 loss=3.148, nll_loss=2.65, ppl=6.28, wps=20172.1, ups=0.31, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.982, loss_scale=16, train_wall=293, gb_free=19.9, wall=47004
2022-03-07 01:49:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:49:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:49:55 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 9.54 | nll_loss 9.18 | ppl 580.07 | wps 40654.4 | wpb 510.9 | bsz 1 | num_updates 14794 | best_loss 7.054
2022-03-07 01:49:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 14794 updates
2022-03-07 01:49:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 01:49:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 01:49:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 76 @ 14794 updates, score 9.54) (writing took 3.5634867381304502 seconds)
2022-03-07 01:49:59 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-07 01:49:59 | INFO | train | epoch 076 | loss 3.187 | nll_loss 2.69 | ppl 6.45 | wps 20441.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14794 | lr 0.00025999 | gnorm 0.988 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 47309
2022-03-07 01:49:59 | INFO | fairseq.trainer | begin training epoch 77
2022-03-07 01:49:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:50:17 | INFO | train_inner | epoch 077:      6 / 196 loss=3.218, nll_loss=2.722, ppl=6.6, wps=20170.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14800, lr=0.000259938, gnorm=0.996, loss_scale=8, train_wall=292, gb_free=19.9, wall=47328
2022-03-07 01:55:30 | INFO | train_inner | epoch 077:    106 / 196 loss=3.136, nll_loss=2.637, ppl=6.22, wps=20964.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.998, loss_scale=8, train_wall=290, gb_free=19.9, wall=47640
2022-03-07 02:00:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:00:16 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 9.563 | nll_loss 9.202 | ppl 589.14 | wps 40692.7 | wpb 510.9 | bsz 1 | num_updates 14990 | best_loss 7.054
2022-03-07 02:00:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 14990 updates
2022-03-07 02:00:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 02:00:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 02:00:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 77 @ 14990 updates, score 9.563) (writing took 3.5682790391147137 seconds)
2022-03-07 02:00:20 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-07 02:00:20 | INFO | train | epoch 077 | loss 3.171 | nll_loss 2.674 | ppl 6.38 | wps 20651 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 14990 | lr 0.000258285 | gnorm 1.003 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 47930
2022-03-07 02:00:20 | INFO | fairseq.trainer | begin training epoch 78
2022-03-07 02:00:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:00:51 | INFO | train_inner | epoch 078:     10 / 196 loss=3.204, nll_loss=2.709, ppl=6.54, wps=20358.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=1.003, loss_scale=16, train_wall=290, gb_free=19.9, wall=47962
2022-03-07 02:03:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:06:07 | INFO | train_inner | epoch 078:    111 / 196 loss=3.113, nll_loss=2.613, ppl=6.12, wps=20746.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.984, loss_scale=16, train_wall=293, gb_free=19.9, wall=48277
2022-03-07 02:10:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:10:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:10:37 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 9.639 | nll_loss 9.278 | ppl 620.82 | wps 40479 | wpb 510.9 | bsz 1 | num_updates 15184 | best_loss 7.054
2022-03-07 02:10:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 15184 updates
2022-03-07 02:10:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 02:10:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 02:10:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 78 @ 15184 updates, score 9.639) (writing took 3.541967609897256 seconds)
2022-03-07 02:10:41 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-07 02:10:41 | INFO | train | epoch 078 | loss 3.154 | nll_loss 2.655 | ppl 6.3 | wps 20465 | ups 0.31 | wpb 65534.1 | bsz 128 | num_updates 15184 | lr 0.00025663 | gnorm 0.989 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 48551
2022-03-07 02:10:41 | INFO | fairseq.trainer | begin training epoch 79
2022-03-07 02:10:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:11:31 | INFO | train_inner | epoch 079:     16 / 196 loss=3.184, nll_loss=2.687, ppl=6.44, wps=20218.8, ups=0.31, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.994, loss_scale=16, train_wall=292, gb_free=19.9, wall=48602
2022-03-07 02:16:44 | INFO | train_inner | epoch 079:    116 / 196 loss=3.109, nll_loss=2.608, ppl=6.1, wps=20942, ups=0.32, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.989, loss_scale=16, train_wall=290, gb_free=19.9, wall=48915
2022-03-07 02:17:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:18:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:20:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:20:59 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 9.635 | nll_loss 9.273 | ppl 618.75 | wps 40645.9 | wpb 510.9 | bsz 1 | num_updates 15378 | best_loss 7.054
2022-03-07 02:20:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 15378 updates
2022-03-07 02:20:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 02:21:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 02:21:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 79 @ 15378 updates, score 9.635) (writing took 3.510504229925573 seconds)
2022-03-07 02:21:02 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-07 02:21:02 | INFO | train | epoch 079 | loss 3.138 | nll_loss 2.639 | ppl 6.23 | wps 20430 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 15378 | lr 0.000255006 | gnorm 0.997 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 49173
2022-03-07 02:21:02 | INFO | fairseq.trainer | begin training epoch 80
2022-03-07 02:21:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:22:11 | INFO | train_inner | epoch 080:     22 / 196 loss=3.161, nll_loss=2.663, ppl=6.33, wps=19974.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=1.008, loss_scale=8, train_wall=295, gb_free=19.9, wall=49242
2022-03-07 02:27:24 | INFO | train_inner | epoch 080:    122 / 196 loss=3.097, nll_loss=2.596, ppl=6.05, wps=20957.2, ups=0.32, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.997, loss_scale=16, train_wall=290, gb_free=19.9, wall=49554
2022-03-07 02:31:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:31:20 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 9.654 | nll_loss 9.293 | ppl 627.37 | wps 40785.8 | wpb 510.9 | bsz 1 | num_updates 15574 | best_loss 7.054
2022-03-07 02:31:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 15574 updates
2022-03-07 02:31:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 02:31:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 02:31:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 80 @ 15574 updates, score 9.654) (writing took 3.5044865156523883 seconds)
2022-03-07 02:31:24 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-07 02:31:24 | INFO | train | epoch 080 | loss 3.124 | nll_loss 2.625 | ppl 6.17 | wps 20647.3 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 15574 | lr 0.000253396 | gnorm 0.997 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 49794
2022-03-07 02:31:24 | INFO | fairseq.trainer | begin training epoch 81
2022-03-07 02:31:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:32:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:32:48 | INFO | train_inner | epoch 081:     27 / 196 loss=3.152, nll_loss=2.654, ppl=6.29, wps=20158.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15600, lr=0.000253185, gnorm=0.992, loss_scale=16, train_wall=293, gb_free=19.9, wall=49879
2022-03-07 02:38:01 | INFO | train_inner | epoch 081:    127 / 196 loss=3.087, nll_loss=2.585, ppl=6, wps=20950.1, ups=0.32, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.995, loss_scale=16, train_wall=290, gb_free=19.9, wall=50192
2022-03-07 02:39:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:41:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:41:42 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 9.732 | nll_loss 9.374 | ppl 663.48 | wps 40636.2 | wpb 510.9 | bsz 1 | num_updates 15768 | best_loss 7.054
2022-03-07 02:41:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 15768 updates
2022-03-07 02:41:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 02:41:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 02:41:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 81 @ 15768 updates, score 9.732) (writing took 3.5115231750532985 seconds)
2022-03-07 02:41:45 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-07 02:41:45 | INFO | train | epoch 081 | loss 3.109 | nll_loss 2.608 | ppl 6.1 | wps 20432.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 15768 | lr 0.000251832 | gnorm 1.003 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 50416
2022-03-07 02:41:45 | INFO | fairseq.trainer | begin training epoch 82
2022-03-07 02:41:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:43:25 | INFO | train_inner | epoch 082:     32 / 196 loss=3.12, nll_loss=2.62, ppl=6.15, wps=20153.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=1.013, loss_scale=16, train_wall=293, gb_free=19.9, wall=50516
2022-03-07 02:46:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:48:41 | INFO | train_inner | epoch 082:    133 / 196 loss=3.081, nll_loss=2.578, ppl=5.97, wps=20741.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=15900, lr=0.000250785, gnorm=1, loss_scale=16, train_wall=293, gb_free=19.9, wall=50832
2022-03-07 02:51:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:52:03 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.753 | nll_loss 9.395 | ppl 673.25 | wps 40288.9 | wpb 510.9 | bsz 1 | num_updates 15963 | best_loss 7.054
2022-03-07 02:52:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 15963 updates
2022-03-07 02:52:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 02:52:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 02:52:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 82 @ 15963 updates, score 9.753) (writing took 3.4036834272556007 seconds)
2022-03-07 02:52:06 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-07 02:52:06 | INFO | train | epoch 082 | loss 3.095 | nll_loss 2.594 | ppl 6.04 | wps 20538.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15963 | lr 0.00025029 | gnorm 0.998 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 51037
2022-03-07 02:52:06 | INFO | fairseq.trainer | begin training epoch 83
2022-03-07 02:52:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:53:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:54:05 | INFO | train_inner | epoch 083:     38 / 196 loss=3.095, nll_loss=2.594, ppl=6.04, wps=20169.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=16000, lr=0.00025, gnorm=1, loss_scale=16, train_wall=293, gb_free=19.9, wall=51156
2022-03-07 02:59:18 | INFO | train_inner | epoch 083:    138 / 196 loss=3.075, nll_loss=2.573, ppl=5.95, wps=20940.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=16100, lr=0.000249222, gnorm=1.017, loss_scale=16, train_wall=290, gb_free=19.9, wall=51469
2022-03-07 03:00:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:02:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:02:24 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 9.79 | nll_loss 9.429 | ppl 689.33 | wps 40609.7 | wpb 510.9 | bsz 1 | num_updates 16157 | best_loss 7.054
2022-03-07 03:02:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 16157 updates
2022-03-07 03:02:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 03:02:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 03:02:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 83 @ 16157 updates, score 9.79) (writing took 3.433656964916736 seconds)
2022-03-07 03:02:28 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-07 03:02:28 | INFO | train | epoch 083 | loss 3.08 | nll_loss 2.578 | ppl 5.97 | wps 20431 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 16157 | lr 0.000248782 | gnorm 1.015 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 51658
2022-03-07 03:02:28 | INFO | fairseq.trainer | begin training epoch 84
2022-03-07 03:02:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:04:43 | INFO | train_inner | epoch 084:     43 / 196 loss=3.074, nll_loss=2.571, ppl=5.94, wps=20158.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=1.009, loss_scale=16, train_wall=293, gb_free=19.9, wall=51793
2022-03-07 03:07:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:09:58 | INFO | train_inner | epoch 084:    144 / 196 loss=3.071, nll_loss=2.569, ppl=5.93, wps=20744.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=16300, lr=0.000247689, gnorm=1.026, loss_scale=16, train_wall=293, gb_free=19.9, wall=52109
2022-03-07 03:12:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:12:46 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 9.866 | nll_loss 9.507 | ppl 727.71 | wps 40489.3 | wpb 510.9 | bsz 1 | num_updates 16352 | best_loss 7.054
2022-03-07 03:12:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 16352 updates
2022-03-07 03:12:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 03:12:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 03:12:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 84 @ 16352 updates, score 9.866) (writing took 3.419219431001693 seconds)
2022-03-07 03:12:49 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-07 03:12:49 | INFO | train | epoch 084 | loss 3.067 | nll_loss 2.564 | ppl 5.91 | wps 20538.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16352 | lr 0.000247295 | gnorm 1.019 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 52280
2022-03-07 03:12:49 | INFO | fairseq.trainer | begin training epoch 85
2022-03-07 03:12:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:14:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:14:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:15:26 | INFO | train_inner | epoch 085:     50 / 196 loss=3.062, nll_loss=2.559, ppl=5.89, wps=19974.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=16400, lr=0.000246932, gnorm=1.023, loss_scale=8, train_wall=295, gb_free=19.9, wall=52436
2022-03-07 03:20:38 | INFO | train_inner | epoch 085:    150 / 196 loss=3.058, nll_loss=2.555, ppl=5.88, wps=20968.1, ups=0.32, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=1.012, loss_scale=8, train_wall=290, gb_free=19.9, wall=52749
2022-03-07 03:23:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:23:07 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 9.834 | nll_loss 9.474 | ppl 710.91 | wps 40675.5 | wpb 510.9 | bsz 1 | num_updates 16546 | best_loss 7.054
2022-03-07 03:23:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 16546 updates
2022-03-07 03:23:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 03:23:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 03:23:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 85 @ 16546 updates, score 9.834) (writing took 3.396148565225303 seconds)
2022-03-07 03:23:10 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-07 03:23:10 | INFO | train | epoch 085 | loss 3.053 | nll_loss 2.55 | ppl 5.86 | wps 20448.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 16546 | lr 0.000245841 | gnorm 1.016 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 52901
2022-03-07 03:23:10 | INFO | fairseq.trainer | begin training epoch 86
2022-03-07 03:23:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:25:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:26:02 | INFO | train_inner | epoch 086:     55 / 196 loss=3.035, nll_loss=2.53, ppl=5.78, wps=20175.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=1.016, loss_scale=8, train_wall=292, gb_free=19.9, wall=53073
2022-03-07 03:31:15 | INFO | train_inner | epoch 086:    155 / 196 loss=3.05, nll_loss=2.546, ppl=5.84, wps=20946.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=16700, lr=0.000244704, gnorm=1.021, loss_scale=8, train_wall=290, gb_free=19.9, wall=53386
2022-03-07 03:33:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:33:28 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 9.904 | nll_loss 9.547 | ppl 748.3 | wps 40570.8 | wpb 510.9 | bsz 1 | num_updates 16741 | best_loss 7.054
2022-03-07 03:33:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 16741 updates
2022-03-07 03:33:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 03:33:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 03:33:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 86 @ 16741 updates, score 9.904) (writing took 3.420903737191111 seconds)
2022-03-07 03:33:32 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-07 03:33:32 | INFO | train | epoch 086 | loss 3.041 | nll_loss 2.536 | ppl 5.8 | wps 20539.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16741 | lr 0.000244405 | gnorm 1.021 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 53522
2022-03-07 03:33:32 | INFO | fairseq.trainer | begin training epoch 87
2022-03-07 03:33:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:36:36 | INFO | train_inner | epoch 087:     59 / 196 loss=3.026, nll_loss=2.521, ppl=5.74, wps=20364.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=16800, lr=0.000243975, gnorm=1.017, loss_scale=16, train_wall=290, gb_free=19.9, wall=53707
2022-03-07 03:39:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:41:52 | INFO | train_inner | epoch 087:    160 / 196 loss=3.035, nll_loss=2.531, ppl=5.78, wps=20747.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=16900, lr=0.000243252, gnorm=1.018, loss_scale=16, train_wall=293, gb_free=19.9, wall=54023
2022-03-07 03:43:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:43:49 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 9.915 | nll_loss 9.558 | ppl 753.62 | wps 40715.7 | wpb 510.9 | bsz 1 | num_updates 16936 | best_loss 7.054
2022-03-07 03:43:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 16936 updates
2022-03-07 03:43:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 03:43:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 03:43:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 87 @ 16936 updates, score 9.915) (writing took 3.5052010002546012 seconds)
2022-03-07 03:43:53 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-07 03:43:53 | INFO | train | epoch 087 | loss 3.028 | nll_loss 2.523 | ppl 5.75 | wps 20541.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16936 | lr 0.000242993 | gnorm 1.016 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 54143
2022-03-07 03:43:53 | INFO | fairseq.trainer | begin training epoch 88
2022-03-07 03:43:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:46:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:47:16 | INFO | train_inner | epoch 088:     65 / 196 loss=3.01, nll_loss=2.504, ppl=5.67, wps=20157.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=1.02, loss_scale=16, train_wall=293, gb_free=19.9, wall=54347
2022-03-07 03:52:29 | INFO | train_inner | epoch 088:    165 / 196 loss=3.029, nll_loss=2.524, ppl=5.75, wps=20957.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=1.019, loss_scale=16, train_wall=290, gb_free=19.9, wall=54660
2022-03-07 03:53:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:54:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:54:11 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 9.935 | nll_loss 9.576 | ppl 763.25 | wps 40509.2 | wpb 510.9 | bsz 1 | num_updates 17130 | best_loss 7.054
2022-03-07 03:54:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 17130 updates
2022-03-07 03:54:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 03:54:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 03:54:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 88 @ 17130 updates, score 9.935) (writing took 3.4164961078204215 seconds)
2022-03-07 03:54:14 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-07 03:54:14 | INFO | train | epoch 088 | loss 3.015 | nll_loss 2.509 | ppl 5.69 | wps 20438.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 17130 | lr 0.000241614 | gnorm 1.023 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 54765
2022-03-07 03:54:14 | INFO | fairseq.trainer | begin training epoch 89
2022-03-07 03:54:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:57:53 | INFO | train_inner | epoch 089:     70 / 196 loss=2.99, nll_loss=2.484, ppl=5.59, wps=20160.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=17200, lr=0.000241121, gnorm=1.034, loss_scale=16, train_wall=293, gb_free=19.9, wall=54984
2022-03-07 04:00:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:03:09 | INFO | train_inner | epoch 089:    171 / 196 loss=3.02, nll_loss=2.514, ppl=5.71, wps=20742.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=17300, lr=0.000240424, gnorm=1.031, loss_scale=16, train_wall=293, gb_free=19.9, wall=55300
2022-03-07 04:04:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:04:32 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 9.961 | nll_loss 9.603 | ppl 777.44 | wps 40564.6 | wpb 510.9 | bsz 1 | num_updates 17325 | best_loss 7.054
2022-03-07 04:04:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 17325 updates
2022-03-07 04:04:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 04:04:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 04:04:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 89 @ 17325 updates, score 9.961) (writing took 3.407295220065862 seconds)
2022-03-07 04:04:35 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-07 04:04:35 | INFO | train | epoch 089 | loss 3.003 | nll_loss 2.497 | ppl 5.65 | wps 20540.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 17325 | lr 0.00024025 | gnorm 1.031 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 55386
2022-03-07 04:04:35 | INFO | fairseq.trainer | begin training epoch 90
2022-03-07 04:04:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:07:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:08:33 | INFO | train_inner | epoch 090:     76 / 196 loss=2.977, nll_loss=2.469, ppl=5.54, wps=20166.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=1.02, loss_scale=16, train_wall=293, gb_free=19.9, wall=55624
2022-03-07 04:13:46 | INFO | train_inner | epoch 090:    176 / 196 loss=3.016, nll_loss=2.51, ppl=5.7, wps=20930.3, ups=0.32, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=1.038, loss_scale=16, train_wall=291, gb_free=19.9, wall=55937
2022-03-07 04:14:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:14:54 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 9.949 | nll_loss 9.588 | ppl 769.73 | wps 40576.7 | wpb 510.9 | bsz 1 | num_updates 17520 | best_loss 7.054
2022-03-07 04:14:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 17520 updates
2022-03-07 04:14:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 04:14:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 04:14:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 90 @ 17520 updates, score 9.949) (writing took 3.518724618013948 seconds)
2022-03-07 04:14:57 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-07 04:14:57 | INFO | train | epoch 090 | loss 2.99 | nll_loss 2.484 | ppl 5.59 | wps 20525.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 17520 | lr 0.000238909 | gnorm 1.025 | loss_scale 32 | train_wall 569 | gb_free 19.9 | wall 56008
2022-03-07 04:14:57 | INFO | fairseq.trainer | begin training epoch 91
2022-03-07 04:14:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:16:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:19:11 | INFO | train_inner | epoch 091:     81 / 196 loss=2.955, nll_loss=2.447, ppl=5.45, wps=20132.5, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=17600, lr=0.000238366, gnorm=1.011, loss_scale=16, train_wall=293, gb_free=19.9, wall=56262
2022-03-07 04:24:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:24:27 | INFO | train_inner | epoch 091:    182 / 196 loss=3.007, nll_loss=2.501, ppl=5.66, wps=20720.6, ups=0.32, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=1.029, loss_scale=16, train_wall=293, gb_free=19.9, wall=56578
2022-03-07 04:25:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:25:16 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 9.99 | nll_loss 9.63 | ppl 792.11 | wps 40876.8 | wpb 510.9 | bsz 1 | num_updates 17714 | best_loss 7.054
2022-03-07 04:25:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 17714 updates
2022-03-07 04:25:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 04:25:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 04:25:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 91 @ 17714 updates, score 9.99) (writing took 3.5115008340217173 seconds)
2022-03-07 04:25:19 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-07 04:25:19 | INFO | train | epoch 091 | loss 2.979 | nll_loss 2.471 | ppl 5.55 | wps 20409.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 17714 | lr 0.000237597 | gnorm 1.021 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 56630
2022-03-07 04:25:19 | INFO | fairseq.trainer | begin training epoch 92
2022-03-07 04:25:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:26:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:29:52 | INFO | train_inner | epoch 092:     87 / 196 loss=2.937, nll_loss=2.428, ppl=5.38, wps=20148.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=1.039, loss_scale=8, train_wall=293, gb_free=19.9, wall=56902
2022-03-07 04:35:04 | INFO | train_inner | epoch 092:    187 / 196 loss=3.003, nll_loss=2.496, ppl=5.64, wps=20953.6, ups=0.32, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=1.04, loss_scale=16, train_wall=290, gb_free=19.9, wall=57215
2022-03-07 04:35:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:35:37 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 10.036 | nll_loss 9.682 | ppl 821.59 | wps 40547.9 | wpb 510.9 | bsz 1 | num_updates 17909 | best_loss 7.054
2022-03-07 04:35:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 17909 updates
2022-03-07 04:35:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 04:35:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 04:35:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 92 @ 17909 updates, score 10.036) (writing took 3.25848827091977 seconds)
2022-03-07 04:35:41 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-07 04:35:41 | INFO | train | epoch 092 | loss 2.968 | nll_loss 2.46 | ppl 5.5 | wps 20540.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 17909 | lr 0.0002363 | gnorm 1.041 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 57251
2022-03-07 04:35:41 | INFO | fairseq.trainer | begin training epoch 93
2022-03-07 04:35:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:37:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:40:28 | INFO | train_inner | epoch 093:     92 / 196 loss=2.918, nll_loss=2.408, ppl=5.31, wps=20182.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18000, lr=0.000235702, gnorm=1.027, loss_scale=8, train_wall=292, gb_free=19.9, wall=57539
2022-03-07 04:45:41 | INFO | train_inner | epoch 093:    192 / 196 loss=2.996, nll_loss=2.49, ppl=5.62, wps=20950.5, ups=0.32, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=1.059, loss_scale=16, train_wall=290, gb_free=19.9, wall=57852
2022-03-07 04:45:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:45:58 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 10.037 | nll_loss 9.679 | ppl 819.51 | wps 40410.4 | wpb 510.9 | bsz 1 | num_updates 18104 | best_loss 7.054
2022-03-07 04:45:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 18104 updates
2022-03-07 04:45:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 04:46:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 04:46:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 93 @ 18104 updates, score 10.037) (writing took 3.4902608189731836 seconds)
2022-03-07 04:46:02 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-07 04:46:02 | INFO | train | epoch 093 | loss 2.955 | nll_loss 2.446 | ppl 5.45 | wps 20540.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18104 | lr 0.000235024 | gnorm 1.043 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 57872
2022-03-07 04:46:02 | INFO | fairseq.trainer | begin training epoch 94
2022-03-07 04:46:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:51:02 | INFO | train_inner | epoch 094:     96 / 196 loss=2.905, nll_loss=2.395, ppl=5.26, wps=20342.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=1.027, loss_scale=16, train_wall=290, gb_free=19.9, wall=58173
2022-03-07 04:51:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:56:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:56:20 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 10.085 | nll_loss 9.726 | ppl 846.79 | wps 40601.5 | wpb 510.9 | bsz 1 | num_updates 18299 | best_loss 7.054
2022-03-07 04:56:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 18299 updates
2022-03-07 04:56:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 04:56:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 04:56:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 94 @ 18299 updates, score 10.085) (writing took 3.5257356739602983 seconds)
2022-03-07 04:56:23 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-07 04:56:23 | INFO | train | epoch 094 | loss 2.944 | nll_loss 2.435 | ppl 5.41 | wps 20531.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18299 | lr 0.000233769 | gnorm 1.033 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 58494
2022-03-07 04:56:24 | INFO | fairseq.trainer | begin training epoch 95
2022-03-07 04:56:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:56:27 | INFO | train_inner | epoch 095:      1 / 196 loss=2.987, nll_loss=2.48, ppl=5.58, wps=20159, ups=0.31, wpb=65367, bsz=127.7, num_updates=18300, lr=0.000233762, gnorm=1.042, loss_scale=16, train_wall=293, gb_free=19.9, wall=58497
2022-03-07 04:58:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:01:43 | INFO | train_inner | epoch 095:    102 / 196 loss=2.899, nll_loss=2.388, ppl=5.23, wps=20751, ups=0.32, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=1.026, loss_scale=16, train_wall=293, gb_free=19.9, wall=58813
2022-03-07 05:05:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:06:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:06:41 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 10.085 | nll_loss 9.727 | ppl 847.23 | wps 40688.5 | wpb 510.9 | bsz 1 | num_updates 18493 | best_loss 7.054
2022-03-07 05:06:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 18493 updates
2022-03-07 05:06:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 05:06:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 05:06:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 95 @ 18493 updates, score 10.085) (writing took 3.516772349830717 seconds)
2022-03-07 05:06:45 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-07 05:06:45 | INFO | train | epoch 095 | loss 2.932 | nll_loss 2.423 | ppl 5.36 | wps 20439.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 18493 | lr 0.000232539 | gnorm 1.034 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 59115
2022-03-07 05:06:45 | INFO | fairseq.trainer | begin training epoch 96
2022-03-07 05:06:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:07:07 | INFO | train_inner | epoch 096:      7 / 196 loss=2.958, nll_loss=2.45, ppl=5.46, wps=20166.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18500, lr=0.000232495, gnorm=1.042, loss_scale=16, train_wall=292, gb_free=19.9, wall=59137
2022-03-07 05:12:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:12:23 | INFO | train_inner | epoch 096:    108 / 196 loss=2.891, nll_loss=2.379, ppl=5.2, wps=20746.8, ups=0.32, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=1.03, loss_scale=16, train_wall=293, gb_free=19.9, wall=59453
2022-03-07 05:16:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:17:02 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 10.151 | nll_loss 9.8 | ppl 891.42 | wps 40578.5 | wpb 510.9 | bsz 1 | num_updates 18688 | best_loss 7.054
2022-03-07 05:17:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 18688 updates
2022-03-07 05:17:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 05:17:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 05:17:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 96 @ 18688 updates, score 10.151) (writing took 3.5071821389719844 seconds)
2022-03-07 05:17:06 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-07 05:17:06 | INFO | train | epoch 096 | loss 2.923 | nll_loss 2.413 | ppl 5.33 | wps 20541.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18688 | lr 0.000231323 | gnorm 1.043 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 59737
2022-03-07 05:17:06 | INFO | fairseq.trainer | begin training epoch 97
2022-03-07 05:17:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:17:44 | INFO | train_inner | epoch 097:     12 / 196 loss=2.952, nll_loss=2.443, ppl=5.44, wps=20354.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=1.056, loss_scale=16, train_wall=290, gb_free=19.9, wall=59774
2022-03-07 05:19:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:23:00 | INFO | train_inner | epoch 097:    113 / 196 loss=2.884, nll_loss=2.372, ppl=5.18, wps=20745.5, ups=0.32, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=1.031, loss_scale=16, train_wall=293, gb_free=19.9, wall=60090
2022-03-07 05:26:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:27:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:27:24 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 10.179 | nll_loss 9.821 | ppl 904.67 | wps 40548.7 | wpb 510.9 | bsz 1 | num_updates 18882 | best_loss 7.054
2022-03-07 05:27:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 18882 updates
2022-03-07 05:27:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 05:27:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 05:27:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 97 @ 18882 updates, score 10.179) (writing took 3.5101172500289977 seconds)
2022-03-07 05:27:27 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-07 05:27:27 | INFO | train | epoch 097 | loss 2.912 | nll_loss 2.402 | ppl 5.28 | wps 20428.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 18882 | lr 0.000230131 | gnorm 1.042 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 60358
2022-03-07 05:27:28 | INFO | fairseq.trainer | begin training epoch 98
2022-03-07 05:27:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:28:24 | INFO | train_inner | epoch 098:     18 / 196 loss=2.93, nll_loss=2.42, ppl=5.35, wps=20156, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18900, lr=0.000230022, gnorm=1.048, loss_scale=16, train_wall=293, gb_free=19.9, wall=60414
2022-03-07 05:33:37 | INFO | train_inner | epoch 098:    118 / 196 loss=2.877, nll_loss=2.365, ppl=5.15, wps=20949, ups=0.32, wpb=65532.4, bsz=128, num_updates=19000, lr=0.000229416, gnorm=1.05, loss_scale=32, train_wall=290, gb_free=19.9, wall=60727
2022-03-07 05:33:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:37:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:37:45 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 10.144 | nll_loss 9.786 | ppl 882.72 | wps 40601.9 | wpb 510.9 | bsz 1 | num_updates 19077 | best_loss 7.054
2022-03-07 05:37:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 19077 updates
2022-03-07 05:37:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 05:37:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 05:37:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 98 @ 19077 updates, score 10.144) (writing took 3.5024960851296782 seconds)
2022-03-07 05:37:49 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-07 05:37:49 | INFO | train | epoch 098 | loss 2.901 | nll_loss 2.39 | ppl 5.24 | wps 20537.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19077 | lr 0.000228952 | gnorm 1.044 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 60979
2022-03-07 05:37:49 | INFO | fairseq.trainer | begin training epoch 99
2022-03-07 05:37:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:39:01 | INFO | train_inner | epoch 099:     23 / 196 loss=2.925, nll_loss=2.416, ppl=5.34, wps=20164.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=1.044, loss_scale=16, train_wall=293, gb_free=19.9, wall=61051
2022-03-07 05:41:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:44:17 | INFO | train_inner | epoch 099:    124 / 196 loss=2.869, nll_loss=2.357, ppl=5.12, wps=20747, ups=0.32, wpb=65532.4, bsz=128, num_updates=19200, lr=0.000228218, gnorm=1.048, loss_scale=16, train_wall=293, gb_free=19.9, wall=61367
2022-03-07 05:48:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:48:07 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 10.187 | nll_loss 9.832 | ppl 911.7 | wps 40553.8 | wpb 510.9 | bsz 1 | num_updates 19272 | best_loss 7.054
2022-03-07 05:48:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 19272 updates
2022-03-07 05:48:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 05:48:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 05:48:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 99 @ 19272 updates, score 10.187) (writing took 3.474629601929337 seconds)
2022-03-07 05:48:10 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-07 05:48:10 | INFO | train | epoch 099 | loss 2.892 | nll_loss 2.38 | ppl 5.21 | wps 20541.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19272 | lr 0.000227791 | gnorm 1.051 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 61601
2022-03-07 05:48:10 | INFO | fairseq.trainer | begin training epoch 100
2022-03-07 05:48:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:48:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:49:41 | INFO | train_inner | epoch 100:     29 / 196 loss=2.908, nll_loss=2.398, ppl=5.27, wps=20160.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=19300, lr=0.000227626, gnorm=1.044, loss_scale=16, train_wall=293, gb_free=19.9, wall=61692
2022-03-07 05:54:54 | INFO | train_inner | epoch 100:    129 / 196 loss=2.867, nll_loss=2.354, ppl=5.11, wps=20956.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=19400, lr=0.000227038, gnorm=1.051, loss_scale=16, train_wall=290, gb_free=19.9, wall=62004
2022-03-07 05:55:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:58:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:58:28 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 10.233 | nll_loss 9.878 | ppl 940.73 | wps 40625.5 | wpb 510.9 | bsz 1 | num_updates 19466 | best_loss 7.054
2022-03-07 05:58:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 19466 updates
2022-03-07 05:58:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 05:58:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 05:58:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 100 @ 19466 updates, score 10.233) (writing took 3.406311016064137 seconds)
2022-03-07 05:58:31 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-07 05:58:31 | INFO | train | epoch 100 | loss 2.882 | nll_loss 2.37 | ppl 5.17 | wps 20437.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19466 | lr 0.000226653 | gnorm 1.051 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 62222
2022-03-07 05:58:31 | INFO | fairseq.trainer | begin training epoch 101
2022-03-07 05:58:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:00:18 | INFO | train_inner | epoch 101:     34 / 196 loss=2.89, nll_loss=2.379, ppl=5.2, wps=20162.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=1.069, loss_scale=16, train_wall=293, gb_free=19.9, wall=62328
2022-03-07 06:01:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:05:34 | INFO | train_inner | epoch 101:    135 / 196 loss=2.863, nll_loss=2.35, ppl=5.1, wps=20744.6, ups=0.32, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=1.046, loss_scale=16, train_wall=293, gb_free=19.9, wall=62644
2022-03-07 06:08:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:08:49 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 10.217 | nll_loss 9.863 | ppl 931.15 | wps 40625.3 | wpb 510.9 | bsz 1 | num_updates 19661 | best_loss 7.054
2022-03-07 06:08:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 19661 updates
2022-03-07 06:08:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 06:08:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 06:08:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 101 @ 19661 updates, score 10.217) (writing took 3.449741489253938 seconds)
2022-03-07 06:08:53 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-07 06:08:53 | INFO | train | epoch 101 | loss 2.871 | nll_loss 2.359 | ppl 5.13 | wps 20538.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19661 | lr 0.000225526 | gnorm 1.054 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 62843
2022-03-07 06:08:53 | INFO | fairseq.trainer | begin training epoch 102
2022-03-07 06:08:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:09:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:10:58 | INFO | train_inner | epoch 102:     40 / 196 loss=2.867, nll_loss=2.354, ppl=5.11, wps=20157, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=19700, lr=0.000225303, gnorm=1.055, loss_scale=16, train_wall=293, gb_free=19.9, wall=62969
2022-03-07 06:16:11 | INFO | train_inner | epoch 102:    140 / 196 loss=2.86, nll_loss=2.347, ppl=5.09, wps=20946.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=19800, lr=0.000224733, gnorm=1.076, loss_scale=32, train_wall=290, gb_free=19.9, wall=63281
2022-03-07 06:16:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:19:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:19:11 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 10.23 | nll_loss 9.871 | ppl 936.69 | wps 40812.1 | wpb 510.9 | bsz 1 | num_updates 19855 | best_loss 7.054
2022-03-07 06:19:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 19855 updates
2022-03-07 06:19:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 06:19:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 06:19:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 102 @ 19855 updates, score 10.23) (writing took 3.4867679770104587 seconds)
2022-03-07 06:19:14 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-07 06:19:14 | INFO | train | epoch 102 | loss 2.862 | nll_loss 2.349 | ppl 5.09 | wps 20432.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19855 | lr 0.000224422 | gnorm 1.064 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 63465
2022-03-07 06:19:14 | INFO | fairseq.trainer | begin training epoch 103
2022-03-07 06:19:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:21:35 | INFO | train_inner | epoch 103:     45 / 196 loss=2.865, nll_loss=2.352, ppl=5.11, wps=20171.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=1.046, loss_scale=16, train_wall=292, gb_free=19.9, wall=63606
2022-03-07 06:23:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:26:51 | INFO | train_inner | epoch 103:    146 / 196 loss=2.856, nll_loss=2.343, ppl=5.07, wps=20752.4, ups=0.32, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=1.052, loss_scale=16, train_wall=293, gb_free=19.9, wall=63921
2022-03-07 06:29:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:29:32 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 10.276 | nll_loss 9.917 | ppl 967.03 | wps 40574.8 | wpb 510.9 | bsz 1 | num_updates 20050 | best_loss 7.054
2022-03-07 06:29:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 20050 updates
2022-03-07 06:29:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 06:29:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 06:29:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 103 @ 20050 updates, score 10.276) (writing took 3.487135082948953 seconds)
2022-03-07 06:29:35 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-07 06:29:35 | INFO | train | epoch 103 | loss 2.854 | nll_loss 2.34 | ppl 5.06 | wps 20548 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 20050 | lr 0.000223328 | gnorm 1.05 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 64086
2022-03-07 06:29:35 | INFO | fairseq.trainer | begin training epoch 104
2022-03-07 06:29:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:30:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:32:15 | INFO | train_inner | epoch 104:     51 / 196 loss=2.838, nll_loss=2.323, ppl=5.01, wps=20161.3, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=20100, lr=0.00022305, gnorm=1.061, loss_scale=16, train_wall=293, gb_free=19.9, wall=64246
2022-03-07 06:37:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:37:31 | INFO | train_inner | epoch 104:    152 / 196 loss=2.848, nll_loss=2.334, ppl=5.04, wps=20743.2, ups=0.32, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=1.063, loss_scale=16, train_wall=293, gb_free=19.9, wall=64561
2022-03-07 06:39:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:39:53 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 10.29 | nll_loss 9.934 | ppl 978.39 | wps 40544.5 | wpb 510.9 | bsz 1 | num_updates 20244 | best_loss 7.054
2022-03-07 06:39:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 20244 updates
2022-03-07 06:39:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 06:39:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 06:39:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 104 @ 20244 updates, score 10.29) (writing took 3.491089846007526 seconds)
2022-03-07 06:39:57 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-07 06:39:57 | INFO | train | epoch 104 | loss 2.844 | nll_loss 2.33 | ppl 5.03 | wps 20428.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 20244 | lr 0.000222255 | gnorm 1.066 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 64707
2022-03-07 06:39:57 | INFO | fairseq.trainer | begin training epoch 105
2022-03-07 06:39:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:42:52 | INFO | train_inner | epoch 105:     56 / 196 loss=2.836, nll_loss=2.322, ppl=5, wps=20345, ups=0.31, wpb=65367, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=1.06, loss_scale=16, train_wall=290, gb_free=19.9, wall=64883
2022-03-07 06:44:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:48:08 | INFO | train_inner | epoch 105:    157 / 196 loss=2.85, nll_loss=2.336, ppl=5.05, wps=20753.4, ups=0.32, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=1.059, loss_scale=16, train_wall=293, gb_free=19.9, wall=65199
2022-03-07 06:50:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:50:15 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 10.3 | nll_loss 9.944 | ppl 985.19 | wps 40483.2 | wpb 510.9 | bsz 1 | num_updates 20439 | best_loss 7.054
2022-03-07 06:50:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 20439 updates
2022-03-07 06:50:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 06:50:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 06:50:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 105 @ 20439 updates, score 10.3) (writing took 3.433996091596782 seconds)
2022-03-07 06:50:18 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-07 06:50:18 | INFO | train | epoch 105 | loss 2.835 | nll_loss 2.32 | ppl 5 | wps 20540.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 20439 | lr 0.000221192 | gnorm 1.058 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 65329
2022-03-07 06:50:18 | INFO | fairseq.trainer | begin training epoch 106
2022-03-07 06:50:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:50:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:53:32 | INFO | train_inner | epoch 106:     62 / 196 loss=2.813, nll_loss=2.298, ppl=4.92, wps=20166.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=20500, lr=0.000220863, gnorm=1.051, loss_scale=16, train_wall=293, gb_free=19.9, wall=65523
2022-03-07 06:57:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:58:48 | INFO | train_inner | epoch 106:    163 / 196 loss=2.841, nll_loss=2.327, ppl=5.02, wps=20739.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=20600, lr=0.000220326, gnorm=1.062, loss_scale=16, train_wall=293, gb_free=19.9, wall=65839
2022-03-07 06:59:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 07:00:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:00:36 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 10.307 | nll_loss 9.948 | ppl 988 | wps 40498 | wpb 510.9 | bsz 1 | num_updates 20632 | best_loss 7.054
2022-03-07 07:00:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 20632 updates
2022-03-07 07:00:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 07:00:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 07:00:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 106 @ 20632 updates, score 10.307) (writing took 3.4370160456746817 seconds)
2022-03-07 07:00:39 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-07 07:00:39 | INFO | train | epoch 106 | loss 2.825 | nll_loss 2.31 | ppl 4.96 | wps 20332.5 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 20632 | lr 0.000220155 | gnorm 1.058 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 65950
2022-03-07 07:00:39 | INFO | fairseq.trainer | begin training epoch 107
2022-03-07 07:00:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:04:12 | INFO | train_inner | epoch 107:     68 / 196 loss=2.799, nll_loss=2.283, ppl=4.87, wps=20169.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=1.056, loss_scale=8, train_wall=292, gb_free=19.9, wall=66163
2022-03-07 07:09:25 | INFO | train_inner | epoch 107:    168 / 196 loss=2.84, nll_loss=2.325, ppl=5.01, wps=20958.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=20800, lr=0.000219265, gnorm=1.067, loss_scale=16, train_wall=290, gb_free=19.9, wall=66475
2022-03-07 07:10:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:10:57 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 10.371 | nll_loss 10.017 | ppl 1035.84 | wps 40537.2 | wpb 510.9 | bsz 1 | num_updates 20828 | best_loss 7.054
2022-03-07 07:10:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 20828 updates
2022-03-07 07:10:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 07:11:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 07:11:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 107 @ 20828 updates, score 10.371) (writing took 3.5759267969988286 seconds)
2022-03-07 07:11:01 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-07 07:11:01 | INFO | train | epoch 107 | loss 2.818 | nll_loss 2.303 | ppl 4.93 | wps 20647.9 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 20828 | lr 0.000219117 | gnorm 1.063 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 66571
2022-03-07 07:11:01 | INFO | fairseq.trainer | begin training epoch 108
2022-03-07 07:11:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:14:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:14:49 | INFO | train_inner | epoch 108:     73 / 196 loss=2.79, nll_loss=2.274, ppl=4.84, wps=20164.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=20900, lr=0.000218739, gnorm=1.058, loss_scale=16, train_wall=292, gb_free=19.9, wall=66800
2022-03-07 07:20:02 | INFO | train_inner | epoch 108:    173 / 196 loss=2.834, nll_loss=2.319, ppl=4.99, wps=20941.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=1.067, loss_scale=16, train_wall=290, gb_free=19.9, wall=67113
2022-03-07 07:21:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:21:19 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 10.347 | nll_loss 9.992 | ppl 1018.11 | wps 40510.6 | wpb 510.9 | bsz 1 | num_updates 21023 | best_loss 7.054
2022-03-07 07:21:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 21023 updates
2022-03-07 07:21:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 07:21:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 07:21:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 108 @ 21023 updates, score 10.347) (writing took 3.5175783657468855 seconds)
2022-03-07 07:21:22 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-07 07:21:22 | INFO | train | epoch 108 | loss 2.808 | nll_loss 2.292 | ppl 4.9 | wps 20537.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21023 | lr 0.000218098 | gnorm 1.06 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 67193
2022-03-07 07:21:22 | INFO | fairseq.trainer | begin training epoch 109
2022-03-07 07:21:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:21:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:25:26 | INFO | train_inner | epoch 109:     78 / 196 loss=2.769, nll_loss=2.252, ppl=4.76, wps=20160.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=1.063, loss_scale=16, train_wall=292, gb_free=19.9, wall=67437
2022-03-07 07:29:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:30:42 | INFO | train_inner | epoch 109:    179 / 196 loss=2.83, nll_loss=2.316, ppl=4.98, wps=20731.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=21200, lr=0.000217186, gnorm=1.071, loss_scale=16, train_wall=293, gb_free=19.9, wall=67753
2022-03-07 07:31:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:31:40 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 10.43 | nll_loss 10.076 | ppl 1079.15 | wps 40801.2 | wpb 510.9 | bsz 1 | num_updates 21217 | best_loss 7.054
2022-03-07 07:31:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 21217 updates
2022-03-07 07:31:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 07:31:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 07:31:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 109 @ 21217 updates, score 10.43) (writing took 3.4139758269302547 seconds)
2022-03-07 07:31:43 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-07 07:31:43 | INFO | train | epoch 109 | loss 2.799 | nll_loss 2.282 | ppl 4.87 | wps 20433.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 21217 | lr 0.000217099 | gnorm 1.068 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 67814
2022-03-07 07:31:43 | INFO | fairseq.trainer | begin training epoch 110
2022-03-07 07:31:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:36:03 | INFO | train_inner | epoch 110:     83 / 196 loss=2.767, nll_loss=2.249, ppl=4.76, wps=20378.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21300, lr=0.000216676, gnorm=1.053, loss_scale=32, train_wall=289, gb_free=19.9, wall=68074
2022-03-07 07:37:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:41:19 | INFO | train_inner | epoch 110:    184 / 196 loss=2.821, nll_loss=2.306, ppl=4.95, wps=20738.4, ups=0.32, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=1.072, loss_scale=16, train_wall=293, gb_free=19.9, wall=68390
2022-03-07 07:41:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:42:01 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 10.379 | nll_loss 10.021 | ppl 1039.06 | wps 40725 | wpb 510.9 | bsz 1 | num_updates 21412 | best_loss 7.054
2022-03-07 07:42:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 21412 updates
2022-03-07 07:42:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 07:42:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 07:42:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 110 @ 21412 updates, score 10.379) (writing took 3.4285973659716547 seconds)
2022-03-07 07:42:05 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-07 07:42:05 | INFO | train | epoch 110 | loss 2.791 | nll_loss 2.275 | ppl 4.84 | wps 20544.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21412 | lr 0.000216108 | gnorm 1.066 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 68435
2022-03-07 07:42:05 | INFO | fairseq.trainer | begin training epoch 111
2022-03-07 07:42:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:44:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:46:43 | INFO | train_inner | epoch 111:     89 / 196 loss=2.751, nll_loss=2.233, ppl=4.7, wps=20172.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=1.078, loss_scale=16, train_wall=292, gb_free=19.9, wall=68714
2022-03-07 07:51:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:51:59 | INFO | train_inner | epoch 111:    190 / 196 loss=2.821, nll_loss=2.306, ppl=4.95, wps=20740.4, ups=0.32, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=1.075, loss_scale=16, train_wall=293, gb_free=19.9, wall=69030
2022-03-07 07:52:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:52:22 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 10.448 | nll_loss 10.094 | ppl 1093.15 | wps 40636.1 | wpb 510.9 | bsz 1 | num_updates 21606 | best_loss 7.054
2022-03-07 07:52:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 21606 updates
2022-03-07 07:52:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 07:52:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 07:52:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 111 @ 21606 updates, score 10.448) (writing took 3.453193943016231 seconds)
2022-03-07 07:52:26 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-07 07:52:26 | INFO | train | epoch 111 | loss 2.783 | nll_loss 2.266 | ppl 4.81 | wps 20436.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 21606 | lr 0.000215136 | gnorm 1.072 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 69056
2022-03-07 07:52:26 | INFO | fairseq.trainer | begin training epoch 112
2022-03-07 07:52:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:57:20 | INFO | train_inner | epoch 112:     94 / 196 loss=2.743, nll_loss=2.224, ppl=4.67, wps=20362.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21700, lr=0.000214669, gnorm=1.064, loss_scale=16, train_wall=290, gb_free=19.9, wall=69351
2022-03-07 07:58:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:02:36 | INFO | train_inner | epoch 112:    195 / 196 loss=2.81, nll_loss=2.294, ppl=4.91, wps=20746.5, ups=0.32, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=1.07, loss_scale=16, train_wall=293, gb_free=19.9, wall=69666
2022-03-07 08:02:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:02:44 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 10.432 | nll_loss 10.078 | ppl 1080.68 | wps 40947.8 | wpb 510.9 | bsz 1 | num_updates 21801 | best_loss 7.054
2022-03-07 08:02:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 21801 updates
2022-03-07 08:02:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 08:02:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 08:02:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 112 @ 21801 updates, score 10.432) (writing took 3.4330749809741974 seconds)
2022-03-07 08:02:47 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-07 08:02:47 | INFO | train | epoch 112 | loss 2.775 | nll_loss 2.257 | ppl 4.78 | wps 20542.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21801 | lr 0.000214172 | gnorm 1.067 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 69678
2022-03-07 08:02:47 | INFO | fairseq.trainer | begin training epoch 113
2022-03-07 08:02:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:05:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:08:00 | INFO | train_inner | epoch 113:    100 / 196 loss=2.73, nll_loss=2.211, ppl=4.63, wps=20167.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=21900, lr=0.000213687, gnorm=1.067, loss_scale=16, train_wall=292, gb_free=19.9, wall=69991
2022-03-07 08:13:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:13:05 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 10.503 | nll_loss 10.149 | ppl 1135.14 | wps 40384 | wpb 510.9 | bsz 1 | num_updates 21996 | best_loss 7.054
2022-03-07 08:13:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 21996 updates
2022-03-07 08:13:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 08:13:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 08:13:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 113 @ 21996 updates, score 10.503) (writing took 3.356801628600806 seconds)
2022-03-07 08:13:09 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-07 08:13:09 | INFO | train | epoch 113 | loss 2.768 | nll_loss 2.25 | ppl 4.76 | wps 20539.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21996 | lr 0.00021322 | gnorm 1.069 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 70299
2022-03-07 08:13:09 | INFO | fairseq.trainer | begin training epoch 114
2022-03-07 08:13:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:13:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:13:24 | INFO | train_inner | epoch 114:      5 / 196 loss=2.802, nll_loss=2.287, ppl=4.88, wps=20162.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22000, lr=0.000213201, gnorm=1.072, loss_scale=16, train_wall=293, gb_free=19.9, wall=70315
2022-03-07 08:18:37 | INFO | train_inner | epoch 114:    105 / 196 loss=2.719, nll_loss=2.198, ppl=4.59, wps=20950.8, ups=0.32, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=1.07, loss_scale=16, train_wall=290, gb_free=19.9, wall=70628
2022-03-07 08:20:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:23:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:23:26 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 10.488 | nll_loss 10.132 | ppl 1122.17 | wps 39998.7 | wpb 510.9 | bsz 1 | num_updates 22190 | best_loss 7.054
2022-03-07 08:23:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 22190 updates
2022-03-07 08:23:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 08:23:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 08:23:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 114 @ 22190 updates, score 10.488) (writing took 3.405435933265835 seconds)
2022-03-07 08:23:30 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-07 08:23:30 | INFO | train | epoch 114 | loss 2.759 | nll_loss 2.241 | ppl 4.73 | wps 20435.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22190 | lr 0.000212286 | gnorm 1.074 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 70920
2022-03-07 08:23:30 | INFO | fairseq.trainer | begin training epoch 115
2022-03-07 08:23:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:24:01 | INFO | train_inner | epoch 115:     10 / 196 loss=2.793, nll_loss=2.277, ppl=4.85, wps=20164.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22200, lr=0.000212238, gnorm=1.074, loss_scale=16, train_wall=293, gb_free=19.9, wall=70952
2022-03-07 08:27:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:29:17 | INFO | train_inner | epoch 115:    111 / 196 loss=2.725, nll_loss=2.206, ppl=4.61, wps=20754.5, ups=0.32, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=1.072, loss_scale=16, train_wall=293, gb_free=19.9, wall=71268
2022-03-07 08:33:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:33:48 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 10.534 | nll_loss 10.182 | ppl 1161.9 | wps 40299.4 | wpb 510.9 | bsz 1 | num_updates 22385 | best_loss 7.054
2022-03-07 08:33:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 22385 updates
2022-03-07 08:33:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 08:33:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 08:33:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 115 @ 22385 updates, score 10.534) (writing took 3.4503700551576912 seconds)
2022-03-07 08:33:51 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-07 08:33:51 | INFO | train | epoch 115 | loss 2.752 | nll_loss 2.234 | ppl 4.7 | wps 20540.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22385 | lr 0.000211359 | gnorm 1.075 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 71542
2022-03-07 08:33:51 | INFO | fairseq.trainer | begin training epoch 116
2022-03-07 08:33:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:34:38 | INFO | train_inner | epoch 116:     15 / 196 loss=2.774, nll_loss=2.257, ppl=4.78, wps=20345.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22400, lr=0.000211289, gnorm=1.081, loss_scale=32, train_wall=290, gb_free=19.9, wall=71589
2022-03-07 08:35:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:39:54 | INFO | train_inner | epoch 116:    116 / 196 loss=2.718, nll_loss=2.198, ppl=4.59, wps=20739.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=22500, lr=0.000210819, gnorm=1.064, loss_scale=16, train_wall=293, gb_free=19.9, wall=71905
2022-03-07 08:42:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:44:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:44:09 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 10.522 | nll_loss 10.171 | ppl 1152.72 | wps 40575.1 | wpb 510.9 | bsz 1 | num_updates 22579 | best_loss 7.054
2022-03-07 08:44:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 22579 updates
2022-03-07 08:44:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 08:44:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 08:44:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 116 @ 22579 updates, score 10.522) (writing took 3.6896705529652536 seconds)
2022-03-07 08:44:13 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-07 08:44:13 | INFO | train | epoch 116 | loss 2.744 | nll_loss 2.225 | ppl 4.68 | wps 20416.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22579 | lr 0.000210449 | gnorm 1.078 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 72164
2022-03-07 08:44:13 | INFO | fairseq.trainer | begin training epoch 117
2022-03-07 08:44:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:45:19 | INFO | train_inner | epoch 117:     21 / 196 loss=2.762, nll_loss=2.244, ppl=4.74, wps=20137.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22600, lr=0.000210352, gnorm=1.093, loss_scale=16, train_wall=293, gb_free=19.9, wall=72229
2022-03-07 08:49:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:50:35 | INFO | train_inner | epoch 117:    122 / 196 loss=2.722, nll_loss=2.202, ppl=4.6, wps=20746.1, ups=0.32, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=1.081, loss_scale=16, train_wall=293, gb_free=19.9, wall=72545
2022-03-07 08:54:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:54:31 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 10.558 | nll_loss 10.204 | ppl 1179.89 | wps 40555 | wpb 510.9 | bsz 1 | num_updates 22774 | best_loss 7.054
2022-03-07 08:54:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 22774 updates
2022-03-07 08:54:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 08:54:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 08:54:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 117 @ 22774 updates, score 10.558) (writing took 3.4152637608349323 seconds)
2022-03-07 08:54:34 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-07 08:54:34 | INFO | train | epoch 117 | loss 2.736 | nll_loss 2.217 | ppl 4.65 | wps 20544.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22774 | lr 0.000209546 | gnorm 1.085 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 72785
2022-03-07 08:54:34 | INFO | fairseq.trainer | begin training epoch 118
2022-03-07 08:54:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:55:56 | INFO | train_inner | epoch 118:     26 / 196 loss=2.744, nll_loss=2.226, ppl=4.68, wps=20365.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=22800, lr=0.000209427, gnorm=1.087, loss_scale=16, train_wall=290, gb_free=19.9, wall=72866
2022-03-07 08:56:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:01:11 | INFO | train_inner | epoch 118:    127 / 196 loss=2.718, nll_loss=2.198, ppl=4.59, wps=20758.6, ups=0.32, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=1.086, loss_scale=16, train_wall=293, gb_free=19.9, wall=73182
2022-03-07 09:03:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:04:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:04:52 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 10.573 | nll_loss 10.219 | ppl 1192.01 | wps 40657.6 | wpb 510.9 | bsz 1 | num_updates 22968 | best_loss 7.054
2022-03-07 09:04:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 22968 updates
2022-03-07 09:04:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 09:04:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 09:04:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 118 @ 22968 updates, score 10.573) (writing took 3.295526118017733 seconds)
2022-03-07 09:04:55 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-07 09:04:55 | INFO | train | epoch 118 | loss 2.729 | nll_loss 2.21 | ppl 4.63 | wps 20449.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22968 | lr 0.00020866 | gnorm 1.085 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 73406
2022-03-07 09:04:55 | INFO | fairseq.trainer | begin training epoch 119
2022-03-07 09:04:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:06:35 | INFO | train_inner | epoch 119:     32 / 196 loss=2.738, nll_loss=2.219, ppl=4.66, wps=20172.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23000, lr=0.000208514, gnorm=1.08, loss_scale=16, train_wall=293, gb_free=19.9, wall=73506
2022-03-07 09:10:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:11:51 | INFO | train_inner | epoch 119:    133 / 196 loss=2.712, nll_loss=2.191, ppl=4.57, wps=20748.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=23100, lr=0.000208063, gnorm=1.098, loss_scale=16, train_wall=293, gb_free=19.9, wall=73822
2022-03-07 09:15:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:15:13 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 10.584 | nll_loss 10.23 | ppl 1201.38 | wps 40672.5 | wpb 510.9 | bsz 1 | num_updates 23163 | best_loss 7.054
2022-03-07 09:15:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 23163 updates
2022-03-07 09:15:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 09:15:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 09:15:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 119 @ 23163 updates, score 10.584) (writing took 3.3509885109961033 seconds)
2022-03-07 09:15:16 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-07 09:15:16 | INFO | train | epoch 119 | loss 2.722 | nll_loss 2.202 | ppl 4.6 | wps 20543.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 23163 | lr 0.000207779 | gnorm 1.089 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 74027
2022-03-07 09:15:16 | INFO | fairseq.trainer | begin training epoch 120
2022-03-07 09:15:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:17:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:17:15 | INFO | train_inner | epoch 120:     38 / 196 loss=2.73, nll_loss=2.211, ppl=4.63, wps=20171.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23200, lr=0.000207614, gnorm=1.089, loss_scale=16, train_wall=293, gb_free=19.9, wall=74146
2022-03-07 09:22:28 | INFO | train_inner | epoch 120:    138 / 196 loss=2.706, nll_loss=2.185, ppl=4.55, wps=20955.2, ups=0.32, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=1.068, loss_scale=16, train_wall=290, gb_free=19.9, wall=74459
2022-03-07 09:24:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:25:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:25:34 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 10.593 | nll_loss 10.239 | ppl 1208.55 | wps 40675.3 | wpb 510.9 | bsz 1 | num_updates 23357 | best_loss 7.054
2022-03-07 09:25:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 23357 updates
2022-03-07 09:25:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 09:25:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 09:25:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 120 @ 23357 updates, score 10.593) (writing took 3.3459963998757303 seconds)
2022-03-07 09:25:37 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-07 09:25:37 | INFO | train | epoch 120 | loss 2.715 | nll_loss 2.195 | ppl 4.58 | wps 20442.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23357 | lr 0.000206915 | gnorm 1.08 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 74648
2022-03-07 09:25:37 | INFO | fairseq.trainer | begin training epoch 121
2022-03-07 09:25:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:27:52 | INFO | train_inner | epoch 121:     43 / 196 loss=2.713, nll_loss=2.193, ppl=4.57, wps=20171.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=23400, lr=0.000206725, gnorm=1.091, loss_scale=16, train_wall=293, gb_free=19.9, wall=74783
2022-03-07 09:31:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:33:08 | INFO | train_inner | epoch 121:    144 / 196 loss=2.707, nll_loss=2.186, ppl=4.55, wps=20746.1, ups=0.32, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=1.09, loss_scale=16, train_wall=293, gb_free=19.9, wall=75099
2022-03-07 09:35:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:35:55 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 10.565 | nll_loss 10.21 | ppl 1184.61 | wps 40696 | wpb 510.9 | bsz 1 | num_updates 23552 | best_loss 7.054
2022-03-07 09:35:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 23552 updates
2022-03-07 09:35:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 09:35:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 09:35:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 121 @ 23552 updates, score 10.565) (writing took 3.329626115038991 seconds)
2022-03-07 09:35:59 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-07 09:35:59 | INFO | train | epoch 121 | loss 2.708 | nll_loss 2.188 | ppl 4.56 | wps 20543.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 23552 | lr 0.000206056 | gnorm 1.091 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 75269
2022-03-07 09:35:59 | INFO | fairseq.trainer | begin training epoch 122
2022-03-07 09:35:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:37:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:38:32 | INFO | train_inner | epoch 122:     49 / 196 loss=2.702, nll_loss=2.181, ppl=4.54, wps=20173.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23600, lr=0.000205847, gnorm=1.09, loss_scale=16, train_wall=293, gb_free=19.9, wall=75423
2022-03-07 09:43:45 | INFO | train_inner | epoch 122:    149 / 196 loss=2.706, nll_loss=2.185, ppl=4.55, wps=20964.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=23700, lr=0.000205412, gnorm=1.1, loss_scale=16, train_wall=290, gb_free=19.9, wall=75735
2022-03-07 09:45:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:46:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:46:16 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 10.612 | nll_loss 10.261 | ppl 1227.17 | wps 40656.2 | wpb 510.9 | bsz 1 | num_updates 23746 | best_loss 7.054
2022-03-07 09:46:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 23746 updates
2022-03-07 09:46:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 09:46:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 09:46:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 122 @ 23746 updates, score 10.612) (writing took 3.3000416168943048 seconds)
2022-03-07 09:46:19 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-07 09:46:19 | INFO | train | epoch 122 | loss 2.701 | nll_loss 2.18 | ppl 4.53 | wps 20456.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23746 | lr 0.000205213 | gnorm 1.091 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 75890
2022-03-07 09:46:19 | INFO | fairseq.trainer | begin training epoch 123
2022-03-07 09:46:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:49:08 | INFO | train_inner | epoch 123:     54 / 196 loss=2.697, nll_loss=2.176, ppl=4.52, wps=20210.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=23800, lr=0.00020498, gnorm=1.073, loss_scale=16, train_wall=292, gb_free=19.9, wall=76059
2022-03-07 09:52:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:54:23 | INFO | train_inner | epoch 123:    155 / 196 loss=2.696, nll_loss=2.175, ppl=4.52, wps=20779.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=23900, lr=0.000204551, gnorm=1.097, loss_scale=16, train_wall=293, gb_free=19.9, wall=76374
2022-03-07 09:56:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:56:36 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 10.643 | nll_loss 10.29 | ppl 1251.55 | wps 40767.5 | wpb 510.9 | bsz 1 | num_updates 23941 | best_loss 7.054
2022-03-07 09:56:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 23941 updates
2022-03-07 09:56:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 09:56:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 09:56:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 123 @ 23941 updates, score 10.643) (writing took 3.2793989977799356 seconds)
2022-03-07 09:56:39 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-07 09:56:39 | INFO | train | epoch 123 | loss 2.694 | nll_loss 2.172 | ppl 4.51 | wps 20586.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 23941 | lr 0.000204376 | gnorm 1.088 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 76510
2022-03-07 09:56:39 | INFO | fairseq.trainer | begin training epoch 124
2022-03-07 09:56:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:59:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:59:47 | INFO | train_inner | epoch 124:     60 / 196 loss=2.684, nll_loss=2.163, ppl=4.48, wps=20211.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=24000, lr=0.000204124, gnorm=1.096, loss_scale=16, train_wall=292, gb_free=19.9, wall=76697
2022-03-07 10:04:59 | INFO | train_inner | epoch 124:    160 / 196 loss=2.698, nll_loss=2.177, ppl=4.52, wps=20979.8, ups=0.32, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=1.099, loss_scale=16, train_wall=290, gb_free=19.9, wall=77010
2022-03-07 10:06:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:06:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:06:56 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 10.639 | nll_loss 10.287 | ppl 1249.79 | wps 40901.9 | wpb 510.9 | bsz 1 | num_updates 24135 | best_loss 7.054
2022-03-07 10:06:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 24135 updates
2022-03-07 10:06:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 10:06:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 10:06:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 124 @ 24135 updates, score 10.639) (writing took 3.253662556875497 seconds)
2022-03-07 10:06:59 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-07 10:06:59 | INFO | train | epoch 124 | loss 2.689 | nll_loss 2.167 | ppl 4.49 | wps 20474.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 24135 | lr 0.000203552 | gnorm 1.098 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 77130
2022-03-07 10:06:59 | INFO | fairseq.trainer | begin training epoch 125
2022-03-07 10:06:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:10:23 | INFO | train_inner | epoch 125:     65 / 196 loss=2.674, nll_loss=2.152, ppl=4.44, wps=20212.7, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=24200, lr=0.000203279, gnorm=1.094, loss_scale=16, train_wall=292, gb_free=19.9, wall=77333
2022-03-07 10:13:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:15:38 | INFO | train_inner | epoch 125:    166 / 196 loss=2.692, nll_loss=2.17, ppl=4.5, wps=20778.7, ups=0.32, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=1.101, loss_scale=16, train_wall=293, gb_free=19.9, wall=77648
2022-03-07 10:17:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:17:16 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 10.654 | nll_loss 10.301 | ppl 1261.43 | wps 40925.6 | wpb 510.9 | bsz 1 | num_updates 24330 | best_loss 7.054
2022-03-07 10:17:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 24330 updates
2022-03-07 10:17:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 10:17:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 10:17:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 125 @ 24330 updates, score 10.654) (writing took 3.255970241036266 seconds)
2022-03-07 10:17:19 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-07 10:17:19 | INFO | train | epoch 125 | loss 2.681 | nll_loss 2.159 | ppl 4.47 | wps 20582.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24330 | lr 0.000202735 | gnorm 1.103 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 77750
2022-03-07 10:17:19 | INFO | fairseq.trainer | begin training epoch 126
2022-03-07 10:17:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:20:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:21:01 | INFO | train_inner | epoch 126:     71 / 196 loss=2.659, nll_loss=2.136, ppl=4.4, wps=20213.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=24400, lr=0.000202444, gnorm=1.098, loss_scale=16, train_wall=292, gb_free=19.9, wall=77972
2022-03-07 10:26:14 | INFO | train_inner | epoch 126:    171 / 196 loss=2.696, nll_loss=2.175, ppl=4.52, wps=20979.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=24500, lr=0.000202031, gnorm=1.096, loss_scale=16, train_wall=290, gb_free=19.9, wall=78284
2022-03-07 10:27:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:27:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:27:36 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 10.668 | nll_loss 10.316 | ppl 1274.97 | wps 40611.7 | wpb 510.9 | bsz 1 | num_updates 24524 | best_loss 7.054
2022-03-07 10:27:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 24524 updates
2022-03-07 10:27:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 10:27:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 10:27:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 126 @ 24524 updates, score 10.668) (writing took 3.380206068046391 seconds)
2022-03-07 10:27:40 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-07 10:27:40 | INFO | train | epoch 126 | loss 2.674 | nll_loss 2.152 | ppl 4.44 | wps 20466.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 24524 | lr 0.000201932 | gnorm 1.092 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 78370
2022-03-07 10:27:40 | INFO | fairseq.trainer | begin training epoch 127
2022-03-07 10:27:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:31:37 | INFO | train_inner | epoch 127:     76 / 196 loss=2.649, nll_loss=2.126, ppl=4.37, wps=20187.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=24600, lr=0.000201619, gnorm=1.085, loss_scale=16, train_wall=292, gb_free=19.9, wall=78608
2022-03-07 10:34:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:36:53 | INFO | train_inner | epoch 127:    177 / 196 loss=2.691, nll_loss=2.169, ppl=4.5, wps=20775.2, ups=0.32, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=1.106, loss_scale=16, train_wall=293, gb_free=19.9, wall=78923
2022-03-07 10:37:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:37:57 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 10.674 | nll_loss 10.323 | ppl 1280.66 | wps 40709.1 | wpb 510.9 | bsz 1 | num_updates 24719 | best_loss 7.054
2022-03-07 10:37:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 24719 updates
2022-03-07 10:37:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 10:38:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 10:38:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 127 @ 24719 updates, score 10.674) (writing took 3.3971872027032077 seconds)
2022-03-07 10:38:00 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-07 10:38:00 | INFO | train | epoch 127 | loss 2.669 | nll_loss 2.146 | ppl 4.43 | wps 20568.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24719 | lr 0.000201134 | gnorm 1.098 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 78991
2022-03-07 10:38:00 | INFO | fairseq.trainer | begin training epoch 128
2022-03-07 10:38:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:41:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:42:17 | INFO | train_inner | epoch 128:     82 / 196 loss=2.639, nll_loss=2.115, ppl=4.33, wps=20197.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=24800, lr=0.000200805, gnorm=1.088, loss_scale=16, train_wall=292, gb_free=19.9, wall=79247
2022-03-07 10:47:29 | INFO | train_inner | epoch 128:    182 / 196 loss=2.69, nll_loss=2.169, ppl=4.5, wps=20974.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=24900, lr=0.000200401, gnorm=1.107, loss_scale=16, train_wall=290, gb_free=19.9, wall=79560
2022-03-07 10:48:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:48:17 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 10.692 | nll_loss 10.34 | ppl 1295.74 | wps 40828.4 | wpb 510.9 | bsz 1 | num_updates 24914 | best_loss 7.054
2022-03-07 10:48:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 24914 updates
2022-03-07 10:48:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 10:48:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 10:48:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 128 @ 24914 updates, score 10.692) (writing took 3.403304222971201 seconds)
2022-03-07 10:48:21 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-07 10:48:21 | INFO | train | epoch 128 | loss 2.663 | nll_loss 2.14 | ppl 4.41 | wps 20570.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24914 | lr 0.000200345 | gnorm 1.098 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 79611
2022-03-07 10:48:21 | INFO | fairseq.trainer | begin training epoch 129
2022-03-07 10:48:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:48:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:52:53 | INFO | train_inner | epoch 129:     87 / 196 loss=2.63, nll_loss=2.105, ppl=4.3, wps=20204.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=25000, lr=0.0002, gnorm=1.115, loss_scale=16, train_wall=292, gb_free=19.9, wall=79883
2022-03-07 10:55:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:58:08 | INFO | train_inner | epoch 129:    188 / 196 loss=2.687, nll_loss=2.166, ppl=4.49, wps=20775.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=25100, lr=0.000199601, gnorm=1.094, loss_scale=16, train_wall=293, gb_free=19.9, wall=80199
2022-03-07 10:58:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:58:38 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 10.732 | nll_loss 10.378 | ppl 1330.73 | wps 40638.8 | wpb 510.9 | bsz 1 | num_updates 25108 | best_loss 7.054
2022-03-07 10:58:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 25108 updates
2022-03-07 10:58:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 10:58:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 10:58:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 129 @ 25108 updates, score 10.732) (writing took 3.3672730340622365 seconds)
2022-03-07 10:58:41 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-07 10:58:41 | INFO | train | epoch 129 | loss 2.655 | nll_loss 2.132 | ppl 4.38 | wps 20470.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25108 | lr 0.000199569 | gnorm 1.103 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 80232
2022-03-07 10:58:41 | INFO | fairseq.trainer | begin training epoch 130
2022-03-07 10:58:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:02:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:03:31 | INFO | train_inner | epoch 130:     93 / 196 loss=2.617, nll_loss=2.092, ppl=4.26, wps=20202.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25200, lr=0.000199205, gnorm=1.095, loss_scale=16, train_wall=292, gb_free=19.9, wall=80522
2022-03-07 11:08:44 | INFO | train_inner | epoch 130:    193 / 196 loss=2.685, nll_loss=2.164, ppl=4.48, wps=20977.9, ups=0.32, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=1.121, loss_scale=16, train_wall=290, gb_free=19.9, wall=80834
2022-03-07 11:08:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:08:58 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 10.733 | nll_loss 10.381 | ppl 1333.41 | wps 40381.9 | wpb 510.9 | bsz 1 | num_updates 25303 | best_loss 7.054
2022-03-07 11:08:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 25303 updates
2022-03-07 11:08:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 11:09:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 11:09:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 130 @ 25303 updates, score 10.733) (writing took 3.402884758077562 seconds)
2022-03-07 11:09:01 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-07 11:09:01 | INFO | train | epoch 130 | loss 2.65 | nll_loss 2.126 | ppl 4.37 | wps 20570.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 25303 | lr 0.000198799 | gnorm 1.11 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 80852
2022-03-07 11:09:01 | INFO | fairseq.trainer | begin training epoch 131
2022-03-07 11:09:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:09:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:14:08 | INFO | train_inner | epoch 131:     98 / 196 loss=2.61, nll_loss=2.085, ppl=4.24, wps=20182.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25400, lr=0.000198419, gnorm=1.1, loss_scale=16, train_wall=292, gb_free=19.9, wall=81158
2022-03-07 11:16:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:19:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:19:19 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 10.762 | nll_loss 10.413 | ppl 1363.07 | wps 40697.4 | wpb 510.9 | bsz 1 | num_updates 25497 | best_loss 7.054
2022-03-07 11:19:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 25497 updates
2022-03-07 11:19:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 11:19:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 11:19:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 131 @ 25497 updates, score 10.762) (writing took 3.4066640702076256 seconds)
2022-03-07 11:19:22 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-07 11:19:22 | INFO | train | epoch 131 | loss 2.642 | nll_loss 2.118 | ppl 4.34 | wps 20459.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25497 | lr 0.000198041 | gnorm 1.099 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 81473
2022-03-07 11:19:22 | INFO | fairseq.trainer | begin training epoch 132
2022-03-07 11:19:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:19:31 | INFO | train_inner | epoch 132:      3 / 196 loss=2.673, nll_loss=2.151, ppl=4.44, wps=20195, ups=0.31, wpb=65367, bsz=127.7, num_updates=25500, lr=0.00019803, gnorm=1.102, loss_scale=16, train_wall=292, gb_free=19.9, wall=81482
2022-03-07 11:24:44 | INFO | train_inner | epoch 132:    103 / 196 loss=2.603, nll_loss=2.077, ppl=4.22, wps=20983, ups=0.32, wpb=65532.4, bsz=128, num_updates=25600, lr=0.000197642, gnorm=1.082, loss_scale=32, train_wall=290, gb_free=19.9, wall=81794
2022-03-07 11:24:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:29:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:29:39 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 10.758 | nll_loss 10.406 | ppl 1357.17 | wps 40518.4 | wpb 510.9 | bsz 1 | num_updates 25692 | best_loss 7.054
2022-03-07 11:29:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 25692 updates
2022-03-07 11:29:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 11:29:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 11:29:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 132 @ 25692 updates, score 10.758) (writing took 3.3901487793773413 seconds)
2022-03-07 11:29:42 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-07 11:29:42 | INFO | train | epoch 132 | loss 2.637 | nll_loss 2.113 | ppl 4.33 | wps 20569.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 25692 | lr 0.000197288 | gnorm 1.095 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 82093
2022-03-07 11:29:42 | INFO | fairseq.trainer | begin training epoch 133
2022-03-07 11:29:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:30:08 | INFO | train_inner | epoch 133:      8 / 196 loss=2.669, nll_loss=2.147, ppl=4.43, wps=20190.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=25700, lr=0.000197257, gnorm=1.108, loss_scale=16, train_wall=292, gb_free=19.9, wall=82118
2022-03-07 11:31:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:35:23 | INFO | train_inner | epoch 133:    109 / 196 loss=2.6, nll_loss=2.074, ppl=4.21, wps=20779.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=25800, lr=0.000196875, gnorm=1.117, loss_scale=16, train_wall=293, gb_free=19.9, wall=82433
2022-03-07 11:38:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:39:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:39:59 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 10.772 | nll_loss 10.42 | ppl 1370.22 | wps 40690.6 | wpb 510.9 | bsz 1 | num_updates 25886 | best_loss 7.054
2022-03-07 11:39:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 25886 updates
2022-03-07 11:39:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 11:40:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 11:40:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 133 @ 25886 updates, score 10.772) (writing took 3.394411761779338 seconds)
2022-03-07 11:40:03 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-07 11:40:03 | INFO | train | epoch 133 | loss 2.631 | nll_loss 2.107 | ppl 4.31 | wps 20467.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25886 | lr 0.000196548 | gnorm 1.111 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 82713
2022-03-07 11:40:03 | INFO | fairseq.trainer | begin training epoch 134
2022-03-07 11:40:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:40:47 | INFO | train_inner | epoch 134:     14 / 196 loss=2.657, nll_loss=2.134, ppl=4.39, wps=20194.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=25900, lr=0.000196494, gnorm=1.101, loss_scale=16, train_wall=292, gb_free=19.9, wall=82757
2022-03-07 11:45:59 | INFO | train_inner | epoch 134:    114 / 196 loss=2.602, nll_loss=2.076, ppl=4.22, wps=20980.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=26000, lr=0.000196116, gnorm=1.102, loss_scale=32, train_wall=290, gb_free=19.9, wall=83069
2022-03-07 11:46:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:50:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:50:20 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 10.8 | nll_loss 10.451 | ppl 1400.19 | wps 40532 | wpb 510.9 | bsz 1 | num_updates 26081 | best_loss 7.054
2022-03-07 11:50:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 26081 updates
2022-03-07 11:50:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 11:50:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 11:50:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 134 @ 26081 updates, score 10.8) (writing took 3.403810968156904 seconds)
2022-03-07 11:50:23 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-07 11:50:23 | INFO | train | epoch 134 | loss 2.625 | nll_loss 2.101 | ppl 4.29 | wps 20569.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 26081 | lr 0.000195811 | gnorm 1.103 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 83334
2022-03-07 11:50:23 | INFO | fairseq.trainer | begin training epoch 135
2022-03-07 11:50:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:51:23 | INFO | train_inner | epoch 135:     19 / 196 loss=2.643, nll_loss=2.12, ppl=4.35, wps=20194.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=26100, lr=0.00019574, gnorm=1.106, loss_scale=16, train_wall=292, gb_free=19.9, wall=83393
2022-03-07 11:53:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:56:38 | INFO | train_inner | epoch 135:    120 / 196 loss=2.599, nll_loss=2.073, ppl=4.21, wps=20783.4, ups=0.32, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=1.094, loss_scale=16, train_wall=293, gb_free=19.9, wall=83708
2022-03-07 12:00:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:00:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:00:40 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 10.816 | nll_loss 10.466 | ppl 1414.27 | wps 40792.5 | wpb 510.9 | bsz 1 | num_updates 26275 | best_loss 7.054
2022-03-07 12:00:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 26275 updates
2022-03-07 12:00:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 12:00:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 12:00:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 135 @ 26275 updates, score 10.816) (writing took 3.4346375996246934 seconds)
2022-03-07 12:00:43 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-07 12:00:43 | INFO | train | epoch 135 | loss 2.619 | nll_loss 2.095 | ppl 4.27 | wps 20471.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26275 | lr 0.000195087 | gnorm 1.106 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 83954
2022-03-07 12:00:43 | INFO | fairseq.trainer | begin training epoch 136
2022-03-07 12:00:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:02:02 | INFO | train_inner | epoch 136:     25 / 196 loss=2.632, nll_loss=2.108, ppl=4.31, wps=20192.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=26300, lr=0.000194994, gnorm=1.11, loss_scale=16, train_wall=292, gb_free=19.9, wall=84032
2022-03-07 12:07:14 | INFO | train_inner | epoch 136:    125 / 196 loss=2.601, nll_loss=2.076, ppl=4.22, wps=20973.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=26400, lr=0.000194625, gnorm=1.105, loss_scale=16, train_wall=290, gb_free=19.9, wall=84345
2022-03-07 12:07:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:10:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:11:01 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 10.859 | nll_loss 10.509 | ppl 1457.29 | wps 40593.7 | wpb 510.9 | bsz 1 | num_updates 26470 | best_loss 7.054
2022-03-07 12:11:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 26470 updates
2022-03-07 12:11:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 12:11:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 12:11:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 136 @ 26470 updates, score 10.859) (writing took 3.4205178329721093 seconds)
2022-03-07 12:11:04 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-07 12:11:04 | INFO | train | epoch 136 | loss 2.614 | nll_loss 2.09 | ppl 4.26 | wps 20562.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 26470 | lr 0.000194367 | gnorm 1.104 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 84575
2022-03-07 12:11:04 | INFO | fairseq.trainer | begin training epoch 137
2022-03-07 12:11:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:12:38 | INFO | train_inner | epoch 137:     30 / 196 loss=2.63, nll_loss=2.106, ppl=4.3, wps=20187.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=26500, lr=0.000194257, gnorm=1.114, loss_scale=16, train_wall=292, gb_free=19.9, wall=84668
2022-03-07 12:14:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:17:53 | INFO | train_inner | epoch 137:    131 / 196 loss=2.593, nll_loss=2.067, ppl=4.19, wps=20766.8, ups=0.32, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=1.097, loss_scale=16, train_wall=293, gb_free=19.9, wall=84984
2022-03-07 12:21:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:21:21 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 10.803 | nll_loss 10.451 | ppl 1399.66 | wps 40732.9 | wpb 510.9 | bsz 1 | num_updates 26665 | best_loss 7.054
2022-03-07 12:21:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 26665 updates
2022-03-07 12:21:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 12:21:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 12:21:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 137 @ 26665 updates, score 10.803) (writing took 3.4087218968197703 seconds)
2022-03-07 12:21:25 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-07 12:21:25 | INFO | train | epoch 137 | loss 2.608 | nll_loss 2.083 | ppl 4.24 | wps 20565 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 26665 | lr 0.000193655 | gnorm 1.107 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 85195
2022-03-07 12:21:25 | INFO | fairseq.trainer | begin training epoch 138
2022-03-07 12:21:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:21:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:23:17 | INFO | train_inner | epoch 138:     36 / 196 loss=2.617, nll_loss=2.092, ppl=4.26, wps=20194.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=26700, lr=0.000193528, gnorm=1.119, loss_scale=16, train_wall=292, gb_free=19.9, wall=85308
2022-03-07 12:28:29 | INFO | train_inner | epoch 138:    136 / 196 loss=2.596, nll_loss=2.07, ppl=4.2, wps=20982.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=26800, lr=0.000193167, gnorm=1.12, loss_scale=32, train_wall=290, gb_free=19.9, wall=85620
2022-03-07 12:28:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:31:42 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 10.818 | nll_loss 10.465 | ppl 1413.15 | wps 40665.4 | wpb 510.9 | bsz 1 | num_updates 26859 | best_loss 7.054
2022-03-07 12:31:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 26859 updates
2022-03-07 12:31:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 12:31:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 12:31:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 138 @ 26859 updates, score 10.818) (writing took 3.3703898061066866 seconds)
2022-03-07 12:31:45 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-07 12:31:45 | INFO | train | epoch 138 | loss 2.604 | nll_loss 2.078 | ppl 4.22 | wps 20470.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26859 | lr 0.000192955 | gnorm 1.116 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 85815
2022-03-07 12:31:45 | INFO | fairseq.trainer | begin training epoch 139
2022-03-07 12:31:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:33:53 | INFO | train_inner | epoch 139:     41 / 196 loss=2.603, nll_loss=2.078, ppl=4.22, wps=20203.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=26900, lr=0.000192807, gnorm=1.11, loss_scale=16, train_wall=292, gb_free=19.9, wall=85944
2022-03-07 12:35:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:39:08 | INFO | train_inner | epoch 139:    142 / 196 loss=2.594, nll_loss=2.068, ppl=4.19, wps=20773.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=27000, lr=0.00019245, gnorm=1.118, loss_scale=16, train_wall=293, gb_free=19.9, wall=86259
2022-03-07 12:41:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:42:02 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 10.855 | nll_loss 10.506 | ppl 1454.07 | wps 40534.6 | wpb 510.9 | bsz 1 | num_updates 27054 | best_loss 7.054
2022-03-07 12:42:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 27054 updates
2022-03-07 12:42:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 12:42:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 12:42:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 139 @ 27054 updates, score 10.855) (writing took 3.4586363211274147 seconds)
2022-03-07 12:42:05 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-07 12:42:05 | INFO | train | epoch 139 | loss 2.597 | nll_loss 2.072 | ppl 4.2 | wps 20571.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27054 | lr 0.000192258 | gnorm 1.118 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 86436
2022-03-07 12:42:05 | INFO | fairseq.trainer | begin training epoch 140
2022-03-07 12:42:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:42:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:44:32 | INFO | train_inner | epoch 140:     47 / 196 loss=2.596, nll_loss=2.07, ppl=4.2, wps=20190.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=27100, lr=0.000192095, gnorm=1.104, loss_scale=16, train_wall=292, gb_free=19.9, wall=86583
2022-03-07 12:49:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:49:48 | INFO | train_inner | epoch 140:    148 / 196 loss=2.599, nll_loss=2.073, ppl=4.21, wps=20770.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=27200, lr=0.000191741, gnorm=1.108, loss_scale=16, train_wall=293, gb_free=19.9, wall=86898
2022-03-07 12:52:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:52:22 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 10.859 | nll_loss 10.505 | ppl 1452.86 | wps 40758.8 | wpb 510.9 | bsz 1 | num_updates 27248 | best_loss 7.054
2022-03-07 12:52:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 27248 updates
2022-03-07 12:52:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 12:52:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 12:52:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 140 @ 27248 updates, score 10.859) (writing took 3.3812879067845643 seconds)
2022-03-07 12:52:26 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-07 12:52:26 | INFO | train | epoch 140 | loss 2.593 | nll_loss 2.067 | ppl 4.19 | wps 20464 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27248 | lr 0.000191572 | gnorm 1.111 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 87056
2022-03-07 12:52:26 | INFO | fairseq.trainer | begin training epoch 141
2022-03-07 12:52:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:55:08 | INFO | train_inner | epoch 141:     52 / 196 loss=2.583, nll_loss=2.056, ppl=4.16, wps=20396.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=27300, lr=0.00019139, gnorm=1.119, loss_scale=16, train_wall=289, gb_free=19.9, wall=87219
2022-03-07 12:56:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:00:24 | INFO | train_inner | epoch 141:    153 / 196 loss=2.597, nll_loss=2.072, ppl=4.2, wps=20770.6, ups=0.32, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=1.107, loss_scale=16, train_wall=293, gb_free=19.9, wall=87534
2022-03-07 13:02:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:02:43 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 10.911 | nll_loss 10.561 | ppl 1510.3 | wps 40544 | wpb 510.9 | bsz 1 | num_updates 27443 | best_loss 7.054
2022-03-07 13:02:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 27443 updates
2022-03-07 13:02:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 13:02:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 13:02:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 141 @ 27443 updates, score 10.911) (writing took 3.3990005650557578 seconds)
2022-03-07 13:02:46 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-07 13:02:46 | INFO | train | epoch 141 | loss 2.588 | nll_loss 2.062 | ppl 4.17 | wps 20572 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27443 | lr 0.00019089 | gnorm 1.113 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 87677
2022-03-07 13:02:46 | INFO | fairseq.trainer | begin training epoch 142
2022-03-07 13:02:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:03:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:05:47 | INFO | train_inner | epoch 142:     58 / 196 loss=2.576, nll_loss=2.049, ppl=4.14, wps=20197.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=27500, lr=0.000190693, gnorm=1.107, loss_scale=16, train_wall=292, gb_free=19.9, wall=87858
2022-03-07 13:10:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:11:03 | INFO | train_inner | epoch 142:    159 / 196 loss=2.589, nll_loss=2.063, ppl=4.18, wps=20768.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=27600, lr=0.000190347, gnorm=1.111, loss_scale=16, train_wall=293, gb_free=19.9, wall=88173
2022-03-07 13:12:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:13:03 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 10.925 | nll_loss 10.577 | ppl 1527.79 | wps 40608.9 | wpb 510.9 | bsz 1 | num_updates 27637 | best_loss 7.054
2022-03-07 13:13:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 27637 updates
2022-03-07 13:13:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 13:13:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 13:13:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 142 @ 27637 updates, score 10.925) (writing took 3.426793124061078 seconds)
2022-03-07 13:13:07 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-07 13:13:07 | INFO | train | epoch 142 | loss 2.58 | nll_loss 2.054 | ppl 4.15 | wps 20464.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27637 | lr 0.000190219 | gnorm 1.107 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 88297
2022-03-07 13:13:07 | INFO | fairseq.trainer | begin training epoch 143
2022-03-07 13:13:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:16:23 | INFO | train_inner | epoch 143:     63 / 196 loss=2.567, nll_loss=2.039, ppl=4.11, wps=20393.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=27700, lr=0.000190003, gnorm=1.117, loss_scale=16, train_wall=289, gb_free=19.9, wall=88494
2022-03-07 13:17:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:21:39 | INFO | train_inner | epoch 143:    164 / 196 loss=2.587, nll_loss=2.061, ppl=4.17, wps=20774.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=27800, lr=0.000189661, gnorm=1.115, loss_scale=16, train_wall=293, gb_free=19.9, wall=88809
2022-03-07 13:23:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:23:23 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 10.876 | nll_loss 10.529 | ppl 1477.22 | wps 40791.1 | wpb 510.9 | bsz 1 | num_updates 27832 | best_loss 7.054
2022-03-07 13:23:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 27832 updates
2022-03-07 13:23:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 13:23:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 13:23:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 143 @ 27832 updates, score 10.876) (writing took 3.4234667317941785 seconds)
2022-03-07 13:23:27 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-07 13:23:27 | INFO | train | epoch 143 | loss 2.576 | nll_loss 2.049 | ppl 4.14 | wps 20574.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27832 | lr 0.000189552 | gnorm 1.112 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 88917
2022-03-07 13:23:27 | INFO | fairseq.trainer | begin training epoch 144
2022-03-07 13:23:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:24:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:27:02 | INFO | train_inner | epoch 144:     69 / 196 loss=2.565, nll_loss=2.038, ppl=4.11, wps=20200.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=27900, lr=0.000189321, gnorm=1.108, loss_scale=16, train_wall=292, gb_free=19.9, wall=89133
2022-03-07 13:31:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:32:18 | INFO | train_inner | epoch 144:    170 / 196 loss=2.588, nll_loss=2.062, ppl=4.17, wps=20776.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=28000, lr=0.000188982, gnorm=1.136, loss_scale=16, train_wall=293, gb_free=19.9, wall=89448
2022-03-07 13:33:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:33:44 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 10.915 | nll_loss 10.566 | ppl 1516.4 | wps 40771.6 | wpb 510.9 | bsz 1 | num_updates 28026 | best_loss 7.054
2022-03-07 13:33:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 28026 updates
2022-03-07 13:33:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 13:33:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 13:33:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 144 @ 28026 updates, score 10.915) (writing took 3.400086037814617 seconds)
2022-03-07 13:33:47 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-07 13:33:47 | INFO | train | epoch 144 | loss 2.572 | nll_loss 2.045 | ppl 4.13 | wps 20469.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28026 | lr 0.000188895 | gnorm 1.126 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 89538
2022-03-07 13:33:47 | INFO | fairseq.trainer | begin training epoch 145
2022-03-07 13:33:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:37:38 | INFO | train_inner | epoch 145:     74 / 196 loss=2.55, nll_loss=2.022, ppl=4.06, wps=20391.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28100, lr=0.000188646, gnorm=1.115, loss_scale=16, train_wall=289, gb_free=19.9, wall=89769
2022-03-07 13:39:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:42:54 | INFO | train_inner | epoch 145:    175 / 196 loss=2.587, nll_loss=2.061, ppl=4.17, wps=20773.1, ups=0.32, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=1.132, loss_scale=16, train_wall=293, gb_free=19.9, wall=90084
2022-03-07 13:43:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:44:04 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 10.918 | nll_loss 10.568 | ppl 1518.3 | wps 40550.2 | wpb 510.9 | bsz 1 | num_updates 28221 | best_loss 7.054
2022-03-07 13:44:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 28221 updates
2022-03-07 13:44:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 13:44:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 13:44:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 145 @ 28221 updates, score 10.918) (writing took 3.3708482501097023 seconds)
2022-03-07 13:44:08 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-07 13:44:08 | INFO | train | epoch 145 | loss 2.567 | nll_loss 2.039 | ppl 4.11 | wps 20567.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28221 | lr 0.000188241 | gnorm 1.124 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 90158
2022-03-07 13:44:08 | INFO | fairseq.trainer | begin training epoch 146
2022-03-07 13:44:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:46:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:48:18 | INFO | train_inner | epoch 146:     80 / 196 loss=2.541, nll_loss=2.012, ppl=4.03, wps=20195.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=28300, lr=0.000187978, gnorm=1.133, loss_scale=16, train_wall=292, gb_free=19.9, wall=90408
2022-03-07 13:53:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:53:33 | INFO | train_inner | epoch 146:    181 / 196 loss=2.586, nll_loss=2.06, ppl=4.17, wps=20769.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=28400, lr=0.000187647, gnorm=1.147, loss_scale=16, train_wall=293, gb_free=19.9, wall=90724
2022-03-07 13:54:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:54:25 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 10.963 | nll_loss 10.616 | ppl 1569.3 | wps 40518.6 | wpb 510.9 | bsz 1 | num_updates 28415 | best_loss 7.054
2022-03-07 13:54:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 28415 updates
2022-03-07 13:54:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 13:54:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 13:54:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 146 @ 28415 updates, score 10.963) (writing took 3.4140696497634053 seconds)
2022-03-07 13:54:28 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-07 13:54:28 | INFO | train | epoch 146 | loss 2.561 | nll_loss 2.034 | ppl 4.1 | wps 20463.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28415 | lr 0.000187597 | gnorm 1.137 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 90779
2022-03-07 13:54:28 | INFO | fairseq.trainer | begin training epoch 147
2022-03-07 13:54:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:58:54 | INFO | train_inner | epoch 147:     85 / 196 loss=2.529, nll_loss=2, ppl=4, wps=20396.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=28500, lr=0.000187317, gnorm=1.113, loss_scale=16, train_wall=289, gb_free=19.9, wall=91044
2022-03-07 14:00:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:04:09 | INFO | train_inner | epoch 147:    186 / 196 loss=2.587, nll_loss=2.061, ppl=4.17, wps=20771.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=28600, lr=0.000186989, gnorm=1.115, loss_scale=16, train_wall=293, gb_free=19.9, wall=91360
2022-03-07 14:04:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:04:45 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 10.922 | nll_loss 10.575 | ppl 1525.71 | wps 40863.3 | wpb 510.9 | bsz 1 | num_updates 28610 | best_loss 7.054
2022-03-07 14:04:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 28610 updates
2022-03-07 14:04:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 14:04:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 14:04:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 147 @ 28610 updates, score 10.922) (writing took 3.397363386116922 seconds)
2022-03-07 14:04:48 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-07 14:04:48 | INFO | train | epoch 147 | loss 2.556 | nll_loss 2.028 | ppl 4.08 | wps 20574 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28610 | lr 0.000186957 | gnorm 1.114 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 91399
2022-03-07 14:04:48 | INFO | fairseq.trainer | begin training epoch 148
2022-03-07 14:04:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:07:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:09:33 | INFO | train_inner | epoch 148:     91 / 196 loss=2.526, nll_loss=1.997, ppl=3.99, wps=20201.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28700, lr=0.000186663, gnorm=1.115, loss_scale=16, train_wall=292, gb_free=19.9, wall=91683
2022-03-07 14:14:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:14:48 | INFO | train_inner | epoch 148:    192 / 196 loss=2.58, nll_loss=2.054, ppl=4.15, wps=20777.3, ups=0.32, wpb=65536, bsz=128, num_updates=28800, lr=0.000186339, gnorm=1.11, loss_scale=16, train_wall=293, gb_free=19.9, wall=91999
2022-03-07 14:15:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:15:05 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 10.908 | nll_loss 10.56 | ppl 1510.06 | wps 40540.5 | wpb 510.9 | bsz 1 | num_updates 28804 | best_loss 7.054
2022-03-07 14:15:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 28804 updates
2022-03-07 14:15:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 14:15:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 14:15:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 148 @ 28804 updates, score 10.908) (writing took 3.4173986739479005 seconds)
2022-03-07 14:15:09 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-07 14:15:09 | INFO | train | epoch 148 | loss 2.551 | nll_loss 2.023 | ppl 4.07 | wps 20469.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28804 | lr 0.000186326 | gnorm 1.111 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 92019
2022-03-07 14:15:09 | INFO | fairseq.trainer | begin training epoch 149
2022-03-07 14:15:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:20:09 | INFO | train_inner | epoch 149:     96 / 196 loss=2.51, nll_loss=1.98, ppl=3.94, wps=20390.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28900, lr=0.000186016, gnorm=1.114, loss_scale=16, train_wall=289, gb_free=19.9, wall=92319
2022-03-07 14:21:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:25:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:25:26 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 10.965 | nll_loss 10.616 | ppl 1569.95 | wps 40652.2 | wpb 510.9 | bsz 1 | num_updates 28999 | best_loss 7.054
2022-03-07 14:25:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 28999 updates
2022-03-07 14:25:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 14:25:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 14:25:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 149 @ 28999 updates, score 10.965) (writing took 3.502833562903106 seconds)
2022-03-07 14:25:29 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-07 14:25:29 | INFO | train | epoch 149 | loss 2.548 | nll_loss 2.02 | ppl 4.05 | wps 20567.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28999 | lr 0.000185699 | gnorm 1.118 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 92640
2022-03-07 14:25:29 | INFO | fairseq.trainer | begin training epoch 150
2022-03-07 14:25:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:25:32 | INFO | train_inner | epoch 150:      1 / 196 loss=2.588, nll_loss=2.062, ppl=4.18, wps=20188.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=29000, lr=0.000185695, gnorm=1.122, loss_scale=16, train_wall=292, gb_free=19.9, wall=92643
2022-03-07 14:28:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:30:48 | INFO | train_inner | epoch 150:    102 / 196 loss=2.51, nll_loss=1.98, ppl=3.95, wps=20778.3, ups=0.32, wpb=65536, bsz=128, num_updates=29100, lr=0.000185376, gnorm=1.117, loss_scale=16, train_wall=293, gb_free=19.9, wall=92958
2022-03-07 14:34:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:35:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:35:46 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 11.004 | nll_loss 10.659 | ppl 1616.37 | wps 40607.1 | wpb 510.9 | bsz 1 | num_updates 29193 | best_loss 7.054
2022-03-07 14:35:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 29193 updates
2022-03-07 14:35:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 14:35:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 14:35:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 150 @ 29193 updates, score 11.004) (writing took 3.3885085838846862 seconds)
2022-03-07 14:35:50 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-07 14:35:50 | INFO | train | epoch 150 | loss 2.542 | nll_loss 2.013 | ppl 4.04 | wps 20465.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 29193 | lr 0.00018508 | gnorm 1.122 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 93260
2022-03-07 14:35:50 | INFO | fairseq.trainer | begin training epoch 151
2022-03-07 14:35:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:36:12 | INFO | train_inner | epoch 151:      7 / 196 loss=2.568, nll_loss=2.041, ppl=4.12, wps=20192.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=29200, lr=0.000185058, gnorm=1.13, loss_scale=16, train_wall=292, gb_free=19.9, wall=93282
2022-03-07 14:41:24 | INFO | train_inner | epoch 151:    107 / 196 loss=2.503, nll_loss=1.973, ppl=3.93, wps=20981.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=29300, lr=0.000184742, gnorm=1.113, loss_scale=16, train_wall=290, gb_free=19.9, wall=93594
2022-03-07 14:42:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:46:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:46:07 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 10.983 | nll_loss 10.634 | ppl 1588.72 | wps 40577.6 | wpb 510.9 | bsz 1 | num_updates 29388 | best_loss 7.054
2022-03-07 14:46:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 29388 updates
2022-03-07 14:46:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 14:46:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 14:46:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 151 @ 29388 updates, score 10.983) (writing took 3.4178837691433728 seconds)
2022-03-07 14:46:10 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-07 14:46:10 | INFO | train | epoch 151 | loss 2.537 | nll_loss 2.008 | ppl 4.02 | wps 20571 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 29388 | lr 0.000184465 | gnorm 1.131 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 93881
2022-03-07 14:46:10 | INFO | fairseq.trainer | begin training epoch 152
2022-03-07 14:46:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:46:48 | INFO | train_inner | epoch 152:     12 / 196 loss=2.568, nll_loss=2.041, ppl=4.11, wps=20195.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=29400, lr=0.000184428, gnorm=1.143, loss_scale=16, train_wall=292, gb_free=19.9, wall=93918
2022-03-07 14:49:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:52:03 | INFO | train_inner | epoch 152:    113 / 196 loss=2.502, nll_loss=1.971, ppl=3.92, wps=20778.9, ups=0.32, wpb=65536, bsz=128, num_updates=29500, lr=0.000184115, gnorm=1.117, loss_scale=16, train_wall=293, gb_free=19.9, wall=94233
2022-03-07 14:55:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:56:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:56:27 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 11.014 | nll_loss 10.666 | ppl 1624.35 | wps 40820.1 | wpb 510.9 | bsz 1 | num_updates 29582 | best_loss 7.054
2022-03-07 14:56:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 29582 updates
2022-03-07 14:56:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 14:56:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 14:56:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 152 @ 29582 updates, score 11.014) (writing took 3.2484290837310255 seconds)
2022-03-07 14:56:30 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-03-07 14:56:30 | INFO | train | epoch 152 | loss 2.532 | nll_loss 2.004 | ppl 4.01 | wps 20478.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 29582 | lr 0.00018386 | gnorm 1.136 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 94501
2022-03-07 14:56:30 | INFO | fairseq.trainer | begin training epoch 153
2022-03-07 14:56:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:57:26 | INFO | train_inner | epoch 153:     18 / 196 loss=2.557, nll_loss=2.03, ppl=4.08, wps=20216, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=29600, lr=0.000183804, gnorm=1.156, loss_scale=16, train_wall=292, gb_free=19.9, wall=94557
2022-03-07 15:02:39 | INFO | train_inner | epoch 153:    118 / 196 loss=2.509, nll_loss=1.979, ppl=3.94, wps=20985.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=29700, lr=0.000183494, gnorm=1.108, loss_scale=16, train_wall=290, gb_free=19.9, wall=94869
2022-03-07 15:02:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:06:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:06:47 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 11.028 | nll_loss 10.683 | ppl 1644.3 | wps 40932.6 | wpb 510.9 | bsz 1 | num_updates 29777 | best_loss 7.054
2022-03-07 15:06:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 29777 updates
2022-03-07 15:06:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 15:06:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 15:06:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 153 @ 29777 updates, score 11.028) (writing took 3.2630184781737626 seconds)
2022-03-07 15:06:50 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-03-07 15:06:50 | INFO | train | epoch 153 | loss 2.527 | nll_loss 1.998 | ppl 3.99 | wps 20584.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 29777 | lr 0.000183257 | gnorm 1.119 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 95121
2022-03-07 15:06:50 | INFO | fairseq.trainer | begin training epoch 154
2022-03-07 15:06:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:08:02 | INFO | train_inner | epoch 154:     23 / 196 loss=2.541, nll_loss=2.013, ppl=4.04, wps=20212.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=29800, lr=0.000183186, gnorm=1.129, loss_scale=16, train_wall=292, gb_free=19.9, wall=95192
2022-03-07 15:09:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:13:17 | INFO | train_inner | epoch 154:    124 / 196 loss=2.506, nll_loss=1.976, ppl=3.93, wps=20769.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=29900, lr=0.000182879, gnorm=1.118, loss_scale=16, train_wall=293, gb_free=19.9, wall=95508
2022-03-07 15:16:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:17:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:17:07 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 11.003 | nll_loss 10.654 | ppl 1611.74 | wps 40891.7 | wpb 510.9 | bsz 1 | num_updates 29971 | best_loss 7.054
2022-03-07 15:17:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 29971 updates
2022-03-07 15:17:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 15:17:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 15:17:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 154 @ 29971 updates, score 11.003) (writing took 3.2806017748080194 seconds)
2022-03-07 15:17:10 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-03-07 15:17:10 | INFO | train | epoch 154 | loss 2.524 | nll_loss 1.995 | ppl 3.99 | wps 20469.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 29971 | lr 0.000182662 | gnorm 1.126 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 95741
2022-03-07 15:17:10 | INFO | fairseq.trainer | begin training epoch 155
2022-03-07 15:17:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:18:41 | INFO | train_inner | epoch 155:     29 / 196 loss=2.539, nll_loss=2.011, ppl=4.03, wps=20207.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=30000, lr=0.000182574, gnorm=1.136, loss_scale=16, train_wall=292, gb_free=19.9, wall=95832
2022-03-07 15:23:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:23:56 | INFO | train_inner | epoch 155:    130 / 196 loss=2.511, nll_loss=1.981, ppl=3.95, wps=20782.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=30100, lr=0.000182271, gnorm=1.139, loss_scale=16, train_wall=293, gb_free=19.9, wall=96147
2022-03-07 15:27:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:27:27 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 11.042 | nll_loss 10.694 | ppl 1656.77 | wps 40441 | wpb 510.9 | bsz 1 | num_updates 30166 | best_loss 7.054
2022-03-07 15:27:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 30166 updates
2022-03-07 15:27:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 15:27:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 15:27:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 155 @ 30166 updates, score 11.042) (writing took 3.2828511502593756 seconds)
2022-03-07 15:27:30 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-03-07 15:27:30 | INFO | train | epoch 155 | loss 2.521 | nll_loss 1.991 | ppl 3.98 | wps 20578.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 30166 | lr 0.000182071 | gnorm 1.133 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 96361
2022-03-07 15:27:30 | INFO | fairseq.trainer | begin training epoch 156
2022-03-07 15:27:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:29:17 | INFO | train_inner | epoch 156:     34 / 196 loss=2.531, nll_loss=2.003, ppl=4.01, wps=20399.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=30200, lr=0.000181969, gnorm=1.124, loss_scale=16, train_wall=289, gb_free=19.9, wall=96467
2022-03-07 15:30:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:34:32 | INFO | train_inner | epoch 156:    135 / 196 loss=2.505, nll_loss=1.974, ppl=3.93, wps=20784.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=30300, lr=0.000181668, gnorm=1.124, loss_scale=16, train_wall=293, gb_free=19.9, wall=96783
2022-03-07 15:37:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:37:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:37:47 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 11.017 | nll_loss 10.667 | ppl 1626.44 | wps 40846.7 | wpb 510.9 | bsz 1 | num_updates 30360 | best_loss 7.054
2022-03-07 15:37:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 30360 updates
2022-03-07 15:37:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 15:37:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 15:37:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 156 @ 30360 updates, score 11.017) (writing took 3.2760229338891804 seconds)
2022-03-07 15:37:50 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-03-07 15:37:50 | INFO | train | epoch 156 | loss 2.514 | nll_loss 1.984 | ppl 3.96 | wps 20481.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 30360 | lr 0.000181489 | gnorm 1.123 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 96981
2022-03-07 15:37:50 | INFO | fairseq.trainer | begin training epoch 157
2022-03-07 15:37:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:39:55 | INFO | train_inner | epoch 157:     40 / 196 loss=2.515, nll_loss=1.986, ppl=3.96, wps=20214.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=30400, lr=0.000181369, gnorm=1.132, loss_scale=16, train_wall=292, gb_free=19.9, wall=97106
2022-03-07 15:44:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:45:11 | INFO | train_inner | epoch 157:    141 / 196 loss=2.506, nll_loss=1.976, ppl=3.93, wps=20776.7, ups=0.32, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=1.136, loss_scale=16, train_wall=293, gb_free=19.9, wall=97421
2022-03-07 15:48:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:48:07 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 11.04 | nll_loss 10.694 | ppl 1656.18 | wps 40756.7 | wpb 510.9 | bsz 1 | num_updates 30555 | best_loss 7.054
2022-03-07 15:48:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 30555 updates
2022-03-07 15:48:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 15:48:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 15:48:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 157 @ 30555 updates, score 11.04) (writing took 3.25038451468572 seconds)
2022-03-07 15:48:10 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-03-07 15:48:10 | INFO | train | epoch 157 | loss 2.511 | nll_loss 1.982 | ppl 3.95 | wps 20584.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 30555 | lr 0.000180908 | gnorm 1.139 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 97601
2022-03-07 15:48:10 | INFO | fairseq.trainer | begin training epoch 158
2022-03-07 15:48:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:50:31 | INFO | train_inner | epoch 158:     45 / 196 loss=2.514, nll_loss=1.985, ppl=3.96, wps=20411.3, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=30600, lr=0.000180775, gnorm=1.142, loss_scale=16, train_wall=289, gb_free=19.9, wall=97742
2022-03-07 15:50:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:55:47 | INFO | train_inner | epoch 158:    146 / 196 loss=2.504, nll_loss=1.974, ppl=3.93, wps=20769.8, ups=0.32, wpb=65536, bsz=128, num_updates=30700, lr=0.000180481, gnorm=1.14, loss_scale=16, train_wall=293, gb_free=19.9, wall=98057
2022-03-07 15:58:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:58:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:58:27 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 11.067 | nll_loss 10.722 | ppl 1689.38 | wps 40666.7 | wpb 510.9 | bsz 1 | num_updates 30749 | best_loss 7.054
2022-03-07 15:58:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 30749 updates
2022-03-07 15:58:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 15:58:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 15:58:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 158 @ 30749 updates, score 11.067) (writing took 3.2586139859631658 seconds)
2022-03-07 15:58:31 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-03-07 15:58:31 | INFO | train | epoch 158 | loss 2.505 | nll_loss 1.975 | ppl 3.93 | wps 20468 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 30749 | lr 0.000180337 | gnorm 1.135 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 98221
2022-03-07 15:58:31 | INFO | fairseq.trainer | begin training epoch 159
2022-03-07 15:58:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:01:10 | INFO | train_inner | epoch 159:     51 / 196 loss=2.5, nll_loss=1.97, ppl=3.92, wps=20202.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=30800, lr=0.000180187, gnorm=1.125, loss_scale=16, train_wall=292, gb_free=19.9, wall=98381
2022-03-07 16:05:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:06:27 | INFO | train_inner | epoch 159:    152 / 196 loss=2.509, nll_loss=1.979, ppl=3.94, wps=20707.6, ups=0.32, wpb=65536, bsz=128, num_updates=30900, lr=0.000179896, gnorm=1.137, loss_scale=16, train_wall=294, gb_free=19.9, wall=98697
2022-03-07 16:08:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:08:49 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 11.063 | nll_loss 10.717 | ppl 1682.86 | wps 40742.3 | wpb 510.9 | bsz 1 | num_updates 30944 | best_loss 7.054
2022-03-07 16:08:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 30944 updates
2022-03-07 16:08:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 16:08:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 16:08:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 159 @ 30944 updates, score 11.063) (writing took 3.2396819349378347 seconds)
2022-03-07 16:08:53 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-03-07 16:08:53 | INFO | train | epoch 159 | loss 2.503 | nll_loss 1.973 | ppl 3.93 | wps 20519.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 30944 | lr 0.000179768 | gnorm 1.133 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 98843
2022-03-07 16:08:53 | INFO | fairseq.trainer | begin training epoch 160
2022-03-07 16:08:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:11:48 | INFO | train_inner | epoch 160:     56 / 196 loss=2.493, nll_loss=1.962, ppl=3.9, wps=20364.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=31000, lr=0.000179605, gnorm=1.122, loss_scale=16, train_wall=290, gb_free=19.9, wall=99018
2022-03-07 16:11:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:17:03 | INFO | train_inner | epoch 160:    157 / 196 loss=2.506, nll_loss=1.976, ppl=3.94, wps=20776.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=31100, lr=0.000179316, gnorm=1.126, loss_scale=16, train_wall=293, gb_free=19.9, wall=99334
2022-03-07 16:19:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:19:09 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 11.08 | nll_loss 10.733 | ppl 1702.19 | wps 40809.6 | wpb 510.9 | bsz 1 | num_updates 31139 | best_loss 7.054
2022-03-07 16:19:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 31139 updates
2022-03-07 16:19:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 16:19:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 16:19:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 160 @ 31139 updates, score 11.08) (writing took 3.271453619003296 seconds)
2022-03-07 16:19:13 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-03-07 16:19:13 | INFO | train | epoch 160 | loss 2.499 | nll_loss 1.969 | ppl 3.91 | wps 20586.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 31139 | lr 0.000179204 | gnorm 1.124 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 99463
2022-03-07 16:19:13 | INFO | fairseq.trainer | begin training epoch 161
2022-03-07 16:19:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:19:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:22:26 | INFO | train_inner | epoch 161:     62 / 196 loss=2.487, nll_loss=1.957, ppl=3.88, wps=20221.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=31200, lr=0.000179029, gnorm=1.132, loss_scale=16, train_wall=292, gb_free=19.9, wall=99657
2022-03-07 16:26:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:27:42 | INFO | train_inner | epoch 161:    163 / 196 loss=2.502, nll_loss=1.972, ppl=3.92, wps=20778.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=31300, lr=0.000178743, gnorm=1.138, loss_scale=16, train_wall=293, gb_free=19.9, wall=99972
2022-03-07 16:29:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:29:29 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 11.101 | nll_loss 10.756 | ppl 1729.25 | wps 40988.6 | wpb 510.9 | bsz 1 | num_updates 31333 | best_loss 7.054
2022-03-07 16:29:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 31333 updates
2022-03-07 16:29:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 16:29:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 16:29:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 161 @ 31333 updates, score 11.101) (writing took 3.286068833898753 seconds)
2022-03-07 16:29:33 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-03-07 16:29:33 | INFO | train | epoch 161 | loss 2.493 | nll_loss 1.962 | ppl 3.9 | wps 20479.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 31333 | lr 0.000178648 | gnorm 1.139 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 100083
2022-03-07 16:29:33 | INFO | fairseq.trainer | begin training epoch 162
2022-03-07 16:29:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:33:02 | INFO | train_inner | epoch 162:     67 / 196 loss=2.485, nll_loss=1.954, ppl=3.87, wps=20416.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=31400, lr=0.000178458, gnorm=1.143, loss_scale=32, train_wall=289, gb_free=19.9, wall=100292
2022-03-07 16:33:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:38:17 | INFO | train_inner | epoch 162:    168 / 196 loss=2.507, nll_loss=1.977, ppl=3.94, wps=20776.6, ups=0.32, wpb=65536, bsz=128, num_updates=31500, lr=0.000178174, gnorm=1.143, loss_scale=16, train_wall=293, gb_free=19.9, wall=100608
2022-03-07 16:39:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:39:49 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 11.112 | nll_loss 10.764 | ppl 1738.7 | wps 40740.5 | wpb 510.9 | bsz 1 | num_updates 31528 | best_loss 7.054
2022-03-07 16:39:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 31528 updates
2022-03-07 16:39:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 16:39:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 16:39:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 162 @ 31528 updates, score 11.112) (writing took 3.3876215452328324 seconds)
2022-03-07 16:39:53 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-03-07 16:39:53 | INFO | train | epoch 162 | loss 2.49 | nll_loss 1.96 | ppl 3.89 | wps 20580.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 31528 | lr 0.000178095 | gnorm 1.144 | loss_scale 32 | train_wall 567 | gb_free 19.9 | wall 100703
2022-03-07 16:39:53 | INFO | fairseq.trainer | begin training epoch 163
2022-03-07 16:39:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:39:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:43:41 | INFO | train_inner | epoch 163:     73 / 196 loss=2.462, nll_loss=1.93, ppl=3.81, wps=20203.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=31600, lr=0.000177892, gnorm=1.143, loss_scale=16, train_wall=292, gb_free=19.9, wall=100931
2022-03-07 16:46:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:48:57 | INFO | train_inner | epoch 163:    174 / 196 loss=2.511, nll_loss=1.981, ppl=3.95, wps=20751.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=31700, lr=0.000177611, gnorm=1.125, loss_scale=16, train_wall=293, gb_free=19.9, wall=101247
2022-03-07 16:50:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:50:10 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 11.155 | nll_loss 10.81 | ppl 1795.74 | wps 40704.5 | wpb 510.9 | bsz 1 | num_updates 31722 | best_loss 7.054
2022-03-07 16:50:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 31722 updates
2022-03-07 16:50:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 16:50:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 16:50:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 163 @ 31722 updates, score 11.155) (writing took 3.295204397290945 seconds)
2022-03-07 16:50:13 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-03-07 16:50:13 | INFO | train | epoch 163 | loss 2.485 | nll_loss 1.954 | ppl 3.88 | wps 20455.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 31722 | lr 0.00017755 | gnorm 1.131 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 101324
2022-03-07 16:50:13 | INFO | fairseq.trainer | begin training epoch 164
2022-03-07 16:50:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:53:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:54:21 | INFO | train_inner | epoch 164:     79 / 196 loss=2.465, nll_loss=1.933, ppl=3.82, wps=20156, ups=0.31, wpb=65367, bsz=127.7, num_updates=31800, lr=0.000177332, gnorm=1.127, loss_scale=16, train_wall=293, gb_free=19.9, wall=101571
2022-03-07 16:59:34 | INFO | train_inner | epoch 164:    179 / 196 loss=2.501, nll_loss=1.971, ppl=3.92, wps=20926.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=31900, lr=0.000177054, gnorm=1.149, loss_scale=16, train_wall=291, gb_free=19.9, wall=101885
2022-03-07 17:00:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:00:32 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 11.13 | nll_loss 10.784 | ppl 1763.01 | wps 40676.2 | wpb 510.9 | bsz 1 | num_updates 31917 | best_loss 7.054
2022-03-07 17:00:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 31917 updates
2022-03-07 17:00:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 17:00:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 17:00:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 164 @ 31917 updates, score 11.13) (writing took 3.0538586103357375 seconds)
2022-03-07 17:00:35 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-03-07 17:00:35 | INFO | train | epoch 164 | loss 2.482 | nll_loss 1.951 | ppl 3.87 | wps 20532.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 31917 | lr 0.000177006 | gnorm 1.139 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 101945
2022-03-07 17:00:35 | INFO | fairseq.trainer | begin training epoch 165
2022-03-07 17:00:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:00:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:04:58 | INFO | train_inner | epoch 165:     84 / 196 loss=2.456, nll_loss=1.923, ppl=3.79, wps=20174.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=32000, lr=0.000176777, gnorm=1.14, loss_scale=16, train_wall=293, gb_free=19.9, wall=102209
2022-03-07 17:07:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:10:14 | INFO | train_inner | epoch 165:    185 / 196 loss=2.503, nll_loss=1.973, ppl=3.93, wps=20717.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=32100, lr=0.000176501, gnorm=1.147, loss_scale=16, train_wall=293, gb_free=19.9, wall=102525
2022-03-07 17:10:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:10:53 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 11.092 | nll_loss 10.748 | ppl 1719.22 | wps 40418.4 | wpb 510.9 | bsz 1 | num_updates 32111 | best_loss 7.054
2022-03-07 17:10:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 32111 updates
2022-03-07 17:10:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 17:10:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 17:10:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 165 @ 32111 updates, score 11.092) (writing took 3.206297502387315 seconds)
2022-03-07 17:10:57 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-03-07 17:10:57 | INFO | train | epoch 165 | loss 2.477 | nll_loss 1.945 | ppl 3.85 | wps 20421.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 32111 | lr 0.000176471 | gnorm 1.143 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 102567
2022-03-07 17:10:57 | INFO | fairseq.trainer | begin training epoch 166
2022-03-07 17:10:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:14:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:15:39 | INFO | train_inner | epoch 166:     90 / 196 loss=2.445, nll_loss=1.912, ppl=3.76, wps=20157.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=32200, lr=0.000176227, gnorm=1.149, loss_scale=16, train_wall=293, gb_free=19.9, wall=102849
2022-03-07 17:20:52 | INFO | train_inner | epoch 166:    190 / 196 loss=2.504, nll_loss=1.974, ppl=3.93, wps=20924.5, ups=0.32, wpb=65536, bsz=128, num_updates=32300, lr=0.000175954, gnorm=1.156, loss_scale=16, train_wall=291, gb_free=19.9, wall=103162
2022-03-07 17:21:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:21:15 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 11.166 | nll_loss 10.822 | ppl 1810.38 | wps 40376.3 | wpb 510.9 | bsz 1 | num_updates 32306 | best_loss 7.054
2022-03-07 17:21:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 32306 updates
2022-03-07 17:21:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 17:21:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 17:21:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 166 @ 32306 updates, score 11.166) (writing took 3.1760780476033688 seconds)
2022-03-07 17:21:18 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-03-07 17:21:18 | INFO | train | epoch 166 | loss 2.474 | nll_loss 1.943 | ppl 3.84 | wps 20525.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 32306 | lr 0.000175937 | gnorm 1.15 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 103189
2022-03-07 17:21:18 | INFO | fairseq.trainer | begin training epoch 167
2022-03-07 17:21:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:21:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:26:16 | INFO | train_inner | epoch 167:     95 / 196 loss=2.437, nll_loss=1.903, ppl=3.74, wps=20166.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=32400, lr=0.000175682, gnorm=1.123, loss_scale=16, train_wall=293, gb_free=19.9, wall=103486
2022-03-07 17:28:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:31:31 | INFO | train_inner | epoch 167:    196 / 196 loss=2.506, nll_loss=1.976, ppl=3.93, wps=20712.8, ups=0.32, wpb=65367, bsz=127.7, num_updates=32500, lr=0.000175412, gnorm=1.137, loss_scale=16, train_wall=293, gb_free=19.9, wall=103802
2022-03-07 17:31:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:31:37 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 11.148 | nll_loss 10.802 | ppl 1785.06 | wps 40466.8 | wpb 510.9 | bsz 1 | num_updates 32500 | best_loss 7.054
2022-03-07 17:31:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 32500 updates
2022-03-07 17:31:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 17:31:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 17:31:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 167 @ 32500 updates, score 11.148) (writing took 3.189811102114618 seconds)
2022-03-07 17:31:40 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-03-07 17:31:40 | INFO | train | epoch 167 | loss 2.468 | nll_loss 1.937 | ppl 3.83 | wps 20422.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 32500 | lr 0.000175412 | gnorm 1.129 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 103811
2022-03-07 17:31:40 | INFO | fairseq.trainer | begin training epoch 168
2022-03-07 17:31:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:35:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:36:56 | INFO | train_inner | epoch 168:    101 / 196 loss=2.437, nll_loss=1.904, ppl=3.74, wps=20165.2, ups=0.31, wpb=65536, bsz=128, num_updates=32600, lr=0.000175142, gnorm=1.138, loss_scale=16, train_wall=293, gb_free=19.9, wall=104127
2022-03-07 17:41:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:41:59 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 11.139 | nll_loss 10.795 | ppl 1776.32 | wps 40594.7 | wpb 510.9 | bsz 1 | num_updates 32695 | best_loss 7.054
2022-03-07 17:41:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 32695 updates
2022-03-07 17:41:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 17:42:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 17:42:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 168 @ 32695 updates, score 11.139) (writing took 3.079980819951743 seconds)
2022-03-07 17:42:02 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-03-07 17:42:02 | INFO | train | epoch 168 | loss 2.466 | nll_loss 1.934 | ppl 3.82 | wps 20528.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 32695 | lr 0.000174888 | gnorm 1.141 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 104432
2022-03-07 17:42:02 | INFO | fairseq.trainer | begin training epoch 169
2022-03-07 17:42:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:42:18 | INFO | train_inner | epoch 169:      5 / 196 loss=2.491, nll_loss=1.961, ppl=3.89, wps=20353.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=32700, lr=0.000174874, gnorm=1.144, loss_scale=16, train_wall=290, gb_free=19.9, wall=104448
2022-03-07 17:42:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:47:36 | INFO | train_inner | epoch 169:    106 / 196 loss=2.432, nll_loss=1.899, ppl=3.73, wps=20564.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=32800, lr=0.000174608, gnorm=1.141, loss_scale=16, train_wall=294, gb_free=19.9, wall=104767
2022-03-07 17:49:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:52:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:52:25 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 11.165 | nll_loss 10.821 | ppl 1808.93 | wps 40937.3 | wpb 510.9 | bsz 1 | num_updates 32889 | best_loss 7.054
2022-03-07 17:52:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 32889 updates
2022-03-07 17:52:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 17:52:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 17:52:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 169 @ 32889 updates, score 11.165) (writing took 3.6342505561187863 seconds)
2022-03-07 17:52:29 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-03-07 17:52:29 | INFO | train | epoch 169 | loss 2.462 | nll_loss 1.93 | ppl 3.81 | wps 20256.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 32889 | lr 0.000174371 | gnorm 1.144 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 105059
2022-03-07 17:52:29 | INFO | fairseq.trainer | begin training epoch 170
2022-03-07 17:52:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:53:03 | INFO | train_inner | epoch 170:     11 / 196 loss=2.486, nll_loss=1.956, ppl=3.88, wps=20004.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=32900, lr=0.000174342, gnorm=1.144, loss_scale=16, train_wall=293, gb_free=19.9, wall=105094
2022-03-07 17:56:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:58:19 | INFO | train_inner | epoch 170:    112 / 196 loss=2.433, nll_loss=1.899, ppl=3.73, wps=20728, ups=0.32, wpb=65532.4, bsz=128, num_updates=33000, lr=0.000174078, gnorm=1.145, loss_scale=16, train_wall=293, gb_free=19.9, wall=105410
2022-03-07 18:02:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:02:47 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 11.183 | nll_loss 10.84 | ppl 1832.79 | wps 40558.1 | wpb 510.9 | bsz 1 | num_updates 33084 | best_loss 7.054
2022-03-07 18:02:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 33084 updates
2022-03-07 18:02:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 18:02:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 18:02:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 170 @ 33084 updates, score 11.183) (writing took 3.6495950208045542 seconds)
2022-03-07 18:02:50 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-03-07 18:02:50 | INFO | train | epoch 170 | loss 2.458 | nll_loss 1.926 | ppl 3.8 | wps 20522 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 33084 | lr 0.000173857 | gnorm 1.15 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 105681
2022-03-07 18:02:51 | INFO | fairseq.trainer | begin training epoch 171
2022-03-07 18:02:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:03:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:03:44 | INFO | train_inner | epoch 171:     17 / 196 loss=2.48, nll_loss=1.949, ppl=3.86, wps=20138.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=33100, lr=0.000173814, gnorm=1.156, loss_scale=16, train_wall=293, gb_free=19.9, wall=105734
2022-03-07 18:08:57 | INFO | train_inner | epoch 171:    117 / 196 loss=2.436, nll_loss=1.903, ppl=3.74, wps=20941.2, ups=0.32, wpb=65536, bsz=128, num_updates=33200, lr=0.000173553, gnorm=1.139, loss_scale=16, train_wall=290, gb_free=19.9, wall=106047
2022-03-07 18:10:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:13:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:13:09 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 11.163 | nll_loss 10.819 | ppl 1805.93 | wps 40536.2 | wpb 510.9 | bsz 1 | num_updates 33278 | best_loss 7.054
2022-03-07 18:13:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 33278 updates
2022-03-07 18:13:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 18:13:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 18:13:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 171 @ 33278 updates, score 11.163) (writing took 3.399756978265941 seconds)
2022-03-07 18:13:12 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-03-07 18:13:12 | INFO | train | epoch 171 | loss 2.454 | nll_loss 1.921 | ppl 3.79 | wps 20417 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 33278 | lr 0.000173349 | gnorm 1.144 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 106303
2022-03-07 18:13:12 | INFO | fairseq.trainer | begin training epoch 172
2022-03-07 18:13:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:14:21 | INFO | train_inner | epoch 172:     22 / 196 loss=2.466, nll_loss=1.934, ppl=3.82, wps=20129.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=33300, lr=0.000173292, gnorm=1.148, loss_scale=16, train_wall=293, gb_free=19.9, wall=106372
2022-03-07 18:17:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:19:38 | INFO | train_inner | epoch 172:    123 / 196 loss=2.434, nll_loss=1.9, ppl=3.73, wps=20722.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=33400, lr=0.000173032, gnorm=1.142, loss_scale=16, train_wall=293, gb_free=19.9, wall=106688
2022-03-07 18:23:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:23:31 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 11.209 | nll_loss 10.867 | ppl 1868.28 | wps 40607.1 | wpb 510.9 | bsz 1 | num_updates 33473 | best_loss 7.054
2022-03-07 18:23:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 33473 updates
2022-03-07 18:23:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 18:23:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 18:23:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 172 @ 33473 updates, score 11.209) (writing took 3.4817330799996853 seconds)
2022-03-07 18:23:35 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-03-07 18:23:35 | INFO | train | epoch 172 | loss 2.451 | nll_loss 1.918 | ppl 3.78 | wps 20503.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 33473 | lr 0.000172843 | gnorm 1.147 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 106925
2022-03-07 18:23:35 | INFO | fairseq.trainer | begin training epoch 173
2022-03-07 18:23:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:24:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:25:03 | INFO | train_inner | epoch 173:     28 / 196 loss=2.465, nll_loss=1.933, ppl=3.82, wps=20125.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=33500, lr=0.000172774, gnorm=1.145, loss_scale=16, train_wall=293, gb_free=19.9, wall=107013
2022-03-07 18:30:18 | INFO | train_inner | epoch 173:    128 / 196 loss=2.439, nll_loss=1.906, ppl=3.75, wps=20780, ups=0.32, wpb=65536, bsz=128, num_updates=33600, lr=0.000172516, gnorm=1.145, loss_scale=16, train_wall=292, gb_free=19.9, wall=107328
2022-03-07 18:31:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:33:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:33:57 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 11.239 | nll_loss 10.899 | ppl 1909.54 | wps 40530.8 | wpb 510.9 | bsz 1 | num_updates 33667 | best_loss 7.054
2022-03-07 18:33:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 33667 updates
2022-03-07 18:33:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 18:34:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 18:34:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 173 @ 33667 updates, score 11.239) (writing took 3.420816189609468 seconds)
2022-03-07 18:34:00 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-03-07 18:34:00 | INFO | train | epoch 173 | loss 2.448 | nll_loss 1.915 | ppl 3.77 | wps 20307.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 33667 | lr 0.000172345 | gnorm 1.146 | loss_scale 16 | train_wall 571 | gb_free 19.9 | wall 107551
2022-03-07 18:34:00 | INFO | fairseq.trainer | begin training epoch 174
2022-03-07 18:34:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:35:44 | INFO | train_inner | epoch 174:     33 / 196 loss=2.45, nll_loss=1.918, ppl=3.78, wps=20074.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=33700, lr=0.00017226, gnorm=1.158, loss_scale=16, train_wall=293, gb_free=19.9, wall=107654
2022-03-07 18:38:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:41:00 | INFO | train_inner | epoch 174:    134 / 196 loss=2.437, nll_loss=1.904, ppl=3.74, wps=20731.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=33800, lr=0.000172005, gnorm=1.142, loss_scale=16, train_wall=293, gb_free=19.9, wall=107970
2022-03-07 18:44:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:44:18 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 11.168 | nll_loss 10.823 | ppl 1811.7 | wps 40558.9 | wpb 510.9 | bsz 1 | num_updates 33862 | best_loss 7.054
2022-03-07 18:44:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 33862 updates
2022-03-07 18:44:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 18:44:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 18:44:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 174 @ 33862 updates, score 11.168) (writing took 3.437647826038301 seconds)
2022-03-07 18:44:22 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-03-07 18:44:22 | INFO | train | epoch 174 | loss 2.444 | nll_loss 1.911 | ppl 3.76 | wps 20526.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 33862 | lr 0.000171848 | gnorm 1.154 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 108172
2022-03-07 18:44:22 | INFO | fairseq.trainer | begin training epoch 175
2022-03-07 18:44:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:45:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:46:24 | INFO | train_inner | epoch 175:     39 / 196 loss=2.45, nll_loss=1.917, ppl=3.78, wps=20155.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=33900, lr=0.000171751, gnorm=1.158, loss_scale=16, train_wall=293, gb_free=19.9, wall=108294
2022-03-07 18:51:37 | INFO | train_inner | epoch 175:    139 / 196 loss=2.435, nll_loss=1.901, ppl=3.74, wps=20921.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=34000, lr=0.000171499, gnorm=1.152, loss_scale=16, train_wall=291, gb_free=19.9, wall=108608
2022-03-07 18:52:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:54:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:54:40 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 11.225 | nll_loss 10.883 | ppl 1888.48 | wps 40456.1 | wpb 510.9 | bsz 1 | num_updates 34056 | best_loss 7.054
2022-03-07 18:54:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 34056 updates
2022-03-07 18:54:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 18:54:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 18:54:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 175 @ 34056 updates, score 11.225) (writing took 3.4636620990931988 seconds)
2022-03-07 18:54:44 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-03-07 18:54:44 | INFO | train | epoch 175 | loss 2.44 | nll_loss 1.907 | ppl 3.75 | wps 20412.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 34056 | lr 0.000171358 | gnorm 1.148 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 108794
2022-03-07 18:54:44 | INFO | fairseq.trainer | begin training epoch 176
2022-03-07 18:54:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:57:02 | INFO | train_inner | epoch 176:     44 / 196 loss=2.433, nll_loss=1.9, ppl=3.73, wps=20142.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=34100, lr=0.000171247, gnorm=1.143, loss_scale=16, train_wall=293, gb_free=19.9, wall=108932
2022-03-07 18:59:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:02:18 | INFO | train_inner | epoch 176:    145 / 196 loss=2.441, nll_loss=1.908, ppl=3.75, wps=20705, ups=0.32, wpb=65532.4, bsz=128, num_updates=34200, lr=0.000170996, gnorm=1.155, loss_scale=16, train_wall=294, gb_free=19.9, wall=109249
2022-03-07 19:04:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:05:03 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 11.192 | nll_loss 10.848 | ppl 1842.73 | wps 40489.5 | wpb 510.9 | bsz 1 | num_updates 34251 | best_loss 7.054
2022-03-07 19:05:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 34251 updates
2022-03-07 19:05:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 19:05:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 19:05:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 176 @ 34251 updates, score 11.192) (writing took 3.4782194099389017 seconds)
2022-03-07 19:05:06 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-03-07 19:05:06 | INFO | train | epoch 176 | loss 2.436 | nll_loss 1.903 | ppl 3.74 | wps 20505.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 34251 | lr 0.000170869 | gnorm 1.147 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 109417
2022-03-07 19:05:06 | INFO | fairseq.trainer | begin training epoch 177
2022-03-07 19:05:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:06:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:07:43 | INFO | train_inner | epoch 177:     50 / 196 loss=2.435, nll_loss=1.901, ppl=3.74, wps=20133, ups=0.31, wpb=65367, bsz=127.7, num_updates=34300, lr=0.000170747, gnorm=1.143, loss_scale=16, train_wall=293, gb_free=19.9, wall=109573
2022-03-07 19:12:56 | INFO | train_inner | epoch 177:    150 / 196 loss=2.432, nll_loss=1.898, ppl=3.73, wps=20915.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=34400, lr=0.000170499, gnorm=1.153, loss_scale=16, train_wall=291, gb_free=19.9, wall=109887
2022-03-07 19:13:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:15:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:15:25 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 11.208 | nll_loss 10.862 | ppl 1861.77 | wps 40665.3 | wpb 510.9 | bsz 1 | num_updates 34445 | best_loss 7.054
2022-03-07 19:15:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 34445 updates
2022-03-07 19:15:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 19:15:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 19:15:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 177 @ 34445 updates, score 11.208) (writing took 3.432218805886805 seconds)
2022-03-07 19:15:28 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-03-07 19:15:28 | INFO | train | epoch 177 | loss 2.432 | nll_loss 1.898 | ppl 3.73 | wps 20410 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 34445 | lr 0.000170387 | gnorm 1.153 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 110039
2022-03-07 19:15:28 | INFO | fairseq.trainer | begin training epoch 178
2022-03-07 19:15:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:18:21 | INFO | train_inner | epoch 178:     55 / 196 loss=2.429, nll_loss=1.896, ppl=3.72, wps=20151.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=34500, lr=0.000170251, gnorm=1.139, loss_scale=16, train_wall=293, gb_free=19.9, wall=110211
2022-03-07 19:20:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:23:37 | INFO | train_inner | epoch 178:    156 / 196 loss=2.429, nll_loss=1.895, ppl=3.72, wps=20718.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=34600, lr=0.000170005, gnorm=1.165, loss_scale=16, train_wall=294, gb_free=19.9, wall=110527
2022-03-07 19:25:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:25:47 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 11.209 | nll_loss 10.868 | ppl 1868.33 | wps 40706.1 | wpb 510.9 | bsz 1 | num_updates 34640 | best_loss 7.054
2022-03-07 19:25:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 34640 updates
2022-03-07 19:25:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 19:25:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 19:25:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 178 @ 34640 updates, score 11.209) (writing took 3.3890318609774113 seconds)
2022-03-07 19:25:50 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-03-07 19:25:50 | INFO | train | epoch 178 | loss 2.429 | nll_loss 1.896 | ppl 3.72 | wps 20521.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 34640 | lr 0.000169907 | gnorm 1.154 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 110661
2022-03-07 19:25:50 | INFO | fairseq.trainer | begin training epoch 179
2022-03-07 19:25:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:27:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:29:01 | INFO | train_inner | epoch 179:     61 / 196 loss=2.428, nll_loss=1.894, ppl=3.72, wps=20155.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=34700, lr=0.00016976, gnorm=1.166, loss_scale=16, train_wall=293, gb_free=19.9, wall=110852
2022-03-07 19:34:14 | INFO | train_inner | epoch 179:    161 / 196 loss=2.436, nll_loss=1.903, ppl=3.74, wps=20925.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=34800, lr=0.000169516, gnorm=1.14, loss_scale=16, train_wall=291, gb_free=19.9, wall=111165
2022-03-07 19:34:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:36:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:36:09 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 11.243 | nll_loss 10.9 | ppl 1911.32 | wps 40635.6 | wpb 510.9 | bsz 1 | num_updates 34834 | best_loss 7.054
2022-03-07 19:36:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 34834 updates
2022-03-07 19:36:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 19:36:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 19:36:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 179 @ 34834 updates, score 11.243) (writing took 3.3757840991020203 seconds)
2022-03-07 19:36:12 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-03-07 19:36:12 | INFO | train | epoch 179 | loss 2.426 | nll_loss 1.893 | ppl 3.71 | wps 20420.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 34834 | lr 0.000169433 | gnorm 1.153 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 111282
2022-03-07 19:36:12 | INFO | fairseq.trainer | begin training epoch 180
2022-03-07 19:36:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:39:39 | INFO | train_inner | epoch 180:     66 / 196 loss=2.41, nll_loss=1.876, ppl=3.67, wps=20149.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=34900, lr=0.000169273, gnorm=1.159, loss_scale=16, train_wall=293, gb_free=19.9, wall=111489
2022-03-07 19:41:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:44:55 | INFO | train_inner | epoch 180:    167 / 196 loss=2.435, nll_loss=1.902, ppl=3.74, wps=20714.4, ups=0.32, wpb=65536, bsz=128, num_updates=35000, lr=0.000169031, gnorm=1.166, loss_scale=16, train_wall=294, gb_free=19.9, wall=111806
2022-03-07 19:46:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:46:31 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 11.247 | nll_loss 10.905 | ppl 1917.73 | wps 40522.1 | wpb 510.9 | bsz 1 | num_updates 35029 | best_loss 7.054
2022-03-07 19:46:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 35029 updates
2022-03-07 19:46:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 19:46:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 19:46:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 180 @ 35029 updates, score 11.247) (writing took 3.3962099221535027 seconds)
2022-03-07 19:46:34 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-03-07 19:46:34 | INFO | train | epoch 180 | loss 2.423 | nll_loss 1.889 | ppl 3.7 | wps 20514.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 35029 | lr 0.000168961 | gnorm 1.159 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 111905
2022-03-07 19:46:34 | INFO | fairseq.trainer | begin training epoch 181
2022-03-07 19:46:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:48:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:50:19 | INFO | train_inner | epoch 181:     72 / 196 loss=2.412, nll_loss=1.877, ppl=3.67, wps=20157.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=35100, lr=0.00016879, gnorm=1.146, loss_scale=16, train_wall=293, gb_free=19.9, wall=112130
2022-03-07 19:55:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:55:36 | INFO | train_inner | epoch 181:    173 / 196 loss=2.431, nll_loss=1.898, ppl=3.73, wps=20716.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=35200, lr=0.00016855, gnorm=1.18, loss_scale=16, train_wall=293, gb_free=19.9, wall=112446
2022-03-07 19:56:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:56:52 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 11.278 | nll_loss 10.935 | ppl 1958.08 | wps 40367 | wpb 510.9 | bsz 1 | num_updates 35223 | best_loss 7.054
2022-03-07 19:56:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 35223 updates
2022-03-07 19:56:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 19:56:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 19:56:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 181 @ 35223 updates, score 11.278) (writing took 3.3982077850960195 seconds)
2022-03-07 19:56:56 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-03-07 19:56:56 | INFO | train | epoch 181 | loss 2.419 | nll_loss 1.885 | ppl 3.69 | wps 20417 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 35223 | lr 0.000168495 | gnorm 1.165 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 112526
2022-03-07 19:56:56 | INFO | fairseq.trainer | begin training epoch 182
2022-03-07 19:56:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:00:57 | INFO | train_inner | epoch 182:     77 / 196 loss=2.397, nll_loss=1.862, ppl=3.63, wps=20339.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=35300, lr=0.000168311, gnorm=1.146, loss_scale=16, train_wall=290, gb_free=19.9, wall=112768
2022-03-07 20:02:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:06:13 | INFO | train_inner | epoch 182:    178 / 196 loss=2.437, nll_loss=1.904, ppl=3.74, wps=20719.6, ups=0.32, wpb=65536, bsz=128, num_updates=35400, lr=0.000168073, gnorm=1.155, loss_scale=16, train_wall=293, gb_free=19.9, wall=113084
2022-03-07 20:07:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:07:14 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 11.277 | nll_loss 10.935 | ppl 1957.77 | wps 40618.3 | wpb 510.9 | bsz 1 | num_updates 35418 | best_loss 7.054
2022-03-07 20:07:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 35418 updates
2022-03-07 20:07:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 20:07:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 20:07:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 182 @ 35418 updates, score 11.277) (writing took 3.3650039681233466 seconds)
2022-03-07 20:07:18 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-03-07 20:07:18 | INFO | train | epoch 182 | loss 2.416 | nll_loss 1.882 | ppl 3.69 | wps 20523 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 35418 | lr 0.00016803 | gnorm 1.149 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 113148
2022-03-07 20:07:18 | INFO | fairseq.trainer | begin training epoch 183
2022-03-07 20:07:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:08:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:11:38 | INFO | train_inner | epoch 183:     83 / 196 loss=2.394, nll_loss=1.859, ppl=3.63, wps=20158.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=35500, lr=0.000167836, gnorm=1.145, loss_scale=16, train_wall=293, gb_free=19.9, wall=113408
2022-03-07 20:15:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:16:54 | INFO | train_inner | epoch 183:    184 / 196 loss=2.437, nll_loss=1.904, ppl=3.74, wps=20722.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=35600, lr=0.0001676, gnorm=1.175, loss_scale=16, train_wall=293, gb_free=19.9, wall=113724
2022-03-07 20:17:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:17:36 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 11.28 | nll_loss 10.938 | ppl 1961.42 | wps 40605.8 | wpb 510.9 | bsz 1 | num_updates 35612 | best_loss 7.054
2022-03-07 20:17:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 35612 updates
2022-03-07 20:17:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 20:17:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 20:17:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 183 @ 35612 updates, score 11.28) (writing took 3.4002285581082106 seconds)
2022-03-07 20:17:40 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-03-07 20:17:40 | INFO | train | epoch 183 | loss 2.412 | nll_loss 1.878 | ppl 3.68 | wps 20420.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 35612 | lr 0.000167572 | gnorm 1.16 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 113770
2022-03-07 20:17:40 | INFO | fairseq.trainer | begin training epoch 184
2022-03-07 20:17:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:22:15 | INFO | train_inner | epoch 184:     88 / 196 loss=2.383, nll_loss=1.848, ppl=3.6, wps=20345.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=35700, lr=0.000167365, gnorm=1.143, loss_scale=16, train_wall=290, gb_free=19.9, wall=114046
2022-03-07 20:22:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:27:31 | INFO | train_inner | epoch 184:    189 / 196 loss=2.438, nll_loss=1.905, ppl=3.75, wps=20721.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=35800, lr=0.000167132, gnorm=1.164, loss_scale=16, train_wall=293, gb_free=19.9, wall=114362
2022-03-07 20:27:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:27:58 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 11.319 | nll_loss 10.976 | ppl 2014.59 | wps 40231.4 | wpb 510.9 | bsz 1 | num_updates 35807 | best_loss 7.054
2022-03-07 20:27:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 35807 updates
2022-03-07 20:27:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 20:28:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 20:28:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 184 @ 35807 updates, score 11.319) (writing took 3.338898755144328 seconds)
2022-03-07 20:28:01 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-03-07 20:28:01 | INFO | train | epoch 184 | loss 2.409 | nll_loss 1.874 | ppl 3.67 | wps 20522.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 35807 | lr 0.000167115 | gnorm 1.153 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 114392
2022-03-07 20:28:01 | INFO | fairseq.trainer | begin training epoch 185
2022-03-07 20:28:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:29:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:32:56 | INFO | train_inner | epoch 185:     94 / 196 loss=2.375, nll_loss=1.839, ppl=3.58, wps=20153.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=35900, lr=0.000166899, gnorm=1.14, loss_scale=16, train_wall=293, gb_free=19.9, wall=114686
2022-03-07 20:36:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:38:12 | INFO | train_inner | epoch 185:    195 / 196 loss=2.44, nll_loss=1.908, ppl=3.75, wps=20720.8, ups=0.32, wpb=65536, bsz=128, num_updates=36000, lr=0.000166667, gnorm=1.159, loss_scale=16, train_wall=294, gb_free=19.9, wall=115003
2022-03-07 20:38:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:38:20 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 11.241 | nll_loss 10.899 | ppl 1909.16 | wps 40420.5 | wpb 510.9 | bsz 1 | num_updates 36001 | best_loss 7.054
2022-03-07 20:38:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 36001 updates
2022-03-07 20:38:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 20:38:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 20:38:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 185 @ 36001 updates, score 11.241) (writing took 3.4281165543943644 seconds)
2022-03-07 20:38:23 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-03-07 20:38:23 | INFO | train | epoch 185 | loss 2.406 | nll_loss 1.872 | ppl 3.66 | wps 20415.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 36001 | lr 0.000166664 | gnorm 1.149 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 115014
2022-03-07 20:38:23 | INFO | fairseq.trainer | begin training epoch 186
2022-03-07 20:38:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:43:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:43:37 | INFO | train_inner | epoch 186:    100 / 196 loss=2.371, nll_loss=1.835, ppl=3.57, wps=20130.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=36100, lr=0.000166436, gnorm=1.144, loss_scale=16, train_wall=293, gb_free=19.9, wall=115327
2022-03-07 20:48:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:48:43 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 11.33 | nll_loss 10.987 | ppl 2030.26 | wps 40209.9 | wpb 510.9 | bsz 1 | num_updates 36196 | best_loss 7.054
2022-03-07 20:48:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 36196 updates
2022-03-07 20:48:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 20:48:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 20:48:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 186 @ 36196 updates, score 11.33) (writing took 3.351178139913827 seconds)
2022-03-07 20:48:46 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-03-07 20:48:46 | INFO | train | epoch 186 | loss 2.402 | nll_loss 1.868 | ppl 3.65 | wps 20488.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 36196 | lr 0.000166215 | gnorm 1.156 | loss_scale 16 | train_wall 570 | gb_free 19.9 | wall 115637
2022-03-07 20:48:46 | INFO | fairseq.trainer | begin training epoch 187
2022-03-07 20:48:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:48:59 | INFO | train_inner | epoch 187:      4 / 196 loss=2.432, nll_loss=1.899, ppl=3.73, wps=20297.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=36200, lr=0.000166206, gnorm=1.169, loss_scale=16, train_wall=290, gb_free=19.9, wall=115649
2022-03-07 20:50:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:54:15 | INFO | train_inner | epoch 187:    105 / 196 loss=2.374, nll_loss=1.838, ppl=3.57, wps=20721.1, ups=0.32, wpb=65536, bsz=128, num_updates=36300, lr=0.000165977, gnorm=1.162, loss_scale=16, train_wall=293, gb_free=19.9, wall=115966
2022-03-07 20:56:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:58:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:59:05 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 11.278 | nll_loss 10.934 | ppl 1956.96 | wps 40543.8 | wpb 510.9 | bsz 1 | num_updates 36390 | best_loss 7.054
2022-03-07 20:59:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 36390 updates
2022-03-07 20:59:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 20:59:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 20:59:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 187 @ 36390 updates, score 11.278) (writing took 3.4268864691257477 seconds)
2022-03-07 20:59:08 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-03-07 20:59:08 | INFO | train | epoch 187 | loss 2.399 | nll_loss 1.864 | ppl 3.64 | wps 20415.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 36390 | lr 0.000165771 | gnorm 1.161 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 116259
2022-03-07 20:59:08 | INFO | fairseq.trainer | begin training epoch 188
2022-03-07 20:59:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:59:40 | INFO | train_inner | epoch 188:     10 / 196 loss=2.418, nll_loss=1.884, ppl=3.69, wps=20147.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=36400, lr=0.000165748, gnorm=1.157, loss_scale=16, train_wall=293, gb_free=19.9, wall=116290
2022-03-07 21:03:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:04:56 | INFO | train_inner | epoch 188:    111 / 196 loss=2.374, nll_loss=1.837, ppl=3.57, wps=20719.1, ups=0.32, wpb=65536, bsz=128, num_updates=36500, lr=0.000165521, gnorm=1.144, loss_scale=16, train_wall=293, gb_free=19.9, wall=116606
2022-03-07 21:09:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:09:27 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 11.328 | nll_loss 10.988 | ppl 2031.54 | wps 40682.9 | wpb 510.9 | bsz 1 | num_updates 36585 | best_loss 7.054
2022-03-07 21:09:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 36585 updates
2022-03-07 21:09:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 21:09:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 21:09:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 188 @ 36585 updates, score 11.328) (writing took 3.3013769262470305 seconds)
2022-03-07 21:09:30 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-03-07 21:09:30 | INFO | train | epoch 188 | loss 2.396 | nll_loss 1.862 | ppl 3.63 | wps 20518.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 36585 | lr 0.000165329 | gnorm 1.154 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 116881
2022-03-07 21:09:30 | INFO | fairseq.trainer | begin training epoch 189
2022-03-07 21:09:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:10:17 | INFO | train_inner | epoch 189:     15 / 196 loss=2.416, nll_loss=1.882, ppl=3.69, wps=20338.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=36600, lr=0.000165295, gnorm=1.167, loss_scale=16, train_wall=290, gb_free=19.9, wall=116928
2022-03-07 21:10:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:15:34 | INFO | train_inner | epoch 189:    116 / 196 loss=2.371, nll_loss=1.835, ppl=3.57, wps=20710.2, ups=0.32, wpb=65536, bsz=128, num_updates=36700, lr=0.00016507, gnorm=1.155, loss_scale=16, train_wall=294, gb_free=19.9, wall=117244
2022-03-07 21:17:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:19:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:19:49 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 11.357 | nll_loss 11.015 | ppl 2069.59 | wps 40498.5 | wpb 510.9 | bsz 1 | num_updates 36779 | best_loss 7.054
2022-03-07 21:19:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 36779 updates
2022-03-07 21:19:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 21:19:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 21:19:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 189 @ 36779 updates, score 11.357) (writing took 3.336814454291016 seconds)
2022-03-07 21:19:52 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-03-07 21:19:52 | INFO | train | epoch 189 | loss 2.393 | nll_loss 1.858 | ppl 3.62 | wps 20412.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 36779 | lr 0.000164892 | gnorm 1.16 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 117503
2022-03-07 21:19:52 | INFO | fairseq.trainer | begin training epoch 190
2022-03-07 21:19:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:20:58 | INFO | train_inner | epoch 190:     21 / 196 loss=2.41, nll_loss=1.876, ppl=3.67, wps=20155.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=36800, lr=0.000164845, gnorm=1.161, loss_scale=16, train_wall=293, gb_free=19.9, wall=117568
2022-03-07 21:24:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:26:14 | INFO | train_inner | epoch 190:    122 / 196 loss=2.372, nll_loss=1.836, ppl=3.57, wps=20718.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=36900, lr=0.000164622, gnorm=1.153, loss_scale=16, train_wall=294, gb_free=19.9, wall=117885
2022-03-07 21:30:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:30:11 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 11.345 | nll_loss 11.004 | ppl 2053.43 | wps 40691.8 | wpb 510.9 | bsz 1 | num_updates 36974 | best_loss 7.054
2022-03-07 21:30:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 36974 updates
2022-03-07 21:30:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 21:30:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 21:30:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 190 @ 36974 updates, score 11.345) (writing took 3.3312184270471334 seconds)
2022-03-07 21:30:14 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-03-07 21:30:14 | INFO | train | epoch 190 | loss 2.39 | nll_loss 1.855 | ppl 3.62 | wps 20521.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 36974 | lr 0.000164457 | gnorm 1.166 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 118125
2022-03-07 21:30:14 | INFO | fairseq.trainer | begin training epoch 191
2022-03-07 21:30:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:31:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:31:39 | INFO | train_inner | epoch 191:     27 / 196 loss=2.406, nll_loss=1.872, ppl=3.66, wps=20148.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=37000, lr=0.000164399, gnorm=1.178, loss_scale=16, train_wall=293, gb_free=19.9, wall=118209
2022-03-07 21:36:52 | INFO | train_inner | epoch 191:    127 / 196 loss=2.375, nll_loss=1.839, ppl=3.58, wps=20931.4, ups=0.32, wpb=65536, bsz=128, num_updates=37100, lr=0.000164177, gnorm=1.156, loss_scale=16, train_wall=291, gb_free=19.9, wall=118522
2022-03-07 21:38:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:40:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:40:32 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 11.353 | nll_loss 11.01 | ppl 2061.71 | wps 40633 | wpb 510.9 | bsz 1 | num_updates 37168 | best_loss 7.054
2022-03-07 21:40:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 37168 updates
2022-03-07 21:40:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 21:40:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 21:40:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 191 @ 37168 updates, score 11.353) (writing took 3.311322181019932 seconds)
2022-03-07 21:40:36 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-03-07 21:40:36 | INFO | train | epoch 191 | loss 2.388 | nll_loss 1.853 | ppl 3.61 | wps 20422.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 37168 | lr 0.000164027 | gnorm 1.165 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 118746
2022-03-07 21:40:36 | INFO | fairseq.trainer | begin training epoch 192
2022-03-07 21:40:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:42:16 | INFO | train_inner | epoch 192:     32 / 196 loss=2.399, nll_loss=1.865, ppl=3.64, wps=20155.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=37200, lr=0.000163956, gnorm=1.168, loss_scale=16, train_wall=293, gb_free=19.9, wall=118847
2022-03-07 21:45:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:47:32 | INFO | train_inner | epoch 192:    133 / 196 loss=2.372, nll_loss=1.836, ppl=3.57, wps=20711.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=37300, lr=0.000163737, gnorm=1.159, loss_scale=16, train_wall=294, gb_free=19.9, wall=119163
2022-03-07 21:50:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:50:54 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 11.324 | nll_loss 10.984 | ppl 2025.27 | wps 40477.3 | wpb 510.9 | bsz 1 | num_updates 37363 | best_loss 7.054
2022-03-07 21:50:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 37363 updates
2022-03-07 21:50:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 21:50:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 21:50:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 192 @ 37363 updates, score 11.324) (writing took 3.319533484056592 seconds)
2022-03-07 21:50:58 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-03-07 21:50:58 | INFO | train | epoch 192 | loss 2.384 | nll_loss 1.849 | ppl 3.6 | wps 20519.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 37363 | lr 0.000163598 | gnorm 1.158 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 119368
2022-03-07 21:50:58 | INFO | fairseq.trainer | begin training epoch 193
2022-03-07 21:50:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:52:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:52:57 | INFO | train_inner | epoch 193:     38 / 196 loss=2.392, nll_loss=1.858, ppl=3.62, wps=20157.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=37400, lr=0.000163517, gnorm=1.175, loss_scale=16, train_wall=293, gb_free=19.9, wall=119487
2022-03-07 21:58:10 | INFO | train_inner | epoch 193:    138 / 196 loss=2.373, nll_loss=1.837, ppl=3.57, wps=20921.3, ups=0.32, wpb=65536, bsz=128, num_updates=37500, lr=0.000163299, gnorm=1.171, loss_scale=16, train_wall=291, gb_free=19.9, wall=119801
2022-03-07 21:59:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:01:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:01:16 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 11.353 | nll_loss 11.013 | ppl 2066.09 | wps 40597.6 | wpb 510.9 | bsz 1 | num_updates 37557 | best_loss 7.054
2022-03-07 22:01:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 37557 updates
2022-03-07 22:01:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 22:01:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 22:01:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 193 @ 37557 updates, score 11.353) (writing took 3.30328905582428 seconds)
2022-03-07 22:01:20 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-03-07 22:01:20 | INFO | train | epoch 193 | loss 2.38 | nll_loss 1.845 | ppl 3.59 | wps 20418.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 37557 | lr 0.000163175 | gnorm 1.17 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 119990
2022-03-07 22:01:20 | INFO | fairseq.trainer | begin training epoch 194
2022-03-07 22:01:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:03:34 | INFO | train_inner | epoch 194:     43 / 196 loss=2.388, nll_loss=1.853, ppl=3.61, wps=20157, ups=0.31, wpb=65367, bsz=127.7, num_updates=37600, lr=0.000163082, gnorm=1.152, loss_scale=16, train_wall=293, gb_free=19.9, wall=120125
2022-03-07 22:06:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:08:51 | INFO | train_inner | epoch 194:    144 / 196 loss=2.373, nll_loss=1.838, ppl=3.57, wps=20716.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=37700, lr=0.000162866, gnorm=1.164, loss_scale=16, train_wall=294, gb_free=19.9, wall=120441
2022-03-07 22:11:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:11:38 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 11.358 | nll_loss 11.015 | ppl 2070.11 | wps 40769.3 | wpb 510.9 | bsz 1 | num_updates 37752 | best_loss 7.054
2022-03-07 22:11:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 37752 updates
2022-03-07 22:11:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 22:11:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 22:11:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 194 @ 37752 updates, score 11.358) (writing took 3.297672064974904 seconds)
2022-03-07 22:11:41 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-03-07 22:11:41 | INFO | train | epoch 194 | loss 2.378 | nll_loss 1.842 | ppl 3.59 | wps 20523.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 37752 | lr 0.000162753 | gnorm 1.16 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 120612
2022-03-07 22:11:41 | INFO | fairseq.trainer | begin training epoch 195
2022-03-07 22:11:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:13:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:14:15 | INFO | train_inner | epoch 195:     49 / 196 loss=2.373, nll_loss=1.837, ppl=3.57, wps=20163.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=37800, lr=0.00016265, gnorm=1.164, loss_scale=16, train_wall=293, gb_free=19.9, wall=120765
2022-03-07 22:19:28 | INFO | train_inner | epoch 195:    149 / 196 loss=2.376, nll_loss=1.841, ppl=3.58, wps=20929.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=37900, lr=0.000162435, gnorm=1.157, loss_scale=16, train_wall=291, gb_free=19.9, wall=121078
2022-03-07 22:19:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:21:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:22:00 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 11.364 | nll_loss 11.022 | ppl 2080.19 | wps 40482.7 | wpb 510.9 | bsz 1 | num_updates 37946 | best_loss 7.054
2022-03-07 22:22:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 37946 updates
2022-03-07 22:22:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 22:22:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 22:22:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 195 @ 37946 updates, score 11.364) (writing took 3.3305963380262256 seconds)
2022-03-07 22:22:03 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-03-07 22:22:03 | INFO | train | epoch 195 | loss 2.375 | nll_loss 1.839 | ppl 3.58 | wps 20423 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 37946 | lr 0.000162337 | gnorm 1.167 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 121234
2022-03-07 22:22:03 | INFO | fairseq.trainer | begin training epoch 196
2022-03-07 22:22:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:24:52 | INFO | train_inner | epoch 196:     54 / 196 loss=2.372, nll_loss=1.836, ppl=3.57, wps=20156.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=38000, lr=0.000162221, gnorm=1.172, loss_scale=16, train_wall=293, gb_free=19.9, wall=121403
2022-03-07 22:26:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:30:08 | INFO | train_inner | epoch 196:    155 / 196 loss=2.374, nll_loss=1.838, ppl=3.58, wps=20720.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=38100, lr=0.000162008, gnorm=1.162, loss_scale=16, train_wall=294, gb_free=19.9, wall=121719
2022-03-07 22:32:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:32:22 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 11.373 | nll_loss 11.033 | ppl 2095.2 | wps 40354.9 | wpb 510.9 | bsz 1 | num_updates 38141 | best_loss 7.054
2022-03-07 22:32:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 38141 updates
2022-03-07 22:32:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 22:32:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 22:32:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 196 @ 38141 updates, score 11.373) (writing took 3.2966439351439476 seconds)
2022-03-07 22:32:25 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-03-07 22:32:25 | INFO | train | epoch 196 | loss 2.371 | nll_loss 1.835 | ppl 3.57 | wps 20524 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 38141 | lr 0.000161921 | gnorm 1.166 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 121855
2022-03-07 22:32:25 | INFO | fairseq.trainer | begin training epoch 197
2022-03-07 22:32:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:33:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:35:33 | INFO | train_inner | epoch 197:     60 / 196 loss=2.363, nll_loss=1.827, ppl=3.55, wps=20159.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=38200, lr=0.000161796, gnorm=1.171, loss_scale=16, train_wall=293, gb_free=19.9, wall=122043
2022-03-07 22:40:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:40:49 | INFO | train_inner | epoch 197:    161 / 196 loss=2.382, nll_loss=1.846, ppl=3.6, wps=20721.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=38300, lr=0.000161585, gnorm=1.17, loss_scale=16, train_wall=293, gb_free=19.9, wall=122360
2022-03-07 22:42:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:42:43 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 11.358 | nll_loss 11.015 | ppl 2069.8 | wps 40567.2 | wpb 510.9 | bsz 1 | num_updates 38335 | best_loss 7.054
2022-03-07 22:42:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 38335 updates
2022-03-07 22:42:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 22:42:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 22:42:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 197 @ 38335 updates, score 11.358) (writing took 3.3219456621445715 seconds)
2022-03-07 22:42:47 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-03-07 22:42:47 | INFO | train | epoch 197 | loss 2.369 | nll_loss 1.833 | ppl 3.56 | wps 20424.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 38335 | lr 0.000161511 | gnorm 1.17 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 122477
2022-03-07 22:42:47 | INFO | fairseq.trainer | begin training epoch 198
2022-03-07 22:42:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:46:10 | INFO | train_inner | epoch 198:     65 / 196 loss=2.352, nll_loss=1.815, ppl=3.52, wps=20354.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=38400, lr=0.000161374, gnorm=1.164, loss_scale=16, train_wall=290, gb_free=19.9, wall=122681
2022-03-07 22:47:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:51:26 | INFO | train_inner | epoch 198:    166 / 196 loss=2.38, nll_loss=1.844, ppl=3.59, wps=20719.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=38500, lr=0.000161165, gnorm=1.165, loss_scale=16, train_wall=293, gb_free=19.9, wall=122997
2022-03-07 22:53:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:53:05 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 11.345 | nll_loss 11.005 | ppl 2055.05 | wps 40181.7 | wpb 510.9 | bsz 1 | num_updates 38530 | best_loss 7.054
2022-03-07 22:53:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 38530 updates
2022-03-07 22:53:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 22:53:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 22:53:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 198 @ 38530 updates, score 11.345) (writing took 3.3479107157327235 seconds)
2022-03-07 22:53:08 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-03-07 22:53:08 | INFO | train | epoch 198 | loss 2.366 | nll_loss 1.83 | ppl 3.55 | wps 20522 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 38530 | lr 0.000161102 | gnorm 1.17 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 123099
2022-03-07 22:53:08 | INFO | fairseq.trainer | begin training epoch 199
2022-03-07 22:53:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:54:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:56:51 | INFO | train_inner | epoch 199:     71 / 196 loss=2.349, nll_loss=1.812, ppl=3.51, wps=20153.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=38600, lr=0.000160956, gnorm=1.18, loss_scale=16, train_wall=293, gb_free=19.9, wall=123321
2022-03-07 23:00:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:02:07 | INFO | train_inner | epoch 199:    172 / 196 loss=2.377, nll_loss=1.842, ppl=3.58, wps=20725.1, ups=0.32, wpb=65536, bsz=128, num_updates=38700, lr=0.000160748, gnorm=1.176, loss_scale=16, train_wall=293, gb_free=19.9, wall=123638
2022-03-07 23:03:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:03:27 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 11.38 | nll_loss 11.039 | ppl 2104.42 | wps 40209.3 | wpb 510.9 | bsz 1 | num_updates 38724 | best_loss 7.054
2022-03-07 23:03:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 38724 updates
2022-03-07 23:03:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 23:03:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 23:03:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 199 @ 38724 updates, score 11.38) (writing took 3.333300902042538 seconds)
2022-03-07 23:03:30 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-03-07 23:03:30 | INFO | train | epoch 199 | loss 2.363 | nll_loss 1.826 | ppl 3.55 | wps 20420 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 38724 | lr 0.000160698 | gnorm 1.168 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 123721
2022-03-07 23:03:30 | INFO | fairseq.trainer | begin training epoch 200
2022-03-07 23:03:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:07:28 | INFO | train_inner | epoch 200:     76 / 196 loss=2.349, nll_loss=1.812, ppl=3.51, wps=20347.7, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=38800, lr=0.00016054, gnorm=1.169, loss_scale=16, train_wall=290, gb_free=19.9, wall=123959
2022-03-07 23:07:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:12:44 | INFO | train_inner | epoch 200:    177 / 196 loss=2.379, nll_loss=1.844, ppl=3.59, wps=20726.5, ups=0.32, wpb=65536, bsz=128, num_updates=38900, lr=0.000160334, gnorm=1.169, loss_scale=16, train_wall=293, gb_free=19.9, wall=124275
2022-03-07 23:13:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:13:48 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 11.401 | nll_loss 11.059 | ppl 2133.4 | wps 40629 | wpb 510.9 | bsz 1 | num_updates 38919 | best_loss 7.054
2022-03-07 23:13:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 38919 updates
2022-03-07 23:13:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 23:13:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 23:13:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 200 @ 38919 updates, score 11.401) (writing took 3.4519481998868287 seconds)
2022-03-07 23:13:52 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-03-07 23:13:52 | INFO | train | epoch 200 | loss 2.36 | nll_loss 1.824 | ppl 3.54 | wps 20525.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 38919 | lr 0.000160295 | gnorm 1.17 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 124342
2022-03-07 23:13:52 | INFO | fairseq.trainer | begin training epoch 201
2022-03-07 23:13:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:14:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:18:09 | INFO | train_inner | epoch 201:     82 / 196 loss=2.339, nll_loss=1.802, ppl=3.49, wps=20157.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=39000, lr=0.000160128, gnorm=1.173, loss_scale=16, train_wall=293, gb_free=19.9, wall=124599
2022-03-07 23:21:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:23:25 | INFO | train_inner | epoch 201:    183 / 196 loss=2.373, nll_loss=1.838, ppl=3.58, wps=20718.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=39100, lr=0.000159923, gnorm=1.161, loss_scale=16, train_wall=293, gb_free=19.9, wall=124916
2022-03-07 23:24:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:24:10 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 11.406 | nll_loss 11.066 | ppl 2144.6 | wps 40644.8 | wpb 510.9 | bsz 1 | num_updates 39113 | best_loss 7.054
2022-03-07 23:24:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 39113 updates
2022-03-07 23:24:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 23:24:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 23:24:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 201 @ 39113 updates, score 11.406) (writing took 3.2977807531133294 seconds)
2022-03-07 23:24:14 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-03-07 23:24:14 | INFO | train | epoch 201 | loss 2.356 | nll_loss 1.82 | ppl 3.53 | wps 20422.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 39113 | lr 0.000159897 | gnorm 1.168 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 124964
2022-03-07 23:24:14 | INFO | fairseq.trainer | begin training epoch 202
2022-03-07 23:24:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:28:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:28:49 | INFO | train_inner | epoch 202:     88 / 196 loss=2.332, nll_loss=1.794, ppl=3.47, wps=20151.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=39200, lr=0.000159719, gnorm=1.179, loss_scale=16, train_wall=293, gb_free=19.9, wall=125240
2022-03-07 23:34:02 | INFO | train_inner | epoch 202:    188 / 196 loss=2.382, nll_loss=1.847, ppl=3.6, wps=20928.3, ups=0.32, wpb=65536, bsz=128, num_updates=39300, lr=0.000159516, gnorm=1.16, loss_scale=16, train_wall=291, gb_free=19.9, wall=125553
2022-03-07 23:34:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:34:32 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 11.372 | nll_loss 11.033 | ppl 2095.31 | wps 40521.8 | wpb 510.9 | bsz 1 | num_updates 39308 | best_loss 7.054
2022-03-07 23:34:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 39308 updates
2022-03-07 23:34:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 23:34:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 23:34:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 202 @ 39308 updates, score 11.372) (writing took 3.3175967866554856 seconds)
2022-03-07 23:34:35 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-03-07 23:34:35 | INFO | train | epoch 202 | loss 2.355 | nll_loss 1.819 | ppl 3.53 | wps 20523.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 39308 | lr 0.0001595 | gnorm 1.169 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 125586
2022-03-07 23:34:36 | INFO | fairseq.trainer | begin training epoch 203
2022-03-07 23:34:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:35:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:39:27 | INFO | train_inner | epoch 203:     93 / 196 loss=2.33, nll_loss=1.792, ppl=3.46, wps=20160.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=39400, lr=0.000159313, gnorm=1.159, loss_scale=16, train_wall=293, gb_free=19.9, wall=125877
2022-03-07 23:42:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:44:43 | INFO | train_inner | epoch 203:    194 / 196 loss=2.376, nll_loss=1.841, ppl=3.58, wps=20720.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=39500, lr=0.000159111, gnorm=1.183, loss_scale=16, train_wall=293, gb_free=19.9, wall=126193
2022-03-07 23:44:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:44:54 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 11.355 | nll_loss 11.015 | ppl 2068.85 | wps 40567.7 | wpb 510.9 | bsz 1 | num_updates 39502 | best_loss 7.054
2022-03-07 23:44:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 39502 updates
2022-03-07 23:44:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 23:44:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 23:44:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 203 @ 39502 updates, score 11.355) (writing took 3.3264570930041373 seconds)
2022-03-07 23:44:57 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-03-07 23:44:57 | INFO | train | epoch 203 | loss 2.351 | nll_loss 1.814 | ppl 3.52 | wps 20422.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 39502 | lr 0.000159107 | gnorm 1.171 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 126208
2022-03-07 23:44:57 | INFO | fairseq.trainer | begin training epoch 204
2022-03-07 23:44:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:49:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:50:07 | INFO | train_inner | epoch 204:     99 / 196 loss=2.321, nll_loss=1.783, ppl=3.44, wps=20159.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=39600, lr=0.00015891, gnorm=1.17, loss_scale=16, train_wall=293, gb_free=19.9, wall=126518
2022-03-07 23:55:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:55:16 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 11.371 | nll_loss 11.03 | ppl 2090.91 | wps 40398.3 | wpb 510.9 | bsz 1 | num_updates 39697 | best_loss 7.054
2022-03-07 23:55:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 39697 updates
2022-03-07 23:55:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 23:55:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-07 23:55:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 204 @ 39697 updates, score 11.371) (writing took 3.3092004070058465 seconds)
2022-03-07 23:55:19 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-03-07 23:55:19 | INFO | train | epoch 204 | loss 2.349 | nll_loss 1.812 | ppl 3.51 | wps 20526.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 39697 | lr 0.000158716 | gnorm 1.168 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 126829
2022-03-07 23:55:19 | INFO | fairseq.trainer | begin training epoch 205
2022-03-07 23:55:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:55:28 | INFO | train_inner | epoch 205:      3 / 196 loss=2.375, nll_loss=1.84, ppl=3.58, wps=20347.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=39700, lr=0.00015871, gnorm=1.168, loss_scale=16, train_wall=290, gb_free=19.9, wall=126839
2022-03-07 23:56:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:00:45 | INFO | train_inner | epoch 205:    104 / 196 loss=2.321, nll_loss=1.782, ppl=3.44, wps=20724.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=39800, lr=0.000158511, gnorm=1.162, loss_scale=16, train_wall=293, gb_free=19.9, wall=127155
2022-03-08 00:03:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:05:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:05:37 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 11.408 | nll_loss 11.069 | ppl 2148.21 | wps 40575.1 | wpb 510.9 | bsz 1 | num_updates 39891 | best_loss 7.054
2022-03-08 00:05:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 39891 updates
2022-03-08 00:05:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 00:05:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 00:05:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 205 @ 39891 updates, score 11.408) (writing took 3.3096596840769053 seconds)
2022-03-08 00:05:41 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-03-08 00:05:41 | INFO | train | epoch 205 | loss 2.347 | nll_loss 1.81 | ppl 3.51 | wps 20417.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 39891 | lr 0.00015833 | gnorm 1.169 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 127451
2022-03-08 00:05:41 | INFO | fairseq.trainer | begin training epoch 206
2022-03-08 00:05:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:06:09 | INFO | train_inner | epoch 206:      9 / 196 loss=2.367, nll_loss=1.831, ppl=3.56, wps=20150.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=39900, lr=0.000158312, gnorm=1.176, loss_scale=16, train_wall=293, gb_free=19.9, wall=127480
2022-03-08 00:09:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:11:25 | INFO | train_inner | epoch 206:    110 / 196 loss=2.32, nll_loss=1.782, ppl=3.44, wps=20713.9, ups=0.32, wpb=65536, bsz=128, num_updates=40000, lr=0.000158114, gnorm=1.163, loss_scale=16, train_wall=294, gb_free=19.9, wall=127796
2022-03-08 00:15:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:15:59 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 11.435 | nll_loss 11.09 | ppl 2179.9 | wps 40833 | wpb 510.9 | bsz 1 | num_updates 40086 | best_loss 7.054
2022-03-08 00:15:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 40086 updates
2022-03-08 00:15:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 00:16:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 00:16:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 206 @ 40086 updates, score 11.435) (writing took 3.389338991139084 seconds)
2022-03-08 00:16:03 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-03-08 00:16:03 | INFO | train | epoch 206 | loss 2.343 | nll_loss 1.806 | ppl 3.5 | wps 20521.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 40086 | lr 0.000157944 | gnorm 1.173 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 128073
2022-03-08 00:16:03 | INFO | fairseq.trainer | begin training epoch 207
2022-03-08 00:16:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:16:47 | INFO | train_inner | epoch 207:     14 / 196 loss=2.365, nll_loss=1.83, ppl=3.55, wps=20352.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=40100, lr=0.000157917, gnorm=1.181, loss_scale=32, train_wall=290, gb_free=19.9, wall=128117
2022-03-08 00:16:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:22:03 | INFO | train_inner | epoch 207:    115 / 196 loss=2.319, nll_loss=1.781, ppl=3.44, wps=20715.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=40200, lr=0.00015772, gnorm=1.159, loss_scale=16, train_wall=294, gb_free=19.9, wall=128433
2022-03-08 00:23:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:26:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:26:21 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 11.409 | nll_loss 11.071 | ppl 2150.64 | wps 40481 | wpb 510.9 | bsz 1 | num_updates 40280 | best_loss 7.054
2022-03-08 00:26:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 40280 updates
2022-03-08 00:26:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 00:26:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 00:26:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 207 @ 40280 updates, score 11.409) (writing took 3.298378889914602 seconds)
2022-03-08 00:26:24 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-03-08 00:26:24 | INFO | train | epoch 207 | loss 2.341 | nll_loss 1.804 | ppl 3.49 | wps 20418.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 40280 | lr 0.000157563 | gnorm 1.169 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 128695
2022-03-08 00:26:25 | INFO | fairseq.trainer | begin training epoch 208
2022-03-08 00:26:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:27:27 | INFO | train_inner | epoch 208:     20 / 196 loss=2.36, nll_loss=1.824, ppl=3.54, wps=20160.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=40300, lr=0.000157524, gnorm=1.185, loss_scale=16, train_wall=293, gb_free=19.9, wall=128758
2022-03-08 00:30:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:32:43 | INFO | train_inner | epoch 208:    121 / 196 loss=2.32, nll_loss=1.782, ppl=3.44, wps=20724.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=40400, lr=0.000157329, gnorm=1.169, loss_scale=16, train_wall=293, gb_free=19.9, wall=129074
2022-03-08 00:36:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:36:43 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 11.395 | nll_loss 11.057 | ppl 2129.92 | wps 40576.5 | wpb 510.9 | bsz 1 | num_updates 40475 | best_loss 7.054
2022-03-08 00:36:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 40475 updates
2022-03-08 00:36:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 00:36:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 00:36:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 208 @ 40475 updates, score 11.395) (writing took 3.342427940107882 seconds)
2022-03-08 00:36:46 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-03-08 00:36:46 | INFO | train | epoch 208 | loss 2.338 | nll_loss 1.801 | ppl 3.48 | wps 20522.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 40475 | lr 0.000157183 | gnorm 1.175 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 129317
2022-03-08 00:36:46 | INFO | fairseq.trainer | begin training epoch 209
2022-03-08 00:36:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:37:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:38:08 | INFO | train_inner | epoch 209:     26 / 196 loss=2.35, nll_loss=1.814, ppl=3.52, wps=20150.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=40500, lr=0.000157135, gnorm=1.171, loss_scale=16, train_wall=293, gb_free=19.9, wall=129398
2022-03-08 00:43:21 | INFO | train_inner | epoch 209:    126 / 196 loss=2.319, nll_loss=1.78, ppl=3.44, wps=20932.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=40600, lr=0.000156941, gnorm=1.167, loss_scale=16, train_wall=290, gb_free=19.9, wall=129711
2022-03-08 00:44:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:46:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:47:05 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 11.449 | nll_loss 11.106 | ppl 2204.64 | wps 40536.1 | wpb 510.9 | bsz 1 | num_updates 40669 | best_loss 7.054
2022-03-08 00:47:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 40669 updates
2022-03-08 00:47:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 00:47:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 00:47:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 209 @ 40669 updates, score 11.449) (writing took 3.404688118956983 seconds)
2022-03-08 00:47:08 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-03-08 00:47:08 | INFO | train | epoch 209 | loss 2.334 | nll_loss 1.797 | ppl 3.47 | wps 20423.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 40669 | lr 0.000156808 | gnorm 1.172 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 129939
2022-03-08 00:47:08 | INFO | fairseq.trainer | begin training epoch 210
2022-03-08 00:47:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:48:45 | INFO | train_inner | epoch 210:     31 / 196 loss=2.349, nll_loss=1.812, ppl=3.51, wps=20155.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=40700, lr=0.000156748, gnorm=1.174, loss_scale=16, train_wall=293, gb_free=19.9, wall=130036
2022-03-08 00:51:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:54:01 | INFO | train_inner | epoch 210:    132 / 196 loss=2.325, nll_loss=1.787, ppl=3.45, wps=20721.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=40800, lr=0.000156556, gnorm=1.172, loss_scale=16, train_wall=294, gb_free=19.9, wall=130352
2022-03-08 00:57:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:57:26 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 11.449 | nll_loss 11.109 | ppl 2209.24 | wps 40459.4 | wpb 510.9 | bsz 1 | num_updates 40864 | best_loss 7.054
2022-03-08 00:57:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 40864 updates
2022-03-08 00:57:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 00:57:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 00:57:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 210 @ 40864 updates, score 11.449) (writing took 3.4379662834107876 seconds)
2022-03-08 00:57:30 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-03-08 00:57:30 | INFO | train | epoch 210 | loss 2.333 | nll_loss 1.795 | ppl 3.47 | wps 20520.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 40864 | lr 0.000156433 | gnorm 1.171 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 130560
2022-03-08 00:57:30 | INFO | fairseq.trainer | begin training epoch 211
2022-03-08 00:57:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:58:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:59:26 | INFO | train_inner | epoch 211:     37 / 196 loss=2.339, nll_loss=1.802, ppl=3.49, wps=20147.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=40900, lr=0.000156365, gnorm=1.172, loss_scale=16, train_wall=293, gb_free=19.9, wall=130676
2022-03-08 01:04:39 | INFO | train_inner | epoch 211:    137 / 196 loss=2.324, nll_loss=1.786, ppl=3.45, wps=20924.5, ups=0.32, wpb=65536, bsz=128, num_updates=41000, lr=0.000156174, gnorm=1.164, loss_scale=16, train_wall=291, gb_free=19.9, wall=130990
2022-03-08 01:04:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:07:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:07:48 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 11.422 | nll_loss 11.084 | ppl 2170.11 | wps 40779.1 | wpb 510.9 | bsz 1 | num_updates 41058 | best_loss 7.054
2022-03-08 01:07:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 41058 updates
2022-03-08 01:07:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 01:07:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 01:07:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 211 @ 41058 updates, score 11.422) (writing took 3.440316279884428 seconds)
2022-03-08 01:07:52 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-03-08 01:07:52 | INFO | train | epoch 211 | loss 2.331 | nll_loss 1.793 | ppl 3.47 | wps 20416.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 41058 | lr 0.000156063 | gnorm 1.174 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 131182
2022-03-08 01:07:52 | INFO | fairseq.trainer | begin training epoch 212
2022-03-08 01:07:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:10:03 | INFO | train_inner | epoch 212:     42 / 196 loss=2.327, nll_loss=1.789, ppl=3.46, wps=20155.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=41100, lr=0.000155984, gnorm=1.188, loss_scale=16, train_wall=293, gb_free=19.9, wall=131314
2022-03-08 01:11:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:15:20 | INFO | train_inner | epoch 212:    143 / 196 loss=2.328, nll_loss=1.79, ppl=3.46, wps=20718.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=41200, lr=0.000155794, gnorm=1.175, loss_scale=16, train_wall=293, gb_free=19.9, wall=131630
2022-03-08 01:18:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:18:10 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 11.422 | nll_loss 11.083 | ppl 2169.69 | wps 40399 | wpb 510.9 | bsz 1 | num_updates 41253 | best_loss 7.054
2022-03-08 01:18:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 41253 updates
2022-03-08 01:18:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 01:18:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 01:18:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 212 @ 41253 updates, score 11.422) (writing took 3.337972889188677 seconds)
2022-03-08 01:18:14 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-03-08 01:18:14 | INFO | train | epoch 212 | loss 2.328 | nll_loss 1.79 | ppl 3.46 | wps 20522.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 41253 | lr 0.000155694 | gnorm 1.179 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 131804
2022-03-08 01:18:14 | INFO | fairseq.trainer | begin training epoch 213
2022-03-08 01:18:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:18:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:20:44 | INFO | train_inner | epoch 213:     48 / 196 loss=2.328, nll_loss=1.791, ppl=3.46, wps=20159.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=41300, lr=0.000155606, gnorm=1.181, loss_scale=16, train_wall=293, gb_free=19.9, wall=131954
2022-03-08 01:25:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:26:00 | INFO | train_inner | epoch 213:    149 / 196 loss=2.325, nll_loss=1.787, ppl=3.45, wps=20716.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=41400, lr=0.000155417, gnorm=1.173, loss_scale=16, train_wall=294, gb_free=19.9, wall=132271
2022-03-08 01:28:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:28:32 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 11.444 | nll_loss 11.103 | ppl 2199.81 | wps 40322.3 | wpb 510.9 | bsz 1 | num_updates 41447 | best_loss 7.054
2022-03-08 01:28:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 41447 updates
2022-03-08 01:28:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 01:28:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 01:28:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 213 @ 41447 updates, score 11.444) (writing took 3.3061958821490407 seconds)
2022-03-08 01:28:35 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-03-08 01:28:35 | INFO | train | epoch 213 | loss 2.325 | nll_loss 1.787 | ppl 3.45 | wps 20421.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 41447 | lr 0.000155329 | gnorm 1.174 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 132426
2022-03-08 01:28:35 | INFO | fairseq.trainer | begin training epoch 214
2022-03-08 01:28:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:31:22 | INFO | train_inner | epoch 214:     53 / 196 loss=2.319, nll_loss=1.781, ppl=3.44, wps=20327.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=41500, lr=0.00015523, gnorm=1.17, loss_scale=16, train_wall=290, gb_free=19.9, wall=132592
2022-03-08 01:32:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:36:39 | INFO | train_inner | epoch 214:    154 / 196 loss=2.327, nll_loss=1.789, ppl=3.46, wps=20633.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=41600, lr=0.000155043, gnorm=1.185, loss_scale=16, train_wall=295, gb_free=19.9, wall=132910
2022-03-08 01:38:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:38:56 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 11.463 | nll_loss 11.123 | ppl 2230.97 | wps 40577.5 | wpb 510.9 | bsz 1 | num_updates 41642 | best_loss 7.054
2022-03-08 01:38:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 41642 updates
2022-03-08 01:38:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 01:38:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 01:39:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 214 @ 41642 updates, score 11.463) (writing took 3.3517846702598035 seconds)
2022-03-08 01:39:00 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-03-08 01:39:00 | INFO | train | epoch 214 | loss 2.323 | nll_loss 1.785 | ppl 3.45 | wps 20447 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 41642 | lr 0.000154965 | gnorm 1.177 | loss_scale 16 | train_wall 571 | gb_free 19.9 | wall 133050
2022-03-08 01:39:00 | INFO | fairseq.trainer | begin training epoch 215
2022-03-08 01:39:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:39:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:42:04 | INFO | train_inner | epoch 215:     59 / 196 loss=2.317, nll_loss=1.779, ppl=3.43, wps=20111.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=41700, lr=0.000154857, gnorm=1.168, loss_scale=16, train_wall=293, gb_free=19.9, wall=133235
2022-03-08 01:46:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:47:21 | INFO | train_inner | epoch 215:    160 / 196 loss=2.325, nll_loss=1.787, ppl=3.45, wps=20719.9, ups=0.32, wpb=65536, bsz=128, num_updates=41800, lr=0.000154672, gnorm=1.174, loss_scale=16, train_wall=293, gb_free=19.9, wall=133551
2022-03-08 01:49:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:49:18 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 11.435 | nll_loss 11.096 | ppl 2188.96 | wps 40519 | wpb 510.9 | bsz 1 | num_updates 41836 | best_loss 7.054
2022-03-08 01:49:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 41836 updates
2022-03-08 01:49:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 01:49:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 01:49:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 215 @ 41836 updates, score 11.435) (writing took 3.4781680437736213 seconds)
2022-03-08 01:49:22 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-03-08 01:49:22 | INFO | train | epoch 215 | loss 2.32 | nll_loss 1.782 | ppl 3.44 | wps 20411.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 41836 | lr 0.000154605 | gnorm 1.17 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 133672
2022-03-08 01:49:22 | INFO | fairseq.trainer | begin training epoch 216
2022-03-08 01:49:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:52:42 | INFO | train_inner | epoch 216:     64 / 196 loss=2.311, nll_loss=1.773, ppl=3.42, wps=20343, ups=0.31, wpb=65367, bsz=127.7, num_updates=41900, lr=0.000154487, gnorm=1.169, loss_scale=16, train_wall=290, gb_free=19.9, wall=133873
2022-03-08 01:53:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:57:58 | INFO | train_inner | epoch 216:    165 / 196 loss=2.335, nll_loss=1.798, ppl=3.48, wps=20718.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=42000, lr=0.000154303, gnorm=1.181, loss_scale=16, train_wall=293, gb_free=19.9, wall=134189
2022-03-08 01:59:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:59:40 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 11.454 | nll_loss 11.114 | ppl 2215.73 | wps 40342.7 | wpb 510.9 | bsz 1 | num_updates 42031 | best_loss 7.054
2022-03-08 01:59:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 42031 updates
2022-03-08 01:59:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 01:59:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 01:59:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 216 @ 42031 updates, score 11.454) (writing took 3.465505332686007 seconds)
2022-03-08 01:59:44 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-03-08 01:59:44 | INFO | train | epoch 216 | loss 2.318 | nll_loss 1.78 | ppl 3.43 | wps 20519.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 42031 | lr 0.000154246 | gnorm 1.185 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 134294
2022-03-08 01:59:44 | INFO | fairseq.trainer | begin training epoch 217
2022-03-08 01:59:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:00:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:03:23 | INFO | train_inner | epoch 217:     70 / 196 loss=2.303, nll_loss=1.765, ppl=3.4, wps=20146.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=42100, lr=0.00015412, gnorm=1.196, loss_scale=16, train_wall=293, gb_free=19.9, wall=134513
2022-03-08 02:06:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:08:39 | INFO | train_inner | epoch 217:    171 / 196 loss=2.325, nll_loss=1.788, ppl=3.45, wps=20723.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=42200, lr=0.000153937, gnorm=1.187, loss_scale=16, train_wall=293, gb_free=19.9, wall=134830
2022-03-08 02:09:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:10:02 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 11.469 | nll_loss 11.128 | ppl 2238.66 | wps 40711.6 | wpb 510.9 | bsz 1 | num_updates 42225 | best_loss 7.054
2022-03-08 02:10:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 42225 updates
2022-03-08 02:10:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 02:10:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 02:10:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 217 @ 42225 updates, score 11.469) (writing took 3.4376234649680555 seconds)
2022-03-08 02:10:05 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-03-08 02:10:05 | INFO | train | epoch 217 | loss 2.313 | nll_loss 1.775 | ppl 3.42 | wps 20421 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 42225 | lr 0.000153892 | gnorm 1.183 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 134916
2022-03-08 02:10:05 | INFO | fairseq.trainer | begin training epoch 218
2022-03-08 02:10:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:13:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:14:03 | INFO | train_inner | epoch 218:     76 / 196 loss=2.295, nll_loss=1.756, ppl=3.38, wps=20155.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=42300, lr=0.000153755, gnorm=1.171, loss_scale=16, train_wall=293, gb_free=19.9, wall=135154
2022-03-08 02:19:16 | INFO | train_inner | epoch 218:    176 / 196 loss=2.332, nll_loss=1.794, ppl=3.47, wps=20931.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=42400, lr=0.000153574, gnorm=1.194, loss_scale=16, train_wall=290, gb_free=19.9, wall=135467
2022-03-08 02:20:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:20:24 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 11.452 | nll_loss 11.111 | ppl 2211.84 | wps 40348.9 | wpb 510.9 | bsz 1 | num_updates 42420 | best_loss 7.054
2022-03-08 02:20:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 42420 updates
2022-03-08 02:20:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 02:20:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 02:20:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 218 @ 42420 updates, score 11.452) (writing took 3.4023622181266546 seconds)
2022-03-08 02:20:27 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-03-08 02:20:27 | INFO | train | epoch 218 | loss 2.312 | nll_loss 1.774 | ppl 3.42 | wps 20523.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 42420 | lr 0.000153538 | gnorm 1.185 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 135538
2022-03-08 02:20:27 | INFO | fairseq.trainer | begin training epoch 219
2022-03-08 02:20:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:20:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:24:41 | INFO | train_inner | epoch 219:     81 / 196 loss=2.292, nll_loss=1.753, ppl=3.37, wps=20147.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=42500, lr=0.000153393, gnorm=1.182, loss_scale=16, train_wall=293, gb_free=19.9, wall=135791
2022-03-08 02:27:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:29:57 | INFO | train_inner | epoch 219:    182 / 196 loss=2.331, nll_loss=1.794, ppl=3.47, wps=20720.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=42600, lr=0.000153213, gnorm=1.186, loss_scale=16, train_wall=293, gb_free=19.9, wall=136108
2022-03-08 02:30:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:30:46 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 11.439 | nll_loss 11.101 | ppl 2196.27 | wps 40453.5 | wpb 510.9 | bsz 1 | num_updates 42614 | best_loss 7.054
2022-03-08 02:30:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 42614 updates
2022-03-08 02:30:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 02:30:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 02:30:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 219 @ 42614 updates, score 11.439) (writing took 3.429912480060011 seconds)
2022-03-08 02:30:49 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-03-08 02:30:49 | INFO | train | epoch 219 | loss 2.31 | nll_loss 1.771 | ppl 3.41 | wps 20415.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 42614 | lr 0.000153188 | gnorm 1.184 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 136160
2022-03-08 02:30:49 | INFO | fairseq.trainer | begin training epoch 220
2022-03-08 02:30:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:34:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:35:22 | INFO | train_inner | epoch 220:     87 / 196 loss=2.285, nll_loss=1.746, ppl=3.35, wps=20150, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=42700, lr=0.000153033, gnorm=1.187, loss_scale=16, train_wall=293, gb_free=19.9, wall=136432
2022-03-08 02:40:35 | INFO | train_inner | epoch 220:    187 / 196 loss=2.333, nll_loss=1.795, ppl=3.47, wps=20924.2, ups=0.32, wpb=65536, bsz=128, num_updates=42800, lr=0.000152854, gnorm=1.191, loss_scale=16, train_wall=291, gb_free=19.9, wall=136745
2022-03-08 02:41:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:41:08 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 11.513 | nll_loss 11.176 | ppl 2312.96 | wps 40694.6 | wpb 510.9 | bsz 1 | num_updates 42809 | best_loss 7.054
2022-03-08 02:41:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 42809 updates
2022-03-08 02:41:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 02:41:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 02:41:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 220 @ 42809 updates, score 11.513) (writing took 3.385102898813784 seconds)
2022-03-08 02:41:11 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-03-08 02:41:11 | INFO | train | epoch 220 | loss 2.307 | nll_loss 1.768 | ppl 3.41 | wps 20523.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 42809 | lr 0.000152838 | gnorm 1.189 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 136781
2022-03-08 02:41:11 | INFO | fairseq.trainer | begin training epoch 221
2022-03-08 02:41:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:41:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:45:59 | INFO | train_inner | epoch 221:     92 / 196 loss=2.281, nll_loss=1.741, ppl=3.34, wps=20152, ups=0.31, wpb=65367, bsz=127.7, num_updates=42900, lr=0.000152676, gnorm=1.176, loss_scale=16, train_wall=293, gb_free=19.9, wall=137070
2022-03-08 02:48:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:51:15 | INFO | train_inner | epoch 221:    193 / 196 loss=2.331, nll_loss=1.793, ppl=3.47, wps=20719.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=43000, lr=0.000152499, gnorm=1.181, loss_scale=16, train_wall=293, gb_free=19.9, wall=137386
2022-03-08 02:51:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:51:30 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 11.47 | nll_loss 11.131 | ppl 2243.31 | wps 40289.1 | wpb 510.9 | bsz 1 | num_updates 43003 | best_loss 7.054
2022-03-08 02:51:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 43003 updates
2022-03-08 02:51:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 02:51:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 02:51:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 221 @ 43003 updates, score 11.47) (writing took 3.394166013225913 seconds)
2022-03-08 02:51:33 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-03-08 02:51:33 | INFO | train | epoch 221 | loss 2.304 | nll_loss 1.766 | ppl 3.4 | wps 20413 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43003 | lr 0.000152493 | gnorm 1.176 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 137403
2022-03-08 02:51:33 | INFO | fairseq.trainer | begin training epoch 222
2022-03-08 02:51:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:54:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:56:40 | INFO | train_inner | epoch 222:     98 / 196 loss=2.275, nll_loss=1.735, ppl=3.33, wps=20147.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=43100, lr=0.000152322, gnorm=1.179, loss_scale=16, train_wall=293, gb_free=19.9, wall=137710
2022-03-08 03:01:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:01:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:01:51 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 11.462 | nll_loss 11.121 | ppl 2227.72 | wps 40642.4 | wpb 510.9 | bsz 1 | num_updates 43197 | best_loss 7.054
2022-03-08 03:01:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 43197 updates
2022-03-08 03:01:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 03:01:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 03:01:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 222 @ 43197 updates, score 11.462) (writing took 3.4259684812277555 seconds)
2022-03-08 03:01:55 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-03-08 03:01:55 | INFO | train | epoch 222 | loss 2.303 | nll_loss 1.764 | ppl 3.4 | wps 20419.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43197 | lr 0.00015215 | gnorm 1.189 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 138025
2022-03-08 03:01:55 | INFO | fairseq.trainer | begin training epoch 223
2022-03-08 03:01:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:02:04 | INFO | train_inner | epoch 223:      3 / 196 loss=2.332, nll_loss=1.795, ppl=3.47, wps=20151.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=43200, lr=0.000152145, gnorm=1.199, loss_scale=16, train_wall=293, gb_free=19.9, wall=138035
2022-03-08 03:07:17 | INFO | train_inner | epoch 223:    103 / 196 loss=2.275, nll_loss=1.735, ppl=3.33, wps=20928.2, ups=0.32, wpb=65536, bsz=128, num_updates=43300, lr=0.000151969, gnorm=1.173, loss_scale=16, train_wall=291, gb_free=19.9, wall=138348
2022-03-08 03:08:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:12:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:12:13 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 11.448 | nll_loss 11.109 | ppl 2208.75 | wps 40582.7 | wpb 510.9 | bsz 1 | num_updates 43392 | best_loss 7.054
2022-03-08 03:12:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 43392 updates
2022-03-08 03:12:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 03:12:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 03:12:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 223 @ 43392 updates, score 11.448) (writing took 3.3415376362390816 seconds)
2022-03-08 03:12:17 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-03-08 03:12:17 | INFO | train | epoch 223 | loss 2.301 | nll_loss 1.762 | ppl 3.39 | wps 20523.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 43392 | lr 0.000151808 | gnorm 1.181 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 138647
2022-03-08 03:12:17 | INFO | fairseq.trainer | begin training epoch 224
2022-03-08 03:12:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:12:42 | INFO | train_inner | epoch 224:      8 / 196 loss=2.322, nll_loss=1.785, ppl=3.45, wps=20152.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=43400, lr=0.000151794, gnorm=1.189, loss_scale=16, train_wall=293, gb_free=19.9, wall=138672
2022-03-08 03:15:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:17:58 | INFO | train_inner | epoch 224:    109 / 196 loss=2.275, nll_loss=1.735, ppl=3.33, wps=20721.8, ups=0.32, wpb=65536, bsz=128, num_updates=43500, lr=0.00015162, gnorm=1.184, loss_scale=16, train_wall=293, gb_free=19.9, wall=138989
2022-03-08 03:22:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:22:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:22:35 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 11.501 | nll_loss 11.164 | ppl 2294.99 | wps 40318.7 | wpb 510.9 | bsz 1 | num_updates 43586 | best_loss 7.054
2022-03-08 03:22:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 43586 updates
2022-03-08 03:22:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 03:22:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 03:22:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 224 @ 43586 updates, score 11.501) (writing took 3.3406274123117328 seconds)
2022-03-08 03:22:38 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-03-08 03:22:38 | INFO | train | epoch 224 | loss 2.298 | nll_loss 1.759 | ppl 3.38 | wps 20418.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43586 | lr 0.00015147 | gnorm 1.185 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 139269
2022-03-08 03:22:38 | INFO | fairseq.trainer | begin training epoch 225
2022-03-08 03:22:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:23:22 | INFO | train_inner | epoch 225:     14 / 196 loss=2.317, nll_loss=1.779, ppl=3.43, wps=20149.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=43600, lr=0.000151446, gnorm=1.181, loss_scale=16, train_wall=293, gb_free=19.9, wall=139313
2022-03-08 03:28:36 | INFO | train_inner | epoch 225:    114 / 196 loss=2.277, nll_loss=1.737, ppl=3.33, wps=20925.3, ups=0.32, wpb=65536, bsz=128, num_updates=43700, lr=0.000151272, gnorm=1.171, loss_scale=16, train_wall=291, gb_free=19.9, wall=139626
2022-03-08 03:29:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:32:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:32:57 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 11.519 | nll_loss 11.184 | ppl 2326.09 | wps 40418.6 | wpb 510.9 | bsz 1 | num_updates 43781 | best_loss 7.054
2022-03-08 03:32:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 43781 updates
2022-03-08 03:32:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 03:33:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 03:33:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 225 @ 43781 updates, score 11.519) (writing took 3.350217906292528 seconds)
2022-03-08 03:33:00 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-03-08 03:33:00 | INFO | train | epoch 225 | loss 2.296 | nll_loss 1.757 | ppl 3.38 | wps 20521.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 43781 | lr 0.000151132 | gnorm 1.18 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 139891
2022-03-08 03:33:00 | INFO | fairseq.trainer | begin training epoch 226
2022-03-08 03:33:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:34:00 | INFO | train_inner | epoch 226:     19 / 196 loss=2.312, nll_loss=1.774, ppl=3.42, wps=20156.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=43800, lr=0.000151099, gnorm=1.188, loss_scale=16, train_wall=293, gb_free=19.9, wall=139950
2022-03-08 03:36:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:39:16 | INFO | train_inner | epoch 226:    120 / 196 loss=2.276, nll_loss=1.736, ppl=3.33, wps=20715.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=43900, lr=0.000150927, gnorm=1.18, loss_scale=16, train_wall=294, gb_free=19.9, wall=140267
2022-03-08 03:42:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:43:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:43:19 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 11.523 | nll_loss 11.185 | ppl 2328.73 | wps 40341 | wpb 510.9 | bsz 1 | num_updates 43975 | best_loss 7.054
2022-03-08 03:43:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 43975 updates
2022-03-08 03:43:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 03:43:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 03:43:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 226 @ 43975 updates, score 11.523) (writing took 3.422981093171984 seconds)
2022-03-08 03:43:22 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-03-08 03:43:22 | INFO | train | epoch 226 | loss 2.292 | nll_loss 1.753 | ppl 3.37 | wps 20414.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43975 | lr 0.000150799 | gnorm 1.182 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 140513
2022-03-08 03:43:22 | INFO | fairseq.trainer | begin training epoch 227
2022-03-08 03:43:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:44:41 | INFO | train_inner | epoch 227:     25 / 196 loss=2.306, nll_loss=1.767, ppl=3.4, wps=20149.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=44000, lr=0.000150756, gnorm=1.186, loss_scale=16, train_wall=293, gb_free=19.9, wall=140591
2022-03-08 03:49:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:49:57 | INFO | train_inner | epoch 227:    126 / 196 loss=2.276, nll_loss=1.736, ppl=3.33, wps=20715, ups=0.32, wpb=65536, bsz=128, num_updates=44100, lr=0.000150585, gnorm=1.176, loss_scale=16, train_wall=293, gb_free=19.9, wall=140908
2022-03-08 03:53:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:53:41 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 11.523 | nll_loss 11.185 | ppl 2328.94 | wps 40220.9 | wpb 510.9 | bsz 1 | num_updates 44170 | best_loss 7.054
2022-03-08 03:53:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 44170 updates
2022-03-08 03:53:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 03:53:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 03:53:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 227 @ 44170 updates, score 11.523) (writing took 3.3481562952511013 seconds)
2022-03-08 03:53:44 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-03-08 03:53:44 | INFO | train | epoch 227 | loss 2.291 | nll_loss 1.752 | ppl 3.37 | wps 20520.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 44170 | lr 0.000150465 | gnorm 1.183 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 141135
2022-03-08 03:53:44 | INFO | fairseq.trainer | begin training epoch 228
2022-03-08 03:53:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:55:18 | INFO | train_inner | epoch 228:     30 / 196 loss=2.305, nll_loss=1.766, ppl=3.4, wps=20344.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=44200, lr=0.000150414, gnorm=1.19, loss_scale=16, train_wall=290, gb_free=19.9, wall=141229
2022-03-08 03:56:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:00:35 | INFO | train_inner | epoch 228:    131 / 196 loss=2.275, nll_loss=1.735, ppl=3.33, wps=20714.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=44300, lr=0.000150244, gnorm=1.174, loss_scale=16, train_wall=294, gb_free=19.9, wall=141545
2022-03-08 04:03:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:03:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:04:03 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 11.518 | nll_loss 11.179 | ppl 2317.86 | wps 40604.3 | wpb 510.9 | bsz 1 | num_updates 44364 | best_loss 7.054
2022-03-08 04:04:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 44364 updates
2022-03-08 04:04:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 04:04:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 04:04:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 228 @ 44364 updates, score 11.518) (writing took 3.2730526267550886 seconds)
2022-03-08 04:04:06 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-03-08 04:04:06 | INFO | train | epoch 228 | loss 2.288 | nll_loss 1.749 | ppl 3.36 | wps 20418.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 44364 | lr 0.000150136 | gnorm 1.18 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 141757
2022-03-08 04:04:06 | INFO | fairseq.trainer | begin training epoch 229
2022-03-08 04:04:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:05:59 | INFO | train_inner | epoch 229:     36 / 196 loss=2.296, nll_loss=1.757, ppl=3.38, wps=20161.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=44400, lr=0.000150075, gnorm=1.19, loss_scale=16, train_wall=293, gb_free=19.9, wall=141869
2022-03-08 04:10:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:11:15 | INFO | train_inner | epoch 229:    137 / 196 loss=2.283, nll_loss=1.743, ppl=3.35, wps=20719.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=44500, lr=0.000149906, gnorm=1.176, loss_scale=16, train_wall=293, gb_free=19.9, wall=142186
2022-03-08 04:14:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:14:25 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 11.529 | nll_loss 11.192 | ppl 2339.38 | wps 40581.4 | wpb 510.9 | bsz 1 | num_updates 44559 | best_loss 7.054
2022-03-08 04:14:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 44559 updates
2022-03-08 04:14:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 04:14:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 04:14:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 229 @ 44559 updates, score 11.529) (writing took 3.2750167390331626 seconds)
2022-03-08 04:14:28 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-03-08 04:14:28 | INFO | train | epoch 229 | loss 2.286 | nll_loss 1.747 | ppl 3.36 | wps 20523.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 44559 | lr 0.000149807 | gnorm 1.184 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 142378
2022-03-08 04:14:28 | INFO | fairseq.trainer | begin training epoch 230
2022-03-08 04:14:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:16:36 | INFO | train_inner | epoch 230:     41 / 196 loss=2.287, nll_loss=1.747, ppl=3.36, wps=20349.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=44600, lr=0.000149738, gnorm=1.198, loss_scale=16, train_wall=290, gb_free=19.9, wall=142507
2022-03-08 04:17:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:21:53 | INFO | train_inner | epoch 230:    142 / 196 loss=2.28, nll_loss=1.741, ppl=3.34, wps=20712.7, ups=0.32, wpb=65536, bsz=128, num_updates=44700, lr=0.000149571, gnorm=1.174, loss_scale=16, train_wall=294, gb_free=19.9, wall=142823
2022-03-08 04:23:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:24:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:24:47 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 11.505 | nll_loss 11.169 | ppl 2302.66 | wps 40426.2 | wpb 510.9 | bsz 1 | num_updates 44753 | best_loss 7.054
2022-03-08 04:24:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 44753 updates
2022-03-08 04:24:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 04:24:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 04:24:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 230 @ 44753 updates, score 11.505) (writing took 3.4356355597265065 seconds)
2022-03-08 04:24:50 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-03-08 04:24:50 | INFO | train | epoch 230 | loss 2.284 | nll_loss 1.744 | ppl 3.35 | wps 20406.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 44753 | lr 0.000149482 | gnorm 1.185 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 143001
2022-03-08 04:24:50 | INFO | fairseq.trainer | begin training epoch 231
2022-03-08 04:24:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:27:17 | INFO | train_inner | epoch 231:     47 / 196 loss=2.288, nll_loss=1.748, ppl=3.36, wps=20139.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=44800, lr=0.000149404, gnorm=1.19, loss_scale=16, train_wall=293, gb_free=19.9, wall=143148
2022-03-08 04:30:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:32:34 | INFO | train_inner | epoch 231:    148 / 196 loss=2.28, nll_loss=1.74, ppl=3.34, wps=20721.6, ups=0.32, wpb=65536, bsz=128, num_updates=44900, lr=0.000149237, gnorm=1.186, loss_scale=16, train_wall=293, gb_free=19.9, wall=143464
2022-03-08 04:35:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:35:08 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 11.532 | nll_loss 11.196 | ppl 2345.92 | wps 40551.1 | wpb 510.9 | bsz 1 | num_updates 44948 | best_loss 7.054
2022-03-08 04:35:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 44948 updates
2022-03-08 04:35:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 04:35:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 04:35:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 231 @ 44948 updates, score 11.532) (writing took 3.406004959717393 seconds)
2022-03-08 04:35:12 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-03-08 04:35:12 | INFO | train | epoch 231 | loss 2.282 | nll_loss 1.742 | ppl 3.35 | wps 20522 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 44948 | lr 0.000149157 | gnorm 1.185 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 143622
2022-03-08 04:35:12 | INFO | fairseq.trainer | begin training epoch 232
2022-03-08 04:35:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:37:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:37:58 | INFO | train_inner | epoch 232:     53 / 196 loss=2.283, nll_loss=1.744, ppl=3.35, wps=20151.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=45000, lr=0.000149071, gnorm=1.186, loss_scale=16, train_wall=293, gb_free=19.9, wall=143788
2022-03-08 04:43:11 | INFO | train_inner | epoch 232:    153 / 196 loss=2.282, nll_loss=1.742, ppl=3.35, wps=20930.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=45100, lr=0.000148906, gnorm=1.211, loss_scale=16, train_wall=291, gb_free=19.9, wall=144102
2022-03-08 04:44:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:45:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:45:30 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 11.56 | nll_loss 11.222 | ppl 2388.63 | wps 40510 | wpb 510.9 | bsz 1 | num_updates 45142 | best_loss 7.054
2022-03-08 04:45:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 45142 updates
2022-03-08 04:45:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 04:45:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 04:45:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 232 @ 45142 updates, score 11.56) (writing took 3.420189836062491 seconds)
2022-03-08 04:45:34 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-03-08 04:45:34 | INFO | train | epoch 232 | loss 2.28 | nll_loss 1.74 | ppl 3.34 | wps 20416.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 45142 | lr 0.000148837 | gnorm 1.199 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 144244
2022-03-08 04:45:34 | INFO | fairseq.trainer | begin training epoch 233
2022-03-08 04:45:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:48:35 | INFO | train_inner | epoch 233:     58 / 196 loss=2.275, nll_loss=1.735, ppl=3.33, wps=20150, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=45200, lr=0.000148741, gnorm=1.175, loss_scale=16, train_wall=293, gb_free=19.9, wall=144426
2022-03-08 04:51:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:53:52 | INFO | train_inner | epoch 233:    159 / 196 loss=2.282, nll_loss=1.742, ppl=3.35, wps=20719.9, ups=0.32, wpb=65536, bsz=128, num_updates=45300, lr=0.000148577, gnorm=1.205, loss_scale=16, train_wall=293, gb_free=19.9, wall=144742
2022-03-08 04:55:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:55:52 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 11.506 | nll_loss 11.167 | ppl 2300.1 | wps 40546.7 | wpb 510.9 | bsz 1 | num_updates 45337 | best_loss 7.054
2022-03-08 04:55:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 45337 updates
2022-03-08 04:55:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 04:55:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 04:55:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 233 @ 45337 updates, score 11.506) (writing took 3.4455816517584026 seconds)
2022-03-08 04:55:56 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-03-08 04:55:56 | INFO | train | epoch 233 | loss 2.278 | nll_loss 1.738 | ppl 3.34 | wps 20521.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 45337 | lr 0.000148516 | gnorm 1.192 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 144866
2022-03-08 04:55:56 | INFO | fairseq.trainer | begin training epoch 234
2022-03-08 04:55:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:58:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:59:16 | INFO | train_inner | epoch 234:     64 / 196 loss=2.27, nll_loss=1.73, ppl=3.32, wps=20146.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=45400, lr=0.000148413, gnorm=1.175, loss_scale=16, train_wall=293, gb_free=19.9, wall=145067
2022-03-08 05:04:29 | INFO | train_inner | epoch 234:    164 / 196 loss=2.284, nll_loss=1.745, ppl=3.35, wps=20925.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=45500, lr=0.00014825, gnorm=1.188, loss_scale=16, train_wall=291, gb_free=19.9, wall=145380
2022-03-08 05:05:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:06:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:06:14 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 11.539 | nll_loss 11.202 | ppl 2355.41 | wps 40383.5 | wpb 510.9 | bsz 1 | num_updates 45531 | best_loss 7.054
2022-03-08 05:06:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 45531 updates
2022-03-08 05:06:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 05:06:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 05:06:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 234 @ 45531 updates, score 11.539) (writing took 3.501411926932633 seconds)
2022-03-08 05:06:18 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-03-08 05:06:18 | INFO | train | epoch 234 | loss 2.276 | nll_loss 1.736 | ppl 3.33 | wps 20410.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 45531 | lr 0.000148199 | gnorm 1.18 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 145488
2022-03-08 05:06:18 | INFO | fairseq.trainer | begin training epoch 235
2022-03-08 05:06:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:09:54 | INFO | train_inner | epoch 235:     69 / 196 loss=2.264, nll_loss=1.723, ppl=3.3, wps=20146.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=45600, lr=0.000148087, gnorm=1.177, loss_scale=16, train_wall=293, gb_free=19.9, wall=145704
2022-03-08 05:12:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:15:10 | INFO | train_inner | epoch 235:    170 / 196 loss=2.292, nll_loss=1.753, ppl=3.37, wps=20729.8, ups=0.32, wpb=65536, bsz=128, num_updates=45700, lr=0.000147925, gnorm=1.189, loss_scale=16, train_wall=293, gb_free=19.9, wall=146020
2022-03-08 05:16:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:16:36 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 11.559 | nll_loss 11.223 | ppl 2390.3 | wps 40409.9 | wpb 510.9 | bsz 1 | num_updates 45726 | best_loss 7.054
2022-03-08 05:16:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 45726 updates
2022-03-08 05:16:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 05:16:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 05:16:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 235 @ 45726 updates, score 11.559) (writing took 3.560682099778205 seconds)
2022-03-08 05:16:40 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-03-08 05:16:40 | INFO | train | epoch 235 | loss 2.274 | nll_loss 1.734 | ppl 3.33 | wps 20523.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 45726 | lr 0.000147883 | gnorm 1.185 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 146110
2022-03-08 05:16:40 | INFO | fairseq.trainer | begin training epoch 236
2022-03-08 05:16:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:18:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:20:34 | INFO | train_inner | epoch 236:     75 / 196 loss=2.257, nll_loss=1.716, ppl=3.29, wps=20139.8, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=45800, lr=0.000147764, gnorm=1.184, loss_scale=16, train_wall=293, gb_free=19.9, wall=146345
2022-03-08 05:25:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:25:51 | INFO | train_inner | epoch 236:    176 / 196 loss=2.287, nll_loss=1.748, ppl=3.36, wps=20722.8, ups=0.32, wpb=65536, bsz=128, num_updates=45900, lr=0.000147602, gnorm=1.195, loss_scale=16, train_wall=293, gb_free=19.9, wall=146661
2022-03-08 05:26:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:26:58 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 11.577 | nll_loss 11.241 | ppl 2421.1 | wps 40485.7 | wpb 510.9 | bsz 1 | num_updates 45920 | best_loss 7.054
2022-03-08 05:26:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 45920 updates
2022-03-08 05:26:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 05:27:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 05:27:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 236 @ 45920 updates, score 11.577) (writing took 3.4839908350259066 seconds)
2022-03-08 05:27:01 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-03-08 05:27:01 | INFO | train | epoch 236 | loss 2.271 | nll_loss 1.731 | ppl 3.32 | wps 20415.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 45920 | lr 0.00014757 | gnorm 1.186 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 146732
2022-03-08 05:27:02 | INFO | fairseq.trainer | begin training epoch 237
2022-03-08 05:27:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:31:12 | INFO | train_inner | epoch 237:     80 / 196 loss=2.246, nll_loss=1.705, ppl=3.26, wps=20343.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=46000, lr=0.000147442, gnorm=1.177, loss_scale=16, train_wall=290, gb_free=19.9, wall=146983
2022-03-08 05:32:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:36:28 | INFO | train_inner | epoch 237:    181 / 196 loss=2.291, nll_loss=1.752, ppl=3.37, wps=20734.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=46100, lr=0.000147282, gnorm=1.186, loss_scale=16, train_wall=293, gb_free=19.9, wall=147299
2022-03-08 05:37:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:37:20 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 11.596 | nll_loss 11.259 | ppl 2450.01 | wps 40614.2 | wpb 510.9 | bsz 1 | num_updates 46115 | best_loss 7.054
2022-03-08 05:37:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 46115 updates
2022-03-08 05:37:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 05:37:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 05:37:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 237 @ 46115 updates, score 11.596) (writing took 3.5260656140744686 seconds)
2022-03-08 05:37:23 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-03-08 05:37:23 | INFO | train | epoch 237 | loss 2.269 | nll_loss 1.729 | ppl 3.31 | wps 20527.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 46115 | lr 0.000147258 | gnorm 1.184 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 147354
2022-03-08 05:37:23 | INFO | fairseq.trainer | begin training epoch 238
2022-03-08 05:37:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:39:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:41:52 | INFO | train_inner | epoch 238:     86 / 196 loss=2.244, nll_loss=1.702, ppl=3.25, wps=20156, ups=0.31, wpb=65367, bsz=127.7, num_updates=46200, lr=0.000147122, gnorm=1.179, loss_scale=16, train_wall=293, gb_free=19.9, wall=147623
2022-03-08 05:46:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:47:08 | INFO | train_inner | epoch 238:    187 / 196 loss=2.292, nll_loss=1.753, ppl=3.37, wps=20732.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=46300, lr=0.000146964, gnorm=1.186, loss_scale=16, train_wall=293, gb_free=19.9, wall=147939
2022-03-08 05:47:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:47:41 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 11.558 | nll_loss 11.221 | ppl 2386.71 | wps 40375.4 | wpb 510.9 | bsz 1 | num_updates 46309 | best_loss 7.054
2022-03-08 05:47:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 46309 updates
2022-03-08 05:47:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 05:47:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 05:47:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 238 @ 46309 updates, score 11.558) (writing took 3.5037181666120887 seconds)
2022-03-08 05:47:45 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-03-08 05:47:45 | INFO | train | epoch 238 | loss 2.266 | nll_loss 1.725 | ppl 3.31 | wps 20423.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 46309 | lr 0.000146949 | gnorm 1.183 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 147975
2022-03-08 05:47:45 | INFO | fairseq.trainer | begin training epoch 239
2022-03-08 05:47:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:52:30 | INFO | train_inner | epoch 239:     91 / 196 loss=2.237, nll_loss=1.695, ppl=3.24, wps=20343.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=46400, lr=0.000146805, gnorm=1.186, loss_scale=16, train_wall=290, gb_free=19.9, wall=148260
2022-03-08 05:53:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:57:46 | INFO | train_inner | epoch 239:    192 / 196 loss=2.296, nll_loss=1.758, ppl=3.38, wps=20735.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=46500, lr=0.000146647, gnorm=1.191, loss_scale=16, train_wall=293, gb_free=19.9, wall=148576
2022-03-08 05:57:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:58:03 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 11.604 | nll_loss 11.268 | ppl 2465.32 | wps 40453.2 | wpb 510.9 | bsz 1 | num_updates 46504 | best_loss 7.054
2022-03-08 05:58:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 46504 updates
2022-03-08 05:58:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 05:58:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 05:58:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 239 @ 46504 updates, score 11.604) (writing took 3.785099248867482 seconds)
2022-03-08 05:58:07 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-03-08 05:58:07 | INFO | train | epoch 239 | loss 2.265 | nll_loss 1.725 | ppl 3.31 | wps 20519.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 46504 | lr 0.000146641 | gnorm 1.187 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 148597
2022-03-08 05:58:07 | INFO | fairseq.trainer | begin training epoch 240
2022-03-08 05:58:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 06:00:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 06:03:11 | INFO | train_inner | epoch 240:     97 / 196 loss=2.235, nll_loss=1.693, ppl=3.23, wps=20125.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=46600, lr=0.00014649, gnorm=1.189, loss_scale=16, train_wall=293, gb_free=19.9, wall=148901
2022-03-08 06:06:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 06:08:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 06:08:25 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 11.53 | nll_loss 11.194 | ppl 2342.3 | wps 40506.8 | wpb 510.9 | bsz 1 | num_updates 46698 | best_loss 7.054
2022-03-08 06:08:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 46698 updates
2022-03-08 06:08:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 06:08:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 06:08:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 240 @ 46698 updates, score 11.53) (writing took 3.548742563929409 seconds)
2022-03-08 06:08:29 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-03-08 06:08:29 | INFO | train | epoch 240 | loss 2.263 | nll_loss 1.722 | ppl 3.3 | wps 20415.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 46698 | lr 0.000146336 | gnorm 1.204 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 149219
2022-03-08 06:08:29 | INFO | fairseq.trainer | begin training epoch 241
2022-03-08 06:08:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 06:08:35 | INFO | train_inner | epoch 241:      2 / 196 loss=2.291, nll_loss=1.753, ppl=3.37, wps=20142.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=46700, lr=0.000146333, gnorm=1.218, loss_scale=16, train_wall=293, gb_free=19.9, wall=149226
2022-03-08 06:13:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 06:13:51 | INFO | train_inner | epoch 241:    103 / 196 loss=2.234, nll_loss=1.692, ppl=3.23, wps=20720.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=46800, lr=0.000146176, gnorm=1.185, loss_scale=16, train_wall=293, gb_free=19.9, wall=149542
2022-03-08 06:18:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 06:18:47 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 11.568 | nll_loss 11.232 | ppl 2405.77 | wps 40304.6 | wpb 510.9 | bsz 1 | num_updates 46893 | best_loss 7.054
2022-03-08 06:18:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 46893 updates
2022-03-08 06:18:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 06:18:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt
2022-03-08 06:18:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.02_#2/checkpoint_last.pt (epoch 241 @ 46893 updates, score 11.568) (writing took 3.5231010890565813 seconds)
2022-03-08 06:18:51 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-03-08 06:18:51 | INFO | train | epoch 241 | loss 2.26 | nll_loss 1.719 | ppl 3.29 | wps 20508.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 46893 | lr 0.000146031 | gnorm 1.193 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 149842
2022-03-08 06:18:51 | INFO | fairseq.trainer | begin training epoch 242
2022-03-08 06:18:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 06:19:13 | INFO | train_inner | epoch 242:      7 / 196 loss=2.283, nll_loss=1.744, ppl=3.35, wps=20322.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=46900, lr=0.00014602, gnorm=1.199, loss_scale=16, train_wall=290, gb_free=19.9, wall=149864
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
Exception in thread Thread-966:
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/threading.py", line 932, in _bootstrap_inner
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    self.run()
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/threading.py", line 870, in run
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    self._target(*self._args, **self._kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/utils/data/_utils/pin_memory.py", line 25, in _pin_memory_loop
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 116, in get
    return _ForkingPickler.loads(res)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/multiprocessing/reductions.py", line 282, in rebuild_storage_fd
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    fd = df.detach()
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/resource_sharer.py", line 57, in detach
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    with _resource_sharer.get_connection(self._id) as conn:
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/resource_sharer.py", line 87, in get_connection
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    c = Client(address, authkey=process.current_process().authkey)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 508, in Client
    answer_challenge(c, authkey)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 757, in answer_challenge
    Variable._execution_engine.run_backward(
KeyboardInterrupt
    response = connection.recv_bytes(256)        # reject large message
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
