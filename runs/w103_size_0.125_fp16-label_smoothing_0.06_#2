Sender: LSF System <lsfadmin@eu-g3-062>
Subject: Job 207345374: <w103_size_0.125_fp16_label_smoothing_0.06_#2> in cluster <euler> Exited

Job <w103_size_0.125_fp16_label_smoothing_0.06_#2> was submitted from host <eu-login-10> by user <andriusb> in cluster <euler> at Sun Mar  6 12:41:36 2022
Job was executed on host(s) <eu-g3-062>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Sun Mar  6 12:41:47 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Sun Mar  6 12:41:47 2022
Terminated at Tue Mar  8 06:20:45 2022
Results reported at Tue Mar  8 06:20:45 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.06 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575612 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   149865.27 sec.
    Max Memory :                                 6764 MB
    Average Memory :                             3779.42 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               13236.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   149938 sec.
    Turnaround time :                            149949 sec.

The output (if any) follows:

2022-03-06 12:41:54 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575612, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575612, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.06, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-06 12:41:54 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-06 12:41:57 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-06 12:41:57 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-06 12:41:57 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-06 12:41:57 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-06 12:41:57 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-06 12:41:57 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-06 12:41:57 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-06 12:42:00 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-06 12:42:00 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 12:42:00 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-06 12:42:00 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 12:42:00 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-06 12:42:00 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-06 12:42:00 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 12:42:00 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 12:42:00 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-06 12:42:00 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-06 12:42:00 | INFO | fairseq.trainer | begin training epoch 1
2022-03-06 12:42:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 12:42:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-06 12:42:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 12:42:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 12:42:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 12:42:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-06 12:47:51 | INFO | train_inner | epoch 001:    105 / 196 loss=16.526, nll_loss=16.408, ppl=86965.3, wps=21023.4, ups=0.32, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.402, loss_scale=4, train_wall=327, gb_free=19.9, wall=351
2022-03-06 12:52:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 12:52:39 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.335 | nll_loss 13.009 | ppl 8243.64 | wps 41769.1 | wpb 510.9 | bsz 1 | num_updates 191
2022-03-06 12:52:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-06 12:52:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 12:52:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 12:52:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.335) (writing took 8.808036654023454 seconds)
2022-03-06 12:52:48 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-06 12:52:48 | INFO | train | epoch 001 | loss 15.48 | nll_loss 15.296 | ppl 40228.6 | wps 20527.6 | ups 0.31 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.462 | loss_scale 8 | train_wall 590 | gb_free 19.9 | wall 648
2022-03-06 12:52:48 | INFO | fairseq.trainer | begin training epoch 2
2022-03-06 12:52:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 12:53:16 | INFO | train_inner | epoch 002:      9 / 196 loss=14.242, nll_loss=13.979, ppl=16149.8, wps=20094.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.406, loss_scale=8, train_wall=289, gb_free=19.9, wall=676
2022-03-06 12:58:28 | INFO | train_inner | epoch 002:    109 / 196 loss=12.415, nll_loss=12.017, ppl=4145.47, wps=21027.5, ups=0.32, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.916, loss_scale=16, train_wall=290, gb_free=19.9, wall=988
2022-03-06 13:02:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:03:03 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.866 | nll_loss 10.295 | ppl 1256.56 | wps 41690.8 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 10.866
2022-03-06 13:03:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-06 13:03:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 13:03:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 13:03:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 2 @ 387 updates, score 10.866) (writing took 7.2199432358611375 seconds)
2022-03-06 13:03:10 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-06 13:03:10 | INFO | train | epoch 002 | loss 11.935 | nll_loss 11.488 | ppl 2871.77 | wps 20601.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.768 | loss_scale 32 | train_wall 567 | gb_free 19.9 | wall 1271
2022-03-06 13:03:10 | INFO | fairseq.trainer | begin training epoch 3
2022-03-06 13:03:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:03:51 | INFO | train_inner | epoch 003:     13 / 196 loss=11.187, nll_loss=10.663, ppl=1621.01, wps=20205.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.542, loss_scale=32, train_wall=289, gb_free=19.9, wall=1311
2022-03-06 13:09:03 | INFO | train_inner | epoch 003:    113 / 196 loss=10.676, nll_loss=10.071, ppl=1075.84, wps=21019.7, ups=0.32, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.496, loss_scale=32, train_wall=290, gb_free=19.9, wall=1623
2022-03-06 13:09:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:13:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:13:26 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.233 | nll_loss 9.573 | ppl 761.7 | wps 41775.3 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 10.233
2022-03-06 13:13:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 582 updates
2022-03-06 13:13:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 13:13:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 13:13:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 3 @ 582 updates, score 10.233) (writing took 7.326416202122346 seconds)
2022-03-06 13:13:33 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-06 13:13:33 | INFO | train | epoch 003 | loss 10.576 | nll_loss 9.96 | ppl 995.74 | wps 20485.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 582 | lr 7.28355e-05 | gnorm 0.509 | loss_scale 32 | train_wall 567 | gb_free 19.9 | wall 1894
2022-03-06 13:13:33 | INFO | fairseq.trainer | begin training epoch 4
2022-03-06 13:13:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:14:30 | INFO | train_inner | epoch 004:     18 / 196 loss=10.375, nll_loss=9.732, ppl=850.65, wps=20000.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.555, loss_scale=32, train_wall=292, gb_free=19.9, wall=1950
2022-03-06 13:15:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 13:19:44 | INFO | train_inner | epoch 004:    119 / 196 loss=10.097, nll_loss=9.428, ppl=688.64, wps=20815.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.663, loss_scale=16, train_wall=293, gb_free=19.9, wall=2265
2022-03-06 13:23:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:23:49 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.785 | nll_loss 9.082 | ppl 541.97 | wps 41734.8 | wpb 510.9 | bsz 1 | num_updates 777 | best_loss 9.785
2022-03-06 13:23:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 777 updates
2022-03-06 13:23:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 13:23:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 13:23:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 4 @ 777 updates, score 9.785) (writing took 7.29620682192035 seconds)
2022-03-06 13:23:56 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-06 13:23:56 | INFO | train | epoch 004 | loss 10.033 | nll_loss 9.358 | ppl 656.2 | wps 20489.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 777 | lr 9.72056e-05 | gnorm 0.683 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 2517
2022-03-06 13:23:56 | INFO | fairseq.trainer | begin training epoch 5
2022-03-06 13:23:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:25:08 | INFO | train_inner | epoch 005:     23 / 196 loss=9.868, nll_loss=9.177, ppl=578.8, wps=20197.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.695, loss_scale=32, train_wall=289, gb_free=19.9, wall=2588
2022-03-06 13:28:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:30:23 | INFO | train_inner | epoch 005:    124 / 196 loss=9.653, nll_loss=8.942, ppl=491.75, wps=20808.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.796, loss_scale=32, train_wall=293, gb_free=19.9, wall=2903
2022-03-06 13:34:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:34:12 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.414 | nll_loss 8.684 | ppl 411.29 | wps 41946.9 | wpb 510.9 | bsz 1 | num_updates 972 | best_loss 9.414
2022-03-06 13:34:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 972 updates
2022-03-06 13:34:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 13:34:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 13:34:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 5 @ 972 updates, score 9.414) (writing took 7.5269198508467525 seconds)
2022-03-06 13:34:20 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-06 13:34:20 | INFO | train | epoch 005 | loss 9.604 | nll_loss 8.889 | ppl 474.15 | wps 20474.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 972 | lr 0.000121576 | gnorm 0.788 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 3140
2022-03-06 13:34:20 | INFO | fairseq.trainer | begin training epoch 6
2022-03-06 13:34:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:35:47 | INFO | train_inner | epoch 006:     28 / 196 loss=9.458, nll_loss=8.73, ppl=424.52, wps=20174.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.831, loss_scale=64, train_wall=289, gb_free=19.9, wall=3227
2022-03-06 13:35:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:41:02 | INFO | train_inner | epoch 006:    129 / 196 loss=9.275, nll_loss=8.53, ppl=369.55, wps=20796.5, ups=0.32, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.845, loss_scale=32, train_wall=293, gb_free=19.9, wall=3542
2022-03-06 13:41:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 13:44:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:44:36 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.103 | nll_loss 8.335 | ppl 322.84 | wps 41709.6 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 9.103
2022-03-06 13:44:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1166 updates
2022-03-06 13:44:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 13:44:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 13:44:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 6 @ 1166 updates, score 9.103) (writing took 6.8112472298089415 seconds)
2022-03-06 13:44:42 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-06 13:44:42 | INFO | train | epoch 006 | loss 9.249 | nll_loss 8.502 | ppl 362.42 | wps 20384.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1166 | lr 0.000145821 | gnorm 0.857 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 3763
2022-03-06 13:44:42 | INFO | fairseq.trainer | begin training epoch 7
2022-03-06 13:44:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:46:29 | INFO | train_inner | epoch 007:     34 / 196 loss=9.118, nll_loss=8.359, ppl=328.35, wps=20023.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.876, loss_scale=16, train_wall=292, gb_free=19.9, wall=3869
2022-03-06 13:51:41 | INFO | train_inner | epoch 007:    134 / 196 loss=8.968, nll_loss=8.194, ppl=292.93, wps=20990.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.863, loss_scale=32, train_wall=290, gb_free=19.9, wall=4181
2022-03-06 13:54:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:54:59 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.86 | nll_loss 8.066 | ppl 268.01 | wps 41723.5 | wpb 510.9 | bsz 1 | num_updates 1362 | best_loss 8.86
2022-03-06 13:54:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1362 updates
2022-03-06 13:54:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 13:55:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 13:55:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 7 @ 1362 updates, score 8.86) (writing took 8.830164189916104 seconds)
2022-03-06 13:55:08 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-06 13:55:08 | INFO | train | epoch 007 | loss 8.952 | nll_loss 8.178 | ppl 289.52 | wps 20515.6 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 1362 | lr 0.000170316 | gnorm 0.863 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 4388
2022-03-06 13:55:08 | INFO | fairseq.trainer | begin training epoch 8
2022-03-06 13:55:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:55:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:57:09 | INFO | train_inner | epoch 008:     39 / 196 loss=8.828, nll_loss=8.043, ppl=263.76, wps=19891.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.885, loss_scale=32, train_wall=292, gb_free=19.9, wall=4510
2022-03-06 14:02:21 | INFO | train_inner | epoch 008:    139 / 196 loss=8.692, nll_loss=7.893, ppl=237.77, wps=21003.4, ups=0.32, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.871, loss_scale=32, train_wall=290, gb_free=19.9, wall=4822
2022-03-06 14:03:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:05:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:05:24 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.632 | nll_loss 7.814 | ppl 225.02 | wps 41210.7 | wpb 510.9 | bsz 1 | num_updates 1556 | best_loss 8.632
2022-03-06 14:05:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1556 updates
2022-03-06 14:05:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 14:05:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 14:05:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 8 @ 1556 updates, score 8.632) (writing took 8.630498587852344 seconds)
2022-03-06 14:05:32 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-06 14:05:32 | INFO | train | epoch 008 | loss 8.684 | nll_loss 7.885 | ppl 236.36 | wps 20323.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1556 | lr 0.000194561 | gnorm 0.889 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 5013
2022-03-06 14:05:32 | INFO | fairseq.trainer | begin training epoch 9
2022-03-06 14:05:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:07:50 | INFO | train_inner | epoch 009:     44 / 196 loss=8.567, nll_loss=7.758, ppl=216.41, wps=19902.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.917, loss_scale=32, train_wall=292, gb_free=19.9, wall=5150
2022-03-06 14:10:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:13:05 | INFO | train_inner | epoch 009:    145 / 196 loss=8.436, nll_loss=7.614, ppl=195.94, wps=20793.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.876, loss_scale=32, train_wall=293, gb_free=19.9, wall=5465
2022-03-06 14:15:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:15:49 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.435 | nll_loss 7.598 | ppl 193.77 | wps 41439.7 | wpb 510.9 | bsz 1 | num_updates 1751 | best_loss 8.435
2022-03-06 14:15:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1751 updates
2022-03-06 14:15:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 14:15:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 14:15:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 9 @ 1751 updates, score 8.435) (writing took 6.851973239099607 seconds)
2022-03-06 14:15:55 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-06 14:15:55 | INFO | train | epoch 009 | loss 8.434 | nll_loss 7.612 | ppl 195.62 | wps 20484 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1751 | lr 0.000218931 | gnorm 0.902 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 5636
2022-03-06 14:15:55 | INFO | fairseq.trainer | begin training epoch 10
2022-03-06 14:15:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:17:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:18:32 | INFO | train_inner | epoch 010:     50 / 196 loss=8.313, nll_loss=7.481, ppl=178.63, wps=20018.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.897, loss_scale=32, train_wall=292, gb_free=19.9, wall=5792
2022-03-06 14:23:44 | INFO | train_inner | epoch 010:    150 / 196 loss=8.195, nll_loss=7.351, ppl=163.23, wps=21002.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.868, loss_scale=32, train_wall=290, gb_free=19.9, wall=6104
2022-03-06 14:24:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:26:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:26:12 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.271 | nll_loss 7.415 | ppl 170.64 | wps 41448.4 | wpb 510.9 | bsz 1 | num_updates 1945 | best_loss 8.271
2022-03-06 14:26:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1945 updates
2022-03-06 14:26:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 14:26:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 14:26:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 10 @ 1945 updates, score 8.271) (writing took 9.471606622915715 seconds)
2022-03-06 14:26:21 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-06 14:26:21 | INFO | train | epoch 010 | loss 8.198 | nll_loss 7.354 | ppl 163.64 | wps 20295.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1945 | lr 0.000243176 | gnorm 0.868 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 6261
2022-03-06 14:26:21 | INFO | fairseq.trainer | begin training epoch 11
2022-03-06 14:26:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:29:13 | INFO | train_inner | epoch 011:     55 / 196 loss=8.078, nll_loss=7.223, ppl=149.42, wps=19853.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.867, loss_scale=32, train_wall=292, gb_free=19.9, wall=6433
2022-03-06 14:32:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:34:28 | INFO | train_inner | epoch 011:    156 / 196 loss=7.979, nll_loss=7.114, ppl=138.57, wps=20801.5, ups=0.32, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.851, loss_scale=32, train_wall=293, gb_free=19.9, wall=6748
2022-03-06 14:36:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:36:37 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.138 | nll_loss 7.268 | ppl 154.08 | wps 41578 | wpb 510.9 | bsz 1 | num_updates 2140 | best_loss 8.138
2022-03-06 14:36:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2140 updates
2022-03-06 14:36:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 14:36:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 14:36:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 11 @ 2140 updates, score 8.138) (writing took 8.910444205161184 seconds)
2022-03-06 14:36:46 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-06 14:36:46 | INFO | train | epoch 011 | loss 7.981 | nll_loss 7.117 | ppl 138.79 | wps 20416.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2140 | lr 0.000267547 | gnorm 0.854 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 6886
2022-03-06 14:36:46 | INFO | fairseq.trainer | begin training epoch 12
2022-03-06 14:36:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:39:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 14:39:57 | INFO | train_inner | epoch 012:     61 / 196 loss=7.857, nll_loss=6.982, ppl=126.42, wps=19887.5, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.845, loss_scale=16, train_wall=292, gb_free=19.9, wall=7077
2022-03-06 14:45:08 | INFO | train_inner | epoch 012:    161 / 196 loss=7.771, nll_loss=6.886, ppl=118.32, wps=21014.1, ups=0.32, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.834, loss_scale=16, train_wall=290, gb_free=19.9, wall=7389
2022-03-06 14:46:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:47:02 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.01 | nll_loss 7.138 | ppl 140.83 | wps 41945.7 | wpb 510.9 | bsz 1 | num_updates 2335 | best_loss 8.01
2022-03-06 14:47:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2335 updates
2022-03-06 14:47:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 14:47:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 14:47:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 12 @ 2335 updates, score 8.01) (writing took 9.295370185980573 seconds)
2022-03-06 14:47:11 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-06 14:47:11 | INFO | train | epoch 012 | loss 7.778 | nll_loss 6.895 | ppl 118.98 | wps 20412.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2335 | lr 0.000291917 | gnorm 0.837 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 7512
2022-03-06 14:47:11 | INFO | fairseq.trainer | begin training epoch 13
2022-03-06 14:47:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:50:34 | INFO | train_inner | epoch 013:     65 / 196 loss=7.654, nll_loss=6.76, ppl=108.36, wps=20058.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.829, loss_scale=32, train_wall=289, gb_free=19.9, wall=7715
2022-03-06 14:52:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:55:49 | INFO | train_inner | epoch 013:    166 / 196 loss=7.588, nll_loss=6.686, ppl=102.98, wps=20800.4, ups=0.32, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.82, loss_scale=32, train_wall=293, gb_free=19.9, wall=8030
2022-03-06 14:57:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:57:27 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.907 | nll_loss 7.009 | ppl 128.77 | wps 41732.6 | wpb 510.9 | bsz 1 | num_updates 2530 | best_loss 7.907
2022-03-06 14:57:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2530 updates
2022-03-06 14:57:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 14:57:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 14:57:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 13 @ 2530 updates, score 7.907) (writing took 7.163403142942116 seconds)
2022-03-06 14:57:35 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-06 14:57:35 | INFO | train | epoch 013 | loss 7.59 | nll_loss 6.689 | ppl 103.17 | wps 20476 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2530 | lr 0.000316287 | gnorm 0.815 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 8135
2022-03-06 14:57:35 | INFO | fairseq.trainer | begin training epoch 14
2022-03-06 14:57:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:00:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:01:16 | INFO | train_inner | epoch 014:     71 / 196 loss=7.468, nll_loss=6.556, ppl=94.08, wps=19991.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.807, loss_scale=32, train_wall=292, gb_free=19.9, wall=8357
2022-03-06 15:03:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:06:31 | INFO | train_inner | epoch 014:    172 / 196 loss=7.411, nll_loss=6.492, ppl=90.01, wps=20800.3, ups=0.32, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.807, loss_scale=16, train_wall=293, gb_free=19.9, wall=8672
2022-03-06 15:07:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:07:51 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.827 | nll_loss 6.913 | ppl 120.51 | wps 41551.4 | wpb 510.9 | bsz 1 | num_updates 2724 | best_loss 7.827
2022-03-06 15:07:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2724 updates
2022-03-06 15:07:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 15:07:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 15:07:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 14 @ 2724 updates, score 7.827) (writing took 6.730517657939345 seconds)
2022-03-06 15:07:57 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-06 15:07:57 | INFO | train | epoch 014 | loss 7.419 | nll_loss 6.501 | ppl 90.59 | wps 20385.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 2724 | lr 0.000340532 | gnorm 0.813 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 8758
2022-03-06 15:07:57 | INFO | fairseq.trainer | begin training epoch 15
2022-03-06 15:07:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:11:55 | INFO | train_inner | epoch 015:     76 / 196 loss=7.305, nll_loss=6.377, ppl=83.12, wps=20222.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.822, loss_scale=32, train_wall=289, gb_free=19.9, wall=8995
2022-03-06 15:12:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:17:09 | INFO | train_inner | epoch 015:    177 / 196 loss=7.259, nll_loss=6.325, ppl=80.17, wps=20811.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.791, loss_scale=16, train_wall=293, gb_free=19.9, wall=9310
2022-03-06 15:18:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:18:13 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.779 | nll_loss 6.871 | ppl 117.07 | wps 41589.9 | wpb 510.9 | bsz 1 | num_updates 2919 | best_loss 7.779
2022-03-06 15:18:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2919 updates
2022-03-06 15:18:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 15:18:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 15:18:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 15 @ 2919 updates, score 7.779) (writing took 9.164229954825714 seconds)
2022-03-06 15:18:22 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-06 15:18:22 | INFO | train | epoch 015 | loss 7.264 | nll_loss 6.33 | ppl 80.47 | wps 20420.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2919 | lr 0.000364902 | gnorm 0.807 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 9383
2022-03-06 15:18:22 | INFO | fairseq.trainer | begin training epoch 16
2022-03-06 15:18:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:22:35 | INFO | train_inner | epoch 016:     81 / 196 loss=7.139, nll_loss=6.194, ppl=73.22, wps=20071, ups=0.31, wpb=65367, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.785, loss_scale=32, train_wall=289, gb_free=19.9, wall=9635
2022-03-06 15:24:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:27:50 | INFO | train_inner | epoch 016:    182 / 196 loss=7.122, nll_loss=6.175, ppl=72.23, wps=20813.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.797, loss_scale=16, train_wall=293, gb_free=19.9, wall=9950
2022-03-06 15:28:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:28:38 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.734 | nll_loss 6.805 | ppl 111.83 | wps 41831.4 | wpb 510.9 | bsz 1 | num_updates 3114 | best_loss 7.734
2022-03-06 15:28:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3114 updates
2022-03-06 15:28:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 15:28:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 15:28:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 16 @ 3114 updates, score 7.734) (writing took 9.273172697983682 seconds)
2022-03-06 15:28:47 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-06 15:28:47 | INFO | train | epoch 016 | loss 7.116 | nll_loss 6.169 | ppl 71.93 | wps 20419 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3114 | lr 0.000389272 | gnorm 0.786 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 10008
2022-03-06 15:28:47 | INFO | fairseq.trainer | begin training epoch 17
2022-03-06 15:28:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:33:16 | INFO | train_inner | epoch 017:     86 / 196 loss=6.986, nll_loss=6.027, ppl=65.21, wps=20057.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.781, loss_scale=32, train_wall=289, gb_free=19.9, wall=10276
2022-03-06 15:38:28 | INFO | train_inner | epoch 017:    186 / 196 loss=6.986, nll_loss=6.024, ppl=65.09, wps=21006.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.788, loss_scale=64, train_wall=290, gb_free=19.9, wall=10588
2022-03-06 15:38:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:38:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:39:04 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.713 | nll_loss 6.785 | ppl 110.26 | wps 41746.8 | wpb 510.9 | bsz 1 | num_updates 3309 | best_loss 7.713
2022-03-06 15:39:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3309 updates
2022-03-06 15:39:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 15:39:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 15:39:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 17 @ 3309 updates, score 7.713) (writing took 9.559112502960488 seconds)
2022-03-06 15:39:13 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-06 15:39:13 | INFO | train | epoch 017 | loss 6.979 | nll_loss 6.018 | ppl 64.79 | wps 20397.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3309 | lr 0.000413642 | gnorm 0.787 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 10633
2022-03-06 15:39:13 | INFO | fairseq.trainer | begin training epoch 18
2022-03-06 15:39:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:41:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:44:00 | INFO | train_inner | epoch 018:     92 / 196 loss=6.85, nll_loss=5.877, ppl=58.76, wps=19665, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.773, loss_scale=16, train_wall=295, gb_free=19.9, wall=10921
2022-03-06 15:49:12 | INFO | train_inner | epoch 018:    192 / 196 loss=6.868, nll_loss=5.894, ppl=59.48, wps=21018.1, ups=0.32, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.795, loss_scale=32, train_wall=290, gb_free=19.9, wall=11232
2022-03-06 15:49:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:49:29 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.7 | nll_loss 6.777 | ppl 109.65 | wps 41744.6 | wpb 510.9 | bsz 1 | num_updates 3504 | best_loss 7.7
2022-03-06 15:49:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3504 updates
2022-03-06 15:49:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 15:49:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 15:49:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 18 @ 3504 updates, score 7.7) (writing took 8.579144285991788 seconds)
2022-03-06 15:49:38 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-06 15:49:38 | INFO | train | epoch 018 | loss 6.851 | nll_loss 5.877 | ppl 58.78 | wps 20436.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3504 | lr 0.000438012 | gnorm 0.781 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 11258
2022-03-06 15:49:38 | INFO | fairseq.trainer | begin training epoch 19
2022-03-06 15:49:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:52:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:54:40 | INFO | train_inner | epoch 019:     97 / 196 loss=6.713, nll_loss=5.726, ppl=52.94, wps=19917.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.756, loss_scale=16, train_wall=292, gb_free=19.9, wall=11561
2022-03-06 15:59:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:59:53 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.696 | nll_loss 6.774 | ppl 109.42 | wps 41835.4 | wpb 510.9 | bsz 1 | num_updates 3699 | best_loss 7.696
2022-03-06 15:59:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3699 updates
2022-03-06 15:59:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 15:59:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt
2022-03-06 16:00:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_best.pt (epoch 19 @ 3699 updates, score 7.696) (writing took 7.357279733987525 seconds)
2022-03-06 16:00:01 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-06 16:00:01 | INFO | train | epoch 019 | loss 6.729 | nll_loss 5.743 | ppl 53.56 | wps 20480.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3699 | lr 0.000462383 | gnorm 0.768 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 11881
2022-03-06 16:00:01 | INFO | fairseq.trainer | begin training epoch 20
2022-03-06 16:00:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:00:04 | INFO | train_inner | epoch 020:      1 / 196 loss=6.747, nll_loss=5.762, ppl=54.26, wps=20189.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.777, loss_scale=32, train_wall=289, gb_free=19.9, wall=11884
2022-03-06 16:01:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:05:19 | INFO | train_inner | epoch 020:    102 / 196 loss=6.586, nll_loss=5.587, ppl=48.05, wps=20813.6, ups=0.32, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.779, loss_scale=16, train_wall=293, gb_free=19.9, wall=12199
2022-03-06 16:08:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:10:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:10:16 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.703 | nll_loss 6.783 | ppl 110.15 | wps 41532.2 | wpb 510.9 | bsz 1 | num_updates 3893 | best_loss 7.696
2022-03-06 16:10:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3893 updates
2022-03-06 16:10:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 16:10:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 16:10:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 20 @ 3893 updates, score 7.703) (writing took 3.6763337638694793 seconds)
2022-03-06 16:10:20 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-06 16:10:20 | INFO | train | epoch 020 | loss 6.613 | nll_loss 5.616 | ppl 49.03 | wps 20498 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 3893 | lr 0.000486628 | gnorm 0.769 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 12501
2022-03-06 16:10:20 | INFO | fairseq.trainer | begin training epoch 21
2022-03-06 16:10:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:10:42 | INFO | train_inner | epoch 021:      7 / 196 loss=6.631, nll_loss=5.634, ppl=49.67, wps=20219.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3900, lr=0.000487503, gnorm=0.768, loss_scale=16, train_wall=292, gb_free=19.9, wall=12522
2022-03-06 16:15:54 | INFO | train_inner | epoch 021:    107 / 196 loss=6.476, nll_loss=5.465, ppl=44.16, wps=21012.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.765, loss_scale=32, train_wall=290, gb_free=19.9, wall=12834
2022-03-06 16:16:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:20:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:20:36 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.706 | nll_loss 6.759 | ppl 108.31 | wps 41728.8 | wpb 510.9 | bsz 1 | num_updates 4088 | best_loss 7.696
2022-03-06 16:20:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4088 updates
2022-03-06 16:20:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 16:20:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 16:20:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 21 @ 4088 updates, score 7.706) (writing took 3.0736196760553867 seconds)
2022-03-06 16:20:39 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-06 16:20:39 | INFO | train | epoch 021 | loss 6.504 | nll_loss 5.494 | ppl 45.08 | wps 20626 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 4088 | lr 0.000494589 | gnorm 0.777 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 13119
2022-03-06 16:20:39 | INFO | fairseq.trainer | begin training epoch 22
2022-03-06 16:20:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:21:16 | INFO | train_inner | epoch 022:     12 / 196 loss=6.511, nll_loss=5.502, ppl=45.31, wps=20273.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.77, loss_scale=16, train_wall=292, gb_free=19.9, wall=13157
2022-03-06 16:23:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:26:31 | INFO | train_inner | epoch 022:    113 / 196 loss=6.365, nll_loss=5.343, ppl=40.59, wps=20832.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.744, loss_scale=16, train_wall=293, gb_free=19.9, wall=13471
2022-03-06 16:30:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:30:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:30:54 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.718 | nll_loss 6.757 | ppl 108.13 | wps 42148.3 | wpb 510.9 | bsz 1 | num_updates 4282 | best_loss 7.696
2022-03-06 16:30:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4282 updates
2022-03-06 16:30:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 16:30:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 16:30:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 22 @ 4282 updates, score 7.718) (writing took 3.593115861993283 seconds)
2022-03-06 16:30:58 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-06 16:30:58 | INFO | train | epoch 022 | loss 6.385 | nll_loss 5.364 | ppl 41.19 | wps 20521.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 4282 | lr 0.000483255 | gnorm 0.74 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 13738
2022-03-06 16:30:58 | INFO | fairseq.trainer | begin training epoch 23
2022-03-06 16:30:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:31:54 | INFO | train_inner | epoch 023:     18 / 196 loss=6.386, nll_loss=5.364, ppl=41.2, wps=20255.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=4300, lr=0.000482243, gnorm=0.749, loss_scale=16, train_wall=292, gb_free=19.9, wall=13794
2022-03-06 16:37:05 | INFO | train_inner | epoch 023:    118 / 196 loss=6.253, nll_loss=5.22, ppl=37.26, wps=21039.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.737, loss_scale=16, train_wall=290, gb_free=19.9, wall=14106
2022-03-06 16:37:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:41:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:41:13 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.725 | nll_loss 6.783 | ppl 110.12 | wps 41851.6 | wpb 510.9 | bsz 1 | num_updates 4477 | best_loss 7.696
2022-03-06 16:41:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4477 updates
2022-03-06 16:41:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 16:41:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 16:41:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 23 @ 4477 updates, score 7.725) (writing took 3.719258826924488 seconds)
2022-03-06 16:41:16 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-06 16:41:16 | INFO | train | epoch 023 | loss 6.273 | nll_loss 5.241 | ppl 37.81 | wps 20626 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 4477 | lr 0.000472614 | gnorm 0.735 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 14357
2022-03-06 16:41:16 | INFO | fairseq.trainer | begin training epoch 24
2022-03-06 16:41:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:42:28 | INFO | train_inner | epoch 024:     23 / 196 loss=6.257, nll_loss=5.223, ppl=37.35, wps=20235.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.708, loss_scale=16, train_wall=292, gb_free=19.9, wall=14429
2022-03-06 16:44:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:47:43 | INFO | train_inner | epoch 024:    124 / 196 loss=6.153, nll_loss=5.109, ppl=34.51, wps=20821.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.742, loss_scale=16, train_wall=293, gb_free=19.9, wall=14743
2022-03-06 16:51:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:51:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:51:32 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.756 | nll_loss 6.799 | ppl 111.32 | wps 41668.7 | wpb 510.9 | bsz 1 | num_updates 4671 | best_loss 7.696
2022-03-06 16:51:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4671 updates
2022-03-06 16:51:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 16:51:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 16:51:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 24 @ 4671 updates, score 7.756) (writing took 3.0288438559509814 seconds)
2022-03-06 16:51:35 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-06 16:51:35 | INFO | train | epoch 024 | loss 6.167 | nll_loss 5.125 | ppl 34.89 | wps 20528.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 4671 | lr 0.000462695 | gnorm 0.722 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 14975
2022-03-06 16:51:35 | INFO | fairseq.trainer | begin training epoch 25
2022-03-06 16:51:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:53:05 | INFO | train_inner | epoch 025:     29 / 196 loss=6.154, nll_loss=5.109, ppl=34.52, wps=20275.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.713, loss_scale=16, train_wall=292, gb_free=19.9, wall=15066
2022-03-06 16:58:17 | INFO | train_inner | epoch 025:    129 / 196 loss=6.053, nll_loss=4.999, ppl=31.97, wps=21040.8, ups=0.32, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.712, loss_scale=32, train_wall=290, gb_free=19.9, wall=15377
2022-03-06 16:59:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:01:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:01:50 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.777 | nll_loss 6.825 | ppl 113.37 | wps 41617.1 | wpb 510.9 | bsz 1 | num_updates 4866 | best_loss 7.696
2022-03-06 17:01:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4866 updates
2022-03-06 17:01:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 17:01:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 17:01:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 25 @ 4866 updates, score 7.777) (writing took 3.2708342098630965 seconds)
2022-03-06 17:01:53 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-06 17:01:53 | INFO | train | epoch 025 | loss 6.07 | nll_loss 5.017 | ppl 32.38 | wps 20636.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 4866 | lr 0.000453329 | gnorm 0.716 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 15594
2022-03-06 17:01:53 | INFO | fairseq.trainer | begin training epoch 26
2022-03-06 17:01:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:03:39 | INFO | train_inner | epoch 026:     34 / 196 loss=6.053, nll_loss=4.998, ppl=31.96, wps=20269.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.726, loss_scale=16, train_wall=292, gb_free=19.9, wall=15700
2022-03-06 17:06:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:08:54 | INFO | train_inner | epoch 026:    135 / 196 loss=5.972, nll_loss=4.908, ppl=30.03, wps=20827.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.73, loss_scale=16, train_wall=293, gb_free=19.9, wall=16014
2022-03-06 17:12:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:12:08 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.829 | nll_loss 6.88 | ppl 117.8 | wps 42070.2 | wpb 510.9 | bsz 1 | num_updates 5061 | best_loss 7.696
2022-03-06 17:12:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5061 updates
2022-03-06 17:12:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 17:12:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 17:12:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 26 @ 5061 updates, score 7.829) (writing took 3.375255496939644 seconds)
2022-03-06 17:12:12 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-06 17:12:12 | INFO | train | epoch 026 | loss 5.977 | nll_loss 4.915 | ppl 30.16 | wps 20632.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 5061 | lr 0.00044451 | gnorm 0.721 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 16212
2022-03-06 17:12:12 | INFO | fairseq.trainer | begin training epoch 27
2022-03-06 17:12:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:12:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:14:17 | INFO | train_inner | epoch 027:     40 / 196 loss=5.943, nll_loss=4.877, ppl=29.39, wps=20265.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.719, loss_scale=8, train_wall=292, gb_free=19.9, wall=16337
2022-03-06 17:19:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:19:31 | INFO | train_inner | epoch 027:    141 / 196 loss=5.886, nll_loss=4.814, ppl=28.13, wps=20840.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.717, loss_scale=8, train_wall=292, gb_free=19.9, wall=16651
2022-03-06 17:22:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:22:27 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.85 | nll_loss 6.888 | ppl 118.43 | wps 41597.6 | wpb 510.9 | bsz 1 | num_updates 5255 | best_loss 7.696
2022-03-06 17:22:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5255 updates
2022-03-06 17:22:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 17:22:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 17:22:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 27 @ 5255 updates, score 7.85) (writing took 3.054789553862065 seconds)
2022-03-06 17:22:30 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-06 17:22:30 | INFO | train | epoch 027 | loss 5.89 | nll_loss 4.818 | ppl 28.2 | wps 20545.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 5255 | lr 0.000436228 | gnorm 0.722 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 16830
2022-03-06 17:22:30 | INFO | fairseq.trainer | begin training epoch 28
2022-03-06 17:22:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:24:50 | INFO | train_inner | epoch 028:     45 / 196 loss=5.85, nll_loss=4.774, ppl=27.37, wps=20488.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.709, loss_scale=8, train_wall=289, gb_free=19.9, wall=16970
2022-03-06 17:30:01 | INFO | train_inner | epoch 028:    145 / 196 loss=5.805, nll_loss=4.724, ppl=26.43, wps=21047.3, ups=0.32, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.723, loss_scale=16, train_wall=290, gb_free=19.9, wall=17282
2022-03-06 17:30:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:32:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:32:45 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.886 | nll_loss 6.937 | ppl 122.52 | wps 42011.8 | wpb 510.9 | bsz 1 | num_updates 5450 | best_loss 7.696
2022-03-06 17:32:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5450 updates
2022-03-06 17:32:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 17:32:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 17:32:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 28 @ 5450 updates, score 7.886) (writing took 3.3760618939995766 seconds)
2022-03-06 17:32:48 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-06 17:32:48 | INFO | train | epoch 028 | loss 5.808 | nll_loss 4.727 | ppl 26.48 | wps 20647.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 5450 | lr 0.000428353 | gnorm 0.718 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 17448
2022-03-06 17:32:48 | INFO | fairseq.trainer | begin training epoch 29
2022-03-06 17:32:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:35:24 | INFO | train_inner | epoch 029:     50 / 196 loss=5.771, nll_loss=4.686, ppl=25.74, wps=20271.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.719, loss_scale=8, train_wall=292, gb_free=19.9, wall=17604
2022-03-06 17:39:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 17:40:38 | INFO | train_inner | epoch 029:    151 / 196 loss=5.729, nll_loss=4.64, ppl=24.93, wps=20832.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.711, loss_scale=8, train_wall=293, gb_free=19.9, wall=17919
2022-03-06 17:42:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:43:03 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.95 | nll_loss 6.986 | ppl 126.77 | wps 41858.3 | wpb 510.9 | bsz 1 | num_updates 5645 | best_loss 7.696
2022-03-06 17:43:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5645 updates
2022-03-06 17:43:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 17:43:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 17:43:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 29 @ 5645 updates, score 7.95) (writing took 3.301203231792897 seconds)
2022-03-06 17:43:06 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-06 17:43:06 | INFO | train | epoch 029 | loss 5.729 | nll_loss 4.639 | ppl 24.92 | wps 20639.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 5645 | lr 0.000420889 | gnorm 0.715 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 18067
2022-03-06 17:43:06 | INFO | fairseq.trainer | begin training epoch 30
2022-03-06 17:43:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:45:58 | INFO | train_inner | epoch 030:     55 / 196 loss=5.687, nll_loss=4.593, ppl=24.14, wps=20478.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.721, loss_scale=8, train_wall=289, gb_free=19.9, wall=18238
2022-03-06 17:51:09 | INFO | train_inner | epoch 030:    155 / 196 loss=5.662, nll_loss=4.564, ppl=23.66, wps=21043.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.739, loss_scale=16, train_wall=290, gb_free=19.9, wall=18549
2022-03-06 17:53:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:53:21 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.014 | nll_loss 7.064 | ppl 133.8 | wps 41445 | wpb 510.9 | bsz 1 | num_updates 5841 | best_loss 7.696
2022-03-06 17:53:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5841 updates
2022-03-06 17:53:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 17:53:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 17:53:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 30 @ 5841 updates, score 8.014) (writing took 3.077498106053099 seconds)
2022-03-06 17:53:24 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-06 17:53:24 | INFO | train | epoch 030 | loss 5.655 | nll_loss 4.558 | ppl 23.55 | wps 20756.2 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 5841 | lr 0.000413768 | gnorm 0.727 | loss_scale 32 | train_wall 567 | gb_free 19.9 | wall 18685
2022-03-06 17:53:24 | INFO | fairseq.trainer | begin training epoch 31
2022-03-06 17:53:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:53:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:56:31 | INFO | train_inner | epoch 031:     60 / 196 loss=5.597, nll_loss=4.494, ppl=22.54, wps=20281.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.716, loss_scale=16, train_wall=292, gb_free=19.9, wall=18872
2022-03-06 17:56:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:01:46 | INFO | train_inner | epoch 031:    161 / 196 loss=5.6, nll_loss=4.496, ppl=22.56, wps=20848.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.748, loss_scale=8, train_wall=292, gb_free=19.9, wall=19186
2022-03-06 18:03:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:03:39 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.048 | nll_loss 7.098 | ppl 137.03 | wps 41614.8 | wpb 510.9 | bsz 1 | num_updates 6035 | best_loss 7.696
2022-03-06 18:03:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6035 updates
2022-03-06 18:03:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 18:03:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 18:03:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 31 @ 6035 updates, score 8.048) (writing took 3.096325295045972 seconds)
2022-03-06 18:03:42 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-06 18:03:42 | INFO | train | epoch 031 | loss 5.583 | nll_loss 4.477 | ppl 22.27 | wps 20548.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 6035 | lr 0.000407063 | gnorm 0.733 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 19303
2022-03-06 18:03:42 | INFO | fairseq.trainer | begin training epoch 32
2022-03-06 18:03:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:05:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:07:08 | INFO | train_inner | epoch 032:     66 / 196 loss=5.519, nll_loss=4.407, ppl=21.21, wps=20289.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.717, loss_scale=8, train_wall=292, gb_free=19.9, wall=19508
2022-03-06 18:12:19 | INFO | train_inner | epoch 032:    166 / 196 loss=5.536, nll_loss=4.425, ppl=21.48, wps=21064.3, ups=0.32, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.733, loss_scale=16, train_wall=289, gb_free=19.9, wall=19819
2022-03-06 18:13:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:13:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:13:57 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.138 | nll_loss 7.203 | ppl 147.31 | wps 41801.2 | wpb 510.9 | bsz 1 | num_updates 6229 | best_loss 7.696
2022-03-06 18:13:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6229 updates
2022-03-06 18:13:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 18:14:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 18:14:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 32 @ 6229 updates, score 8.138) (writing took 3.386953515931964 seconds)
2022-03-06 18:14:00 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-06 18:14:00 | INFO | train | epoch 032 | loss 5.515 | nll_loss 4.402 | ppl 21.14 | wps 20547.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 6229 | lr 0.000400674 | gnorm 0.732 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 19920
2022-03-06 18:14:00 | INFO | fairseq.trainer | begin training epoch 33
2022-03-06 18:14:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:17:41 | INFO | train_inner | epoch 033:     71 / 196 loss=5.451, nll_loss=4.331, ppl=20.13, wps=20292.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.735, loss_scale=8, train_wall=292, gb_free=19.9, wall=20141
2022-03-06 18:20:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:22:55 | INFO | train_inner | epoch 033:    172 / 196 loss=5.475, nll_loss=4.356, ppl=20.48, wps=20858.5, ups=0.32, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.755, loss_scale=8, train_wall=292, gb_free=19.9, wall=20456
2022-03-06 18:24:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:24:14 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.16 | nll_loss 7.206 | ppl 147.66 | wps 41834.3 | wpb 510.9 | bsz 1 | num_updates 6424 | best_loss 7.696
2022-03-06 18:24:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6424 updates
2022-03-06 18:24:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 18:24:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 18:24:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 33 @ 6424 updates, score 8.16) (writing took 3.3657973781228065 seconds)
2022-03-06 18:24:18 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-06 18:24:18 | INFO | train | epoch 033 | loss 5.451 | nll_loss 4.331 | ppl 20.12 | wps 20665.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 6424 | lr 0.000394546 | gnorm 0.741 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 20538
2022-03-06 18:24:18 | INFO | fairseq.trainer | begin training epoch 34
2022-03-06 18:24:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:28:14 | INFO | train_inner | epoch 034:     76 / 196 loss=5.378, nll_loss=4.25, ppl=19.03, wps=20486.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.771, loss_scale=16, train_wall=289, gb_free=19.9, wall=20775
2022-03-06 18:33:25 | INFO | train_inner | epoch 034:    176 / 196 loss=5.414, nll_loss=4.288, ppl=19.54, wps=21058.1, ups=0.32, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.735, loss_scale=16, train_wall=289, gb_free=19.9, wall=21086
2022-03-06 18:33:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:34:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:34:32 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.195 | nll_loss 7.235 | ppl 150.6 | wps 41994.9 | wpb 510.9 | bsz 1 | num_updates 6619 | best_loss 7.696
2022-03-06 18:34:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6619 updates
2022-03-06 18:34:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 18:34:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 18:34:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 34 @ 6619 updates, score 8.195) (writing took 3.5468000299297273 seconds)
2022-03-06 18:34:36 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-06 18:34:36 | INFO | train | epoch 034 | loss 5.389 | nll_loss 4.262 | ppl 19.19 | wps 20649.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 6619 | lr 0.00038869 | gnorm 0.756 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 21156
2022-03-06 18:34:36 | INFO | fairseq.trainer | begin training epoch 35
2022-03-06 18:34:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:35:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:38:51 | INFO | train_inner | epoch 035:     82 / 196 loss=5.31, nll_loss=4.175, ppl=18.07, wps=20084.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.743, loss_scale=8, train_wall=294, gb_free=19.9, wall=21411
2022-03-06 18:44:02 | INFO | train_inner | epoch 035:    182 / 196 loss=5.369, nll_loss=4.238, ppl=18.88, wps=21067.9, ups=0.32, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.755, loss_scale=16, train_wall=289, gb_free=19.9, wall=21722
2022-03-06 18:44:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:44:50 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.251 | nll_loss 7.307 | ppl 158.39 | wps 41662.6 | wpb 510.9 | bsz 1 | num_updates 6814 | best_loss 7.696
2022-03-06 18:44:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6814 updates
2022-03-06 18:44:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 18:44:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 18:44:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 35 @ 6814 updates, score 8.251) (writing took 3.479124927893281 seconds)
2022-03-06 18:44:54 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-06 18:44:54 | INFO | train | epoch 035 | loss 5.331 | nll_loss 4.197 | ppl 18.34 | wps 20657.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 6814 | lr 0.000383088 | gnorm 0.752 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 21774
2022-03-06 18:44:54 | INFO | fairseq.trainer | begin training epoch 36
2022-03-06 18:44:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:46:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:49:24 | INFO | train_inner | epoch 036:     87 / 196 loss=5.245, nll_loss=4.103, ppl=17.19, wps=20282, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.76, loss_scale=8, train_wall=291, gb_free=19.9, wall=22045
2022-03-06 18:54:35 | INFO | train_inner | epoch 036:    187 / 196 loss=5.314, nll_loss=4.177, ppl=18.09, wps=21066.8, ups=0.32, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.757, loss_scale=16, train_wall=289, gb_free=19.9, wall=22356
2022-03-06 18:55:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:55:08 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.29 | nll_loss 7.32 | ppl 159.83 | wps 41709.9 | wpb 510.9 | bsz 1 | num_updates 7009 | best_loss 7.696
2022-03-06 18:55:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7009 updates
2022-03-06 18:55:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 18:55:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 18:55:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 36 @ 7009 updates, score 8.29) (writing took 3.3753659341018647 seconds)
2022-03-06 18:55:11 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-06 18:55:11 | INFO | train | epoch 036 | loss 5.273 | nll_loss 4.133 | ppl 17.55 | wps 20661.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 7009 | lr 0.000377722 | gnorm 0.759 | loss_scale 16 | train_wall 566 | gb_free 19.9 | wall 22392
2022-03-06 18:55:11 | INFO | fairseq.trainer | begin training epoch 37
2022-03-06 18:55:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:55:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:59:58 | INFO | train_inner | epoch 037:     92 / 196 loss=5.188, nll_loss=4.04, ppl=16.45, wps=20288.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.766, loss_scale=8, train_wall=291, gb_free=19.9, wall=22678
2022-03-06 19:03:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:05:12 | INFO | train_inner | epoch 037:    193 / 196 loss=5.26, nll_loss=4.117, ppl=17.35, wps=20860.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.76, loss_scale=8, train_wall=292, gb_free=19.9, wall=22992
2022-03-06 19:05:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:05:25 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.33 | nll_loss 7.378 | ppl 166.3 | wps 41689.2 | wpb 510.9 | bsz 1 | num_updates 7203 | best_loss 7.696
2022-03-06 19:05:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7203 updates
2022-03-06 19:05:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 19:05:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 19:05:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 37 @ 7203 updates, score 8.33) (writing took 3.3838798832148314 seconds)
2022-03-06 19:05:29 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-06 19:05:29 | INFO | train | epoch 037 | loss 5.219 | nll_loss 4.073 | ppl 16.83 | wps 20555.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7203 | lr 0.0003726 | gnorm 0.761 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 23009
2022-03-06 19:05:29 | INFO | fairseq.trainer | begin training epoch 38
2022-03-06 19:05:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:10:31 | INFO | train_inner | epoch 038:     97 / 196 loss=5.127, nll_loss=3.971, ppl=15.68, wps=20482.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.772, loss_scale=16, train_wall=289, gb_free=19.9, wall=23311
2022-03-06 19:12:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:15:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:15:43 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.418 | nll_loss 7.474 | ppl 177.83 | wps 41635 | wpb 510.9 | bsz 1 | num_updates 7398 | best_loss 7.696
2022-03-06 19:15:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7398 updates
2022-03-06 19:15:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 19:15:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 19:15:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 38 @ 7398 updates, score 8.418) (writing took 3.3650023001246154 seconds)
2022-03-06 19:15:47 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-06 19:15:47 | INFO | train | epoch 038 | loss 5.168 | nll_loss 4.016 | ppl 16.18 | wps 20660.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 7398 | lr 0.000367657 | gnorm 0.775 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 23627
2022-03-06 19:15:47 | INFO | fairseq.trainer | begin training epoch 39
2022-03-06 19:15:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:15:53 | INFO | train_inner | epoch 039:      2 / 196 loss=5.209, nll_loss=4.061, ppl=16.69, wps=20291.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.777, loss_scale=8, train_wall=291, gb_free=19.9, wall=23633
2022-03-06 19:21:04 | INFO | train_inner | epoch 039:    102 / 196 loss=5.07, nll_loss=3.908, ppl=15.01, wps=21066.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.773, loss_scale=16, train_wall=289, gb_free=19.9, wall=23944
2022-03-06 19:21:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:25:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:26:01 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.487 | nll_loss 7.549 | ppl 187.31 | wps 41575.6 | wpb 510.9 | bsz 1 | num_updates 7593 | best_loss 7.696
2022-03-06 19:26:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7593 updates
2022-03-06 19:26:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 19:26:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 19:26:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 39 @ 7593 updates, score 8.487) (writing took 3.3947364559862763 seconds)
2022-03-06 19:26:04 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-06 19:26:04 | INFO | train | epoch 039 | loss 5.118 | nll_loss 3.961 | ppl 15.57 | wps 20659.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 7593 | lr 0.000362905 | gnorm 0.781 | loss_scale 8 | train_wall 566 | gb_free 19.9 | wall 24245
2022-03-06 19:26:04 | INFO | fairseq.trainer | begin training epoch 40
2022-03-06 19:26:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:26:26 | INFO | train_inner | epoch 040:      7 / 196 loss=5.158, nll_loss=4.003, ppl=16.04, wps=20286.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=7600, lr=0.000362738, gnorm=0.787, loss_scale=8, train_wall=292, gb_free=19.9, wall=24267
2022-03-06 19:31:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:31:41 | INFO | train_inner | epoch 040:    108 / 196 loss=5.035, nll_loss=3.868, ppl=14.61, wps=20824.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.784, loss_scale=8, train_wall=293, gb_free=19.9, wall=24581
2022-03-06 19:36:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:36:20 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.524 | nll_loss 7.58 | ppl 191.33 | wps 41610.8 | wpb 510.9 | bsz 1 | num_updates 7788 | best_loss 7.696
2022-03-06 19:36:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7788 updates
2022-03-06 19:36:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 19:36:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 19:36:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 40 @ 7788 updates, score 8.524) (writing took 3.2836672749836 seconds)
2022-03-06 19:36:23 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-06 19:36:23 | INFO | train | epoch 040 | loss 5.071 | nll_loss 3.908 | ppl 15.01 | wps 20628 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 7788 | lr 0.000358333 | gnorm 0.787 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 24863
2022-03-06 19:36:23 | INFO | fairseq.trainer | begin training epoch 41
2022-03-06 19:36:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:37:01 | INFO | train_inner | epoch 041:     12 / 196 loss=5.101, nll_loss=3.941, ppl=15.36, wps=20450.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.791, loss_scale=8, train_wall=289, gb_free=19.9, wall=24901
2022-03-06 19:41:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:42:16 | INFO | train_inner | epoch 041:    113 / 196 loss=4.988, nll_loss=3.816, ppl=14.09, wps=20804.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.787, loss_scale=8, train_wall=293, gb_free=19.9, wall=25216
2022-03-06 19:46:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:46:39 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.581 | nll_loss 7.639 | ppl 199.38 | wps 41699.4 | wpb 510.9 | bsz 1 | num_updates 7983 | best_loss 7.696
2022-03-06 19:46:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 7983 updates
2022-03-06 19:46:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 19:46:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 19:46:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 41 @ 7983 updates, score 8.581) (writing took 3.247570879990235 seconds)
2022-03-06 19:46:42 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-06 19:46:42 | INFO | train | epoch 041 | loss 5.025 | nll_loss 3.856 | ppl 14.48 | wps 20619.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 7983 | lr 0.00035393 | gnorm 0.785 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 25482
2022-03-06 19:46:42 | INFO | fairseq.trainer | begin training epoch 42
2022-03-06 19:46:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:47:35 | INFO | train_inner | epoch 042:     17 / 196 loss=5.049, nll_loss=3.882, ppl=14.75, wps=20456.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=8000, lr=0.000353553, gnorm=0.791, loss_scale=8, train_wall=289, gb_free=19.9, wall=25535
2022-03-06 19:49:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:52:50 | INFO | train_inner | epoch 042:    118 / 196 loss=4.953, nll_loss=3.776, ppl=13.7, wps=20796, ups=0.32, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.79, loss_scale=8, train_wall=293, gb_free=19.9, wall=25851
2022-03-06 19:56:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:56:58 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.556 | nll_loss 7.606 | ppl 194.85 | wps 41393.4 | wpb 510.9 | bsz 1 | num_updates 8178 | best_loss 7.696
2022-03-06 19:56:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8178 updates
2022-03-06 19:56:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 19:57:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 19:57:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 42 @ 8178 updates, score 8.556) (writing took 3.1851219329982996 seconds)
2022-03-06 19:57:01 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-06 19:57:01 | INFO | train | epoch 042 | loss 4.981 | nll_loss 3.807 | ppl 14 | wps 20618.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 8178 | lr 0.000349685 | gnorm 0.798 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 26101
2022-03-06 19:57:01 | INFO | fairseq.trainer | begin training epoch 43
2022-03-06 19:57:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:57:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:58:13 | INFO | train_inner | epoch 043:     23 / 196 loss=4.995, nll_loss=3.823, ppl=14.15, wps=20271.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.804, loss_scale=8, train_wall=292, gb_free=19.9, wall=26173
2022-03-06 20:03:24 | INFO | train_inner | epoch 043:    123 / 196 loss=4.919, nll_loss=3.739, ppl=13.35, wps=21041.8, ups=0.32, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.802, loss_scale=8, train_wall=290, gb_free=19.9, wall=26484
2022-03-06 20:06:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:07:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:07:16 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.67 | nll_loss 7.722 | ppl 211.06 | wps 41640.4 | wpb 510.9 | bsz 1 | num_updates 8372 | best_loss 7.696
2022-03-06 20:07:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8372 updates
2022-03-06 20:07:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 20:07:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 20:07:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 43 @ 8372 updates, score 8.67) (writing took 3.2104495908133686 seconds)
2022-03-06 20:07:19 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-06 20:07:19 | INFO | train | epoch 043 | loss 4.938 | nll_loss 3.76 | ppl 13.55 | wps 20532.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 8372 | lr 0.000345609 | gnorm 0.808 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 26720
2022-03-06 20:07:19 | INFO | fairseq.trainer | begin training epoch 44
2022-03-06 20:07:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:08:47 | INFO | train_inner | epoch 044:     28 / 196 loss=4.939, nll_loss=3.76, ppl=13.55, wps=20263.9, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=8400, lr=0.000345033, gnorm=0.809, loss_scale=8, train_wall=292, gb_free=19.9, wall=26807
2022-03-06 20:13:58 | INFO | train_inner | epoch 044:    128 / 196 loss=4.883, nll_loss=3.698, ppl=12.98, wps=21039.6, ups=0.32, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.806, loss_scale=16, train_wall=290, gb_free=19.9, wall=27118
2022-03-06 20:15:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:17:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:17:34 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.709 | nll_loss 7.765 | ppl 217.47 | wps 41676.4 | wpb 510.9 | bsz 1 | num_updates 8567 | best_loss 7.696
2022-03-06 20:17:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8567 updates
2022-03-06 20:17:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 20:17:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 20:17:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 44 @ 8567 updates, score 8.709) (writing took 3.200484183151275 seconds)
2022-03-06 20:17:38 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-06 20:17:38 | INFO | train | epoch 044 | loss 4.899 | nll_loss 3.715 | ppl 13.13 | wps 20640.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 8567 | lr 0.000341653 | gnorm 0.81 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 27338
2022-03-06 20:17:38 | INFO | fairseq.trainer | begin training epoch 45
2022-03-06 20:17:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:19:21 | INFO | train_inner | epoch 045:     33 / 196 loss=4.898, nll_loss=3.715, ppl=13.13, wps=20275.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.818, loss_scale=8, train_wall=292, gb_free=19.9, wall=27441
2022-03-06 20:22:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:24:35 | INFO | train_inner | epoch 045:    134 / 196 loss=4.848, nll_loss=3.659, ppl=12.63, wps=20830.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.817, loss_scale=8, train_wall=293, gb_free=19.9, wall=27755
2022-03-06 20:27:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:27:53 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.78 | nll_loss 7.845 | ppl 229.85 | wps 41671.7 | wpb 510.9 | bsz 1 | num_updates 8762 | best_loss 7.696
2022-03-06 20:27:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8762 updates
2022-03-06 20:27:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 20:27:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 20:27:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 45 @ 8762 updates, score 8.78) (writing took 3.200131607009098 seconds)
2022-03-06 20:27:56 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-06 20:27:56 | INFO | train | epoch 045 | loss 4.859 | nll_loss 3.671 | ppl 12.74 | wps 20639.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 8762 | lr 0.00033783 | gnorm 0.822 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 27956
2022-03-06 20:27:56 | INFO | fairseq.trainer | begin training epoch 46
2022-03-06 20:27:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:29:54 | INFO | train_inner | epoch 046:     38 / 196 loss=4.859, nll_loss=3.67, ppl=12.73, wps=20474.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=8800, lr=0.0003371, gnorm=0.834, loss_scale=16, train_wall=289, gb_free=19.9, wall=28075
2022-03-06 20:31:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:35:09 | INFO | train_inner | epoch 046:    139 / 196 loss=4.81, nll_loss=3.617, ppl=12.27, wps=20840.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.821, loss_scale=8, train_wall=292, gb_free=19.9, wall=28389
2022-03-06 20:38:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:38:11 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.797 | nll_loss 7.857 | ppl 231.78 | wps 41727.6 | wpb 510.9 | bsz 1 | num_updates 8957 | best_loss 7.696
2022-03-06 20:38:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8957 updates
2022-03-06 20:38:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 20:38:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 20:38:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 46 @ 8957 updates, score 8.797) (writing took 3.223146795993671 seconds)
2022-03-06 20:38:14 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-06 20:38:14 | INFO | train | epoch 046 | loss 4.821 | nll_loss 3.629 | ppl 12.37 | wps 20644.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 8957 | lr 0.000334132 | gnorm 0.825 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 28574
2022-03-06 20:38:14 | INFO | fairseq.trainer | begin training epoch 47
2022-03-06 20:38:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:38:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:40:31 | INFO | train_inner | epoch 047:     44 / 196 loss=4.808, nll_loss=3.614, ppl=12.25, wps=20264.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.827, loss_scale=8, train_wall=292, gb_free=19.9, wall=28712
2022-03-06 20:45:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:45:46 | INFO | train_inner | epoch 047:    145 / 196 loss=4.782, nll_loss=3.586, ppl=12, wps=20804.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.826, loss_scale=8, train_wall=293, gb_free=19.9, wall=29027
2022-03-06 20:48:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:48:30 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.837 | nll_loss 7.901 | ppl 239.1 | wps 41501.2 | wpb 510.9 | bsz 1 | num_updates 9151 | best_loss 7.696
2022-03-06 20:48:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 9151 updates
2022-03-06 20:48:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 20:48:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 20:48:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 47 @ 9151 updates, score 8.837) (writing took 3.1853545152116567 seconds)
2022-03-06 20:48:33 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-06 20:48:33 | INFO | train | epoch 047 | loss 4.785 | nll_loss 3.588 | ppl 12.03 | wps 20507.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 9151 | lr 0.000330572 | gnorm 0.832 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 29194
2022-03-06 20:48:33 | INFO | fairseq.trainer | begin training epoch 48
2022-03-06 20:48:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:51:06 | INFO | train_inner | epoch 048:     49 / 196 loss=4.763, nll_loss=3.565, ppl=11.83, wps=20440.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=9200, lr=0.00032969, gnorm=0.835, loss_scale=8, train_wall=289, gb_free=19.9, wall=29347
2022-03-06 20:52:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:56:21 | INFO | train_inner | epoch 048:    150 / 196 loss=4.761, nll_loss=3.561, ppl=11.8, wps=20804.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.831, loss_scale=8, train_wall=293, gb_free=19.9, wall=29662
2022-03-06 20:58:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:58:49 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.898 | nll_loss 7.954 | ppl 248.02 | wps 41591.9 | wpb 510.9 | bsz 1 | num_updates 9346 | best_loss 7.696
2022-03-06 20:58:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 9346 updates
2022-03-06 20:58:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 20:58:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 20:58:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 48 @ 9346 updates, score 8.898) (writing took 3.192918635904789 seconds)
2022-03-06 20:58:52 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-06 20:58:52 | INFO | train | epoch 048 | loss 4.751 | nll_loss 3.55 | ppl 11.71 | wps 20612.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 9346 | lr 0.000327105 | gnorm 0.833 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 29813
2022-03-06 20:58:52 | INFO | fairseq.trainer | begin training epoch 49
2022-03-06 20:58:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:01:41 | INFO | train_inner | epoch 049:     54 / 196 loss=4.721, nll_loss=3.517, ppl=11.45, wps=20446.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.84, loss_scale=16, train_wall=289, gb_free=19.9, wall=29981
2022-03-06 21:06:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 21:06:56 | INFO | train_inner | epoch 049:    155 / 196 loss=4.729, nll_loss=3.526, ppl=11.52, wps=20801, ups=0.32, wpb=65532.4, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.835, loss_scale=16, train_wall=293, gb_free=19.9, wall=30296
2022-03-06 21:09:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:09:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:09:08 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.948 | nll_loss 8.003 | ppl 256.52 | wps 41565 | wpb 510.9 | bsz 1 | num_updates 9540 | best_loss 7.696
2022-03-06 21:09:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 9540 updates
2022-03-06 21:09:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 21:09:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 21:09:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 49 @ 9540 updates, score 8.948) (writing took 3.1795322708785534 seconds)
2022-03-06 21:09:11 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-06 21:09:11 | INFO | train | epoch 049 | loss 4.716 | nll_loss 3.512 | ppl 11.41 | wps 20509.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 9540 | lr 0.000323762 | gnorm 0.839 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 30432
2022-03-06 21:09:11 | INFO | fairseq.trainer | begin training epoch 50
2022-03-06 21:09:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:12:19 | INFO | train_inner | epoch 050:     60 / 196 loss=4.686, nll_loss=3.478, ppl=11.14, wps=20262, ups=0.31, wpb=65367, bsz=127.7, num_updates=9600, lr=0.000322749, gnorm=0.854, loss_scale=8, train_wall=292, gb_free=19.9, wall=30619
2022-03-06 21:16:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:17:33 | INFO | train_inner | epoch 050:    161 / 196 loss=4.699, nll_loss=3.492, ppl=11.25, wps=20823, ups=0.32, wpb=65532.4, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.843, loss_scale=8, train_wall=293, gb_free=19.9, wall=30934
2022-03-06 21:19:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:19:27 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.984 | nll_loss 8.049 | ppl 264.93 | wps 41573.3 | wpb 510.9 | bsz 1 | num_updates 9735 | best_loss 7.696
2022-03-06 21:19:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 9735 updates
2022-03-06 21:19:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 21:19:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 21:19:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 50 @ 9735 updates, score 8.984) (writing took 3.1748761599883437 seconds)
2022-03-06 21:19:30 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-06 21:19:30 | INFO | train | epoch 050 | loss 4.684 | nll_loss 3.476 | ppl 11.12 | wps 20632.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 9735 | lr 0.000320503 | gnorm 0.847 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 31050
2022-03-06 21:19:30 | INFO | fairseq.trainer | begin training epoch 51
2022-03-06 21:19:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:22:53 | INFO | train_inner | epoch 051:     65 / 196 loss=4.64, nll_loss=3.427, ppl=10.76, wps=20461.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.865, loss_scale=8, train_wall=289, gb_free=19.9, wall=31253
2022-03-06 21:25:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:28:08 | INFO | train_inner | epoch 051:    166 / 196 loss=4.676, nll_loss=3.466, ppl=11.05, wps=20810.5, ups=0.32, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.844, loss_scale=8, train_wall=293, gb_free=19.9, wall=31568
2022-03-06 21:29:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:29:46 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.026 | nll_loss 8.106 | ppl 275.43 | wps 41150.7 | wpb 510.9 | bsz 1 | num_updates 9930 | best_loss 7.696
2022-03-06 21:29:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 9930 updates
2022-03-06 21:29:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 21:29:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 21:29:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 51 @ 9930 updates, score 9.026) (writing took 3.247750242939219 seconds)
2022-03-06 21:29:49 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-06 21:29:49 | INFO | train | epoch 051 | loss 4.654 | nll_loss 3.442 | ppl 10.86 | wps 20620.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 9930 | lr 0.00031734 | gnorm 0.857 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 31669
2022-03-06 21:29:49 | INFO | fairseq.trainer | begin training epoch 52
2022-03-06 21:29:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:32:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:33:30 | INFO | train_inner | epoch 052:     71 / 196 loss=4.616, nll_loss=3.399, ppl=10.55, wps=20243.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10000, lr=0.000316228, gnorm=0.863, loss_scale=8, train_wall=292, gb_free=19.9, wall=31891
2022-03-06 21:38:42 | INFO | train_inner | epoch 052:    171 / 196 loss=4.651, nll_loss=3.438, ppl=10.84, wps=21005.9, ups=0.32, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.858, loss_scale=8, train_wall=290, gb_free=19.9, wall=32203
2022-03-06 21:39:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:40:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:40:05 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.096 | nll_loss 8.173 | ppl 288.56 | wps 41469.4 | wpb 510.9 | bsz 1 | num_updates 10124 | best_loss 7.696
2022-03-06 21:40:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 10124 updates
2022-03-06 21:40:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 21:40:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 21:40:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 52 @ 10124 updates, score 9.096) (writing took 3.1826430461369455 seconds)
2022-03-06 21:40:08 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-06 21:40:08 | INFO | train | epoch 052 | loss 4.623 | nll_loss 3.407 | ppl 10.61 | wps 20503.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 10124 | lr 0.000314285 | gnorm 0.861 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 32288
2022-03-06 21:40:08 | INFO | fairseq.trainer | begin training epoch 53
2022-03-06 21:40:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:44:05 | INFO | train_inner | epoch 053:     76 / 196 loss=4.575, nll_loss=3.355, ppl=10.23, wps=20258.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.878, loss_scale=8, train_wall=292, gb_free=19.9, wall=32525
2022-03-06 21:48:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:49:20 | INFO | train_inner | epoch 053:    177 / 196 loss=4.626, nll_loss=3.41, ppl=10.63, wps=20818.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.869, loss_scale=8, train_wall=293, gb_free=19.9, wall=32840
2022-03-06 21:50:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:50:24 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.128 | nll_loss 8.205 | ppl 295.04 | wps 41601.5 | wpb 510.9 | bsz 1 | num_updates 10319 | best_loss 7.696
2022-03-06 21:50:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 10319 updates
2022-03-06 21:50:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 21:50:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 21:50:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 53 @ 10319 updates, score 9.128) (writing took 3.216496975161135 seconds)
2022-03-06 21:50:27 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-06 21:50:27 | INFO | train | epoch 053 | loss 4.594 | nll_loss 3.375 | ppl 10.38 | wps 20625.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 10319 | lr 0.000311301 | gnorm 0.874 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 32907
2022-03-06 21:50:27 | INFO | fairseq.trainer | begin training epoch 54
2022-03-06 21:50:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:54:40 | INFO | train_inner | epoch 054:     81 / 196 loss=4.531, nll_loss=3.305, ppl=9.88, wps=20451.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=10400, lr=0.000310087, gnorm=0.868, loss_scale=8, train_wall=289, gb_free=19.9, wall=33160
2022-03-06 21:56:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:59:54 | INFO | train_inner | epoch 054:    182 / 196 loss=4.606, nll_loss=3.387, ppl=10.46, wps=20820.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.877, loss_scale=8, train_wall=293, gb_free=19.9, wall=33475
2022-03-06 22:00:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:00:42 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.147 | nll_loss 8.223 | ppl 298.69 | wps 41285.6 | wpb 510.9 | bsz 1 | num_updates 10514 | best_loss 7.696
2022-03-06 22:00:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 10514 updates
2022-03-06 22:00:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 22:00:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 22:00:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 54 @ 10514 updates, score 9.147) (writing took 3.220034272875637 seconds)
2022-03-06 22:00:46 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-06 22:00:46 | INFO | train | epoch 054 | loss 4.566 | nll_loss 3.344 | ppl 10.15 | wps 20623.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 10514 | lr 0.000308401 | gnorm 0.87 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 33526
2022-03-06 22:00:46 | INFO | fairseq.trainer | begin training epoch 55
2022-03-06 22:00:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:05:14 | INFO | train_inner | epoch 055:     86 / 196 loss=4.506, nll_loss=3.277, ppl=9.69, wps=20450.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.86, loss_scale=16, train_wall=289, gb_free=19.9, wall=33794
2022-03-06 22:07:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:10:29 | INFO | train_inner | epoch 055:    187 / 196 loss=4.583, nll_loss=3.362, ppl=10.28, wps=20824, ups=0.32, wpb=65532.4, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.883, loss_scale=8, train_wall=293, gb_free=19.9, wall=34109
2022-03-06 22:10:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:11:01 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.196 | nll_loss 8.271 | ppl 308.94 | wps 41851.3 | wpb 510.9 | bsz 1 | num_updates 10709 | best_loss 7.696
2022-03-06 22:11:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 10709 updates
2022-03-06 22:11:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 22:11:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 22:11:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 55 @ 10709 updates, score 9.196) (writing took 3.173177855089307 seconds)
2022-03-06 22:11:04 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-06 22:11:04 | INFO | train | epoch 055 | loss 4.54 | nll_loss 3.314 | ppl 9.95 | wps 20629.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 10709 | lr 0.00030558 | gnorm 0.874 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 34145
2022-03-06 22:11:04 | INFO | fairseq.trainer | begin training epoch 56
2022-03-06 22:11:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:15:48 | INFO | train_inner | epoch 056:     91 / 196 loss=4.474, nll_loss=3.241, ppl=9.45, wps=20452.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10800, lr=0.00030429, gnorm=0.888, loss_scale=16, train_wall=289, gb_free=19.9, wall=34429
2022-03-06 22:16:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:21:03 | INFO | train_inner | epoch 056:    192 / 196 loss=4.555, nll_loss=3.331, ppl=10.06, wps=20809.1, ups=0.32, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.882, loss_scale=8, train_wall=293, gb_free=19.9, wall=34743
2022-03-06 22:21:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:21:20 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.199 | nll_loss 8.264 | ppl 307.42 | wps 41534.3 | wpb 510.9 | bsz 1 | num_updates 10904 | best_loss 7.696
2022-03-06 22:21:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 10904 updates
2022-03-06 22:21:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 22:21:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 22:21:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 56 @ 10904 updates, score 9.199) (writing took 3.209272396983579 seconds)
2022-03-06 22:21:23 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-06 22:21:23 | INFO | train | epoch 056 | loss 4.512 | nll_loss 3.284 | ppl 9.74 | wps 20617.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 10904 | lr 0.000302836 | gnorm 0.883 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 34764
2022-03-06 22:21:23 | INFO | fairseq.trainer | begin training epoch 57
2022-03-06 22:21:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:23:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:26:26 | INFO | train_inner | epoch 057:     97 / 196 loss=4.447, nll_loss=3.21, ppl=9.26, wps=20253, ups=0.31, wpb=65367, bsz=127.7, num_updates=11000, lr=0.000301511, gnorm=0.888, loss_scale=8, train_wall=292, gb_free=19.9, wall=35066
2022-03-06 22:31:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:31:39 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.279 | nll_loss 8.354 | ppl 327.2 | wps 41591.5 | wpb 510.9 | bsz 1 | num_updates 11099 | best_loss 7.696
2022-03-06 22:31:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 11099 updates
2022-03-06 22:31:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 22:31:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 22:31:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 57 @ 11099 updates, score 9.279) (writing took 3.169649991206825 seconds)
2022-03-06 22:31:42 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-06 22:31:42 | INFO | train | epoch 057 | loss 4.486 | nll_loss 3.254 | ppl 9.54 | wps 20621.7 | ups 0.32 | wpb 65449.4 | bsz 127.8 | num_updates 11099 | lr 0.000300164 | gnorm 0.895 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 35383
2022-03-06 22:31:42 | INFO | fairseq.trainer | begin training epoch 58
2022-03-06 22:31:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:31:45 | INFO | train_inner | epoch 058:      1 / 196 loss=4.529, nll_loss=3.302, ppl=9.86, wps=20456, ups=0.31, wpb=65367, bsz=127.7, num_updates=11100, lr=0.00030015, gnorm=0.902, loss_scale=16, train_wall=289, gb_free=19.9, wall=35386
2022-03-06 22:34:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:37:00 | INFO | train_inner | epoch 058:    102 / 196 loss=4.418, nll_loss=3.178, ppl=9.05, wps=20810.7, ups=0.32, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.885, loss_scale=8, train_wall=293, gb_free=19.9, wall=35701
2022-03-06 22:41:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:41:58 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.339 | nll_loss 8.41 | ppl 340.19 | wps 41731.3 | wpb 510.9 | bsz 1 | num_updates 11294 | best_loss 7.696
2022-03-06 22:41:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 11294 updates
2022-03-06 22:41:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 22:42:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 22:42:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 58 @ 11294 updates, score 9.339) (writing took 3.159855931997299 seconds)
2022-03-06 22:42:01 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-06 22:42:01 | INFO | train | epoch 058 | loss 4.463 | nll_loss 3.228 | ppl 9.37 | wps 20628 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 11294 | lr 0.000297561 | gnorm 0.891 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 36001
2022-03-06 22:42:01 | INFO | fairseq.trainer | begin training epoch 59
2022-03-06 22:42:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:42:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:42:23 | INFO | train_inner | epoch 059:      7 / 196 loss=4.501, nll_loss=3.27, ppl=9.64, wps=20270.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=11300, lr=0.000297482, gnorm=0.898, loss_scale=8, train_wall=292, gb_free=19.9, wall=36023
2022-03-06 22:47:35 | INFO | train_inner | epoch 059:    107 / 196 loss=4.396, nll_loss=3.154, ppl=8.9, wps=21026.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.884, loss_scale=8, train_wall=290, gb_free=19.9, wall=36335
2022-03-06 22:50:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:52:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:52:17 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.364 | nll_loss 8.437 | ppl 346.67 | wps 41275.3 | wpb 510.9 | bsz 1 | num_updates 11488 | best_loss 7.696
2022-03-06 22:52:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 11488 updates
2022-03-06 22:52:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 22:52:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 22:52:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 59 @ 11488 updates, score 9.364) (writing took 3.567302235169336 seconds)
2022-03-06 22:52:20 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-06 22:52:20 | INFO | train | epoch 059 | loss 4.438 | nll_loss 3.2 | ppl 9.19 | wps 20496.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 11488 | lr 0.000295038 | gnorm 0.892 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 36621
2022-03-06 22:52:20 | INFO | fairseq.trainer | begin training epoch 60
2022-03-06 22:52:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:52:58 | INFO | train_inner | epoch 060:     12 / 196 loss=4.473, nll_loss=3.238, ppl=9.44, wps=20206.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=11500, lr=0.000294884, gnorm=0.898, loss_scale=8, train_wall=292, gb_free=19.9, wall=36658
2022-03-06 22:58:10 | INFO | train_inner | epoch 060:    112 / 196 loss=4.383, nll_loss=3.139, ppl=8.81, wps=20996.1, ups=0.32, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.882, loss_scale=16, train_wall=290, gb_free=19.9, wall=36970
2022-03-06 23:00:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:02:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:02:37 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.38 | nll_loss 8.452 | ppl 350.3 | wps 41484 | wpb 510.9 | bsz 1 | num_updates 11683 | best_loss 7.696
2022-03-06 23:02:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 11683 updates
2022-03-06 23:02:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 23:02:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 23:02:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 60 @ 11683 updates, score 9.38) (writing took 3.0498713890556246 seconds)
2022-03-06 23:02:40 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-06 23:02:40 | INFO | train | epoch 060 | loss 4.416 | nll_loss 3.175 | ppl 9.03 | wps 20602.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11683 | lr 0.000292565 | gnorm 0.891 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 37240
2022-03-06 23:02:40 | INFO | fairseq.trainer | begin training epoch 61
2022-03-06 23:02:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:03:33 | INFO | train_inner | epoch 061:     17 / 196 loss=4.439, nll_loss=3.201, ppl=9.2, wps=20244.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=11700, lr=0.000292353, gnorm=0.905, loss_scale=8, train_wall=292, gb_free=19.9, wall=37293
2022-03-06 23:07:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:08:48 | INFO | train_inner | epoch 061:    118 / 196 loss=4.364, nll_loss=3.117, ppl=8.68, wps=20794.5, ups=0.32, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.908, loss_scale=8, train_wall=293, gb_free=19.9, wall=37608
2022-03-06 23:12:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:12:56 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.46 | nll_loss 8.545 | ppl 373.59 | wps 41555.3 | wpb 510.9 | bsz 1 | num_updates 11878 | best_loss 7.696
2022-03-06 23:12:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 11878 updates
2022-03-06 23:12:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 23:12:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 23:12:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 61 @ 11878 updates, score 9.46) (writing took 3.1348778661340475 seconds)
2022-03-06 23:12:59 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-06 23:12:59 | INFO | train | epoch 061 | loss 4.393 | nll_loss 3.15 | ppl 8.88 | wps 20602.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11878 | lr 0.000290154 | gnorm 0.913 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 37860
2022-03-06 23:12:59 | INFO | fairseq.trainer | begin training epoch 62
2022-03-06 23:12:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:14:08 | INFO | train_inner | epoch 062:     22 / 196 loss=4.413, nll_loss=3.172, ppl=9.01, wps=20437.5, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=11900, lr=0.000289886, gnorm=0.914, loss_scale=16, train_wall=289, gb_free=19.9, wall=37928
2022-03-06 23:19:20 | INFO | train_inner | epoch 062:    122 / 196 loss=4.343, nll_loss=3.094, ppl=8.54, wps=21001.2, ups=0.32, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.906, loss_scale=16, train_wall=290, gb_free=19.9, wall=38240
2022-03-06 23:19:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:23:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:23:15 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.468 | nll_loss 8.543 | ppl 373.05 | wps 41433.9 | wpb 510.9 | bsz 1 | num_updates 12073 | best_loss 7.696
2022-03-06 23:23:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 12073 updates
2022-03-06 23:23:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 23:23:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 23:23:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 62 @ 12073 updates, score 9.468) (writing took 3.167980292113498 seconds)
2022-03-06 23:23:19 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-06 23:23:19 | INFO | train | epoch 062 | loss 4.371 | nll_loss 3.125 | ppl 8.72 | wps 20607.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12073 | lr 0.000287801 | gnorm 0.911 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 38479
2022-03-06 23:23:19 | INFO | fairseq.trainer | begin training epoch 63
2022-03-06 23:23:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:24:43 | INFO | train_inner | epoch 063:     27 / 196 loss=4.392, nll_loss=3.149, ppl=8.87, wps=20245.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=12100, lr=0.00028748, gnorm=0.923, loss_scale=8, train_wall=292, gb_free=19.9, wall=38563
2022-03-06 23:27:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:29:58 | INFO | train_inner | epoch 063:    128 / 196 loss=4.328, nll_loss=3.078, ppl=8.44, wps=20795.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.911, loss_scale=8, train_wall=293, gb_free=19.9, wall=38878
2022-03-06 23:33:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:33:35 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.493 | nll_loss 8.569 | ppl 379.89 | wps 41473.5 | wpb 510.9 | bsz 1 | num_updates 12268 | best_loss 7.696
2022-03-06 23:33:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 12268 updates
2022-03-06 23:33:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 23:33:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 23:33:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 63 @ 12268 updates, score 9.493) (writing took 3.1562076560221612 seconds)
2022-03-06 23:33:38 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-06 23:33:38 | INFO | train | epoch 063 | loss 4.35 | nll_loss 3.101 | ppl 8.58 | wps 20602 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12268 | lr 0.000285505 | gnorm 0.921 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 39098
2022-03-06 23:33:38 | INFO | fairseq.trainer | begin training epoch 64
2022-03-06 23:33:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:35:18 | INFO | train_inner | epoch 064:     32 / 196 loss=4.359, nll_loss=3.111, ppl=8.64, wps=20429.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=12300, lr=0.000285133, gnorm=0.929, loss_scale=16, train_wall=289, gb_free=19.9, wall=39198
2022-03-06 23:36:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:40:33 | INFO | train_inner | epoch 064:    133 / 196 loss=4.313, nll_loss=3.061, ppl=8.34, wps=20792, ups=0.32, wpb=65532.4, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.922, loss_scale=8, train_wall=293, gb_free=19.9, wall=39513
2022-03-06 23:43:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:43:54 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.484 | nll_loss 8.565 | ppl 378.83 | wps 41616 | wpb 510.9 | bsz 1 | num_updates 12463 | best_loss 7.696
2022-03-06 23:43:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 12463 updates
2022-03-06 23:43:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 23:43:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 23:43:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 64 @ 12463 updates, score 9.484) (writing took 3.161764949094504 seconds)
2022-03-06 23:43:57 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-06 23:43:57 | INFO | train | epoch 064 | loss 4.329 | nll_loss 3.078 | ppl 8.44 | wps 20603.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12463 | lr 0.000283262 | gnorm 0.923 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 39718
2022-03-06 23:43:57 | INFO | fairseq.trainer | begin training epoch 65
2022-03-06 23:43:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:44:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:45:56 | INFO | train_inner | epoch 065:     38 / 196 loss=4.332, nll_loss=3.081, ppl=8.46, wps=20235.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=12500, lr=0.000282843, gnorm=0.939, loss_scale=8, train_wall=292, gb_free=19.9, wall=39837
2022-03-06 23:51:08 | INFO | train_inner | epoch 065:    138 / 196 loss=4.299, nll_loss=3.044, ppl=8.25, wps=21001.7, ups=0.32, wpb=65536, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.925, loss_scale=8, train_wall=290, gb_free=19.9, wall=40149
2022-03-06 23:53:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:54:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:54:14 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.527 | nll_loss 8.601 | ppl 388.34 | wps 41705.9 | wpb 510.9 | bsz 1 | num_updates 12657 | best_loss 7.696
2022-03-06 23:54:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 12657 updates
2022-03-06 23:54:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 23:54:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-06 23:54:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 65 @ 12657 updates, score 9.527) (writing took 3.1904123530257493 seconds)
2022-03-06 23:54:17 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-06 23:54:17 | INFO | train | epoch 065 | loss 4.308 | nll_loss 3.054 | ppl 8.31 | wps 20493.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 12657 | lr 0.000281083 | gnorm 0.933 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 40337
2022-03-06 23:54:17 | INFO | fairseq.trainer | begin training epoch 66
2022-03-06 23:54:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:56:31 | INFO | train_inner | epoch 066:     43 / 196 loss=4.305, nll_loss=3.051, ppl=8.29, wps=20235.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=12700, lr=0.000280607, gnorm=0.929, loss_scale=8, train_wall=292, gb_free=19.9, wall=40472
2022-03-07 00:01:43 | INFO | train_inner | epoch 066:    143 / 196 loss=4.29, nll_loss=3.034, ppl=8.19, wps=21001.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.93, loss_scale=16, train_wall=290, gb_free=19.9, wall=40784
2022-03-07 00:04:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:04:33 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.61 | nll_loss 8.703 | ppl 416.64 | wps 41617.8 | wpb 510.9 | bsz 1 | num_updates 12853 | best_loss 7.696
2022-03-07 00:04:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 12853 updates
2022-03-07 00:04:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 00:04:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 00:04:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 66 @ 12853 updates, score 9.61) (writing took 3.146592617034912 seconds)
2022-03-07 00:04:36 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-07 00:04:36 | INFO | train | epoch 066 | loss 4.29 | nll_loss 3.035 | ppl 8.19 | wps 20709.1 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 12853 | lr 0.000278932 | gnorm 0.933 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 40957
2022-03-07 00:04:36 | INFO | fairseq.trainer | begin training epoch 67
2022-03-07 00:04:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:07:03 | INFO | train_inner | epoch 067:     47 / 196 loss=4.281, nll_loss=3.024, ppl=8.13, wps=20432.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=12900, lr=0.000278423, gnorm=0.931, loss_scale=16, train_wall=289, gb_free=19.9, wall=41104
2022-03-07 00:07:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:12:18 | INFO | train_inner | epoch 067:    148 / 196 loss=4.271, nll_loss=3.013, ppl=8.07, wps=20785.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.927, loss_scale=16, train_wall=293, gb_free=19.9, wall=41419
2022-03-07 00:14:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:14:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:14:53 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.613 | nll_loss 8.699 | ppl 415.66 | wps 41327.9 | wpb 510.9 | bsz 1 | num_updates 13047 | best_loss 7.696
2022-03-07 00:14:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 13047 updates
2022-03-07 00:14:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 00:14:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 00:14:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 67 @ 13047 updates, score 9.613) (writing took 3.14892336493358 seconds)
2022-03-07 00:14:56 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-07 00:14:56 | INFO | train | epoch 067 | loss 4.27 | nll_loss 3.012 | ppl 8.07 | wps 20494.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 13047 | lr 0.00027685 | gnorm 0.927 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 41576
2022-03-07 00:14:56 | INFO | fairseq.trainer | begin training epoch 68
2022-03-07 00:14:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:17:41 | INFO | train_inner | epoch 068:     53 / 196 loss=4.258, nll_loss=2.998, ppl=7.99, wps=20241.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=13100, lr=0.000276289, gnorm=0.952, loss_scale=16, train_wall=292, gb_free=19.9, wall=41742
2022-03-07 00:21:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:22:56 | INFO | train_inner | epoch 068:    154 / 196 loss=4.261, nll_loss=3.001, ppl=8.01, wps=20800.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.948, loss_scale=8, train_wall=293, gb_free=19.9, wall=42057
2022-03-07 00:25:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:25:12 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.656 | nll_loss 8.744 | ppl 428.81 | wps 41704.3 | wpb 510.9 | bsz 1 | num_updates 13242 | best_loss 7.696
2022-03-07 00:25:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 13242 updates
2022-03-07 00:25:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 00:25:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 00:25:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 68 @ 13242 updates, score 9.656) (writing took 3.194636998930946 seconds)
2022-03-07 00:25:15 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-07 00:25:15 | INFO | train | epoch 068 | loss 4.253 | nll_loss 2.992 | ppl 7.96 | wps 20610.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13242 | lr 0.000274804 | gnorm 0.95 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 42195
2022-03-07 00:25:15 | INFO | fairseq.trainer | begin training epoch 69
2022-03-07 00:25:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:28:16 | INFO | train_inner | epoch 069:     58 / 196 loss=4.232, nll_loss=2.969, ppl=7.83, wps=20440.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=13300, lr=0.000274204, gnorm=0.936, loss_scale=16, train_wall=289, gb_free=19.9, wall=42377
2022-03-07 00:30:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:33:31 | INFO | train_inner | epoch 069:    159 / 196 loss=4.247, nll_loss=2.985, ppl=7.92, wps=20799.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.942, loss_scale=8, train_wall=293, gb_free=19.9, wall=42692
2022-03-07 00:35:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:35:31 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.673 | nll_loss 8.759 | ppl 433.13 | wps 41424.9 | wpb 510.9 | bsz 1 | num_updates 13437 | best_loss 7.696
2022-03-07 00:35:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 13437 updates
2022-03-07 00:35:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 00:35:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 00:35:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 69 @ 13437 updates, score 9.673) (writing took 3.205498259048909 seconds)
2022-03-07 00:35:35 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-07 00:35:35 | INFO | train | epoch 069 | loss 4.235 | nll_loss 2.973 | ppl 7.85 | wps 20604 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13437 | lr 0.000272803 | gnorm 0.938 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 42815
2022-03-07 00:35:35 | INFO | fairseq.trainer | begin training epoch 70
2022-03-07 00:35:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:38:51 | INFO | train_inner | epoch 070:     63 / 196 loss=4.212, nll_loss=2.947, ppl=7.71, wps=20433.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=0.946, loss_scale=16, train_wall=289, gb_free=19.9, wall=43012
2022-03-07 00:44:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:44:06 | INFO | train_inner | epoch 070:    164 / 196 loss=4.234, nll_loss=2.972, ppl=7.85, wps=20795.7, ups=0.32, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.942, loss_scale=16, train_wall=293, gb_free=19.9, wall=43327
2022-03-07 00:45:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:45:51 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.692 | nll_loss 8.783 | ppl 440.45 | wps 41526.9 | wpb 510.9 | bsz 1 | num_updates 13632 | best_loss 7.696
2022-03-07 00:45:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 13632 updates
2022-03-07 00:45:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 00:45:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 00:45:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 70 @ 13632 updates, score 9.692) (writing took 3.2220350760035217 seconds)
2022-03-07 00:45:54 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-07 00:45:54 | INFO | train | epoch 070 | loss 4.217 | nll_loss 2.953 | ppl 7.74 | wps 20604.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13632 | lr 0.000270845 | gnorm 0.947 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 43434
2022-03-07 00:45:54 | INFO | fairseq.trainer | begin training epoch 71
2022-03-07 00:45:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:49:26 | INFO | train_inner | epoch 071:     68 / 196 loss=4.188, nll_loss=2.92, ppl=7.57, wps=20429.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=13700, lr=0.000270172, gnorm=0.953, loss_scale=16, train_wall=290, gb_free=19.9, wall=43647
2022-03-07 00:50:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:54:42 | INFO | train_inner | epoch 071:    169 / 196 loss=4.22, nll_loss=2.956, ppl=7.76, wps=20787, ups=0.32, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.952, loss_scale=16, train_wall=293, gb_free=19.9, wall=43962
2022-03-07 00:56:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:56:10 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.762 | nll_loss 8.858 | ppl 464.13 | wps 41566.8 | wpb 510.9 | bsz 1 | num_updates 13827 | best_loss 7.696
2022-03-07 00:56:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 13827 updates
2022-03-07 00:56:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 00:56:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 00:56:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 71 @ 13827 updates, score 9.762) (writing took 3.2258488219231367 seconds)
2022-03-07 00:56:14 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-07 00:56:14 | INFO | train | epoch 071 | loss 4.201 | nll_loss 2.935 | ppl 7.65 | wps 20596 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13827 | lr 0.000268928 | gnorm 0.954 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 44054
2022-03-07 00:56:14 | INFO | fairseq.trainer | begin training epoch 72
2022-03-07 00:56:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:57:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:58:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:00:08 | INFO | train_inner | epoch 072:     75 / 196 loss=4.165, nll_loss=2.895, ppl=7.44, wps=20045.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=0.953, loss_scale=8, train_wall=295, gb_free=19.9, wall=44288
2022-03-07 01:04:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:05:23 | INFO | train_inner | epoch 072:    176 / 196 loss=4.208, nll_loss=2.942, ppl=7.69, wps=20806.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.955, loss_scale=8, train_wall=293, gb_free=19.9, wall=44603
2022-03-07 01:06:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:06:30 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.789 | nll_loss 8.882 | ppl 471.89 | wps 41826.8 | wpb 510.9 | bsz 1 | num_updates 14020 | best_loss 7.696
2022-03-07 01:06:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 14020 updates
2022-03-07 01:06:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 01:06:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 01:06:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 72 @ 14020 updates, score 9.789) (writing took 3.236276676878333 seconds)
2022-03-07 01:06:33 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-07 01:06:33 | INFO | train | epoch 072 | loss 4.183 | nll_loss 2.915 | ppl 7.54 | wps 20399.4 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 14020 | lr 0.000267071 | gnorm 0.954 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 44673
2022-03-07 01:06:33 | INFO | fairseq.trainer | begin training epoch 73
2022-03-07 01:06:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:10:43 | INFO | train_inner | epoch 073:     80 / 196 loss=4.148, nll_loss=2.875, ppl=7.34, wps=20431.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14100, lr=0.000266312, gnorm=0.962, loss_scale=8, train_wall=289, gb_free=19.9, wall=44923
2022-03-07 01:13:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:15:58 | INFO | train_inner | epoch 073:    181 / 196 loss=4.198, nll_loss=2.931, ppl=7.63, wps=20790.4, ups=0.32, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.96, loss_scale=8, train_wall=293, gb_free=19.9, wall=45238
2022-03-07 01:16:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:16:49 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.796 | nll_loss 8.893 | ppl 475.41 | wps 41673 | wpb 510.9 | bsz 1 | num_updates 14215 | best_loss 7.696
2022-03-07 01:16:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 14215 updates
2022-03-07 01:16:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 01:16:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 01:16:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 73 @ 14215 updates, score 9.796) (writing took 3.2038686270825565 seconds)
2022-03-07 01:16:52 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-07 01:16:52 | INFO | train | epoch 073 | loss 4.169 | nll_loss 2.898 | ppl 7.46 | wps 20600.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14215 | lr 0.000265232 | gnorm 0.963 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 45293
2022-03-07 01:16:52 | INFO | fairseq.trainer | begin training epoch 74
2022-03-07 01:16:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:21:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:21:21 | INFO | train_inner | epoch 074:     86 / 196 loss=4.123, nll_loss=2.848, ppl=7.2, wps=20240.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=0.969, loss_scale=8, train_wall=292, gb_free=19.9, wall=45561
2022-03-07 01:26:33 | INFO | train_inner | epoch 074:    186 / 196 loss=4.19, nll_loss=2.922, ppl=7.58, wps=21007.4, ups=0.32, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.967, loss_scale=8, train_wall=290, gb_free=19.9, wall=45873
2022-03-07 01:27:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:27:08 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.852 | nll_loss 8.955 | ppl 496.4 | wps 41716.5 | wpb 510.9 | bsz 1 | num_updates 14410 | best_loss 7.696
2022-03-07 01:27:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 14410 updates
2022-03-07 01:27:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 01:27:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 01:27:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 74 @ 14410 updates, score 9.852) (writing took 3.226437207078561 seconds)
2022-03-07 01:27:12 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-07 01:27:12 | INFO | train | epoch 074 | loss 4.153 | nll_loss 2.881 | ppl 7.37 | wps 20607.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14410 | lr 0.000263432 | gnorm 0.967 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 45912
2022-03-07 01:27:12 | INFO | fairseq.trainer | begin training epoch 75
2022-03-07 01:27:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:28:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:31:56 | INFO | train_inner | epoch 075:     91 / 196 loss=4.1, nll_loss=2.822, ppl=7.07, wps=20242.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=14500, lr=0.000262613, gnorm=0.978, loss_scale=8, train_wall=292, gb_free=19.9, wall=46196
2022-03-07 01:35:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:37:11 | INFO | train_inner | epoch 075:    192 / 196 loss=4.18, nll_loss=2.911, ppl=7.52, wps=20801.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.981, loss_scale=8, train_wall=293, gb_free=19.9, wall=46511
2022-03-07 01:37:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:37:28 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.868 | nll_loss 8.969 | ppl 501 | wps 41522.4 | wpb 510.9 | bsz 1 | num_updates 14604 | best_loss 7.696
2022-03-07 01:37:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 14604 updates
2022-03-07 01:37:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 01:37:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 01:37:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 75 @ 14604 updates, score 9.868) (writing took 3.264696419937536 seconds)
2022-03-07 01:37:31 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-07 01:37:31 | INFO | train | epoch 075 | loss 4.136 | nll_loss 2.862 | ppl 7.27 | wps 20502.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14604 | lr 0.000261676 | gnorm 0.978 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 46531
2022-03-07 01:37:31 | INFO | fairseq.trainer | begin training epoch 76
2022-03-07 01:37:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:42:30 | INFO | train_inner | epoch 076:     96 / 196 loss=4.085, nll_loss=2.805, ppl=6.99, wps=20436.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=14700, lr=0.00026082, gnorm=0.968, loss_scale=8, train_wall=289, gb_free=19.9, wall=46831
2022-03-07 01:42:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:47:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:47:47 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 9.88 | nll_loss 8.976 | ppl 503.66 | wps 41664 | wpb 510.9 | bsz 1 | num_updates 14799 | best_loss 7.696
2022-03-07 01:47:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 14799 updates
2022-03-07 01:47:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 01:47:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 01:47:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 76 @ 14799 updates, score 9.88) (writing took 3.270167333073914 seconds)
2022-03-07 01:47:50 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-07 01:47:50 | INFO | train | epoch 076 | loss 4.123 | nll_loss 2.847 | ppl 7.2 | wps 20604.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14799 | lr 0.000259946 | gnorm 0.97 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 47151
2022-03-07 01:47:50 | INFO | fairseq.trainer | begin training epoch 77
2022-03-07 01:47:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:47:54 | INFO | train_inner | epoch 077:      1 / 196 loss=4.162, nll_loss=2.891, ppl=7.42, wps=20235.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14800, lr=0.000259938, gnorm=0.973, loss_scale=8, train_wall=292, gb_free=19.9, wall=47154
2022-03-07 01:53:06 | INFO | train_inner | epoch 077:    101 / 196 loss=4.067, nll_loss=2.785, ppl=6.89, wps=20990, ups=0.32, wpb=65532.4, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.965, loss_scale=16, train_wall=290, gb_free=19.9, wall=47466
2022-03-07 01:53:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:58:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:58:07 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 9.893 | nll_loss 8.983 | ppl 506.03 | wps 41397.9 | wpb 510.9 | bsz 1 | num_updates 14994 | best_loss 7.696
2022-03-07 01:58:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 14994 updates
2022-03-07 01:58:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 01:58:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 01:58:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 77 @ 14994 updates, score 9.893) (writing took 3.343147485051304 seconds)
2022-03-07 01:58:10 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-07 01:58:10 | INFO | train | epoch 077 | loss 4.11 | nll_loss 2.832 | ppl 7.12 | wps 20580.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14994 | lr 0.000258251 | gnorm 0.979 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 47771
2022-03-07 01:58:10 | INFO | fairseq.trainer | begin training epoch 78
2022-03-07 01:58:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:58:29 | INFO | train_inner | epoch 078:      6 / 196 loss=4.146, nll_loss=2.873, ppl=7.33, wps=20205.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=0.993, loss_scale=8, train_wall=293, gb_free=19.9, wall=47790
2022-03-07 02:03:41 | INFO | train_inner | epoch 078:    106 / 196 loss=4.052, nll_loss=2.768, ppl=6.81, wps=20986.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.987, loss_scale=16, train_wall=290, gb_free=19.9, wall=48102
2022-03-07 02:07:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:08:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:08:27 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 9.938 | nll_loss 9.026 | ppl 521.23 | wps 41619.6 | wpb 510.9 | bsz 1 | num_updates 15189 | best_loss 7.696
2022-03-07 02:08:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 15189 updates
2022-03-07 02:08:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 02:08:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 02:08:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 78 @ 15189 updates, score 9.938) (writing took 3.2618622118607163 seconds)
2022-03-07 02:08:30 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-07 02:08:30 | INFO | train | epoch 078 | loss 4.096 | nll_loss 2.817 | ppl 7.05 | wps 20591.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15189 | lr 0.000256587 | gnorm 0.996 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 48391
2022-03-07 02:08:30 | INFO | fairseq.trainer | begin training epoch 79
2022-03-07 02:08:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:09:05 | INFO | train_inner | epoch 079:     11 / 196 loss=4.134, nll_loss=2.86, ppl=7.26, wps=20228, ups=0.31, wpb=65367, bsz=127.7, num_updates=15200, lr=0.000256495, gnorm=1.001, loss_scale=16, train_wall=292, gb_free=19.9, wall=48425
2022-03-07 02:10:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:14:20 | INFO | train_inner | epoch 079:    112 / 196 loss=4.045, nll_loss=2.76, ppl=6.77, wps=20784.1, ups=0.32, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.975, loss_scale=8, train_wall=293, gb_free=19.9, wall=48740
2022-03-07 02:18:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:18:47 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 9.928 | nll_loss 9.023 | ppl 520.14 | wps 41583.6 | wpb 510.9 | bsz 1 | num_updates 15384 | best_loss 7.696
2022-03-07 02:18:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 15384 updates
2022-03-07 02:18:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 02:18:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 02:18:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 79 @ 15384 updates, score 9.928) (writing took 3.318115010857582 seconds)
2022-03-07 02:18:50 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-07 02:18:50 | INFO | train | epoch 079 | loss 4.081 | nll_loss 2.8 | ppl 6.97 | wps 20592.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15384 | lr 0.000254956 | gnorm 0.984 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 49010
2022-03-07 02:18:50 | INFO | fairseq.trainer | begin training epoch 80
2022-03-07 02:18:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:19:40 | INFO | train_inner | epoch 080:     16 / 196 loss=4.11, nll_loss=2.833, ppl=7.13, wps=20421.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=0.993, loss_scale=16, train_wall=289, gb_free=19.9, wall=49060
2022-03-07 02:23:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:24:55 | INFO | train_inner | epoch 080:    117 / 196 loss=4.037, nll_loss=2.751, ppl=6.73, wps=20784.8, ups=0.32, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.983, loss_scale=8, train_wall=293, gb_free=19.9, wall=49376
2022-03-07 02:29:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:29:06 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 9.994 | nll_loss 9.096 | ppl 547.1 | wps 41566.5 | wpb 510.9 | bsz 1 | num_updates 15579 | best_loss 7.696
2022-03-07 02:29:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 15579 updates
2022-03-07 02:29:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 02:29:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 02:29:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 80 @ 15579 updates, score 9.994) (writing took 3.2444557670969516 seconds)
2022-03-07 02:29:10 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-07 02:29:10 | INFO | train | epoch 080 | loss 4.067 | nll_loss 2.785 | ppl 6.89 | wps 20593.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15579 | lr 0.000253355 | gnorm 0.987 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 49630
2022-03-07 02:29:10 | INFO | fairseq.trainer | begin training epoch 81
2022-03-07 02:29:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:30:15 | INFO | train_inner | epoch 081:     21 / 196 loss=4.098, nll_loss=2.82, ppl=7.06, wps=20424, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15600, lr=0.000253185, gnorm=0.985, loss_scale=8, train_wall=289, gb_free=19.9, wall=49696
2022-03-07 02:35:28 | INFO | train_inner | epoch 081:    121 / 196 loss=4.031, nll_loss=2.744, ppl=6.7, wps=20986.4, ups=0.32, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.991, loss_scale=16, train_wall=290, gb_free=19.9, wall=50008
2022-03-07 02:37:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:38:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:39:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:39:26 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 10.036 | nll_loss 9.142 | ppl 564.98 | wps 41476.8 | wpb 510.9 | bsz 1 | num_updates 15773 | best_loss 7.696
2022-03-07 02:39:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 15773 updates
2022-03-07 02:39:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 02:39:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 02:39:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 81 @ 15773 updates, score 10.036) (writing took 3.2261365209706128 seconds)
2022-03-07 02:39:30 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-07 02:39:30 | INFO | train | epoch 081 | loss 4.055 | nll_loss 2.771 | ppl 6.82 | wps 20481.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 15773 | lr 0.000251793 | gnorm 0.987 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 50250
2022-03-07 02:39:30 | INFO | fairseq.trainer | begin training epoch 82
2022-03-07 02:39:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:40:54 | INFO | train_inner | epoch 082:     27 / 196 loss=4.068, nll_loss=2.786, ppl=6.9, wps=20028.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=0.988, loss_scale=8, train_wall=295, gb_free=19.9, wall=50334
2022-03-07 02:46:06 | INFO | train_inner | epoch 082:    127 / 196 loss=4.022, nll_loss=2.735, ppl=6.66, wps=20991.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.975, loss_scale=16, train_wall=290, gb_free=19.9, wall=50646
2022-03-07 02:49:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:49:46 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.999 | nll_loss 9.101 | ppl 549.18 | wps 41830.4 | wpb 510.9 | bsz 1 | num_updates 15969 | best_loss 7.696
2022-03-07 02:49:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 15969 updates
2022-03-07 02:49:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 02:49:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 02:49:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 82 @ 15969 updates, score 9.999) (writing took 3.2771846430841833 seconds)
2022-03-07 02:49:49 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-07 02:49:49 | INFO | train | epoch 082 | loss 4.043 | nll_loss 2.758 | ppl 6.76 | wps 20697.2 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 15969 | lr 0.000250243 | gnorm 0.979 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 50870
2022-03-07 02:49:49 | INFO | fairseq.trainer | begin training epoch 83
2022-03-07 02:49:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:50:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:51:29 | INFO | train_inner | epoch 083:     32 / 196 loss=4.053, nll_loss=2.769, ppl=6.82, wps=20224.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=16000, lr=0.00025, gnorm=0.993, loss_scale=8, train_wall=292, gb_free=19.9, wall=50970
2022-03-07 02:56:42 | INFO | train_inner | epoch 083:    132 / 196 loss=4.017, nll_loss=2.728, ppl=6.63, wps=20995, ups=0.32, wpb=65532.4, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.99, loss_scale=8, train_wall=290, gb_free=19.9, wall=51282
2022-03-07 03:00:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:00:06 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 10.077 | nll_loss 9.186 | ppl 582.29 | wps 41483.4 | wpb 510.9 | bsz 1 | num_updates 16164 | best_loss 7.696
2022-03-07 03:00:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 16164 updates
2022-03-07 03:00:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 03:00:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 03:00:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 83 @ 16164 updates, score 10.077) (writing took 3.2661048320587724 seconds)
2022-03-07 03:00:09 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-07 03:00:09 | INFO | train | epoch 083 | loss 4.03 | nll_loss 2.743 | ppl 6.69 | wps 20593.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16164 | lr 0.000248729 | gnorm 0.998 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 51489
2022-03-07 03:00:09 | INFO | fairseq.trainer | begin training epoch 84
2022-03-07 03:00:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:02:02 | INFO | train_inner | epoch 084:     36 / 196 loss=4.031, nll_loss=2.744, ppl=6.7, wps=20415.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=1.002, loss_scale=16, train_wall=290, gb_free=19.9, wall=51602
2022-03-07 03:04:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:05:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:07:20 | INFO | train_inner | epoch 084:    138 / 196 loss=4.017, nll_loss=2.728, ppl=6.63, wps=20584.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.999, loss_scale=8, train_wall=296, gb_free=19.9, wall=51920
2022-03-07 03:10:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:10:26 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 10.085 | nll_loss 9.183 | ppl 581.32 | wps 41392.6 | wpb 510.9 | bsz 1 | num_updates 16358 | best_loss 7.696
2022-03-07 03:10:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 16358 updates
2022-03-07 03:10:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 03:10:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 03:10:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 84 @ 16358 updates, score 10.085) (writing took 3.265813782811165 seconds)
2022-03-07 03:10:29 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-07 03:10:29 | INFO | train | epoch 084 | loss 4.017 | nll_loss 2.729 | ppl 6.63 | wps 20485.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 16358 | lr 0.000247249 | gnorm 0.999 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 52109
2022-03-07 03:10:29 | INFO | fairseq.trainer | begin training epoch 85
2022-03-07 03:10:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:12:40 | INFO | train_inner | epoch 085:     42 / 196 loss=4.017, nll_loss=2.729, ppl=6.63, wps=20425.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=16400, lr=0.000246932, gnorm=0.993, loss_scale=16, train_wall=289, gb_free=19.9, wall=52240
2022-03-07 03:17:52 | INFO | train_inner | epoch 085:    142 / 196 loss=4.003, nll_loss=2.713, ppl=6.56, wps=20994.4, ups=0.32, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.998, loss_scale=16, train_wall=290, gb_free=19.9, wall=52553
2022-03-07 03:19:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:20:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:20:45 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 10.065 | nll_loss 9.158 | ppl 571.38 | wps 41639.1 | wpb 510.9 | bsz 1 | num_updates 16553 | best_loss 7.696
2022-03-07 03:20:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 16553 updates
2022-03-07 03:20:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 03:20:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 03:20:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 85 @ 16553 updates, score 10.065) (writing took 3.193076573079452 seconds)
2022-03-07 03:20:48 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-07 03:20:48 | INFO | train | epoch 085 | loss 4.005 | nll_loss 2.716 | ppl 6.57 | wps 20598.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16553 | lr 0.000245789 | gnorm 0.994 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 52729
2022-03-07 03:20:48 | INFO | fairseq.trainer | begin training epoch 86
2022-03-07 03:20:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:23:15 | INFO | train_inner | epoch 086:     47 / 196 loss=3.996, nll_loss=2.705, ppl=6.52, wps=20230.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=0.994, loss_scale=16, train_wall=292, gb_free=19.9, wall=52876
2022-03-07 03:23:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:28:30 | INFO | train_inner | epoch 086:    148 / 196 loss=3.996, nll_loss=2.705, ppl=6.52, wps=20794.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=16700, lr=0.000244704, gnorm=1.005, loss_scale=8, train_wall=293, gb_free=19.9, wall=53191
2022-03-07 03:31:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:31:05 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 10.101 | nll_loss 9.211 | ppl 592.65 | wps 41519.7 | wpb 510.9 | bsz 1 | num_updates 16748 | best_loss 7.696
2022-03-07 03:31:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 16748 updates
2022-03-07 03:31:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 03:31:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 03:31:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 86 @ 16748 updates, score 10.101) (writing took 3.2831892690155655 seconds)
2022-03-07 03:31:08 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-07 03:31:08 | INFO | train | epoch 086 | loss 3.993 | nll_loss 2.702 | ppl 6.51 | wps 20595.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16748 | lr 0.000244353 | gnorm 1.002 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 53348
2022-03-07 03:31:08 | INFO | fairseq.trainer | begin training epoch 87
2022-03-07 03:31:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:33:51 | INFO | train_inner | epoch 087:     52 / 196 loss=3.986, nll_loss=2.694, ppl=6.47, wps=20419.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=16800, lr=0.000243975, gnorm=0.999, loss_scale=16, train_wall=290, gb_free=19.9, wall=53511
2022-03-07 03:37:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:39:06 | INFO | train_inner | epoch 087:    153 / 196 loss=3.983, nll_loss=2.691, ppl=6.46, wps=20776.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=16900, lr=0.000243252, gnorm=1.017, loss_scale=16, train_wall=293, gb_free=19.9, wall=53826
2022-03-07 03:41:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:41:25 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 10.147 | nll_loss 9.251 | ppl 609.26 | wps 41607.6 | wpb 510.9 | bsz 1 | num_updates 16943 | best_loss 7.696
2022-03-07 03:41:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 16943 updates
2022-03-07 03:41:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 03:41:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 03:41:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 87 @ 16943 updates, score 10.147) (writing took 3.2883861549198627 seconds)
2022-03-07 03:41:28 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-07 03:41:28 | INFO | train | epoch 087 | loss 3.982 | nll_loss 2.69 | ppl 6.45 | wps 20585.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16943 | lr 0.000242943 | gnorm 1.014 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 53968
2022-03-07 03:41:28 | INFO | fairseq.trainer | begin training epoch 88
2022-03-07 03:41:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:44:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:44:29 | INFO | train_inner | epoch 088:     58 / 196 loss=3.973, nll_loss=2.68, ppl=6.41, wps=20221.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=1.003, loss_scale=16, train_wall=292, gb_free=19.9, wall=54150
2022-03-07 03:49:41 | INFO | train_inner | epoch 088:    158 / 196 loss=3.98, nll_loss=2.687, ppl=6.44, wps=20995.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=1.001, loss_scale=16, train_wall=290, gb_free=19.9, wall=54462
2022-03-07 03:51:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:51:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:51:44 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 10.148 | nll_loss 9.247 | ppl 607.81 | wps 41627 | wpb 510.9 | bsz 1 | num_updates 17137 | best_loss 7.696
2022-03-07 03:51:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 17137 updates
2022-03-07 03:51:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 03:51:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 03:51:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 88 @ 17137 updates, score 10.148) (writing took 3.2943381681106985 seconds)
2022-03-07 03:51:48 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-07 03:51:48 | INFO | train | epoch 088 | loss 3.971 | nll_loss 2.678 | ppl 6.4 | wps 20487.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 17137 | lr 0.000241564 | gnorm 0.999 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 54588
2022-03-07 03:51:48 | INFO | fairseq.trainer | begin training epoch 89
2022-03-07 03:51:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:53:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:55:08 | INFO | train_inner | epoch 089:     64 / 196 loss=3.953, nll_loss=2.658, ppl=6.31, wps=20034.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=17200, lr=0.000241121, gnorm=1.005, loss_scale=8, train_wall=295, gb_free=19.9, wall=54788
2022-03-07 04:00:20 | INFO | train_inner | epoch 089:    164 / 196 loss=3.972, nll_loss=2.679, ppl=6.4, wps=20999, ups=0.32, wpb=65532.4, bsz=128, num_updates=17300, lr=0.000240424, gnorm=1.014, loss_scale=16, train_wall=290, gb_free=19.9, wall=55100
2022-03-07 04:01:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:01:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:02:04 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 10.181 | nll_loss 9.283 | ppl 623.02 | wps 41488.4 | wpb 510.9 | bsz 1 | num_updates 17331 | best_loss 7.696
2022-03-07 04:02:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 17331 updates
2022-03-07 04:02:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 04:02:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 04:02:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 89 @ 17331 updates, score 10.181) (writing took 3.3039014430250973 seconds)
2022-03-07 04:02:07 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-07 04:02:07 | INFO | train | epoch 089 | loss 3.961 | nll_loss 2.666 | ppl 6.35 | wps 20491.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 17331 | lr 0.000240208 | gnorm 1.012 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 55208
2022-03-07 04:02:07 | INFO | fairseq.trainer | begin training epoch 90
2022-03-07 04:02:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:05:43 | INFO | train_inner | epoch 090:     69 / 196 loss=3.939, nll_loss=2.642, ppl=6.24, wps=20227.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=1.006, loss_scale=8, train_wall=292, gb_free=19.9, wall=55423
2022-03-07 04:10:55 | INFO | train_inner | epoch 090:    169 / 196 loss=3.975, nll_loss=2.681, ppl=6.41, wps=20997.9, ups=0.32, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=1.013, loss_scale=16, train_wall=290, gb_free=19.9, wall=55735
2022-03-07 04:12:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:12:24 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 10.174 | nll_loss 9.285 | ppl 623.91 | wps 41536.5 | wpb 510.9 | bsz 1 | num_updates 17527 | best_loss 7.696
2022-03-07 04:12:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 17527 updates
2022-03-07 04:12:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 04:12:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 04:12:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 90 @ 17527 updates, score 10.174) (writing took 3.3002029550261796 seconds)
2022-03-07 04:12:27 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-07 04:12:27 | INFO | train | epoch 090 | loss 3.951 | nll_loss 2.655 | ppl 6.3 | wps 20701 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 17527 | lr 0.000238862 | gnorm 1.009 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 55827
2022-03-07 04:12:27 | INFO | fairseq.trainer | begin training epoch 91
2022-03-07 04:12:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:15:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:16:18 | INFO | train_inner | epoch 091:     74 / 196 loss=3.921, nll_loss=2.622, ppl=6.15, wps=20220.1, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=17600, lr=0.000238366, gnorm=1.021, loss_scale=16, train_wall=292, gb_free=19.9, wall=56059
2022-03-07 04:21:30 | INFO | train_inner | epoch 091:    174 / 196 loss=3.962, nll_loss=2.667, ppl=6.35, wps=20988.6, ups=0.32, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=1.018, loss_scale=16, train_wall=290, gb_free=19.9, wall=56371
2022-03-07 04:22:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:22:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:22:44 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 10.216 | nll_loss 9.325 | ppl 641.23 | wps 41469.2 | wpb 510.9 | bsz 1 | num_updates 17721 | best_loss 7.696
2022-03-07 04:22:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 17721 updates
2022-03-07 04:22:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 04:22:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 04:22:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 91 @ 17721 updates, score 10.216) (writing took 3.274097645189613 seconds)
2022-03-07 04:22:47 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-07 04:22:47 | INFO | train | epoch 091 | loss 3.938 | nll_loss 2.641 | ppl 6.24 | wps 20484.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 17721 | lr 0.00023755 | gnorm 1.021 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 56447
2022-03-07 04:22:47 | INFO | fairseq.trainer | begin training epoch 92
2022-03-07 04:22:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:26:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:26:57 | INFO | train_inner | epoch 092:     80 / 196 loss=3.905, nll_loss=2.604, ppl=6.08, wps=20034.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=1.027, loss_scale=8, train_wall=295, gb_free=19.9, wall=56697
2022-03-07 04:32:09 | INFO | train_inner | epoch 092:    180 / 196 loss=3.957, nll_loss=2.662, ppl=6.33, wps=21000.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=1.021, loss_scale=8, train_wall=290, gb_free=19.9, wall=57009
2022-03-07 04:32:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:33:03 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 10.235 | nll_loss 9.342 | ppl 648.85 | wps 41583.4 | wpb 510.9 | bsz 1 | num_updates 17916 | best_loss 7.696
2022-03-07 04:33:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 17916 updates
2022-03-07 04:33:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 04:33:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 04:33:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 92 @ 17916 updates, score 10.235) (writing took 3.2768750928808004 seconds)
2022-03-07 04:33:07 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-07 04:33:07 | INFO | train | epoch 092 | loss 3.93 | nll_loss 2.631 | ppl 6.2 | wps 20596.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 17916 | lr 0.000236254 | gnorm 1.024 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 57067
2022-03-07 04:33:07 | INFO | fairseq.trainer | begin training epoch 93
2022-03-07 04:33:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:37:29 | INFO | train_inner | epoch 093:     84 / 196 loss=3.89, nll_loss=2.587, ppl=6.01, wps=20425.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18000, lr=0.000235702, gnorm=1.016, loss_scale=16, train_wall=290, gb_free=19.9, wall=57329
2022-03-07 04:39:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:42:44 | INFO | train_inner | epoch 093:    185 / 196 loss=3.954, nll_loss=2.659, ppl=6.31, wps=20788.3, ups=0.32, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=1.015, loss_scale=16, train_wall=293, gb_free=19.9, wall=57644
2022-03-07 04:43:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:43:23 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 10.24 | nll_loss 9.348 | ppl 651.68 | wps 41457.1 | wpb 510.9 | bsz 1 | num_updates 18111 | best_loss 7.696
2022-03-07 04:43:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 18111 updates
2022-03-07 04:43:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 04:43:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 04:43:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 93 @ 18111 updates, score 10.24) (writing took 3.2373338129837066 seconds)
2022-03-07 04:43:26 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-07 04:43:26 | INFO | train | epoch 093 | loss 3.919 | nll_loss 2.62 | ppl 6.15 | wps 20595.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18111 | lr 0.000234979 | gnorm 1.014 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 57687
2022-03-07 04:43:26 | INFO | fairseq.trainer | begin training epoch 94
2022-03-07 04:43:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:46:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:48:07 | INFO | train_inner | epoch 094:     90 / 196 loss=3.877, nll_loss=2.573, ppl=5.95, wps=20219, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=1.023, loss_scale=16, train_wall=292, gb_free=19.9, wall=57968
2022-03-07 04:52:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:53:23 | INFO | train_inner | epoch 094:    191 / 196 loss=3.948, nll_loss=2.652, ppl=6.28, wps=20784.2, ups=0.32, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=1.03, loss_scale=8, train_wall=293, gb_free=19.9, wall=58283
2022-03-07 04:53:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:53:43 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 10.271 | nll_loss 9.388 | ppl 669.94 | wps 41782.5 | wpb 510.9 | bsz 1 | num_updates 18305 | best_loss 7.696
2022-03-07 04:53:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 18305 updates
2022-03-07 04:53:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 04:53:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 04:53:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 94 @ 18305 updates, score 10.271) (writing took 3.269659877056256 seconds)
2022-03-07 04:53:46 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-07 04:53:46 | INFO | train | epoch 094 | loss 3.909 | nll_loss 2.608 | ppl 6.1 | wps 20483.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 18305 | lr 0.00023373 | gnorm 1.025 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 58306
2022-03-07 04:53:46 | INFO | fairseq.trainer | begin training epoch 95
2022-03-07 04:53:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:58:43 | INFO | train_inner | epoch 095:     95 / 196 loss=3.868, nll_loss=2.563, ppl=5.91, wps=20426.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=18400, lr=0.000233126, gnorm=1.017, loss_scale=8, train_wall=290, gb_free=19.9, wall=58603
2022-03-07 05:03:55 | INFO | train_inner | epoch 095:    195 / 196 loss=3.935, nll_loss=2.637, ppl=6.22, wps=20993.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=18500, lr=0.000232495, gnorm=1.034, loss_scale=16, train_wall=290, gb_free=19.9, wall=58915
2022-03-07 05:03:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:04:02 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 10.273 | nll_loss 9.388 | ppl 670.06 | wps 41761.8 | wpb 510.9 | bsz 1 | num_updates 18501 | best_loss 7.696
2022-03-07 05:04:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 18501 updates
2022-03-07 05:04:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 05:04:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 05:04:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 95 @ 18501 updates, score 10.273) (writing took 3.171651388052851 seconds)
2022-03-07 05:04:06 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-07 05:04:06 | INFO | train | epoch 095 | loss 3.9 | nll_loss 2.598 | ppl 6.06 | wps 20704.1 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 18501 | lr 0.000232489 | gnorm 1.026 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 58926
2022-03-07 05:04:06 | INFO | fairseq.trainer | begin training epoch 96
2022-03-07 05:04:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:06:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:07:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 05:09:21 | INFO | train_inner | epoch 096:    101 / 196 loss=3.855, nll_loss=2.548, ppl=5.85, wps=20048.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=18600, lr=0.000231869, gnorm=1.034, loss_scale=8, train_wall=295, gb_free=19.9, wall=59241
2022-03-07 05:14:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:14:22 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 10.304 | nll_loss 9.421 | ppl 685.54 | wps 41501.6 | wpb 510.9 | bsz 1 | num_updates 18695 | best_loss 7.696
2022-03-07 05:14:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 18695 updates
2022-03-07 05:14:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 05:14:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 05:14:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 96 @ 18695 updates, score 10.304) (writing took 3.1638280188199133 seconds)
2022-03-07 05:14:25 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-07 05:14:25 | INFO | train | epoch 096 | loss 3.89 | nll_loss 2.587 | ppl 6.01 | wps 20503.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 18695 | lr 0.00023128 | gnorm 1.036 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 59545
2022-03-07 05:14:25 | INFO | fairseq.trainer | begin training epoch 97
2022-03-07 05:14:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:14:41 | INFO | train_inner | epoch 097:      5 / 196 loss=3.922, nll_loss=2.623, ppl=6.16, wps=20445.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=1.04, loss_scale=16, train_wall=289, gb_free=19.9, wall=59561
2022-03-07 05:19:53 | INFO | train_inner | epoch 097:    105 / 196 loss=3.847, nll_loss=2.539, ppl=5.81, wps=20995.1, ups=0.32, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=1.023, loss_scale=16, train_wall=290, gb_free=19.9, wall=59873
2022-03-07 05:20:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:24:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:24:41 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 10.321 | nll_loss 9.433 | ppl 691.36 | wps 41508.4 | wpb 510.9 | bsz 1 | num_updates 18890 | best_loss 7.696
2022-03-07 05:24:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 18890 updates
2022-03-07 05:24:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 05:24:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 05:24:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 97 @ 18890 updates, score 10.321) (writing took 3.152145966887474 seconds)
2022-03-07 05:24:44 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-07 05:24:44 | INFO | train | epoch 097 | loss 3.881 | nll_loss 2.577 | ppl 5.97 | wps 20600.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18890 | lr 0.000230083 | gnorm 1.036 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 60165
2022-03-07 05:24:44 | INFO | fairseq.trainer | begin training epoch 98
2022-03-07 05:24:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:25:16 | INFO | train_inner | epoch 098:     10 / 196 loss=3.911, nll_loss=2.61, ppl=6.11, wps=20239.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18900, lr=0.000230022, gnorm=1.049, loss_scale=16, train_wall=292, gb_free=19.9, wall=60196
2022-03-07 05:27:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:30:31 | INFO | train_inner | epoch 098:    111 / 196 loss=3.837, nll_loss=2.528, ppl=5.77, wps=20793.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=19000, lr=0.000229416, gnorm=1.022, loss_scale=16, train_wall=293, gb_free=19.9, wall=60511
2022-03-07 05:34:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:34:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:35:01 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 10.363 | nll_loss 9.474 | ppl 711.1 | wps 41614.1 | wpb 510.9 | bsz 1 | num_updates 19084 | best_loss 7.696
2022-03-07 05:35:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 19084 updates
2022-03-07 05:35:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 05:35:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 05:35:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 98 @ 19084 updates, score 10.363) (writing took 3.1746962529141456 seconds)
2022-03-07 05:35:04 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-07 05:35:04 | INFO | train | epoch 098 | loss 3.872 | nll_loss 2.567 | ppl 5.93 | wps 20499.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19084 | lr 0.00022891 | gnorm 1.031 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 60784
2022-03-07 05:35:04 | INFO | fairseq.trainer | begin training epoch 99
2022-03-07 05:35:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:35:54 | INFO | train_inner | epoch 099:     16 / 196 loss=3.905, nll_loss=2.603, ppl=6.08, wps=20239.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=1.033, loss_scale=16, train_wall=292, gb_free=19.9, wall=60834
2022-03-07 05:41:06 | INFO | train_inner | epoch 099:    116 / 196 loss=3.833, nll_loss=2.524, ppl=5.75, wps=20987.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=19200, lr=0.000228218, gnorm=1.037, loss_scale=16, train_wall=290, gb_free=19.9, wall=61146
2022-03-07 05:41:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:41:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 05:45:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:45:20 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 10.386 | nll_loss 9.512 | ppl 729.95 | wps 41477.8 | wpb 510.9 | bsz 1 | num_updates 19278 | best_loss 7.696
2022-03-07 05:45:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 19278 updates
2022-03-07 05:45:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 05:45:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 05:45:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 99 @ 19278 updates, score 10.386) (writing took 3.2004075369331986 seconds)
2022-03-07 05:45:24 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-07 05:45:24 | INFO | train | epoch 099 | loss 3.863 | nll_loss 2.557 | ppl 5.89 | wps 20487.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19278 | lr 0.000227756 | gnorm 1.037 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 61404
2022-03-07 05:45:24 | INFO | fairseq.trainer | begin training epoch 100
2022-03-07 05:45:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:46:32 | INFO | train_inner | epoch 100:     22 / 196 loss=3.887, nll_loss=2.584, ppl=6, wps=20038.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=19300, lr=0.000227626, gnorm=1.045, loss_scale=8, train_wall=295, gb_free=19.9, wall=61473
2022-03-07 05:51:44 | INFO | train_inner | epoch 100:    122 / 196 loss=3.834, nll_loss=2.524, ppl=5.75, wps=20996.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=19400, lr=0.000227038, gnorm=1.033, loss_scale=16, train_wall=290, gb_free=19.9, wall=61785
2022-03-07 05:55:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:55:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:55:40 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 10.383 | nll_loss 9.502 | ppl 725.23 | wps 41397.9 | wpb 510.9 | bsz 1 | num_updates 19473 | best_loss 7.696
2022-03-07 05:55:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 19473 updates
2022-03-07 05:55:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 05:55:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 05:55:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 100 @ 19473 updates, score 10.383) (writing took 3.167273518163711 seconds)
2022-03-07 05:55:43 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-07 05:55:43 | INFO | train | epoch 100 | loss 3.854 | nll_loss 2.547 | ppl 5.85 | wps 20598.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19473 | lr 0.000226612 | gnorm 1.034 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 62023
2022-03-07 05:55:43 | INFO | fairseq.trainer | begin training epoch 101
2022-03-07 05:55:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:57:08 | INFO | train_inner | epoch 101:     27 / 196 loss=3.869, nll_loss=2.564, ppl=5.91, wps=20229.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=1.034, loss_scale=16, train_wall=293, gb_free=19.9, wall=62108
2022-03-07 06:02:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:02:23 | INFO | train_inner | epoch 101:    128 / 196 loss=3.83, nll_loss=2.52, ppl=5.74, wps=20784.5, ups=0.32, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=1.03, loss_scale=16, train_wall=293, gb_free=19.9, wall=62423
2022-03-07 06:05:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:06:00 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 10.344 | nll_loss 9.454 | ppl 701.37 | wps 41636.4 | wpb 510.9 | bsz 1 | num_updates 19668 | best_loss 7.696
2022-03-07 06:06:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 19668 updates
2022-03-07 06:06:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 06:06:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 06:06:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 101 @ 19668 updates, score 10.344) (writing took 3.2768031950108707 seconds)
2022-03-07 06:06:03 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-07 06:06:03 | INFO | train | epoch 101 | loss 3.846 | nll_loss 2.538 | ppl 5.81 | wps 20588.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19668 | lr 0.000225486 | gnorm 1.042 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 62643
2022-03-07 06:06:03 | INFO | fairseq.trainer | begin training epoch 102
2022-03-07 06:06:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:07:43 | INFO | train_inner | epoch 102:     32 / 196 loss=3.851, nll_loss=2.544, ppl=5.83, wps=20417.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=19700, lr=0.000225303, gnorm=1.053, loss_scale=16, train_wall=290, gb_free=19.9, wall=62743
2022-03-07 06:07:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 06:12:58 | INFO | train_inner | epoch 102:    133 / 196 loss=3.826, nll_loss=2.516, ppl=5.72, wps=20796.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=19800, lr=0.000224733, gnorm=1.047, loss_scale=8, train_wall=293, gb_free=19.9, wall=63058
2022-03-07 06:16:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:16:19 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 10.378 | nll_loss 9.49 | ppl 719.24 | wps 41521.3 | wpb 510.9 | bsz 1 | num_updates 19863 | best_loss 7.696
2022-03-07 06:16:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 19863 updates
2022-03-07 06:16:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 06:16:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 06:16:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 102 @ 19863 updates, score 10.378) (writing took 3.08222265006043 seconds)
2022-03-07 06:16:22 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-07 06:16:22 | INFO | train | epoch 102 | loss 3.837 | nll_loss 2.529 | ppl 5.77 | wps 20607 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19863 | lr 0.000224377 | gnorm 1.039 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 63263
2022-03-07 06:16:22 | INFO | fairseq.trainer | begin training epoch 103
2022-03-07 06:16:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:18:18 | INFO | train_inner | epoch 103:     37 / 196 loss=3.849, nll_loss=2.542, ppl=5.82, wps=20452.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=1.031, loss_scale=16, train_wall=289, gb_free=19.9, wall=63378
2022-03-07 06:21:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:23:32 | INFO | train_inner | epoch 103:    138 / 196 loss=3.823, nll_loss=2.512, ppl=5.7, wps=20819.9, ups=0.32, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=1.061, loss_scale=16, train_wall=293, gb_free=19.9, wall=63693
2022-03-07 06:26:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:26:38 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 10.419 | nll_loss 9.533 | ppl 740.91 | wps 41766 | wpb 510.9 | bsz 1 | num_updates 20058 | best_loss 7.696
2022-03-07 06:26:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 20058 updates
2022-03-07 06:26:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 06:26:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 06:26:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 103 @ 20058 updates, score 10.419) (writing took 3.208629031898454 seconds)
2022-03-07 06:26:41 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-07 06:26:41 | INFO | train | epoch 103 | loss 3.829 | nll_loss 2.519 | ppl 5.73 | wps 20631.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 20058 | lr 0.000223283 | gnorm 1.053 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 63881
2022-03-07 06:26:41 | INFO | fairseq.trainer | begin training epoch 104
2022-03-07 06:26:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:28:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:28:55 | INFO | train_inner | epoch 104:     43 / 196 loss=3.824, nll_loss=2.513, ppl=5.71, wps=20266.4, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=20100, lr=0.00022305, gnorm=1.04, loss_scale=16, train_wall=292, gb_free=19.9, wall=64015
2022-03-07 06:34:07 | INFO | train_inner | epoch 104:    143 / 196 loss=3.815, nll_loss=2.504, ppl=5.67, wps=21021.6, ups=0.32, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=1.065, loss_scale=16, train_wall=290, gb_free=19.9, wall=64327
2022-03-07 06:35:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:36:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:36:56 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 10.405 | nll_loss 9.522 | ppl 735.05 | wps 41656.3 | wpb 510.9 | bsz 1 | num_updates 20252 | best_loss 7.696
2022-03-07 06:36:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 20252 updates
2022-03-07 06:36:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 06:37:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 06:37:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 104 @ 20252 updates, score 10.405) (writing took 3.149140069959685 seconds)
2022-03-07 06:37:00 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-07 06:37:00 | INFO | train | epoch 104 | loss 3.82 | nll_loss 2.509 | ppl 5.69 | wps 20517.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 20252 | lr 0.000222211 | gnorm 1.053 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 64500
2022-03-07 06:37:00 | INFO | fairseq.trainer | begin training epoch 105
2022-03-07 06:37:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:38:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 06:39:33 | INFO | train_inner | epoch 105:     49 / 196 loss=3.817, nll_loss=2.506, ppl=5.68, wps=20066.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=1.043, loss_scale=8, train_wall=295, gb_free=19.9, wall=64653
2022-03-07 06:44:44 | INFO | train_inner | epoch 105:    149 / 196 loss=3.822, nll_loss=2.511, ppl=5.7, wps=21037.3, ups=0.32, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=1.067, loss_scale=8, train_wall=290, gb_free=19.9, wall=64964
2022-03-07 06:47:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:47:15 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 10.394 | nll_loss 9.507 | ppl 727.58 | wps 41481.2 | wpb 510.9 | bsz 1 | num_updates 20447 | best_loss 7.696
2022-03-07 06:47:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 20447 updates
2022-03-07 06:47:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 06:47:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 06:47:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 105 @ 20447 updates, score 10.394) (writing took 3.133463303092867 seconds)
2022-03-07 06:47:18 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-07 06:47:18 | INFO | train | epoch 105 | loss 3.813 | nll_loss 2.501 | ppl 5.66 | wps 20633.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 20447 | lr 0.000221149 | gnorm 1.058 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 65119
2022-03-07 06:47:18 | INFO | fairseq.trainer | begin training epoch 106
2022-03-07 06:47:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:50:04 | INFO | train_inner | epoch 106:     53 / 196 loss=3.8, nll_loss=2.487, ppl=5.6, wps=20457.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=20500, lr=0.000220863, gnorm=1.057, loss_scale=16, train_wall=289, gb_free=19.9, wall=65284
2022-03-07 06:52:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:55:18 | INFO | train_inner | epoch 106:    154 / 196 loss=3.812, nll_loss=2.5, ppl=5.66, wps=20814.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=20600, lr=0.000220326, gnorm=1.062, loss_scale=16, train_wall=293, gb_free=19.9, wall=65599
2022-03-07 06:57:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:57:34 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 10.47 | nll_loss 9.591 | ppl 771.27 | wps 41611 | wpb 510.9 | bsz 1 | num_updates 20642 | best_loss 7.696
2022-03-07 06:57:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 20642 updates
2022-03-07 06:57:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 06:57:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 06:57:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 106 @ 20642 updates, score 10.47) (writing took 3.1349164831917733 seconds)
2022-03-07 06:57:37 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-07 06:57:37 | INFO | train | epoch 106 | loss 3.805 | nll_loss 2.492 | ppl 5.63 | wps 20623.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 20642 | lr 0.000220102 | gnorm 1.058 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 65737
2022-03-07 06:57:37 | INFO | fairseq.trainer | begin training epoch 107
2022-03-07 06:57:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:59:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:00:41 | INFO | train_inner | epoch 107:     59 / 196 loss=3.793, nll_loss=2.479, ppl=5.58, wps=20261.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=1.057, loss_scale=16, train_wall=292, gb_free=19.9, wall=65921
2022-03-07 07:03:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 07:05:56 | INFO | train_inner | epoch 107:    160 / 196 loss=3.807, nll_loss=2.494, ppl=5.63, wps=20814.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=20800, lr=0.000219265, gnorm=1.044, loss_scale=8, train_wall=293, gb_free=19.9, wall=66236
2022-03-07 07:07:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:07:53 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 10.488 | nll_loss 9.611 | ppl 782.13 | wps 41590.8 | wpb 510.9 | bsz 1 | num_updates 20836 | best_loss 7.696
2022-03-07 07:07:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 20836 updates
2022-03-07 07:07:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 07:07:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 07:07:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 107 @ 20836 updates, score 10.488) (writing took 3.163822582922876 seconds)
2022-03-07 07:07:56 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-07 07:07:56 | INFO | train | epoch 107 | loss 3.797 | nll_loss 2.484 | ppl 5.59 | wps 20521.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 20836 | lr 0.000219075 | gnorm 1.048 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 66356
2022-03-07 07:07:56 | INFO | fairseq.trainer | begin training epoch 108
2022-03-07 07:07:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:11:15 | INFO | train_inner | epoch 108:     64 / 196 loss=3.778, nll_loss=2.463, ppl=5.51, wps=20453.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=20900, lr=0.000218739, gnorm=1.041, loss_scale=16, train_wall=289, gb_free=19.9, wall=66556
2022-03-07 07:16:27 | INFO | train_inner | epoch 108:    164 / 196 loss=3.811, nll_loss=2.499, ppl=5.65, wps=21017.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=1.062, loss_scale=16, train_wall=290, gb_free=19.9, wall=66868
2022-03-07 07:16:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:18:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:18:11 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 10.446 | nll_loss 9.566 | ppl 757.83 | wps 41528 | wpb 510.9 | bsz 1 | num_updates 21031 | best_loss 7.696
2022-03-07 07:18:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 21031 updates
2022-03-07 07:18:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 07:18:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 07:18:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 108 @ 21031 updates, score 10.446) (writing took 3.1356009549926966 seconds)
2022-03-07 07:18:15 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-07 07:18:15 | INFO | train | epoch 108 | loss 3.791 | nll_loss 2.477 | ppl 5.57 | wps 20619.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 21031 | lr 0.000218057 | gnorm 1.058 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 66975
2022-03-07 07:18:15 | INFO | fairseq.trainer | begin training epoch 109
2022-03-07 07:18:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:21:50 | INFO | train_inner | epoch 109:     69 / 196 loss=3.762, nll_loss=2.444, ppl=5.44, wps=20256.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=1.057, loss_scale=16, train_wall=292, gb_free=19.9, wall=67190
2022-03-07 07:23:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:27:05 | INFO | train_inner | epoch 109:    170 / 196 loss=3.808, nll_loss=2.496, ppl=5.64, wps=20818.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=21200, lr=0.000217186, gnorm=1.067, loss_scale=16, train_wall=293, gb_free=19.9, wall=67505
2022-03-07 07:28:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:28:30 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 10.493 | nll_loss 9.615 | ppl 784.14 | wps 41613.8 | wpb 510.9 | bsz 1 | num_updates 21226 | best_loss 7.696
2022-03-07 07:28:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 21226 updates
2022-03-07 07:28:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 07:28:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 07:28:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 109 @ 21226 updates, score 10.493) (writing took 3.1335400850512087 seconds)
2022-03-07 07:28:33 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-07 07:28:33 | INFO | train | epoch 109 | loss 3.784 | nll_loss 2.469 | ppl 5.54 | wps 20624.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 21226 | lr 0.000217053 | gnorm 1.064 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 67594
2022-03-07 07:28:33 | INFO | fairseq.trainer | begin training epoch 110
2022-03-07 07:28:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:30:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:32:27 | INFO | train_inner | epoch 110:     75 / 196 loss=3.757, nll_loss=2.439, ppl=5.42, wps=20255.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21300, lr=0.000216676, gnorm=1.057, loss_scale=16, train_wall=292, gb_free=19.9, wall=67828
2022-03-07 07:37:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:37:42 | INFO | train_inner | epoch 110:    176 / 196 loss=3.801, nll_loss=2.488, ppl=5.61, wps=20815.8, ups=0.32, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=1.069, loss_scale=16, train_wall=293, gb_free=19.9, wall=68143
2022-03-07 07:38:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:38:49 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 10.515 | nll_loss 9.636 | ppl 795.46 | wps 41540.5 | wpb 510.9 | bsz 1 | num_updates 21420 | best_loss 7.696
2022-03-07 07:38:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 21420 updates
2022-03-07 07:38:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 07:38:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 07:38:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 110 @ 21420 updates, score 10.515) (writing took 3.1332439749967307 seconds)
2022-03-07 07:38:52 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-07 07:38:52 | INFO | train | epoch 110 | loss 3.775 | nll_loss 2.459 | ppl 5.5 | wps 20518 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 21420 | lr 0.000216068 | gnorm 1.057 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 68213
2022-03-07 07:38:52 | INFO | fairseq.trainer | begin training epoch 111
2022-03-07 07:38:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:43:02 | INFO | train_inner | epoch 111:     80 / 196 loss=3.741, nll_loss=2.421, ppl=5.36, wps=20449.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=1.061, loss_scale=16, train_wall=289, gb_free=19.9, wall=68462
2022-03-07 07:44:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:48:17 | INFO | train_inner | epoch 111:    181 / 196 loss=3.8, nll_loss=2.487, ppl=5.61, wps=20817.1, ups=0.32, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=1.075, loss_scale=16, train_wall=293, gb_free=19.9, wall=68777
2022-03-07 07:49:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:49:08 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 10.515 | nll_loss 9.633 | ppl 794.14 | wps 41631.7 | wpb 510.9 | bsz 1 | num_updates 21615 | best_loss 7.696
2022-03-07 07:49:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 21615 updates
2022-03-07 07:49:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 07:49:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 07:49:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 111 @ 21615 updates, score 10.515) (writing took 3.1640640450641513 seconds)
2022-03-07 07:49:11 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-07 07:49:11 | INFO | train | epoch 111 | loss 3.769 | nll_loss 2.452 | ppl 5.47 | wps 20621.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 21615 | lr 0.000215091 | gnorm 1.069 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 68831
2022-03-07 07:49:11 | INFO | fairseq.trainer | begin training epoch 112
2022-03-07 07:49:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:50:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 07:53:39 | INFO | train_inner | epoch 112:     86 / 196 loss=3.734, nll_loss=2.413, ppl=5.33, wps=20264.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21700, lr=0.000214669, gnorm=1.052, loss_scale=8, train_wall=292, gb_free=19.9, wall=69100
2022-03-07 07:58:51 | INFO | train_inner | epoch 112:    186 / 196 loss=3.791, nll_loss=2.477, ppl=5.57, wps=21022, ups=0.32, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=1.069, loss_scale=16, train_wall=290, gb_free=19.9, wall=69411
2022-03-07 07:59:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:59:27 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 10.529 | nll_loss 9.655 | ppl 806.14 | wps 41444.1 | wpb 510.9 | bsz 1 | num_updates 21810 | best_loss 7.696
2022-03-07 07:59:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 21810 updates
2022-03-07 07:59:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 07:59:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 07:59:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 112 @ 21810 updates, score 10.529) (writing took 3.110874217003584 seconds)
2022-03-07 07:59:30 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-07 07:59:30 | INFO | train | epoch 112 | loss 3.761 | nll_loss 2.443 | ppl 5.44 | wps 20627.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 21810 | lr 0.000214127 | gnorm 1.061 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 69450
2022-03-07 07:59:30 | INFO | fairseq.trainer | begin training epoch 113
2022-03-07 07:59:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:01:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 08:04:14 | INFO | train_inner | epoch 113:     91 / 196 loss=3.723, nll_loss=2.401, ppl=5.28, wps=20258.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=21900, lr=0.000213687, gnorm=1.067, loss_scale=8, train_wall=292, gb_free=19.9, wall=69734
2022-03-07 08:09:25 | INFO | train_inner | epoch 113:    191 / 196 loss=3.792, nll_loss=2.478, ppl=5.57, wps=21018, ups=0.32, wpb=65532.4, bsz=128, num_updates=22000, lr=0.000213201, gnorm=1.065, loss_scale=16, train_wall=290, gb_free=19.9, wall=70046
2022-03-07 08:09:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:09:46 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 10.539 | nll_loss 9.667 | ppl 812.83 | wps 41847.2 | wpb 510.9 | bsz 1 | num_updates 22005 | best_loss 7.696
2022-03-07 08:09:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 22005 updates
2022-03-07 08:09:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 08:09:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 08:09:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 113 @ 22005 updates, score 10.539) (writing took 3.1053330809809268 seconds)
2022-03-07 08:09:49 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-07 08:09:49 | INFO | train | epoch 113 | loss 3.755 | nll_loss 2.436 | ppl 5.41 | wps 20623.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22005 | lr 0.000213176 | gnorm 1.067 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 70069
2022-03-07 08:09:49 | INFO | fairseq.trainer | begin training epoch 114
2022-03-07 08:09:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:14:45 | INFO | train_inner | epoch 114:     95 / 196 loss=3.705, nll_loss=2.38, ppl=5.21, wps=20458.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=22100, lr=0.000212718, gnorm=1.068, loss_scale=16, train_wall=289, gb_free=19.9, wall=70365
2022-03-07 08:14:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:19:59 | INFO | train_inner | epoch 114:    196 / 196 loss=3.793, nll_loss=2.479, ppl=5.57, wps=20814.9, ups=0.32, wpb=65363.4, bsz=127.7, num_updates=22200, lr=0.000212238, gnorm=1.082, loss_scale=16, train_wall=292, gb_free=19.9, wall=70679
2022-03-07 08:19:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:20:04 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 10.568 | nll_loss 9.698 | ppl 830.49 | wps 41425.4 | wpb 510.9 | bsz 1 | num_updates 22200 | best_loss 7.696
2022-03-07 08:20:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 22200 updates
2022-03-07 08:20:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 08:20:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 08:20:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 114 @ 22200 updates, score 10.568) (writing took 3.117804049048573 seconds)
2022-03-07 08:20:07 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-07 08:20:07 | INFO | train | epoch 114 | loss 3.747 | nll_loss 2.428 | ppl 5.38 | wps 20624.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22200 | lr 0.000212238 | gnorm 1.075 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 70688
2022-03-07 08:20:07 | INFO | fairseq.trainer | begin training epoch 115
2022-03-07 08:20:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:21:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:25:23 | INFO | train_inner | epoch 115:    101 / 196 loss=3.707, nll_loss=2.384, ppl=5.22, wps=20248.7, ups=0.31, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=1.061, loss_scale=16, train_wall=293, gb_free=19.9, wall=71003
2022-03-07 08:28:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:30:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:30:23 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 10.615 | nll_loss 9.746 | ppl 858.55 | wps 41515.5 | wpb 510.9 | bsz 1 | num_updates 22394 | best_loss 7.696
2022-03-07 08:30:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 22394 updates
2022-03-07 08:30:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 08:30:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 08:30:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 115 @ 22394 updates, score 10.615) (writing took 3.100980825955048 seconds)
2022-03-07 08:30:27 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-07 08:30:27 | INFO | train | epoch 115 | loss 3.74 | nll_loss 2.421 | ppl 5.35 | wps 20507.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22394 | lr 0.000211317 | gnorm 1.069 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 71307
2022-03-07 08:30:27 | INFO | fairseq.trainer | begin training epoch 116
2022-03-07 08:30:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:30:45 | INFO | train_inner | epoch 116:      6 / 196 loss=3.767, nll_loss=2.45, ppl=5.47, wps=20252.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22400, lr=0.000211289, gnorm=1.078, loss_scale=16, train_wall=292, gb_free=19.9, wall=71326
2022-03-07 08:35:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:36:00 | INFO | train_inner | epoch 116:    107 / 196 loss=3.699, nll_loss=2.374, ppl=5.18, wps=20812, ups=0.32, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=1.068, loss_scale=16, train_wall=293, gb_free=19.9, wall=71641
2022-03-07 08:40:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:40:42 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 10.567 | nll_loss 9.694 | ppl 828.05 | wps 41570 | wpb 510.9 | bsz 1 | num_updates 22589 | best_loss 7.696
2022-03-07 08:40:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 22589 updates
2022-03-07 08:40:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 08:40:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 08:40:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 116 @ 22589 updates, score 10.567) (writing took 3.291342998156324 seconds)
2022-03-07 08:40:46 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-07 08:40:46 | INFO | train | epoch 116 | loss 3.734 | nll_loss 2.413 | ppl 5.33 | wps 20617.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22589 | lr 0.000210403 | gnorm 1.07 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 71926
2022-03-07 08:40:46 | INFO | fairseq.trainer | begin training epoch 117
2022-03-07 08:40:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:41:20 | INFO | train_inner | epoch 117:     11 / 196 loss=3.764, nll_loss=2.447, ppl=5.45, wps=20444.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22600, lr=0.000210352, gnorm=1.069, loss_scale=16, train_wall=289, gb_free=19.9, wall=71960
2022-03-07 08:41:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 08:46:35 | INFO | train_inner | epoch 117:    112 / 196 loss=3.703, nll_loss=2.378, ppl=5.2, wps=20816.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=22700, lr=0.000209888, gnorm=1.07, loss_scale=8, train_wall=293, gb_free=19.9, wall=72275
2022-03-07 08:50:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:51:01 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 10.555 | nll_loss 9.683 | ppl 821.96 | wps 41703 | wpb 510.9 | bsz 1 | num_updates 22784 | best_loss 7.696
2022-03-07 08:51:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 22784 updates
2022-03-07 08:51:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 08:51:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 08:51:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 117 @ 22784 updates, score 10.555) (writing took 3.118734868010506 seconds)
2022-03-07 08:51:04 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-07 08:51:04 | INFO | train | epoch 117 | loss 3.728 | nll_loss 2.406 | ppl 5.3 | wps 20627.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22784 | lr 0.0002095 | gnorm 1.069 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 72545
2022-03-07 08:51:04 | INFO | fairseq.trainer | begin training epoch 118
2022-03-07 08:51:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:51:54 | INFO | train_inner | epoch 118:     16 / 196 loss=3.748, nll_loss=2.429, ppl=5.39, wps=20460.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=22800, lr=0.000209427, gnorm=1.065, loss_scale=16, train_wall=289, gb_free=19.9, wall=72595
2022-03-07 08:55:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:57:09 | INFO | train_inner | epoch 118:    117 / 196 loss=3.695, nll_loss=2.37, ppl=5.17, wps=20806.3, ups=0.32, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=1.065, loss_scale=16, train_wall=293, gb_free=19.9, wall=72910
2022-03-07 09:01:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:01:20 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 10.535 | nll_loss 9.656 | ppl 806.71 | wps 41763.1 | wpb 510.9 | bsz 1 | num_updates 22979 | best_loss 7.696
2022-03-07 09:01:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 22979 updates
2022-03-07 09:01:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 09:01:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 09:01:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 118 @ 22979 updates, score 10.535) (writing took 3.116420231992379 seconds)
2022-03-07 09:01:23 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-07 09:01:23 | INFO | train | epoch 118 | loss 3.721 | nll_loss 2.399 | ppl 5.27 | wps 20621.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 22979 | lr 0.00020861 | gnorm 1.074 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 73163
2022-03-07 09:01:23 | INFO | fairseq.trainer | begin training epoch 119
2022-03-07 09:01:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:02:29 | INFO | train_inner | epoch 119:     21 / 196 loss=3.743, nll_loss=2.424, ppl=5.37, wps=20460.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23000, lr=0.000208514, gnorm=1.076, loss_scale=32, train_wall=289, gb_free=19.9, wall=73229
2022-03-07 09:02:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:07:44 | INFO | train_inner | epoch 119:    122 / 196 loss=3.696, nll_loss=2.371, ppl=5.17, wps=20804, ups=0.32, wpb=65532.4, bsz=128, num_updates=23100, lr=0.000208063, gnorm=1.077, loss_scale=16, train_wall=293, gb_free=19.9, wall=73544
2022-03-07 09:09:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:11:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:11:39 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 10.614 | nll_loss 9.738 | ppl 854.1 | wps 41479.5 | wpb 510.9 | bsz 1 | num_updates 23173 | best_loss 7.696
2022-03-07 09:11:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 23173 updates
2022-03-07 09:11:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 09:11:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 09:11:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 119 @ 23173 updates, score 10.614) (writing took 3.0809717350639403 seconds)
2022-03-07 09:11:42 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-07 09:11:42 | INFO | train | epoch 119 | loss 3.714 | nll_loss 2.392 | ppl 5.25 | wps 20513.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23173 | lr 0.000207735 | gnorm 1.078 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 73782
2022-03-07 09:11:42 | INFO | fairseq.trainer | begin training epoch 120
2022-03-07 09:11:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:13:06 | INFO | train_inner | epoch 120:     27 / 196 loss=3.728, nll_loss=2.407, ppl=5.3, wps=20257.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23200, lr=0.000207614, gnorm=1.087, loss_scale=16, train_wall=292, gb_free=19.9, wall=73867
2022-03-07 09:16:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:18:21 | INFO | train_inner | epoch 120:    128 / 196 loss=3.694, nll_loss=2.369, ppl=5.17, wps=20811.1, ups=0.32, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=1.083, loss_scale=16, train_wall=293, gb_free=19.9, wall=74182
2022-03-07 09:21:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:21:58 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 10.615 | nll_loss 9.744 | ppl 857.8 | wps 41456 | wpb 510.9 | bsz 1 | num_updates 23368 | best_loss 7.696
2022-03-07 09:21:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 23368 updates
2022-03-07 09:21:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 09:22:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 09:22:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 120 @ 23368 updates, score 10.615) (writing took 3.130557021824643 seconds)
2022-03-07 09:22:01 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-07 09:22:01 | INFO | train | epoch 120 | loss 3.709 | nll_loss 2.385 | ppl 5.22 | wps 20620.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 23368 | lr 0.000206866 | gnorm 1.087 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 74401
2022-03-07 09:22:01 | INFO | fairseq.trainer | begin training epoch 121
2022-03-07 09:22:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:23:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:23:44 | INFO | train_inner | epoch 121:     33 / 196 loss=3.719, nll_loss=2.397, ppl=5.27, wps=20255.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=23400, lr=0.000206725, gnorm=1.09, loss_scale=16, train_wall=292, gb_free=19.9, wall=74504
2022-03-07 09:28:56 | INFO | train_inner | epoch 121:    133 / 196 loss=3.69, nll_loss=2.364, ppl=5.15, wps=21011.7, ups=0.32, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=1.082, loss_scale=16, train_wall=290, gb_free=19.9, wall=74816
2022-03-07 09:29:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 09:32:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:32:17 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 10.591 | nll_loss 9.718 | ppl 842.1 | wps 41776.6 | wpb 510.9 | bsz 1 | num_updates 23562 | best_loss 7.696
2022-03-07 09:32:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 23562 updates
2022-03-07 09:32:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 09:32:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 09:32:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 121 @ 23562 updates, score 10.591) (writing took 3.071243294980377 seconds)
2022-03-07 09:32:20 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-07 09:32:20 | INFO | train | epoch 121 | loss 3.702 | nll_loss 2.378 | ppl 5.2 | wps 20510.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23562 | lr 0.000206013 | gnorm 1.089 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 75020
2022-03-07 09:32:20 | INFO | fairseq.trainer | begin training epoch 122
2022-03-07 09:32:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:34:19 | INFO | train_inner | epoch 122:     38 / 196 loss=3.708, nll_loss=2.385, ppl=5.22, wps=20258.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23600, lr=0.000205847, gnorm=1.096, loss_scale=8, train_wall=292, gb_free=19.9, wall=75139
2022-03-07 09:39:30 | INFO | train_inner | epoch 122:    138 / 196 loss=3.691, nll_loss=2.366, ppl=5.15, wps=21025.9, ups=0.32, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=1.076, loss_scale=16, train_wall=290, gb_free=19.9, wall=75451
2022-03-07 09:42:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:42:36 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 10.62 | nll_loss 9.75 | ppl 860.95 | wps 41553.8 | wpb 510.9 | bsz 1 | num_updates 23758 | best_loss 7.696
2022-03-07 09:42:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 23758 updates
2022-03-07 09:42:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 09:42:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 09:42:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 122 @ 23758 updates, score 10.62) (writing took 3.084730237023905 seconds)
2022-03-07 09:42:39 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-07 09:42:39 | INFO | train | epoch 122 | loss 3.697 | nll_loss 2.372 | ppl 5.18 | wps 20735.2 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 23758 | lr 0.000205161 | gnorm 1.081 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 75639
2022-03-07 09:42:39 | INFO | fairseq.trainer | begin training epoch 123
2022-03-07 09:42:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:43:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:44:53 | INFO | train_inner | epoch 123:     43 / 196 loss=3.706, nll_loss=2.382, ppl=5.21, wps=20261.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23800, lr=0.00020498, gnorm=1.085, loss_scale=16, train_wall=292, gb_free=19.9, wall=75773
2022-03-07 09:49:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:50:08 | INFO | train_inner | epoch 123:    144 / 196 loss=3.684, nll_loss=2.358, ppl=5.13, wps=20807.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=23900, lr=0.000204551, gnorm=1.087, loss_scale=16, train_wall=293, gb_free=19.9, wall=76088
2022-03-07 09:52:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:52:54 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 10.644 | nll_loss 9.779 | ppl 878.27 | wps 41558.9 | wpb 510.9 | bsz 1 | num_updates 23952 | best_loss 7.696
2022-03-07 09:52:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 23952 updates
2022-03-07 09:52:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 09:52:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 09:52:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 123 @ 23952 updates, score 10.644) (writing took 3.0964981249999255 seconds)
2022-03-07 09:52:58 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-07 09:52:58 | INFO | train | epoch 123 | loss 3.69 | nll_loss 2.365 | ppl 5.15 | wps 20514.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23952 | lr 0.000204329 | gnorm 1.085 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 76258
2022-03-07 09:52:58 | INFO | fairseq.trainer | begin training epoch 124
2022-03-07 09:52:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:55:27 | INFO | train_inner | epoch 124:     48 / 196 loss=3.689, nll_loss=2.363, ppl=5.14, wps=20454.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=24000, lr=0.000204124, gnorm=1.087, loss_scale=16, train_wall=289, gb_free=19.9, wall=76408
2022-03-07 09:56:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:57:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 10:00:45 | INFO | train_inner | epoch 124:    150 / 196 loss=3.686, nll_loss=2.36, ppl=5.13, wps=20616.5, ups=0.31, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=1.1, loss_scale=8, train_wall=296, gb_free=19.9, wall=76726
2022-03-07 10:03:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:03:13 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 10.649 | nll_loss 9.78 | ppl 879.18 | wps 41882.4 | wpb 510.9 | bsz 1 | num_updates 24146 | best_loss 7.696
2022-03-07 10:03:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 24146 updates
2022-03-07 10:03:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 10:03:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 10:03:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 124 @ 24146 updates, score 10.649) (writing took 3.1191353278700262 seconds)
2022-03-07 10:03:16 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-07 10:03:16 | INFO | train | epoch 124 | loss 3.684 | nll_loss 2.358 | ppl 5.13 | wps 20526.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 24146 | lr 0.000203506 | gnorm 1.096 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 76876
2022-03-07 10:03:16 | INFO | fairseq.trainer | begin training epoch 125
2022-03-07 10:03:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:06:05 | INFO | train_inner | epoch 125:     54 / 196 loss=3.678, nll_loss=2.351, ppl=5.1, wps=20465.4, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=24200, lr=0.000203279, gnorm=1.085, loss_scale=16, train_wall=289, gb_free=19.9, wall=77045
2022-03-07 10:10:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:11:20 | INFO | train_inner | epoch 125:    155 / 196 loss=3.682, nll_loss=2.356, ppl=5.12, wps=20802.1, ups=0.32, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=1.115, loss_scale=16, train_wall=293, gb_free=19.9, wall=77360
2022-03-07 10:13:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:13:32 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 10.677 | nll_loss 9.81 | ppl 897.74 | wps 41367.6 | wpb 510.9 | bsz 1 | num_updates 24341 | best_loss 7.696
2022-03-07 10:13:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 24341 updates
2022-03-07 10:13:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 10:13:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 10:13:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 125 @ 24341 updates, score 10.677) (writing took 3.0688201680313796 seconds)
2022-03-07 10:13:35 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-07 10:13:35 | INFO | train | epoch 125 | loss 3.679 | nll_loss 2.352 | ppl 5.11 | wps 20613.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24341 | lr 0.000202689 | gnorm 1.102 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 77496
2022-03-07 10:13:35 | INFO | fairseq.trainer | begin training epoch 126
2022-03-07 10:13:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:16:40 | INFO | train_inner | epoch 126:     59 / 196 loss=3.669, nll_loss=2.341, ppl=5.07, wps=20436.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=24400, lr=0.000202444, gnorm=1.087, loss_scale=16, train_wall=289, gb_free=19.9, wall=77680
2022-03-07 10:17:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:21:55 | INFO | train_inner | epoch 126:    160 / 196 loss=3.688, nll_loss=2.362, ppl=5.14, wps=20790.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=24500, lr=0.000202031, gnorm=1.095, loss_scale=16, train_wall=293, gb_free=19.9, wall=77995
2022-03-07 10:23:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:23:52 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 10.72 | nll_loss 9.855 | ppl 926.08 | wps 41465.8 | wpb 510.9 | bsz 1 | num_updates 24536 | best_loss 7.696
2022-03-07 10:23:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 24536 updates
2022-03-07 10:23:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 10:23:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 10:23:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 126 @ 24536 updates, score 10.72) (writing took 3.501866410020739 seconds)
2022-03-07 10:23:55 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-07 10:23:55 | INFO | train | epoch 126 | loss 3.674 | nll_loss 2.347 | ppl 5.09 | wps 20591.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24536 | lr 0.000201882 | gnorm 1.089 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 78115
2022-03-07 10:23:55 | INFO | fairseq.trainer | begin training epoch 127
2022-03-07 10:23:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:24:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:27:19 | INFO | train_inner | epoch 127:     65 / 196 loss=3.655, nll_loss=2.326, ppl=5.01, wps=20159.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=24600, lr=0.000201619, gnorm=1.085, loss_scale=16, train_wall=293, gb_free=19.9, wall=78319
2022-03-07 10:31:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:32:34 | INFO | train_inner | epoch 127:    166 / 196 loss=3.68, nll_loss=2.353, ppl=5.11, wps=20799.8, ups=0.32, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=1.106, loss_scale=16, train_wall=293, gb_free=19.9, wall=78634
2022-03-07 10:34:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:34:12 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 10.641 | nll_loss 9.772 | ppl 874.43 | wps 41725.5 | wpb 510.9 | bsz 1 | num_updates 24730 | best_loss 7.696
2022-03-07 10:34:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 24730 updates
2022-03-07 10:34:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 10:34:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 10:34:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 127 @ 24730 updates, score 10.641) (writing took 3.303063128143549 seconds)
2022-03-07 10:34:15 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-07 10:34:15 | INFO | train | epoch 127 | loss 3.668 | nll_loss 2.34 | ppl 5.06 | wps 20465.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 24730 | lr 0.000201089 | gnorm 1.097 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 78736
2022-03-07 10:34:16 | INFO | fairseq.trainer | begin training epoch 128
2022-03-07 10:34:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:37:54 | INFO | train_inner | epoch 128:     70 / 196 loss=3.65, nll_loss=2.32, ppl=4.99, wps=20439.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=24800, lr=0.000200805, gnorm=1.094, loss_scale=16, train_wall=289, gb_free=19.9, wall=78954
2022-03-07 10:38:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:43:09 | INFO | train_inner | epoch 128:    171 / 196 loss=3.679, nll_loss=2.353, ppl=5.11, wps=20810.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=24900, lr=0.000200401, gnorm=1.093, loss_scale=16, train_wall=293, gb_free=19.9, wall=79269
2022-03-07 10:44:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:44:31 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 10.687 | nll_loss 9.82 | ppl 903.98 | wps 41698 | wpb 510.9 | bsz 1 | num_updates 24925 | best_loss 7.696
2022-03-07 10:44:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 24925 updates
2022-03-07 10:44:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 10:44:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 10:44:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 128 @ 24925 updates, score 10.687) (writing took 3.296254483051598 seconds)
2022-03-07 10:44:35 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-07 10:44:35 | INFO | train | epoch 128 | loss 3.662 | nll_loss 2.334 | ppl 5.04 | wps 20615.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24925 | lr 0.000200301 | gnorm 1.091 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 79355
2022-03-07 10:44:35 | INFO | fairseq.trainer | begin training epoch 129
2022-03-07 10:44:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:45:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:48:32 | INFO | train_inner | epoch 129:     76 / 196 loss=3.64, nll_loss=2.309, ppl=4.95, wps=20241.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=25000, lr=0.0002, gnorm=1.086, loss_scale=16, train_wall=292, gb_free=19.9, wall=79592
2022-03-07 10:51:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:53:47 | INFO | train_inner | epoch 129:    177 / 196 loss=3.677, nll_loss=2.351, ppl=5.1, wps=20804.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=25100, lr=0.000199601, gnorm=1.103, loss_scale=16, train_wall=293, gb_free=19.9, wall=79907
2022-03-07 10:54:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:54:50 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 10.701 | nll_loss 9.841 | ppl 917.21 | wps 41723.2 | wpb 510.9 | bsz 1 | num_updates 25119 | best_loss 7.696
2022-03-07 10:54:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 25119 updates
2022-03-07 10:54:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 10:54:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 10:54:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 129 @ 25119 updates, score 10.701) (writing took 3.2489652561489493 seconds)
2022-03-07 10:54:54 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-07 10:54:54 | INFO | train | epoch 129 | loss 3.656 | nll_loss 2.327 | ppl 5.02 | wps 20506.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25119 | lr 0.000199526 | gnorm 1.098 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 79974
2022-03-07 10:54:54 | INFO | fairseq.trainer | begin training epoch 130
2022-03-07 10:54:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:58:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:59:10 | INFO | train_inner | epoch 130:     82 / 196 loss=3.629, nll_loss=2.297, ppl=4.92, wps=20245.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25200, lr=0.000199205, gnorm=1.098, loss_scale=16, train_wall=292, gb_free=19.9, wall=80230
2022-03-07 11:04:21 | INFO | train_inner | epoch 130:    182 / 196 loss=3.678, nll_loss=2.352, ppl=5.1, wps=21009.1, ups=0.32, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=1.112, loss_scale=16, train_wall=290, gb_free=19.9, wall=80542
2022-03-07 11:04:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 11:05:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:05:10 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 10.719 | nll_loss 9.858 | ppl 928.02 | wps 41379.6 | wpb 510.9 | bsz 1 | num_updates 25313 | best_loss 7.696
2022-03-07 11:05:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 25313 updates
2022-03-07 11:05:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 11:05:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 11:05:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 130 @ 25313 updates, score 10.719) (writing took 3.4668187631759793 seconds)
2022-03-07 11:05:13 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-07 11:05:13 | INFO | train | epoch 130 | loss 3.652 | nll_loss 2.322 | ppl 5 | wps 20496.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25313 | lr 0.00019876 | gnorm 1.107 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 80593
2022-03-07 11:05:13 | INFO | fairseq.trainer | begin training epoch 131
2022-03-07 11:05:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:09:45 | INFO | train_inner | epoch 131:     87 / 196 loss=3.622, nll_loss=2.289, ppl=4.89, wps=20222, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25400, lr=0.000198419, gnorm=1.092, loss_scale=8, train_wall=292, gb_free=19.9, wall=80865
2022-03-07 11:14:57 | INFO | train_inner | epoch 131:    187 / 196 loss=3.677, nll_loss=2.351, ppl=5.1, wps=21012.3, ups=0.32, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=1.109, loss_scale=16, train_wall=290, gb_free=19.9, wall=81177
2022-03-07 11:15:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:15:29 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 10.729 | nll_loss 9.866 | ppl 933.24 | wps 41493.6 | wpb 510.9 | bsz 1 | num_updates 25509 | best_loss 7.696
2022-03-07 11:15:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 25509 updates
2022-03-07 11:15:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 11:15:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 11:15:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 131 @ 25509 updates, score 10.729) (writing took 3.2027983609586954 seconds)
2022-03-07 11:15:32 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-07 11:15:32 | INFO | train | epoch 131 | loss 3.647 | nll_loss 2.317 | ppl 4.98 | wps 20715 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 25509 | lr 0.000197995 | gnorm 1.097 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 81213
2022-03-07 11:15:32 | INFO | fairseq.trainer | begin training epoch 132
2022-03-07 11:15:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:18:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:20:20 | INFO | train_inner | epoch 132:     92 / 196 loss=3.61, nll_loss=2.275, ppl=4.84, wps=20237.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25600, lr=0.000197642, gnorm=1.093, loss_scale=16, train_wall=292, gb_free=19.9, wall=81500
2022-03-07 11:25:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:25:35 | INFO | train_inner | epoch 132:    193 / 196 loss=3.676, nll_loss=2.349, ppl=5.09, wps=20794.9, ups=0.32, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=1.113, loss_scale=16, train_wall=293, gb_free=19.9, wall=81815
2022-03-07 11:25:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:25:49 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 10.746 | nll_loss 9.891 | ppl 949.63 | wps 41545.2 | wpb 510.9 | bsz 1 | num_updates 25703 | best_loss 7.696
2022-03-07 11:25:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 25703 updates
2022-03-07 11:25:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 11:25:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 11:25:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 132 @ 25703 updates, score 10.746) (writing took 3.2202027519233525 seconds)
2022-03-07 11:25:52 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-07 11:25:52 | INFO | train | epoch 132 | loss 3.64 | nll_loss 2.31 | ppl 4.96 | wps 20497.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25703 | lr 0.000197246 | gnorm 1.101 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 81832
2022-03-07 11:25:52 | INFO | fairseq.trainer | begin training epoch 133
2022-03-07 11:25:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:30:54 | INFO | train_inner | epoch 133:     97 / 196 loss=3.602, nll_loss=2.267, ppl=4.81, wps=20445.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25800, lr=0.000196875, gnorm=1.084, loss_scale=16, train_wall=289, gb_free=19.9, wall=82135
2022-03-07 11:31:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:36:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:36:08 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 10.742 | nll_loss 9.878 | ppl 940.8 | wps 41532.4 | wpb 510.9 | bsz 1 | num_updates 25898 | best_loss 7.696
2022-03-07 11:36:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 25898 updates
2022-03-07 11:36:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 11:36:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 11:36:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 133 @ 25898 updates, score 10.742) (writing took 3.1223442319314927 seconds)
2022-03-07 11:36:11 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-07 11:36:11 | INFO | train | epoch 133 | loss 3.635 | nll_loss 2.304 | ppl 4.94 | wps 20620.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 25898 | lr 0.000196502 | gnorm 1.095 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 82451
2022-03-07 11:36:11 | INFO | fairseq.trainer | begin training epoch 134
2022-03-07 11:36:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:36:17 | INFO | train_inner | epoch 134:      2 / 196 loss=3.669, nll_loss=2.341, ppl=5.07, wps=20258.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=25900, lr=0.000196494, gnorm=1.107, loss_scale=16, train_wall=292, gb_free=19.9, wall=82457
2022-03-07 11:38:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:41:32 | INFO | train_inner | epoch 134:    103 / 196 loss=3.596, nll_loss=2.26, ppl=4.79, wps=20792.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=26000, lr=0.000196116, gnorm=1.102, loss_scale=16, train_wall=293, gb_free=19.9, wall=82773
2022-03-07 11:45:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:46:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:46:27 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 10.729 | nll_loss 9.867 | ppl 933.58 | wps 41560.5 | wpb 510.9 | bsz 1 | num_updates 26092 | best_loss 7.696
2022-03-07 11:46:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 26092 updates
2022-03-07 11:46:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 11:46:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 11:46:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 134 @ 26092 updates, score 10.729) (writing took 3.0468171751126647 seconds)
2022-03-07 11:46:30 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-07 11:46:30 | INFO | train | epoch 134 | loss 3.631 | nll_loss 2.299 | ppl 4.92 | wps 20505.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26092 | lr 0.00019577 | gnorm 1.11 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 83070
2022-03-07 11:46:30 | INFO | fairseq.trainer | begin training epoch 135
2022-03-07 11:46:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:46:55 | INFO | train_inner | epoch 135:      8 / 196 loss=3.66, nll_loss=2.332, ppl=5.03, wps=20253.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=26100, lr=0.00019574, gnorm=1.117, loss_scale=16, train_wall=292, gb_free=19.9, wall=83095
2022-03-07 11:52:07 | INFO | train_inner | epoch 135:    108 / 196 loss=3.597, nll_loss=2.261, ppl=4.79, wps=21012.2, ups=0.32, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=1.088, loss_scale=16, train_wall=290, gb_free=19.9, wall=83407
2022-03-07 11:52:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:56:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:56:46 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 10.719 | nll_loss 9.857 | ppl 927.23 | wps 41718.6 | wpb 510.9 | bsz 1 | num_updates 26287 | best_loss 7.696
2022-03-07 11:56:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 26287 updates
2022-03-07 11:56:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 11:56:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 11:56:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 135 @ 26287 updates, score 10.719) (writing took 3.130310108885169 seconds)
2022-03-07 11:56:49 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-07 11:56:49 | INFO | train | epoch 135 | loss 3.627 | nll_loss 2.295 | ppl 4.91 | wps 20617.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 26287 | lr 0.000195043 | gnorm 1.098 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 83689
2022-03-07 11:56:49 | INFO | fairseq.trainer | begin training epoch 136
2022-03-07 11:56:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:57:30 | INFO | train_inner | epoch 136:     13 / 196 loss=3.651, nll_loss=2.322, ppl=5, wps=20254.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=26300, lr=0.000194994, gnorm=1.108, loss_scale=16, train_wall=292, gb_free=19.9, wall=83730
2022-03-07 11:59:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:02:45 | INFO | train_inner | epoch 136:    114 / 196 loss=3.593, nll_loss=2.257, ppl=4.78, wps=20793.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=26400, lr=0.000194625, gnorm=1.093, loss_scale=16, train_wall=293, gb_free=19.9, wall=84045
2022-03-07 12:06:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:07:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:07:05 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 10.771 | nll_loss 9.912 | ppl 963.63 | wps 41504.2 | wpb 510.9 | bsz 1 | num_updates 26481 | best_loss 7.696
2022-03-07 12:07:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 26481 updates
2022-03-07 12:07:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 12:07:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 12:07:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 136 @ 26481 updates, score 10.771) (writing took 3.1374068618752062 seconds)
2022-03-07 12:07:08 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-07 12:07:08 | INFO | train | epoch 136 | loss 3.621 | nll_loss 2.288 | ppl 4.88 | wps 20502.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26481 | lr 0.000194327 | gnorm 1.11 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 84309
2022-03-07 12:07:08 | INFO | fairseq.trainer | begin training epoch 137
2022-03-07 12:07:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:08:08 | INFO | train_inner | epoch 137:     19 / 196 loss=3.647, nll_loss=2.317, ppl=4.98, wps=20244.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=26500, lr=0.000194257, gnorm=1.119, loss_scale=16, train_wall=292, gb_free=19.9, wall=84368
2022-03-07 12:12:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:13:23 | INFO | train_inner | epoch 137:    120 / 196 loss=3.592, nll_loss=2.256, ppl=4.78, wps=20801.3, ups=0.32, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=1.098, loss_scale=16, train_wall=293, gb_free=19.9, wall=84683
2022-03-07 12:17:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:17:24 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 10.757 | nll_loss 9.898 | ppl 953.87 | wps 41629.5 | wpb 510.9 | bsz 1 | num_updates 26676 | best_loss 7.696
2022-03-07 12:17:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 26676 updates
2022-03-07 12:17:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 12:17:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 12:17:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 137 @ 26676 updates, score 10.757) (writing took 3.083014503121376 seconds)
2022-03-07 12:17:27 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-07 12:17:27 | INFO | train | epoch 137 | loss 3.617 | nll_loss 2.283 | ppl 4.87 | wps 20612.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 26676 | lr 0.000193615 | gnorm 1.104 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 84928
2022-03-07 12:17:27 | INFO | fairseq.trainer | begin training epoch 138
2022-03-07 12:17:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:18:42 | INFO | train_inner | epoch 138:     24 / 196 loss=3.636, nll_loss=2.305, ppl=4.94, wps=20451.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=26700, lr=0.000193528, gnorm=1.12, loss_scale=16, train_wall=289, gb_free=19.9, wall=85003
2022-03-07 12:19:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:23:57 | INFO | train_inner | epoch 138:    125 / 196 loss=3.593, nll_loss=2.257, ppl=4.78, wps=20803.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=26800, lr=0.000193167, gnorm=1.087, loss_scale=16, train_wall=293, gb_free=19.9, wall=85318
2022-03-07 12:26:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:27:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:27:43 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 10.777 | nll_loss 9.912 | ppl 963.47 | wps 41480.5 | wpb 510.9 | bsz 1 | num_updates 26870 | best_loss 7.696
2022-03-07 12:27:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 26870 updates
2022-03-07 12:27:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 12:27:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 12:27:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 138 @ 26870 updates, score 10.777) (writing took 3.1485906129237264 seconds)
2022-03-07 12:27:47 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-07 12:27:47 | INFO | train | epoch 138 | loss 3.612 | nll_loss 2.278 | ppl 4.85 | wps 20508.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26870 | lr 0.000192915 | gnorm 1.098 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 85547
2022-03-07 12:27:47 | INFO | fairseq.trainer | begin training epoch 139
2022-03-07 12:27:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:29:20 | INFO | train_inner | epoch 139:     30 / 196 loss=3.625, nll_loss=2.293, ppl=4.9, wps=20241.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=26900, lr=0.000192807, gnorm=1.101, loss_scale=16, train_wall=292, gb_free=19.9, wall=85641
2022-03-07 12:33:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 12:34:35 | INFO | train_inner | epoch 139:    131 / 196 loss=3.595, nll_loss=2.259, ppl=4.79, wps=20798.9, ups=0.32, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=1.121, loss_scale=8, train_wall=293, gb_free=19.9, wall=85956
2022-03-07 12:37:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:38:03 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 10.8 | nll_loss 9.944 | ppl 985.27 | wps 41709.8 | wpb 510.9 | bsz 1 | num_updates 27065 | best_loss 7.696
2022-03-07 12:38:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 27065 updates
2022-03-07 12:38:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 12:38:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 12:38:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 139 @ 27065 updates, score 10.8) (writing took 3.1091967329848558 seconds)
2022-03-07 12:38:06 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-07 12:38:06 | INFO | train | epoch 139 | loss 3.607 | nll_loss 2.273 | ppl 4.83 | wps 20610.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27065 | lr 0.000192219 | gnorm 1.113 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 86166
2022-03-07 12:38:06 | INFO | fairseq.trainer | begin training epoch 140
2022-03-07 12:38:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:39:55 | INFO | train_inner | epoch 140:     35 / 196 loss=3.614, nll_loss=2.281, ppl=4.86, wps=20453.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=27100, lr=0.000192095, gnorm=1.117, loss_scale=8, train_wall=289, gb_free=19.9, wall=86275
2022-03-07 12:45:07 | INFO | train_inner | epoch 140:    135 / 196 loss=3.597, nll_loss=2.262, ppl=4.79, wps=21016.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=27200, lr=0.000191741, gnorm=1.1, loss_scale=16, train_wall=290, gb_free=19.9, wall=86587
2022-03-07 12:46:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:48:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:48:22 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 10.803 | nll_loss 9.939 | ppl 981.85 | wps 41571.4 | wpb 510.9 | bsz 1 | num_updates 27260 | best_loss 7.696
2022-03-07 12:48:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 27260 updates
2022-03-07 12:48:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 12:48:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 12:48:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 140 @ 27260 updates, score 10.803) (writing took 3.1197761329822242 seconds)
2022-03-07 12:48:25 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-07 12:48:25 | INFO | train | epoch 140 | loss 3.603 | nll_loss 2.268 | ppl 4.82 | wps 20619.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 27260 | lr 0.00019153 | gnorm 1.122 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 86785
2022-03-07 12:48:25 | INFO | fairseq.trainer | begin training epoch 141
2022-03-07 12:48:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:50:30 | INFO | train_inner | epoch 141:     40 / 196 loss=3.604, nll_loss=2.269, ppl=4.82, wps=20247.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=27300, lr=0.00019139, gnorm=1.131, loss_scale=16, train_wall=292, gb_free=19.9, wall=86910
2022-03-07 12:53:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:55:45 | INFO | train_inner | epoch 141:    141 / 196 loss=3.594, nll_loss=2.258, ppl=4.78, wps=20792.3, ups=0.32, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=1.119, loss_scale=16, train_wall=293, gb_free=19.9, wall=87225
2022-03-07 12:58:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:58:41 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 10.797 | nll_loss 9.937 | ppl 980.15 | wps 41759.6 | wpb 510.9 | bsz 1 | num_updates 27455 | best_loss 7.696
2022-03-07 12:58:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 27455 updates
2022-03-07 12:58:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 12:58:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 12:58:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 141 @ 27455 updates, score 10.797) (writing took 3.1481724069453776 seconds)
2022-03-07 12:58:44 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-07 12:58:44 | INFO | train | epoch 141 | loss 3.598 | nll_loss 2.262 | ppl 4.8 | wps 20606.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27455 | lr 0.000190849 | gnorm 1.113 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 87404
2022-03-07 12:58:44 | INFO | fairseq.trainer | begin training epoch 142
2022-03-07 12:58:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:00:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:01:08 | INFO | train_inner | epoch 142:     46 / 196 loss=3.599, nll_loss=2.264, ppl=4.8, wps=20242.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=27500, lr=0.000190693, gnorm=1.107, loss_scale=16, train_wall=292, gb_free=19.9, wall=87548
2022-03-07 13:06:20 | INFO | train_inner | epoch 142:    146 / 196 loss=3.592, nll_loss=2.256, ppl=4.78, wps=21004.5, ups=0.32, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=1.131, loss_scale=16, train_wall=290, gb_free=19.9, wall=87860
2022-03-07 13:07:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:08:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:09:00 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 10.816 | nll_loss 9.96 | ppl 996.06 | wps 41445.7 | wpb 510.9 | bsz 1 | num_updates 27649 | best_loss 7.696
2022-03-07 13:09:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 27649 updates
2022-03-07 13:09:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 13:09:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 13:09:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 142 @ 27649 updates, score 10.816) (writing took 3.188816598150879 seconds)
2022-03-07 13:09:03 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-07 13:09:03 | INFO | train | epoch 142 | loss 3.594 | nll_loss 2.258 | ppl 4.78 | wps 20498.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27649 | lr 0.000190178 | gnorm 1.126 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 88024
2022-03-07 13:09:03 | INFO | fairseq.trainer | begin training epoch 143
2022-03-07 13:09:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:11:43 | INFO | train_inner | epoch 143:     51 / 196 loss=3.589, nll_loss=2.253, ppl=4.77, wps=20235.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=27700, lr=0.000190003, gnorm=1.136, loss_scale=16, train_wall=292, gb_free=19.9, wall=88183
2022-03-07 13:14:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:16:58 | INFO | train_inner | epoch 143:    152 / 196 loss=3.59, nll_loss=2.254, ppl=4.77, wps=20796.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=27800, lr=0.000189661, gnorm=1.113, loss_scale=16, train_wall=293, gb_free=19.9, wall=88498
2022-03-07 13:19:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:19:20 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 10.783 | nll_loss 9.923 | ppl 970.9 | wps 41607.4 | wpb 510.9 | bsz 1 | num_updates 27844 | best_loss 7.696
2022-03-07 13:19:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 27844 updates
2022-03-07 13:19:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 13:19:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 13:19:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 143 @ 27844 updates, score 10.783) (writing took 3.160700412001461 seconds)
2022-03-07 13:19:23 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-07 13:19:23 | INFO | train | epoch 143 | loss 3.589 | nll_loss 2.253 | ppl 4.77 | wps 20605.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27844 | lr 0.000189511 | gnorm 1.12 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 88643
2022-03-07 13:19:23 | INFO | fairseq.trainer | begin training epoch 144
2022-03-07 13:19:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:20:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:22:21 | INFO | train_inner | epoch 144:     57 / 196 loss=3.587, nll_loss=2.251, ppl=4.76, wps=20240.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=27900, lr=0.000189321, gnorm=1.115, loss_scale=16, train_wall=292, gb_free=19.9, wall=88821
2022-03-07 13:27:33 | INFO | train_inner | epoch 144:    157 / 196 loss=3.591, nll_loss=2.255, ppl=4.77, wps=20998.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=28000, lr=0.000188982, gnorm=1.115, loss_scale=16, train_wall=290, gb_free=19.9, wall=89133
2022-03-07 13:27:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:29:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:29:39 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 10.83 | nll_loss 9.972 | ppl 1004.35 | wps 41496.2 | wpb 510.9 | bsz 1 | num_updates 28038 | best_loss 7.696
2022-03-07 13:29:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 28038 updates
2022-03-07 13:29:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 13:29:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 13:29:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 144 @ 28038 updates, score 10.83) (writing took 3.1726294569671154 seconds)
2022-03-07 13:29:42 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-07 13:29:42 | INFO | train | epoch 144 | loss 3.585 | nll_loss 2.248 | ppl 4.75 | wps 20496.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28038 | lr 0.000188854 | gnorm 1.118 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 89263
2022-03-07 13:29:42 | INFO | fairseq.trainer | begin training epoch 145
2022-03-07 13:29:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:32:56 | INFO | train_inner | epoch 145:     62 / 196 loss=3.575, nll_loss=2.238, ppl=4.72, wps=20239.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28100, lr=0.000188646, gnorm=1.125, loss_scale=16, train_wall=292, gb_free=19.9, wall=89456
2022-03-07 13:34:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:38:11 | INFO | train_inner | epoch 145:    163 / 196 loss=3.59, nll_loss=2.254, ppl=4.77, wps=20790.6, ups=0.32, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=1.118, loss_scale=16, train_wall=293, gb_free=19.9, wall=89771
2022-03-07 13:39:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:39:59 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 10.828 | nll_loss 9.967 | ppl 1001.09 | wps 41623.7 | wpb 510.9 | bsz 1 | num_updates 28233 | best_loss 7.696
2022-03-07 13:39:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 28233 updates
2022-03-07 13:39:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 13:40:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 13:40:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 145 @ 28233 updates, score 10.828) (writing took 3.1179130510427058 seconds)
2022-03-07 13:40:02 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-07 13:40:02 | INFO | train | epoch 145 | loss 3.581 | nll_loss 2.244 | ppl 4.74 | wps 20604 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28233 | lr 0.000188201 | gnorm 1.113 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 89882
2022-03-07 13:40:02 | INFO | fairseq.trainer | begin training epoch 146
2022-03-07 13:40:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:41:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:43:34 | INFO | train_inner | epoch 146:     68 / 196 loss=3.566, nll_loss=2.227, ppl=4.68, wps=20244.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=28300, lr=0.000187978, gnorm=1.107, loss_scale=16, train_wall=292, gb_free=19.9, wall=90094
2022-03-07 13:48:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:48:49 | INFO | train_inner | epoch 146:    169 / 196 loss=3.592, nll_loss=2.256, ppl=4.78, wps=20796.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=28400, lr=0.000187647, gnorm=1.111, loss_scale=16, train_wall=293, gb_free=19.9, wall=90409
2022-03-07 13:50:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:50:18 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 10.817 | nll_loss 9.961 | ppl 996.36 | wps 41467.8 | wpb 510.9 | bsz 1 | num_updates 28427 | best_loss 7.696
2022-03-07 13:50:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 28427 updates
2022-03-07 13:50:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 13:50:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 13:50:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 146 @ 28427 updates, score 10.817) (writing took 3.143173235002905 seconds)
2022-03-07 13:50:21 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-07 13:50:21 | INFO | train | epoch 146 | loss 3.576 | nll_loss 2.238 | ppl 4.72 | wps 20501.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28427 | lr 0.000187558 | gnorm 1.117 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 90501
2022-03-07 13:50:21 | INFO | fairseq.trainer | begin training epoch 147
2022-03-07 13:50:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:54:09 | INFO | train_inner | epoch 147:     73 / 196 loss=3.555, nll_loss=2.214, ppl=4.64, wps=20435.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=28500, lr=0.000187317, gnorm=1.141, loss_scale=16, train_wall=289, gb_free=19.9, wall=90729
2022-03-07 13:55:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:59:24 | INFO | train_inner | epoch 147:    174 / 196 loss=3.591, nll_loss=2.256, ppl=4.78, wps=20811, ups=0.32, wpb=65532.4, bsz=128, num_updates=28600, lr=0.000186989, gnorm=1.133, loss_scale=16, train_wall=293, gb_free=19.9, wall=91044
2022-03-07 14:00:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:00:37 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 10.902 | nll_loss 10.053 | ppl 1062.49 | wps 41507.8 | wpb 510.9 | bsz 1 | num_updates 28622 | best_loss 7.696
2022-03-07 14:00:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 28622 updates
2022-03-07 14:00:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 14:00:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 14:00:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 147 @ 28622 updates, score 10.902) (writing took 3.090500681195408 seconds)
2022-03-07 14:00:40 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-07 14:00:40 | INFO | train | epoch 147 | loss 3.572 | nll_loss 2.234 | ppl 4.7 | wps 20615 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28622 | lr 0.000186918 | gnorm 1.135 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 91120
2022-03-07 14:00:40 | INFO | fairseq.trainer | begin training epoch 148
2022-03-07 14:00:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:02:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:04:47 | INFO | train_inner | epoch 148:     79 / 196 loss=3.552, nll_loss=2.211, ppl=4.63, wps=20248.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28700, lr=0.000186663, gnorm=1.133, loss_scale=16, train_wall=292, gb_free=19.9, wall=91367
2022-03-07 14:08:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:10:02 | INFO | train_inner | epoch 148:    180 / 196 loss=3.591, nll_loss=2.255, ppl=4.77, wps=20809.9, ups=0.32, wpb=65536, bsz=128, num_updates=28800, lr=0.000186339, gnorm=1.13, loss_scale=16, train_wall=293, gb_free=19.9, wall=91682
2022-03-07 14:10:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:10:56 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 10.815 | nll_loss 9.964 | ppl 999.07 | wps 41253.5 | wpb 510.9 | bsz 1 | num_updates 28816 | best_loss 7.696
2022-03-07 14:10:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 28816 updates
2022-03-07 14:10:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 14:10:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 14:10:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 148 @ 28816 updates, score 10.815) (writing took 3.083912634057924 seconds)
2022-03-07 14:10:59 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-07 14:10:59 | INFO | train | epoch 148 | loss 3.568 | nll_loss 2.229 | ppl 4.69 | wps 20511.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28816 | lr 0.000186287 | gnorm 1.132 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 91739
2022-03-07 14:10:59 | INFO | fairseq.trainer | begin training epoch 149
2022-03-07 14:10:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:15:21 | INFO | train_inner | epoch 149:     84 / 196 loss=3.533, nll_loss=2.19, ppl=4.56, wps=20454, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28900, lr=0.000186016, gnorm=1.128, loss_scale=16, train_wall=289, gb_free=19.9, wall=92001
2022-03-07 14:15:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:20:36 | INFO | train_inner | epoch 149:    185 / 196 loss=3.599, nll_loss=2.264, ppl=4.8, wps=20810.7, ups=0.32, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=1.136, loss_scale=16, train_wall=293, gb_free=19.9, wall=92316
2022-03-07 14:21:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:21:15 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 10.836 | nll_loss 9.978 | ppl 1008.35 | wps 41578.4 | wpb 510.9 | bsz 1 | num_updates 29011 | best_loss 7.696
2022-03-07 14:21:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 29011 updates
2022-03-07 14:21:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 14:21:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 14:21:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 149 @ 29011 updates, score 10.836) (writing took 3.0933249620720744 seconds)
2022-03-07 14:21:18 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-07 14:21:18 | INFO | train | epoch 149 | loss 3.564 | nll_loss 2.225 | ppl 4.68 | wps 20621.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 29011 | lr 0.00018566 | gnorm 1.13 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 92358
2022-03-07 14:21:18 | INFO | fairseq.trainer | begin training epoch 150
2022-03-07 14:21:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:22:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:25:59 | INFO | train_inner | epoch 150:     90 / 196 loss=3.534, nll_loss=2.191, ppl=4.57, wps=20250.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=29100, lr=0.000185376, gnorm=1.139, loss_scale=16, train_wall=292, gb_free=19.9, wall=92639
2022-03-07 14:29:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:30:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 14:31:17 | INFO | train_inner | epoch 150:    192 / 196 loss=3.588, nll_loss=2.252, ppl=4.76, wps=20598.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=29200, lr=0.000185058, gnorm=1.138, loss_scale=8, train_wall=296, gb_free=19.9, wall=92957
2022-03-07 14:31:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:31:34 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 10.85 | nll_loss 9.994 | ppl 1019.97 | wps 41643.9 | wpb 510.9 | bsz 1 | num_updates 29204 | best_loss 7.696
2022-03-07 14:31:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 29204 updates
2022-03-07 14:31:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 14:31:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 14:31:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 150 @ 29204 updates, score 10.85) (writing took 3.107198375975713 seconds)
2022-03-07 14:31:37 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-07 14:31:37 | INFO | train | epoch 150 | loss 3.558 | nll_loss 2.219 | ppl 4.66 | wps 20401.5 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 29204 | lr 0.000185046 | gnorm 1.138 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 92977
2022-03-07 14:31:37 | INFO | fairseq.trainer | begin training epoch 151
2022-03-07 14:31:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:36:37 | INFO | train_inner | epoch 151:     96 / 196 loss=3.515, nll_loss=2.17, ppl=4.5, wps=20449.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=29300, lr=0.000184742, gnorm=1.094, loss_scale=8, train_wall=289, gb_free=19.9, wall=93277
2022-03-07 14:41:48 | INFO | train_inner | epoch 151:    196 / 196 loss=3.599, nll_loss=2.264, ppl=4.8, wps=21009.1, ups=0.32, wpb=65367, bsz=127.7, num_updates=29400, lr=0.000184428, gnorm=1.143, loss_scale=16, train_wall=289, gb_free=19.9, wall=93588
2022-03-07 14:41:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:41:53 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 10.852 | nll_loss 10 | ppl 1024.22 | wps 41727.1 | wpb 510.9 | bsz 1 | num_updates 29400 | best_loss 7.696
2022-03-07 14:41:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 29400 updates
2022-03-07 14:41:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 14:41:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 14:41:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 151 @ 29400 updates, score 10.852) (writing took 3.062551948009059 seconds)
2022-03-07 14:41:56 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-07 14:41:56 | INFO | train | epoch 151 | loss 3.556 | nll_loss 2.216 | ppl 4.65 | wps 20722.9 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 29400 | lr 0.000184428 | gnorm 1.118 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 93596
2022-03-07 14:41:56 | INFO | fairseq.trainer | begin training epoch 152
2022-03-07 14:41:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:43:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:47:11 | INFO | train_inner | epoch 152:    101 / 196 loss=3.512, nll_loss=2.167, ppl=4.49, wps=20263.7, ups=0.31, wpb=65536, bsz=128, num_updates=29500, lr=0.000184115, gnorm=1.122, loss_scale=16, train_wall=293, gb_free=19.9, wall=93911
2022-03-07 14:50:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:52:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:52:12 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 10.903 | nll_loss 10.049 | ppl 1059.57 | wps 41563.4 | wpb 510.9 | bsz 1 | num_updates 29594 | best_loss 7.696
2022-03-07 14:52:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 29594 updates
2022-03-07 14:52:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 14:52:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 14:52:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 152 @ 29594 updates, score 10.903) (writing took 3.099776790011674 seconds)
2022-03-07 14:52:15 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-03-07 14:52:15 | INFO | train | epoch 152 | loss 3.552 | nll_loss 2.212 | ppl 4.63 | wps 20513.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 29594 | lr 0.000183822 | gnorm 1.139 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 94215
2022-03-07 14:52:15 | INFO | fairseq.trainer | begin training epoch 153
2022-03-07 14:52:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:52:34 | INFO | train_inner | epoch 153:      6 / 196 loss=3.588, nll_loss=2.251, ppl=4.76, wps=20254.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=29600, lr=0.000183804, gnorm=1.155, loss_scale=16, train_wall=292, gb_free=19.9, wall=94234
2022-03-07 14:57:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:57:49 | INFO | train_inner | epoch 153:    107 / 196 loss=3.517, nll_loss=2.173, ppl=4.51, wps=20806.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=29700, lr=0.000183494, gnorm=1.119, loss_scale=16, train_wall=293, gb_free=19.9, wall=94549
2022-03-07 15:02:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:02:31 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 10.913 | nll_loss 10.066 | ppl 1072.09 | wps 41565.9 | wpb 510.9 | bsz 1 | num_updates 29789 | best_loss 7.696
2022-03-07 15:02:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 29789 updates
2022-03-07 15:02:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 15:02:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 15:02:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 153 @ 29789 updates, score 10.913) (writing took 3.111995079088956 seconds)
2022-03-07 15:02:34 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-03-07 15:02:34 | INFO | train | epoch 153 | loss 3.547 | nll_loss 2.206 | ppl 4.61 | wps 20620.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 29789 | lr 0.00018322 | gnorm 1.127 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 94834
2022-03-07 15:02:34 | INFO | fairseq.trainer | begin training epoch 154
2022-03-07 15:02:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:03:08 | INFO | train_inner | epoch 154:     11 / 196 loss=3.574, nll_loss=2.237, ppl=4.71, wps=20456.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=29800, lr=0.000183186, gnorm=1.137, loss_scale=16, train_wall=289, gb_free=19.9, wall=94869
2022-03-07 15:04:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:08:23 | INFO | train_inner | epoch 154:    112 / 196 loss=3.515, nll_loss=2.17, ppl=4.5, wps=20794.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=29900, lr=0.000182879, gnorm=1.125, loss_scale=16, train_wall=293, gb_free=19.9, wall=95184
2022-03-07 15:10:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:12:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:12:50 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 10.864 | nll_loss 10.012 | ppl 1032.72 | wps 41644.3 | wpb 510.9 | bsz 1 | num_updates 29983 | best_loss 7.696
2022-03-07 15:12:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 29983 updates
2022-03-07 15:12:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 15:12:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 15:12:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 154 @ 29983 updates, score 10.864) (writing took 3.0872175560798496 seconds)
2022-03-07 15:12:53 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-03-07 15:12:53 | INFO | train | epoch 154 | loss 3.544 | nll_loss 2.202 | ppl 4.6 | wps 20504.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 29983 | lr 0.000182626 | gnorm 1.128 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 95453
2022-03-07 15:12:53 | INFO | fairseq.trainer | begin training epoch 155
2022-03-07 15:12:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:13:46 | INFO | train_inner | epoch 155:     17 / 196 loss=3.567, nll_loss=2.229, ppl=4.69, wps=20247.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=30000, lr=0.000182574, gnorm=1.131, loss_scale=16, train_wall=292, gb_free=19.9, wall=95507
2022-03-07 15:17:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:19:01 | INFO | train_inner | epoch 155:    118 / 196 loss=3.519, nll_loss=2.175, ppl=4.52, wps=20800.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=30100, lr=0.000182271, gnorm=1.117, loss_scale=16, train_wall=293, gb_free=19.9, wall=95822
2022-03-07 15:23:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:23:09 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 10.851 | nll_loss 9.998 | ppl 1022.74 | wps 41458.8 | wpb 510.9 | bsz 1 | num_updates 30178 | best_loss 7.696
2022-03-07 15:23:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 30178 updates
2022-03-07 15:23:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 15:23:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 15:23:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 155 @ 30178 updates, score 10.851) (writing took 3.0950777928810567 seconds)
2022-03-07 15:23:12 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-03-07 15:23:12 | INFO | train | epoch 155 | loss 3.54 | nll_loss 2.198 | ppl 4.59 | wps 20611.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 30178 | lr 0.000182035 | gnorm 1.129 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 96073
2022-03-07 15:23:12 | INFO | fairseq.trainer | begin training epoch 156
2022-03-07 15:23:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:24:21 | INFO | train_inner | epoch 156:     22 / 196 loss=3.559, nll_loss=2.22, ppl=4.66, wps=20449.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=30200, lr=0.000181969, gnorm=1.132, loss_scale=16, train_wall=289, gb_free=19.9, wall=96141
2022-03-07 15:24:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:29:36 | INFO | train_inner | epoch 156:    123 / 196 loss=3.521, nll_loss=2.177, ppl=4.52, wps=20807.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=30300, lr=0.000181668, gnorm=1.13, loss_scale=16, train_wall=293, gb_free=19.9, wall=96456
2022-03-07 15:31:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:33:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:33:28 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 10.895 | nll_loss 10.046 | ppl 1056.97 | wps 41252.9 | wpb 510.9 | bsz 1 | num_updates 30372 | best_loss 7.696
2022-03-07 15:33:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 30372 updates
2022-03-07 15:33:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 15:33:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 15:33:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 156 @ 30372 updates, score 10.895) (writing took 3.0941239478997886 seconds)
2022-03-07 15:33:31 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-03-07 15:33:31 | INFO | train | epoch 156 | loss 3.537 | nll_loss 2.196 | ppl 4.58 | wps 20512.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 30372 | lr 0.000181453 | gnorm 1.141 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 96692
2022-03-07 15:33:31 | INFO | fairseq.trainer | begin training epoch 157
2022-03-07 15:33:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:34:59 | INFO | train_inner | epoch 157:     28 / 196 loss=3.547, nll_loss=2.206, ppl=4.62, wps=20245.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=30400, lr=0.000181369, gnorm=1.149, loss_scale=16, train_wall=292, gb_free=19.9, wall=96779
2022-03-07 15:38:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:40:14 | INFO | train_inner | epoch 157:    129 / 196 loss=3.515, nll_loss=2.17, ppl=4.5, wps=20804.3, ups=0.32, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=1.132, loss_scale=16, train_wall=293, gb_free=19.9, wall=97094
2022-03-07 15:43:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:43:47 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 10.856 | nll_loss 10.002 | ppl 1025.4 | wps 41578.7 | wpb 510.9 | bsz 1 | num_updates 30567 | best_loss 7.696
2022-03-07 15:43:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 30567 updates
2022-03-07 15:43:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 15:43:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 15:43:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 157 @ 30567 updates, score 10.856) (writing took 3.0884437449276447 seconds)
2022-03-07 15:43:50 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-03-07 15:43:50 | INFO | train | epoch 157 | loss 3.531 | nll_loss 2.189 | ppl 4.56 | wps 20611.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 30567 | lr 0.000180873 | gnorm 1.135 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 97311
2022-03-07 15:43:51 | INFO | fairseq.trainer | begin training epoch 158
2022-03-07 15:43:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:44:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:45:37 | INFO | train_inner | epoch 158:     34 / 196 loss=3.544, nll_loss=2.204, ppl=4.61, wps=20254.2, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=30600, lr=0.000180775, gnorm=1.151, loss_scale=16, train_wall=292, gb_free=19.9, wall=97417
2022-03-07 15:50:48 | INFO | train_inner | epoch 158:    134 / 196 loss=3.521, nll_loss=2.178, ppl=4.52, wps=21020.3, ups=0.32, wpb=65536, bsz=128, num_updates=30700, lr=0.000180481, gnorm=1.137, loss_scale=16, train_wall=290, gb_free=19.9, wall=97729
2022-03-07 15:51:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:54:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:54:06 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 10.877 | nll_loss 10.025 | ppl 1041.63 | wps 41797.7 | wpb 510.9 | bsz 1 | num_updates 30761 | best_loss 7.696
2022-03-07 15:54:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 30761 updates
2022-03-07 15:54:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 15:54:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 15:54:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 158 @ 30761 updates, score 10.877) (writing took 3.1231004397850484 seconds)
2022-03-07 15:54:09 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-03-07 15:54:09 | INFO | train | epoch 158 | loss 3.528 | nll_loss 2.185 | ppl 4.55 | wps 20518.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 30761 | lr 0.000180302 | gnorm 1.147 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 97930
2022-03-07 15:54:09 | INFO | fairseq.trainer | begin training epoch 159
2022-03-07 15:54:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:56:11 | INFO | train_inner | epoch 159:     39 / 196 loss=3.527, nll_loss=2.185, ppl=4.55, wps=20260, ups=0.31, wpb=65367, bsz=127.7, num_updates=30800, lr=0.000180187, gnorm=1.153, loss_scale=16, train_wall=292, gb_free=19.9, wall=98051
2022-03-07 15:58:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:01:26 | INFO | train_inner | epoch 159:    140 / 196 loss=3.523, nll_loss=2.18, ppl=4.53, wps=20807.8, ups=0.32, wpb=65536, bsz=128, num_updates=30900, lr=0.000179896, gnorm=1.145, loss_scale=16, train_wall=293, gb_free=19.9, wall=98366
2022-03-07 16:04:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:04:25 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 10.915 | nll_loss 10.064 | ppl 1070.58 | wps 41802.5 | wpb 510.9 | bsz 1 | num_updates 30956 | best_loss 7.696
2022-03-07 16:04:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 30956 updates
2022-03-07 16:04:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 16:04:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 16:04:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 159 @ 30956 updates, score 10.915) (writing took 3.1055625500157475 seconds)
2022-03-07 16:04:28 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-03-07 16:04:28 | INFO | train | epoch 159 | loss 3.525 | nll_loss 2.182 | ppl 4.54 | wps 20622.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 30956 | lr 0.000179733 | gnorm 1.14 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 98548
2022-03-07 16:04:28 | INFO | fairseq.trainer | begin training epoch 160
2022-03-07 16:04:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:05:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:06:49 | INFO | train_inner | epoch 160:     45 / 196 loss=3.524, nll_loss=2.18, ppl=4.53, wps=20261.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=31000, lr=0.000179605, gnorm=1.134, loss_scale=16, train_wall=292, gb_free=19.9, wall=98689
2022-03-07 16:12:00 | INFO | train_inner | epoch 160:    145 / 196 loss=3.522, nll_loss=2.178, ppl=4.52, wps=21021.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=31100, lr=0.000179316, gnorm=1.154, loss_scale=16, train_wall=290, gb_free=19.9, wall=99001
2022-03-07 16:12:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:14:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:14:44 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 10.928 | nll_loss 10.08 | ppl 1082.55 | wps 41531 | wpb 510.9 | bsz 1 | num_updates 31150 | best_loss 7.696
2022-03-07 16:14:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 31150 updates
2022-03-07 16:14:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 16:14:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 16:14:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 160 @ 31150 updates, score 10.928) (writing took 3.1839044869411737 seconds)
2022-03-07 16:14:47 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-03-07 16:14:47 | INFO | train | epoch 160 | loss 3.521 | nll_loss 2.177 | ppl 4.52 | wps 20514.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 31150 | lr 0.000179172 | gnorm 1.144 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 99167
2022-03-07 16:14:47 | INFO | fairseq.trainer | begin training epoch 161
2022-03-07 16:14:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:17:23 | INFO | train_inner | epoch 161:     50 / 196 loss=3.518, nll_loss=2.174, ppl=4.51, wps=20250.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=31200, lr=0.000179029, gnorm=1.128, loss_scale=16, train_wall=292, gb_free=19.9, wall=99323
2022-03-07 16:19:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:22:38 | INFO | train_inner | epoch 161:    151 / 196 loss=3.52, nll_loss=2.177, ppl=4.52, wps=20810.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=31300, lr=0.000178743, gnorm=1.143, loss_scale=16, train_wall=293, gb_free=19.9, wall=99638
2022-03-07 16:24:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 16:24:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:25:03 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 10.916 | nll_loss 10.063 | ppl 1069.95 | wps 41600.6 | wpb 510.9 | bsz 1 | num_updates 31344 | best_loss 7.696
2022-03-07 16:25:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 31344 updates
2022-03-07 16:25:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 16:25:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 16:25:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 161 @ 31344 updates, score 10.916) (writing took 3.092344874981791 seconds)
2022-03-07 16:25:06 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-03-07 16:25:06 | INFO | train | epoch 161 | loss 3.517 | nll_loss 2.173 | ppl 4.51 | wps 20519.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 31344 | lr 0.000178617 | gnorm 1.14 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 99786
2022-03-07 16:25:06 | INFO | fairseq.trainer | begin training epoch 162
2022-03-07 16:25:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:28:01 | INFO | train_inner | epoch 162:     56 / 196 loss=3.513, nll_loss=2.169, ppl=4.5, wps=20266.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=31400, lr=0.000178458, gnorm=1.138, loss_scale=8, train_wall=292, gb_free=19.9, wall=99961
2022-03-07 16:33:13 | INFO | train_inner | epoch 162:    156 / 196 loss=3.525, nll_loss=2.182, ppl=4.54, wps=21004.5, ups=0.32, wpb=65536, bsz=128, num_updates=31500, lr=0.000178174, gnorm=1.139, loss_scale=16, train_wall=290, gb_free=19.9, wall=100273
2022-03-07 16:35:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:35:22 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 10.907 | nll_loss 10.052 | ppl 1061.42 | wps 41555 | wpb 510.9 | bsz 1 | num_updates 31540 | best_loss 7.696
2022-03-07 16:35:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 31540 updates
2022-03-07 16:35:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 16:35:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 16:35:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 162 @ 31540 updates, score 10.907) (writing took 3.1333981649950147 seconds)
2022-03-07 16:35:25 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-03-07 16:35:25 | INFO | train | epoch 162 | loss 3.515 | nll_loss 2.171 | ppl 4.5 | wps 20720.4 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 31540 | lr 0.000178061 | gnorm 1.139 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 100405
2022-03-07 16:35:25 | INFO | fairseq.trainer | begin training epoch 163
2022-03-07 16:35:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:38:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:38:35 | INFO | train_inner | epoch 163:     61 / 196 loss=3.496, nll_loss=2.15, ppl=4.44, wps=20253.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=31600, lr=0.000177892, gnorm=1.153, loss_scale=16, train_wall=292, gb_free=19.9, wall=100596
2022-03-07 16:43:47 | INFO | train_inner | epoch 163:    161 / 196 loss=3.526, nll_loss=2.183, ppl=4.54, wps=21009.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=31700, lr=0.000177611, gnorm=1.145, loss_scale=16, train_wall=290, gb_free=19.9, wall=100908
2022-03-07 16:44:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:45:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:45:41 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 10.95 | nll_loss 10.1 | ppl 1097.14 | wps 41290.3 | wpb 510.9 | bsz 1 | num_updates 31734 | best_loss 7.696
2022-03-07 16:45:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 31734 updates
2022-03-07 16:45:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 16:45:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 16:45:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 163 @ 31734 updates, score 10.95) (writing took 3.16612188401632 seconds)
2022-03-07 16:45:44 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-03-07 16:45:44 | INFO | train | epoch 163 | loss 3.511 | nll_loss 2.166 | ppl 4.49 | wps 20509.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 31734 | lr 0.000177516 | gnorm 1.149 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 101024
2022-03-07 16:45:44 | INFO | fairseq.trainer | begin training epoch 164
2022-03-07 16:45:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:49:10 | INFO | train_inner | epoch 164:     66 / 196 loss=3.498, nll_loss=2.152, ppl=4.44, wps=20252, ups=0.31, wpb=65367, bsz=127.7, num_updates=31800, lr=0.000177332, gnorm=1.15, loss_scale=16, train_wall=292, gb_free=19.9, wall=101230
2022-03-07 16:51:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:54:25 | INFO | train_inner | epoch 164:    167 / 196 loss=3.519, nll_loss=2.175, ppl=4.52, wps=20808.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=31900, lr=0.000177054, gnorm=1.139, loss_scale=16, train_wall=293, gb_free=19.9, wall=101545
2022-03-07 16:55:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:56:00 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 10.924 | nll_loss 10.075 | ppl 1078.84 | wps 41533.8 | wpb 510.9 | bsz 1 | num_updates 31929 | best_loss 7.696
2022-03-07 16:56:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 31929 updates
2022-03-07 16:56:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 16:56:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 16:56:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 164 @ 31929 updates, score 10.924) (writing took 3.1047176490537822 seconds)
2022-03-07 16:56:03 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-03-07 16:56:03 | INFO | train | epoch 164 | loss 3.507 | nll_loss 2.163 | ppl 4.48 | wps 20616.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 31929 | lr 0.000176973 | gnorm 1.144 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 101643
2022-03-07 16:56:03 | INFO | fairseq.trainer | begin training epoch 165
2022-03-07 16:56:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:58:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:59:48 | INFO | train_inner | epoch 165:     72 / 196 loss=3.493, nll_loss=2.147, ppl=4.43, wps=20252.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=32000, lr=0.000176777, gnorm=1.145, loss_scale=16, train_wall=292, gb_free=19.9, wall=101868
2022-03-07 17:04:59 | INFO | train_inner | epoch 165:    172 / 196 loss=3.524, nll_loss=2.181, ppl=4.53, wps=21016.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=32100, lr=0.000176501, gnorm=1.139, loss_scale=16, train_wall=290, gb_free=19.9, wall=102180
2022-03-07 17:05:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:06:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:06:19 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 10.938 | nll_loss 10.089 | ppl 1089.1 | wps 41659.2 | wpb 510.9 | bsz 1 | num_updates 32123 | best_loss 7.696
2022-03-07 17:06:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 32123 updates
2022-03-07 17:06:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 17:06:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 17:06:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 165 @ 32123 updates, score 10.938) (writing took 3.087320829043165 seconds)
2022-03-07 17:06:22 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-03-07 17:06:22 | INFO | train | epoch 165 | loss 3.505 | nll_loss 2.16 | ppl 4.47 | wps 20516 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 32123 | lr 0.000176438 | gnorm 1.141 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 102262
2022-03-07 17:06:22 | INFO | fairseq.trainer | begin training epoch 166
2022-03-07 17:06:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:10:22 | INFO | train_inner | epoch 166:     77 / 196 loss=3.479, nll_loss=2.131, ppl=4.38, wps=20259.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=32200, lr=0.000176227, gnorm=1.134, loss_scale=16, train_wall=292, gb_free=19.9, wall=102502
2022-03-07 17:12:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:15:37 | INFO | train_inner | epoch 166:    178 / 196 loss=3.524, nll_loss=2.181, ppl=4.54, wps=20815.9, ups=0.32, wpb=65536, bsz=128, num_updates=32300, lr=0.000175954, gnorm=1.158, loss_scale=16, train_wall=293, gb_free=19.9, wall=102817
2022-03-07 17:16:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:16:38 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 10.938 | nll_loss 10.092 | ppl 1091.28 | wps 41425.3 | wpb 510.9 | bsz 1 | num_updates 32318 | best_loss 7.696
2022-03-07 17:16:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 32318 updates
2022-03-07 17:16:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 17:16:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 17:16:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 166 @ 32318 updates, score 10.938) (writing took 3.0999188269488513 seconds)
2022-03-07 17:16:41 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-03-07 17:16:41 | INFO | train | epoch 166 | loss 3.5 | nll_loss 2.155 | ppl 4.45 | wps 20625.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 32318 | lr 0.000175905 | gnorm 1.143 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 102881
2022-03-07 17:16:41 | INFO | fairseq.trainer | begin training epoch 167
2022-03-07 17:16:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:18:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:21:00 | INFO | train_inner | epoch 167:     83 / 196 loss=3.473, nll_loss=2.125, ppl=4.36, wps=20258.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=32400, lr=0.000175682, gnorm=1.145, loss_scale=16, train_wall=292, gb_free=19.9, wall=103140
2022-03-07 17:25:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:26:14 | INFO | train_inner | epoch 167:    184 / 196 loss=3.526, nll_loss=2.184, ppl=4.54, wps=20811.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=32500, lr=0.000175412, gnorm=1.151, loss_scale=16, train_wall=293, gb_free=19.9, wall=103455
2022-03-07 17:26:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:26:56 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 10.938 | nll_loss 10.091 | ppl 1090.49 | wps 41537.1 | wpb 510.9 | bsz 1 | num_updates 32512 | best_loss 7.696
2022-03-07 17:26:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 32512 updates
2022-03-07 17:26:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 17:26:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 17:27:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 167 @ 32512 updates, score 10.938) (writing took 3.0900464369915426 seconds)
2022-03-07 17:27:00 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-03-07 17:27:00 | INFO | train | epoch 167 | loss 3.498 | nll_loss 2.152 | ppl 4.44 | wps 20516.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 32512 | lr 0.000175379 | gnorm 1.148 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 103500
2022-03-07 17:27:00 | INFO | fairseq.trainer | begin training epoch 168
2022-03-07 17:27:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:31:34 | INFO | train_inner | epoch 168:     88 / 196 loss=3.47, nll_loss=2.121, ppl=4.35, wps=20457.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=32600, lr=0.000175142, gnorm=1.129, loss_scale=16, train_wall=289, gb_free=19.9, wall=103774
2022-03-07 17:32:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:36:49 | INFO | train_inner | epoch 168:    189 / 196 loss=3.518, nll_loss=2.174, ppl=4.51, wps=20818.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=32700, lr=0.000174874, gnorm=1.148, loss_scale=16, train_wall=293, gb_free=19.9, wall=104089
2022-03-07 17:37:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:37:15 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 10.961 | nll_loss 10.116 | ppl 1109.66 | wps 41448.6 | wpb 510.9 | bsz 1 | num_updates 32707 | best_loss 7.696
2022-03-07 17:37:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 32707 updates
2022-03-07 17:37:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 17:37:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 17:37:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 168 @ 32707 updates, score 10.961) (writing took 3.1029088380746543 seconds)
2022-03-07 17:37:18 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-03-07 17:37:18 | INFO | train | epoch 168 | loss 3.494 | nll_loss 2.147 | ppl 4.43 | wps 20627.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 32707 | lr 0.000174856 | gnorm 1.142 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 104119
2022-03-07 17:37:18 | INFO | fairseq.trainer | begin training epoch 169
2022-03-07 17:37:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:39:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:42:11 | INFO | train_inner | epoch 169:     94 / 196 loss=3.463, nll_loss=2.113, ppl=4.33, wps=20253.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=32800, lr=0.000174608, gnorm=1.152, loss_scale=16, train_wall=292, gb_free=19.9, wall=104412
2022-03-07 17:45:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:47:27 | INFO | train_inner | epoch 169:    195 / 196 loss=3.525, nll_loss=2.183, ppl=4.54, wps=20800.7, ups=0.32, wpb=65536, bsz=128, num_updates=32900, lr=0.000174342, gnorm=1.168, loss_scale=16, train_wall=293, gb_free=19.9, wall=104727
2022-03-07 17:47:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:47:34 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 10.992 | nll_loss 10.148 | ppl 1134.73 | wps 41483.7 | wpb 510.9 | bsz 1 | num_updates 32901 | best_loss 7.696
2022-03-07 17:47:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 32901 updates
2022-03-07 17:47:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 17:47:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 17:47:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 169 @ 32901 updates, score 10.992) (writing took 3.077723774127662 seconds)
2022-03-07 17:47:37 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-03-07 17:47:37 | INFO | train | epoch 169 | loss 3.491 | nll_loss 2.144 | ppl 4.42 | wps 20509.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 32901 | lr 0.000174339 | gnorm 1.158 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 104738
2022-03-07 17:47:37 | INFO | fairseq.trainer | begin training epoch 170
2022-03-07 17:47:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:52:46 | INFO | train_inner | epoch 170:     99 / 196 loss=3.452, nll_loss=2.101, ppl=4.29, wps=20447.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=33000, lr=0.000174078, gnorm=1.139, loss_scale=32, train_wall=289, gb_free=19.9, wall=105047
2022-03-07 17:52:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:57:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:57:53 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 10.962 | nll_loss 10.115 | ppl 1109.06 | wps 41294.8 | wpb 510.9 | bsz 1 | num_updates 33096 | best_loss 7.696
2022-03-07 17:57:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 33096 updates
2022-03-07 17:57:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 17:57:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 17:57:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 170 @ 33096 updates, score 10.962) (writing took 3.148496838985011 seconds)
2022-03-07 17:57:56 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-03-07 17:57:56 | INFO | train | epoch 170 | loss 3.488 | nll_loss 2.141 | ppl 4.41 | wps 20613.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 33096 | lr 0.000173825 | gnorm 1.159 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 105357
2022-03-07 17:57:56 | INFO | fairseq.trainer | begin training epoch 171
2022-03-07 17:57:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:58:09 | INFO | train_inner | epoch 171:      4 / 196 loss=3.521, nll_loss=2.178, ppl=4.53, wps=20252.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=33100, lr=0.000173814, gnorm=1.181, loss_scale=16, train_wall=292, gb_free=19.9, wall=105369
2022-03-07 17:59:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:03:24 | INFO | train_inner | epoch 171:    105 / 196 loss=3.456, nll_loss=2.105, ppl=4.3, wps=20808.5, ups=0.32, wpb=65536, bsz=128, num_updates=33200, lr=0.000173553, gnorm=1.157, loss_scale=16, train_wall=293, gb_free=19.9, wall=105684
2022-03-07 18:06:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:08:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:08:12 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 11.003 | nll_loss 10.155 | ppl 1140.51 | wps 41662.5 | wpb 510.9 | bsz 1 | num_updates 33290 | best_loss 7.696
2022-03-07 18:08:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 33290 updates
2022-03-07 18:08:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 18:08:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 18:08:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 171 @ 33290 updates, score 11.003) (writing took 3.1367210939060897 seconds)
2022-03-07 18:08:15 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-03-07 18:08:15 | INFO | train | epoch 171 | loss 3.484 | nll_loss 2.136 | ppl 4.4 | wps 20513.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 33290 | lr 0.000173318 | gnorm 1.156 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 105976
2022-03-07 18:08:15 | INFO | fairseq.trainer | begin training epoch 172
2022-03-07 18:08:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:08:47 | INFO | train_inner | epoch 172:     10 / 196 loss=3.508, nll_loss=2.164, ppl=4.48, wps=20255.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=33300, lr=0.000173292, gnorm=1.152, loss_scale=16, train_wall=292, gb_free=19.9, wall=106007
2022-03-07 18:13:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:14:02 | INFO | train_inner | epoch 172:    111 / 196 loss=3.451, nll_loss=2.1, ppl=4.29, wps=20808.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=33400, lr=0.000173032, gnorm=1.141, loss_scale=16, train_wall=293, gb_free=19.9, wall=106322
2022-03-07 18:18:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:18:31 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 10.99 | nll_loss 10.144 | ppl 1131.74 | wps 41539.3 | wpb 510.9 | bsz 1 | num_updates 33485 | best_loss 7.696
2022-03-07 18:18:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 33485 updates
2022-03-07 18:18:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 18:18:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 18:18:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 172 @ 33485 updates, score 10.99) (writing took 3.1267312578856945 seconds)
2022-03-07 18:18:34 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-03-07 18:18:34 | INFO | train | epoch 172 | loss 3.482 | nll_loss 2.134 | ppl 4.39 | wps 20619.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 33485 | lr 0.000172812 | gnorm 1.15 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 106595
2022-03-07 18:18:34 | INFO | fairseq.trainer | begin training epoch 173
2022-03-07 18:18:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:18:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 18:19:24 | INFO | train_inner | epoch 173:     16 / 196 loss=3.507, nll_loss=2.163, ppl=4.48, wps=20261.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=33500, lr=0.000172774, gnorm=1.162, loss_scale=8, train_wall=292, gb_free=19.9, wall=106645
2022-03-07 18:24:36 | INFO | train_inner | epoch 173:    116 / 196 loss=3.457, nll_loss=2.107, ppl=4.31, wps=21019.4, ups=0.32, wpb=65536, bsz=128, num_updates=33600, lr=0.000172516, gnorm=1.145, loss_scale=8, train_wall=290, gb_free=19.9, wall=106956
2022-03-07 18:28:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:28:50 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 10.992 | nll_loss 10.152 | ppl 1137.93 | wps 41498.4 | wpb 510.9 | bsz 1 | num_updates 33680 | best_loss 7.696
2022-03-07 18:28:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 33680 updates
2022-03-07 18:28:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 18:28:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 18:28:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 173 @ 33680 updates, score 10.992) (writing took 3.084145351080224 seconds)
2022-03-07 18:28:53 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-03-07 18:28:53 | INFO | train | epoch 173 | loss 3.478 | nll_loss 2.13 | ppl 4.38 | wps 20625.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 33680 | lr 0.000172311 | gnorm 1.152 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 107213
2022-03-07 18:28:53 | INFO | fairseq.trainer | begin training epoch 174
2022-03-07 18:28:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:29:55 | INFO | train_inner | epoch 174:     20 / 196 loss=3.494, nll_loss=2.148, ppl=4.43, wps=20457.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=33700, lr=0.00017226, gnorm=1.154, loss_scale=16, train_wall=289, gb_free=19.9, wall=107276
2022-03-07 18:32:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:35:10 | INFO | train_inner | epoch 174:    121 / 196 loss=3.461, nll_loss=2.111, ppl=4.32, wps=20810.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=33800, lr=0.000172005, gnorm=1.141, loss_scale=16, train_wall=293, gb_free=19.9, wall=107591
2022-03-07 18:39:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:39:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:39:09 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 11.025 | nll_loss 10.179 | ppl 1159.29 | wps 41605.6 | wpb 510.9 | bsz 1 | num_updates 33874 | best_loss 7.696
2022-03-07 18:39:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 33874 updates
2022-03-07 18:39:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 18:39:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 18:39:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 174 @ 33874 updates, score 11.025) (writing took 3.173425463028252 seconds)
2022-03-07 18:39:12 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-03-07 18:39:12 | INFO | train | epoch 174 | loss 3.476 | nll_loss 2.127 | ppl 4.37 | wps 20514.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 33874 | lr 0.000171817 | gnorm 1.152 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 107832
2022-03-07 18:39:12 | INFO | fairseq.trainer | begin training epoch 175
2022-03-07 18:39:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:40:33 | INFO | train_inner | epoch 175:     26 / 196 loss=3.486, nll_loss=2.139, ppl=4.4, wps=20251, ups=0.31, wpb=65367, bsz=127.7, num_updates=33900, lr=0.000171751, gnorm=1.159, loss_scale=16, train_wall=292, gb_free=19.9, wall=107913
2022-03-07 18:45:45 | INFO | train_inner | epoch 175:    126 / 196 loss=3.459, nll_loss=2.109, ppl=4.31, wps=21019.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=34000, lr=0.000171499, gnorm=1.141, loss_scale=16, train_wall=290, gb_free=19.9, wall=108225
2022-03-07 18:45:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:49:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:49:28 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 10.993 | nll_loss 10.151 | ppl 1137.07 | wps 41381.3 | wpb 510.9 | bsz 1 | num_updates 34069 | best_loss 7.696
2022-03-07 18:49:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 34069 updates
2022-03-07 18:49:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 18:49:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 18:49:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 175 @ 34069 updates, score 10.993) (writing took 3.028680803021416 seconds)
2022-03-07 18:49:31 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-03-07 18:49:31 | INFO | train | epoch 175 | loss 3.472 | nll_loss 2.124 | ppl 4.36 | wps 20625.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 34069 | lr 0.000171325 | gnorm 1.151 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 108451
2022-03-07 18:49:31 | INFO | fairseq.trainer | begin training epoch 176
2022-03-07 18:49:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:51:07 | INFO | train_inner | epoch 176:     31 / 196 loss=3.481, nll_loss=2.134, ppl=4.39, wps=20268.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=34100, lr=0.000171247, gnorm=1.168, loss_scale=16, train_wall=292, gb_free=19.9, wall=108548
2022-03-07 18:52:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:56:22 | INFO | train_inner | epoch 176:    132 / 196 loss=3.46, nll_loss=2.11, ppl=4.32, wps=20811.4, ups=0.32, wpb=65536, bsz=128, num_updates=34200, lr=0.000170996, gnorm=1.171, loss_scale=16, train_wall=293, gb_free=19.9, wall=108863
2022-03-07 18:59:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:59:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:59:46 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 10.968 | nll_loss 10.125 | ppl 1116.47 | wps 41670.5 | wpb 510.9 | bsz 1 | num_updates 34263 | best_loss 7.696
2022-03-07 18:59:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 34263 updates
2022-03-07 18:59:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 18:59:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 18:59:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 176 @ 34263 updates, score 10.968) (writing took 3.0671646208502352 seconds)
2022-03-07 18:59:49 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-03-07 18:59:49 | INFO | train | epoch 176 | loss 3.469 | nll_loss 2.12 | ppl 4.35 | wps 20521.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 34263 | lr 0.000170839 | gnorm 1.17 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 109070
2022-03-07 18:59:49 | INFO | fairseq.trainer | begin training epoch 177
2022-03-07 18:59:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:01:45 | INFO | train_inner | epoch 177:     37 / 196 loss=3.478, nll_loss=2.13, ppl=4.38, wps=20266.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=34300, lr=0.000170747, gnorm=1.164, loss_scale=16, train_wall=292, gb_free=19.9, wall=109185
2022-03-07 19:06:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:07:00 | INFO | train_inner | epoch 177:    138 / 196 loss=3.457, nll_loss=2.107, ppl=4.31, wps=20811.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=34400, lr=0.000170499, gnorm=1.162, loss_scale=16, train_wall=293, gb_free=19.9, wall=109500
2022-03-07 19:10:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:10:05 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 11.014 | nll_loss 10.171 | ppl 1152.68 | wps 41512.9 | wpb 510.9 | bsz 1 | num_updates 34458 | best_loss 7.696
2022-03-07 19:10:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 34458 updates
2022-03-07 19:10:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 19:10:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 19:10:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 177 @ 34458 updates, score 11.014) (writing took 3.079957237932831 seconds)
2022-03-07 19:10:08 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-03-07 19:10:08 | INFO | train | epoch 177 | loss 3.466 | nll_loss 2.116 | ppl 4.34 | wps 20626.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 34458 | lr 0.000170355 | gnorm 1.161 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 109688
2022-03-07 19:10:08 | INFO | fairseq.trainer | begin training epoch 178
2022-03-07 19:10:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:12:19 | INFO | train_inner | epoch 178:     42 / 196 loss=3.472, nll_loss=2.124, ppl=4.36, wps=20461.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=34500, lr=0.000170251, gnorm=1.159, loss_scale=16, train_wall=289, gb_free=19.9, wall=109820
2022-03-07 19:13:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:17:34 | INFO | train_inner | epoch 178:    143 / 196 loss=3.455, nll_loss=2.105, ppl=4.3, wps=20808.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=34600, lr=0.000170005, gnorm=1.161, loss_scale=16, train_wall=293, gb_free=19.9, wall=110134
2022-03-07 19:19:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:20:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:20:24 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 11.024 | nll_loss 10.181 | ppl 1160.99 | wps 41739.2 | wpb 510.9 | bsz 1 | num_updates 34652 | best_loss 7.696
2022-03-07 19:20:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 34652 updates
2022-03-07 19:20:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 19:20:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 19:20:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 178 @ 34652 updates, score 11.024) (writing took 3.1549647769425064 seconds)
2022-03-07 19:20:27 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-03-07 19:20:27 | INFO | train | epoch 178 | loss 3.463 | nll_loss 2.113 | ppl 4.33 | wps 20518.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 34652 | lr 0.000169877 | gnorm 1.16 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 110307
2022-03-07 19:20:27 | INFO | fairseq.trainer | begin training epoch 179
2022-03-07 19:20:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:22:57 | INFO | train_inner | epoch 179:     48 / 196 loss=3.467, nll_loss=2.118, ppl=4.34, wps=20265.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=34700, lr=0.00016976, gnorm=1.148, loss_scale=16, train_wall=292, gb_free=19.9, wall=110457
2022-03-07 19:26:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:28:12 | INFO | train_inner | epoch 179:    149 / 196 loss=3.462, nll_loss=2.112, ppl=4.32, wps=20813.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=34800, lr=0.000169516, gnorm=1.156, loss_scale=16, train_wall=293, gb_free=19.9, wall=110772
2022-03-07 19:30:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:30:43 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 10.987 | nll_loss 10.134 | ppl 1123.97 | wps 41582.2 | wpb 510.9 | bsz 1 | num_updates 34847 | best_loss 7.696
2022-03-07 19:30:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 34847 updates
2022-03-07 19:30:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 19:30:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 19:30:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 179 @ 34847 updates, score 10.987) (writing took 3.1210403700824827 seconds)
2022-03-07 19:30:46 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-03-07 19:30:46 | INFO | train | epoch 179 | loss 3.459 | nll_loss 2.109 | ppl 4.31 | wps 20623.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 34847 | lr 0.000169402 | gnorm 1.151 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 110926
2022-03-07 19:30:46 | INFO | fairseq.trainer | begin training epoch 180
2022-03-07 19:30:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:33:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:33:34 | INFO | train_inner | epoch 180:     54 / 196 loss=3.453, nll_loss=2.103, ppl=4.3, wps=20252.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=34900, lr=0.000169273, gnorm=1.166, loss_scale=16, train_wall=292, gb_free=19.9, wall=111095
2022-03-07 19:38:46 | INFO | train_inner | epoch 180:    154 / 196 loss=3.46, nll_loss=2.11, ppl=4.32, wps=21014.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=35000, lr=0.000169031, gnorm=1.155, loss_scale=16, train_wall=290, gb_free=19.9, wall=111406
2022-03-07 19:40:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:40:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:41:02 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 10.967 | nll_loss 10.122 | ppl 1114.57 | wps 41525.9 | wpb 510.9 | bsz 1 | num_updates 35041 | best_loss 7.696
2022-03-07 19:41:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 35041 updates
2022-03-07 19:41:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 19:41:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 19:41:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 180 @ 35041 updates, score 10.967) (writing took 3.127527959179133 seconds)
2022-03-07 19:41:05 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-03-07 19:41:05 | INFO | train | epoch 180 | loss 3.456 | nll_loss 2.106 | ppl 4.31 | wps 20511.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 35041 | lr 0.000168932 | gnorm 1.157 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 111545
2022-03-07 19:41:05 | INFO | fairseq.trainer | begin training epoch 181
2022-03-07 19:41:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:44:09 | INFO | train_inner | epoch 181:     59 / 196 loss=3.451, nll_loss=2.1, ppl=4.29, wps=20247.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=35100, lr=0.00016879, gnorm=1.159, loss_scale=16, train_wall=292, gb_free=19.9, wall=111729
2022-03-07 19:46:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:49:24 | INFO | train_inner | epoch 181:    160 / 196 loss=3.458, nll_loss=2.108, ppl=4.31, wps=20806.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=35200, lr=0.00016855, gnorm=1.161, loss_scale=16, train_wall=293, gb_free=19.9, wall=112044
2022-03-07 19:51:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:51:21 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 10.993 | nll_loss 10.155 | ppl 1140.08 | wps 41611.5 | wpb 510.9 | bsz 1 | num_updates 35236 | best_loss 7.696
2022-03-07 19:51:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 35236 updates
2022-03-07 19:51:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 19:51:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 19:51:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 181 @ 35236 updates, score 10.993) (writing took 3.1339788620825857 seconds)
2022-03-07 19:51:24 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-03-07 19:51:24 | INFO | train | epoch 181 | loss 3.453 | nll_loss 2.103 | ppl 4.29 | wps 20615.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 35236 | lr 0.000168464 | gnorm 1.165 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 112164
2022-03-07 19:51:24 | INFO | fairseq.trainer | begin training epoch 182
2022-03-07 19:51:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:53:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:54:47 | INFO | train_inner | epoch 182:     65 / 196 loss=3.439, nll_loss=2.087, ppl=4.25, wps=20260.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=35300, lr=0.000168311, gnorm=1.152, loss_scale=16, train_wall=292, gb_free=19.9, wall=112367
2022-03-07 19:59:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 20:00:01 | INFO | train_inner | epoch 182:    166 / 196 loss=3.464, nll_loss=2.115, ppl=4.33, wps=20815.8, ups=0.32, wpb=65536, bsz=128, num_updates=35400, lr=0.000168073, gnorm=1.165, loss_scale=8, train_wall=293, gb_free=19.9, wall=112682
2022-03-07 20:01:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:01:39 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 11.057 | nll_loss 10.224 | ppl 1196.37 | wps 41637.3 | wpb 510.9 | bsz 1 | num_updates 35430 | best_loss 7.696
2022-03-07 20:01:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 35430 updates
2022-03-07 20:01:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 20:01:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 20:01:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 182 @ 35430 updates, score 11.057) (writing took 3.147196251899004 seconds)
2022-03-07 20:01:43 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-03-07 20:01:43 | INFO | train | epoch 182 | loss 3.451 | nll_loss 2.1 | ppl 4.29 | wps 20519.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 35430 | lr 0.000168002 | gnorm 1.159 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 112783
2022-03-07 20:01:43 | INFO | fairseq.trainer | begin training epoch 183
2022-03-07 20:01:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:05:21 | INFO | train_inner | epoch 183:     70 / 196 loss=3.437, nll_loss=2.085, ppl=4.24, wps=20441.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=35500, lr=0.000167836, gnorm=1.17, loss_scale=8, train_wall=289, gb_free=19.9, wall=113002
2022-03-07 20:10:33 | INFO | train_inner | epoch 183:    170 / 196 loss=3.466, nll_loss=2.117, ppl=4.34, wps=21006.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=35600, lr=0.0001676, gnorm=1.172, loss_scale=16, train_wall=290, gb_free=19.9, wall=113313
2022-03-07 20:11:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:11:59 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 10.983 | nll_loss 10.139 | ppl 1127.33 | wps 41165.2 | wpb 510.9 | bsz 1 | num_updates 35626 | best_loss 7.696
2022-03-07 20:11:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 35626 updates
2022-03-07 20:11:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 20:12:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 20:12:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 183 @ 35626 updates, score 10.983) (writing took 3.1099895539227873 seconds)
2022-03-07 20:12:02 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-03-07 20:12:02 | INFO | train | epoch 183 | loss 3.448 | nll_loss 2.097 | ppl 4.28 | wps 20711 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 35626 | lr 0.000167539 | gnorm 1.166 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 113402
2022-03-07 20:12:02 | INFO | fairseq.trainer | begin training epoch 184
2022-03-07 20:12:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:12:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:15:56 | INFO | train_inner | epoch 184:     75 / 196 loss=3.426, nll_loss=2.072, ppl=4.21, wps=20247.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=35700, lr=0.000167365, gnorm=1.159, loss_scale=16, train_wall=292, gb_free=19.9, wall=113636
2022-03-07 20:19:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:21:11 | INFO | train_inner | epoch 184:    176 / 196 loss=3.466, nll_loss=2.117, ppl=4.34, wps=20809.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=35800, lr=0.000167132, gnorm=1.168, loss_scale=16, train_wall=293, gb_free=19.9, wall=113951
2022-03-07 20:22:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:22:18 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 11.043 | nll_loss 10.208 | ppl 1183.06 | wps 41219.6 | wpb 510.9 | bsz 1 | num_updates 35820 | best_loss 7.696
2022-03-07 20:22:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 35820 updates
2022-03-07 20:22:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 20:22:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 20:22:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 184 @ 35820 updates, score 11.043) (writing took 3.1272444240748882 seconds)
2022-03-07 20:22:21 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-03-07 20:22:21 | INFO | train | epoch 184 | loss 3.446 | nll_loss 2.094 | ppl 4.27 | wps 20510.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 35820 | lr 0.000167085 | gnorm 1.165 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 114021
2022-03-07 20:22:21 | INFO | fairseq.trainer | begin training epoch 185
2022-03-07 20:22:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:26:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:26:34 | INFO | train_inner | epoch 185:     81 / 196 loss=3.421, nll_loss=2.067, ppl=4.19, wps=20239.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=35900, lr=0.000166899, gnorm=1.176, loss_scale=16, train_wall=292, gb_free=19.9, wall=114274
2022-03-07 20:31:46 | INFO | train_inner | epoch 185:    181 / 196 loss=3.466, nll_loss=2.118, ppl=4.34, wps=21006.1, ups=0.32, wpb=65536, bsz=128, num_updates=36000, lr=0.000166667, gnorm=1.174, loss_scale=16, train_wall=290, gb_free=19.9, wall=114586
2022-03-07 20:32:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:32:37 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 11.052 | nll_loss 10.213 | ppl 1186.73 | wps 41447.6 | wpb 510.9 | bsz 1 | num_updates 36015 | best_loss 7.696
2022-03-07 20:32:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 36015 updates
2022-03-07 20:32:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 20:32:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 20:32:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 185 @ 36015 updates, score 11.052) (writing took 3.1410879271570593 seconds)
2022-03-07 20:32:40 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-03-07 20:32:40 | INFO | train | epoch 185 | loss 3.442 | nll_loss 2.09 | ppl 4.26 | wps 20609 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 36015 | lr 0.000166632 | gnorm 1.174 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 114641
2022-03-07 20:32:40 | INFO | fairseq.trainer | begin training epoch 186
2022-03-07 20:32:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:33:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:37:09 | INFO | train_inner | epoch 186:     86 / 196 loss=3.415, nll_loss=2.06, ppl=4.17, wps=20246.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=36100, lr=0.000166436, gnorm=1.158, loss_scale=16, train_wall=292, gb_free=19.9, wall=114909
2022-03-07 20:39:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:42:24 | INFO | train_inner | epoch 186:    187 / 196 loss=3.465, nll_loss=2.117, ppl=4.34, wps=20794.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=36200, lr=0.000166206, gnorm=1.154, loss_scale=16, train_wall=293, gb_free=19.9, wall=115224
2022-03-07 20:42:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:42:56 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 10.999 | nll_loss 10.157 | ppl 1141.57 | wps 41547.3 | wpb 510.9 | bsz 1 | num_updates 36209 | best_loss 7.696
2022-03-07 20:42:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 36209 updates
2022-03-07 20:42:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 20:42:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 20:42:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 186 @ 36209 updates, score 10.999) (writing took 2.9900480248034 seconds)
2022-03-07 20:42:59 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-03-07 20:42:59 | INFO | train | epoch 186 | loss 3.439 | nll_loss 2.087 | ppl 4.25 | wps 20506.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 36209 | lr 0.000166185 | gnorm 1.156 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 115260
2022-03-07 20:42:59 | INFO | fairseq.trainer | begin training epoch 187
2022-03-07 20:42:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:46:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:47:47 | INFO | train_inner | epoch 187:     92 / 196 loss=3.415, nll_loss=2.061, ppl=4.17, wps=20255.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=36300, lr=0.000165977, gnorm=1.168, loss_scale=16, train_wall=292, gb_free=19.9, wall=115547
2022-03-07 20:52:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 20:53:02 | INFO | train_inner | epoch 187:    193 / 196 loss=3.465, nll_loss=2.117, ppl=4.34, wps=20802.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=36400, lr=0.000165748, gnorm=1.189, loss_scale=8, train_wall=293, gb_free=19.9, wall=115862
2022-03-07 20:53:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:53:15 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 11.053 | nll_loss 10.214 | ppl 1187.98 | wps 41703.2 | wpb 510.9 | bsz 1 | num_updates 36403 | best_loss 7.696
2022-03-07 20:53:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 36403 updates
2022-03-07 20:53:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 20:53:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 20:53:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 187 @ 36403 updates, score 11.053) (writing took 3.075822912855074 seconds)
2022-03-07 20:53:19 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-03-07 20:53:19 | INFO | train | epoch 187 | loss 3.438 | nll_loss 2.085 | ppl 4.24 | wps 20509.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 36403 | lr 0.000165742 | gnorm 1.177 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 115879
2022-03-07 20:53:19 | INFO | fairseq.trainer | begin training epoch 188
2022-03-07 20:53:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:58:21 | INFO | train_inner | epoch 188:     97 / 196 loss=3.401, nll_loss=2.044, ppl=4.13, wps=20455.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=36500, lr=0.000165521, gnorm=1.155, loss_scale=8, train_wall=289, gb_free=19.9, wall=116181
2022-03-07 21:03:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:03:34 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 10.984 | nll_loss 10.144 | ppl 1131.24 | wps 41492.7 | wpb 510.9 | bsz 1 | num_updates 36599 | best_loss 7.696
2022-03-07 21:03:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 36599 updates
2022-03-07 21:03:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 21:03:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 21:03:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 188 @ 36599 updates, score 10.984) (writing took 3.0776675529778004 seconds)
2022-03-07 21:03:37 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-03-07 21:03:37 | INFO | train | epoch 188 | loss 3.434 | nll_loss 2.082 | ppl 4.23 | wps 20728.1 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 36599 | lr 0.000165297 | gnorm 1.163 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 116498
2022-03-07 21:03:37 | INFO | fairseq.trainer | begin training epoch 189
2022-03-07 21:03:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:03:41 | INFO | train_inner | epoch 189:      1 / 196 loss=3.467, nll_loss=2.119, ppl=4.34, wps=20462, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=36600, lr=0.000165295, gnorm=1.17, loss_scale=16, train_wall=289, gb_free=19.9, wall=116501
2022-03-07 21:05:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:08:56 | INFO | train_inner | epoch 189:    102 / 196 loss=3.398, nll_loss=2.041, ppl=4.12, wps=20806, ups=0.32, wpb=65536, bsz=128, num_updates=36700, lr=0.00016507, gnorm=1.158, loss_scale=16, train_wall=293, gb_free=19.9, wall=116816
2022-03-07 21:12:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:13:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:13:53 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 11.027 | nll_loss 10.189 | ppl 1167.73 | wps 41632.4 | wpb 510.9 | bsz 1 | num_updates 36793 | best_loss 7.696
2022-03-07 21:13:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 36793 updates
2022-03-07 21:13:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 21:13:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 21:13:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 189 @ 36793 updates, score 11.027) (writing took 3.09694298915565 seconds)
2022-03-07 21:13:56 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-03-07 21:13:56 | INFO | train | epoch 189 | loss 3.431 | nll_loss 2.078 | ppl 4.22 | wps 20509 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 36793 | lr 0.000164861 | gnorm 1.16 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 117117
2022-03-07 21:13:56 | INFO | fairseq.trainer | begin training epoch 190
2022-03-07 21:13:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:14:18 | INFO | train_inner | epoch 190:      7 / 196 loss=3.459, nll_loss=2.11, ppl=4.32, wps=20248.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=36800, lr=0.000164845, gnorm=1.162, loss_scale=16, train_wall=292, gb_free=19.9, wall=117139
2022-03-07 21:19:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:19:33 | INFO | train_inner | epoch 190:    108 / 196 loss=3.398, nll_loss=2.041, ppl=4.11, wps=20813.2, ups=0.32, wpb=65536, bsz=128, num_updates=36900, lr=0.000164622, gnorm=1.16, loss_scale=16, train_wall=293, gb_free=19.9, wall=117454
2022-03-07 21:24:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:24:12 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 11.048 | nll_loss 10.206 | ppl 1181.14 | wps 41587.5 | wpb 510.9 | bsz 1 | num_updates 36988 | best_loss 7.696
2022-03-07 21:24:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 36988 updates
2022-03-07 21:24:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 21:24:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 21:24:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 190 @ 36988 updates, score 11.048) (writing took 3.136673000175506 seconds)
2022-03-07 21:24:15 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-03-07 21:24:15 | INFO | train | epoch 190 | loss 3.428 | nll_loss 2.075 | ppl 4.21 | wps 20621.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 36988 | lr 0.000164426 | gnorm 1.173 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 117736
2022-03-07 21:24:15 | INFO | fairseq.trainer | begin training epoch 191
2022-03-07 21:24:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:24:53 | INFO | train_inner | epoch 191:     12 / 196 loss=3.456, nll_loss=2.106, ppl=4.31, wps=20452.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=37000, lr=0.000164399, gnorm=1.187, loss_scale=16, train_wall=289, gb_free=19.9, wall=117773
2022-03-07 21:26:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:30:08 | INFO | train_inner | epoch 191:    113 / 196 loss=3.4, nll_loss=2.043, ppl=4.12, wps=20799.7, ups=0.32, wpb=65536, bsz=128, num_updates=37100, lr=0.000164177, gnorm=1.165, loss_scale=16, train_wall=293, gb_free=19.9, wall=118088
2022-03-07 21:30:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 21:34:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:34:31 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 11.036 | nll_loss 10.199 | ppl 1175.64 | wps 41691.3 | wpb 510.9 | bsz 1 | num_updates 37182 | best_loss 7.696
2022-03-07 21:34:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 37182 updates
2022-03-07 21:34:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 21:34:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 21:34:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 191 @ 37182 updates, score 11.036) (writing took 3.104912362061441 seconds)
2022-03-07 21:34:34 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-03-07 21:34:34 | INFO | train | epoch 191 | loss 3.426 | nll_loss 2.073 | ppl 4.21 | wps 20518.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 37182 | lr 0.000163996 | gnorm 1.174 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 118354
2022-03-07 21:34:34 | INFO | fairseq.trainer | begin training epoch 192
2022-03-07 21:34:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:35:30 | INFO | train_inner | epoch 192:     18 / 196 loss=3.451, nll_loss=2.101, ppl=4.29, wps=20271.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=37200, lr=0.000163956, gnorm=1.187, loss_scale=8, train_wall=292, gb_free=19.9, wall=118411
2022-03-07 21:40:42 | INFO | train_inner | epoch 192:    118 / 196 loss=3.405, nll_loss=2.049, ppl=4.14, wps=21020.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=37300, lr=0.000163737, gnorm=1.161, loss_scale=16, train_wall=290, gb_free=19.9, wall=118722
2022-03-07 21:44:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:44:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:44:50 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 11.056 | nll_loss 10.217 | ppl 1190.47 | wps 41311.5 | wpb 510.9 | bsz 1 | num_updates 37377 | best_loss 7.696
2022-03-07 21:44:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 37377 updates
2022-03-07 21:44:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 21:44:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 21:44:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 192 @ 37377 updates, score 11.056) (writing took 3.113701695809141 seconds)
2022-03-07 21:44:53 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-03-07 21:44:53 | INFO | train | epoch 192 | loss 3.423 | nll_loss 2.07 | ppl 4.2 | wps 20623.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 37377 | lr 0.000163568 | gnorm 1.174 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 118973
2022-03-07 21:44:53 | INFO | fairseq.trainer | begin training epoch 193
2022-03-07 21:44:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:46:05 | INFO | train_inner | epoch 193:     23 / 196 loss=3.439, nll_loss=2.087, ppl=4.25, wps=20258.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=37400, lr=0.000163517, gnorm=1.183, loss_scale=16, train_wall=292, gb_free=19.9, wall=119045
2022-03-07 21:51:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:51:20 | INFO | train_inner | epoch 193:    124 / 196 loss=3.404, nll_loss=2.048, ppl=4.14, wps=20808.9, ups=0.32, wpb=65536, bsz=128, num_updates=37500, lr=0.000163299, gnorm=1.16, loss_scale=16, train_wall=293, gb_free=19.9, wall=119360
2022-03-07 21:55:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:55:09 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 11.076 | nll_loss 10.243 | ppl 1211.56 | wps 41872.3 | wpb 510.9 | bsz 1 | num_updates 37572 | best_loss 7.696
2022-03-07 21:55:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 37572 updates
2022-03-07 21:55:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 21:55:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 21:55:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 193 @ 37572 updates, score 11.076) (writing took 3.137419633800164 seconds)
2022-03-07 21:55:12 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-03-07 21:55:12 | INFO | train | epoch 193 | loss 3.421 | nll_loss 2.067 | ppl 4.19 | wps 20621.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 37572 | lr 0.000163143 | gnorm 1.163 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 119592
2022-03-07 21:55:12 | INFO | fairseq.trainer | begin training epoch 194
2022-03-07 21:55:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:56:39 | INFO | train_inner | epoch 194:     28 / 196 loss=3.435, nll_loss=2.084, ppl=4.24, wps=20458.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=37600, lr=0.000163082, gnorm=1.165, loss_scale=16, train_wall=289, gb_free=19.9, wall=119680
2022-03-07 21:57:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:01:54 | INFO | train_inner | epoch 194:    129 / 196 loss=3.405, nll_loss=2.049, ppl=4.14, wps=20821, ups=0.32, wpb=65532.4, bsz=128, num_updates=37700, lr=0.000162866, gnorm=1.164, loss_scale=16, train_wall=293, gb_free=19.9, wall=119994
2022-03-07 22:04:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:05:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:05:27 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 11.082 | nll_loss 10.243 | ppl 1211.5 | wps 41737.2 | wpb 510.9 | bsz 1 | num_updates 37766 | best_loss 7.696
2022-03-07 22:05:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 37766 updates
2022-03-07 22:05:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 22:05:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 22:05:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 194 @ 37766 updates, score 11.082) (writing took 3.1237275681924075 seconds)
2022-03-07 22:05:30 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-03-07 22:05:30 | INFO | train | epoch 194 | loss 3.418 | nll_loss 2.064 | ppl 4.18 | wps 20524.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 37766 | lr 0.000162723 | gnorm 1.18 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 120211
2022-03-07 22:05:30 | INFO | fairseq.trainer | begin training epoch 195
2022-03-07 22:05:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:07:17 | INFO | train_inner | epoch 195:     34 / 196 loss=3.423, nll_loss=2.069, ppl=4.2, wps=20262.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=37800, lr=0.00016265, gnorm=1.186, loss_scale=16, train_wall=292, gb_free=19.9, wall=120317
2022-03-07 22:11:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:12:31 | INFO | train_inner | epoch 195:    135 / 196 loss=3.409, nll_loss=2.054, ppl=4.15, wps=20818.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=37900, lr=0.000162435, gnorm=1.179, loss_scale=16, train_wall=293, gb_free=19.9, wall=120632
2022-03-07 22:15:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:15:46 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 11.11 | nll_loss 10.277 | ppl 1240.46 | wps 41452.7 | wpb 510.9 | bsz 1 | num_updates 37961 | best_loss 7.696
2022-03-07 22:15:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 37961 updates
2022-03-07 22:15:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 22:15:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 22:15:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 195 @ 37961 updates, score 11.11) (writing took 3.106448167003691 seconds)
2022-03-07 22:15:49 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-03-07 22:15:49 | INFO | train | epoch 195 | loss 3.415 | nll_loss 2.061 | ppl 4.17 | wps 20626.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 37961 | lr 0.000162305 | gnorm 1.176 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 120830
2022-03-07 22:15:49 | INFO | fairseq.trainer | begin training epoch 196
2022-03-07 22:15:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:17:52 | INFO | train_inner | epoch 196:     39 / 196 loss=3.419, nll_loss=2.066, ppl=4.19, wps=20400.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=38000, lr=0.000162221, gnorm=1.185, loss_scale=16, train_wall=289, gb_free=19.9, wall=120952
2022-03-07 22:18:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:23:09 | INFO | train_inner | epoch 196:    140 / 196 loss=3.406, nll_loss=2.05, ppl=4.14, wps=20651.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=38100, lr=0.000162008, gnorm=1.168, loss_scale=16, train_wall=293, gb_free=19.9, wall=121269
2022-03-07 22:24:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:26:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:26:09 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 11.072 | nll_loss 10.233 | ppl 1203.08 | wps 41766.1 | wpb 510.9 | bsz 1 | num_updates 38155 | best_loss 7.696
2022-03-07 22:26:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 38155 updates
2022-03-07 22:26:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 22:26:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 22:26:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 196 @ 38155 updates, score 11.072) (writing took 3.0992169310338795 seconds)
2022-03-07 22:26:13 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-03-07 22:26:13 | INFO | train | epoch 196 | loss 3.413 | nll_loss 2.058 | ppl 4.16 | wps 20367.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 38155 | lr 0.000161892 | gnorm 1.18 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 121453
2022-03-07 22:26:13 | INFO | fairseq.trainer | begin training epoch 197
2022-03-07 22:26:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:28:33 | INFO | train_inner | epoch 197:     45 / 196 loss=3.415, nll_loss=2.061, ppl=4.17, wps=20180.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=38200, lr=0.000161796, gnorm=1.195, loss_scale=16, train_wall=292, gb_free=19.9, wall=121593
2022-03-07 22:31:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:32:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 22:33:51 | INFO | train_inner | epoch 197:    147 / 196 loss=3.412, nll_loss=2.058, ppl=4.16, wps=20609.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=38300, lr=0.000161585, gnorm=1.177, loss_scale=8, train_wall=296, gb_free=19.9, wall=121911
2022-03-07 22:36:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:36:28 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 11.076 | nll_loss 10.239 | ppl 1208.67 | wps 41395.2 | wpb 510.9 | bsz 1 | num_updates 38349 | best_loss 7.696
2022-03-07 22:36:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 38349 updates
2022-03-07 22:36:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 22:36:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 22:36:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 197 @ 38349 updates, score 11.076) (writing took 3.133287865901366 seconds)
2022-03-07 22:36:31 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-03-07 22:36:31 | INFO | train | epoch 197 | loss 3.409 | nll_loss 2.054 | ppl 4.15 | wps 20515.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 38349 | lr 0.000161482 | gnorm 1.184 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 122072
2022-03-07 22:36:31 | INFO | fairseq.trainer | begin training epoch 198
2022-03-07 22:36:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:39:11 | INFO | train_inner | epoch 198:     51 / 196 loss=3.404, nll_loss=2.049, ppl=4.14, wps=20447.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=38400, lr=0.000161374, gnorm=1.175, loss_scale=16, train_wall=289, gb_free=19.9, wall=122231
2022-03-07 22:44:22 | INFO | train_inner | epoch 198:    151 / 196 loss=3.41, nll_loss=2.055, ppl=4.16, wps=21018.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=38500, lr=0.000161165, gnorm=1.163, loss_scale=16, train_wall=290, gb_free=19.9, wall=122543
2022-03-07 22:45:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:46:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:46:47 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 11.054 | nll_loss 10.216 | ppl 1189.67 | wps 41222.2 | wpb 510.9 | bsz 1 | num_updates 38544 | best_loss 7.696
2022-03-07 22:46:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 38544 updates
2022-03-07 22:46:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 22:46:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 22:46:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 198 @ 38544 updates, score 11.054) (writing took 3.1076840069144964 seconds)
2022-03-07 22:46:50 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-03-07 22:46:50 | INFO | train | epoch 198 | loss 3.408 | nll_loss 2.053 | ppl 4.15 | wps 20620.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 38544 | lr 0.000161073 | gnorm 1.175 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 122691
2022-03-07 22:46:50 | INFO | fairseq.trainer | begin training epoch 199
2022-03-07 22:46:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:49:45 | INFO | train_inner | epoch 199:     56 / 196 loss=3.403, nll_loss=2.047, ppl=4.13, wps=20262.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=38600, lr=0.000160956, gnorm=1.192, loss_scale=16, train_wall=292, gb_free=19.9, wall=122865
2022-03-07 22:52:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:55:00 | INFO | train_inner | epoch 199:    157 / 196 loss=3.412, nll_loss=2.057, ppl=4.16, wps=20812.6, ups=0.32, wpb=65536, bsz=128, num_updates=38700, lr=0.000160748, gnorm=1.177, loss_scale=16, train_wall=293, gb_free=19.9, wall=123180
2022-03-07 22:57:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:57:06 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 11.056 | nll_loss 10.22 | ppl 1192.55 | wps 41435 | wpb 510.9 | bsz 1 | num_updates 38739 | best_loss 7.696
2022-03-07 22:57:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 38739 updates
2022-03-07 22:57:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 22:57:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 22:57:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 199 @ 38739 updates, score 11.056) (writing took 3.171956499107182 seconds)
2022-03-07 22:57:09 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-03-07 22:57:09 | INFO | train | epoch 199 | loss 3.406 | nll_loss 2.05 | ppl 4.14 | wps 20622.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 38739 | lr 0.000160667 | gnorm 1.182 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 123310
2022-03-07 22:57:09 | INFO | fairseq.trainer | begin training epoch 200
2022-03-07 22:57:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:59:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:00:23 | INFO | train_inner | epoch 200:     62 / 196 loss=3.4, nll_loss=2.044, ppl=4.12, wps=20256.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=38800, lr=0.00016054, gnorm=1.181, loss_scale=16, train_wall=292, gb_free=19.9, wall=123503
2022-03-07 23:05:34 | INFO | train_inner | epoch 200:    162 / 196 loss=3.41, nll_loss=2.055, ppl=4.16, wps=21020.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=38900, lr=0.000160334, gnorm=1.201, loss_scale=16, train_wall=290, gb_free=19.9, wall=123815
2022-03-07 23:05:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:07:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:07:25 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 11.089 | nll_loss 10.253 | ppl 1220.18 | wps 41557.1 | wpb 510.9 | bsz 1 | num_updates 38933 | best_loss 7.696
2022-03-07 23:07:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 38933 updates
2022-03-07 23:07:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 23:07:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 23:07:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 200 @ 38933 updates, score 11.089) (writing took 3.098361280048266 seconds)
2022-03-07 23:07:28 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-03-07 23:07:28 | INFO | train | epoch 200 | loss 3.402 | nll_loss 2.047 | ppl 4.13 | wps 20518.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 38933 | lr 0.000160266 | gnorm 1.188 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 123928
2022-03-07 23:07:28 | INFO | fairseq.trainer | begin training epoch 201
2022-03-07 23:07:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:10:57 | INFO | train_inner | epoch 201:     67 / 196 loss=3.391, nll_loss=2.034, ppl=4.1, wps=20259.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=39000, lr=0.000160128, gnorm=1.163, loss_scale=16, train_wall=292, gb_free=19.9, wall=124137
2022-03-07 23:12:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:16:12 | INFO | train_inner | epoch 201:    168 / 196 loss=3.41, nll_loss=2.055, ppl=4.16, wps=20814.6, ups=0.32, wpb=65536, bsz=128, num_updates=39100, lr=0.000159923, gnorm=1.169, loss_scale=16, train_wall=293, gb_free=19.9, wall=124452
2022-03-07 23:17:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:17:44 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 11.083 | nll_loss 10.251 | ppl 1218.59 | wps 41604.3 | wpb 510.9 | bsz 1 | num_updates 39128 | best_loss 7.696
2022-03-07 23:17:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 39128 updates
2022-03-07 23:17:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 23:17:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 23:17:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 201 @ 39128 updates, score 11.083) (writing took 3.1257758049760014 seconds)
2022-03-07 23:17:47 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-03-07 23:17:47 | INFO | train | epoch 201 | loss 3.4 | nll_loss 2.044 | ppl 4.12 | wps 20625.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 39128 | lr 0.000159866 | gnorm 1.163 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 124547
2022-03-07 23:17:47 | INFO | fairseq.trainer | begin training epoch 202
2022-03-07 23:17:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:19:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:21:35 | INFO | train_inner | epoch 202:     73 / 196 loss=3.384, nll_loss=2.026, ppl=4.07, wps=20256.3, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=39200, lr=0.000159719, gnorm=1.171, loss_scale=16, train_wall=292, gb_free=19.9, wall=124775
2022-03-07 23:26:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:26:49 | INFO | train_inner | epoch 202:    174 / 196 loss=3.416, nll_loss=2.062, ppl=4.17, wps=20818.2, ups=0.32, wpb=65536, bsz=128, num_updates=39300, lr=0.000159516, gnorm=1.19, loss_scale=16, train_wall=293, gb_free=19.9, wall=125090
2022-03-07 23:27:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:28:02 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 11.068 | nll_loss 10.232 | ppl 1202.72 | wps 41607.7 | wpb 510.9 | bsz 1 | num_updates 39322 | best_loss 7.696
2022-03-07 23:28:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 39322 updates
2022-03-07 23:28:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 23:28:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 23:28:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 202 @ 39322 updates, score 11.068) (writing took 3.10774008394219 seconds)
2022-03-07 23:28:06 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-03-07 23:28:06 | INFO | train | epoch 202 | loss 3.398 | nll_loss 2.041 | ppl 4.12 | wps 20518.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 39322 | lr 0.000159471 | gnorm 1.181 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 125166
2022-03-07 23:28:06 | INFO | fairseq.trainer | begin training epoch 203
2022-03-07 23:28:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:32:09 | INFO | train_inner | epoch 203:     78 / 196 loss=3.379, nll_loss=2.02, ppl=4.06, wps=20442.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=39400, lr=0.000159313, gnorm=1.174, loss_scale=16, train_wall=289, gb_free=19.9, wall=125409
2022-03-07 23:33:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:36:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 23:37:27 | INFO | train_inner | epoch 203:    180 / 196 loss=3.414, nll_loss=2.06, ppl=4.17, wps=20607.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=39500, lr=0.000159111, gnorm=1.195, loss_scale=8, train_wall=296, gb_free=19.9, wall=125727
2022-03-07 23:38:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:38:21 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 11.084 | nll_loss 10.247 | ppl 1215.47 | wps 41700.9 | wpb 510.9 | bsz 1 | num_updates 39516 | best_loss 7.696
2022-03-07 23:38:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 39516 updates
2022-03-07 23:38:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 23:38:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 23:38:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 203 @ 39516 updates, score 11.084) (writing took 3.1021575280465186 seconds)
2022-03-07 23:38:25 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-03-07 23:38:25 | INFO | train | epoch 203 | loss 3.395 | nll_loss 2.038 | ppl 4.11 | wps 20511.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 39516 | lr 0.000159079 | gnorm 1.189 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 125785
2022-03-07 23:38:25 | INFO | fairseq.trainer | begin training epoch 204
2022-03-07 23:38:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:42:47 | INFO | train_inner | epoch 204:     84 / 196 loss=3.374, nll_loss=2.015, ppl=4.04, wps=20457.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=39600, lr=0.00015891, gnorm=1.179, loss_scale=8, train_wall=289, gb_free=19.9, wall=126047
2022-03-07 23:47:58 | INFO | train_inner | epoch 204:    184 / 196 loss=3.418, nll_loss=2.064, ppl=4.18, wps=21015.9, ups=0.32, wpb=65536, bsz=128, num_updates=39700, lr=0.00015871, gnorm=1.202, loss_scale=16, train_wall=290, gb_free=19.9, wall=126359
2022-03-07 23:48:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:48:40 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 11.044 | nll_loss 10.205 | ppl 1180.4 | wps 41578.4 | wpb 510.9 | bsz 1 | num_updates 39712 | best_loss 7.696
2022-03-07 23:48:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 39712 updates
2022-03-07 23:48:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 23:48:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 23:48:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 204 @ 39712 updates, score 11.044) (writing took 3.125774048967287 seconds)
2022-03-07 23:48:43 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-03-07 23:48:43 | INFO | train | epoch 204 | loss 3.394 | nll_loss 2.037 | ppl 4.1 | wps 20726.3 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 39712 | lr 0.000158686 | gnorm 1.186 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 126404
2022-03-07 23:48:44 | INFO | fairseq.trainer | begin training epoch 205
2022-03-07 23:48:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:50:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:53:21 | INFO | train_inner | epoch 205:     89 / 196 loss=3.366, nll_loss=2.006, ppl=4.02, wps=20255.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=39800, lr=0.000158511, gnorm=1.164, loss_scale=16, train_wall=292, gb_free=19.9, wall=126681
2022-03-07 23:56:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:58:36 | INFO | train_inner | epoch 205:    190 / 196 loss=3.417, nll_loss=2.063, ppl=4.18, wps=20805.4, ups=0.32, wpb=65536, bsz=128, num_updates=39900, lr=0.000158312, gnorm=1.179, loss_scale=16, train_wall=293, gb_free=19.9, wall=126996
2022-03-07 23:58:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:58:59 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 11.1 | nll_loss 10.269 | ppl 1233.74 | wps 41265.9 | wpb 510.9 | bsz 1 | num_updates 39906 | best_loss 7.696
2022-03-07 23:58:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 39906 updates
2022-03-07 23:58:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 23:59:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-07 23:59:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 205 @ 39906 updates, score 11.1) (writing took 3.122435153927654 seconds)
2022-03-07 23:59:03 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-03-07 23:59:03 | INFO | train | epoch 205 | loss 3.39 | nll_loss 2.033 | ppl 4.09 | wps 20509.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 39906 | lr 0.0001583 | gnorm 1.169 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 127023
2022-03-07 23:59:03 | INFO | fairseq.trainer | begin training epoch 206
2022-03-07 23:59:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:03:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:03:59 | INFO | train_inner | epoch 206:     95 / 196 loss=3.361, nll_loss=2, ppl=4, wps=20236.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=40000, lr=0.000158114, gnorm=1.158, loss_scale=16, train_wall=292, gb_free=19.9, wall=127319
2022-03-08 00:09:11 | INFO | train_inner | epoch 206:    195 / 196 loss=3.42, nll_loss=2.067, ppl=4.19, wps=21001.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=40100, lr=0.000157917, gnorm=1.19, loss_scale=16, train_wall=290, gb_free=19.9, wall=127632
2022-03-08 00:09:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:09:19 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 11.079 | nll_loss 10.239 | ppl 1208.88 | wps 41675.9 | wpb 510.9 | bsz 1 | num_updates 40101 | best_loss 7.696
2022-03-08 00:09:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 40101 updates
2022-03-08 00:09:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 00:09:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 00:09:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 206 @ 40101 updates, score 11.079) (writing took 3.091491356957704 seconds)
2022-03-08 00:09:22 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-03-08 00:09:22 | INFO | train | epoch 206 | loss 3.389 | nll_loss 2.031 | ppl 4.09 | wps 20605.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 40101 | lr 0.000157915 | gnorm 1.174 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 127642
2022-03-08 00:09:22 | INFO | fairseq.trainer | begin training epoch 207
2022-03-08 00:09:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:10:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:14:34 | INFO | train_inner | epoch 207:    100 / 196 loss=3.349, nll_loss=1.988, ppl=3.97, wps=20254.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=40200, lr=0.00015772, gnorm=1.173, loss_scale=16, train_wall=292, gb_free=19.9, wall=127954
2022-03-08 00:17:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:18:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 00:19:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:19:38 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 11.081 | nll_loss 10.246 | ppl 1214.31 | wps 41674.1 | wpb 510.9 | bsz 1 | num_updates 40294 | best_loss 7.696
2022-03-08 00:19:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 40294 updates
2022-03-08 00:19:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 00:19:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 00:19:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 207 @ 40294 updates, score 11.081) (writing took 3.1665716329589486 seconds)
2022-03-08 00:19:41 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-03-08 00:19:41 | INFO | train | epoch 207 | loss 3.386 | nll_loss 2.028 | ppl 4.08 | wps 20402.6 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 40294 | lr 0.000157536 | gnorm 1.178 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 128261
2022-03-08 00:19:41 | INFO | fairseq.trainer | begin training epoch 208
2022-03-08 00:19:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:20:00 | INFO | train_inner | epoch 208:      6 / 196 loss=3.419, nll_loss=2.066, ppl=4.19, wps=20057.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=40300, lr=0.000157524, gnorm=1.183, loss_scale=8, train_wall=295, gb_free=19.9, wall=128280
2022-03-08 00:25:11 | INFO | train_inner | epoch 208:    106 / 196 loss=3.358, nll_loss=1.997, ppl=3.99, wps=21031.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=40400, lr=0.000157329, gnorm=1.176, loss_scale=8, train_wall=290, gb_free=19.9, wall=128592
2022-03-08 00:29:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:29:56 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 11.103 | nll_loss 10.266 | ppl 1231.57 | wps 41594.8 | wpb 510.9 | bsz 1 | num_updates 40490 | best_loss 7.696
2022-03-08 00:29:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 40490 updates
2022-03-08 00:29:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 00:29:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 00:30:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 208 @ 40490 updates, score 11.103) (writing took 3.084038374014199 seconds)
2022-03-08 00:30:00 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-03-08 00:30:00 | INFO | train | epoch 208 | loss 3.384 | nll_loss 2.027 | ppl 4.08 | wps 20738.7 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 40490 | lr 0.000157154 | gnorm 1.184 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 128880
2022-03-08 00:30:00 | INFO | fairseq.trainer | begin training epoch 209
2022-03-08 00:30:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:30:31 | INFO | train_inner | epoch 209:     10 / 196 loss=3.408, nll_loss=2.053, ppl=4.15, wps=20464.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=40500, lr=0.000157135, gnorm=1.192, loss_scale=16, train_wall=289, gb_free=19.9, wall=128911
2022-03-08 00:32:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:35:46 | INFO | train_inner | epoch 209:    111 / 196 loss=3.352, nll_loss=1.99, ppl=3.97, wps=20813.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=40600, lr=0.000156941, gnorm=1.181, loss_scale=16, train_wall=293, gb_free=19.9, wall=129226
2022-03-08 00:36:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 00:40:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:40:15 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 11.11 | nll_loss 10.272 | ppl 1236.78 | wps 41623.3 | wpb 510.9 | bsz 1 | num_updates 40684 | best_loss 7.696
2022-03-08 00:40:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 40684 updates
2022-03-08 00:40:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 00:40:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 00:40:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 209 @ 40684 updates, score 11.11) (writing took 3.098619980039075 seconds)
2022-03-08 00:40:18 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-03-08 00:40:18 | INFO | train | epoch 209 | loss 3.381 | nll_loss 2.023 | ppl 4.06 | wps 20516.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 40684 | lr 0.000156779 | gnorm 1.185 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 129499
2022-03-08 00:40:18 | INFO | fairseq.trainer | begin training epoch 210
2022-03-08 00:40:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:41:08 | INFO | train_inner | epoch 210:     16 / 196 loss=3.408, nll_loss=2.053, ppl=4.15, wps=20252.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=40700, lr=0.000156748, gnorm=1.181, loss_scale=8, train_wall=292, gb_free=19.9, wall=129549
2022-03-08 00:46:20 | INFO | train_inner | epoch 210:    116 / 196 loss=3.36, nll_loss=1.999, ppl=4, wps=21015.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=40800, lr=0.000156556, gnorm=1.178, loss_scale=16, train_wall=290, gb_free=19.9, wall=129861
2022-03-08 00:50:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:50:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:50:34 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 11.1 | nll_loss 10.264 | ppl 1229.29 | wps 41371.6 | wpb 510.9 | bsz 1 | num_updates 40879 | best_loss 7.696
2022-03-08 00:50:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 40879 updates
2022-03-08 00:50:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 00:50:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 00:50:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 210 @ 40879 updates, score 11.1) (writing took 3.127263732952997 seconds)
2022-03-08 00:50:37 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-03-08 00:50:37 | INFO | train | epoch 210 | loss 3.379 | nll_loss 2.021 | ppl 4.06 | wps 20617.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 40879 | lr 0.000156405 | gnorm 1.182 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 130118
2022-03-08 00:50:37 | INFO | fairseq.trainer | begin training epoch 211
2022-03-08 00:50:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:51:43 | INFO | train_inner | epoch 211:     21 / 196 loss=3.395, nll_loss=2.039, ppl=4.11, wps=20257.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=40900, lr=0.000156365, gnorm=1.195, loss_scale=16, train_wall=292, gb_free=19.9, wall=130183
2022-03-08 00:56:55 | INFO | train_inner | epoch 211:    121 / 196 loss=3.359, nll_loss=1.999, ppl=4, wps=21024.3, ups=0.32, wpb=65536, bsz=128, num_updates=41000, lr=0.000156174, gnorm=1.179, loss_scale=16, train_wall=290, gb_free=19.9, wall=130495
2022-03-08 00:57:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:00:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:00:53 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 11.117 | nll_loss 10.283 | ppl 1246.1 | wps 41313.8 | wpb 510.9 | bsz 1 | num_updates 41074 | best_loss 7.696
2022-03-08 01:00:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 41074 updates
2022-03-08 01:00:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 01:00:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 01:00:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 211 @ 41074 updates, score 11.117) (writing took 3.1179662600625306 seconds)
2022-03-08 01:00:56 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-03-08 01:00:56 | INFO | train | epoch 211 | loss 3.377 | nll_loss 2.018 | ppl 4.05 | wps 20619.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 41074 | lr 0.000156033 | gnorm 1.188 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 130737
2022-03-08 01:00:56 | INFO | fairseq.trainer | begin training epoch 212
2022-03-08 01:00:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:02:18 | INFO | train_inner | epoch 212:     26 / 196 loss=3.387, nll_loss=2.03, ppl=4.08, wps=20243.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=41100, lr=0.000155984, gnorm=1.189, loss_scale=16, train_wall=292, gb_free=19.9, wall=130818
2022-03-08 01:04:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:07:32 | INFO | train_inner | epoch 212:    127 / 196 loss=3.364, nll_loss=2.004, ppl=4.01, wps=20808.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=41200, lr=0.000155794, gnorm=1.196, loss_scale=16, train_wall=293, gb_free=19.9, wall=131133
2022-03-08 01:10:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:11:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:11:12 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 11.101 | nll_loss 10.264 | ppl 1229.73 | wps 41425.5 | wpb 510.9 | bsz 1 | num_updates 41268 | best_loss 7.696
2022-03-08 01:11:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 41268 updates
2022-03-08 01:11:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 01:11:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 01:11:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 212 @ 41268 updates, score 11.101) (writing took 3.1276229980867356 seconds)
2022-03-08 01:11:15 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-03-08 01:11:15 | INFO | train | epoch 212 | loss 3.374 | nll_loss 2.015 | ppl 4.04 | wps 20508.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 41268 | lr 0.000155666 | gnorm 1.188 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 131356
2022-03-08 01:11:15 | INFO | fairseq.trainer | begin training epoch 213
2022-03-08 01:11:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:12:55 | INFO | train_inner | epoch 213:     32 / 196 loss=3.384, nll_loss=2.027, ppl=4.07, wps=20246.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=41300, lr=0.000155606, gnorm=1.18, loss_scale=16, train_wall=292, gb_free=19.9, wall=131456
2022-03-08 01:17:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:18:10 | INFO | train_inner | epoch 213:    133 / 196 loss=3.363, nll_loss=2.003, ppl=4.01, wps=20810.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=41400, lr=0.000155417, gnorm=1.175, loss_scale=16, train_wall=293, gb_free=19.9, wall=131771
2022-03-08 01:21:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:21:31 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 11.145 | nll_loss 10.313 | ppl 1272.1 | wps 41674.7 | wpb 510.9 | bsz 1 | num_updates 41463 | best_loss 7.696
2022-03-08 01:21:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 41463 updates
2022-03-08 01:21:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 01:21:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 01:21:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 213 @ 41463 updates, score 11.145) (writing took 3.1287848711945117 seconds)
2022-03-08 01:21:34 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-03-08 01:21:34 | INFO | train | epoch 213 | loss 3.372 | nll_loss 2.014 | ppl 4.04 | wps 20623.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 41463 | lr 0.000155299 | gnorm 1.181 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 131975
2022-03-08 01:21:34 | INFO | fairseq.trainer | begin training epoch 214
2022-03-08 01:21:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:23:30 | INFO | train_inner | epoch 214:     37 / 196 loss=3.378, nll_loss=2.02, ppl=4.06, wps=20458.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=41500, lr=0.00015523, gnorm=1.201, loss_scale=16, train_wall=289, gb_free=19.9, wall=132090
2022-03-08 01:24:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:28:45 | INFO | train_inner | epoch 214:    138 / 196 loss=3.36, nll_loss=2, ppl=4, wps=20805.4, ups=0.32, wpb=65536, bsz=128, num_updates=41600, lr=0.000155043, gnorm=1.183, loss_scale=16, train_wall=293, gb_free=19.9, wall=132405
2022-03-08 01:31:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:31:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:31:50 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 11.091 | nll_loss 10.256 | ppl 1222.71 | wps 41712.5 | wpb 510.9 | bsz 1 | num_updates 41657 | best_loss 7.696
2022-03-08 01:31:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 41657 updates
2022-03-08 01:31:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 01:31:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 01:31:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 214 @ 41657 updates, score 11.091) (writing took 3.1354050741065294 seconds)
2022-03-08 01:31:53 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-03-08 01:31:53 | INFO | train | epoch 214 | loss 3.37 | nll_loss 2.011 | ppl 4.03 | wps 20515.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 41657 | lr 0.000154937 | gnorm 1.194 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 132593
2022-03-08 01:31:53 | INFO | fairseq.trainer | begin training epoch 215
2022-03-08 01:31:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:34:07 | INFO | train_inner | epoch 215:     43 / 196 loss=3.377, nll_loss=2.019, ppl=4.05, wps=20254, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=41700, lr=0.000154857, gnorm=1.191, loss_scale=16, train_wall=292, gb_free=19.9, wall=132728
2022-03-08 01:37:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:39:23 | INFO | train_inner | epoch 215:    144 / 196 loss=3.363, nll_loss=2.003, ppl=4.01, wps=20795.5, ups=0.32, wpb=65536, bsz=128, num_updates=41800, lr=0.000154672, gnorm=1.187, loss_scale=16, train_wall=293, gb_free=19.9, wall=133043
2022-03-08 01:42:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:42:09 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 11.09 | nll_loss 10.258 | ppl 1224.87 | wps 41497.4 | wpb 510.9 | bsz 1 | num_updates 41852 | best_loss 7.696
2022-03-08 01:42:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 41852 updates
2022-03-08 01:42:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 01:42:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 01:42:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 215 @ 41852 updates, score 11.09) (writing took 3.1007120741996914 seconds)
2022-03-08 01:42:12 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-03-08 01:42:12 | INFO | train | epoch 215 | loss 3.368 | nll_loss 2.009 | ppl 4.02 | wps 20608 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 41852 | lr 0.000154576 | gnorm 1.186 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 133213
2022-03-08 01:42:12 | INFO | fairseq.trainer | begin training epoch 216
2022-03-08 01:42:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:42:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 01:44:45 | INFO | train_inner | epoch 216:     49 / 196 loss=3.368, nll_loss=2.009, ppl=4.02, wps=20247.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=41900, lr=0.000154487, gnorm=1.176, loss_scale=8, train_wall=292, gb_free=19.9, wall=133366
2022-03-08 01:49:57 | INFO | train_inner | epoch 216:    149 / 196 loss=3.372, nll_loss=2.013, ppl=4.04, wps=21010.8, ups=0.32, wpb=65536, bsz=128, num_updates=42000, lr=0.000154303, gnorm=1.184, loss_scale=16, train_wall=290, gb_free=19.9, wall=133678
2022-03-08 01:52:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:52:28 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 11.1 | nll_loss 10.264 | ppl 1229.88 | wps 41628.1 | wpb 510.9 | bsz 1 | num_updates 42047 | best_loss 7.696
2022-03-08 01:52:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 42047 updates
2022-03-08 01:52:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 01:52:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 01:52:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 216 @ 42047 updates, score 11.1) (writing took 3.076987743843347 seconds)
2022-03-08 01:52:32 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-03-08 01:52:32 | INFO | train | epoch 216 | loss 3.366 | nll_loss 2.006 | ppl 4.02 | wps 20615 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 42047 | lr 0.000154217 | gnorm 1.18 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 133832
2022-03-08 01:52:32 | INFO | fairseq.trainer | begin training epoch 217
2022-03-08 01:52:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:55:17 | INFO | train_inner | epoch 217:     53 / 196 loss=3.361, nll_loss=2.001, ppl=4, wps=20452.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=42100, lr=0.00015412, gnorm=1.19, loss_scale=16, train_wall=289, gb_free=19.9, wall=133997
2022-03-08 01:56:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:00:32 | INFO | train_inner | epoch 217:    154 / 196 loss=3.367, nll_loss=2.007, ppl=4.02, wps=20804.5, ups=0.32, wpb=65536, bsz=128, num_updates=42200, lr=0.000153937, gnorm=1.209, loss_scale=16, train_wall=293, gb_free=19.9, wall=134312
2022-03-08 02:02:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:02:47 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 11.149 | nll_loss 10.317 | ppl 1275.96 | wps 41431 | wpb 510.9 | bsz 1 | num_updates 42242 | best_loss 7.696
2022-03-08 02:02:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 42242 updates
2022-03-08 02:02:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 02:02:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 02:02:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 217 @ 42242 updates, score 11.149) (writing took 3.072150345891714 seconds)
2022-03-08 02:02:51 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-03-08 02:02:51 | INFO | train | epoch 217 | loss 3.363 | nll_loss 2.003 | ppl 4.01 | wps 20616.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 42242 | lr 0.000153861 | gnorm 1.201 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 134451
2022-03-08 02:02:51 | INFO | fairseq.trainer | begin training epoch 218
2022-03-08 02:02:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:03:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:05:55 | INFO | train_inner | epoch 218:     59 / 196 loss=3.354, nll_loss=1.993, ppl=3.98, wps=20254.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=42300, lr=0.000153755, gnorm=1.193, loss_scale=16, train_wall=292, gb_free=19.9, wall=134635
2022-03-08 02:09:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:11:10 | INFO | train_inner | epoch 218:    160 / 196 loss=3.367, nll_loss=2.008, ppl=4.02, wps=20807.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=42400, lr=0.000153574, gnorm=1.171, loss_scale=16, train_wall=293, gb_free=19.9, wall=134950
2022-03-08 02:13:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:13:06 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 11.078 | nll_loss 10.243 | ppl 1211.44 | wps 41560.9 | wpb 510.9 | bsz 1 | num_updates 42436 | best_loss 7.696
2022-03-08 02:13:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 42436 updates
2022-03-08 02:13:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 02:13:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 02:13:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 218 @ 42436 updates, score 11.078) (writing took 3.046299522044137 seconds)
2022-03-08 02:13:09 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-03-08 02:13:09 | INFO | train | epoch 218 | loss 3.36 | nll_loss 2 | ppl 4 | wps 20516.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 42436 | lr 0.000153509 | gnorm 1.186 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 135070
2022-03-08 02:13:09 | INFO | fairseq.trainer | begin training epoch 219
2022-03-08 02:13:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:16:29 | INFO | train_inner | epoch 219:     64 / 196 loss=3.349, nll_loss=1.988, ppl=3.97, wps=20459.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=42500, lr=0.000153393, gnorm=1.191, loss_scale=16, train_wall=289, gb_free=19.9, wall=135269
2022-03-08 02:16:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:19:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 02:21:47 | INFO | train_inner | epoch 219:    166 / 196 loss=3.372, nll_loss=2.014, ppl=4.04, wps=20610.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=42600, lr=0.000153213, gnorm=1.201, loss_scale=8, train_wall=296, gb_free=19.9, wall=135587
2022-03-08 02:23:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:23:25 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 11.114 | nll_loss 10.281 | ppl 1244.58 | wps 41550.8 | wpb 510.9 | bsz 1 | num_updates 42630 | best_loss 7.696
2022-03-08 02:23:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 42630 updates
2022-03-08 02:23:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 02:23:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 02:23:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 219 @ 42630 updates, score 11.114) (writing took 3.0662551941350102 seconds)
2022-03-08 02:23:28 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-03-08 02:23:28 | INFO | train | epoch 219 | loss 3.358 | nll_loss 1.997 | ppl 3.99 | wps 20520.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 42630 | lr 0.000153159 | gnorm 1.19 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 135688
2022-03-08 02:23:28 | INFO | fairseq.trainer | begin training epoch 220
2022-03-08 02:23:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:27:07 | INFO | train_inner | epoch 220:     70 / 196 loss=3.343, nll_loss=1.981, ppl=3.95, wps=20455.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=42700, lr=0.000153033, gnorm=1.191, loss_scale=16, train_wall=289, gb_free=19.9, wall=135907
2022-03-08 02:32:18 | INFO | train_inner | epoch 220:    170 / 196 loss=3.371, nll_loss=2.013, ppl=4.03, wps=21014.8, ups=0.32, wpb=65536, bsz=128, num_updates=42800, lr=0.000152854, gnorm=1.197, loss_scale=16, train_wall=290, gb_free=19.9, wall=136219
2022-03-08 02:32:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:33:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:33:44 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 11.137 | nll_loss 10.31 | ppl 1269.41 | wps 41401.8 | wpb 510.9 | bsz 1 | num_updates 42825 | best_loss 7.696
2022-03-08 02:33:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 42825 updates
2022-03-08 02:33:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 02:33:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 02:33:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 220 @ 42825 updates, score 11.137) (writing took 3.0536343108396977 seconds)
2022-03-08 02:33:47 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-03-08 02:33:47 | INFO | train | epoch 220 | loss 3.357 | nll_loss 1.997 | ppl 3.99 | wps 20623.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 42825 | lr 0.00015281 | gnorm 1.196 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 136307
2022-03-08 02:33:47 | INFO | fairseq.trainer | begin training epoch 221
2022-03-08 02:33:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:37:41 | INFO | train_inner | epoch 221:     75 / 196 loss=3.342, nll_loss=1.98, ppl=3.94, wps=20270.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=42900, lr=0.000152676, gnorm=1.19, loss_scale=16, train_wall=292, gb_free=19.9, wall=136541
2022-03-08 02:39:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:42:56 | INFO | train_inner | epoch 221:    176 / 196 loss=3.371, nll_loss=2.012, ppl=4.03, wps=20813, ups=0.32, wpb=65532.4, bsz=128, num_updates=43000, lr=0.000152499, gnorm=1.196, loss_scale=16, train_wall=293, gb_free=19.9, wall=136856
2022-03-08 02:43:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:44:03 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 11.139 | nll_loss 10.309 | ppl 1269.01 | wps 41457.6 | wpb 510.9 | bsz 1 | num_updates 43020 | best_loss 7.696
2022-03-08 02:44:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 43020 updates
2022-03-08 02:44:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 02:44:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 02:44:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 221 @ 43020 updates, score 11.139) (writing took 3.0653060749173164 seconds)
2022-03-08 02:44:06 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-03-08 02:44:06 | INFO | train | epoch 221 | loss 3.355 | nll_loss 1.995 | ppl 3.98 | wps 20626 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 43020 | lr 0.000152463 | gnorm 1.194 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 136926
2022-03-08 02:44:06 | INFO | fairseq.trainer | begin training epoch 222
2022-03-08 02:44:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:46:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:48:19 | INFO | train_inner | epoch 222:     81 / 196 loss=3.332, nll_loss=1.968, ppl=3.91, wps=20254.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=43100, lr=0.000152322, gnorm=1.197, loss_scale=16, train_wall=292, gb_free=19.9, wall=137179
2022-03-08 02:53:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:53:33 | INFO | train_inner | epoch 222:    182 / 196 loss=3.38, nll_loss=2.022, ppl=4.06, wps=20807.7, ups=0.32, wpb=65536, bsz=128, num_updates=43200, lr=0.000152145, gnorm=1.218, loss_scale=16, train_wall=293, gb_free=19.9, wall=137494
2022-03-08 02:54:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:54:22 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 11.162 | nll_loss 10.329 | ppl 1286.31 | wps 41530.4 | wpb 510.9 | bsz 1 | num_updates 43214 | best_loss 7.696
2022-03-08 02:54:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 43214 updates
2022-03-08 02:54:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 02:54:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 02:54:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 222 @ 43214 updates, score 11.162) (writing took 3.0746705681085587 seconds)
2022-03-08 02:54:25 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-03-08 02:54:25 | INFO | train | epoch 222 | loss 3.352 | nll_loss 1.991 | ppl 3.98 | wps 20512.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43214 | lr 0.000152121 | gnorm 1.204 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 137545
2022-03-08 02:54:25 | INFO | fairseq.trainer | begin training epoch 223
2022-03-08 02:54:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:58:53 | INFO | train_inner | epoch 223:     86 / 196 loss=3.325, nll_loss=1.961, ppl=3.89, wps=20446.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=43300, lr=0.000151969, gnorm=1.179, loss_scale=16, train_wall=289, gb_free=19.9, wall=137813
2022-03-08 02:59:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:04:08 | INFO | train_inner | epoch 223:    187 / 196 loss=3.374, nll_loss=2.016, ppl=4.04, wps=20801.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=43400, lr=0.000151794, gnorm=1.212, loss_scale=16, train_wall=293, gb_free=19.9, wall=138129
2022-03-08 03:04:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:04:41 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 11.124 | nll_loss 10.294 | ppl 1255.16 | wps 41644.3 | wpb 510.9 | bsz 1 | num_updates 43409 | best_loss 7.696
2022-03-08 03:04:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 43409 updates
2022-03-08 03:04:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 03:04:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 03:04:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 223 @ 43409 updates, score 11.124) (writing took 3.047889629146084 seconds)
2022-03-08 03:04:44 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-03-08 03:04:44 | INFO | train | epoch 223 | loss 3.35 | nll_loss 1.989 | ppl 3.97 | wps 20614.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 43409 | lr 0.000151778 | gnorm 1.197 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 138164
2022-03-08 03:04:44 | INFO | fairseq.trainer | begin training epoch 224
2022-03-08 03:04:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:06:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:09:33 | INFO | train_inner | epoch 224:     92 / 196 loss=3.327, nll_loss=1.963, ppl=3.9, wps=20128.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=43500, lr=0.00015162, gnorm=1.193, loss_scale=16, train_wall=292, gb_free=19.9, wall=138453
2022-03-08 03:13:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:14:50 | INFO | train_inner | epoch 224:    193 / 196 loss=3.375, nll_loss=2.017, ppl=4.05, wps=20646.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=43600, lr=0.000151446, gnorm=1.199, loss_scale=16, train_wall=293, gb_free=19.9, wall=138771
2022-03-08 03:14:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:15:04 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 11.131 | nll_loss 10.302 | ppl 1262.16 | wps 41236.5 | wpb 510.9 | bsz 1 | num_updates 43603 | best_loss 7.696
2022-03-08 03:15:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 43603 updates
2022-03-08 03:15:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 03:15:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 03:15:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 224 @ 43603 updates, score 11.131) (writing took 3.0895162590313703 seconds)
2022-03-08 03:15:07 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-03-08 03:15:07 | INFO | train | epoch 224 | loss 3.349 | nll_loss 1.988 | ppl 3.97 | wps 20361.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43603 | lr 0.00015144 | gnorm 1.195 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 138788
2022-03-08 03:15:07 | INFO | fairseq.trainer | begin training epoch 225
2022-03-08 03:15:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:20:10 | INFO | train_inner | epoch 225:     97 / 196 loss=3.319, nll_loss=1.954, ppl=3.87, wps=20444, ups=0.31, wpb=65367, bsz=127.7, num_updates=43700, lr=0.000151272, gnorm=1.195, loss_scale=16, train_wall=289, gb_free=19.9, wall=139090
2022-03-08 03:20:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:25:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:25:23 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 11.154 | nll_loss 10.324 | ppl 1281.64 | wps 41390.2 | wpb 510.9 | bsz 1 | num_updates 43798 | best_loss 7.696
2022-03-08 03:25:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 43798 updates
2022-03-08 03:25:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 03:25:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 03:25:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 225 @ 43798 updates, score 11.154) (writing took 3.0749506279826164 seconds)
2022-03-08 03:25:26 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-03-08 03:25:26 | INFO | train | epoch 225 | loss 3.347 | nll_loss 1.986 | ppl 3.96 | wps 20615.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 43798 | lr 0.000151103 | gnorm 1.196 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 139407
2022-03-08 03:25:26 | INFO | fairseq.trainer | begin training epoch 226
2022-03-08 03:25:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:25:33 | INFO | train_inner | epoch 226:      2 / 196 loss=3.375, nll_loss=2.018, ppl=4.05, wps=20252.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=43800, lr=0.000151099, gnorm=1.198, loss_scale=16, train_wall=292, gb_free=19.9, wall=139413
2022-03-08 03:27:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:30:48 | INFO | train_inner | epoch 226:    103 / 196 loss=3.319, nll_loss=1.955, ppl=3.88, wps=20803.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=43900, lr=0.000150927, gnorm=1.19, loss_scale=16, train_wall=293, gb_free=19.9, wall=139728
2022-03-08 03:33:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:35:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:35:43 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 11.143 | nll_loss 10.311 | ppl 1270.72 | wps 40735.9 | wpb 510.9 | bsz 1 | num_updates 43992 | best_loss 7.696
2022-03-08 03:35:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 43992 updates
2022-03-08 03:35:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 03:35:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 03:35:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 226 @ 43992 updates, score 11.143) (writing took 3.0693319509737194 seconds)
2022-03-08 03:35:46 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-03-08 03:35:46 | INFO | train | epoch 226 | loss 3.345 | nll_loss 1.983 | ppl 3.95 | wps 20500.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43992 | lr 0.000150769 | gnorm 1.195 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 140026
2022-03-08 03:35:46 | INFO | fairseq.trainer | begin training epoch 227
2022-03-08 03:35:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:36:11 | INFO | train_inner | epoch 227:      8 / 196 loss=3.365, nll_loss=2.007, ppl=4.02, wps=20237.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=44000, lr=0.000150756, gnorm=1.2, loss_scale=16, train_wall=292, gb_free=19.9, wall=140051
2022-03-08 03:40:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:41:28 | INFO | train_inner | epoch 227:    109 / 196 loss=3.318, nll_loss=1.954, ppl=3.87, wps=20683.4, ups=0.32, wpb=65536, bsz=128, num_updates=44100, lr=0.000150585, gnorm=1.18, loss_scale=16, train_wall=294, gb_free=19.9, wall=140368
2022-03-08 03:45:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:46:04 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 11.17 | nll_loss 10.343 | ppl 1299.28 | wps 41620.9 | wpb 510.9 | bsz 1 | num_updates 44187 | best_loss 7.696
2022-03-08 03:46:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 44187 updates
2022-03-08 03:46:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 03:46:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 03:46:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 227 @ 44187 updates, score 11.17) (writing took 3.083093293942511 seconds)
2022-03-08 03:46:07 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-03-08 03:46:07 | INFO | train | epoch 227 | loss 3.343 | nll_loss 1.981 | ppl 3.95 | wps 20555.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 44187 | lr 0.000150436 | gnorm 1.197 | loss_scale 16 | train_wall 569 | gb_free 19.9 | wall 140647
2022-03-08 03:46:07 | INFO | fairseq.trainer | begin training epoch 228
2022-03-08 03:46:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:46:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 03:46:50 | INFO | train_inner | epoch 228:     14 / 196 loss=3.366, nll_loss=2.007, ppl=4.02, wps=20253.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=44200, lr=0.000150414, gnorm=1.224, loss_scale=8, train_wall=292, gb_free=19.9, wall=140691
2022-03-08 03:52:02 | INFO | train_inner | epoch 228:    114 / 196 loss=3.315, nll_loss=1.95, ppl=3.86, wps=21006.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=44300, lr=0.000150244, gnorm=1.189, loss_scale=8, train_wall=290, gb_free=19.9, wall=141003
2022-03-08 03:56:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:56:23 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 11.129 | nll_loss 10.3 | ppl 1260.56 | wps 41608.5 | wpb 510.9 | bsz 1 | num_updates 44382 | best_loss 7.696
2022-03-08 03:56:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 44382 updates
2022-03-08 03:56:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 03:56:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 03:56:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 228 @ 44382 updates, score 11.129) (writing took 3.0591475889086723 seconds)
2022-03-08 03:56:26 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-03-08 03:56:26 | INFO | train | epoch 228 | loss 3.34 | nll_loss 1.978 | ppl 3.94 | wps 20615.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 44382 | lr 0.000150105 | gnorm 1.204 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 141266
2022-03-08 03:56:26 | INFO | fairseq.trainer | begin training epoch 229
2022-03-08 03:56:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:57:22 | INFO | train_inner | epoch 229:     18 / 196 loss=3.36, nll_loss=2.001, ppl=4, wps=20452.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=44400, lr=0.000150075, gnorm=1.207, loss_scale=16, train_wall=289, gb_free=19.9, wall=141322
2022-03-08 03:59:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:02:37 | INFO | train_inner | epoch 229:    119 / 196 loss=3.319, nll_loss=1.955, ppl=3.88, wps=20800.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=44500, lr=0.000149906, gnorm=1.178, loss_scale=16, train_wall=293, gb_free=19.9, wall=141637
2022-03-08 04:06:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:06:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:06:42 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 11.112 | nll_loss 10.279 | ppl 1242.75 | wps 41678.2 | wpb 510.9 | bsz 1 | num_updates 44576 | best_loss 7.696
2022-03-08 04:06:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 44576 updates
2022-03-08 04:06:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 04:06:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 04:06:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 229 @ 44576 updates, score 11.112) (writing took 3.041599973803386 seconds)
2022-03-08 04:06:45 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-03-08 04:06:45 | INFO | train | epoch 229 | loss 3.337 | nll_loss 1.975 | ppl 3.93 | wps 20516.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 44576 | lr 0.000149778 | gnorm 1.191 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 141885
2022-03-08 04:06:45 | INFO | fairseq.trainer | begin training epoch 230
2022-03-08 04:06:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:08:00 | INFO | train_inner | epoch 230:     24 / 196 loss=3.354, nll_loss=1.994, ppl=3.98, wps=20267, ups=0.31, wpb=65367, bsz=127.7, num_updates=44600, lr=0.000149738, gnorm=1.202, loss_scale=16, train_wall=292, gb_free=19.9, wall=141960
2022-03-08 04:13:11 | INFO | train_inner | epoch 230:    124 / 196 loss=3.319, nll_loss=1.955, ppl=3.88, wps=21028.6, ups=0.32, wpb=65536, bsz=128, num_updates=44700, lr=0.000149571, gnorm=1.192, loss_scale=16, train_wall=290, gb_free=19.9, wall=142272
2022-03-08 04:13:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:16:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:17:00 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 11.147 | nll_loss 10.321 | ppl 1279.52 | wps 41683.4 | wpb 510.9 | bsz 1 | num_updates 44771 | best_loss 7.696
2022-03-08 04:17:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 44771 updates
2022-03-08 04:17:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 04:17:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 04:17:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 230 @ 44771 updates, score 11.147) (writing took 3.0434126621112227 seconds)
2022-03-08 04:17:03 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-03-08 04:17:03 | INFO | train | epoch 230 | loss 3.337 | nll_loss 1.974 | ppl 3.93 | wps 20629.3 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 44771 | lr 0.000149452 | gnorm 1.207 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 142504
2022-03-08 04:17:03 | INFO | fairseq.trainer | begin training epoch 231
2022-03-08 04:17:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:18:34 | INFO | train_inner | epoch 231:     29 / 196 loss=3.352, nll_loss=1.992, ppl=3.98, wps=20264.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=44800, lr=0.000149404, gnorm=1.226, loss_scale=16, train_wall=292, gb_free=19.9, wall=142594
2022-03-08 04:20:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:23:49 | INFO | train_inner | epoch 231:    130 / 196 loss=3.32, nll_loss=1.956, ppl=3.88, wps=20820.8, ups=0.32, wpb=65536, bsz=128, num_updates=44900, lr=0.000149237, gnorm=1.186, loss_scale=16, train_wall=293, gb_free=19.9, wall=142909
2022-03-08 04:26:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:27:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:27:19 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 11.156 | nll_loss 10.328 | ppl 1285.24 | wps 41683.5 | wpb 510.9 | bsz 1 | num_updates 44965 | best_loss 7.696
2022-03-08 04:27:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 44965 updates
2022-03-08 04:27:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 04:27:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 04:27:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 231 @ 44965 updates, score 11.156) (writing took 3.0363854949828237 seconds)
2022-03-08 04:27:22 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-03-08 04:27:22 | INFO | train | epoch 231 | loss 3.333 | nll_loss 1.971 | ppl 3.92 | wps 20525.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 44965 | lr 0.000149129 | gnorm 1.191 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 143122
2022-03-08 04:27:22 | INFO | fairseq.trainer | begin training epoch 232
2022-03-08 04:27:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:29:11 | INFO | train_inner | epoch 232:     35 / 196 loss=3.341, nll_loss=1.98, ppl=3.94, wps=20261.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=45000, lr=0.000149071, gnorm=1.19, loss_scale=16, train_wall=292, gb_free=19.9, wall=143231
2022-03-08 04:31:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 04:34:26 | INFO | train_inner | epoch 232:    136 / 196 loss=3.325, nll_loss=1.961, ppl=3.89, wps=20808.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=45100, lr=0.000148906, gnorm=1.204, loss_scale=8, train_wall=293, gb_free=19.9, wall=143546
2022-03-08 04:37:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:37:38 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 11.176 | nll_loss 10.348 | ppl 1303.4 | wps 41664.9 | wpb 510.9 | bsz 1 | num_updates 45160 | best_loss 7.696
2022-03-08 04:37:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 45160 updates
2022-03-08 04:37:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 04:37:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 04:37:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 232 @ 45160 updates, score 11.176) (writing took 3.060995881911367 seconds)
2022-03-08 04:37:41 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-03-08 04:37:41 | INFO | train | epoch 232 | loss 3.332 | nll_loss 1.969 | ppl 3.92 | wps 20623.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 45160 | lr 0.000148807 | gnorm 1.206 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 143741
2022-03-08 04:37:41 | INFO | fairseq.trainer | begin training epoch 233
2022-03-08 04:37:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:39:45 | INFO | train_inner | epoch 233:     40 / 196 loss=3.336, nll_loss=1.974, ppl=3.93, wps=20461.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=45200, lr=0.000148741, gnorm=1.202, loss_scale=16, train_wall=289, gb_free=19.9, wall=143866
2022-03-08 04:44:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:45:00 | INFO | train_inner | epoch 233:    141 / 196 loss=3.324, nll_loss=1.96, ppl=3.89, wps=20806.3, ups=0.32, wpb=65536, bsz=128, num_updates=45300, lr=0.000148577, gnorm=1.19, loss_scale=16, train_wall=293, gb_free=19.9, wall=144181
2022-03-08 04:47:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:47:57 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 11.136 | nll_loss 10.311 | ppl 1269.95 | wps 41549.6 | wpb 510.9 | bsz 1 | num_updates 45355 | best_loss 7.696
2022-03-08 04:47:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 45355 updates
2022-03-08 04:47:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 04:48:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 04:48:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 233 @ 45355 updates, score 11.136) (writing took 3.052183998050168 seconds)
2022-03-08 04:48:00 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-03-08 04:48:00 | INFO | train | epoch 233 | loss 3.329 | nll_loss 1.966 | ppl 3.91 | wps 20617.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 45355 | lr 0.000148487 | gnorm 1.192 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 144360
2022-03-08 04:48:00 | INFO | fairseq.trainer | begin training epoch 234
2022-03-08 04:48:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:50:20 | INFO | train_inner | epoch 234:     45 / 196 loss=3.334, nll_loss=1.971, ppl=3.92, wps=20446, ups=0.31, wpb=65367, bsz=127.7, num_updates=45400, lr=0.000148413, gnorm=1.199, loss_scale=16, train_wall=289, gb_free=19.9, wall=144501
2022-03-08 04:51:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:55:35 | INFO | train_inner | epoch 234:    146 / 196 loss=3.328, nll_loss=1.965, ppl=3.9, wps=20802.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=45500, lr=0.00014825, gnorm=1.203, loss_scale=16, train_wall=293, gb_free=19.9, wall=144816
2022-03-08 04:58:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:58:16 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 11.164 | nll_loss 10.336 | ppl 1292.88 | wps 41669 | wpb 510.9 | bsz 1 | num_updates 45550 | best_loss 7.696
2022-03-08 04:58:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 45550 updates
2022-03-08 04:58:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 04:58:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 04:58:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 234 @ 45550 updates, score 11.164) (writing took 3.061178046045825 seconds)
2022-03-08 04:58:19 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-03-08 04:58:19 | INFO | train | epoch 234 | loss 3.328 | nll_loss 1.965 | ppl 3.91 | wps 20612.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 45550 | lr 0.000148168 | gnorm 1.198 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 144979
2022-03-08 04:58:19 | INFO | fairseq.trainer | begin training epoch 235
2022-03-08 04:58:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:58:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:00:58 | INFO | train_inner | epoch 235:     51 / 196 loss=3.324, nll_loss=1.961, ppl=3.89, wps=20247.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=45600, lr=0.000148087, gnorm=1.193, loss_scale=16, train_wall=292, gb_free=19.9, wall=145138
2022-03-08 05:05:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:06:13 | INFO | train_inner | epoch 235:    152 / 196 loss=3.332, nll_loss=1.969, ppl=3.92, wps=20800.3, ups=0.32, wpb=65536, bsz=128, num_updates=45700, lr=0.000147925, gnorm=1.216, loss_scale=16, train_wall=293, gb_free=19.9, wall=145453
2022-03-08 05:08:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:08:35 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 11.163 | nll_loss 10.333 | ppl 1289.48 | wps 41513.3 | wpb 510.9 | bsz 1 | num_updates 45744 | best_loss 7.696
2022-03-08 05:08:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 45744 updates
2022-03-08 05:08:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 05:08:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 05:08:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 235 @ 45744 updates, score 11.163) (writing took 3.0607109509874135 seconds)
2022-03-08 05:08:38 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-03-08 05:08:38 | INFO | train | epoch 235 | loss 3.326 | nll_loss 1.963 | ppl 3.9 | wps 20504.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 45744 | lr 0.000147854 | gnorm 1.208 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 145598
2022-03-08 05:08:38 | INFO | fairseq.trainer | begin training epoch 236
2022-03-08 05:08:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:11:33 | INFO | train_inner | epoch 236:     56 / 196 loss=3.319, nll_loss=1.955, ppl=3.88, wps=20450.5, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=45800, lr=0.000147764, gnorm=1.201, loss_scale=16, train_wall=289, gb_free=19.9, wall=145773
2022-03-08 05:12:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:15:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 05:16:51 | INFO | train_inner | epoch 236:    158 / 196 loss=3.333, nll_loss=1.97, ppl=3.92, wps=20614.5, ups=0.31, wpb=65536, bsz=128, num_updates=45900, lr=0.000147602, gnorm=1.219, loss_scale=8, train_wall=296, gb_free=19.9, wall=146091
2022-03-08 05:18:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:18:54 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 11.181 | nll_loss 10.355 | ppl 1309.95 | wps 41705.4 | wpb 510.9 | bsz 1 | num_updates 45938 | best_loss 7.696
2022-03-08 05:18:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 45938 updates
2022-03-08 05:18:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 05:18:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 05:18:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 236 @ 45938 updates, score 11.181) (writing took 3.0647434629499912 seconds)
2022-03-08 05:18:57 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-03-08 05:18:57 | INFO | train | epoch 236 | loss 3.323 | nll_loss 1.959 | ppl 3.89 | wps 20523 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 45938 | lr 0.000147541 | gnorm 1.205 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 146217
2022-03-08 05:18:57 | INFO | fairseq.trainer | begin training epoch 237
2022-03-08 05:18:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:22:10 | INFO | train_inner | epoch 237:     62 / 196 loss=3.31, nll_loss=1.944, ppl=3.85, wps=20471.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=46000, lr=0.000147442, gnorm=1.194, loss_scale=16, train_wall=289, gb_free=19.9, wall=146410
2022-03-08 05:27:22 | INFO | train_inner | epoch 237:    162 / 196 loss=3.335, nll_loss=1.972, ppl=3.92, wps=21013.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=46100, lr=0.000147282, gnorm=1.2, loss_scale=16, train_wall=290, gb_free=19.9, wall=146722
2022-03-08 05:28:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:29:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:29:12 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 11.144 | nll_loss 10.315 | ppl 1274.27 | wps 41441.8 | wpb 510.9 | bsz 1 | num_updates 46133 | best_loss 7.696
2022-03-08 05:29:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 46133 updates
2022-03-08 05:29:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 05:29:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 05:29:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 237 @ 46133 updates, score 11.144) (writing took 3.0940707800909877 seconds)
2022-03-08 05:29:15 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-03-08 05:29:15 | INFO | train | epoch 237 | loss 3.323 | nll_loss 1.959 | ppl 3.89 | wps 20626.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 46133 | lr 0.000147229 | gnorm 1.2 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 146836
2022-03-08 05:29:15 | INFO | fairseq.trainer | begin training epoch 238
2022-03-08 05:29:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:32:44 | INFO | train_inner | epoch 238:     67 / 196 loss=3.308, nll_loss=1.943, ppl=3.84, wps=20258.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=46200, lr=0.000147122, gnorm=1.193, loss_scale=16, train_wall=292, gb_free=19.9, wall=147045
2022-03-08 05:35:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:37:59 | INFO | train_inner | epoch 238:    168 / 196 loss=3.334, nll_loss=1.972, ppl=3.92, wps=20807.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=46300, lr=0.000146964, gnorm=1.199, loss_scale=16, train_wall=293, gb_free=19.9, wall=147360
2022-03-08 05:39:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:39:31 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 11.182 | nll_loss 10.357 | ppl 1311.26 | wps 41589.6 | wpb 510.9 | bsz 1 | num_updates 46328 | best_loss 7.696
2022-03-08 05:39:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 46328 updates
2022-03-08 05:39:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 05:39:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 05:39:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 238 @ 46328 updates, score 11.182) (writing took 3.1098062701057643 seconds)
2022-03-08 05:39:34 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-03-08 05:39:34 | INFO | train | epoch 238 | loss 3.32 | nll_loss 1.956 | ppl 3.88 | wps 20618.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 46328 | lr 0.000146919 | gnorm 1.192 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 147455
2022-03-08 05:39:34 | INFO | fairseq.trainer | begin training epoch 239
2022-03-08 05:39:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:42:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:43:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 05:43:25 | INFO | train_inner | epoch 239:     74 / 196 loss=3.302, nll_loss=1.936, ppl=3.83, wps=20059.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=46400, lr=0.000146805, gnorm=1.197, loss_scale=8, train_wall=295, gb_free=19.9, wall=147686
2022-03-08 05:48:37 | INFO | train_inner | epoch 239:    174 / 196 loss=3.335, nll_loss=1.973, ppl=3.93, wps=21016.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=46500, lr=0.000146647, gnorm=1.21, loss_scale=8, train_wall=290, gb_free=19.9, wall=147997
2022-03-08 05:49:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:49:50 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 11.171 | nll_loss 10.344 | ppl 1299.38 | wps 41502.2 | wpb 510.9 | bsz 1 | num_updates 46522 | best_loss 7.696
2022-03-08 05:49:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 46522 updates
2022-03-08 05:49:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 05:49:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 05:49:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 239 @ 46522 updates, score 11.171) (writing took 3.065689231036231 seconds)
2022-03-08 05:49:53 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-03-08 05:49:53 | INFO | train | epoch 239 | loss 3.318 | nll_loss 1.954 | ppl 3.87 | wps 20515.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 46522 | lr 0.000146612 | gnorm 1.207 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 148074
2022-03-08 05:49:53 | INFO | fairseq.trainer | begin training epoch 240
2022-03-08 05:49:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:53:57 | INFO | train_inner | epoch 240:     78 / 196 loss=3.306, nll_loss=1.941, ppl=3.84, wps=20455.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=46600, lr=0.00014649, gnorm=1.197, loss_scale=16, train_wall=289, gb_free=19.9, wall=148317
2022-03-08 05:54:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 05:59:12 | INFO | train_inner | epoch 240:    179 / 196 loss=3.335, nll_loss=1.973, ppl=3.93, wps=20770.1, ups=0.32, wpb=65536, bsz=128, num_updates=46700, lr=0.000146333, gnorm=1.198, loss_scale=8, train_wall=293, gb_free=19.9, wall=148632
2022-03-08 06:00:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 06:00:10 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 11.183 | nll_loss 10.358 | ppl 1312.59 | wps 41407.5 | wpb 510.9 | bsz 1 | num_updates 46717 | best_loss 7.696
2022-03-08 06:00:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 46717 updates
2022-03-08 06:00:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 06:00:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 06:00:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 240 @ 46717 updates, score 11.183) (writing took 3.0664586978964508 seconds)
2022-03-08 06:00:13 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-03-08 06:00:13 | INFO | train | epoch 240 | loss 3.317 | nll_loss 1.953 | ppl 3.87 | wps 20593.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 46717 | lr 0.000146306 | gnorm 1.195 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 148693
2022-03-08 06:00:13 | INFO | fairseq.trainer | begin training epoch 241
2022-03-08 06:00:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 06:04:32 | INFO | train_inner | epoch 241:     83 / 196 loss=3.293, nll_loss=1.926, ppl=3.8, wps=20437.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=46800, lr=0.000146176, gnorm=1.196, loss_scale=16, train_wall=289, gb_free=19.9, wall=148952
2022-03-08 06:07:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 06:09:47 | INFO | train_inner | epoch 241:    184 / 196 loss=3.339, nll_loss=1.978, ppl=3.94, wps=20802.5, ups=0.32, wpb=65536, bsz=128, num_updates=46900, lr=0.00014602, gnorm=1.207, loss_scale=16, train_wall=293, gb_free=19.9, wall=149267
2022-03-08 06:10:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 06:10:29 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 11.198 | nll_loss 10.37 | ppl 1323.74 | wps 41477.8 | wpb 510.9 | bsz 1 | num_updates 46912 | best_loss 7.696
2022-03-08 06:10:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 46912 updates
2022-03-08 06:10:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 06:10:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt
2022-03-08 06:10:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.06_#2/checkpoint_last.pt (epoch 241 @ 46912 updates, score 11.198) (writing took 3.061067916918546 seconds)
2022-03-08 06:10:32 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-03-08 06:10:32 | INFO | train | epoch 241 | loss 3.315 | nll_loss 1.951 | ppl 3.87 | wps 20615.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 46912 | lr 0.000146002 | gnorm 1.204 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 149312
2022-03-08 06:10:32 | INFO | fairseq.trainer | begin training epoch 242
2022-03-08 06:10:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 06:14:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 06:15:10 | INFO | train_inner | epoch 242:     89 / 196 loss=3.289, nll_loss=1.922, ppl=3.79, wps=20241.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=47000, lr=0.000145865, gnorm=1.209, loss_scale=16, train_wall=292, gb_free=19.9, wall=149590
2022-03-08 06:20:22 | INFO | train_inner | epoch 242:    189 / 196 loss=3.338, nll_loss=1.976, ppl=3.93, wps=20997.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=47100, lr=0.00014571, gnorm=1.206, loss_scale=16, train_wall=290, gb_free=19.9, wall=149902
2022-03-08 06:20:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 342, in train
    valid_losses, should_stop = validate_and_save(
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 429, in validate_and_save
    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 499, in validate
    trainer.valid_step(sample)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 1039, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 502, in valid_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 79, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 368, in forward
    x, attn = self.self_attn(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/multihead_attention.py", line 170, in forward
    return F.multi_head_attention_forward(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 4318, in multi_head_attention_forward
    attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)
KeyboardInterrupt
