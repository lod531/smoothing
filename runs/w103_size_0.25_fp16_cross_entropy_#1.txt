Sender: LSF System <lsfadmin@eu-g3-065>
Subject: Job 206753645: <w103_size_0.25_fp16_cross_entropy_#1> in cluster <euler> Exited

Job <w103_size_0.25_fp16_cross_entropy_#1> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Tue Mar  1 09:28:50 2022
Job was executed on host(s) <eu-g3-065>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Tue Mar  1 09:29:21 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar  1 09:29:21 2022
Terminated at Wed Mar  2 07:08:19 2022
Results reported at Wed Mar  2 07:08:19 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --seed 66575611 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   77863.63 sec.
    Max Memory :                                 8349 MB
    Average Memory :                             3397.06 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11651.00 MB
    Max Swap :                                   852 MB
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   77936 sec.
    Turnaround time :                            77969 sec.

The output (if any) follows:

2022-03-01 09:29:28 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-01 09:29:29 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-01 09:29:33 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-01 09:29:33 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-01 09:29:33 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-01 09:29:33 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-01 09:29:33 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-01 09:29:33 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-01 09:29:33 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-01 09:29:36 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-01 09:29:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-01 09:29:36 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-01 09:29:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-01 09:29:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-01 09:29:36 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-01 09:29:36 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 09:29:36 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 09:29:36 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-01 09:29:36 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-01 09:29:37 | INFO | fairseq.trainer | begin training epoch 1
2022-03-01 09:29:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 09:29:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-01 09:29:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-01 09:29:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 09:29:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 09:29:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-01 09:33:40 | INFO | train_inner | epoch 001:    105 / 393 loss=16.86, ppl=118983, wps=29804.3, ups=0.45, wpb=65530.9, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.691, loss_scale=4, train_wall=238, gb_free=13.4, wall=245
2022-03-01 09:37:20 | INFO | train_inner | epoch 001:    205 / 393 loss=14.371, ppl=21190.4, wps=29790.8, ups=0.45, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.603, loss_scale=4, train_wall=215, gb_free=13.4, wall=465
2022-03-01 09:41:00 | INFO | train_inner | epoch 001:    305 / 393 loss=12.275, ppl=4954.89, wps=29850.9, ups=0.46, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.04, loss_scale=4, train_wall=215, gb_free=13.4, wall=684
2022-03-01 09:44:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 09:44:15 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.368 | ppl 1321.42 | wps 75570 | wpb 2034.1 | bsz 4 | num_updates 388
2022-03-01 09:44:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-01 09:44:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 09:44:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 09:44:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 1 @ 388 updates, score 10.368) (writing took 10.87288978509605 seconds)
2022-03-01 09:44:26 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-01 09:44:26 | INFO | train | epoch 001 | loss 13.671 | ppl 13041.7 | wps 29347.8 | ups 0.45 | wpb 65460.6 | bsz 127.9 | num_updates 388 | lr 4.85903e-05 | gnorm 1.775 | loss_scale 4 | train_wall 856 | gb_free 13.4 | wall 890
2022-03-01 09:44:26 | INFO | fairseq.trainer | begin training epoch 2
2022-03-01 09:44:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 09:44:52 | INFO | train_inner | epoch 002:     12 / 393 loss=10.768, ppl=1744.31, wps=28080, ups=0.43, wpb=65248.6, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.613, loss_scale=4, train_wall=214, gb_free=13.4, wall=917
2022-03-01 09:48:31 | INFO | train_inner | epoch 002:    112 / 393 loss=10.104, ppl=1100.61, wps=29886.2, ups=0.46, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.549, loss_scale=4, train_wall=214, gb_free=13.4, wall=1136
2022-03-01 09:52:11 | INFO | train_inner | epoch 002:    212 / 393 loss=9.749, ppl=860.51, wps=29886.7, ups=0.46, wpb=65530.9, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.624, loss_scale=8, train_wall=214, gb_free=13.4, wall=1355
2022-03-01 09:55:50 | INFO | train_inner | epoch 002:    312 / 393 loss=9.466, ppl=707.4, wps=29890, ups=0.46, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.673, loss_scale=8, train_wall=214, gb_free=13.4, wall=1574
2022-03-01 09:58:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 09:58:50 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.089 | ppl 544.63 | wps 74632.7 | wpb 2034.1 | bsz 4 | num_updates 781 | best_loss 9.089
2022-03-01 09:58:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 781 updates
2022-03-01 09:58:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 09:58:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 09:59:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 2 @ 781 updates, score 9.089) (writing took 11.031025104923174 seconds)
2022-03-01 09:59:01 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-01 09:59:01 | INFO | train | epoch 002 | loss 9.681 | ppl 820.65 | wps 29403.5 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 0.638 | loss_scale 8 | train_wall 842 | gb_free 13.4 | wall 1765
2022-03-01 09:59:01 | INFO | fairseq.trainer | begin training epoch 3
2022-03-01 09:59:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 09:59:42 | INFO | train_inner | epoch 003:     19 / 393 loss=9.203, ppl=589.53, wps=28060.2, ups=0.43, wpb=65248.6, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.741, loss_scale=8, train_wall=214, gb_free=13.4, wall=1807
2022-03-01 10:03:22 | INFO | train_inner | epoch 003:    119 / 393 loss=8.954, ppl=495.99, wps=29877.1, ups=0.46, wpb=65530.9, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.874, loss_scale=8, train_wall=214, gb_free=13.4, wall=2026
2022-03-01 10:07:01 | INFO | train_inner | epoch 003:    219 / 393 loss=8.758, ppl=432.9, wps=29886.9, ups=0.46, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.881, loss_scale=8, train_wall=214, gb_free=13.4, wall=2245
2022-03-01 10:10:41 | INFO | train_inner | epoch 003:    319 / 393 loss=8.572, ppl=380.67, wps=29851.2, ups=0.46, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.952, loss_scale=16, train_wall=215, gb_free=13.4, wall=2465
2022-03-01 10:13:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 10:13:25 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.318 | ppl 319.04 | wps 75022 | wpb 2034.1 | bsz 4 | num_updates 1174 | best_loss 8.318
2022-03-01 10:13:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1174 updates
2022-03-01 10:13:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 10:13:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 10:13:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 3 @ 1174 updates, score 8.318) (writing took 10.749092567013577 seconds)
2022-03-01 10:13:36 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-01 10:13:36 | INFO | train | epoch 003 | loss 8.715 | ppl 420.25 | wps 29398.8 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 1174 | lr 0.000146821 | gnorm 0.905 | loss_scale 16 | train_wall 842 | gb_free 13.4 | wall 2640
2022-03-01 10:13:36 | INFO | fairseq.trainer | begin training epoch 4
2022-03-01 10:13:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 10:14:33 | INFO | train_inner | epoch 004:     26 / 393 loss=8.399, ppl=337.61, wps=28089.7, ups=0.43, wpb=65248.6, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.967, loss_scale=16, train_wall=214, gb_free=13.4, wall=2697
2022-03-01 10:18:12 | INFO | train_inner | epoch 004:    126 / 393 loss=8.221, ppl=298.46, wps=29850.2, ups=0.46, wpb=65530.9, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.965, loss_scale=16, train_wall=215, gb_free=13.4, wall=2917
2022-03-01 10:21:52 | INFO | train_inner | epoch 004:    226 / 393 loss=8.091, ppl=272.6, wps=29884.9, ups=0.46, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.984, loss_scale=16, train_wall=214, gb_free=13.4, wall=3136
2022-03-01 10:25:31 | INFO | train_inner | epoch 004:    326 / 393 loss=7.966, ppl=250.06, wps=29867.9, ups=0.46, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.993, loss_scale=16, train_wall=214, gb_free=13.4, wall=3356
2022-03-01 10:27:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 10:28:00 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.786 | ppl 220.77 | wps 75649.6 | wpb 2034.1 | bsz 4 | num_updates 1567 | best_loss 7.786
2022-03-01 10:28:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1567 updates
2022-03-01 10:28:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 10:28:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 10:28:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 4 @ 1567 updates, score 7.786) (writing took 10.643760115141049 seconds)
2022-03-01 10:28:11 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-01 10:28:11 | INFO | train | epoch 004 | loss 8.068 | ppl 268.3 | wps 29400.4 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 1567 | lr 0.000195936 | gnorm 0.981 | loss_scale 32 | train_wall 842 | gb_free 13.4 | wall 3515
2022-03-01 10:28:11 | INFO | fairseq.trainer | begin training epoch 5
2022-03-01 10:28:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 10:29:23 | INFO | train_inner | epoch 005:     33 / 393 loss=7.818, ppl=225.6, wps=28110.8, ups=0.43, wpb=65248.6, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.991, loss_scale=32, train_wall=214, gb_free=13.4, wall=3588
2022-03-01 10:33:03 | INFO | train_inner | epoch 005:    133 / 393 loss=7.668, ppl=203.31, wps=29859.7, ups=0.46, wpb=65535.4, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.953, loss_scale=32, train_wall=215, gb_free=13.4, wall=3807
2022-03-01 10:36:42 | INFO | train_inner | epoch 005:    233 / 393 loss=7.569, ppl=189.9, wps=29864, ups=0.46, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.951, loss_scale=32, train_wall=215, gb_free=13.4, wall=4027
2022-03-01 10:40:22 | INFO | train_inner | epoch 005:    333 / 393 loss=7.473, ppl=177.67, wps=29833.7, ups=0.46, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.946, loss_scale=32, train_wall=215, gb_free=13.4, wall=4246
2022-03-01 10:42:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 10:42:36 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.372 | ppl 165.71 | wps 75219 | wpb 2034.1 | bsz 4 | num_updates 1960 | best_loss 7.372
2022-03-01 10:42:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1960 updates
2022-03-01 10:42:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 10:42:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 10:42:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 5 @ 1960 updates, score 7.372) (writing took 10.698347599012777 seconds)
2022-03-01 10:42:46 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-01 10:42:46 | INFO | train | epoch 005 | loss 7.557 | ppl 188.25 | wps 29379.6 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 1960 | lr 0.000245051 | gnorm 0.953 | loss_scale 32 | train_wall 843 | gb_free 13.4 | wall 4391
2022-03-01 10:42:46 | INFO | fairseq.trainer | begin training epoch 6
2022-03-01 10:42:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 10:44:14 | INFO | train_inner | epoch 006:     40 / 393 loss=7.341, ppl=162.09, wps=28054.6, ups=0.43, wpb=65244.2, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.927, loss_scale=32, train_wall=214, gb_free=13.4, wall=4479
2022-03-01 10:46:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-01 10:47:56 | INFO | train_inner | epoch 006:    141 / 393 loss=7.216, ppl=148.65, wps=29580.2, ups=0.45, wpb=65530.2, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.921, loss_scale=32, train_wall=217, gb_free=13.4, wall=4700
2022-03-01 10:51:35 | INFO | train_inner | epoch 006:    241 / 393 loss=7.135, ppl=140.58, wps=29861.4, ups=0.46, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.898, loss_scale=32, train_wall=215, gb_free=13.4, wall=4920
2022-03-01 10:52:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 10:55:17 | INFO | train_inner | epoch 006:    342 / 393 loss=7.06, ppl=133.47, wps=29594.6, ups=0.45, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.887, loss_scale=16, train_wall=216, gb_free=13.4, wall=5141
2022-03-01 10:57:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 10:57:11 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.056 | ppl 133.05 | wps 75662.1 | wpb 2034.1 | bsz 4 | num_updates 2351 | best_loss 7.056
2022-03-01 10:57:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2351 updates
2022-03-01 10:57:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 10:57:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 10:57:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 6 @ 2351 updates, score 7.056) (writing took 10.519181993091479 seconds)
2022-03-01 10:57:21 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-01 10:57:21 | INFO | train | epoch 006 | loss 7.132 | ppl 140.29 | wps 29259.2 | ups 0.45 | wpb 65461.2 | bsz 127.9 | num_updates 2351 | lr 0.000293916 | gnorm 0.905 | loss_scale 16 | train_wall 842 | gb_free 13.4 | wall 5266
2022-03-01 10:57:21 | INFO | fairseq.trainer | begin training epoch 7
2022-03-01 10:57:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 10:59:09 | INFO | train_inner | epoch 007:     49 / 393 loss=6.932, ppl=122.1, wps=28134.3, ups=0.43, wpb=65249.3, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.906, loss_scale=16, train_wall=213, gb_free=13.4, wall=5373
2022-03-01 11:02:48 | INFO | train_inner | epoch 007:    149 / 393 loss=6.839, ppl=114.52, wps=29869.8, ups=0.46, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.881, loss_scale=16, train_wall=214, gb_free=13.4, wall=5593
2022-03-01 11:06:28 | INFO | train_inner | epoch 007:    249 / 393 loss=6.765, ppl=108.74, wps=29847.1, ups=0.46, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.843, loss_scale=16, train_wall=215, gb_free=13.4, wall=5812
2022-03-01 11:10:07 | INFO | train_inner | epoch 007:    349 / 393 loss=6.726, ppl=105.83, wps=29859.7, ups=0.46, wpb=65535.4, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.844, loss_scale=16, train_wall=215, gb_free=13.4, wall=6032
2022-03-01 11:11:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 11:11:46 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 6.805 | ppl 111.79 | wps 74922.5 | wpb 2034.1 | bsz 4 | num_updates 2744 | best_loss 6.805
2022-03-01 11:11:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2744 updates
2022-03-01 11:11:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 11:11:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 11:11:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 7 @ 2744 updates, score 6.805) (writing took 10.506086992099881 seconds)
2022-03-01 11:11:56 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-01 11:11:56 | INFO | train | epoch 007 | loss 6.776 | ppl 109.58 | wps 29396.8 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 2744 | lr 0.000343031 | gnorm 0.858 | loss_scale 32 | train_wall 842 | gb_free 13.4 | wall 6141
2022-03-01 11:11:56 | INFO | fairseq.trainer | begin training epoch 8
2022-03-01 11:11:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 11:13:59 | INFO | train_inner | epoch 008:     56 / 393 loss=6.582, ppl=95.82, wps=28104.8, ups=0.43, wpb=65243.5, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.836, loss_scale=32, train_wall=214, gb_free=13.4, wall=6264
2022-03-01 11:14:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 11:17:41 | INFO | train_inner | epoch 008:    157 / 393 loss=6.511, ppl=91.22, wps=29539.3, ups=0.45, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.831, loss_scale=16, train_wall=217, gb_free=13.4, wall=6486
2022-03-01 11:21:21 | INFO | train_inner | epoch 008:    257 / 393 loss=6.495, ppl=90.19, wps=29849.2, ups=0.46, wpb=65530.9, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.825, loss_scale=16, train_wall=215, gb_free=13.4, wall=6705
2022-03-01 11:25:01 | INFO | train_inner | epoch 008:    357 / 393 loss=6.439, ppl=86.78, wps=29791.1, ups=0.45, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.827, loss_scale=16, train_wall=215, gb_free=13.4, wall=6925
2022-03-01 11:26:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 11:26:22 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.623 | ppl 98.6 | wps 75199.9 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 6.623
2022-03-01 11:26:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3136 updates
2022-03-01 11:26:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 11:26:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 11:26:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 8 @ 3136 updates, score 6.623) (writing took 10.282189704012126 seconds)
2022-03-01 11:26:32 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-01 11:26:32 | INFO | train | epoch 008 | loss 6.478 | ppl 89.17 | wps 29300.1 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 3136 | lr 0.000392022 | gnorm 0.828 | loss_scale 16 | train_wall 843 | gb_free 13.4 | wall 7017
2022-03-01 11:26:32 | INFO | fairseq.trainer | begin training epoch 9
2022-03-01 11:26:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 11:28:53 | INFO | train_inner | epoch 009:     64 / 393 loss=6.309, ppl=79.28, wps=28110.9, ups=0.43, wpb=65249.3, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.805, loss_scale=16, train_wall=214, gb_free=13.4, wall=7157
2022-03-01 11:32:33 | INFO | train_inner | epoch 009:    164 / 393 loss=6.239, ppl=75.53, wps=29817.2, ups=0.46, wpb=65530.9, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.801, loss_scale=16, train_wall=215, gb_free=13.4, wall=7377
2022-03-01 11:34:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 11:36:15 | INFO | train_inner | epoch 009:    265 / 393 loss=6.23, ppl=75.09, wps=29520.7, ups=0.45, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.791, loss_scale=16, train_wall=217, gb_free=13.4, wall=7599
2022-03-01 11:39:54 | INFO | train_inner | epoch 009:    365 / 393 loss=6.215, ppl=74.27, wps=29835.8, ups=0.46, wpb=65535.4, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.778, loss_scale=16, train_wall=215, gb_free=13.4, wall=7819
2022-03-01 11:40:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 11:40:58 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.46 | ppl 88.03 | wps 74719.8 | wpb 2034.1 | bsz 4 | num_updates 3528 | best_loss 6.46
2022-03-01 11:40:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3528 updates
2022-03-01 11:40:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 11:41:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 11:41:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 9 @ 3528 updates, score 6.46) (writing took 10.563685910077766 seconds)
2022-03-01 11:41:08 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-01 11:41:08 | INFO | train | epoch 009 | loss 6.231 | ppl 75.1 | wps 29284 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 3528 | lr 0.000441012 | gnorm 0.787 | loss_scale 16 | train_wall 843 | gb_free 13.4 | wall 7893
2022-03-01 11:41:08 | INFO | fairseq.trainer | begin training epoch 10
2022-03-01 11:41:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 11:43:47 | INFO | train_inner | epoch 010:     72 / 393 loss=6.069, ppl=67.13, wps=28080, ups=0.43, wpb=65249.3, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.778, loss_scale=16, train_wall=214, gb_free=13.4, wall=8051
2022-03-01 11:47:26 | INFO | train_inner | epoch 010:    172 / 393 loss=6.017, ppl=64.76, wps=29814.3, ups=0.45, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.76, loss_scale=16, train_wall=215, gb_free=13.4, wall=8271
2022-03-01 11:51:06 | INFO | train_inner | epoch 010:    272 / 393 loss=6.028, ppl=65.24, wps=29846.3, ups=0.46, wpb=65530.9, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.775, loss_scale=16, train_wall=215, gb_free=13.4, wall=8490
2022-03-01 11:53:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 11:54:48 | INFO | train_inner | epoch 010:    373 / 393 loss=6.016, ppl=64.72, wps=29583.5, ups=0.45, wpb=65535.4, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.747, loss_scale=16, train_wall=217, gb_free=13.4, wall=8712
2022-03-01 11:55:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 11:55:34 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.37 | ppl 82.71 | wps 75052 | wpb 2034.1 | bsz 4 | num_updates 3920 | best_loss 6.37
2022-03-01 11:55:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3920 updates
2022-03-01 11:55:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 11:55:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 11:55:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 10 @ 3920 updates, score 6.37) (writing took 10.372875778004527 seconds)
2022-03-01 11:55:44 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-01 11:55:44 | INFO | train | epoch 010 | loss 6.02 | ppl 64.91 | wps 29306.4 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 3920 | lr 0.000490002 | gnorm 0.771 | loss_scale 16 | train_wall 843 | gb_free 13.4 | wall 8768
2022-03-01 11:55:44 | INFO | fairseq.trainer | begin training epoch 11
2022-03-01 11:55:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 11:58:40 | INFO | train_inner | epoch 011:     80 / 393 loss=5.856, ppl=57.92, wps=28125, ups=0.43, wpb=65248.6, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.755, loss_scale=16, train_wall=214, gb_free=13.4, wall=8944
2022-03-01 12:02:19 | INFO | train_inner | epoch 011:    180 / 393 loss=5.825, ppl=56.68, wps=29850.7, ups=0.46, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.732, loss_scale=16, train_wall=215, gb_free=13.4, wall=9164
2022-03-01 12:05:59 | INFO | train_inner | epoch 011:    280 / 393 loss=5.837, ppl=57.18, wps=29857, ups=0.46, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.715, loss_scale=16, train_wall=215, gb_free=13.4, wall=9383
2022-03-01 12:09:38 | INFO | train_inner | epoch 011:    380 / 393 loss=5.84, ppl=57.27, wps=29852.5, ups=0.46, wpb=65530.9, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.699, loss_scale=16, train_wall=215, gb_free=13.4, wall=9603
2022-03-01 12:10:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 12:10:09 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.268 | ppl 77.04 | wps 75051.4 | wpb 2034.1 | bsz 4 | num_updates 4313 | best_loss 6.268
2022-03-01 12:10:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4313 updates
2022-03-01 12:10:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 12:10:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 12:10:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 11 @ 4313 updates, score 6.268) (writing took 10.56268288102001 seconds)
2022-03-01 12:10:19 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-01 12:10:19 | INFO | train | epoch 011 | loss 5.831 | ppl 56.93 | wps 29394.6 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 4313 | lr 0.000481515 | gnorm 0.72 | loss_scale 16 | train_wall 842 | gb_free 13.4 | wall 9644
2022-03-01 12:10:19 | INFO | fairseq.trainer | begin training epoch 12
2022-03-01 12:10:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 12:12:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 12:13:32 | INFO | train_inner | epoch 012:     88 / 393 loss=5.643, ppl=49.96, wps=27840.3, ups=0.43, wpb=65249.3, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.689, loss_scale=16, train_wall=216, gb_free=13.4, wall=9837
2022-03-01 12:17:12 | INFO | train_inner | epoch 012:    188 / 393 loss=5.65, ppl=50.21, wps=29838.2, ups=0.46, wpb=65530.9, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.69, loss_scale=16, train_wall=215, gb_free=13.4, wall=10057
2022-03-01 12:20:52 | INFO | train_inner | epoch 012:    288 / 393 loss=5.663, ppl=50.68, wps=29865.8, ups=0.46, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.673, loss_scale=16, train_wall=215, gb_free=13.4, wall=10276
2022-03-01 12:24:31 | INFO | train_inner | epoch 012:    388 / 393 loss=5.672, ppl=50.98, wps=29854.8, ups=0.46, wpb=65535.4, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.663, loss_scale=16, train_wall=215, gb_free=13.4, wall=10496
2022-03-01 12:24:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 12:24:44 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.209 | ppl 73.96 | wps 74839.7 | wpb 2034.1 | bsz 4 | num_updates 4705 | best_loss 6.209
2022-03-01 12:24:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4705 updates
2022-03-01 12:24:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 12:24:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 12:24:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 12 @ 4705 updates, score 6.209) (writing took 10.621871944982558 seconds)
2022-03-01 12:24:55 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-01 12:24:55 | INFO | train | epoch 012 | loss 5.65 | ppl 50.22 | wps 29311.5 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 4705 | lr 0.00046102 | gnorm 0.679 | loss_scale 16 | train_wall 842 | gb_free 13.4 | wall 10519
2022-03-01 12:24:55 | INFO | fairseq.trainer | begin training epoch 13
2022-03-01 12:24:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 12:28:23 | INFO | train_inner | epoch 013:     95 / 393 loss=5.464, ppl=44.15, wps=28102.7, ups=0.43, wpb=65243.5, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.666, loss_scale=16, train_wall=214, gb_free=13.4, wall=10728
2022-03-01 12:32:03 | INFO | train_inner | epoch 013:    195 / 393 loss=5.485, ppl=44.79, wps=29828.7, ups=0.46, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.673, loss_scale=32, train_wall=215, gb_free=13.4, wall=10947
2022-03-01 12:33:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 12:35:45 | INFO | train_inner | epoch 013:    296 / 393 loss=5.518, ppl=45.82, wps=29527.4, ups=0.45, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.661, loss_scale=16, train_wall=217, gb_free=13.4, wall=11169
2022-03-01 12:39:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 12:39:20 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.185 | ppl 72.73 | wps 74703.4 | wpb 2034.1 | bsz 4 | num_updates 5097 | best_loss 6.185
2022-03-01 12:39:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5097 updates
2022-03-01 12:39:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 12:39:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 12:39:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 13 @ 5097 updates, score 6.185) (writing took 10.807696461910382 seconds)
2022-03-01 12:39:31 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-01 12:39:31 | INFO | train | epoch 013 | loss 5.498 | ppl 45.2 | wps 29285.4 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 5097 | lr 0.000442938 | gnorm 0.662 | loss_scale 16 | train_wall 843 | gb_free 13.4 | wall 11395
2022-03-01 12:39:31 | INFO | fairseq.trainer | begin training epoch 14
2022-03-01 12:39:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 12:39:38 | INFO | train_inner | epoch 014:      3 / 393 loss=5.527, ppl=46.11, wps=28038.9, ups=0.43, wpb=65249.3, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.656, loss_scale=16, train_wall=214, gb_free=13.4, wall=11402
2022-03-01 12:43:17 | INFO | train_inner | epoch 014:    103 / 393 loss=5.324, ppl=40.07, wps=29864.9, ups=0.46, wpb=65530.9, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.647, loss_scale=16, train_wall=215, gb_free=13.4, wall=11621
2022-03-01 12:46:56 | INFO | train_inner | epoch 014:    203 / 393 loss=5.362, ppl=41.13, wps=29873.5, ups=0.46, wpb=65535.4, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.644, loss_scale=16, train_wall=214, gb_free=13.4, wall=11841
2022-03-01 12:50:36 | INFO | train_inner | epoch 014:    303 / 393 loss=5.387, ppl=41.83, wps=29869.6, ups=0.46, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.663, loss_scale=16, train_wall=215, gb_free=13.4, wall=12060
2022-03-01 12:52:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 12:53:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 12:53:55 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.162 | ppl 71.61 | wps 74977.9 | wpb 2034.1 | bsz 4 | num_updates 5489 | best_loss 6.162
2022-03-01 12:53:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5489 updates
2022-03-01 12:53:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 12:54:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 12:54:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 14 @ 5489 updates, score 6.162) (writing took 10.185615317896008 seconds)
2022-03-01 12:54:05 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-01 12:54:05 | INFO | train | epoch 014 | loss 5.366 | ppl 41.25 | wps 29342.7 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 5489 | lr 0.000426828 | gnorm 0.644 | loss_scale 16 | train_wall 842 | gb_free 13.4 | wall 12270
2022-03-01 12:54:05 | INFO | fairseq.trainer | begin training epoch 15
2022-03-01 12:54:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 12:54:30 | INFO | train_inner | epoch 015:     11 / 393 loss=5.374, ppl=41.48, wps=27904.9, ups=0.43, wpb=65249.3, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.637, loss_scale=16, train_wall=216, gb_free=13.4, wall=12294
2022-03-01 12:58:09 | INFO | train_inner | epoch 015:    111 / 393 loss=5.204, ppl=36.85, wps=29833.5, ups=0.46, wpb=65535.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.671, loss_scale=16, train_wall=215, gb_free=13.4, wall=12514
2022-03-01 13:01:49 | INFO | train_inner | epoch 015:    211 / 393 loss=5.241, ppl=37.82, wps=29826.7, ups=0.46, wpb=65530.9, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.646, loss_scale=16, train_wall=215, gb_free=13.4, wall=12733
2022-03-01 13:05:29 | INFO | train_inner | epoch 015:    311 / 393 loss=5.281, ppl=38.88, wps=29854.4, ups=0.46, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.656, loss_scale=16, train_wall=215, gb_free=13.4, wall=12953
2022-03-01 13:08:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 13:08:31 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.146 | ppl 70.83 | wps 75321.6 | wpb 2034.1 | bsz 4 | num_updates 5882 | best_loss 6.146
2022-03-01 13:08:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5882 updates
2022-03-01 13:08:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 13:08:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 13:08:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 15 @ 5882 updates, score 6.146) (writing took 11.066783988149837 seconds)
2022-03-01 13:08:42 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-01 13:08:42 | INFO | train | epoch 015 | loss 5.252 | ppl 38.11 | wps 29354.1 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 5882 | lr 0.000412323 | gnorm 0.655 | loss_scale 16 | train_wall 843 | gb_free 13.4 | wall 13146
2022-03-01 13:08:42 | INFO | fairseq.trainer | begin training epoch 16
2022-03-01 13:08:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 13:09:22 | INFO | train_inner | epoch 016:     18 / 393 loss=5.255, ppl=38.19, wps=27997.1, ups=0.43, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.626, loss_scale=16, train_wall=214, gb_free=13.4, wall=13186
2022-03-01 13:13:01 | INFO | train_inner | epoch 016:    118 / 393 loss=5.109, ppl=34.52, wps=29842.4, ups=0.46, wpb=65530.2, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.648, loss_scale=32, train_wall=215, gb_free=13.4, wall=13406
2022-03-01 13:16:41 | INFO | train_inner | epoch 016:    218 / 393 loss=5.144, ppl=35.36, wps=29808.9, ups=0.45, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.645, loss_scale=32, train_wall=215, gb_free=13.4, wall=13625
2022-03-01 13:17:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 13:20:23 | INFO | train_inner | epoch 016:    319 / 393 loss=5.175, ppl=36.13, wps=29562.9, ups=0.45, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.651, loss_scale=16, train_wall=217, gb_free=13.4, wall=13847
2022-03-01 13:23:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 13:23:07 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.152 | ppl 71.12 | wps 74998.4 | wpb 2034.1 | bsz 4 | num_updates 6274 | best_loss 6.146
2022-03-01 13:23:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6274 updates
2022-03-01 13:23:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 13:23:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 13:23:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 16 @ 6274 updates, score 6.152) (writing took 4.370181550970301 seconds)
2022-03-01 13:23:11 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-01 13:23:11 | INFO | train | epoch 016 | loss 5.15 | ppl 35.5 | wps 29509.1 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 6274 | lr 0.000399234 | gnorm 0.646 | loss_scale 16 | train_wall 843 | gb_free 13.4 | wall 14016
2022-03-01 13:23:11 | INFO | fairseq.trainer | begin training epoch 17
2022-03-01 13:23:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 13:24:09 | INFO | train_inner | epoch 017:     26 / 393 loss=5.138, ppl=35.22, wps=28878.9, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.668, loss_scale=16, train_wall=214, gb_free=13.4, wall=14073
2022-03-01 13:27:48 | INFO | train_inner | epoch 017:    126 / 393 loss=5.011, ppl=32.25, wps=29891.8, ups=0.46, wpb=65530.9, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.65, loss_scale=16, train_wall=214, gb_free=13.4, wall=14292
2022-03-01 13:31:27 | INFO | train_inner | epoch 017:    226 / 393 loss=5.052, ppl=33.17, wps=29896.9, ups=0.46, wpb=65535.4, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.655, loss_scale=16, train_wall=214, gb_free=13.4, wall=14512
2022-03-01 13:35:06 | INFO | train_inner | epoch 017:    326 / 393 loss=5.092, ppl=34.1, wps=29883.9, ups=0.46, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.65, loss_scale=16, train_wall=214, gb_free=13.4, wall=14731
2022-03-01 13:37:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 13:37:35 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.139 | ppl 70.48 | wps 74952.2 | wpb 2034.1 | bsz 4 | num_updates 6667 | best_loss 6.139
2022-03-01 13:37:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6667 updates
2022-03-01 13:37:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 13:37:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-01 13:37:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 17 @ 6667 updates, score 6.139) (writing took 9.50155788497068 seconds)
2022-03-01 13:37:45 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-01 13:37:45 | INFO | train | epoch 017 | loss 5.057 | ppl 33.29 | wps 29458.1 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 6667 | lr 0.000387289 | gnorm 0.654 | loss_scale 32 | train_wall 841 | gb_free 13.4 | wall 14889
2022-03-01 13:37:45 | INFO | fairseq.trainer | begin training epoch 18
2022-03-01 13:37:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 13:38:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 13:38:59 | INFO | train_inner | epoch 018:     34 / 393 loss=5.04, ppl=32.9, wps=28006.8, ups=0.43, wpb=65248.6, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.645, loss_scale=16, train_wall=216, gb_free=13.4, wall=14964
2022-03-01 13:42:39 | INFO | train_inner | epoch 018:    134 / 393 loss=4.93, ppl=30.48, wps=29861.8, ups=0.46, wpb=65530.9, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.643, loss_scale=16, train_wall=214, gb_free=13.4, wall=15183
2022-03-01 13:46:18 | INFO | train_inner | epoch 018:    234 / 393 loss=4.974, ppl=31.42, wps=29849.6, ups=0.46, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.68, loss_scale=16, train_wall=215, gb_free=13.4, wall=15403
2022-03-01 13:49:58 | INFO | train_inner | epoch 018:    334 / 393 loss=5.01, ppl=32.21, wps=29858.5, ups=0.46, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.646, loss_scale=16, train_wall=215, gb_free=13.4, wall=15622
2022-03-01 13:52:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 13:52:09 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.159 | ppl 71.47 | wps 75110.1 | wpb 2034.1 | bsz 4 | num_updates 7059 | best_loss 6.139
2022-03-01 13:52:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7059 updates
2022-03-01 13:52:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 13:52:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 13:52:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 18 @ 7059 updates, score 6.159) (writing took 4.343496546847746 seconds)
2022-03-01 13:52:14 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-01 13:52:14 | INFO | train | epoch 018 | loss 4.973 | ppl 31.4 | wps 29536 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 7059 | lr 0.000376382 | gnorm 0.657 | loss_scale 16 | train_wall 842 | gb_free 13.4 | wall 15758
2022-03-01 13:52:14 | INFO | fairseq.trainer | begin training epoch 19
2022-03-01 13:52:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 13:53:44 | INFO | train_inner | epoch 019:     41 / 393 loss=4.94, ppl=30.71, wps=28908.6, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.647, loss_scale=16, train_wall=213, gb_free=13.4, wall=15848
2022-03-01 13:57:23 | INFO | train_inner | epoch 019:    141 / 393 loss=4.859, ppl=29.02, wps=29878.6, ups=0.46, wpb=65530.2, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.664, loss_scale=32, train_wall=214, gb_free=13.4, wall=16067
2022-03-01 13:59:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 14:01:04 | INFO | train_inner | epoch 019:    242 / 393 loss=4.903, ppl=29.91, wps=29583.6, ups=0.45, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.676, loss_scale=16, train_wall=217, gb_free=13.4, wall=16289
2022-03-01 14:04:44 | INFO | train_inner | epoch 019:    342 / 393 loss=4.93, ppl=30.48, wps=29875.2, ups=0.46, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.682, loss_scale=16, train_wall=214, gb_free=13.4, wall=16508
2022-03-01 14:06:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 14:06:38 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.168 | ppl 71.89 | wps 74692.8 | wpb 2034.1 | bsz 4 | num_updates 7451 | best_loss 6.139
2022-03-01 14:06:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7451 updates
2022-03-01 14:06:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 14:06:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 14:06:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 19 @ 7451 updates, score 6.168) (writing took 4.6691311951726675 seconds)
2022-03-01 14:06:42 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-01 14:06:42 | INFO | train | epoch 019 | loss 4.896 | ppl 29.77 | wps 29537.9 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 7451 | lr 0.000366347 | gnorm 0.668 | loss_scale 16 | train_wall 842 | gb_free 13.4 | wall 16627
2022-03-01 14:06:42 | INFO | fairseq.trainer | begin training epoch 20
2022-03-01 14:06:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 14:08:30 | INFO | train_inner | epoch 020:     49 / 393 loss=4.85, ppl=28.83, wps=28860, ups=0.44, wpb=65248.6, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.665, loss_scale=16, train_wall=213, gb_free=13.4, wall=16734
2022-03-01 14:12:09 | INFO | train_inner | epoch 020:    149 / 393 loss=4.788, ppl=27.63, wps=29847.4, ups=0.46, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.662, loss_scale=16, train_wall=215, gb_free=13.4, wall=16954
2022-03-01 14:15:49 | INFO | train_inner | epoch 020:    249 / 393 loss=4.829, ppl=28.42, wps=29868.3, ups=0.46, wpb=65530.9, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.665, loss_scale=16, train_wall=214, gb_free=13.4, wall=17173
2022-03-01 14:18:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 14:19:30 | INFO | train_inner | epoch 020:    350 / 393 loss=4.867, ppl=29.19, wps=29590.2, ups=0.45, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.674, loss_scale=16, train_wall=217, gb_free=13.4, wall=17395
2022-03-01 14:21:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 14:21:07 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.186 | ppl 72.79 | wps 75087 | wpb 2034.1 | bsz 4 | num_updates 7843 | best_loss 6.139
2022-03-01 14:21:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7843 updates
2022-03-01 14:21:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 14:21:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 14:21:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 20 @ 7843 updates, score 6.186) (writing took 4.4259928681422025 seconds)
2022-03-01 14:21:11 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-01 14:21:11 | INFO | train | epoch 020 | loss 4.824 | ppl 28.32 | wps 29538.7 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 7843 | lr 0.000357075 | gnorm 0.672 | loss_scale 16 | train_wall 842 | gb_free 13.4 | wall 17495
2022-03-01 14:21:11 | INFO | fairseq.trainer | begin training epoch 21
2022-03-01 14:21:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 14:23:16 | INFO | train_inner | epoch 021:     57 / 393 loss=4.768, ppl=27.26, wps=28892.4, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.684, loss_scale=16, train_wall=213, gb_free=13.4, wall=17621
2022-03-01 14:26:55 | INFO | train_inner | epoch 021:    157 / 393 loss=4.714, ppl=26.24, wps=29885, ups=0.46, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.671, loss_scale=16, train_wall=214, gb_free=13.4, wall=17840
2022-03-01 14:30:35 | INFO | train_inner | epoch 021:    257 / 393 loss=4.763, ppl=27.15, wps=29895.1, ups=0.46, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.674, loss_scale=16, train_wall=214, gb_free=13.4, wall=18059
2022-03-01 14:34:14 | INFO | train_inner | epoch 021:    357 / 393 loss=4.807, ppl=28, wps=29898.2, ups=0.46, wpb=65530.9, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.679, loss_scale=16, train_wall=214, gb_free=13.4, wall=18278
2022-03-01 14:35:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 14:35:35 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.219 | ppl 74.49 | wps 75817.3 | wpb 2034.1 | bsz 4 | num_updates 8236 | best_loss 6.139
2022-03-01 14:35:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8236 updates
2022-03-01 14:35:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 14:35:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 14:35:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 21 @ 8236 updates, score 6.219) (writing took 4.369016629876569 seconds)
2022-03-01 14:35:39 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-01 14:35:39 | INFO | train | epoch 021 | loss 4.758 | ppl 27.05 | wps 29635.4 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 8236 | lr 0.000348451 | gnorm 0.669 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 18364
2022-03-01 14:35:39 | INFO | fairseq.trainer | begin training epoch 22
2022-03-01 14:35:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 14:38:00 | INFO | train_inner | epoch 022:     64 / 393 loss=4.702, ppl=26.03, wps=28905.8, ups=0.44, wpb=65248.6, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.668, loss_scale=32, train_wall=213, gb_free=13.4, wall=18504
2022-03-01 14:41:39 | INFO | train_inner | epoch 022:    164 / 393 loss=4.664, ppl=25.36, wps=29888.1, ups=0.46, wpb=65535.4, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.675, loss_scale=32, train_wall=214, gb_free=13.4, wall=18723
2022-03-01 14:44:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 14:45:20 | INFO | train_inner | epoch 022:    265 / 393 loss=4.707, ppl=26.11, wps=29585.1, ups=0.45, wpb=65530.9, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.688, loss_scale=16, train_wall=217, gb_free=13.4, wall=18945
2022-03-01 14:48:59 | INFO | train_inner | epoch 022:    365 / 393 loss=4.749, ppl=26.89, wps=29903.5, ups=0.46, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.683, loss_scale=16, train_wall=214, gb_free=13.4, wall=19164
2022-03-01 14:50:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 14:50:03 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.225 | ppl 74.81 | wps 75183.2 | wpb 2034.1 | bsz 4 | num_updates 8628 | best_loss 6.139
2022-03-01 14:50:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8628 updates
2022-03-01 14:50:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 14:50:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 14:50:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 22 @ 8628 updates, score 6.225) (writing took 4.315265822922811 seconds)
2022-03-01 14:50:07 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-01 14:50:07 | INFO | train | epoch 022 | loss 4.696 | ppl 25.91 | wps 29561.9 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 8628 | lr 0.000340443 | gnorm 0.681 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 19232
2022-03-01 14:50:07 | INFO | fairseq.trainer | begin training epoch 23
2022-03-01 14:50:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 14:52:45 | INFO | train_inner | epoch 023:     72 / 393 loss=4.613, ppl=24.47, wps=28927.5, ups=0.44, wpb=65248.6, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.663, loss_scale=16, train_wall=213, gb_free=13.4, wall=19390
2022-03-01 14:56:24 | INFO | train_inner | epoch 023:    172 / 393 loss=4.616, ppl=24.53, wps=29899.6, ups=0.46, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.695, loss_scale=16, train_wall=214, gb_free=13.4, wall=19609
2022-03-01 15:00:03 | INFO | train_inner | epoch 023:    272 / 393 loss=4.661, ppl=25.29, wps=29903, ups=0.46, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.699, loss_scale=16, train_wall=214, gb_free=13.4, wall=19828
2022-03-01 15:03:42 | INFO | train_inner | epoch 023:    372 / 393 loss=4.686, ppl=25.74, wps=29911.8, ups=0.46, wpb=65530.9, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.698, loss_scale=32, train_wall=214, gb_free=13.4, wall=20047
2022-03-01 15:04:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 15:04:31 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.24 | ppl 75.59 | wps 75258.7 | wpb 2034.1 | bsz 4 | num_updates 9021 | best_loss 6.139
2022-03-01 15:04:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9021 updates
2022-03-01 15:04:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 15:04:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 15:04:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 23 @ 9021 updates, score 6.24) (writing took 4.248793330043554 seconds)
2022-03-01 15:04:35 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-01 15:04:35 | INFO | train | epoch 023 | loss 4.638 | ppl 24.9 | wps 29650.1 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 9021 | lr 0.000332945 | gnorm 0.69 | loss_scale 32 | train_wall 841 | gb_free 13.4 | wall 20099
2022-03-01 15:04:35 | INFO | fairseq.trainer | begin training epoch 24
2022-03-01 15:04:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 15:05:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 15:07:30 | INFO | train_inner | epoch 024:     80 / 393 loss=4.539, ppl=23.24, wps=28639, ups=0.44, wpb=65248.6, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.695, loss_scale=16, train_wall=216, gb_free=13.4, wall=20275
2022-03-01 15:11:09 | INFO | train_inner | epoch 024:    180 / 393 loss=4.555, ppl=23.51, wps=29907.9, ups=0.46, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.709, loss_scale=16, train_wall=214, gb_free=13.4, wall=20494
2022-03-01 15:14:49 | INFO | train_inner | epoch 024:    280 / 393 loss=4.605, ppl=24.34, wps=29906.9, ups=0.46, wpb=65530.9, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.711, loss_scale=16, train_wall=214, gb_free=13.4, wall=20713
2022-03-01 15:18:28 | INFO | train_inner | epoch 024:    380 / 393 loss=4.65, ppl=25.11, wps=29889.2, ups=0.46, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.706, loss_scale=16, train_wall=214, gb_free=13.4, wall=20932
2022-03-01 15:18:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 15:18:58 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.247 | ppl 75.95 | wps 74588 | wpb 2034.1 | bsz 4 | num_updates 9413 | best_loss 6.139
2022-03-01 15:18:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9413 updates
2022-03-01 15:18:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 15:19:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 15:19:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 24 @ 9413 updates, score 6.247) (writing took 4.227765610907227 seconds)
2022-03-01 15:19:03 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-01 15:19:03 | INFO | train | epoch 024 | loss 4.583 | ppl 23.97 | wps 29570.8 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 9413 | lr 0.000325939 | gnorm 0.706 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 20967
2022-03-01 15:19:03 | INFO | fairseq.trainer | begin training epoch 25
2022-03-01 15:19:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 15:22:13 | INFO | train_inner | epoch 025:     87 / 393 loss=4.473, ppl=22.21, wps=28930.2, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.686, loss_scale=16, train_wall=213, gb_free=13.4, wall=21158
2022-03-01 15:24:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 15:25:55 | INFO | train_inner | epoch 025:    188 / 393 loss=4.509, ppl=22.76, wps=29610.4, ups=0.45, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.723, loss_scale=16, train_wall=216, gb_free=13.4, wall=21379
2022-03-01 15:29:35 | INFO | train_inner | epoch 025:    288 / 393 loss=4.56, ppl=23.59, wps=29791.7, ups=0.45, wpb=65530.9, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.717, loss_scale=16, train_wall=215, gb_free=13.4, wall=21599
2022-03-01 15:33:14 | INFO | train_inner | epoch 025:    388 / 393 loss=4.594, ppl=24.16, wps=29909.7, ups=0.46, wpb=65535.4, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.718, loss_scale=16, train_wall=214, gb_free=13.4, wall=21818
2022-03-01 15:33:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 15:33:27 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.285 | ppl 77.97 | wps 75409.5 | wpb 2034.1 | bsz 4 | num_updates 9805 | best_loss 6.139
2022-03-01 15:33:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9805 updates
2022-03-01 15:33:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 15:33:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 15:33:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 25 @ 9805 updates, score 6.285) (writing took 4.090377143817022 seconds)
2022-03-01 15:33:31 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-01 15:33:31 | INFO | train | epoch 025 | loss 4.531 | ppl 23.12 | wps 29556.5 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 9805 | lr 0.000319357 | gnorm 0.712 | loss_scale 16 | train_wall 842 | gb_free 13.4 | wall 21835
2022-03-01 15:33:31 | INFO | fairseq.trainer | begin training epoch 26
2022-03-01 15:33:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 15:36:59 | INFO | train_inner | epoch 026:     95 / 393 loss=4.404, ppl=21.17, wps=28973.2, ups=0.44, wpb=65244.2, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.699, loss_scale=16, train_wall=213, gb_free=13.4, wall=22043
2022-03-01 15:40:38 | INFO | train_inner | epoch 026:    195 / 393 loss=4.47, ppl=22.17, wps=29907.9, ups=0.46, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.7, loss_scale=16, train_wall=214, gb_free=13.4, wall=22263
2022-03-01 15:43:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 15:44:19 | INFO | train_inner | epoch 026:    296 / 393 loss=4.51, ppl=22.78, wps=29604.3, ups=0.45, wpb=65535.4, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.721, loss_scale=16, train_wall=216, gb_free=13.4, wall=22484
2022-03-01 15:47:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 15:47:54 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.33 | ppl 80.42 | wps 74278 | wpb 2034.1 | bsz 4 | num_updates 10197 | best_loss 6.139
2022-03-01 15:47:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10197 updates
2022-03-01 15:47:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 15:47:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 15:47:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 26 @ 10197 updates, score 6.33) (writing took 4.283981696935371 seconds)
2022-03-01 15:47:58 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-01 15:47:58 | INFO | train | epoch 026 | loss 4.482 | ppl 22.35 | wps 29581.3 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 10197 | lr 0.000313158 | gnorm 0.709 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 22703
2022-03-01 15:47:58 | INFO | fairseq.trainer | begin training epoch 27
2022-03-01 15:47:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 15:48:05 | INFO | train_inner | epoch 027:      3 / 393 loss=4.543, ppl=23.31, wps=28938.8, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.716, loss_scale=16, train_wall=213, gb_free=13.4, wall=22709
2022-03-01 15:51:44 | INFO | train_inner | epoch 027:    103 / 393 loss=4.36, ppl=20.53, wps=29900, ups=0.46, wpb=65530.9, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.719, loss_scale=16, train_wall=214, gb_free=13.4, wall=22929
2022-03-01 15:55:23 | INFO | train_inner | epoch 027:    203 / 393 loss=4.424, ppl=21.46, wps=29910.5, ups=0.46, wpb=65535.4, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.721, loss_scale=16, train_wall=214, gb_free=13.4, wall=23148
2022-03-01 15:59:02 | INFO | train_inner | epoch 027:    303 / 393 loss=4.467, ppl=22.12, wps=29899.5, ups=0.46, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.724, loss_scale=16, train_wall=214, gb_free=13.4, wall=23367
2022-03-01 16:02:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 16:02:22 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.349 | ppl 81.53 | wps 75469.5 | wpb 2034.1 | bsz 4 | num_updates 10590 | best_loss 6.139
2022-03-01 16:02:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10590 updates
2022-03-01 16:02:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 16:02:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 16:02:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 27 @ 10590 updates, score 6.349) (writing took 4.504281939007342 seconds)
2022-03-01 16:02:26 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-01 16:02:26 | INFO | train | epoch 027 | loss 4.437 | ppl 21.66 | wps 29638.7 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 10590 | lr 0.000307293 | gnorm 0.717 | loss_scale 32 | train_wall 841 | gb_free 13.4 | wall 23571
2022-03-01 16:02:26 | INFO | fairseq.trainer | begin training epoch 28
2022-03-01 16:02:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 16:02:48 | INFO | train_inner | epoch 028:     10 / 393 loss=4.485, ppl=22.39, wps=28881.4, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.706, loss_scale=32, train_wall=214, gb_free=13.4, wall=23593
2022-03-01 16:03:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 16:06:30 | INFO | train_inner | epoch 028:    111 / 393 loss=4.323, ppl=20.01, wps=29619.8, ups=0.45, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.712, loss_scale=16, train_wall=216, gb_free=13.4, wall=23814
2022-03-01 16:10:09 | INFO | train_inner | epoch 028:    211 / 393 loss=4.38, ppl=20.82, wps=29899, ups=0.46, wpb=65530.9, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.729, loss_scale=16, train_wall=214, gb_free=13.4, wall=24033
2022-03-01 16:13:48 | INFO | train_inner | epoch 028:    311 / 393 loss=4.418, ppl=21.37, wps=29916.9, ups=0.46, wpb=65535.4, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.736, loss_scale=16, train_wall=214, gb_free=13.4, wall=24252
2022-03-01 16:16:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 16:16:50 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.375 | ppl 82.99 | wps 74870.3 | wpb 2034.1 | bsz 4 | num_updates 10982 | best_loss 6.139
2022-03-01 16:16:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10982 updates
2022-03-01 16:16:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 16:16:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 16:16:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 28 @ 10982 updates, score 6.375) (writing took 4.969099838053808 seconds)
2022-03-01 16:16:55 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-01 16:16:55 | INFO | train | epoch 028 | loss 4.392 | ppl 21 | wps 29552.1 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 10982 | lr 0.000301758 | gnorm 0.726 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 24439
2022-03-01 16:16:55 | INFO | fairseq.trainer | begin training epoch 29
2022-03-01 16:16:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 16:17:34 | INFO | train_inner | epoch 029:     18 / 393 loss=4.436, ppl=21.64, wps=28833.6, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.734, loss_scale=16, train_wall=213, gb_free=13.4, wall=24479
2022-03-01 16:21:13 | INFO | train_inner | epoch 029:    118 / 393 loss=4.285, ppl=19.49, wps=29892.4, ups=0.46, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.72, loss_scale=16, train_wall=214, gb_free=13.4, wall=24698
2022-03-01 16:24:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 16:24:55 | INFO | train_inner | epoch 029:    219 / 393 loss=4.345, ppl=20.32, wps=29580.9, ups=0.45, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.747, loss_scale=16, train_wall=217, gb_free=13.4, wall=24919
2022-03-01 16:28:34 | INFO | train_inner | epoch 029:    319 / 393 loss=4.39, ppl=20.97, wps=29908.3, ups=0.46, wpb=65530.2, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.751, loss_scale=16, train_wall=214, gb_free=13.4, wall=25138
2022-03-01 16:31:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 16:31:18 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.4 | ppl 84.47 | wps 75238 | wpb 2034.1 | bsz 4 | num_updates 11374 | best_loss 6.139
2022-03-01 16:31:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11374 updates
2022-03-01 16:31:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 16:31:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 16:31:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 29 @ 11374 updates, score 6.4) (writing took 5.082359284861013 seconds)
2022-03-01 16:31:23 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-01 16:31:23 | INFO | train | epoch 029 | loss 4.35 | ppl 20.4 | wps 29542.7 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 11374 | lr 0.000296513 | gnorm 0.741 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 25308
2022-03-01 16:31:23 | INFO | fairseq.trainer | begin training epoch 30
2022-03-01 16:31:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 16:32:20 | INFO | train_inner | epoch 030:     26 / 393 loss=4.366, ppl=20.61, wps=28831.2, ups=0.44, wpb=65244.2, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.738, loss_scale=16, train_wall=213, gb_free=13.4, wall=25365
2022-03-01 16:35:59 | INFO | train_inner | epoch 030:    126 / 393 loss=4.245, ppl=18.96, wps=29900.1, ups=0.46, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.739, loss_scale=16, train_wall=214, gb_free=13.4, wall=25584
2022-03-01 16:39:39 | INFO | train_inner | epoch 030:    226 / 393 loss=4.307, ppl=19.8, wps=29912.7, ups=0.46, wpb=65535.4, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.754, loss_scale=16, train_wall=214, gb_free=13.4, wall=25803
2022-03-01 16:43:18 | INFO | train_inner | epoch 030:    326 / 393 loss=4.352, ppl=20.42, wps=29932, ups=0.46, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.747, loss_scale=16, train_wall=214, gb_free=13.4, wall=26022
2022-03-01 16:44:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 16:45:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 16:45:46 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.433 | ppl 86.39 | wps 74475.1 | wpb 2034.1 | bsz 4 | num_updates 11766 | best_loss 6.139
2022-03-01 16:45:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11766 updates
2022-03-01 16:45:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 16:45:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 16:45:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 30 @ 11766 updates, score 6.433) (writing took 4.584680151892826 seconds)
2022-03-01 16:45:51 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-01 16:45:51 | INFO | train | epoch 030 | loss 4.31 | ppl 19.84 | wps 29574.4 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 11766 | lr 0.000291532 | gnorm 0.746 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 26175
2022-03-01 16:45:51 | INFO | fairseq.trainer | begin training epoch 31
2022-03-01 16:45:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 16:47:05 | INFO | train_inner | epoch 031:     34 / 393 loss=4.314, ppl=19.89, wps=28639.3, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.75, loss_scale=16, train_wall=215, gb_free=13.4, wall=26250
2022-03-01 16:50:44 | INFO | train_inner | epoch 031:    134 / 393 loss=4.222, ppl=18.67, wps=29914.9, ups=0.46, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.766, loss_scale=16, train_wall=214, gb_free=13.4, wall=26469
2022-03-01 16:54:24 | INFO | train_inner | epoch 031:    234 / 393 loss=4.26, ppl=19.16, wps=29899.9, ups=0.46, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.76, loss_scale=16, train_wall=214, gb_free=13.4, wall=26688
2022-03-01 16:58:03 | INFO | train_inner | epoch 031:    334 / 393 loss=4.321, ppl=19.98, wps=29916.5, ups=0.46, wpb=65530.2, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.76, loss_scale=16, train_wall=214, gb_free=13.4, wall=26907
2022-03-01 17:00:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 17:00:14 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.459 | ppl 87.99 | wps 74985.7 | wpb 2034.1 | bsz 4 | num_updates 12159 | best_loss 6.139
2022-03-01 17:00:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12159 updates
2022-03-01 17:00:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 17:00:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 17:00:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 31 @ 12159 updates, score 6.459) (writing took 4.282389498082921 seconds)
2022-03-01 17:00:18 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-01 17:00:18 | INFO | train | epoch 031 | loss 4.273 | ppl 19.33 | wps 29662.5 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 12159 | lr 0.000286781 | gnorm 0.766 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 27043
2022-03-01 17:00:18 | INFO | fairseq.trainer | begin training epoch 32
2022-03-01 17:00:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 17:01:48 | INFO | train_inner | epoch 032:     41 / 393 loss=4.27, ppl=19.29, wps=28945.2, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.779, loss_scale=16, train_wall=213, gb_free=13.4, wall=27133
2022-03-01 17:03:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 17:05:29 | INFO | train_inner | epoch 032:    142 / 393 loss=4.174, ppl=18.05, wps=29627.8, ups=0.45, wpb=65535.4, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.762, loss_scale=16, train_wall=216, gb_free=13.4, wall=27354
2022-03-01 17:09:08 | INFO | train_inner | epoch 032:    242 / 393 loss=4.239, ppl=18.89, wps=29926, ups=0.46, wpb=65530.9, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.751, loss_scale=16, train_wall=214, gb_free=13.4, wall=27573
2022-03-01 17:12:47 | INFO | train_inner | epoch 032:    342 / 393 loss=4.282, ppl=19.45, wps=29916.2, ups=0.46, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.751, loss_scale=16, train_wall=214, gb_free=13.4, wall=27792
2022-03-01 17:14:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 17:14:41 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.488 | ppl 89.77 | wps 75205.1 | wpb 2034.1 | bsz 4 | num_updates 12551 | best_loss 6.139
2022-03-01 17:14:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12551 updates
2022-03-01 17:14:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 17:14:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 17:14:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 32 @ 12551 updates, score 6.488) (writing took 4.242822143016383 seconds)
2022-03-01 17:14:45 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-01 17:14:45 | INFO | train | epoch 032 | loss 4.236 | ppl 18.84 | wps 29593.3 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 12551 | lr 0.000282267 | gnorm 0.759 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 27910
2022-03-01 17:14:45 | INFO | fairseq.trainer | begin training epoch 33
2022-03-01 17:14:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 17:16:33 | INFO | train_inner | epoch 033:     49 / 393 loss=4.227, ppl=18.72, wps=28965.2, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.766, loss_scale=16, train_wall=213, gb_free=13.4, wall=28017
2022-03-01 17:20:12 | INFO | train_inner | epoch 033:    149 / 393 loss=4.153, ppl=17.79, wps=29915.7, ups=0.46, wpb=65530.9, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.752, loss_scale=16, train_wall=214, gb_free=13.4, wall=28236
2022-03-01 17:22:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 17:23:53 | INFO | train_inner | epoch 033:    250 / 393 loss=4.197, ppl=18.34, wps=29639.8, ups=0.45, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.764, loss_scale=16, train_wall=216, gb_free=13.4, wall=28457
2022-03-01 17:27:32 | INFO | train_inner | epoch 033:    350 / 393 loss=4.251, ppl=19.04, wps=29915.4, ups=0.46, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.769, loss_scale=16, train_wall=214, gb_free=13.4, wall=28676
2022-03-01 17:29:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 17:29:08 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.531 | ppl 92.46 | wps 75365.1 | wpb 2034.1 | bsz 4 | num_updates 12943 | best_loss 6.139
2022-03-01 17:29:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12943 updates
2022-03-01 17:29:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 17:29:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 17:29:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 33 @ 12943 updates, score 6.531) (writing took 4.256287268130109 seconds)
2022-03-01 17:29:12 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-01 17:29:12 | INFO | train | epoch 033 | loss 4.201 | ppl 18.39 | wps 29600.6 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 12943 | lr 0.00027796 | gnorm 0.763 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 28777
2022-03-01 17:29:12 | INFO | fairseq.trainer | begin training epoch 34
2022-03-01 17:29:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 17:31:17 | INFO | train_inner | epoch 034:     57 / 393 loss=4.171, ppl=18.01, wps=28972.7, ups=0.44, wpb=65248.6, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.765, loss_scale=16, train_wall=213, gb_free=13.4, wall=28901
2022-03-01 17:34:56 | INFO | train_inner | epoch 034:    157 / 393 loss=4.126, ppl=17.46, wps=29925.9, ups=0.46, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.794, loss_scale=16, train_wall=214, gb_free=13.4, wall=29120
2022-03-01 17:38:35 | INFO | train_inner | epoch 034:    257 / 393 loss=4.182, ppl=18.15, wps=29923.9, ups=0.46, wpb=65530.9, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.793, loss_scale=16, train_wall=214, gb_free=13.4, wall=29339
2022-03-01 17:41:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 17:42:16 | INFO | train_inner | epoch 034:    358 / 393 loss=4.221, ppl=18.65, wps=29625.7, ups=0.45, wpb=65535.4, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.788, loss_scale=16, train_wall=216, gb_free=13.4, wall=29561
2022-03-01 17:43:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 17:43:35 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.57 | ppl 95.02 | wps 74872 | wpb 2034.1 | bsz 4 | num_updates 13335 | best_loss 6.139
2022-03-01 17:43:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13335 updates
2022-03-01 17:43:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 17:43:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 17:43:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 34 @ 13335 updates, score 6.57) (writing took 4.163132925052196 seconds)
2022-03-01 17:43:39 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-01 17:43:39 | INFO | train | epoch 034 | loss 4.167 | ppl 17.96 | wps 29601.8 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 13335 | lr 0.000273844 | gnorm 0.787 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 29644
2022-03-01 17:43:39 | INFO | fairseq.trainer | begin training epoch 35
2022-03-01 17:43:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 17:46:02 | INFO | train_inner | epoch 035:     65 / 393 loss=4.114, ppl=17.32, wps=28960, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.779, loss_scale=16, train_wall=213, gb_free=13.4, wall=29786
2022-03-01 17:49:40 | INFO | train_inner | epoch 035:    165 / 393 loss=4.096, ppl=17.1, wps=29934.9, ups=0.46, wpb=65530.9, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.753, loss_scale=16, train_wall=214, gb_free=13.4, wall=30005
2022-03-01 17:53:19 | INFO | train_inner | epoch 035:    265 / 393 loss=4.153, ppl=17.79, wps=29922.2, ups=0.46, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.786, loss_scale=16, train_wall=214, gb_free=13.4, wall=30224
2022-03-01 17:56:58 | INFO | train_inner | epoch 035:    365 / 393 loss=4.186, ppl=18.2, wps=29923.3, ups=0.46, wpb=65535.4, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.771, loss_scale=16, train_wall=214, gb_free=13.4, wall=30443
2022-03-01 17:57:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 17:58:02 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.607 | ppl 97.44 | wps 75022.5 | wpb 2034.1 | bsz 4 | num_updates 13728 | best_loss 6.139
2022-03-01 17:58:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13728 updates
2022-03-01 17:58:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 17:58:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 17:58:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 35 @ 13728 updates, score 6.607) (writing took 4.149067860096693 seconds)
2022-03-01 17:58:06 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-01 17:58:06 | INFO | train | epoch 035 | loss 4.135 | ppl 17.56 | wps 29675.3 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 13728 | lr 0.000269896 | gnorm 0.768 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 30510
2022-03-01 17:58:06 | INFO | fairseq.trainer | begin training epoch 36
2022-03-01 17:58:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 18:00:44 | INFO | train_inner | epoch 036:     72 / 393 loss=4.075, ppl=16.85, wps=28976, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.777, loss_scale=16, train_wall=213, gb_free=13.4, wall=30668
2022-03-01 18:00:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 18:04:25 | INFO | train_inner | epoch 036:    173 / 393 loss=4.073, ppl=16.83, wps=29628.2, ups=0.45, wpb=65530.2, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.786, loss_scale=16, train_wall=216, gb_free=13.4, wall=30889
2022-03-01 18:08:04 | INFO | train_inner | epoch 036:    273 / 393 loss=4.115, ppl=17.32, wps=29915.8, ups=0.46, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.8, loss_scale=16, train_wall=214, gb_free=13.4, wall=31108
2022-03-01 18:11:43 | INFO | train_inner | epoch 036:    373 / 393 loss=4.165, ppl=17.94, wps=29921.3, ups=0.46, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.775, loss_scale=16, train_wall=214, gb_free=13.4, wall=31327
2022-03-01 18:12:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 18:12:29 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.645 | ppl 100.06 | wps 75325.8 | wpb 2034.1 | bsz 4 | num_updates 14120 | best_loss 6.139
2022-03-01 18:12:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14120 updates
2022-03-01 18:12:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 18:12:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 18:12:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 36 @ 14120 updates, score 6.645) (writing took 4.164342188974842 seconds)
2022-03-01 18:12:33 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-01 18:12:33 | INFO | train | epoch 036 | loss 4.103 | ppl 17.18 | wps 29602.4 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 14120 | lr 0.000266123 | gnorm 0.787 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 31377
2022-03-01 18:12:33 | INFO | fairseq.trainer | begin training epoch 37
2022-03-01 18:12:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 18:15:28 | INFO | train_inner | epoch 037:     80 / 393 loss=4.03, ppl=16.33, wps=28970.5, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.797, loss_scale=16, train_wall=213, gb_free=13.4, wall=31553
2022-03-01 18:19:07 | INFO | train_inner | epoch 037:    180 / 393 loss=4.044, ppl=16.49, wps=29924.6, ups=0.46, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.795, loss_scale=16, train_wall=214, gb_free=13.4, wall=31772
2022-03-01 18:19:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 18:22:48 | INFO | train_inner | epoch 037:    281 / 393 loss=4.094, ppl=17.08, wps=29618.1, ups=0.45, wpb=65535.4, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.792, loss_scale=16, train_wall=216, gb_free=13.4, wall=31993
2022-03-01 18:26:28 | INFO | train_inner | epoch 037:    381 / 393 loss=4.135, ppl=17.57, wps=29822.5, ups=0.46, wpb=65530.9, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.806, loss_scale=16, train_wall=215, gb_free=13.4, wall=32213
2022-03-01 18:26:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 18:26:56 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.662 | ppl 101.28 | wps 75265.6 | wpb 2034.1 | bsz 4 | num_updates 14512 | best_loss 6.139
2022-03-01 18:26:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14512 updates
2022-03-01 18:26:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 18:27:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 18:27:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 37 @ 14512 updates, score 6.662) (writing took 4.138064715079963 seconds)
2022-03-01 18:27:01 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-01 18:27:01 | INFO | train | epoch 037 | loss 4.073 | ppl 16.83 | wps 29572.4 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 14512 | lr 0.000262504 | gnorm 0.795 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 32245
2022-03-01 18:27:01 | INFO | fairseq.trainer | begin training epoch 38
2022-03-01 18:27:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 18:30:13 | INFO | train_inner | epoch 038:     88 / 393 loss=3.996, ppl=15.96, wps=28957.5, ups=0.44, wpb=65244.2, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.786, loss_scale=16, train_wall=213, gb_free=13.4, wall=32438
2022-03-01 18:33:53 | INFO | train_inner | epoch 038:    188 / 393 loss=4.02, ppl=16.22, wps=29904.3, ups=0.46, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.791, loss_scale=16, train_wall=214, gb_free=13.4, wall=32657
2022-03-01 18:37:32 | INFO | train_inner | epoch 038:    288 / 393 loss=4.069, ppl=16.79, wps=29896.7, ups=0.46, wpb=65535.4, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.807, loss_scale=16, train_wall=214, gb_free=13.4, wall=32876
2022-03-01 18:38:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 18:41:13 | INFO | train_inner | epoch 038:    389 / 393 loss=4.106, ppl=17.22, wps=29626.6, ups=0.45, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.821, loss_scale=16, train_wall=216, gb_free=13.4, wall=33098
2022-03-01 18:41:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 18:41:24 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.704 | ppl 104.26 | wps 75711.8 | wpb 2034.1 | bsz 4 | num_updates 14904 | best_loss 6.139
2022-03-01 18:41:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14904 updates
2022-03-01 18:41:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 18:41:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 18:41:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 38 @ 14904 updates, score 6.704) (writing took 4.203985973959789 seconds)
2022-03-01 18:41:28 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-01 18:41:28 | INFO | train | epoch 038 | loss 4.044 | ppl 16.5 | wps 29583.2 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 14904 | lr 0.000259029 | gnorm 0.803 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 33112
2022-03-01 18:41:28 | INFO | fairseq.trainer | begin training epoch 39
2022-03-01 18:41:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 18:44:58 | INFO | train_inner | epoch 039:     96 / 393 loss=3.949, ppl=15.45, wps=28950.2, ups=0.44, wpb=65244.2, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.788, loss_scale=16, train_wall=213, gb_free=13.4, wall=33323
2022-03-01 18:48:37 | INFO | train_inner | epoch 039:    196 / 393 loss=3.999, ppl=15.99, wps=29915.7, ups=0.46, wpb=65535.4, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.806, loss_scale=16, train_wall=214, gb_free=13.4, wall=33542
2022-03-01 18:52:17 | INFO | train_inner | epoch 039:    296 / 393 loss=4.04, ppl=16.45, wps=29915.5, ups=0.46, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.808, loss_scale=16, train_wall=214, gb_free=13.4, wall=33761
2022-03-01 18:55:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 18:55:51 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.75 | ppl 107.64 | wps 75194.2 | wpb 2034.1 | bsz 4 | num_updates 15297 | best_loss 6.139
2022-03-01 18:55:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15297 updates
2022-03-01 18:55:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 18:55:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 18:55:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 39 @ 15297 updates, score 6.75) (writing took 4.175229617161676 seconds)
2022-03-01 18:55:55 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-01 18:55:55 | INFO | train | epoch 039 | loss 4.016 | ppl 16.17 | wps 29656 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 15297 | lr 0.00025568 | gnorm 0.806 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 33980
2022-03-01 18:55:56 | INFO | fairseq.trainer | begin training epoch 40
2022-03-01 18:55:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 18:56:02 | INFO | train_inner | epoch 040:      3 / 393 loss=4.074, ppl=16.84, wps=28924.2, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.82, loss_scale=16, train_wall=213, gb_free=13.4, wall=33987
2022-03-01 18:58:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 18:59:43 | INFO | train_inner | epoch 040:    104 / 393 loss=3.912, ppl=15.05, wps=29614.4, ups=0.45, wpb=65530.2, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.809, loss_scale=16, train_wall=216, gb_free=13.4, wall=34208
2022-03-01 19:03:22 | INFO | train_inner | epoch 040:    204 / 393 loss=3.974, ppl=15.71, wps=29927.5, ups=0.46, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.806, loss_scale=16, train_wall=214, gb_free=13.4, wall=34427
2022-03-01 19:07:02 | INFO | train_inner | epoch 040:    304 / 393 loss=4.011, ppl=16.12, wps=29900.5, ups=0.46, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.805, loss_scale=16, train_wall=214, gb_free=13.4, wall=34646
2022-03-01 19:10:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 19:10:18 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.741 | ppl 106.95 | wps 75255.8 | wpb 2034.1 | bsz 4 | num_updates 15689 | best_loss 6.139
2022-03-01 19:10:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15689 updates
2022-03-01 19:10:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 19:10:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 19:10:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 40 @ 15689 updates, score 6.741) (writing took 4.19540707510896 seconds)
2022-03-01 19:10:23 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-01 19:10:23 | INFO | train | epoch 040 | loss 3.989 | ppl 15.88 | wps 29590.4 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 15689 | lr 0.000252466 | gnorm 0.812 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 34847
2022-03-01 19:10:23 | INFO | fairseq.trainer | begin training epoch 41
2022-03-01 19:10:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 19:10:47 | INFO | train_inner | epoch 041:     11 / 393 loss=4.049, ppl=16.55, wps=28961.2, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.83, loss_scale=16, train_wall=213, gb_free=13.4, wall=34871
2022-03-01 19:14:26 | INFO | train_inner | epoch 041:    111 / 393 loss=3.89, ppl=14.82, wps=29932.4, ups=0.46, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.786, loss_scale=16, train_wall=214, gb_free=13.4, wall=35090
2022-03-01 19:17:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 19:18:07 | INFO | train_inner | epoch 041:    212 / 393 loss=3.944, ppl=15.39, wps=29623, ups=0.45, wpb=65530.2, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.808, loss_scale=16, train_wall=216, gb_free=13.4, wall=35311
2022-03-01 19:21:46 | INFO | train_inner | epoch 041:    312 / 393 loss=3.999, ppl=15.98, wps=29915.1, ups=0.46, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.835, loss_scale=16, train_wall=214, gb_free=13.4, wall=35531
2022-03-01 19:24:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 19:24:45 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.809 | ppl 112.09 | wps 75608.5 | wpb 2034.1 | bsz 4 | num_updates 16081 | best_loss 6.139
2022-03-01 19:24:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16081 updates
2022-03-01 19:24:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 19:24:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 19:24:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 41 @ 16081 updates, score 6.809) (writing took 4.241592731093988 seconds)
2022-03-01 19:24:50 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-01 19:24:50 | INFO | train | epoch 041 | loss 3.962 | ppl 15.58 | wps 29595.8 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 16081 | lr 0.00024937 | gnorm 0.814 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 35714
2022-03-01 19:24:50 | INFO | fairseq.trainer | begin training epoch 42
2022-03-01 19:24:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 19:25:31 | INFO | train_inner | epoch 042:     19 / 393 loss=3.999, ppl=15.99, wps=28955.6, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.824, loss_scale=16, train_wall=213, gb_free=13.4, wall=35756
2022-03-01 19:29:11 | INFO | train_inner | epoch 042:    119 / 393 loss=3.875, ppl=14.68, wps=29894.7, ups=0.46, wpb=65535.4, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.803, loss_scale=16, train_wall=214, gb_free=13.4, wall=35975
2022-03-01 19:32:50 | INFO | train_inner | epoch 042:    219 / 393 loss=3.93, ppl=15.25, wps=29875.8, ups=0.46, wpb=65530.9, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.825, loss_scale=16, train_wall=214, gb_free=13.4, wall=36194
2022-03-01 19:36:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 19:36:32 | INFO | train_inner | epoch 042:    320 / 393 loss=3.967, ppl=15.64, wps=29573.5, ups=0.45, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.819, loss_scale=16, train_wall=217, gb_free=13.4, wall=36416
2022-03-01 19:39:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 19:39:14 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.808 | ppl 112.06 | wps 74947.2 | wpb 2034.1 | bsz 4 | num_updates 16473 | best_loss 6.139
2022-03-01 19:39:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16473 updates
2022-03-01 19:39:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 19:39:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 19:39:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 42 @ 16473 updates, score 6.808) (writing took 4.175216176081449 seconds)
2022-03-01 19:39:18 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-01 19:39:18 | INFO | train | epoch 042 | loss 3.937 | ppl 15.31 | wps 29563.7 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 16473 | lr 0.000246385 | gnorm 0.824 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 36582
2022-03-01 19:39:18 | INFO | fairseq.trainer | begin training epoch 43
2022-03-01 19:39:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 19:40:17 | INFO | train_inner | epoch 043:     27 / 393 loss=3.961, ppl=15.57, wps=28955.8, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.852, loss_scale=16, train_wall=213, gb_free=13.4, wall=36641
2022-03-01 19:43:56 | INFO | train_inner | epoch 043:    127 / 393 loss=3.848, ppl=14.4, wps=29906.4, ups=0.46, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.806, loss_scale=16, train_wall=214, gb_free=13.4, wall=36861
2022-03-01 19:47:35 | INFO | train_inner | epoch 043:    227 / 393 loss=3.908, ppl=15.01, wps=29906.9, ups=0.46, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.818, loss_scale=16, train_wall=214, gb_free=13.4, wall=37080
2022-03-01 19:51:14 | INFO | train_inner | epoch 043:    327 / 393 loss=3.956, ppl=15.52, wps=29904, ups=0.46, wpb=65530.2, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.826, loss_scale=16, train_wall=214, gb_free=13.4, wall=37299
2022-03-01 19:53:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 19:53:41 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.879 | ppl 117.74 | wps 74226.6 | wpb 2034.1 | bsz 4 | num_updates 16866 | best_loss 6.139
2022-03-01 19:53:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16866 updates
2022-03-01 19:53:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 19:53:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 19:53:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 43 @ 16866 updates, score 6.879) (writing took 4.164035398047417 seconds)
2022-03-01 19:53:45 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-01 19:53:45 | INFO | train | epoch 043 | loss 3.911 | ppl 15.04 | wps 29648.5 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 16866 | lr 0.000243497 | gnorm 0.822 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 37450
2022-03-01 19:53:45 | INFO | fairseq.trainer | begin training epoch 44
2022-03-01 19:53:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 19:55:00 | INFO | train_inner | epoch 044:     34 / 393 loss=3.919, ppl=15.12, wps=28900.5, ups=0.44, wpb=65244.2, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.842, loss_scale=16, train_wall=214, gb_free=13.4, wall=37525
2022-03-01 19:55:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 19:58:42 | INFO | train_inner | epoch 044:    135 / 393 loss=3.832, ppl=14.24, wps=29563.5, ups=0.45, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.822, loss_scale=16, train_wall=217, gb_free=13.4, wall=37746
2022-03-01 20:02:21 | INFO | train_inner | epoch 044:    235 / 393 loss=3.885, ppl=14.78, wps=29878, ups=0.46, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.833, loss_scale=16, train_wall=214, gb_free=13.4, wall=37966
2022-03-01 20:06:01 | INFO | train_inner | epoch 044:    335 / 393 loss=3.934, ppl=15.28, wps=29854.2, ups=0.46, wpb=65535.4, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.835, loss_scale=16, train_wall=215, gb_free=13.4, wall=38185
2022-03-01 20:08:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 20:08:10 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.898 | ppl 119.23 | wps 75013.7 | wpb 2034.1 | bsz 4 | num_updates 17258 | best_loss 6.139
2022-03-01 20:08:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17258 updates
2022-03-01 20:08:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 20:08:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 20:08:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 44 @ 17258 updates, score 6.898) (writing took 4.116556607885286 seconds)
2022-03-01 20:08:14 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-01 20:08:14 | INFO | train | epoch 044 | loss 3.888 | ppl 14.81 | wps 29545.5 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 17258 | lr 0.000240716 | gnorm 0.832 | loss_scale 16 | train_wall 842 | gb_free 13.4 | wall 38318
2022-03-01 20:08:14 | INFO | fairseq.trainer | begin training epoch 45
2022-03-01 20:08:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 20:09:46 | INFO | train_inner | epoch 045:     42 / 393 loss=3.882, ppl=14.75, wps=28963.1, ups=0.44, wpb=65248.6, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.838, loss_scale=16, train_wall=213, gb_free=13.4, wall=38410
2022-03-01 20:13:25 | INFO | train_inner | epoch 045:    142 / 393 loss=3.816, ppl=14.09, wps=29893.2, ups=0.46, wpb=65530.9, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.84, loss_scale=16, train_wall=214, gb_free=13.4, wall=38630
2022-03-01 20:14:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 20:17:06 | INFO | train_inner | epoch 045:    243 / 393 loss=3.866, ppl=14.58, wps=29639.1, ups=0.45, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.83, loss_scale=16, train_wall=216, gb_free=13.4, wall=38851
2022-03-01 20:20:45 | INFO | train_inner | epoch 045:    343 / 393 loss=3.917, ppl=15.11, wps=29920.7, ups=0.46, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.834, loss_scale=16, train_wall=214, gb_free=13.4, wall=39070
2022-03-01 20:22:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 20:22:37 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.932 | ppl 122.14 | wps 74963.2 | wpb 2034.1 | bsz 4 | num_updates 17650 | best_loss 6.139
2022-03-01 20:22:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17650 updates
2022-03-01 20:22:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 20:22:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 20:22:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 45 @ 17650 updates, score 6.932) (writing took 4.128809143090621 seconds)
2022-03-01 20:22:41 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-01 20:22:41 | INFO | train | epoch 045 | loss 3.865 | ppl 14.57 | wps 29596.9 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 17650 | lr 0.000238028 | gnorm 0.836 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 39185
2022-03-01 20:22:41 | INFO | fairseq.trainer | begin training epoch 46
2022-03-01 20:22:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 20:24:31 | INFO | train_inner | epoch 046:     50 / 393 loss=3.85, ppl=14.42, wps=28975.4, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.834, loss_scale=16, train_wall=213, gb_free=13.4, wall=39295
2022-03-01 20:28:10 | INFO | train_inner | epoch 046:    150 / 393 loss=3.795, ppl=13.88, wps=29926, ups=0.46, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.848, loss_scale=16, train_wall=214, gb_free=13.4, wall=39514
2022-03-01 20:31:48 | INFO | train_inner | epoch 046:    250 / 393 loss=3.853, ppl=14.45, wps=29928.1, ups=0.46, wpb=65530.9, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.86, loss_scale=16, train_wall=214, gb_free=13.4, wall=39733
2022-03-01 20:33:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 20:35:30 | INFO | train_inner | epoch 046:    351 / 393 loss=3.889, ppl=14.81, wps=29628.8, ups=0.45, wpb=65535.4, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.846, loss_scale=16, train_wall=216, gb_free=13.4, wall=39954
2022-03-01 20:37:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 20:37:04 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.954 | ppl 123.99 | wps 74764.5 | wpb 2034.1 | bsz 4 | num_updates 18042 | best_loss 6.139
2022-03-01 20:37:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18042 updates
2022-03-01 20:37:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 20:37:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 20:37:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 46 @ 18042 updates, score 6.954) (writing took 4.157447070814669 seconds)
2022-03-01 20:37:08 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-01 20:37:08 | INFO | train | epoch 046 | loss 3.843 | ppl 14.35 | wps 29601.9 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 18042 | lr 0.000235428 | gnorm 0.852 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 40052
2022-03-01 20:37:08 | INFO | fairseq.trainer | begin training epoch 47
2022-03-01 20:37:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 20:39:15 | INFO | train_inner | epoch 047:     58 / 393 loss=3.81, ppl=14.03, wps=28945.8, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.856, loss_scale=16, train_wall=213, gb_free=13.4, wall=40180
2022-03-01 20:42:54 | INFO | train_inner | epoch 047:    158 / 393 loss=3.778, ppl=13.72, wps=29929.4, ups=0.46, wpb=65535.4, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.832, loss_scale=16, train_wall=214, gb_free=13.4, wall=40399
2022-03-01 20:46:33 | INFO | train_inner | epoch 047:    258 / 393 loss=3.837, ppl=14.29, wps=29911, ups=0.46, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.841, loss_scale=16, train_wall=214, gb_free=13.4, wall=40618
2022-03-01 20:50:12 | INFO | train_inner | epoch 047:    358 / 393 loss=3.87, ppl=14.62, wps=29921.8, ups=0.46, wpb=65530.9, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.854, loss_scale=16, train_wall=214, gb_free=13.4, wall=40837
2022-03-01 20:51:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 20:51:31 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.006 | ppl 128.51 | wps 74935.4 | wpb 2034.1 | bsz 4 | num_updates 18435 | best_loss 6.139
2022-03-01 20:51:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18435 updates
2022-03-01 20:51:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 20:51:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 20:51:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 47 @ 18435 updates, score 7.006) (writing took 4.1511169478762895 seconds)
2022-03-01 20:51:35 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-01 20:51:35 | INFO | train | epoch 047 | loss 3.821 | ppl 14.13 | wps 29669 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 18435 | lr 0.000232905 | gnorm 0.844 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 40919
2022-03-01 20:51:35 | INFO | fairseq.trainer | begin training epoch 48
2022-03-01 20:51:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 20:52:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 20:54:00 | INFO | train_inner | epoch 048:     66 / 393 loss=3.774, ppl=13.68, wps=28701.8, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.863, loss_scale=16, train_wall=215, gb_free=13.4, wall=41064
2022-03-01 20:57:39 | INFO | train_inner | epoch 048:    166 / 393 loss=3.77, ppl=13.64, wps=29895.5, ups=0.46, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.87, loss_scale=16, train_wall=214, gb_free=13.4, wall=41283
2022-03-01 21:01:18 | INFO | train_inner | epoch 048:    266 / 393 loss=3.808, ppl=14, wps=29919, ups=0.46, wpb=65530.2, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.862, loss_scale=16, train_wall=214, gb_free=13.4, wall=41502
2022-03-01 21:04:57 | INFO | train_inner | epoch 048:    366 / 393 loss=3.86, ppl=14.52, wps=29937.3, ups=0.46, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.863, loss_scale=16, train_wall=214, gb_free=13.4, wall=41721
2022-03-01 21:05:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 21:05:58 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.013 | ppl 129.13 | wps 75195.9 | wpb 2034.1 | bsz 4 | num_updates 18827 | best_loss 6.139
2022-03-01 21:05:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18827 updates
2022-03-01 21:05:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 21:06:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 21:06:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 48 @ 18827 updates, score 7.013) (writing took 4.152588177006692 seconds)
2022-03-01 21:06:02 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-01 21:06:02 | INFO | train | epoch 048 | loss 3.799 | ppl 13.92 | wps 29597.5 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 18827 | lr 0.000230467 | gnorm 0.863 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 41786
2022-03-01 21:06:02 | INFO | fairseq.trainer | begin training epoch 49
2022-03-01 21:06:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 21:08:42 | INFO | train_inner | epoch 049:     73 / 393 loss=3.741, ppl=13.37, wps=28963.4, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.85, loss_scale=16, train_wall=213, gb_free=13.4, wall=41946
2022-03-01 21:11:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 21:12:23 | INFO | train_inner | epoch 049:    174 / 393 loss=3.749, ppl=13.45, wps=29624.4, ups=0.45, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.884, loss_scale=16, train_wall=216, gb_free=13.4, wall=42168
2022-03-01 21:13:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 21:16:04 | INFO | train_inner | epoch 049:    275 / 393 loss=3.8, ppl=13.93, wps=29638.9, ups=0.45, wpb=65535.4, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.851, loss_scale=8, train_wall=216, gb_free=13.4, wall=42389
2022-03-01 21:19:43 | INFO | train_inner | epoch 049:    375 / 393 loss=3.829, ppl=14.21, wps=29943.1, ups=0.46, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.866, loss_scale=8, train_wall=214, gb_free=13.4, wall=42608
2022-03-01 21:20:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 21:20:25 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.076 | ppl 134.92 | wps 75111.6 | wpb 2034.1 | bsz 4 | num_updates 19218 | best_loss 6.139
2022-03-01 21:20:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19218 updates
2022-03-01 21:20:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 21:20:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 21:20:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 49 @ 19218 updates, score 7.076) (writing took 4.422637860989198 seconds)
2022-03-01 21:20:29 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-01 21:20:29 | INFO | train | epoch 049 | loss 3.778 | ppl 13.72 | wps 29520.3 | ups 0.45 | wpb 65461.2 | bsz 127.9 | num_updates 19218 | lr 0.000228111 | gnorm 0.865 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 42653
2022-03-01 21:20:29 | INFO | fairseq.trainer | begin training epoch 50
2022-03-01 21:20:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 21:23:28 | INFO | train_inner | epoch 050:     82 / 393 loss=3.717, ppl=13.15, wps=28958.5, ups=0.44, wpb=65243.5, bsz=127.4, num_updates=19300, lr=0.000227626, gnorm=0.857, loss_scale=8, train_wall=213, gb_free=13.4, wall=42833
2022-03-01 21:27:07 | INFO | train_inner | epoch 050:    182 / 393 loss=3.734, ppl=13.31, wps=29943.9, ups=0.46, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.85, loss_scale=8, train_wall=214, gb_free=13.4, wall=43052
2022-03-01 21:30:46 | INFO | train_inner | epoch 050:    282 / 393 loss=3.776, ppl=13.7, wps=29938.9, ups=0.46, wpb=65530.9, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.856, loss_scale=8, train_wall=214, gb_free=13.4, wall=43271
2022-03-01 21:34:25 | INFO | train_inner | epoch 050:    382 / 393 loss=3.822, ppl=14.14, wps=29923.8, ups=0.46, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.878, loss_scale=16, train_wall=214, gb_free=13.4, wall=43490
2022-03-01 21:34:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 21:34:51 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.1 | ppl 137.21 | wps 75336 | wpb 2034.1 | bsz 4 | num_updates 19611 | best_loss 6.139
2022-03-01 21:34:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19611 updates
2022-03-01 21:34:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 21:34:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 21:34:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 50 @ 19611 updates, score 7.1) (writing took 4.165609962772578 seconds)
2022-03-01 21:34:55 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-01 21:34:55 | INFO | train | epoch 050 | loss 3.759 | ppl 13.54 | wps 29691.7 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 19611 | lr 0.000225814 | gnorm 0.859 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 43520
2022-03-01 21:34:55 | INFO | fairseq.trainer | begin training epoch 51
2022-03-01 21:34:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 21:37:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 21:38:12 | INFO | train_inner | epoch 051:     90 / 393 loss=3.687, ppl=12.88, wps=28715.6, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.878, loss_scale=8, train_wall=215, gb_free=13.4, wall=43717
2022-03-01 21:41:51 | INFO | train_inner | epoch 051:    190 / 393 loss=3.713, ppl=13.11, wps=29931.1, ups=0.46, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.864, loss_scale=8, train_wall=214, gb_free=13.4, wall=43936
2022-03-01 21:45:30 | INFO | train_inner | epoch 051:    290 / 393 loss=3.761, ppl=13.56, wps=29941.4, ups=0.46, wpb=65530.9, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.873, loss_scale=8, train_wall=214, gb_free=13.4, wall=44155
2022-03-01 21:49:09 | INFO | train_inner | epoch 051:    390 / 393 loss=3.806, ppl=13.99, wps=29934.1, ups=0.46, wpb=65535.4, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.891, loss_scale=8, train_wall=214, gb_free=13.4, wall=44374
2022-03-01 21:49:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 21:49:18 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.123 | ppl 139.38 | wps 75457.2 | wpb 2034.1 | bsz 4 | num_updates 20003 | best_loss 6.139
2022-03-01 21:49:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20003 updates
2022-03-01 21:49:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 21:49:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 21:49:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 51 @ 20003 updates, score 7.123) (writing took 4.20638814708218 seconds)
2022-03-01 21:49:22 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-01 21:49:22 | INFO | train | epoch 051 | loss 3.739 | ppl 13.35 | wps 29614.5 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 20003 | lr 0.00022359 | gnorm 0.877 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 44386
2022-03-01 21:49:22 | INFO | fairseq.trainer | begin training epoch 52
2022-03-01 21:49:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 21:52:54 | INFO | train_inner | epoch 052:     97 / 393 loss=3.651, ppl=12.56, wps=28968.6, ups=0.44, wpb=65243.5, bsz=127.4, num_updates=20100, lr=0.00022305, gnorm=0.867, loss_scale=8, train_wall=213, gb_free=13.4, wall=44599
2022-03-01 21:56:33 | INFO | train_inner | epoch 052:    197 / 393 loss=3.699, ppl=12.99, wps=29933.9, ups=0.46, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.871, loss_scale=16, train_wall=214, gb_free=13.4, wall=44818
2022-03-01 22:00:12 | INFO | train_inner | epoch 052:    297 / 393 loss=3.751, ppl=13.46, wps=29915, ups=0.46, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.901, loss_scale=16, train_wall=214, gb_free=13.4, wall=45037
2022-03-01 22:03:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 22:03:45 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.171 | ppl 144.12 | wps 75743.2 | wpb 2034.1 | bsz 4 | num_updates 20396 | best_loss 6.139
2022-03-01 22:03:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20396 updates
2022-03-01 22:03:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 22:03:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 22:03:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 52 @ 20396 updates, score 7.171) (writing took 4.191058865049854 seconds)
2022-03-01 22:03:49 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-01 22:03:49 | INFO | train | epoch 052 | loss 3.72 | ppl 13.17 | wps 29676.9 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 20396 | lr 0.000221425 | gnorm 0.877 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 45253
2022-03-01 22:03:49 | INFO | fairseq.trainer | begin training epoch 53
2022-03-01 22:03:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 22:03:58 | INFO | train_inner | epoch 053:      4 / 393 loss=3.778, ppl=13.72, wps=28967.9, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.868, loss_scale=16, train_wall=213, gb_free=13.4, wall=45262
2022-03-01 22:07:37 | INFO | train_inner | epoch 053:    104 / 393 loss=3.626, ppl=12.34, wps=29935.7, ups=0.46, wpb=65535.4, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.86, loss_scale=16, train_wall=214, gb_free=13.4, wall=45481
2022-03-01 22:11:16 | INFO | train_inner | epoch 053:    204 / 393 loss=3.68, ppl=12.82, wps=29904.2, ups=0.46, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.87, loss_scale=16, train_wall=214, gb_free=13.4, wall=45700
2022-03-01 22:14:55 | INFO | train_inner | epoch 053:    304 / 393 loss=3.736, ppl=13.32, wps=29904.6, ups=0.46, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.873, loss_scale=16, train_wall=214, gb_free=13.4, wall=45919
2022-03-01 22:15:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 22:18:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 22:18:12 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.196 | ppl 146.65 | wps 75196 | wpb 2034.1 | bsz 4 | num_updates 20788 | best_loss 6.139
2022-03-01 22:18:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20788 updates
2022-03-01 22:18:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 22:18:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 22:18:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 53 @ 20788 updates, score 7.196) (writing took 4.157526331953704 seconds)
2022-03-01 22:18:16 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-01 22:18:16 | INFO | train | epoch 053 | loss 3.702 | ppl 13.01 | wps 29593.5 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 20788 | lr 0.000219328 | gnorm 0.871 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 46120
2022-03-01 22:18:16 | INFO | fairseq.trainer | begin training epoch 54
2022-03-01 22:18:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 22:18:42 | INFO | train_inner | epoch 054:     12 / 393 loss=3.754, ppl=13.49, wps=28694.4, ups=0.44, wpb=65244.2, bsz=127.4, num_updates=20800, lr=0.000219265, gnorm=0.879, loss_scale=16, train_wall=215, gb_free=13.4, wall=46147
2022-03-01 22:22:21 | INFO | train_inner | epoch 054:    112 / 393 loss=3.619, ppl=12.29, wps=29933.7, ups=0.46, wpb=65535.4, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.875, loss_scale=16, train_wall=214, gb_free=13.4, wall=46366
2022-03-01 22:26:00 | INFO | train_inner | epoch 054:    212 / 393 loss=3.671, ppl=12.74, wps=29916.1, ups=0.46, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.881, loss_scale=16, train_wall=214, gb_free=13.4, wall=46585
2022-03-01 22:29:39 | INFO | train_inner | epoch 054:    312 / 393 loss=3.717, ppl=13.15, wps=29910.6, ups=0.46, wpb=65530.9, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.897, loss_scale=16, train_wall=214, gb_free=13.4, wall=46804
2022-03-01 22:32:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 22:32:39 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.229 | ppl 150.03 | wps 75081.2 | wpb 2034.1 | bsz 4 | num_updates 21181 | best_loss 6.139
2022-03-01 22:32:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21181 updates
2022-03-01 22:32:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 22:32:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 22:32:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 54 @ 21181 updates, score 7.229) (writing took 4.139284965116531 seconds)
2022-03-01 22:32:43 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-01 22:32:43 | INFO | train | epoch 054 | loss 3.684 | ppl 12.85 | wps 29673.8 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 21181 | lr 0.000217284 | gnorm 0.886 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 46987
2022-03-01 22:32:43 | INFO | fairseq.trainer | begin training epoch 55
2022-03-01 22:32:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 22:33:25 | INFO | train_inner | epoch 055:     19 / 393 loss=3.713, ppl=13.12, wps=28971.5, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.892, loss_scale=16, train_wall=213, gb_free=13.4, wall=47029
2022-03-01 22:34:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-01 22:37:06 | INFO | train_inner | epoch 055:    120 / 393 loss=3.607, ppl=12.19, wps=29631.7, ups=0.45, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.873, loss_scale=16, train_wall=216, gb_free=13.4, wall=47250
2022-03-01 22:40:45 | INFO | train_inner | epoch 055:    220 / 393 loss=3.666, ppl=12.69, wps=29931, ups=0.46, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.89, loss_scale=16, train_wall=214, gb_free=13.4, wall=47469
2022-03-01 22:44:24 | INFO | train_inner | epoch 055:    320 / 393 loss=3.7, ppl=12.99, wps=29914.6, ups=0.46, wpb=65530.9, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.892, loss_scale=16, train_wall=214, gb_free=13.4, wall=47688
2022-03-01 22:47:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 22:47:06 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.256 | ppl 152.88 | wps 75242.5 | wpb 2034.1 | bsz 4 | num_updates 21573 | best_loss 6.139
2022-03-01 22:47:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 21573 updates
2022-03-01 22:47:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 22:47:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 22:47:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 55 @ 21573 updates, score 7.256) (writing took 4.167463001096621 seconds)
2022-03-01 22:47:10 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-01 22:47:10 | INFO | train | epoch 055 | loss 3.666 | ppl 12.7 | wps 29599.9 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 21573 | lr 0.0002153 | gnorm 0.887 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 47854
2022-03-01 22:47:10 | INFO | fairseq.trainer | begin training epoch 56
2022-03-01 22:47:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 22:48:09 | INFO | train_inner | epoch 056:     27 / 393 loss=3.686, ppl=12.87, wps=28974.3, ups=0.44, wpb=65248.6, bsz=127.4, num_updates=21600, lr=0.000215166, gnorm=0.899, loss_scale=16, train_wall=213, gb_free=13.4, wall=47913
2022-03-01 22:49:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 22:51:50 | INFO | train_inner | epoch 056:    128 / 393 loss=3.595, ppl=12.09, wps=29637.2, ups=0.45, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.882, loss_scale=8, train_wall=216, gb_free=13.4, wall=48135
2022-03-01 22:55:29 | INFO | train_inner | epoch 056:    228 / 393 loss=3.639, ppl=12.46, wps=29938.8, ups=0.46, wpb=65530.2, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.876, loss_scale=8, train_wall=214, gb_free=13.4, wall=48353
2022-03-01 22:59:08 | INFO | train_inner | epoch 056:    328 / 393 loss=3.691, ppl=12.92, wps=29942.5, ups=0.46, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.902, loss_scale=8, train_wall=214, gb_free=13.4, wall=48572
2022-03-01 23:01:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 23:01:32 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.249 | ppl 152.08 | wps 75559.9 | wpb 2034.1 | bsz 4 | num_updates 21965 | best_loss 6.139
2022-03-01 23:01:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 21965 updates
2022-03-01 23:01:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 23:01:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 23:01:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 56 @ 21965 updates, score 7.249) (writing took 4.160327235003933 seconds)
2022-03-01 23:01:36 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-01 23:01:36 | INFO | train | epoch 056 | loss 3.649 | ppl 12.55 | wps 29614.1 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 21965 | lr 0.000213371 | gnorm 0.891 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 48721
2022-03-01 23:01:36 | INFO | fairseq.trainer | begin training epoch 57
2022-03-01 23:01:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 23:02:53 | INFO | train_inner | epoch 057:     35 / 393 loss=3.661, ppl=12.65, wps=28974.5, ups=0.44, wpb=65248.6, bsz=127.4, num_updates=22000, lr=0.000213201, gnorm=0.913, loss_scale=8, train_wall=213, gb_free=13.4, wall=48797
2022-03-01 23:06:32 | INFO | train_inner | epoch 057:    135 / 393 loss=3.582, ppl=11.97, wps=29893.2, ups=0.46, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.892, loss_scale=8, train_wall=214, gb_free=13.4, wall=49017
2022-03-01 23:09:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 23:10:13 | INFO | train_inner | epoch 057:    236 / 393 loss=3.624, ppl=12.33, wps=29633.9, ups=0.45, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.895, loss_scale=8, train_wall=216, gb_free=13.4, wall=49238
2022-03-01 23:13:52 | INFO | train_inner | epoch 057:    336 / 393 loss=3.679, ppl=12.81, wps=29947.1, ups=0.46, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.917, loss_scale=8, train_wall=214, gb_free=13.4, wall=49457
2022-03-01 23:15:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 23:15:59 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.302 | ppl 157.78 | wps 74836.1 | wpb 2034.1 | bsz 4 | num_updates 22357 | best_loss 6.139
2022-03-01 23:15:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 22357 updates
2022-03-01 23:15:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 23:16:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 23:16:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 57 @ 22357 updates, score 7.302) (writing took 4.309880984015763 seconds)
2022-03-01 23:16:03 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-01 23:16:03 | INFO | train | epoch 057 | loss 3.633 | ppl 12.4 | wps 29597 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 22357 | lr 0.000211492 | gnorm 0.906 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 49588
2022-03-01 23:16:03 | INFO | fairseq.trainer | begin training epoch 58
2022-03-01 23:16:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 23:17:38 | INFO | train_inner | epoch 058:     43 / 393 loss=3.625, ppl=12.34, wps=28962.5, ups=0.44, wpb=65244.2, bsz=127.4, num_updates=22400, lr=0.000211289, gnorm=0.902, loss_scale=8, train_wall=213, gb_free=13.4, wall=49682
2022-03-01 23:21:17 | INFO | train_inner | epoch 058:    143 / 393 loss=3.566, ppl=11.84, wps=29928.6, ups=0.46, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.882, loss_scale=8, train_wall=214, gb_free=13.4, wall=49901
2022-03-01 23:24:55 | INFO | train_inner | epoch 058:    243 / 393 loss=3.62, ppl=12.29, wps=29931.7, ups=0.46, wpb=65530.2, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.904, loss_scale=8, train_wall=214, gb_free=13.4, wall=50120
2022-03-01 23:28:34 | INFO | train_inner | epoch 058:    343 / 393 loss=3.664, ppl=12.68, wps=29939.5, ups=0.46, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.925, loss_scale=16, train_wall=214, gb_free=13.4, wall=50339
2022-03-01 23:30:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 23:30:26 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.318 | ppl 159.57 | wps 74817 | wpb 2034.1 | bsz 4 | num_updates 22750 | best_loss 6.139
2022-03-01 23:30:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 22750 updates
2022-03-01 23:30:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 23:30:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 23:30:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 58 @ 22750 updates, score 7.318) (writing took 4.133163327118382 seconds)
2022-03-01 23:30:30 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-01 23:30:30 | INFO | train | epoch 058 | loss 3.616 | ppl 12.26 | wps 29685 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 22750 | lr 0.000209657 | gnorm 0.904 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 50454
2022-03-01 23:30:30 | INFO | fairseq.trainer | begin training epoch 59
2022-03-01 23:30:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 23:32:20 | INFO | train_inner | epoch 059:     50 / 393 loss=3.608, ppl=12.2, wps=28965.6, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=22800, lr=0.000209427, gnorm=0.92, loss_scale=16, train_wall=213, gb_free=13.4, wall=50564
2022-03-01 23:35:59 | INFO | train_inner | epoch 059:    150 / 393 loss=3.551, ppl=11.72, wps=29935.2, ups=0.46, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.906, loss_scale=16, train_wall=214, gb_free=13.4, wall=50783
2022-03-01 23:37:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-01 23:39:40 | INFO | train_inner | epoch 059:    251 / 393 loss=3.604, ppl=12.16, wps=29629.2, ups=0.45, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.908, loss_scale=8, train_wall=216, gb_free=13.4, wall=51004
2022-03-01 23:43:19 | INFO | train_inner | epoch 059:    351 / 393 loss=3.656, ppl=12.6, wps=29946.2, ups=0.46, wpb=65530.2, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.909, loss_scale=8, train_wall=214, gb_free=13.4, wall=51223
2022-03-01 23:44:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 23:44:52 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.322 | ppl 160.02 | wps 75382.2 | wpb 2034.1 | bsz 4 | num_updates 23142 | best_loss 6.139
2022-03-01 23:44:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 23142 updates
2022-03-01 23:44:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 23:44:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 23:44:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 59 @ 23142 updates, score 7.322) (writing took 4.10028903814964 seconds)
2022-03-01 23:44:57 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-01 23:44:57 | INFO | train | epoch 059 | loss 3.6 | ppl 12.12 | wps 29611.7 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 23142 | lr 0.000207874 | gnorm 0.908 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 51321
2022-03-01 23:44:57 | INFO | fairseq.trainer | begin training epoch 60
2022-03-01 23:44:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-01 23:47:04 | INFO | train_inner | epoch 060:     58 / 393 loss=3.572, ppl=11.9, wps=29002.2, ups=0.44, wpb=65248.6, bsz=127.4, num_updates=23200, lr=0.000207614, gnorm=0.895, loss_scale=8, train_wall=213, gb_free=13.4, wall=51448
2022-03-01 23:50:43 | INFO | train_inner | epoch 060:    158 / 393 loss=3.548, ppl=11.7, wps=29908.5, ups=0.46, wpb=65530.9, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.896, loss_scale=8, train_wall=214, gb_free=13.4, wall=51667
2022-03-01 23:54:21 | INFO | train_inner | epoch 060:    258 / 393 loss=3.59, ppl=12.04, wps=29947.2, ups=0.46, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.907, loss_scale=8, train_wall=214, gb_free=13.4, wall=51886
2022-03-01 23:58:00 | INFO | train_inner | epoch 060:    358 / 393 loss=3.635, ppl=12.43, wps=29930.8, ups=0.46, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.923, loss_scale=16, train_wall=214, gb_free=13.4, wall=52105
2022-03-01 23:59:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-01 23:59:19 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.351 | ppl 163.21 | wps 74865.1 | wpb 2034.1 | bsz 4 | num_updates 23535 | best_loss 6.139
2022-03-01 23:59:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 23535 updates
2022-03-01 23:59:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 23:59:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-01 23:59:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 60 @ 23535 updates, score 7.351) (writing took 4.125874117948115 seconds)
2022-03-01 23:59:23 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-01 23:59:23 | INFO | train | epoch 060 | loss 3.585 | ppl 12 | wps 29684.8 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 23535 | lr 0.000206131 | gnorm 0.909 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 52188
2022-03-01 23:59:23 | INFO | fairseq.trainer | begin training epoch 61
2022-03-01 23:59:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 00:01:46 | INFO | train_inner | epoch 061:     65 / 393 loss=3.553, ppl=11.74, wps=28961.5, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=23600, lr=0.000205847, gnorm=0.93, loss_scale=16, train_wall=213, gb_free=13.4, wall=52330
2022-03-02 00:02:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 00:05:27 | INFO | train_inner | epoch 061:    166 / 393 loss=3.537, ppl=11.61, wps=29632.8, ups=0.45, wpb=65530.9, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.915, loss_scale=8, train_wall=216, gb_free=13.4, wall=52551
2022-03-02 00:09:06 | INFO | train_inner | epoch 061:    266 / 393 loss=3.575, ppl=11.92, wps=29903.4, ups=0.46, wpb=65535.4, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.9, loss_scale=8, train_wall=214, gb_free=13.4, wall=52770
2022-03-02 00:12:45 | INFO | train_inner | epoch 061:    366 / 393 loss=3.626, ppl=12.34, wps=29932.2, ups=0.46, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.92, loss_scale=8, train_wall=214, gb_free=13.4, wall=52989
2022-03-02 00:13:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 00:13:46 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.373 | ppl 165.78 | wps 75181.7 | wpb 2034.1 | bsz 4 | num_updates 23927 | best_loss 6.139
2022-03-02 00:13:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 23927 updates
2022-03-02 00:13:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 00:13:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 00:13:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 61 @ 23927 updates, score 7.373) (writing took 4.182811008999124 seconds)
2022-03-02 00:13:50 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-02 00:13:50 | INFO | train | epoch 061 | loss 3.57 | ppl 11.87 | wps 29596.1 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 23927 | lr 0.000204435 | gnorm 0.916 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 53055
2022-03-02 00:13:50 | INFO | fairseq.trainer | begin training epoch 62
2022-03-02 00:13:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 00:16:30 | INFO | train_inner | epoch 062:     73 / 393 loss=3.526, ppl=11.52, wps=28987.8, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=24000, lr=0.000204124, gnorm=0.909, loss_scale=8, train_wall=213, gb_free=13.4, wall=53215
2022-03-02 00:20:09 | INFO | train_inner | epoch 062:    173 / 393 loss=3.524, ppl=11.5, wps=29944.5, ups=0.46, wpb=65530.9, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.919, loss_scale=8, train_wall=214, gb_free=13.4, wall=53433
2022-03-02 00:23:48 | INFO | train_inner | epoch 062:    273 / 393 loss=3.57, ppl=11.88, wps=29916.8, ups=0.46, wpb=65535.4, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.914, loss_scale=16, train_wall=214, gb_free=13.4, wall=53652
2022-03-02 00:27:27 | INFO | train_inner | epoch 062:    373 / 393 loss=3.611, ppl=12.22, wps=29938.4, ups=0.46, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.926, loss_scale=16, train_wall=214, gb_free=13.4, wall=53871
2022-03-02 00:28:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 00:28:13 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 7.42 | ppl 171.28 | wps 75023.4 | wpb 2034.1 | bsz 4 | num_updates 24320 | best_loss 6.139
2022-03-02 00:28:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 24320 updates
2022-03-02 00:28:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 00:28:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 00:28:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 62 @ 24320 updates, score 7.42) (writing took 4.107993100071326 seconds)
2022-03-02 00:28:17 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-02 00:28:17 | INFO | train | epoch 062 | loss 3.555 | ppl 11.75 | wps 29688.7 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 24320 | lr 0.000202777 | gnorm 0.916 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 53921
2022-03-02 00:28:17 | INFO | fairseq.trainer | begin training epoch 63
2022-03-02 00:28:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 00:31:12 | INFO | train_inner | epoch 063:     80 / 393 loss=3.509, ppl=11.38, wps=28976.8, ups=0.44, wpb=65243.5, bsz=127.4, num_updates=24400, lr=0.000202444, gnorm=0.916, loss_scale=16, train_wall=213, gb_free=13.4, wall=54096
2022-03-02 00:34:51 | INFO | train_inner | epoch 063:    180 / 393 loss=3.509, ppl=11.38, wps=29921.5, ups=0.46, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.914, loss_scale=16, train_wall=214, gb_free=13.4, wall=54316
2022-03-02 00:38:30 | INFO | train_inner | epoch 063:    280 / 393 loss=3.557, ppl=11.77, wps=29930.2, ups=0.46, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.927, loss_scale=16, train_wall=214, gb_free=13.4, wall=54534
2022-03-02 00:40:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 00:42:11 | INFO | train_inner | epoch 063:    381 / 393 loss=3.598, ppl=12.11, wps=29626.7, ups=0.45, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.927, loss_scale=16, train_wall=216, gb_free=13.4, wall=54756
2022-03-02 00:42:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 00:42:39 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 7.434 | ppl 172.94 | wps 75205.9 | wpb 2034.1 | bsz 4 | num_updates 24712 | best_loss 6.139
2022-03-02 00:42:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 24712 updates
2022-03-02 00:42:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 00:42:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 00:42:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 63 @ 24712 updates, score 7.434) (writing took 4.172609593952075 seconds)
2022-03-02 00:42:44 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-02 00:42:44 | INFO | train | epoch 063 | loss 3.541 | ppl 11.64 | wps 29602.1 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 24712 | lr 0.000201162 | gnorm 0.922 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 54788
2022-03-02 00:42:44 | INFO | fairseq.trainer | begin training epoch 64
2022-03-02 00:42:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 00:45:57 | INFO | train_inner | epoch 064:     88 / 393 loss=3.474, ppl=11.11, wps=28958.2, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=24800, lr=0.000200805, gnorm=0.921, loss_scale=16, train_wall=213, gb_free=13.4, wall=54981
2022-03-02 00:49:36 | INFO | train_inner | epoch 064:    188 / 393 loss=3.506, ppl=11.36, wps=29922.8, ups=0.46, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.922, loss_scale=16, train_wall=214, gb_free=13.4, wall=55200
2022-03-02 00:53:14 | INFO | train_inner | epoch 064:    288 / 393 loss=3.544, ppl=11.67, wps=29935.5, ups=0.46, wpb=65530.2, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.916, loss_scale=16, train_wall=214, gb_free=13.4, wall=55419
2022-03-02 00:56:53 | INFO | train_inner | epoch 064:    388 / 393 loss=3.593, ppl=12.07, wps=29931.9, ups=0.46, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.936, loss_scale=16, train_wall=214, gb_free=13.4, wall=55638
2022-03-02 00:57:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 00:57:06 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 7.461 | ppl 176.14 | wps 74911.7 | wpb 2034.1 | bsz 4 | num_updates 25105 | best_loss 6.139
2022-03-02 00:57:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 25105 updates
2022-03-02 00:57:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 00:57:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 00:57:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 64 @ 25105 updates, score 7.461) (writing took 4.145219501806423 seconds)
2022-03-02 00:57:11 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-02 00:57:11 | INFO | train | epoch 064 | loss 3.527 | ppl 11.53 | wps 29677.7 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 25105 | lr 0.000199581 | gnorm 0.924 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 55655
2022-03-02 00:57:11 | INFO | fairseq.trainer | begin training epoch 65
2022-03-02 00:57:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 00:59:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 01:00:41 | INFO | train_inner | epoch 065:     96 / 393 loss=3.452, ppl=10.94, wps=28714.9, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=25200, lr=0.000199205, gnorm=0.925, loss_scale=16, train_wall=215, gb_free=13.4, wall=55865
2022-03-02 01:04:20 | INFO | train_inner | epoch 065:    196 / 393 loss=3.489, ppl=11.23, wps=29897.8, ups=0.46, wpb=65530.9, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.923, loss_scale=16, train_wall=214, gb_free=13.4, wall=56084
2022-03-02 01:07:59 | INFO | train_inner | epoch 065:    296 / 393 loss=3.543, ppl=11.66, wps=29944.6, ups=0.46, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.917, loss_scale=16, train_wall=214, gb_free=13.4, wall=56303
2022-03-02 01:11:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 01:11:33 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 7.506 | ppl 181.75 | wps 75251.5 | wpb 2034.1 | bsz 4 | num_updates 25497 | best_loss 6.139
2022-03-02 01:11:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 25497 updates
2022-03-02 01:11:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 01:11:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 01:11:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 65 @ 25497 updates, score 7.506) (writing took 4.221534486860037 seconds)
2022-03-02 01:11:37 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-02 01:11:37 | INFO | train | epoch 065 | loss 3.513 | ppl 11.42 | wps 29603.5 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 25497 | lr 0.000198041 | gnorm 0.923 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 56522
2022-03-02 01:11:37 | INFO | fairseq.trainer | begin training epoch 66
2022-03-02 01:11:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 01:11:44 | INFO | train_inner | epoch 066:      3 / 393 loss=3.57, ppl=11.87, wps=28960.3, ups=0.44, wpb=65248.6, bsz=127.4, num_updates=25500, lr=0.00019803, gnorm=0.933, loss_scale=16, train_wall=213, gb_free=13.4, wall=56528
2022-03-02 01:15:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 01:15:25 | INFO | train_inner | epoch 066:    104 / 393 loss=3.425, ppl=10.74, wps=29628, ups=0.45, wpb=65530.9, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.918, loss_scale=8, train_wall=216, gb_free=13.4, wall=56750
2022-03-02 01:19:04 | INFO | train_inner | epoch 066:    204 / 393 loss=3.493, ppl=11.26, wps=29935.4, ups=0.46, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.949, loss_scale=8, train_wall=214, gb_free=13.4, wall=56969
2022-03-02 01:22:43 | INFO | train_inner | epoch 066:    304 / 393 loss=3.528, ppl=11.53, wps=29938.8, ups=0.46, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.939, loss_scale=8, train_wall=214, gb_free=13.4, wall=57187
2022-03-02 01:25:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 01:26:00 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 7.501 | ppl 181.15 | wps 75207.3 | wpb 2034.1 | bsz 4 | num_updates 25889 | best_loss 6.139
2022-03-02 01:26:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 25889 updates
2022-03-02 01:26:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 01:26:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 01:26:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 66 @ 25889 updates, score 7.501) (writing took 4.131881679175422 seconds)
2022-03-02 01:26:04 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-02 01:26:04 | INFO | train | epoch 066 | loss 3.5 | ppl 11.31 | wps 29614.7 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 25889 | lr 0.000196536 | gnorm 0.937 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 57388
2022-03-02 01:26:04 | INFO | fairseq.trainer | begin training epoch 67
2022-03-02 01:26:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 01:26:28 | INFO | train_inner | epoch 067:     11 / 393 loss=3.542, ppl=11.65, wps=29003.1, ups=0.44, wpb=65248.6, bsz=127.4, num_updates=25900, lr=0.000196494, gnorm=0.943, loss_scale=8, train_wall=213, gb_free=13.4, wall=57412
2022-03-02 01:30:07 | INFO | train_inner | epoch 067:    111 / 393 loss=3.416, ppl=10.67, wps=29924.3, ups=0.46, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.917, loss_scale=8, train_wall=214, gb_free=13.4, wall=57631
2022-03-02 01:33:46 | INFO | train_inner | epoch 067:    211 / 393 loss=3.485, ppl=11.2, wps=29927.6, ups=0.46, wpb=65535.4, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.926, loss_scale=8, train_wall=214, gb_free=13.4, wall=57850
2022-03-02 01:35:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 01:37:27 | INFO | train_inner | epoch 067:    312 / 393 loss=3.522, ppl=11.49, wps=29633.2, ups=0.45, wpb=65530.9, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.941, loss_scale=8, train_wall=216, gb_free=13.4, wall=58072
2022-03-02 01:40:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 01:40:26 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 7.52 | ppl 183.61 | wps 74537.5 | wpb 2034.1 | bsz 4 | num_updates 26281 | best_loss 6.139
2022-03-02 01:40:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 26281 updates
2022-03-02 01:40:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 01:40:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 01:40:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 67 @ 26281 updates, score 7.52) (writing took 4.1813776339404285 seconds)
2022-03-02 01:40:31 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-02 01:40:31 | INFO | train | epoch 067 | loss 3.486 | ppl 11.21 | wps 29602.3 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 26281 | lr 0.000195065 | gnorm 0.932 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 58255
2022-03-02 01:40:31 | INFO | fairseq.trainer | begin training epoch 68
2022-03-02 01:40:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 01:41:12 | INFO | train_inner | epoch 068:     19 / 393 loss=3.513, ppl=11.42, wps=28961.9, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=26300, lr=0.000194994, gnorm=0.942, loss_scale=8, train_wall=213, gb_free=13.4, wall=58297
2022-03-02 01:44:51 | INFO | train_inner | epoch 068:    119 / 393 loss=3.405, ppl=10.6, wps=29947.8, ups=0.46, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.925, loss_scale=8, train_wall=214, gb_free=13.4, wall=58516
2022-03-02 01:48:30 | INFO | train_inner | epoch 068:    219 / 393 loss=3.47, ppl=11.08, wps=29929, ups=0.46, wpb=65535.4, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.94, loss_scale=8, train_wall=214, gb_free=13.4, wall=58735
2022-03-02 01:52:09 | INFO | train_inner | epoch 068:    319 / 393 loss=3.514, ppl=11.43, wps=29938.7, ups=0.46, wpb=65530.9, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.971, loss_scale=8, train_wall=214, gb_free=13.4, wall=58954
2022-03-02 01:54:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 01:54:53 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 7.516 | ppl 183.08 | wps 75010.9 | wpb 2034.1 | bsz 4 | num_updates 26674 | best_loss 6.139
2022-03-02 01:54:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 26674 updates
2022-03-02 01:54:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 01:54:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 01:54:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 68 @ 26674 updates, score 7.516) (writing took 4.109301395015791 seconds)
2022-03-02 01:54:57 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-02 01:54:57 | INFO | train | epoch 068 | loss 3.474 | ppl 11.11 | wps 29690.9 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 26674 | lr 0.000193623 | gnorm 0.95 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 59122
2022-03-02 01:54:57 | INFO | fairseq.trainer | begin training epoch 69
2022-03-02 01:54:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 01:55:54 | INFO | train_inner | epoch 069:     26 / 393 loss=3.498, ppl=11.3, wps=28992.3, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=26700, lr=0.000193528, gnorm=0.971, loss_scale=16, train_wall=213, gb_free=13.4, wall=59179
2022-03-02 01:59:33 | INFO | train_inner | epoch 069:    126 / 393 loss=3.412, ppl=10.64, wps=29934.1, ups=0.46, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.911, loss_scale=16, train_wall=214, gb_free=13.4, wall=59398
2022-03-02 02:03:12 | INFO | train_inner | epoch 069:    226 / 393 loss=3.448, ppl=10.91, wps=29933.7, ups=0.46, wpb=65535.4, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.944, loss_scale=16, train_wall=214, gb_free=13.4, wall=59616
2022-03-02 02:06:51 | INFO | train_inner | epoch 069:    326 / 393 loss=3.503, ppl=11.34, wps=29928.2, ups=0.46, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.954, loss_scale=16, train_wall=214, gb_free=13.4, wall=59835
2022-03-02 02:09:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 02:09:20 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 7.534 | ppl 185.37 | wps 75379.3 | wpb 2034.1 | bsz 4 | num_updates 27067 | best_loss 6.139
2022-03-02 02:09:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 27067 updates
2022-03-02 02:09:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 02:09:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 02:09:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 69 @ 27067 updates, score 7.534) (writing took 4.154306737938896 seconds)
2022-03-02 02:09:24 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-02 02:09:24 | INFO | train | epoch 069 | loss 3.461 | ppl 11.01 | wps 29679.8 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 27067 | lr 0.000192212 | gnorm 0.942 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 59988
2022-03-02 02:09:24 | INFO | fairseq.trainer | begin training epoch 70
2022-03-02 02:09:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 02:10:36 | INFO | train_inner | epoch 070:     33 / 393 loss=3.478, ppl=11.14, wps=28959.7, ups=0.44, wpb=65239, bsz=127.4, num_updates=27100, lr=0.000192095, gnorm=0.957, loss_scale=16, train_wall=213, gb_free=13.4, wall=60061
2022-03-02 02:13:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 02:14:18 | INFO | train_inner | epoch 070:    134 / 393 loss=3.399, ppl=10.55, wps=29620.5, ups=0.45, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.935, loss_scale=16, train_wall=216, gb_free=13.4, wall=60282
2022-03-02 02:15:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 02:17:59 | INFO | train_inner | epoch 070:    235 / 393 loss=3.438, ppl=10.84, wps=29638.8, ups=0.45, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.947, loss_scale=8, train_wall=216, gb_free=13.4, wall=60503
2022-03-02 02:21:38 | INFO | train_inner | epoch 070:    335 / 393 loss=3.488, ppl=11.22, wps=29938.8, ups=0.46, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.965, loss_scale=8, train_wall=214, gb_free=13.4, wall=60722
2022-03-02 02:23:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 02:23:46 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 7.55 | ppl 187.39 | wps 75259.4 | wpb 2034.1 | bsz 4 | num_updates 27458 | best_loss 6.139
2022-03-02 02:23:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 27458 updates
2022-03-02 02:23:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 02:23:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 02:23:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 70 @ 27458 updates, score 7.55) (writing took 4.1213511121459305 seconds)
2022-03-02 02:23:51 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-02 02:23:51 | INFO | train | epoch 070 | loss 3.448 | ppl 10.92 | wps 29534.3 | ups 0.45 | wpb 65461.2 | bsz 127.9 | num_updates 27458 | lr 0.000190838 | gnorm 0.954 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 60855
2022-03-02 02:23:51 | INFO | fairseq.trainer | begin training epoch 71
2022-03-02 02:23:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 02:25:23 | INFO | train_inner | epoch 071:     42 / 393 loss=3.449, ppl=10.92, wps=28990, ups=0.44, wpb=65248.6, bsz=127.4, num_updates=27500, lr=0.000190693, gnorm=0.979, loss_scale=8, train_wall=213, gb_free=13.4, wall=60947
2022-03-02 02:29:01 | INFO | train_inner | epoch 071:    142 / 393 loss=3.394, ppl=10.51, wps=29944.9, ups=0.46, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.954, loss_scale=8, train_wall=214, gb_free=13.4, wall=61166
2022-03-02 02:32:40 | INFO | train_inner | epoch 071:    242 / 393 loss=3.429, ppl=10.77, wps=29951.6, ups=0.46, wpb=65530.2, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.962, loss_scale=8, train_wall=214, gb_free=13.4, wall=61385
2022-03-02 02:36:19 | INFO | train_inner | epoch 071:    342 / 393 loss=3.485, ppl=11.2, wps=29936.7, ups=0.46, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.98, loss_scale=16, train_wall=214, gb_free=13.4, wall=61604
2022-03-02 02:38:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 02:38:13 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 7.586 | ppl 192.18 | wps 73792.4 | wpb 2034.1 | bsz 4 | num_updates 27851 | best_loss 6.139
2022-03-02 02:38:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 27851 updates
2022-03-02 02:38:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 02:38:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 02:38:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 71 @ 27851 updates, score 7.586) (writing took 4.150959403021261 seconds)
2022-03-02 02:38:17 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-02 02:38:17 | INFO | train | epoch 071 | loss 3.437 | ppl 10.83 | wps 29682.3 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 27851 | lr 0.000189487 | gnorm 0.964 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 61722
2022-03-02 02:38:17 | INFO | fairseq.trainer | begin training epoch 72
2022-03-02 02:38:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 02:40:05 | INFO | train_inner | epoch 072:     49 / 393 loss=3.432, ppl=10.79, wps=28935.7, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=27900, lr=0.000189321, gnorm=0.946, loss_scale=16, train_wall=213, gb_free=13.4, wall=61829
2022-03-02 02:41:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 02:43:46 | INFO | train_inner | epoch 072:    150 / 393 loss=3.379, ppl=10.41, wps=29646.5, ups=0.45, wpb=65536, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.949, loss_scale=8, train_wall=216, gb_free=13.4, wall=62050
2022-03-02 02:47:25 | INFO | train_inner | epoch 072:    250 / 393 loss=3.424, ppl=10.73, wps=29946.7, ups=0.46, wpb=65530.9, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.96, loss_scale=8, train_wall=214, gb_free=13.4, wall=62269
2022-03-02 02:51:03 | INFO | train_inner | epoch 072:    350 / 393 loss=3.474, ppl=11.11, wps=29937.1, ups=0.46, wpb=65535.4, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.964, loss_scale=8, train_wall=214, gb_free=13.4, wall=62488
2022-03-02 02:52:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 02:52:40 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 7.639 | ppl 199.38 | wps 75005.7 | wpb 2034.1 | bsz 4 | num_updates 28243 | best_loss 6.139
2022-03-02 02:52:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 28243 updates
2022-03-02 02:52:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 02:52:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 02:52:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 72 @ 28243 updates, score 7.639) (writing took 4.135103116976097 seconds)
2022-03-02 02:52:44 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-02 02:52:44 | INFO | train | epoch 072 | loss 3.425 | ppl 10.74 | wps 29618.2 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 28243 | lr 0.000188167 | gnorm 0.959 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 62588
2022-03-02 02:52:44 | INFO | fairseq.trainer | begin training epoch 73
2022-03-02 02:52:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 02:54:49 | INFO | train_inner | epoch 073:     57 / 393 loss=3.416, ppl=10.67, wps=28992, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=28300, lr=0.000187978, gnorm=0.959, loss_scale=8, train_wall=213, gb_free=13.4, wall=62713
2022-03-02 02:58:27 | INFO | train_inner | epoch 073:    157 / 393 loss=3.367, ppl=10.32, wps=29928.8, ups=0.46, wpb=65535.4, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.96, loss_scale=8, train_wall=214, gb_free=13.4, wall=62932
2022-03-02 03:02:06 | INFO | train_inner | epoch 073:    257 / 393 loss=3.419, ppl=10.7, wps=29933.2, ups=0.46, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.946, loss_scale=16, train_wall=214, gb_free=13.4, wall=63151
2022-03-02 03:05:45 | INFO | train_inner | epoch 073:    357 / 393 loss=3.468, ppl=11.06, wps=29927.4, ups=0.46, wpb=65536, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.965, loss_scale=16, train_wall=214, gb_free=13.4, wall=63370
2022-03-02 03:07:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 03:07:06 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 7.609 | ppl 195.29 | wps 75277.6 | wpb 2034.1 | bsz 4 | num_updates 28636 | best_loss 6.139
2022-03-02 03:07:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 28636 updates
2022-03-02 03:07:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 03:07:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 03:07:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 73 @ 28636 updates, score 7.609) (writing took 4.159471747931093 seconds)
2022-03-02 03:07:10 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-02 03:07:10 | INFO | train | epoch 073 | loss 3.414 | ppl 10.66 | wps 29684.6 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 28636 | lr 0.000186872 | gnorm 0.958 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 63455
2022-03-02 03:07:10 | INFO | fairseq.trainer | begin training epoch 74
2022-03-02 03:07:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 03:09:31 | INFO | train_inner | epoch 074:     64 / 393 loss=3.386, ppl=10.46, wps=28976, ups=0.44, wpb=65244.2, bsz=127.4, num_updates=28700, lr=0.000186663, gnorm=0.968, loss_scale=16, train_wall=213, gb_free=13.4, wall=63595
2022-03-02 03:13:10 | INFO | train_inner | epoch 074:    164 / 393 loss=3.37, ppl=10.34, wps=29924.3, ups=0.46, wpb=65535.4, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.951, loss_scale=16, train_wall=214, gb_free=13.4, wall=63814
2022-03-02 03:16:49 | INFO | train_inner | epoch 074:    264 / 393 loss=3.409, ppl=10.62, wps=29907.1, ups=0.46, wpb=65536, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.968, loss_scale=16, train_wall=214, gb_free=13.4, wall=64033
2022-03-02 03:18:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-02 03:20:30 | INFO | train_inner | epoch 074:    365 / 393 loss=3.453, ppl=10.95, wps=29623.1, ups=0.45, wpb=65530.9, bsz=128, num_updates=29000, lr=0.000185695, gnorm=0.966, loss_scale=16, train_wall=216, gb_free=13.4, wall=64254
2022-03-02 03:21:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 03:21:33 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 7.657 | ppl 201.88 | wps 75303.6 | wpb 2034.1 | bsz 4 | num_updates 29028 | best_loss 6.139
2022-03-02 03:21:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 29028 updates
2022-03-02 03:21:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 03:21:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 03:21:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 74 @ 29028 updates, score 7.657) (writing took 4.147413984872401 seconds)
2022-03-02 03:21:37 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-02 03:21:37 | INFO | train | epoch 074 | loss 3.402 | ppl 10.57 | wps 29595.8 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 29028 | lr 0.000185606 | gnorm 0.963 | loss_scale 16 | train_wall 841 | gb_free 13.4 | wall 64322
2022-03-02 03:21:37 | INFO | fairseq.trainer | begin training epoch 75
2022-03-02 03:21:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 03:24:15 | INFO | train_inner | epoch 075:     72 / 393 loss=3.363, ppl=10.29, wps=28964.6, ups=0.44, wpb=65244.2, bsz=127.4, num_updates=29100, lr=0.000185376, gnorm=0.966, loss_scale=16, train_wall=213, gb_free=13.4, wall=64480
2022-03-02 03:27:54 | INFO | train_inner | epoch 075:    172 / 393 loss=3.364, ppl=10.29, wps=29898.8, ups=0.46, wpb=65535.4, bsz=128, num_updates=29200, lr=0.000185058, gnorm=0.964, loss_scale=16, train_wall=214, gb_free=13.4, wall=64699
2022-03-02 03:31:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 03:31:36 | INFO | train_inner | epoch 075:    273 / 393 loss=3.404, ppl=10.59, wps=29613.5, ups=0.45, wpb=65536, bsz=128, num_updates=29300, lr=0.000184742, gnorm=0.957, loss_scale=8, train_wall=216, gb_free=13.4, wall=64920
2022-03-02 03:35:15 | INFO | train_inner | epoch 075:    373 / 393 loss=3.446, ppl=10.9, wps=29923.6, ups=0.46, wpb=65536, bsz=128, num_updates=29400, lr=0.000184428, gnorm=0.997, loss_scale=8, train_wall=214, gb_free=13.4, wall=65139
2022-03-02 03:35:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 03:36:00 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 7.655 | ppl 201.6 | wps 75203.7 | wpb 2034.1 | bsz 4 | num_updates 29420 | best_loss 6.139
2022-03-02 03:36:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 29420 updates
2022-03-02 03:36:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 03:36:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 03:36:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 75 @ 29420 updates, score 7.655) (writing took 4.104038306046277 seconds)
2022-03-02 03:36:05 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-02 03:36:05 | INFO | train | epoch 075 | loss 3.391 | ppl 10.49 | wps 29591.2 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 29420 | lr 0.000184365 | gnorm 0.97 | loss_scale 8 | train_wall 841 | gb_free 13.4 | wall 65189
2022-03-02 03:36:05 | INFO | fairseq.trainer | begin training epoch 76
2022-03-02 03:36:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 03:39:00 | INFO | train_inner | epoch 076:     80 / 393 loss=3.33, ppl=10.06, wps=28988.5, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=29500, lr=0.000184115, gnorm=0.956, loss_scale=8, train_wall=213, gb_free=13.4, wall=65364
2022-03-02 03:42:39 | INFO | train_inner | epoch 076:    180 / 393 loss=3.352, ppl=10.21, wps=29931.9, ups=0.46, wpb=65536, bsz=128, num_updates=29600, lr=0.000183804, gnorm=0.963, loss_scale=8, train_wall=214, gb_free=13.4, wall=65583
2022-03-02 03:46:18 | INFO | train_inner | epoch 076:    280 / 393 loss=3.406, ppl=10.6, wps=29935.6, ups=0.46, wpb=65530.2, bsz=128, num_updates=29700, lr=0.000183494, gnorm=0.967, loss_scale=8, train_wall=214, gb_free=13.4, wall=65802
2022-03-02 03:49:57 | INFO | train_inner | epoch 076:    380 / 393 loss=3.437, ppl=10.83, wps=29927.3, ups=0.46, wpb=65536, bsz=128, num_updates=29800, lr=0.000183186, gnorm=0.979, loss_scale=8, train_wall=214, gb_free=13.4, wall=66021
2022-03-02 03:50:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 03:50:27 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 7.657 | ppl 201.78 | wps 75060.3 | wpb 2034.1 | bsz 4 | num_updates 29813 | best_loss 6.139
2022-03-02 03:50:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 29813 updates
2022-03-02 03:50:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 03:50:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 03:50:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 76 @ 29813 updates, score 7.657) (writing took 4.1408947091549635 seconds)
2022-03-02 03:50:31 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-02 03:50:31 | INFO | train | epoch 076 | loss 3.38 | ppl 10.41 | wps 29684 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 29813 | lr 0.000183146 | gnorm 0.968 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 66056
2022-03-02 03:50:31 | INFO | fairseq.trainer | begin training epoch 77
2022-03-02 03:50:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 03:53:42 | INFO | train_inner | epoch 077:     87 / 393 loss=3.317, ppl=9.96, wps=28975.4, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=29900, lr=0.000182879, gnorm=0.969, loss_scale=16, train_wall=213, gb_free=13.4, wall=66246
2022-03-02 03:57:21 | INFO | train_inner | epoch 077:    187 / 393 loss=3.339, ppl=10.12, wps=29934.3, ups=0.46, wpb=65530.9, bsz=128, num_updates=30000, lr=0.000182574, gnorm=0.97, loss_scale=16, train_wall=214, gb_free=13.4, wall=66465
2022-03-02 04:01:00 | INFO | train_inner | epoch 077:    287 / 393 loss=3.393, ppl=10.5, wps=29921.8, ups=0.46, wpb=65536, bsz=128, num_updates=30100, lr=0.000182271, gnorm=0.983, loss_scale=16, train_wall=214, gb_free=13.4, wall=66684
2022-03-02 04:01:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 04:04:41 | INFO | train_inner | epoch 077:    388 / 393 loss=3.436, ppl=10.83, wps=29656.2, ups=0.45, wpb=65535.4, bsz=128, num_updates=30200, lr=0.000181969, gnorm=0.982, loss_scale=8, train_wall=216, gb_free=13.4, wall=66905
2022-03-02 04:04:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 04:04:54 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 7.702 | ppl 208.23 | wps 75222.2 | wpb 2034.1 | bsz 4 | num_updates 30205 | best_loss 6.139
2022-03-02 04:04:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 30205 updates
2022-03-02 04:04:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 04:04:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 04:04:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 77 @ 30205 updates, score 7.702) (writing took 4.128573535010219 seconds)
2022-03-02 04:04:58 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-02 04:04:58 | INFO | train | epoch 077 | loss 3.369 | ppl 10.33 | wps 29612.2 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 30205 | lr 0.000181954 | gnorm 0.974 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 66922
2022-03-02 04:04:58 | INFO | fairseq.trainer | begin training epoch 78
2022-03-02 04:04:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 04:08:26 | INFO | train_inner | epoch 078:     95 / 393 loss=3.295, ppl=9.81, wps=28990.5, ups=0.44, wpb=65243.5, bsz=127.4, num_updates=30300, lr=0.000181668, gnorm=0.964, loss_scale=8, train_wall=213, gb_free=13.4, wall=67130
2022-03-02 04:12:05 | INFO | train_inner | epoch 078:    195 / 393 loss=3.342, ppl=10.14, wps=29908.3, ups=0.46, wpb=65536, bsz=128, num_updates=30400, lr=0.000181369, gnorm=0.964, loss_scale=8, train_wall=214, gb_free=13.4, wall=67349
2022-03-02 04:15:44 | INFO | train_inner | epoch 078:    295 / 393 loss=3.383, ppl=10.44, wps=29939.8, ups=0.46, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=0.99, loss_scale=8, train_wall=214, gb_free=13.4, wall=67568
2022-03-02 04:19:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 04:19:20 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 7.733 | ppl 212.82 | wps 75167 | wpb 2034.1 | bsz 4 | num_updates 30598 | best_loss 6.139
2022-03-02 04:19:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 30598 updates
2022-03-02 04:19:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 04:19:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 04:19:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 78 @ 30598 updates, score 7.733) (writing took 4.211618169909343 seconds)
2022-03-02 04:19:25 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-02 04:19:25 | INFO | train | epoch 078 | loss 3.359 | ppl 10.26 | wps 29681 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 30598 | lr 0.000180781 | gnorm 0.978 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 67789
2022-03-02 04:19:25 | INFO | fairseq.trainer | begin training epoch 79
2022-03-02 04:19:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 04:19:29 | INFO | train_inner | epoch 079:      2 / 393 loss=3.419, ppl=10.7, wps=28967.5, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=30600, lr=0.000180775, gnorm=0.993, loss_scale=8, train_wall=213, gb_free=13.4, wall=67794
2022-03-02 04:23:08 | INFO | train_inner | epoch 079:    102 / 393 loss=3.279, ppl=9.7, wps=29949.4, ups=0.46, wpb=65530.9, bsz=128, num_updates=30700, lr=0.000180481, gnorm=0.975, loss_scale=16, train_wall=214, gb_free=13.4, wall=68012
2022-03-02 04:26:47 | INFO | train_inner | epoch 079:    202 / 393 loss=3.33, ppl=10.06, wps=29910.8, ups=0.46, wpb=65536, bsz=128, num_updates=30800, lr=0.000180187, gnorm=0.983, loss_scale=16, train_wall=214, gb_free=13.4, wall=68231
2022-03-02 04:30:26 | INFO | train_inner | epoch 079:    302 / 393 loss=3.379, ppl=10.4, wps=29914.8, ups=0.46, wpb=65535.4, bsz=128, num_updates=30900, lr=0.000179896, gnorm=0.972, loss_scale=16, train_wall=214, gb_free=13.4, wall=68451
2022-03-02 04:33:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 04:33:47 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 7.695 | ppl 207.21 | wps 74980.1 | wpb 2034.1 | bsz 4 | num_updates 30991 | best_loss 6.139
2022-03-02 04:33:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 30991 updates
2022-03-02 04:33:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 04:33:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 04:33:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 79 @ 30991 updates, score 7.695) (writing took 4.118115606019273 seconds)
2022-03-02 04:33:51 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-02 04:33:51 | INFO | train | epoch 079 | loss 3.349 | ppl 10.19 | wps 29678.7 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 30991 | lr 0.000179631 | gnorm 0.983 | loss_scale 16 | train_wall 840 | gb_free 13.4 | wall 68656
2022-03-02 04:33:51 | INFO | fairseq.trainer | begin training epoch 80
2022-03-02 04:33:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 04:34:11 | INFO | train_inner | epoch 080:      9 / 393 loss=3.402, ppl=10.57, wps=28980.6, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=31000, lr=0.000179605, gnorm=1.006, loss_scale=16, train_wall=213, gb_free=13.4, wall=68676
2022-03-02 04:37:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 04:37:52 | INFO | train_inner | epoch 080:    110 / 393 loss=3.271, ppl=9.65, wps=29626.9, ups=0.45, wpb=65536, bsz=128, num_updates=31100, lr=0.000179316, gnorm=0.969, loss_scale=8, train_wall=216, gb_free=13.4, wall=68897
2022-03-02 04:41:31 | INFO | train_inner | epoch 080:    210 / 393 loss=3.329, ppl=10.05, wps=29939.8, ups=0.46, wpb=65536, bsz=128, num_updates=31200, lr=0.000179029, gnorm=0.986, loss_scale=8, train_wall=214, gb_free=13.4, wall=69116
2022-03-02 04:45:10 | INFO | train_inner | epoch 080:    310 / 393 loss=3.369, ppl=10.33, wps=29931.2, ups=0.46, wpb=65536, bsz=128, num_updates=31300, lr=0.000178743, gnorm=0.985, loss_scale=8, train_wall=214, gb_free=13.4, wall=69335
2022-03-02 04:48:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 04:48:14 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 7.751 | ppl 215.42 | wps 75203.7 | wpb 2034.1 | bsz 4 | num_updates 31383 | best_loss 6.139
2022-03-02 04:48:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 31383 updates
2022-03-02 04:48:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 04:48:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 04:48:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 80 @ 31383 updates, score 7.751) (writing took 4.098635884001851 seconds)
2022-03-02 04:48:18 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-02 04:48:18 | INFO | train | epoch 080 | loss 3.339 | ppl 10.12 | wps 29610 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 31383 | lr 0.000178506 | gnorm 0.983 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 69522
2022-03-02 04:48:18 | INFO | fairseq.trainer | begin training epoch 81
2022-03-02 04:48:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 04:48:55 | INFO | train_inner | epoch 081:     17 / 393 loss=3.379, ppl=10.4, wps=28993.3, ups=0.44, wpb=65243.5, bsz=127.4, num_updates=31400, lr=0.000178458, gnorm=0.983, loss_scale=8, train_wall=213, gb_free=13.4, wall=69560
2022-03-02 04:52:34 | INFO | train_inner | epoch 081:    117 / 393 loss=3.27, ppl=9.65, wps=29952.8, ups=0.46, wpb=65536, bsz=128, num_updates=31500, lr=0.000178174, gnorm=1.002, loss_scale=8, train_wall=214, gb_free=13.4, wall=69779
2022-03-02 04:56:13 | INFO | train_inner | epoch 081:    217 / 393 loss=3.321, ppl=9.99, wps=29924, ups=0.46, wpb=65530.2, bsz=128, num_updates=31600, lr=0.000177892, gnorm=0.981, loss_scale=8, train_wall=214, gb_free=13.4, wall=69998
2022-03-02 04:59:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 04:59:54 | INFO | train_inner | epoch 081:    318 / 393 loss=3.356, ppl=10.24, wps=29627.7, ups=0.45, wpb=65536, bsz=128, num_updates=31700, lr=0.000177611, gnorm=0.991, loss_scale=8, train_wall=216, gb_free=13.4, wall=70219
2022-03-02 05:02:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 05:02:40 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 7.773 | ppl 218.79 | wps 75204.4 | wpb 2034.1 | bsz 4 | num_updates 31775 | best_loss 6.139
2022-03-02 05:02:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 31775 updates
2022-03-02 05:02:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 05:02:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 05:02:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 81 @ 31775 updates, score 7.773) (writing took 4.188565215095878 seconds)
2022-03-02 05:02:45 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-02 05:02:45 | INFO | train | epoch 081 | loss 3.329 | ppl 10.05 | wps 29613.2 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 31775 | lr 0.000177401 | gnorm 0.988 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 70389
2022-03-02 05:02:45 | INFO | fairseq.trainer | begin training epoch 82
2022-03-02 05:02:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 05:03:39 | INFO | train_inner | epoch 082:     25 / 393 loss=3.357, ppl=10.25, wps=28981.6, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=31800, lr=0.000177332, gnorm=0.978, loss_scale=8, train_wall=213, gb_free=13.4, wall=70444
2022-03-02 05:07:18 | INFO | train_inner | epoch 082:    125 / 393 loss=3.26, ppl=9.58, wps=29941.7, ups=0.46, wpb=65536, bsz=128, num_updates=31900, lr=0.000177054, gnorm=0.988, loss_scale=8, train_wall=214, gb_free=13.4, wall=70663
2022-03-02 05:10:57 | INFO | train_inner | epoch 082:    225 / 393 loss=3.317, ppl=9.97, wps=29933.2, ups=0.46, wpb=65536, bsz=128, num_updates=32000, lr=0.000176777, gnorm=0.995, loss_scale=8, train_wall=214, gb_free=13.4, wall=70882
2022-03-02 05:14:36 | INFO | train_inner | epoch 082:    325 / 393 loss=3.362, ppl=10.28, wps=29934, ups=0.46, wpb=65530.2, bsz=128, num_updates=32100, lr=0.000176501, gnorm=0.988, loss_scale=8, train_wall=214, gb_free=13.4, wall=71101
2022-03-02 05:17:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 05:17:07 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 7.792 | ppl 221.61 | wps 75128 | wpb 2034.1 | bsz 4 | num_updates 32168 | best_loss 6.139
2022-03-02 05:17:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 32168 updates
2022-03-02 05:17:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 05:17:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 05:17:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 82 @ 32168 updates, score 7.792) (writing took 4.116211909102276 seconds)
2022-03-02 05:17:11 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-02 05:17:11 | INFO | train | epoch 082 | loss 3.32 | ppl 9.98 | wps 29691.3 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 32168 | lr 0.000176314 | gnorm 0.99 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 71255
2022-03-02 05:17:11 | INFO | fairseq.trainer | begin training epoch 83
2022-03-02 05:17:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 05:18:21 | INFO | train_inner | epoch 083:     32 / 393 loss=3.332, ppl=10.07, wps=28994.8, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=32200, lr=0.000176227, gnorm=1.005, loss_scale=16, train_wall=213, gb_free=13.4, wall=71326
2022-03-02 05:22:00 | INFO | train_inner | epoch 083:    132 / 393 loss=3.255, ppl=9.55, wps=29914.9, ups=0.46, wpb=65530.9, bsz=128, num_updates=32300, lr=0.000175954, gnorm=0.978, loss_scale=16, train_wall=214, gb_free=13.4, wall=71545
2022-03-02 05:25:39 | INFO | train_inner | epoch 083:    232 / 393 loss=3.31, ppl=9.92, wps=29927.4, ups=0.46, wpb=65536, bsz=128, num_updates=32400, lr=0.000175682, gnorm=0.985, loss_scale=16, train_wall=214, gb_free=13.4, wall=71764
2022-03-02 05:29:18 | INFO | train_inner | epoch 083:    332 / 393 loss=3.345, ppl=10.16, wps=29897.5, ups=0.46, wpb=65535.4, bsz=128, num_updates=32500, lr=0.000175412, gnorm=0.982, loss_scale=16, train_wall=214, gb_free=13.4, wall=71983
2022-03-02 05:29:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 05:31:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 05:31:34 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 7.829 | ppl 227.38 | wps 74968.5 | wpb 2034.1 | bsz 4 | num_updates 32560 | best_loss 6.139
2022-03-02 05:31:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 32560 updates
2022-03-02 05:31:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 05:31:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 05:31:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 83 @ 32560 updates, score 7.829) (writing took 4.1103694890625775 seconds)
2022-03-02 05:31:38 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-02 05:31:38 | INFO | train | epoch 083 | loss 3.309 | ppl 9.91 | wps 29597.4 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 32560 | lr 0.00017525 | gnorm 0.99 | loss_scale 8 | train_wall 841 | gb_free 13.4 | wall 72122
2022-03-02 05:31:38 | INFO | fairseq.trainer | begin training epoch 84
2022-03-02 05:31:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 05:33:06 | INFO | train_inner | epoch 084:     40 / 393 loss=3.312, ppl=9.93, wps=28707.1, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=32600, lr=0.000175142, gnorm=1.004, loss_scale=8, train_wall=215, gb_free=13.4, wall=72210
2022-03-02 05:36:45 | INFO | train_inner | epoch 084:    140 / 393 loss=3.263, ppl=9.6, wps=29925.8, ups=0.46, wpb=65536, bsz=128, num_updates=32700, lr=0.000174874, gnorm=0.986, loss_scale=8, train_wall=214, gb_free=13.4, wall=72429
2022-03-02 05:40:24 | INFO | train_inner | epoch 084:    240 / 393 loss=3.302, ppl=9.86, wps=29936, ups=0.46, wpb=65530.9, bsz=128, num_updates=32800, lr=0.000174608, gnorm=0.989, loss_scale=8, train_wall=214, gb_free=13.4, wall=72648
2022-03-02 05:44:02 | INFO | train_inner | epoch 084:    340 / 393 loss=3.34, ppl=10.13, wps=29944.8, ups=0.46, wpb=65536, bsz=128, num_updates=32900, lr=0.000174342, gnorm=0.996, loss_scale=8, train_wall=214, gb_free=13.4, wall=72867
2022-03-02 05:45:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 05:46:00 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 7.834 | ppl 228.16 | wps 75057.5 | wpb 2034.1 | bsz 4 | num_updates 32953 | best_loss 6.139
2022-03-02 05:46:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 32953 updates
2022-03-02 05:46:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 05:46:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 05:46:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 84 @ 32953 updates, score 7.834) (writing took 4.1274127590004355 seconds)
2022-03-02 05:46:05 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-02 05:46:05 | INFO | train | epoch 084 | loss 3.301 | ppl 9.85 | wps 29688.7 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 32953 | lr 0.000174202 | gnorm 0.99 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 72989
2022-03-02 05:46:05 | INFO | fairseq.trainer | begin training epoch 85
2022-03-02 05:46:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 05:47:47 | INFO | train_inner | epoch 085:     47 / 393 loss=3.288, ppl=9.76, wps=29000.7, ups=0.44, wpb=65248.6, bsz=127.4, num_updates=33000, lr=0.000174078, gnorm=0.988, loss_scale=8, train_wall=213, gb_free=13.4, wall=73092
2022-03-02 05:51:27 | INFO | train_inner | epoch 085:    147 / 393 loss=3.262, ppl=9.59, wps=29900.6, ups=0.46, wpb=65536, bsz=128, num_updates=33100, lr=0.000173814, gnorm=0.992, loss_scale=16, train_wall=214, gb_free=13.4, wall=73311
2022-03-02 05:55:06 | INFO | train_inner | epoch 085:    247 / 393 loss=3.289, ppl=9.77, wps=29829.9, ups=0.46, wpb=65535.4, bsz=128, num_updates=33200, lr=0.000173553, gnorm=0.989, loss_scale=16, train_wall=215, gb_free=13.4, wall=73531
2022-03-02 05:58:45 | INFO | train_inner | epoch 085:    347 / 393 loss=3.332, ppl=10.07, wps=29924.6, ups=0.46, wpb=65530.9, bsz=128, num_updates=33300, lr=0.000173292, gnorm=0.997, loss_scale=16, train_wall=214, gb_free=13.4, wall=73750
2022-03-02 05:59:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 06:00:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 06:00:28 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 7.825 | ppl 226.72 | wps 74935.7 | wpb 2034.1 | bsz 4 | num_updates 33345 | best_loss 6.139
2022-03-02 06:00:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 33345 updates
2022-03-02 06:00:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 06:00:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 06:00:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 85 @ 33345 updates, score 7.825) (writing took 4.1355230279732496 seconds)
2022-03-02 06:00:32 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-02 06:00:32 | INFO | train | epoch 085 | loss 3.292 | ppl 9.79 | wps 29578 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 33345 | lr 0.000173175 | gnorm 0.997 | loss_scale 8 | train_wall 841 | gb_free 13.4 | wall 73857
2022-03-02 06:00:32 | INFO | fairseq.trainer | begin training epoch 86
2022-03-02 06:00:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 06:02:32 | INFO | train_inner | epoch 086:     55 / 393 loss=3.285, ppl=9.75, wps=28725, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=33400, lr=0.000173032, gnorm=1.002, loss_scale=8, train_wall=215, gb_free=13.4, wall=73977
2022-03-02 06:06:11 | INFO | train_inner | epoch 086:    155 / 393 loss=3.247, ppl=9.49, wps=29933.2, ups=0.46, wpb=65535.4, bsz=128, num_updates=33500, lr=0.000172774, gnorm=0.996, loss_scale=8, train_wall=214, gb_free=13.4, wall=74196
2022-03-02 06:09:50 | INFO | train_inner | epoch 086:    255 / 393 loss=3.292, ppl=9.79, wps=29938.4, ups=0.46, wpb=65530.9, bsz=128, num_updates=33600, lr=0.000172516, gnorm=0.992, loss_scale=8, train_wall=214, gb_free=13.4, wall=74415
2022-03-02 06:13:29 | INFO | train_inner | epoch 086:    355 / 393 loss=3.321, ppl=9.99, wps=29922.9, ups=0.46, wpb=65536, bsz=128, num_updates=33700, lr=0.00017226, gnorm=0.998, loss_scale=8, train_wall=214, gb_free=13.4, wall=74634
2022-03-02 06:14:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 06:14:55 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 7.853 | ppl 231.13 | wps 74732 | wpb 2034.1 | bsz 4 | num_updates 33738 | best_loss 6.139
2022-03-02 06:14:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 33738 updates
2022-03-02 06:14:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 06:14:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 06:14:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 86 @ 33738 updates, score 7.853) (writing took 4.1974133918993175 seconds)
2022-03-02 06:14:59 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-02 06:14:59 | INFO | train | epoch 086 | loss 3.283 | ppl 9.73 | wps 29685.8 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 33738 | lr 0.000172163 | gnorm 0.993 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 74723
2022-03-02 06:14:59 | INFO | fairseq.trainer | begin training epoch 87
2022-03-02 06:14:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 06:17:14 | INFO | train_inner | epoch 087:     62 / 393 loss=3.25, ppl=9.52, wps=28979.1, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=33800, lr=0.000172005, gnorm=1.001, loss_scale=8, train_wall=213, gb_free=13.4, wall=74859
2022-03-02 06:20:54 | INFO | train_inner | epoch 087:    162 / 393 loss=3.233, ppl=9.4, wps=29866.2, ups=0.46, wpb=65530.9, bsz=128, num_updates=33900, lr=0.000171751, gnorm=1.001, loss_scale=16, train_wall=214, gb_free=13.4, wall=75078
2022-03-02 06:24:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 06:24:35 | INFO | train_inner | epoch 087:    263 / 393 loss=3.29, ppl=9.78, wps=29576.3, ups=0.45, wpb=65535.4, bsz=128, num_updates=34000, lr=0.000171499, gnorm=1.011, loss_scale=8, train_wall=217, gb_free=13.4, wall=75300
2022-03-02 06:28:15 | INFO | train_inner | epoch 087:    363 / 393 loss=3.325, ppl=10.02, wps=29889.9, ups=0.46, wpb=65536, bsz=128, num_updates=34100, lr=0.000171247, gnorm=1.011, loss_scale=8, train_wall=214, gb_free=13.4, wall=75519
2022-03-02 06:29:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 06:29:22 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 7.884 | ppl 236.2 | wps 74808.8 | wpb 2034.1 | bsz 4 | num_updates 34130 | best_loss 6.139
2022-03-02 06:29:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 34130 updates
2022-03-02 06:29:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 06:29:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 06:29:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 87 @ 34130 updates, score 7.884) (writing took 4.116907880874351 seconds)
2022-03-02 06:29:27 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-02 06:29:27 | INFO | train | epoch 087 | loss 3.273 | ppl 9.67 | wps 29568 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 34130 | lr 0.000171172 | gnorm 1.007 | loss_scale 8 | train_wall 841 | gb_free 13.4 | wall 75591
2022-03-02 06:29:27 | INFO | fairseq.trainer | begin training epoch 88
2022-03-02 06:29:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 06:32:00 | INFO | train_inner | epoch 088:     70 / 393 loss=3.236, ppl=9.42, wps=28948.5, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=34200, lr=0.000170996, gnorm=0.99, loss_scale=8, train_wall=213, gb_free=13.4, wall=75745
2022-03-02 06:35:40 | INFO | train_inner | epoch 088:    170 / 393 loss=3.23, ppl=9.38, wps=29873.3, ups=0.46, wpb=65536, bsz=128, num_updates=34300, lr=0.000170747, gnorm=1.003, loss_scale=8, train_wall=214, gb_free=13.4, wall=75964
2022-03-02 06:39:19 | INFO | train_inner | epoch 088:    270 / 393 loss=3.279, ppl=9.71, wps=29900.6, ups=0.46, wpb=65535.4, bsz=128, num_updates=34400, lr=0.000170499, gnorm=1.007, loss_scale=8, train_wall=214, gb_free=13.4, wall=76183
2022-03-02 06:42:57 | INFO | train_inner | epoch 088:    370 / 393 loss=3.322, ppl=10, wps=29957.2, ups=0.46, wpb=65530.9, bsz=128, num_updates=34500, lr=0.000170251, gnorm=1.021, loss_scale=16, train_wall=214, gb_free=13.4, wall=76402
2022-03-02 06:43:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 06:43:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 06:43:50 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 7.892 | ppl 237.53 | wps 74955 | wpb 2034.1 | bsz 4 | num_updates 34522 | best_loss 6.139
2022-03-02 06:43:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 34522 updates
2022-03-02 06:43:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 06:43:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 06:43:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 88 @ 34522 updates, score 7.892) (writing took 4.168338348157704 seconds)
2022-03-02 06:43:54 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-02 06:43:54 | INFO | train | epoch 088 | loss 3.265 | ppl 9.61 | wps 29586.9 | ups 0.45 | wpb 65461.4 | bsz 127.9 | num_updates 34522 | lr 0.000170197 | gnorm 1.007 | loss_scale 8 | train_wall 841 | gb_free 13.4 | wall 76458
2022-03-02 06:43:54 | INFO | fairseq.trainer | begin training epoch 89
2022-03-02 06:43:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 06:46:45 | INFO | train_inner | epoch 089:     78 / 393 loss=3.221, ppl=9.32, wps=28722.1, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=34600, lr=0.000170005, gnorm=1.001, loss_scale=8, train_wall=215, gb_free=13.4, wall=76629
2022-03-02 06:50:23 | INFO | train_inner | epoch 089:    178 / 393 loss=3.229, ppl=9.38, wps=29948.3, ups=0.46, wpb=65536, bsz=128, num_updates=34700, lr=0.00016976, gnorm=1.006, loss_scale=8, train_wall=214, gb_free=13.4, wall=76848
2022-03-02 06:54:02 | INFO | train_inner | epoch 089:    278 / 393 loss=3.267, ppl=9.63, wps=29954.1, ups=0.46, wpb=65530.2, bsz=128, num_updates=34800, lr=0.000169516, gnorm=1.009, loss_scale=8, train_wall=214, gb_free=13.4, wall=77067
2022-03-02 06:57:41 | INFO | train_inner | epoch 089:    378 / 393 loss=3.32, ppl=9.98, wps=29921.8, ups=0.46, wpb=65536, bsz=128, num_updates=34900, lr=0.000169273, gnorm=1.008, loss_scale=8, train_wall=214, gb_free=13.4, wall=77286
2022-03-02 06:58:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-02 06:58:16 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 7.926 | ppl 243.2 | wps 75199.5 | wpb 2034.1 | bsz 4 | num_updates 34915 | best_loss 6.139
2022-03-02 06:58:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 34915 updates
2022-03-02 06:58:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 06:58:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-02 06:58:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 89 @ 34915 updates, score 7.926) (writing took 4.17559778294526 seconds)
2022-03-02 06:58:20 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-02 06:58:20 | INFO | train | epoch 089 | loss 3.257 | ppl 9.56 | wps 29696.4 | ups 0.45 | wpb 65461.6 | bsz 127.9 | num_updates 34915 | lr 0.000169236 | gnorm 1.005 | loss_scale 8 | train_wall 840 | gb_free 13.4 | wall 77325
2022-03-02 06:58:20 | INFO | fairseq.trainer | begin training epoch 90
2022-03-02 06:58:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-02 07:01:26 | INFO | train_inner | epoch 090:     85 / 393 loss=3.199, ppl=9.18, wps=29002.3, ups=0.44, wpb=65249.3, bsz=127.4, num_updates=35000, lr=0.000169031, gnorm=1.002, loss_scale=8, train_wall=213, gb_free=13.4, wall=77511
2022-03-02 07:03:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-02 07:05:07 | INFO | train_inner | epoch 090:    186 / 393 loss=3.225, ppl=9.35, wps=29643.2, ups=0.45, wpb=65536, bsz=128, num_updates=35100, lr=0.00016879, gnorm=1.01, loss_scale=8, train_wall=216, gb_free=13.4, wall=77732
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
