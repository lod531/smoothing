Sender: LSF System <lsfadmin@eu-g3-075>
Subject: Job 207526046: <w103_fp16_size_0.125_jelinek_0.02_0.08_0.9_#3> in cluster <euler> Exited

Job <w103_fp16_size_0.125_jelinek_0.02_0.08_0.9_#3> was submitted from host <eu-login-27> by user <andriusb> in cluster <euler> at Tue Mar  8 08:10:41 2022
Job was executed on host(s) <eu-g3-075>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Tue Mar  8 08:11:15 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar  8 08:11:15 2022
Terminated at Tue Mar  8 16:28:06 2022
Results reported at Tue Mar  8 16:28:06 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.02, 0.08, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --seed 1321673 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   29780.03 sec.
    Max Memory :                                 7614 MB
    Average Memory :                             4925.68 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               12386.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   29811 sec.
    Turnaround time :                            29845 sec.

The output (if any) follows:

2022-03-08 08:11:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321673, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321673, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.02, 0.08, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-08 08:11:30 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-08 08:11:33 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
Calculating frequency stats:
  0%|          | 0/225169 [00:00<?, ?it/s]  0%|          | 667/225169 [00:00<00:33, 6655.85it/s]  1%|          | 1333/225169 [00:00<00:38, 5861.45it/s]  1%|          | 1926/225169 [00:00<00:39, 5666.00it/s]  1%|          | 2496/225169 [00:00<00:39, 5604.06it/s]  1%|▏         | 3212/225169 [00:00<00:36, 6135.18it/s]  2%|▏         | 3830/225169 [00:00<00:36, 6070.58it/s]  2%|▏         | 4555/225169 [00:00<00:34, 6436.53it/s]  2%|▏         | 5261/225169 [00:00<00:33, 6616.10it/s]  3%|▎         | 5925/225169 [00:00<00:33, 6546.31it/s]  3%|▎         | 6582/225169 [00:01<00:36, 6004.83it/s]  3%|▎         | 7192/225169 [00:01<00:36, 6003.39it/s]  3%|▎         | 7799/225169 [00:01<00:36, 6003.31it/s]  4%|▎         | 8404/225169 [00:01<00:36, 5898.66it/s]  4%|▍         | 9012/225169 [00:01<00:36, 5950.42it/s]  4%|▍         | 9610/225169 [00:01<00:36, 5865.00it/s]  5%|▍         | 10281/225169 [00:01<00:35, 6110.89it/s]  5%|▍         | 10895/225169 [00:01<00:36, 5885.25it/s]  5%|▌         | 11487/225169 [00:01<00:36, 5881.56it/s]  5%|▌         | 12132/225169 [00:02<00:35, 6044.18it/s]  6%|▌         | 12739/225169 [00:02<00:36, 5888.62it/s]  6%|▌         | 13361/225169 [00:02<00:35, 5973.43it/s]  6%|▌         | 14010/225169 [00:02<00:34, 6124.11it/s]  6%|▋         | 14624/225169 [00:02<00:34, 6077.47it/s]  7%|▋         | 15296/225169 [00:02<00:33, 6264.08it/s]  7%|▋         | 15924/225169 [00:02<00:35, 5926.01it/s]  7%|▋         | 16521/225169 [00:02<00:35, 5849.01it/s]  8%|▊         | 17109/225169 [00:02<00:35, 5847.43it/s]  8%|▊         | 17696/225169 [00:02<00:35, 5805.81it/s]  8%|▊         | 18297/225169 [00:03<00:35, 5857.76it/s]  8%|▊         | 19013/225169 [00:03<00:33, 6230.28it/s]  9%|▊         | 19678/225169 [00:03<00:32, 6352.55it/s]  9%|▉         | 20315/225169 [00:03<00:34, 5976.89it/s]  9%|▉         | 20948/225169 [00:03<00:33, 6074.53it/s] 10%|▉         | 21560/225169 [00:03<00:35, 5763.79it/s] 10%|▉         | 22197/225169 [00:03<00:34, 5929.77it/s] 10%|█         | 22855/225169 [00:03<00:33, 6113.01it/s] 10%|█         | 23499/225169 [00:03<00:32, 6206.54it/s] 11%|█         | 24226/225169 [00:03<00:30, 6503.52it/s] 11%|█         | 24932/225169 [00:04<00:30, 6663.69it/s] 11%|█▏        | 25601/225169 [00:04<00:30, 6630.97it/s] 12%|█▏        | 26266/225169 [00:04<00:31, 6237.16it/s] 12%|█▏        | 26896/225169 [00:04<00:33, 5993.19it/s] 12%|█▏        | 27501/225169 [00:04<00:34, 5776.34it/s] 12%|█▏        | 28114/225169 [00:04<00:33, 5873.61it/s] 13%|█▎        | 28809/225169 [00:04<00:31, 6177.63it/s] 13%|█▎        | 29431/225169 [00:04<00:33, 5813.87it/s] 13%|█▎        | 30099/225169 [00:04<00:32, 6055.16it/s] 14%|█▎        | 30711/225169 [00:05<00:34, 5716.72it/s] 14%|█▍        | 31290/225169 [00:05<00:34, 5643.21it/s] 14%|█▍        | 31882/225169 [00:05<00:33, 5718.58it/s] 14%|█▍        | 32458/225169 [00:05<00:33, 5684.13it/s] 15%|█▍        | 33030/225169 [00:05<00:34, 5555.31it/s] 15%|█▍        | 33608/225169 [00:05<00:34, 5619.25it/s] 15%|█▌        | 34172/225169 [00:05<00:34, 5612.07it/s] 15%|█▌        | 34898/225169 [00:05<00:31, 6088.36it/s] 16%|█▌        | 35509/225169 [00:05<00:32, 5844.94it/s] 16%|█▌        | 36103/225169 [00:06<00:32, 5871.09it/s] 16%|█▋        | 36700/225169 [00:06<00:31, 5898.84it/s] 17%|█▋        | 37292/225169 [00:06<00:33, 5602.46it/s] 17%|█▋        | 37857/225169 [00:06<00:33, 5593.82it/s] 17%|█▋        | 38420/225169 [00:06<00:33, 5596.28it/s] 17%|█▋        | 39017/225169 [00:06<00:32, 5701.41it/s] 18%|█▊        | 39589/225169 [00:06<00:32, 5641.39it/s] 18%|█▊        | 40229/225169 [00:06<00:31, 5863.49it/s] 18%|█▊        | 40891/225169 [00:06<00:30, 6085.06it/s] 18%|█▊        | 41501/225169 [00:06<00:30, 5999.89it/s] 19%|█▊        | 42103/225169 [00:07<00:32, 5669.04it/s] 19%|█▉        | 42675/225169 [00:07<00:32, 5654.96it/s] 19%|█▉        | 43244/225169 [00:07<00:32, 5614.34it/s] 20%|█▉        | 43916/225169 [00:07<00:30, 5931.52it/s] 20%|█▉        | 44547/225169 [00:07<00:29, 6042.09it/s] 20%|██        | 45154/225169 [00:07<00:30, 5941.53it/s] 20%|██        | 45780/225169 [00:07<00:29, 6032.37it/s] 21%|██        | 46499/225169 [00:07<00:28, 6372.57it/s] 21%|██        | 47138/225169 [00:07<00:27, 6377.22it/s] 21%|██▏       | 48078/225169 [00:07<00:24, 7275.33it/s] 22%|██▏       | 48808/225169 [00:08<00:25, 7009.69it/s] 22%|██▏       | 49569/225169 [00:08<00:24, 7167.77it/s] 22%|██▏       | 50289/225169 [00:08<00:26, 6538.79it/s] 23%|██▎       | 50955/225169 [00:08<00:27, 6328.89it/s] 23%|██▎       | 51597/225169 [00:08<00:27, 6276.90it/s] 23%|██▎       | 52454/225169 [00:08<00:24, 6912.98it/s] 24%|██▎       | 53154/225169 [00:08<00:26, 6579.82it/s] 24%|██▍       | 53821/225169 [00:08<00:26, 6483.04it/s] 24%|██▍       | 54475/225169 [00:09<00:28, 5973.43it/s] 24%|██▍       | 55104/225169 [00:09<00:28, 6053.58it/s] 25%|██▍       | 55780/225169 [00:09<00:27, 6248.09it/s] 25%|██▌       | 56412/225169 [00:09<00:27, 6209.21it/s] 25%|██▌       | 57038/225169 [00:09<00:28, 5854.45it/s] 26%|██▌       | 57630/225169 [00:09<00:28, 5808.76it/s] 26%|██▌       | 58254/225169 [00:09<00:28, 5928.08it/s] 26%|██▌       | 58935/225169 [00:09<00:26, 6174.18it/s] 26%|██▋       | 59557/225169 [00:09<00:27, 5935.67it/s] 27%|██▋       | 60155/225169 [00:09<00:27, 5929.26it/s] 27%|██▋       | 60849/225169 [00:10<00:26, 6216.75it/s] 27%|██▋       | 61551/225169 [00:10<00:25, 6444.76it/s] 28%|██▊       | 62199/225169 [00:10<00:26, 6207.51it/s] 28%|██▊       | 62832/225169 [00:10<00:26, 6236.73it/s] 28%|██▊       | 63459/225169 [00:10<00:26, 6130.97it/s] 28%|██▊       | 64075/225169 [00:10<00:26, 6062.42it/s] 29%|██▊       | 64683/225169 [00:10<00:26, 6059.86it/s] 29%|██▉       | 65290/225169 [00:10<00:26, 5943.21it/s] 29%|██▉       | 65949/225169 [00:10<00:25, 6128.23it/s] 30%|██▉       | 66590/225169 [00:10<00:25, 6209.75it/s] 30%|██▉       | 67212/225169 [00:11<00:25, 6075.77it/s] 30%|███       | 67821/225169 [00:11<00:27, 5711.22it/s] 30%|███       | 68421/225169 [00:11<00:27, 5788.52it/s] 31%|███       | 69125/225169 [00:11<00:25, 6146.91it/s] 31%|███       | 69819/225169 [00:11<00:24, 6377.27it/s] 31%|███▏      | 70462/225169 [00:11<00:24, 6389.27it/s] 32%|███▏      | 71104/225169 [00:11<00:25, 6040.97it/s] 32%|███▏      | 71740/225169 [00:11<00:25, 6122.14it/s] 32%|███▏      | 72357/225169 [00:11<00:24, 6120.07it/s] 32%|███▏      | 73117/225169 [00:12<00:23, 6550.00it/s] 33%|███▎      | 73803/225169 [00:12<00:22, 6640.77it/s] 33%|███▎      | 74585/225169 [00:12<00:21, 6985.00it/s] 33%|███▎      | 75286/225169 [00:12<00:22, 6642.10it/s] 34%|███▎      | 75956/225169 [00:12<00:22, 6529.08it/s] 34%|███▍      | 76613/225169 [00:12<00:22, 6481.16it/s] 34%|███▍      | 77264/225169 [00:12<00:23, 6210.34it/s] 35%|███▍      | 77962/225169 [00:12<00:22, 6427.77it/s] 35%|███▍      | 78609/225169 [00:12<00:24, 5994.14it/s] 35%|███▌      | 79216/225169 [00:13<00:25, 5672.57it/s] 35%|███▌      | 79810/225169 [00:13<00:25, 5737.46it/s] 36%|███▌      | 80406/225169 [00:13<00:24, 5798.94it/s] 36%|███▌      | 81037/225169 [00:13<00:24, 5942.65it/s] 36%|███▋      | 81691/225169 [00:13<00:23, 6111.05it/s] 37%|███▋      | 82311/225169 [00:13<00:23, 6127.70it/s] 37%|███▋      | 82955/225169 [00:13<00:22, 6218.43it/s] 37%|███▋      | 83622/225169 [00:13<00:22, 6344.83it/s] 37%|███▋      | 84258/225169 [00:13<00:23, 6116.26it/s] 38%|███▊      | 84922/225169 [00:13<00:22, 6267.83it/s] 38%|███▊      | 85552/225169 [00:14<00:22, 6103.89it/s] 38%|███▊      | 86165/225169 [00:14<00:23, 6042.73it/s] 39%|███▊      | 86821/225169 [00:14<00:22, 6192.67it/s] 39%|███▉      | 87520/225169 [00:14<00:21, 6420.57it/s] 39%|███▉      | 88193/225169 [00:14<00:21, 6503.30it/s] 39%|███▉      | 88845/225169 [00:14<00:23, 5848.28it/s] 40%|███▉      | 89443/225169 [00:14<00:23, 5788.76it/s] 40%|███▉      | 90031/225169 [00:14<00:23, 5690.98it/s] 40%|████      | 90629/225169 [00:14<00:23, 5769.09it/s] 41%|████      | 91295/225169 [00:15<00:22, 6023.76it/s] 41%|████      | 91902/225169 [00:15<00:22, 6013.89it/s] 41%|████      | 92601/225169 [00:15<00:21, 6294.94it/s] 41%|████▏     | 93234/225169 [00:15<00:22, 5954.82it/s] 42%|████▏     | 93861/225169 [00:15<00:21, 6043.55it/s] 42%|████▏     | 94474/225169 [00:15<00:21, 6068.18it/s] 42%|████▏     | 95170/225169 [00:15<00:20, 6327.74it/s] 43%|████▎     | 95806/225169 [00:15<00:21, 6036.13it/s] 43%|████▎     | 96553/225169 [00:15<00:19, 6445.81it/s] 43%|████▎     | 97281/225169 [00:15<00:19, 6683.44it/s] 44%|████▎     | 97954/225169 [00:16<00:20, 6291.17it/s] 44%|████▍     | 98591/225169 [00:16<00:20, 6124.77it/s] 44%|████▍     | 99216/225169 [00:16<00:20, 6152.58it/s] 44%|████▍     | 99836/225169 [00:16<00:21, 5939.67it/s] 45%|████▍     | 100434/225169 [00:16<00:21, 5769.74it/s] 45%|████▍     | 101090/225169 [00:16<00:20, 5989.32it/s] 45%|████▌     | 101694/225169 [00:16<00:20, 6003.76it/s] 45%|████▌     | 102337/225169 [00:16<00:20, 6119.70it/s] 46%|████▌     | 102951/225169 [00:16<00:20, 6071.58it/s] 46%|████▌     | 103560/225169 [00:17<00:20, 5896.45it/s] 46%|████▋     | 104152/225169 [00:17<00:21, 5744.18it/s] 47%|████▋     | 104807/225169 [00:17<00:20, 5974.87it/s] 47%|████▋     | 105407/225169 [00:17<00:20, 5933.56it/s] 47%|████▋     | 106023/225169 [00:17<00:19, 5994.72it/s] 47%|████▋     | 106631/225169 [00:17<00:19, 6017.83it/s] 48%|████▊     | 107234/225169 [00:17<00:20, 5863.36it/s] 48%|████▊     | 107856/225169 [00:17<00:19, 5956.69it/s] 48%|████▊     | 108465/225169 [00:17<00:19, 5983.39it/s] 48%|████▊     | 109065/225169 [00:17<00:20, 5753.63it/s] 49%|████▊     | 109643/225169 [00:18<00:20, 5751.22it/s] 49%|████▉     | 110257/225169 [00:18<00:19, 5864.22it/s] 49%|████▉     | 110909/225169 [00:18<00:18, 6056.70it/s] 50%|████▉     | 111558/225169 [00:18<00:18, 6172.90it/s] 50%|████▉     | 112177/225169 [00:18<00:18, 6092.05it/s] 50%|█████     | 112823/225169 [00:18<00:18, 6198.73it/s] 50%|█████     | 113444/225169 [00:18<00:18, 6024.03it/s] 51%|█████     | 114048/225169 [00:18<00:18, 5984.52it/s] 51%|█████     | 114648/225169 [00:18<00:18, 5921.81it/s] 51%|█████     | 115272/225169 [00:18<00:18, 6008.16it/s] 51%|█████▏    | 115874/225169 [00:19<00:19, 5667.61it/s] 52%|█████▏    | 116445/225169 [00:19<00:19, 5675.69it/s] 52%|█████▏    | 117025/225169 [00:19<00:18, 5710.84it/s] 52%|█████▏    | 117608/225169 [00:19<00:18, 5744.53it/s] 52%|█████▏    | 118194/225169 [00:19<00:18, 5778.27it/s] 53%|█████▎    | 118824/225169 [00:19<00:17, 5927.86it/s] 53%|█████▎    | 119418/225169 [00:19<00:18, 5850.78it/s] 53%|█████▎    | 120098/225169 [00:19<00:17, 6129.43it/s] 54%|█████▎    | 120758/225169 [00:19<00:16, 6264.06it/s] 54%|█████▍    | 121386/225169 [00:20<00:17, 6085.98it/s] 54%|█████▍    | 121997/225169 [00:20<00:17, 5888.72it/s] 54%|█████▍    | 122630/225169 [00:20<00:17, 6008.82it/s] 55%|█████▍    | 123233/225169 [00:20<00:17, 5844.02it/s] 55%|█████▌    | 123844/225169 [00:20<00:17, 5918.29it/s] 55%|█████▌    | 124438/225169 [00:20<00:17, 5920.12it/s] 56%|█████▌    | 125049/225169 [00:20<00:16, 5973.52it/s] 56%|█████▌    | 125667/225169 [00:20<00:16, 6027.03it/s] 56%|█████▌    | 126330/225169 [00:20<00:15, 6199.35it/s] 56%|█████▋    | 127001/225169 [00:20<00:15, 6345.76it/s] 57%|█████▋    | 127637/225169 [00:21<00:16, 6036.89it/s] 57%|█████▋    | 128254/225169 [00:21<00:15, 6071.09it/s] 57%|█████▋    | 128864/225169 [00:21<00:15, 6026.10it/s] 58%|█████▊    | 129523/225169 [00:21<00:15, 6185.34it/s] 58%|█████▊    | 130144/225169 [00:21<00:16, 5817.94it/s] 58%|█████▊    | 130731/225169 [00:21<00:16, 5818.92it/s] 58%|█████▊    | 131338/225169 [00:21<00:15, 5883.52it/s] 59%|█████▊    | 131930/225169 [00:21<00:17, 5240.99it/s] 59%|█████▉    | 132613/225169 [00:21<00:16, 5667.41it/s] 59%|█████▉    | 133303/225169 [00:22<00:15, 6002.03it/s] 59%|█████▉    | 133916/225169 [00:22<00:15, 5988.32it/s] 60%|█████▉    | 134618/225169 [00:22<00:14, 6281.83it/s] 60%|██████    | 135351/225169 [00:22<00:13, 6585.15it/s] 60%|██████    | 136016/225169 [00:22<00:14, 6310.56it/s] 61%|██████    | 136745/225169 [00:22<00:13, 6588.42it/s] 61%|██████    | 137472/225169 [00:22<00:12, 6779.80it/s] 61%|██████▏   | 138155/225169 [00:22<00:13, 6338.80it/s] 62%|██████▏   | 138806/225169 [00:22<00:13, 6376.42it/s] 62%|██████▏   | 139450/225169 [00:22<00:13, 6237.43it/s] 62%|██████▏   | 140079/225169 [00:23<00:13, 6201.46it/s] 62%|██████▏   | 140703/225169 [00:23<00:13, 6204.93it/s] 63%|██████▎   | 141326/225169 [00:23<00:14, 5906.34it/s] 63%|██████▎   | 141976/225169 [00:23<00:13, 6072.86it/s] 63%|██████▎   | 142587/225169 [00:23<00:13, 5920.01it/s] 64%|██████▎   | 143182/225169 [00:23<00:14, 5752.47it/s] 64%|██████▍   | 143970/225169 [00:23<00:12, 6352.81it/s] 64%|██████▍   | 144611/225169 [00:23<00:12, 6340.46it/s] 65%|██████▍   | 145299/225169 [00:23<00:12, 6493.46it/s] 65%|██████▍   | 145964/225169 [00:24<00:12, 6536.73it/s] 65%|██████▌   | 146620/225169 [00:24<00:12, 6275.59it/s] 65%|██████▌   | 147251/225169 [00:24<00:12, 6089.99it/s] 66%|██████▌   | 147864/225169 [00:24<00:13, 5769.72it/s] 66%|██████▌   | 148488/225169 [00:24<00:12, 5898.57it/s] 66%|██████▌   | 149144/225169 [00:24<00:12, 6084.87it/s] 67%|██████▋   | 149757/225169 [00:24<00:12, 5937.14it/s] 67%|██████▋   | 150354/225169 [00:24<00:12, 5778.52it/s] 67%|██████▋   | 150935/225169 [00:24<00:12, 5730.55it/s] 67%|██████▋   | 151575/225169 [00:24<00:12, 5922.27it/s] 68%|██████▊   | 152170/225169 [00:25<00:13, 5605.45it/s] 68%|██████▊   | 152837/225169 [00:25<00:12, 5903.15it/s] 68%|██████▊   | 153550/225169 [00:25<00:11, 6255.12it/s] 68%|██████▊   | 154181/225169 [00:25<00:12, 5878.74it/s] 69%|██████▊   | 154777/225169 [00:25<00:11, 5877.29it/s] 69%|██████▉   | 155443/225169 [00:25<00:11, 6099.26it/s] 69%|██████▉   | 156091/225169 [00:25<00:11, 6206.27it/s] 70%|██████▉   | 156716/225169 [00:25<00:11, 6092.26it/s] 70%|██████▉   | 157403/225169 [00:25<00:10, 6316.17it/s] 70%|███████   | 158081/225169 [00:26<00:10, 6446.42it/s] 70%|███████   | 158742/225169 [00:26<00:10, 6493.77it/s] 71%|███████   | 159393/225169 [00:26<00:10, 6150.21it/s] 71%|███████   | 160089/225169 [00:26<00:10, 6373.99it/s] 71%|███████▏  | 160731/225169 [00:26<00:10, 6112.92it/s] 72%|███████▏  | 161380/225169 [00:26<00:10, 6215.80it/s] 72%|███████▏  | 162006/225169 [00:26<00:10, 6062.47it/s] 72%|███████▏  | 162661/225169 [00:26<00:10, 6193.09it/s] 73%|███████▎  | 163313/225169 [00:26<00:09, 6287.79it/s] 73%|███████▎  | 163945/225169 [00:27<00:10, 5936.94it/s] 73%|███████▎  | 164573/225169 [00:27<00:10, 6033.53it/s] 73%|███████▎  | 165257/225169 [00:27<00:09, 6263.44it/s] 74%|███████▎  | 165939/225169 [00:27<00:09, 6424.43it/s] 74%|███████▍  | 166592/225169 [00:27<00:09, 6446.81it/s] 74%|███████▍  | 167252/225169 [00:27<00:08, 6489.88it/s] 75%|███████▍  | 167903/225169 [00:27<00:09, 6104.91it/s] 75%|███████▍  | 168520/225169 [00:27<00:09, 6099.32it/s] 75%|███████▌  | 169162/225169 [00:27<00:09, 6187.17it/s] 75%|███████▌  | 169784/225169 [00:27<00:09, 5978.89it/s] 76%|███████▌  | 170386/225169 [00:28<00:09, 5728.02it/s] 76%|███████▌  | 170990/225169 [00:28<00:09, 5807.14it/s] 76%|███████▌  | 171639/225169 [00:28<00:08, 5992.88it/s] 76%|███████▋  | 172242/225169 [00:28<00:09, 5781.58it/s] 77%|███████▋  | 172872/225169 [00:28<00:08, 5928.99it/s] 77%|███████▋  | 173482/225169 [00:28<00:08, 5971.75it/s] 77%|███████▋  | 174084/225169 [00:28<00:08, 5975.74it/s] 78%|███████▊  | 174787/225169 [00:28<00:08, 6282.02it/s] 78%|███████▊  | 175417/225169 [00:28<00:08, 5988.08it/s] 78%|███████▊  | 176020/225169 [00:29<00:08, 5786.48it/s] 78%|███████▊  | 176603/225169 [00:29<00:08, 5790.03it/s] 79%|███████▊  | 177188/225169 [00:29<00:08, 5805.57it/s] 79%|███████▉  | 177771/225169 [00:29<00:08, 5327.04it/s] 79%|███████▉  | 178447/225169 [00:29<00:08, 5721.53it/s] 80%|███████▉  | 179080/225169 [00:29<00:07, 5888.72it/s] 80%|███████▉  | 179677/225169 [00:29<00:07, 5800.61it/s] 80%|████████  | 180263/225169 [00:29<00:07, 5808.05it/s] 80%|████████  | 180864/225169 [00:29<00:07, 5866.06it/s] 81%|████████  | 181454/225169 [00:29<00:07, 5775.94it/s] 81%|████████  | 182034/225169 [00:30<00:07, 5755.83it/s] 81%|████████  | 182627/225169 [00:30<00:07, 5800.96it/s] 81%|████████▏ | 183209/225169 [00:30<00:07, 5791.26it/s] 82%|████████▏ | 183789/225169 [00:30<00:07, 5437.47it/s] 82%|████████▏ | 184355/225169 [00:30<00:07, 5498.77it/s] 82%|████████▏ | 185006/225169 [00:30<00:06, 5781.07it/s] 82%|████████▏ | 185588/225169 [00:30<00:07, 5481.30it/s] 83%|████████▎ | 186200/225169 [00:30<00:06, 5660.45it/s] 83%|████████▎ | 186771/225169 [00:30<00:06, 5650.91it/s] 83%|████████▎ | 187404/225169 [00:31<00:06, 5840.96it/s] 83%|████████▎ | 187991/225169 [00:31<00:06, 5801.99it/s] 84%|████████▍ | 188704/225169 [00:31<00:05, 6190.32it/s] 84%|████████▍ | 189326/225169 [00:31<00:06, 5948.31it/s] 84%|████████▍ | 189984/225169 [00:31<00:05, 6122.49it/s] 85%|████████▍ | 190619/225169 [00:31<00:05, 6187.22it/s] 85%|████████▍ | 191241/225169 [00:31<00:05, 6128.48it/s] 85%|████████▌ | 191856/225169 [00:31<00:05, 6102.74it/s] 85%|████████▌ | 192482/225169 [00:31<00:05, 6145.98it/s] 86%|████████▌ | 193102/225169 [00:31<00:05, 6142.16it/s] 86%|████████▌ | 193717/225169 [00:32<00:05, 6107.20it/s] 86%|████████▋ | 194351/225169 [00:32<00:04, 6175.97it/s] 87%|████████▋ | 194969/225169 [00:32<00:04, 6093.11it/s] 87%|████████▋ | 195608/225169 [00:32<00:04, 6178.09it/s] 87%|████████▋ | 196227/225169 [00:32<00:04, 5920.10it/s] 87%|████████▋ | 196822/225169 [00:32<00:04, 5878.00it/s] 88%|████████▊ | 197789/225169 [00:32<00:03, 6977.02it/s] 88%|████████▊ | 198492/225169 [00:32<00:04, 6345.27it/s] 88%|████████▊ | 199141/225169 [00:32<00:04, 6246.94it/s] 89%|████████▊ | 199775/225169 [00:33<00:04, 6115.04it/s] 89%|████████▉ | 200393/225169 [00:33<00:04, 5994.29it/s] 89%|████████▉ | 200997/225169 [00:33<00:04, 5947.89it/s] 90%|████████▉ | 201595/225169 [00:33<00:04, 5859.07it/s] 90%|████████▉ | 202225/225169 [00:33<00:03, 5983.01it/s] 90%|█████████ | 202826/225169 [00:33<00:03, 5875.17it/s] 90%|█████████ | 203522/225169 [00:33<00:03, 6178.45it/s] 91%|█████████ | 204148/225169 [00:33<00:03, 6199.43it/s] 91%|█████████ | 204770/225169 [00:33<00:03, 5934.08it/s] 91%|█████████ | 205376/225169 [00:33<00:03, 5966.71it/s] 91%|█████████▏| 205975/225169 [00:34<00:03, 5893.94it/s] 92%|█████████▏| 206593/225169 [00:34<00:03, 5975.62it/s] 92%|█████████▏| 207192/225169 [00:34<00:03, 5809.27it/s] 92%|█████████▏| 207783/225169 [00:34<00:02, 5838.10it/s] 93%|█████████▎| 208369/225169 [00:34<00:02, 5784.60it/s] 93%|█████████▎| 208949/225169 [00:34<00:02, 5536.97it/s] 93%|█████████▎| 209542/225169 [00:34<00:02, 5642.39it/s] 93%|█████████▎| 210223/225169 [00:34<00:02, 5978.58it/s] 94%|█████████▎| 210866/225169 [00:34<00:02, 6107.55it/s] 94%|█████████▍| 211499/225169 [00:34<00:02, 6170.08it/s] 94%|█████████▍| 212118/225169 [00:35<00:02, 5958.28it/s] 95%|█████████▍| 212851/225169 [00:35<00:01, 6346.27it/s] 95%|█████████▍| 213489/225169 [00:35<00:01, 6190.46it/s] 95%|█████████▌| 214111/225169 [00:35<00:01, 5942.56it/s] 95%|█████████▌| 214709/225169 [00:35<00:01, 5769.43it/s] 96%|█████████▌| 215289/225169 [00:35<00:01, 5656.00it/s] 96%|█████████▌| 215857/225169 [00:35<00:01, 5591.09it/s] 96%|█████████▌| 216419/225169 [00:35<00:01, 5598.15it/s] 96%|█████████▋| 216999/225169 [00:35<00:01, 5646.65it/s] 97%|█████████▋| 217593/225169 [00:36<00:01, 5726.10it/s] 97%|█████████▋| 218271/225169 [00:36<00:01, 6030.44it/s] 97%|█████████▋| 218876/225169 [00:36<00:01, 5992.69it/s] 97%|█████████▋| 219512/225169 [00:36<00:00, 6100.70it/s] 98%|█████████▊| 220123/225169 [00:36<00:00, 6049.27it/s] 98%|█████████▊| 220729/225169 [00:36<00:00, 5930.92it/s] 98%|█████████▊| 221382/225169 [00:36<00:00, 6104.75it/s] 99%|█████████▊| 221994/225169 [00:36<00:00, 5819.11it/s] 99%|█████████▉| 222580/225169 [00:36<00:00, 5683.82it/s] 99%|█████████▉| 223191/225169 [00:36<00:00, 5798.49it/s] 99%|█████████▉| 223774/225169 [00:37<00:00, 5515.40it/s]100%|█████████▉| 224487/225169 [00:37<00:00, 5965.20it/s]100%|█████████▉| 225110/225169 [00:37<00:00, 6038.79it/s]100%|██████████| 225169/225169 [00:37<00:00, 6035.51it/s]

gathering stats for n=1
  0%|          | 0/225169 [00:00<?, ?it/s]  1%|          | 1893/225169 [00:00<00:11, 18914.23it/s]  2%|▏         | 3935/225169 [00:00<00:11, 19797.16it/s]  3%|▎         | 6154/225169 [00:00<00:10, 20881.37it/s]  4%|▎         | 8243/225169 [00:00<00:10, 19862.73it/s]  5%|▍         | 10261/225169 [00:00<00:10, 19967.89it/s]  5%|▌         | 12263/225169 [00:00<00:10, 19694.31it/s]  6%|▋         | 14314/225169 [00:00<00:10, 19950.56it/s]  7%|▋         | 16312/225169 [00:00<00:10, 19784.50it/s]  8%|▊         | 18293/225169 [00:00<00:10, 19671.44it/s]  9%|▉         | 20363/225169 [00:01<00:10, 19982.13it/s] 10%|▉         | 22363/225169 [00:01<00:10, 19924.14it/s] 11%|█         | 24598/225169 [00:01<00:09, 20653.79it/s] 12%|█▏        | 26665/225169 [00:01<00:09, 20409.77it/s] 13%|█▎        | 28708/225169 [00:01<00:09, 20302.26it/s] 14%|█▎        | 30740/225169 [00:01<00:09, 19742.34it/s] 15%|█▍        | 32718/225169 [00:01<00:09, 19407.62it/s] 15%|█▌        | 34705/225169 [00:01<00:09, 19541.43it/s] 16%|█▋        | 36662/225169 [00:01<00:09, 19434.25it/s] 17%|█▋        | 38608/225169 [00:01<00:09, 19057.79it/s] 18%|█▊        | 40650/225169 [00:02<00:09, 19446.90it/s] 19%|█▉        | 42598/225169 [00:02<00:09, 19103.74it/s] 20%|█▉        | 44607/225169 [00:02<00:09, 19390.48it/s] 21%|██        | 46661/225169 [00:02<00:09, 19722.88it/s] 22%|██▏       | 49061/225169 [00:02<00:08, 20988.46it/s] 23%|██▎       | 51164/225169 [00:02<00:08, 20540.02it/s] 24%|██▎       | 53318/225169 [00:02<00:08, 20832.45it/s] 25%|██▍       | 55405/225169 [00:02<00:08, 20252.99it/s] 26%|██▌       | 57436/225169 [00:02<00:08, 19831.86it/s] 26%|██▋       | 59478/225169 [00:02<00:08, 19998.28it/s] 27%|██▋       | 61594/225169 [00:03<00:08, 20332.39it/s] 28%|██▊       | 63631/225169 [00:03<00:08, 20130.64it/s] 29%|██▉       | 65647/225169 [00:03<00:07, 19960.27it/s] 30%|███       | 67645/225169 [00:03<00:07, 19937.83it/s] 31%|███       | 69701/225169 [00:03<00:07, 20121.15it/s] 32%|███▏      | 71715/225169 [00:03<00:07, 20077.02it/s] 33%|███▎      | 74008/225169 [00:03<00:07, 20921.74it/s] 34%|███▍      | 76102/225169 [00:03<00:07, 20777.61it/s] 35%|███▍      | 78181/225169 [00:03<00:07, 20662.98it/s] 36%|███▌      | 80249/225169 [00:04<00:07, 19798.17it/s] 37%|███▋      | 82291/225169 [00:04<00:07, 19973.65it/s] 37%|███▋      | 84310/225169 [00:04<00:07, 20035.84it/s] 38%|███▊      | 86318/225169 [00:04<00:06, 20031.98it/s] 39%|███▉      | 88411/225169 [00:04<00:06, 20294.10it/s] 40%|████      | 90443/225169 [00:04<00:06, 19770.58it/s] 41%|████      | 92516/225169 [00:04<00:06, 20044.33it/s] 42%|████▏     | 94525/225169 [00:04<00:06, 19872.88it/s] 43%|████▎     | 96707/225169 [00:04<00:06, 20445.07it/s] 44%|████▍     | 98755/225169 [00:04<00:06, 20071.12it/s] 45%|████▍     | 100766/225169 [00:05<00:06, 19686.54it/s] 46%|████▌     | 102804/225169 [00:05<00:06, 19885.53it/s] 47%|████▋     | 104796/225169 [00:05<00:06, 19691.32it/s] 47%|████▋     | 106768/225169 [00:05<00:06, 19669.74it/s] 48%|████▊     | 108737/225169 [00:05<00:06, 19262.19it/s] 49%|████▉     | 110790/225169 [00:05<00:05, 19629.15it/s] 50%|█████     | 112834/225169 [00:05<00:05, 19867.52it/s] 51%|█████     | 114823/225169 [00:05<00:05, 19601.33it/s] 52%|█████▏    | 116786/225169 [00:05<00:05, 19201.71it/s] 53%|█████▎    | 118770/225169 [00:05<00:05, 19385.42it/s] 54%|█████▎    | 120816/225169 [00:06<00:05, 19696.84it/s] 55%|█████▍    | 122788/225169 [00:06<00:05, 19573.09it/s] 55%|█████▌    | 124747/225169 [00:06<00:05, 19252.28it/s] 56%|█████▋    | 126923/225169 [00:06<00:04, 19980.85it/s] 57%|█████▋    | 128924/225169 [00:06<00:04, 19629.26it/s] 58%|█████▊    | 130890/225169 [00:06<00:04, 19404.99it/s] 59%|█████▉    | 132833/225169 [00:06<00:04, 19198.43it/s] 60%|█████▉    | 134989/225169 [00:06<00:04, 19884.15it/s] 61%|██████    | 137210/225169 [00:06<00:04, 20566.91it/s] 62%|██████▏   | 139270/225169 [00:06<00:04, 20279.46it/s] 63%|██████▎   | 141301/225169 [00:07<00:04, 20008.18it/s] 64%|██████▎   | 143305/225169 [00:07<00:04, 19816.72it/s] 65%|██████▍   | 145562/225169 [00:07<00:03, 20622.53it/s] 66%|██████▌   | 147628/225169 [00:07<00:03, 19885.37it/s] 66%|██████▋   | 149624/225169 [00:07<00:03, 19711.34it/s] 67%|██████▋   | 151600/225169 [00:07<00:03, 19579.93it/s] 68%|██████▊   | 153577/225169 [00:07<00:03, 19632.78it/s] 69%|██████▉   | 155543/225169 [00:07<00:03, 19580.44it/s] 70%|██████▉   | 157601/225169 [00:07<00:03, 19874.95it/s] 71%|███████   | 159591/225169 [00:08<00:03, 19678.18it/s] 72%|███████▏  | 161617/225169 [00:08<00:03, 19847.62it/s] 73%|███████▎  | 163629/225169 [00:08<00:03, 19919.12it/s] 74%|███████▎  | 165760/225169 [00:08<00:02, 20327.56it/s] 75%|███████▍  | 167794/225169 [00:08<00:02, 20276.59it/s] 75%|███████▌  | 169823/225169 [00:08<00:02, 19807.77it/s] 76%|███████▋  | 171807/225169 [00:08<00:02, 19630.79it/s] 77%|███████▋  | 173772/225169 [00:08<00:02, 19558.02it/s] 78%|███████▊  | 175730/225169 [00:08<00:02, 19317.94it/s] 79%|███████▉  | 177663/225169 [00:08<00:02, 18725.70it/s] 80%|███████▉  | 179686/225169 [00:09<00:02, 19158.23it/s] 81%|████████  | 181607/225169 [00:09<00:02, 18976.17it/s] 81%|████████▏ | 183508/225169 [00:09<00:02, 18936.34it/s] 82%|████████▏ | 185404/225169 [00:09<00:02, 18482.16it/s] 83%|████████▎ | 187335/225169 [00:09<00:02, 18720.85it/s] 84%|████████▍ | 189296/225169 [00:09<00:01, 18981.16it/s] 85%|████████▍ | 191321/225169 [00:09<00:01, 19349.31it/s] 86%|████████▌ | 193326/225169 [00:09<00:01, 19552.27it/s] 87%|████████▋ | 195310/225169 [00:09<00:01, 19634.47it/s] 88%|████████▊ | 197485/225169 [00:09<00:01, 20262.89it/s] 89%|████████▊ | 199513/225169 [00:10<00:01, 19989.58it/s] 89%|████████▉ | 201514/225169 [00:10<00:01, 19474.96it/s] 90%|█████████ | 203572/225169 [00:10<00:01, 19796.65it/s] 91%|█████████▏| 205556/225169 [00:10<00:01, 19526.53it/s] 92%|█████████▏| 207512/225169 [00:10<00:00, 19431.66it/s] 93%|█████████▎| 209457/225169 [00:10<00:00, 18937.65it/s] 94%|█████████▍| 211523/225169 [00:10<00:00, 19431.73it/s] 95%|█████████▍| 213507/225169 [00:10<00:00, 19539.78it/s] 96%|█████████▌| 215464/225169 [00:10<00:00, 19045.26it/s] 97%|█████████▋| 217373/225169 [00:11<00:00, 19057.52it/s] 97%|█████████▋| 219432/225169 [00:11<00:00, 19506.92it/s] 98%|█████████▊| 221386/225169 [00:11<00:00, 19502.41it/s] 99%|█████████▉| 223339/225169 [00:11<00:00, 18879.12it/s]100%|██████████| 225169/225169 [00:11<00:00, 19746.90it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 64.83it/s]2022-03-08 08:12:28 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-08 08:12:28 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-08 08:12:28 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-08 08:12:28 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-08 08:12:28 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-08 08:12:28 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-08 08:12:28 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-08 08:12:28 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-08 08:12:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-08 08:12:28 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-08 08:12:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-08 08:12:28 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-08 08:12:28 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-08 08:12:28 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 08:12:28 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 08:12:28 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-08 08:12:28 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-08 08:12:28 | INFO | fairseq.trainer | begin training epoch 1
2022-03-08 08:12:28 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-08 08:12:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-08 08:12:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 08:12:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 08:12:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 08:18:09 | INFO | train_inner | epoch 001:    104 / 196 loss=16.351, ppl=83605, wps=20332.8, ups=0.31, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.575, loss_scale=8, train_wall=336, gb_free=14.1, wall=341
2022-03-08 08:23:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 08:23:10 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.165 | ppl 9187.51 | wps 46789.2 | wpb 2034.1 | bsz 4 | num_updates 192
2022-03-08 08:23:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 192 updates
2022-03-08 08:23:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 08:23:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 08:23:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 1 @ 192 updates, score 13.165) (writing took 6.206822537817061 seconds)
2022-03-08 08:23:16 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-08 08:23:16 | INFO | train | epoch 001 | loss 15.287 | ppl 39990.4 | wps 19972.3 | ups 0.31 | wpb 65446.1 | bsz 127.8 | num_updates 192 | lr 2.40952e-05 | gnorm 2.599 | loss_scale 8 | train_wall 627 | gb_free 14.1 | wall 648
2022-03-08 08:23:16 | INFO | fairseq.trainer | begin training epoch 2
2022-03-08 08:23:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 08:23:42 | INFO | train_inner | epoch 002:      8 / 196 loss=14.049, ppl=16945, wps=19651.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.519, loss_scale=8, train_wall=317, gb_free=14.1, wall=674
2022-03-08 08:29:04 | INFO | train_inner | epoch 002:    108 / 196 loss=12.126, ppl=4470.04, wps=20337, ups=0.31, wpb=65527.3, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.987, loss_scale=8, train_wall=317, gb_free=14.1, wall=996
2022-03-08 08:33:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 08:33:52 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.619 | ppl 1572.39 | wps 46625.1 | wpb 2034.1 | bsz 4 | num_updates 388 | best_loss 10.619
2022-03-08 08:33:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 388 updates
2022-03-08 08:33:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 08:33:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 08:33:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 2 @ 388 updates, score 10.619) (writing took 6.25768231600523 seconds)
2022-03-08 08:33:58 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-08 08:33:58 | INFO | train | epoch 002 | loss 11.622 | ppl 3151.26 | wps 19986.3 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 388 | lr 4.85903e-05 | gnorm 0.822 | loss_scale 8 | train_wall 621 | gb_free 14.1 | wall 1290
2022-03-08 08:33:58 | INFO | fairseq.trainer | begin training epoch 3
2022-03-08 08:33:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 08:34:37 | INFO | train_inner | epoch 003:     12 / 196 loss=10.87, ppl=1871.78, wps=19658.1, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.57, loss_scale=8, train_wall=317, gb_free=14.1, wall=1329
2022-03-08 08:39:59 | INFO | train_inner | epoch 003:    112 / 196 loss=10.396, ppl=1347.88, wps=20342, ups=0.31, wpb=65532.4, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.436, loss_scale=8, train_wall=317, gb_free=14.1, wall=1651
2022-03-08 08:44:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 08:44:34 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.024 | ppl 1041.04 | wps 46370.7 | wpb 2034.1 | bsz 4 | num_updates 584 | best_loss 10.024
2022-03-08 08:44:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 584 updates
2022-03-08 08:44:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 08:44:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 08:44:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 3 @ 584 updates, score 10.024) (writing took 6.212040982209146 seconds)
2022-03-08 08:44:40 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-08 08:44:40 | INFO | train | epoch 003 | loss 10.302 | ppl 1262.09 | wps 19988.6 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 584 | lr 7.30854e-05 | gnorm 0.45 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 1932
2022-03-08 08:44:40 | INFO | fairseq.trainer | begin training epoch 4
2022-03-08 08:44:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 08:45:31 | INFO | train_inner | epoch 004:     16 / 196 loss=10.123, ppl=1115.04, wps=19663.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.48, loss_scale=16, train_wall=316, gb_free=14.1, wall=1983
2022-03-08 08:50:53 | INFO | train_inner | epoch 004:    116 / 196 loss=9.878, ppl=941, wps=20343.7, ups=0.31, wpb=65530.9, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.55, loss_scale=16, train_wall=317, gb_free=14.1, wall=2305
2022-03-08 08:55:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 08:55:15 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.607 | ppl 779.93 | wps 46574 | wpb 2034.1 | bsz 4 | num_updates 780 | best_loss 9.607
2022-03-08 08:55:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 780 updates
2022-03-08 08:55:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 08:55:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 08:55:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 4 @ 780 updates, score 9.607) (writing took 6.249458042904735 seconds)
2022-03-08 08:55:21 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-08 08:55:21 | INFO | train | epoch 004 | loss 9.811 | ppl 898.18 | wps 19992.3 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 780 | lr 9.75805e-05 | gnorm 0.576 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 2573
2022-03-08 08:55:21 | INFO | fairseq.trainer | begin training epoch 5
2022-03-08 08:55:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 08:56:26 | INFO | train_inner | epoch 005:     20 / 196 loss=9.664, ppl=811.45, wps=19661.5, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.619, loss_scale=16, train_wall=316, gb_free=14.1, wall=2638
2022-03-08 09:01:48 | INFO | train_inner | epoch 005:    120 / 196 loss=9.467, ppl=707.94, wps=20345, ups=0.31, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.703, loss_scale=16, train_wall=317, gb_free=14.1, wall=2960
2022-03-08 09:05:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:05:57 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.264 | ppl 614.97 | wps 46611.7 | wpb 2034.1 | bsz 4 | num_updates 976 | best_loss 9.264
2022-03-08 09:05:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 976 updates
2022-03-08 09:05:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 09:06:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 09:06:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 5 @ 976 updates, score 9.264) (writing took 6.2263881508260965 seconds)
2022-03-08 09:06:03 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-08 09:06:03 | INFO | train | epoch 005 | loss 9.421 | ppl 685.68 | wps 19984.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 976 | lr 0.000122076 | gnorm 0.716 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 3215
2022-03-08 09:06:03 | INFO | fairseq.trainer | begin training epoch 6
2022-03-08 09:06:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:07:21 | INFO | train_inner | epoch 006:     24 / 196 loss=9.293, ppl=627.16, wps=19651.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.759, loss_scale=16, train_wall=317, gb_free=14.1, wall=3293
2022-03-08 09:12:43 | INFO | train_inner | epoch 006:    124 / 196 loss=9.118, ppl=555.77, wps=20330.8, ups=0.31, wpb=65530.9, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.785, loss_scale=32, train_wall=317, gb_free=14.1, wall=3615
2022-03-08 09:16:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:16:39 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.978 | ppl 504.17 | wps 46372.2 | wpb 2034.1 | bsz 4 | num_updates 1172 | best_loss 8.978
2022-03-08 09:16:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1172 updates
2022-03-08 09:16:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 09:16:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 09:16:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 6 @ 1172 updates, score 8.978) (writing took 6.2624279242008924 seconds)
2022-03-08 09:16:45 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-08 09:16:45 | INFO | train | epoch 006 | loss 9.09 | ppl 544.89 | wps 19976.2 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 1172 | lr 0.000146571 | gnorm 0.791 | loss_scale 32 | train_wall 621 | gb_free 14.1 | wall 3857
2022-03-08 09:16:45 | INFO | fairseq.trainer | begin training epoch 7
2022-03-08 09:16:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:18:16 | INFO | train_inner | epoch 007:     28 / 196 loss=8.983, ppl=506.14, wps=19645.7, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.82, loss_scale=32, train_wall=317, gb_free=14.1, wall=3948
2022-03-08 09:23:38 | INFO | train_inner | epoch 007:    128 / 196 loss=8.832, ppl=455.61, wps=20337.6, ups=0.31, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.794, loss_scale=32, train_wall=317, gb_free=14.1, wall=4270
2022-03-08 09:27:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:27:21 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.756 | ppl 432.24 | wps 46510.8 | wpb 2034.1 | bsz 4 | num_updates 1368 | best_loss 8.756
2022-03-08 09:27:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1368 updates
2022-03-08 09:27:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 09:27:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 09:27:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 7 @ 1368 updates, score 8.756) (writing took 6.24462363589555 seconds)
2022-03-08 09:27:27 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-08 09:27:27 | INFO | train | epoch 007 | loss 8.811 | ppl 449.11 | wps 19985.7 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 1368 | lr 0.000171066 | gnorm 0.796 | loss_scale 32 | train_wall 621 | gb_free 14.1 | wall 4499
2022-03-08 09:27:27 | INFO | fairseq.trainer | begin training epoch 8
2022-03-08 09:27:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:29:11 | INFO | train_inner | epoch 008:     32 / 196 loss=8.708, ppl=418.14, wps=19656.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.801, loss_scale=32, train_wall=317, gb_free=14.1, wall=4602
2022-03-08 09:34:33 | INFO | train_inner | epoch 008:    132 / 196 loss=8.568, ppl=379.57, wps=20330, ups=0.31, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.816, loss_scale=32, train_wall=317, gb_free=14.1, wall=4925
2022-03-08 09:37:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 09:37:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:38:03 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.54 | ppl 372.34 | wps 46563.7 | wpb 2034.1 | bsz 4 | num_updates 1563 | best_loss 8.54
2022-03-08 09:38:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1563 updates
2022-03-08 09:38:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 09:38:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 09:38:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 8 @ 1563 updates, score 8.54) (writing took 6.286592748947442 seconds)
2022-03-08 09:38:09 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-08 09:38:09 | INFO | train | epoch 008 | loss 8.559 | ppl 377.12 | wps 19878.8 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 1563 | lr 0.000195436 | gnorm 0.821 | loss_scale 32 | train_wall 621 | gb_free 14.1 | wall 5141
2022-03-08 09:38:09 | INFO | fairseq.trainer | begin training epoch 9
2022-03-08 09:38:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:40:09 | INFO | train_inner | epoch 009:     37 / 196 loss=8.46, ppl=352.05, wps=19472.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.834, loss_scale=32, train_wall=320, gb_free=14.1, wall=5260
2022-03-08 09:45:31 | INFO | train_inner | epoch 009:    137 / 196 loss=8.334, ppl=322.68, wps=20335.3, ups=0.31, wpb=65530.9, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.815, loss_scale=32, train_wall=317, gb_free=14.1, wall=5583
2022-03-08 09:48:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:48:45 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.353 | ppl 327.01 | wps 46262.5 | wpb 2034.1 | bsz 4 | num_updates 1759 | best_loss 8.353
2022-03-08 09:48:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1759 updates
2022-03-08 09:48:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 09:48:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 09:48:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 9 @ 1759 updates, score 8.353) (writing took 6.268500740639865 seconds)
2022-03-08 09:48:51 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-08 09:48:51 | INFO | train | epoch 009 | loss 8.326 | ppl 320.8 | wps 19983.7 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 1759 | lr 0.000219931 | gnorm 0.82 | loss_scale 32 | train_wall 621 | gb_free 14.1 | wall 5783
2022-03-08 09:48:51 | INFO | fairseq.trainer | begin training epoch 10
2022-03-08 09:48:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 09:51:03 | INFO | train_inner | epoch 010:     41 / 196 loss=8.225, ppl=299.16, wps=19651.7, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.816, loss_scale=32, train_wall=317, gb_free=14.1, wall=5915
2022-03-08 09:56:26 | INFO | train_inner | epoch 010:    141 / 196 loss=8.111, ppl=276.54, wps=20329.3, ups=0.31, wpb=65527.3, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.811, loss_scale=32, train_wall=317, gb_free=14.1, wall=6238
2022-03-08 09:59:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 09:59:27 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.198 | ppl 293.67 | wps 46389.8 | wpb 2034.1 | bsz 4 | num_updates 1955 | best_loss 8.198
2022-03-08 09:59:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1955 updates
2022-03-08 09:59:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 09:59:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 09:59:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 10 @ 1955 updates, score 8.198) (writing took 6.264034230262041 seconds)
2022-03-08 09:59:33 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-08 09:59:33 | INFO | train | epoch 010 | loss 8.106 | ppl 275.59 | wps 19981.3 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 1955 | lr 0.000244426 | gnorm 0.805 | loss_scale 32 | train_wall 621 | gb_free 14.1 | wall 6425
2022-03-08 09:59:33 | INFO | fairseq.trainer | begin training epoch 11
2022-03-08 09:59:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:01:58 | INFO | train_inner | epoch 011:     45 / 196 loss=8, ppl=255.95, wps=19656.2, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.793, loss_scale=32, train_wall=317, gb_free=14.1, wall=6570
2022-03-08 10:07:21 | INFO | train_inner | epoch 011:    145 / 196 loss=7.906, ppl=239.83, wps=20324.3, ups=0.31, wpb=65530.9, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.785, loss_scale=64, train_wall=317, gb_free=14.1, wall=6893
2022-03-08 10:07:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-08 10:10:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:10:09 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.087 | ppl 271.82 | wps 46363.1 | wpb 2034.1 | bsz 4 | num_updates 2150 | best_loss 8.087
2022-03-08 10:10:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2150 updates
2022-03-08 10:10:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 10:10:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 10:10:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 11 @ 2150 updates, score 8.087) (writing took 6.263947978615761 seconds)
2022-03-08 10:10:15 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-08 10:10:15 | INFO | train | epoch 011 | loss 7.902 | ppl 239.21 | wps 19874.7 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2150 | lr 0.000268796 | gnorm 0.792 | loss_scale 32 | train_wall 621 | gb_free 14.1 | wall 7067
2022-03-08 10:10:15 | INFO | fairseq.trainer | begin training epoch 12
2022-03-08 10:10:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:12:57 | INFO | train_inner | epoch 012:     50 / 196 loss=7.809, ppl=224.2, wps=19463.5, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.776, loss_scale=32, train_wall=320, gb_free=14.1, wall=7229
2022-03-08 10:18:19 | INFO | train_inner | epoch 012:    150 / 196 loss=7.703, ppl=208.41, wps=20330.3, ups=0.31, wpb=65530.9, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.764, loss_scale=32, train_wall=317, gb_free=14.1, wall=7551
2022-03-08 10:20:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:20:51 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.968 | ppl 250.31 | wps 46585.1 | wpb 2034.1 | bsz 4 | num_updates 2346 | best_loss 7.968
2022-03-08 10:20:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2346 updates
2022-03-08 10:20:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 10:20:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 10:20:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 12 @ 2346 updates, score 7.968) (writing took 6.270766944624484 seconds)
2022-03-08 10:20:57 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-08 10:20:57 | INFO | train | epoch 012 | loss 7.71 | ppl 209.43 | wps 19977.1 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 2346 | lr 0.000293291 | gnorm 0.767 | loss_scale 32 | train_wall 621 | gb_free 14.1 | wall 7709
2022-03-08 10:20:57 | INFO | fairseq.trainer | begin training epoch 13
2022-03-08 10:20:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:23:52 | INFO | train_inner | epoch 013:     54 / 196 loss=7.61, ppl=195.41, wps=19651.3, ups=0.3, wpb=65365, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.773, loss_scale=32, train_wall=317, gb_free=14.1, wall=7883
2022-03-08 10:29:14 | INFO | train_inner | epoch 013:    154 / 196 loss=7.538, ppl=185.9, wps=20332.4, ups=0.31, wpb=65530.9, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.757, loss_scale=32, train_wall=317, gb_free=14.1, wall=8206
2022-03-08 10:31:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 10:31:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:31:33 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.881 | ppl 235.79 | wps 46636 | wpb 2034.1 | bsz 4 | num_updates 2541 | best_loss 7.881
2022-03-08 10:31:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2541 updates
2022-03-08 10:31:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 10:31:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 10:31:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 13 @ 2541 updates, score 7.881) (writing took 6.247462379746139 seconds)
2022-03-08 10:31:39 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-08 10:31:39 | INFO | train | epoch 013 | loss 7.534 | ppl 185.35 | wps 19881.8 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 2541 | lr 0.000317661 | gnorm 0.773 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 8351
2022-03-08 10:31:39 | INFO | fairseq.trainer | begin training epoch 14
2022-03-08 10:31:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:34:50 | INFO | train_inner | epoch 014:     59 / 196 loss=7.431, ppl=172.62, wps=19474.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.778, loss_scale=16, train_wall=320, gb_free=14.1, wall=8541
2022-03-08 10:40:12 | INFO | train_inner | epoch 014:    159 / 196 loss=7.374, ppl=165.86, wps=20325.7, ups=0.31, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.756, loss_scale=16, train_wall=317, gb_free=14.1, wall=8864
2022-03-08 10:42:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:42:15 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.816 | ppl 225.41 | wps 46496 | wpb 2034.1 | bsz 4 | num_updates 2737 | best_loss 7.816
2022-03-08 10:42:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2737 updates
2022-03-08 10:42:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 10:42:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 10:42:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 14 @ 2737 updates, score 7.816) (writing took 6.25497327838093 seconds)
2022-03-08 10:42:21 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-08 10:42:21 | INFO | train | epoch 014 | loss 7.372 | ppl 165.68 | wps 19978.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 2737 | lr 0.000342157 | gnorm 0.748 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 8993
2022-03-08 10:42:21 | INFO | fairseq.trainer | begin training epoch 15
2022-03-08 10:42:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:45:45 | INFO | train_inner | epoch 015:     63 / 196 loss=7.27, ppl=154.38, wps=19650.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.749, loss_scale=16, train_wall=317, gb_free=14.1, wall=9197
2022-03-08 10:51:07 | INFO | train_inner | epoch 015:    163 / 196 loss=7.226, ppl=149.74, wps=20332.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.759, loss_scale=16, train_wall=317, gb_free=14.1, wall=9519
2022-03-08 10:52:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 10:52:57 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.78 | ppl 219.74 | wps 46495 | wpb 2034.1 | bsz 4 | num_updates 2933 | best_loss 7.78
2022-03-08 10:52:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2933 updates
2022-03-08 10:52:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 10:53:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 10:53:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 15 @ 2933 updates, score 7.78) (writing took 6.27393822837621 seconds)
2022-03-08 10:53:04 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-08 10:53:04 | INFO | train | epoch 015 | loss 7.224 | ppl 149.46 | wps 19977.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 2933 | lr 0.000366652 | gnorm 0.75 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 9635
2022-03-08 10:53:04 | INFO | fairseq.trainer | begin training epoch 16
2022-03-08 10:53:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 10:56:40 | INFO | train_inner | epoch 016:     67 / 196 loss=7.122, ppl=139.29, wps=19654.8, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.729, loss_scale=16, train_wall=317, gb_free=14.1, wall=9851
2022-03-08 11:02:02 | INFO | train_inner | epoch 016:    167 / 196 loss=7.093, ppl=136.49, wps=20331.7, ups=0.31, wpb=65530.9, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.735, loss_scale=32, train_wall=317, gb_free=14.1, wall=10174
2022-03-08 11:02:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 11:03:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:03:39 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.739 | ppl 213.57 | wps 46426 | wpb 2034.1 | bsz 4 | num_updates 3128 | best_loss 7.739
2022-03-08 11:03:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3128 updates
2022-03-08 11:03:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 11:03:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 11:03:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 16 @ 3128 updates, score 7.739) (writing took 6.284620217047632 seconds)
2022-03-08 11:03:45 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-08 11:03:45 | INFO | train | epoch 016 | loss 7.087 | ppl 135.95 | wps 19882.1 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 3128 | lr 0.000391022 | gnorm 0.767 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 10277
2022-03-08 11:03:45 | INFO | fairseq.trainer | begin training epoch 17
2022-03-08 11:03:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:07:38 | INFO | train_inner | epoch 017:     72 / 196 loss=6.983, ppl=126.48, wps=19468.1, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.745, loss_scale=16, train_wall=320, gb_free=14.1, wall=10510
2022-03-08 11:13:00 | INFO | train_inner | epoch 017:    172 / 196 loss=6.964, ppl=124.89, wps=20335.8, ups=0.31, wpb=65527.3, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.728, loss_scale=16, train_wall=317, gb_free=14.1, wall=10832
2022-03-08 11:14:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:14:21 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.729 | ppl 212.18 | wps 46529.5 | wpb 2034.1 | bsz 4 | num_updates 3324 | best_loss 7.729
2022-03-08 11:14:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3324 updates
2022-03-08 11:14:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 11:14:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 11:14:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 17 @ 3324 updates, score 7.729) (writing took 6.3132256250828505 seconds)
2022-03-08 11:14:27 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-08 11:14:27 | INFO | train | epoch 017 | loss 6.956 | ppl 124.13 | wps 19980.6 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 3324 | lr 0.000415517 | gnorm 0.71 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 10919
2022-03-08 11:14:27 | INFO | fairseq.trainer | begin training epoch 18
2022-03-08 11:14:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:18:33 | INFO | train_inner | epoch 018:     76 / 196 loss=6.858, ppl=116.03, wps=19646.4, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.729, loss_scale=16, train_wall=317, gb_free=14.1, wall=11164
2022-03-08 11:23:55 | INFO | train_inner | epoch 018:    176 / 196 loss=6.851, ppl=115.41, wps=20329.3, ups=0.31, wpb=65530.9, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.741, loss_scale=16, train_wall=317, gb_free=14.1, wall=11487
2022-03-08 11:24:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:25:03 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.739 | ppl 213.67 | wps 46590 | wpb 2034.1 | bsz 4 | num_updates 3520 | best_loss 7.729
2022-03-08 11:25:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3520 updates
2022-03-08 11:25:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 11:25:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 11:25:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 18 @ 3520 updates, score 7.739) (writing took 2.8395983185619116 seconds)
2022-03-08 11:25:06 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-08 11:25:06 | INFO | train | epoch 018 | loss 6.838 | ppl 114.43 | wps 20084 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 3520 | lr 0.000440012 | gnorm 0.729 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 11558
2022-03-08 11:25:06 | INFO | fairseq.trainer | begin training epoch 19
2022-03-08 11:25:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:29:24 | INFO | train_inner | epoch 019:     80 / 196 loss=6.726, ppl=105.87, wps=19859.1, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.716, loss_scale=16, train_wall=317, gb_free=14.1, wall=11816
2022-03-08 11:31:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 11:34:50 | INFO | train_inner | epoch 019:    181 / 196 loss=6.739, ppl=106.85, wps=20135.6, ups=0.31, wpb=65530.9, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.754, loss_scale=16, train_wall=320, gb_free=14.1, wall=12141
2022-03-08 11:35:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:35:42 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.723 | ppl 211.27 | wps 46589.3 | wpb 2034.1 | bsz 4 | num_updates 3715 | best_loss 7.723
2022-03-08 11:35:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3715 updates
2022-03-08 11:35:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 11:35:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt
2022-03-08 11:35:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_best.pt (epoch 19 @ 3715 updates, score 7.723) (writing took 6.264114549383521 seconds)
2022-03-08 11:35:48 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-08 11:35:48 | INFO | train | epoch 019 | loss 6.723 | ppl 105.67 | wps 19881.9 | ups 0.3 | wpb 65447.5 | bsz 127.8 | num_updates 3715 | lr 0.000464382 | gnorm 0.743 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 12200
2022-03-08 11:35:48 | INFO | fairseq.trainer | begin training epoch 20
2022-03-08 11:35:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:40:22 | INFO | train_inner | epoch 020:     85 / 196 loss=6.614, ppl=97.95, wps=19657, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=3800, lr=0.000475005, gnorm=0.711, loss_scale=16, train_wall=317, gb_free=14.1, wall=12474
2022-03-08 11:45:44 | INFO | train_inner | epoch 020:    185 / 196 loss=6.638, ppl=99.63, wps=20337, ups=0.31, wpb=65532.4, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.738, loss_scale=16, train_wall=317, gb_free=14.1, wall=12796
2022-03-08 11:46:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:46:24 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.748 | ppl 214.93 | wps 46492.9 | wpb 2034.1 | bsz 4 | num_updates 3911 | best_loss 7.723
2022-03-08 11:46:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3911 updates
2022-03-08 11:46:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 11:46:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 11:46:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 20 @ 3911 updates, score 7.748) (writing took 2.922511507757008 seconds)
2022-03-08 11:46:27 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-08 11:46:27 | INFO | train | epoch 020 | loss 6.617 | ppl 98.12 | wps 20089.6 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 3911 | lr 0.000488877 | gnorm 0.718 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 12839
2022-03-08 11:46:27 | INFO | fairseq.trainer | begin training epoch 21
2022-03-08 11:46:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 11:51:13 | INFO | train_inner | epoch 021:     89 / 196 loss=6.5, ppl=90.48, wps=19859, ups=0.3, wpb=65361.9, bsz=127.7, num_updates=4000, lr=0.0005, gnorm=0.738, loss_scale=16, train_wall=316, gb_free=14.1, wall=13125
2022-03-08 11:56:36 | INFO | train_inner | epoch 021:    189 / 196 loss=6.536, ppl=92.81, wps=20332.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.701, loss_scale=16, train_wall=317, gb_free=14.1, wall=13448
2022-03-08 11:56:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 11:57:02 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.732 | ppl 212.58 | wps 46431.9 | wpb 2034.1 | bsz 4 | num_updates 4107 | best_loss 7.723
2022-03-08 11:57:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4107 updates
2022-03-08 11:57:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 11:57:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 11:57:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 21 @ 4107 updates, score 7.732) (writing took 2.9164945650845766 seconds)
2022-03-08 11:57:05 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-08 11:57:05 | INFO | train | epoch 021 | loss 6.513 | ppl 91.31 | wps 20089.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 4107 | lr 0.000493444 | gnorm 0.722 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 13477
2022-03-08 11:57:05 | INFO | fairseq.trainer | begin training epoch 22
2022-03-08 11:57:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:01:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 12:02:08 | INFO | train_inner | epoch 022:     94 / 196 loss=6.384, ppl=83.51, wps=19668.6, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=4200, lr=0.00048795, gnorm=0.701, loss_scale=16, train_wall=320, gb_free=14.1, wall=13780
2022-03-08 12:07:30 | INFO | train_inner | epoch 022:    194 / 196 loss=6.427, ppl=86.06, wps=20329.3, ups=0.31, wpb=65530.9, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.697, loss_scale=16, train_wall=317, gb_free=14.1, wall=14102
2022-03-08 12:07:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:07:41 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.751 | ppl 215.47 | wps 46283.8 | wpb 2034.1 | bsz 4 | num_updates 4302 | best_loss 7.723
2022-03-08 12:07:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4302 updates
2022-03-08 12:07:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 12:07:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 12:07:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 22 @ 4302 updates, score 7.751) (writing took 2.8909313045442104 seconds)
2022-03-08 12:07:44 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-08 12:07:44 | INFO | train | epoch 022 | loss 6.401 | ppl 84.51 | wps 19985.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4302 | lr 0.000482131 | gnorm 0.7 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 14116
2022-03-08 12:07:44 | INFO | fairseq.trainer | begin training epoch 23
2022-03-08 12:07:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:13:00 | INFO | train_inner | epoch 023:     98 / 196 loss=6.272, ppl=77.27, wps=19859.9, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=4400, lr=0.000476731, gnorm=0.705, loss_scale=16, train_wall=316, gb_free=14.1, wall=14432
2022-03-08 12:18:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:18:19 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.775 | ppl 219.11 | wps 46429.7 | wpb 2034.1 | bsz 4 | num_updates 4498 | best_loss 7.723
2022-03-08 12:18:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4498 updates
2022-03-08 12:18:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 12:18:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 12:18:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 23 @ 4498 updates, score 7.775) (writing took 2.912649313919246 seconds)
2022-03-08 12:18:22 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-08 12:18:22 | INFO | train | epoch 023 | loss 6.298 | ppl 78.66 | wps 20086.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 4498 | lr 0.000471509 | gnorm 0.688 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 14754
2022-03-08 12:18:22 | INFO | fairseq.trainer | begin training epoch 24
2022-03-08 12:18:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:18:29 | INFO | train_inner | epoch 024:      2 / 196 loss=6.324, ppl=80.1, wps=19851.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.675, loss_scale=16, train_wall=317, gb_free=14.1, wall=14761
2022-03-08 12:23:51 | INFO | train_inner | epoch 024:    102 / 196 loss=6.172, ppl=72.11, wps=20329.9, ups=0.31, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.691, loss_scale=16, train_wall=317, gb_free=14.1, wall=15083
2022-03-08 12:28:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:28:58 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.764 | ppl 217.38 | wps 46566.7 | wpb 2034.1 | bsz 4 | num_updates 4694 | best_loss 7.723
2022-03-08 12:28:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4694 updates
2022-03-08 12:28:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 12:29:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 12:29:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 24 @ 4694 updates, score 7.764) (writing took 2.8973045209422708 seconds)
2022-03-08 12:29:01 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-08 12:29:01 | INFO | train | epoch 024 | loss 6.202 | ppl 73.63 | wps 20078.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 4694 | lr 0.00046156 | gnorm 0.685 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 15393
2022-03-08 12:29:01 | INFO | fairseq.trainer | begin training epoch 25
2022-03-08 12:29:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:29:21 | INFO | train_inner | epoch 025:      6 / 196 loss=6.225, ppl=74.83, wps=19842.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.683, loss_scale=32, train_wall=317, gb_free=14.1, wall=15413
2022-03-08 12:29:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 12:34:46 | INFO | train_inner | epoch 025:    107 / 196 loss=6.082, ppl=67.77, wps=20137.3, ups=0.31, wpb=65527.3, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.684, loss_scale=16, train_wall=320, gb_free=14.1, wall=15738
2022-03-08 12:36:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 12:39:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:39:37 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.79 | ppl 221.35 | wps 46307.3 | wpb 2034.1 | bsz 4 | num_updates 4888 | best_loss 7.723
2022-03-08 12:39:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4888 updates
2022-03-08 12:39:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 12:39:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 12:39:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 25 @ 4888 updates, score 7.79) (writing took 2.8886274257674813 seconds)
2022-03-08 12:39:40 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-08 12:39:40 | INFO | train | epoch 025 | loss 6.11 | ppl 69.09 | wps 19887.1 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 4888 | lr 0.000452308 | gnorm 0.676 | loss_scale 8 | train_wall 621 | gb_free 14.1 | wall 16032
2022-03-08 12:39:40 | INFO | fairseq.trainer | begin training epoch 26
2022-03-08 12:39:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:40:18 | INFO | train_inner | epoch 026:     12 / 196 loss=6.125, ppl=69.78, wps=19666.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.669, loss_scale=8, train_wall=320, gb_free=14.1, wall=16070
2022-03-08 12:45:41 | INFO | train_inner | epoch 026:    112 / 196 loss=5.996, ppl=63.83, wps=20337.7, ups=0.31, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.687, loss_scale=8, train_wall=317, gb_free=14.1, wall=16393
2022-03-08 12:50:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 12:50:15 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.818 | ppl 225.63 | wps 46465.8 | wpb 2034.1 | bsz 4 | num_updates 5084 | best_loss 7.723
2022-03-08 12:50:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5084 updates
2022-03-08 12:50:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 12:50:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 12:50:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 26 @ 5084 updates, score 7.818) (writing took 2.8993695452809334 seconds)
2022-03-08 12:50:18 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-08 12:50:18 | INFO | train | epoch 026 | loss 6.027 | ppl 65.21 | wps 20090.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 5084 | lr 0.000443504 | gnorm 0.686 | loss_scale 8 | train_wall 621 | gb_free 14.1 | wall 16670
2022-03-08 12:50:18 | INFO | fairseq.trainer | begin training epoch 27
2022-03-08 12:50:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 12:51:10 | INFO | train_inner | epoch 027:     16 / 196 loss=6.039, ppl=65.75, wps=19862.8, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.686, loss_scale=8, train_wall=316, gb_free=14.1, wall=16722
2022-03-08 12:56:32 | INFO | train_inner | epoch 027:    116 / 196 loss=5.923, ppl=60.68, wps=20334.5, ups=0.31, wpb=65527.3, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.686, loss_scale=8, train_wall=317, gb_free=14.1, wall=17044
2022-03-08 13:00:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:00:54 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.829 | ppl 227.38 | wps 46420.2 | wpb 2034.1 | bsz 4 | num_updates 5280 | best_loss 7.723
2022-03-08 13:00:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5280 updates
2022-03-08 13:00:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 13:00:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 13:00:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 27 @ 5280 updates, score 7.829) (writing took 2.897108092904091 seconds)
2022-03-08 13:00:57 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-08 13:00:57 | INFO | train | epoch 027 | loss 5.947 | ppl 61.68 | wps 20090.9 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 5280 | lr 0.000435194 | gnorm 0.682 | loss_scale 8 | train_wall 621 | gb_free 14.1 | wall 17309
2022-03-08 13:00:57 | INFO | fairseq.trainer | begin training epoch 28
2022-03-08 13:00:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:02:01 | INFO | train_inner | epoch 028:     20 / 196 loss=5.951, ppl=61.86, wps=19861.5, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.667, loss_scale=8, train_wall=317, gb_free=14.1, wall=17373
2022-03-08 13:07:24 | INFO | train_inner | epoch 028:    120 / 196 loss=5.853, ppl=57.82, wps=20329, ups=0.31, wpb=65527.3, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.682, loss_scale=16, train_wall=317, gb_free=14.1, wall=17695
2022-03-08 13:11:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:11:32 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.833 | ppl 228.01 | wps 46223 | wpb 2034.1 | bsz 4 | num_updates 5476 | best_loss 7.723
2022-03-08 13:11:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5476 updates
2022-03-08 13:11:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 13:11:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 13:11:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 28 @ 5476 updates, score 7.833) (writing took 2.914753112010658 seconds)
2022-03-08 13:11:35 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-08 13:11:35 | INFO | train | epoch 028 | loss 5.871 | ppl 58.55 | wps 20083.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 5476 | lr 0.000427335 | gnorm 0.69 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 17947
2022-03-08 13:11:35 | INFO | fairseq.trainer | begin training epoch 29
2022-03-08 13:11:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:12:53 | INFO | train_inner | epoch 029:     24 / 196 loss=5.876, ppl=58.73, wps=19849.4, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.715, loss_scale=16, train_wall=317, gb_free=14.1, wall=18025
2022-03-08 13:18:15 | INFO | train_inner | epoch 029:    124 / 196 loss=5.78, ppl=54.95, wps=20329.5, ups=0.31, wpb=65527.3, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.67, loss_scale=16, train_wall=317, gb_free=14.1, wall=18347
2022-03-08 13:22:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:22:11 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.892 | ppl 237.57 | wps 46147.4 | wpb 2034.1 | bsz 4 | num_updates 5672 | best_loss 7.723
2022-03-08 13:22:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5672 updates
2022-03-08 13:22:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 13:22:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 13:22:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 29 @ 5672 updates, score 7.892) (writing took 2.89091928768903 seconds)
2022-03-08 13:22:14 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-08 13:22:14 | INFO | train | epoch 029 | loss 5.799 | ppl 55.69 | wps 20082.7 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 5672 | lr 0.000419886 | gnorm 0.683 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 18586
2022-03-08 13:22:14 | INFO | fairseq.trainer | begin training epoch 30
2022-03-08 13:22:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:23:44 | INFO | train_inner | epoch 030:     28 / 196 loss=5.793, ppl=55.45, wps=19852.3, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.695, loss_scale=16, train_wall=317, gb_free=14.1, wall=18676
2022-03-08 13:28:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 13:29:10 | INFO | train_inner | epoch 030:    129 / 196 loss=5.719, ppl=52.69, wps=20130.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.69, loss_scale=8, train_wall=321, gb_free=14.1, wall=19002
2022-03-08 13:32:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:32:50 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.915 | ppl 241.37 | wps 46141.4 | wpb 2034.1 | bsz 4 | num_updates 5867 | best_loss 7.723
2022-03-08 13:32:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5867 updates
2022-03-08 13:32:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 13:32:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 13:32:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 30 @ 5867 updates, score 7.915) (writing took 2.8702971199527383 seconds)
2022-03-08 13:32:53 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-08 13:32:53 | INFO | train | epoch 030 | loss 5.732 | ppl 53.15 | wps 19987.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5867 | lr 0.00041285 | gnorm 0.686 | loss_scale 8 | train_wall 621 | gb_free 14.1 | wall 19225
2022-03-08 13:32:53 | INFO | fairseq.trainer | begin training epoch 31
2022-03-08 13:32:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:34:39 | INFO | train_inner | epoch 031:     33 / 196 loss=5.716, ppl=52.55, wps=19864.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.683, loss_scale=8, train_wall=316, gb_free=14.1, wall=19331
2022-03-08 13:40:01 | INFO | train_inner | epoch 031:    133 / 196 loss=5.66, ppl=50.56, wps=20335, ups=0.31, wpb=65532.4, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.677, loss_scale=8, train_wall=317, gb_free=14.1, wall=19653
2022-03-08 13:43:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:43:28 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.922 | ppl 242.47 | wps 45961.2 | wpb 2034.1 | bsz 4 | num_updates 6063 | best_loss 7.723
2022-03-08 13:43:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6063 updates
2022-03-08 13:43:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 13:43:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 13:43:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 31 @ 6063 updates, score 7.922) (writing took 2.869474886916578 seconds)
2022-03-08 13:43:31 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-08 13:43:31 | INFO | train | epoch 031 | loss 5.668 | ppl 50.86 | wps 20089.9 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6063 | lr 0.000406122 | gnorm 0.687 | loss_scale 8 | train_wall 621 | gb_free 14.1 | wall 19863
2022-03-08 13:43:31 | INFO | fairseq.trainer | begin training epoch 32
2022-03-08 13:43:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:45:30 | INFO | train_inner | epoch 032:     37 / 196 loss=5.654, ppl=50.36, wps=19860.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.702, loss_scale=8, train_wall=316, gb_free=14.1, wall=19982
2022-03-08 13:50:53 | INFO | train_inner | epoch 032:    137 / 196 loss=5.603, ppl=48.61, wps=20335.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.698, loss_scale=8, train_wall=317, gb_free=14.1, wall=20305
2022-03-08 13:54:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 13:54:07 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.96 | ppl 249.03 | wps 46028.3 | wpb 2034.1 | bsz 4 | num_updates 6259 | best_loss 7.723
2022-03-08 13:54:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6259 updates
2022-03-08 13:54:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 13:54:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 13:54:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 32 @ 6259 updates, score 7.96) (writing took 2.915075237862766 seconds)
2022-03-08 13:54:10 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-08 13:54:10 | INFO | train | epoch 032 | loss 5.608 | ppl 48.77 | wps 20088.9 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6259 | lr 0.000399712 | gnorm 0.702 | loss_scale 8 | train_wall 621 | gb_free 14.1 | wall 20502
2022-03-08 13:54:10 | INFO | fairseq.trainer | begin training epoch 33
2022-03-08 13:54:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 13:56:22 | INFO | train_inner | epoch 033:     41 / 196 loss=5.586, ppl=48.04, wps=19858.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.694, loss_scale=8, train_wall=316, gb_free=14.1, wall=20634
2022-03-08 13:57:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 14:01:47 | INFO | train_inner | epoch 033:    142 / 196 loss=5.547, ppl=46.75, wps=20129.2, ups=0.31, wpb=65530.9, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.707, loss_scale=8, train_wall=321, gb_free=14.1, wall=20959
2022-03-08 14:04:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:04:45 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.993 | ppl 254.83 | wps 46204.5 | wpb 2034.1 | bsz 4 | num_updates 6454 | best_loss 7.723
2022-03-08 14:04:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6454 updates
2022-03-08 14:04:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 14:04:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 14:04:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 33 @ 6454 updates, score 7.993) (writing took 2.943080812692642 seconds)
2022-03-08 14:04:48 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-08 14:04:48 | INFO | train | epoch 033 | loss 5.549 | ppl 46.82 | wps 19981.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6454 | lr 0.000393628 | gnorm 0.704 | loss_scale 8 | train_wall 621 | gb_free 14.1 | wall 21140
2022-03-08 14:04:48 | INFO | fairseq.trainer | begin training epoch 34
2022-03-08 14:04:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:07:17 | INFO | train_inner | epoch 034:     46 / 196 loss=5.523, ppl=45.99, wps=19853.8, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.688, loss_scale=8, train_wall=317, gb_free=14.1, wall=21289
2022-03-08 14:12:39 | INFO | train_inner | epoch 034:    146 / 196 loss=5.494, ppl=45.07, wps=20327.3, ups=0.31, wpb=65527.3, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.711, loss_scale=8, train_wall=317, gb_free=14.1, wall=21611
2022-03-08 14:15:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:15:24 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.033 | ppl 261.84 | wps 46114.1 | wpb 2034.1 | bsz 4 | num_updates 6650 | best_loss 7.723
2022-03-08 14:15:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6650 updates
2022-03-08 14:15:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 14:15:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 14:15:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 34 @ 6650 updates, score 8.033) (writing took 2.997287360019982 seconds)
2022-03-08 14:15:27 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-08 14:15:27 | INFO | train | epoch 034 | loss 5.495 | ppl 45.09 | wps 20082.6 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6650 | lr 0.000387783 | gnorm 0.699 | loss_scale 8 | train_wall 621 | gb_free 14.1 | wall 21779
2022-03-08 14:15:27 | INFO | fairseq.trainer | begin training epoch 35
2022-03-08 14:15:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:18:08 | INFO | train_inner | epoch 035:     50 / 196 loss=5.47, ppl=44.31, wps=19855.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.709, loss_scale=8, train_wall=316, gb_free=14.1, wall=21940
2022-03-08 14:23:31 | INFO | train_inner | epoch 035:    150 / 196 loss=5.447, ppl=43.64, wps=20333.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.717, loss_scale=8, train_wall=317, gb_free=14.1, wall=22262
2022-03-08 14:25:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:26:03 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.05 | ppl 265.07 | wps 46694.2 | wpb 2034.1 | bsz 4 | num_updates 6846 | best_loss 7.723
2022-03-08 14:26:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6846 updates
2022-03-08 14:26:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 14:26:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 14:26:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 35 @ 6846 updates, score 8.05) (writing took 3.031698568724096 seconds)
2022-03-08 14:26:06 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-08 14:26:06 | INFO | train | epoch 035 | loss 5.441 | ppl 43.46 | wps 20089.1 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6846 | lr 0.000382192 | gnorm 0.71 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 22418
2022-03-08 14:26:06 | INFO | fairseq.trainer | begin training epoch 36
2022-03-08 14:26:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:29:00 | INFO | train_inner | epoch 036:     54 / 196 loss=5.408, ppl=42.46, wps=19855.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.723, loss_scale=16, train_wall=316, gb_free=14.1, wall=22592
2022-03-08 14:34:22 | INFO | train_inner | epoch 036:    154 / 196 loss=5.401, ppl=42.26, wps=20335.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.686, loss_scale=16, train_wall=317, gb_free=14.1, wall=22914
2022-03-08 14:36:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:36:41 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.1 | ppl 274.42 | wps 46292.7 | wpb 2034.1 | bsz 4 | num_updates 7042 | best_loss 7.723
2022-03-08 14:36:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7042 updates
2022-03-08 14:36:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 14:36:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 14:36:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 36 @ 7042 updates, score 8.1) (writing took 3.0698079681023955 seconds)
2022-03-08 14:36:44 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-08 14:36:44 | INFO | train | epoch 036 | loss 5.392 | ppl 42 | wps 20082.9 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 7042 | lr 0.000376836 | gnorm 0.708 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 23056
2022-03-08 14:36:44 | INFO | fairseq.trainer | begin training epoch 37
2022-03-08 14:36:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:39:51 | INFO | train_inner | epoch 037:     58 / 196 loss=5.352, ppl=40.85, wps=19847, ups=0.3, wpb=65367, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.719, loss_scale=16, train_wall=317, gb_free=14.1, wall=23243
2022-03-08 14:45:14 | INFO | train_inner | epoch 037:    158 / 196 loss=5.357, ppl=40.99, wps=20331.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.733, loss_scale=16, train_wall=317, gb_free=14.1, wall=23566
2022-03-08 14:47:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:47:20 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.093 | ppl 273.12 | wps 46509.1 | wpb 2034.1 | bsz 4 | num_updates 7238 | best_loss 7.723
2022-03-08 14:47:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7238 updates
2022-03-08 14:47:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 14:47:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 14:47:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 37 @ 7238 updates, score 8.093) (writing took 3.0158187206834555 seconds)
2022-03-08 14:47:23 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-08 14:47:23 | INFO | train | epoch 037 | loss 5.344 | ppl 40.62 | wps 20084 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 7238 | lr 0.000371698 | gnorm 0.735 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 23695
2022-03-08 14:47:23 | INFO | fairseq.trainer | begin training epoch 38
2022-03-08 14:47:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 14:50:43 | INFO | train_inner | epoch 038:     62 / 196 loss=5.301, ppl=39.44, wps=19855.8, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.725, loss_scale=16, train_wall=316, gb_free=14.1, wall=23895
2022-03-08 14:53:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 14:56:08 | INFO | train_inner | epoch 038:    163 / 196 loss=5.313, ppl=39.76, wps=20134.1, ups=0.31, wpb=65530.9, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.737, loss_scale=16, train_wall=320, gb_free=14.1, wall=24220
2022-03-08 14:57:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 14:57:59 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.146 | ppl 283.18 | wps 46518.9 | wpb 2034.1 | bsz 4 | num_updates 7433 | best_loss 7.723
2022-03-08 14:57:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7433 updates
2022-03-08 14:57:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 14:58:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 14:58:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 38 @ 7433 updates, score 8.146) (writing took 2.97636591270566 seconds)
2022-03-08 14:58:02 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-08 14:58:02 | INFO | train | epoch 038 | loss 5.297 | ppl 39.32 | wps 19987.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7433 | lr 0.00036679 | gnorm 0.724 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 24334
2022-03-08 14:58:02 | INFO | fairseq.trainer | begin training epoch 39
2022-03-08 14:58:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:01:38 | INFO | train_inner | epoch 039:     67 / 196 loss=5.25, ppl=38.06, wps=19854.9, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=7500, lr=0.000365148, gnorm=0.703, loss_scale=16, train_wall=317, gb_free=14.1, wall=24550
2022-03-08 15:07:00 | INFO | train_inner | epoch 039:    167 / 196 loss=5.273, ppl=38.67, wps=20331.5, ups=0.31, wpb=65530.9, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.729, loss_scale=16, train_wall=317, gb_free=14.1, wall=24872
2022-03-08 15:08:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:08:37 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.217 | ppl 297.59 | wps 46523.6 | wpb 2034.1 | bsz 4 | num_updates 7629 | best_loss 7.723
2022-03-08 15:08:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7629 updates
2022-03-08 15:08:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 15:08:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 15:08:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 39 @ 7629 updates, score 8.217) (writing took 2.9746533082798123 seconds)
2022-03-08 15:08:40 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-08 15:08:40 | INFO | train | epoch 039 | loss 5.254 | ppl 38.17 | wps 20083.7 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 7629 | lr 0.000362048 | gnorm 0.714 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 24972
2022-03-08 15:08:40 | INFO | fairseq.trainer | begin training epoch 40
2022-03-08 15:08:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:08:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 15:12:32 | INFO | train_inner | epoch 040:     72 / 196 loss=5.204, ppl=36.87, wps=19668.3, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=7700, lr=0.000360375, gnorm=0.745, loss_scale=8, train_wall=320, gb_free=14.1, wall=25204
2022-03-08 15:17:54 | INFO | train_inner | epoch 040:    172 / 196 loss=5.232, ppl=37.59, wps=20346.5, ups=0.31, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.718, loss_scale=8, train_wall=317, gb_free=14.1, wall=25526
2022-03-08 15:19:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:19:16 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.213 | ppl 296.8 | wps 46313.2 | wpb 2034.1 | bsz 4 | num_updates 7824 | best_loss 7.723
2022-03-08 15:19:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7824 updates
2022-03-08 15:19:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 15:19:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 15:19:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 40 @ 7824 updates, score 8.213) (writing took 13.842931331135333 seconds)
2022-03-08 15:19:30 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-08 15:19:30 | INFO | train | epoch 040 | loss 5.212 | ppl 37.06 | wps 19660.2 | ups 0.3 | wpb 65450.1 | bsz 127.8 | num_updates 7824 | lr 0.000357508 | gnorm 0.736 | loss_scale 8 | train_wall 621 | gb_free 14.1 | wall 25621
2022-03-08 15:19:30 | INFO | fairseq.trainer | begin training epoch 41
2022-03-08 15:19:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:23:34 | INFO | train_inner | epoch 041:     76 / 196 loss=5.16, ppl=35.76, wps=19227.9, ups=0.29, wpb=65368.6, bsz=127.7, num_updates=7900, lr=0.000355784, gnorm=0.719, loss_scale=8, train_wall=316, gb_free=14.1, wall=25866
2022-03-08 15:28:57 | INFO | train_inner | epoch 041:    176 / 196 loss=5.198, ppl=36.71, wps=20331.3, ups=0.31, wpb=65527.3, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.742, loss_scale=8, train_wall=317, gb_free=14.1, wall=26189
2022-03-08 15:30:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:30:05 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.245 | ppl 303.45 | wps 45989.8 | wpb 2034.1 | bsz 4 | num_updates 8020 | best_loss 7.723
2022-03-08 15:30:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 8020 updates
2022-03-08 15:30:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 15:30:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 15:30:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 41 @ 8020 updates, score 8.245) (writing took 2.924466399475932 seconds)
2022-03-08 15:30:08 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-08 15:30:08 | INFO | train | epoch 041 | loss 5.172 | ppl 36.06 | wps 20090.6 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 8020 | lr 0.000353112 | gnorm 0.732 | loss_scale 8 | train_wall 621 | gb_free 14.1 | wall 26260
2022-03-08 15:30:08 | INFO | fairseq.trainer | begin training epoch 42
2022-03-08 15:30:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:34:26 | INFO | train_inner | epoch 042:     80 / 196 loss=5.114, ppl=34.63, wps=19853.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=8100, lr=0.000351364, gnorm=0.737, loss_scale=8, train_wall=317, gb_free=14.1, wall=26518
2022-03-08 15:39:48 | INFO | train_inner | epoch 042:    180 / 196 loss=5.163, ppl=35.82, wps=20337.9, ups=0.31, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.74, loss_scale=16, train_wall=317, gb_free=14.1, wall=26840
2022-03-08 15:40:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:40:44 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.273 | ppl 309.26 | wps 46028 | wpb 2034.1 | bsz 4 | num_updates 8216 | best_loss 7.723
2022-03-08 15:40:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8216 updates
2022-03-08 15:40:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 15:40:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 15:40:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 42 @ 8216 updates, score 8.273) (writing took 2.9325011828914285 seconds)
2022-03-08 15:40:47 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-08 15:40:47 | INFO | train | epoch 042 | loss 5.133 | ppl 35.09 | wps 20086.7 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 8216 | lr 0.000348875 | gnorm 0.737 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 26899
2022-03-08 15:40:47 | INFO | fairseq.trainer | begin training epoch 43
2022-03-08 15:40:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:45:18 | INFO | train_inner | epoch 043:     84 / 196 loss=5.07, ppl=33.59, wps=19847.7, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=8300, lr=0.000347105, gnorm=0.746, loss_scale=16, train_wall=317, gb_free=14.1, wall=27169
2022-03-08 15:50:40 | INFO | train_inner | epoch 043:    184 / 196 loss=5.133, ppl=35.08, wps=20328.6, ups=0.31, wpb=65527.3, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.753, loss_scale=16, train_wall=317, gb_free=14.1, wall=27492
2022-03-08 15:51:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 15:51:23 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.254 | ppl 305.34 | wps 45992.4 | wpb 2034.1 | bsz 4 | num_updates 8412 | best_loss 7.723
2022-03-08 15:51:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8412 updates
2022-03-08 15:51:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 15:51:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 15:51:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 43 @ 8412 updates, score 8.254) (writing took 2.9040524922311306 seconds)
2022-03-08 15:51:25 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-08 15:51:25 | INFO | train | epoch 043 | loss 5.097 | ppl 34.22 | wps 20081.1 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 8412 | lr 0.000344787 | gnorm 0.752 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 27537
2022-03-08 15:51:25 | INFO | fairseq.trainer | begin training epoch 44
2022-03-08 15:51:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 15:56:09 | INFO | train_inner | epoch 044:     88 / 196 loss=5.025, ppl=32.56, wps=19859.5, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=8500, lr=0.000342997, gnorm=0.731, loss_scale=16, train_wall=316, gb_free=14.1, wall=27821
2022-03-08 16:01:31 | INFO | train_inner | epoch 044:    188 / 196 loss=5.102, ppl=34.34, wps=20330.2, ups=0.31, wpb=65527.3, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.747, loss_scale=16, train_wall=317, gb_free=14.1, wall=28143
2022-03-08 16:01:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:02:01 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.299 | ppl 314.99 | wps 46022.6 | wpb 2034.1 | bsz 4 | num_updates 8608 | best_loss 7.723
2022-03-08 16:02:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8608 updates
2022-03-08 16:02:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 16:02:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 16:02:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 44 @ 8608 updates, score 8.299) (writing took 2.952860562130809 seconds)
2022-03-08 16:02:04 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-08 16:02:04 | INFO | train | epoch 044 | loss 5.059 | ppl 33.35 | wps 20087.7 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 8608 | lr 0.000340839 | gnorm 0.736 | loss_scale 16 | train_wall 621 | gb_free 14.1 | wall 28176
2022-03-08 16:02:04 | INFO | fairseq.trainer | begin training epoch 45
2022-03-08 16:02:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:04:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 16:07:04 | INFO | train_inner | epoch 045:     93 / 196 loss=4.992, ppl=31.82, wps=19658.5, ups=0.3, wpb=65368.6, bsz=127.7, num_updates=8700, lr=0.000339032, gnorm=0.739, loss_scale=16, train_wall=320, gb_free=14.1, wall=28476
2022-03-08 16:08:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-08 16:12:29 | INFO | train_inner | epoch 045:    194 / 196 loss=5.064, ppl=33.46, wps=20135.4, ups=0.31, wpb=65530.9, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.748, loss_scale=8, train_wall=320, gb_free=14.1, wall=28801
2022-03-08 16:12:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:12:40 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.365 | ppl 329.62 | wps 46023.2 | wpb 2034.1 | bsz 4 | num_updates 8802 | best_loss 7.723
2022-03-08 16:12:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8802 updates
2022-03-08 16:12:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 16:12:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 16:12:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 45 @ 8802 updates, score 8.365) (writing took 3.005753573961556 seconds)
2022-03-08 16:12:43 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-08 16:12:43 | INFO | train | epoch 045 | loss 5.025 | ppl 32.57 | wps 19879.4 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 8802 | lr 0.000337062 | gnorm 0.745 | loss_scale 8 | train_wall 621 | gb_free 14.1 | wall 28815
2022-03-08 16:12:43 | INFO | fairseq.trainer | begin training epoch 46
2022-03-08 16:12:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:17:59 | INFO | train_inner | epoch 046:     98 / 196 loss=4.954, ppl=30.99, wps=19850.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=8900, lr=0.000335201, gnorm=0.752, loss_scale=8, train_wall=316, gb_free=14.1, wall=29131
2022-03-08 16:23:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 16:23:18 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.356 | ppl 327.55 | wps 46340 | wpb 2034.1 | bsz 4 | num_updates 8998 | best_loss 7.723
2022-03-08 16:23:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8998 updates
2022-03-08 16:23:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 16:23:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt
2022-03-08 16:23:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.125-jelinek_0.02_0.08_0.9_#3/checkpoint_last.pt (epoch 46 @ 8998 updates, score 8.356) (writing took 3.073327192105353 seconds)
2022-03-08 16:23:21 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-08 16:23:21 | INFO | train | epoch 046 | loss 4.994 | ppl 31.86 | wps 20085.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 8998 | lr 0.00033337 | gnorm 0.758 | loss_scale 8 | train_wall 621 | gb_free 14.1 | wall 29453
2022-03-08 16:23:21 | INFO | fairseq.trainer | begin training epoch 47
2022-03-08 16:23:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 16:23:28 | INFO | train_inner | epoch 047:      2 / 196 loss=5.034, ppl=32.77, wps=19851.8, ups=0.3, wpb=65372.2, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.764, loss_scale=8, train_wall=316, gb_free=14.1, wall=29460
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 496, in train_step
    optimizer.backward(loss)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/optim/fp16_optimizer.py", line 105, in backward
    loss.backward()
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
