Sender: LSF System <lsfadmin@eu-g3-076>
Subject: Job 204577723: <w103_fp16_label_smoothing_0.1_#3> in cluster <euler> Exited

Job <w103_fp16_label_smoothing_0.1_#3> was submitted from host <eu-login-08> by user <andriusb> in cluster <euler> at Fri Feb 11 08:44:47 2022
Job was executed on host(s) <eu-g3-076>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Fri Feb 11 09:27:26 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 11 09:27:26 2022
Terminated at Fri Feb 11 09:27:48 2022
Results reported at Fri Feb 11 09:27:48 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 13521653 --fp16 --max-update 5000000
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   18.72 sec.
    Max Memory :                                 3225 MB
    Average Memory :                             92.00 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               16775.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                5
    Run time :                                   21 sec.
    Turnaround time :                            2581 sec.

The output (if any) follows:

2022-02-11 09:27:32 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 13521653, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 5000000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.5, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 13521653, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.5, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-11 09:27:34 | INFO | fairseq.tasks.language_modeling | dictionary: 623952 types
2022-02-11 09:27:39 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(623952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=623952, bias=False)
  )
)
2022-02-11 09:27:39 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-11 09:27:39 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-11 09:27:39 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-02-11 09:27:39 | INFO | fairseq_cli.train | num. shared model params: 338,377,728 (num. trained: 338,377,728)
2022-02-11 09:27:39 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-11 09:27:39 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-full/valid
2022-02-11 09:27:43 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-11 09:27:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 09:27:43 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-11 09:27:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 09:27:43 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-11 09:27:43 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None
2022-02-11 09:27:43 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-02-11 09:27:43 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-02-11 09:27:43 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-11 09:27:43 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103-raw-full/train
2022-02-11 09:27:44 | INFO | fairseq.trainer | begin training epoch 1
2022-02-11 09:27:44 | INFO | fairseq_cli.train | Start iterating over samples
> /cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py(50)label_smoothed_nll_loss()
-> loss = (1.0 - epsilon - eps_i) * nll_loss + eps_i * smooth_loss
(Pdb) 
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 81, in forward
    loss, nll_loss = self.compute_loss(model, net_output, sample, reduce=reduce)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 112, in compute_loss
    loss, nll_loss = label_smoothed_nll_loss(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 50, in label_smoothed_nll_loss
    loss = (1.0 - epsilon - eps_i) * nll_loss + eps_i * smooth_loss
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 50, in label_smoothed_nll_loss
    loss = (1.0 - epsilon - eps_i) * nll_loss + eps_i * smooth_loss
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
Sender: LSF System <lsfadmin@eu-g3-076>
Subject: Job 204594668: <w103_fp16_label_smoothing_0.1_#3> in cluster <euler> Exited

Job <w103_fp16_label_smoothing_0.1_#3> was submitted from host <eu-login-08> by user <andriusb> in cluster <euler> at Fri Feb 11 12:05:33 2022
Job was executed on host(s) <eu-g3-076>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Fri Feb 11 14:39:11 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 11 14:39:11 2022
Terminated at Fri Feb 11 15:15:28 2022
Results reported at Fri Feb 11 15:15:28 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 13521653 --fp16 --max-update 5000000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   2161.65 sec.
    Max Memory :                                 4169 MB
    Average Memory :                             4030.23 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               15831.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   2176 sec.
    Turnaround time :                            11395 sec.

The output (if any) follows:

2022-02-11 14:39:21 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 13521653, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 5000000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.5, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 13521653, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.5, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-11 14:39:22 | INFO | fairseq.tasks.language_modeling | dictionary: 623952 types
2022-02-11 14:39:27 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(623952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=623952, bias=False)
  )
)
2022-02-11 14:39:27 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-11 14:39:27 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-11 14:39:27 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-02-11 14:39:27 | INFO | fairseq_cli.train | num. shared model params: 338,377,728 (num. trained: 338,377,728)
2022-02-11 14:39:27 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-11 14:39:27 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-full/valid
2022-02-11 14:39:36 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-11 14:39:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 14:39:36 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-11 14:39:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 14:39:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-11 14:39:36 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None
2022-02-11 14:39:36 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-02-11 14:39:36 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-02-11 14:39:36 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-11 14:39:40 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103-raw-full/train
2022-02-11 14:39:40 | INFO | fairseq.trainer | begin training epoch 1
2022-02-11 14:39:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 14:39:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-11 14:40:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 14:40:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 14:40:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 14:49:52 | INFO | train_inner | epoch 001:    104 / 1576 loss=18.308, nll_loss=18.122, ppl=285301, wps=11699.8, ups=0.18, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.896, loss_scale=8, train_wall=598, gb_free=8.8, wall=616
2022-02-11 14:59:13 | INFO | train_inner | epoch 001:    204 / 1576 loss=15.962, nll_loss=15.514, ppl=46804.9, wps=11689.5, ups=0.18, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.391, loss_scale=8, train_wall=550, gb_free=8.8, wall=1176
2022-02-11 15:08:33 | INFO | train_inner | epoch 001:    304 / 1576 loss=13.904, nll_loss=13.203, ppl=9431.15, wps=11700.3, ups=0.18, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.996, loss_scale=16, train_wall=550, gb_free=8.8, wall=1737
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 79, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 328, in extract_features_scriptable
    if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():
KeyboardInterrupt
Sender: LSF System <lsfadmin@eu-g3-071>
Subject: Job 204616327: <w103_fp16_label_smoothing_0.1_#3> in cluster <euler> Done

Job <w103_fp16_label_smoothing_0.1_#3> was submitted from host <eu-login-08> by user <andriusb> in cluster <euler> at Fri Feb 11 16:38:52 2022
Job was executed on host(s) <eu-g3-071>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Fri Feb 11 17:39:13 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 11 17:39:13 2022
Terminated at Mon Feb 14 23:42:20 2022
Results reported at Mon Feb 14 23:42:20 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 13521653 --fp16 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   280734.91 sec.
    Max Memory :                                 18002 MB
    Average Memory :                             3200.94 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               1998.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   280986 sec.
    Turnaround time :                            284608 sec.

The output (if any) follows:

2022-02-11 17:39:20 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 13521653, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 13521653, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-11 17:39:21 | INFO | fairseq.tasks.language_modeling | dictionary: 623952 types
2022-02-11 17:39:26 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(623952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=623952, bias=False)
  )
)
2022-02-11 17:39:26 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-11 17:39:26 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-11 17:39:26 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-02-11 17:39:26 | INFO | fairseq_cli.train | num. shared model params: 338,377,728 (num. trained: 338,377,728)
2022-02-11 17:39:26 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-11 17:39:26 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-full/valid
2022-02-11 17:39:30 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-11 17:39:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 17:39:30 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-11 17:39:30 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 17:39:30 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-11 17:39:30 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None
2022-02-11 17:39:30 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-02-11 17:39:30 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-02-11 17:39:30 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-11 17:39:31 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103-raw-full/train
2022-02-11 17:39:31 | INFO | fairseq.trainer | begin training epoch 1
2022-02-11 17:39:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 17:39:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-11 17:39:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 17:40:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 17:40:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 17:49:51 | INFO | train_inner | epoch 001:    104 / 1576 loss=17.957, nll_loss=17.729, ppl=217294, wps=11725.9, ups=0.18, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.487, loss_scale=8, train_wall=606, gb_free=8.8, wall=621
2022-02-11 17:59:10 | INFO | train_inner | epoch 001:    204 / 1576 loss=15.603, nll_loss=15.115, ppl=35475.9, wps=11717.1, ups=0.18, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.372, loss_scale=8, train_wall=549, gb_free=8.8, wall=1180
2022-02-11 18:08:29 | INFO | train_inner | epoch 001:    304 / 1576 loss=13.557, nll_loss=12.813, ppl=7198.05, wps=11720.3, ups=0.18, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.897, loss_scale=16, train_wall=549, gb_free=8.8, wall=1739
2022-02-11 18:17:48 | INFO | train_inner | epoch 001:    404 / 1576 loss=12.044, nll_loss=11.049, ppl=2118.84, wps=11723.8, ups=0.18, wpb=65532.3, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.577, loss_scale=16, train_wall=548, gb_free=8.8, wall=2298
2022-02-11 18:27:08 | INFO | train_inner | epoch 001:    504 / 1576 loss=11.431, nll_loss=10.283, ppl=1246.1, wps=11715, ups=0.18, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.429, loss_scale=16, train_wall=549, gb_free=8.8, wall=2857
2022-02-11 18:36:28 | INFO | train_inner | epoch 001:    604 / 1576 loss=11.13, nll_loss=9.919, ppl=968.19, wps=11704.7, ups=0.18, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.563, loss_scale=32, train_wall=549, gb_free=8.8, wall=3417
2022-02-11 18:45:48 | INFO | train_inner | epoch 001:    704 / 1576 loss=10.866, nll_loss=9.617, ppl=785.44, wps=11703.2, ups=0.18, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.565, loss_scale=32, train_wall=549, gb_free=8.8, wall=3977
2022-02-11 18:53:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 18:55:13 | INFO | train_inner | epoch 001:    805 / 1576 loss=10.648, nll_loss=9.37, ppl=661.58, wps=11587.3, ups=0.18, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.695, loss_scale=32, train_wall=555, gb_free=8.8, wall=4543
2022-02-11 19:04:33 | INFO | train_inner | epoch 001:    905 / 1576 loss=10.449, nll_loss=9.145, ppl=566.17, wps=11705.4, ups=0.18, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.738, loss_scale=32, train_wall=549, gb_free=8.8, wall=5103
2022-02-11 19:13:53 | INFO | train_inner | epoch 001:   1005 / 1576 loss=10.271, nll_loss=8.944, ppl=492.53, wps=11699.4, ups=0.18, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.753, loss_scale=32, train_wall=550, gb_free=8.8, wall=5663
2022-02-11 19:17:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 19:23:19 | INFO | train_inner | epoch 001:   1106 / 1576 loss=10.095, nll_loss=8.746, ppl=429.25, wps=11586.5, ups=0.18, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.846, loss_scale=32, train_wall=555, gb_free=8.8, wall=6229
2022-02-11 19:32:39 | INFO | train_inner | epoch 001:   1206 / 1576 loss=9.953, nll_loss=8.585, ppl=384.02, wps=11703.1, ups=0.18, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.818, loss_scale=32, train_wall=549, gb_free=8.8, wall=6789
2022-02-11 19:41:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 19:42:04 | INFO | train_inner | epoch 001:   1307 / 1576 loss=9.825, nll_loss=8.441, ppl=347.56, wps=11588.3, ups=0.18, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.863, loss_scale=32, train_wall=555, gb_free=8.8, wall=7354
2022-02-11 19:51:25 | INFO | train_inner | epoch 001:   1407 / 1576 loss=9.695, nll_loss=8.293, ppl=313.69, wps=11693.5, ups=0.18, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.869, loss_scale=32, train_wall=550, gb_free=8.8, wall=7915
2022-02-11 20:00:45 | INFO | train_inner | epoch 001:   1507 / 1576 loss=9.584, nll_loss=8.168, ppl=287.69, wps=11698.7, ups=0.18, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.865, loss_scale=32, train_wall=550, gb_free=8.8, wall=8475
2022-02-11 20:05:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 20:07:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 20:07:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.322 | nll_loss 7.863 | ppl 232.88 | wps 32506.1 | wpb 1021.8 | bsz 2 | num_updates 1568
2022-02-11 20:07:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1568 updates
2022-02-11 20:07:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint1.pt
2022-02-11 20:07:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint1.pt
2022-02-11 20:07:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint1.pt (epoch 1 @ 1568 updates, score 9.322) (writing took 28.100984477438033 seconds)
2022-02-11 20:07:42 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-11 20:07:42 | INFO | train | epoch 001 | loss 11.453 | nll_loss 10.314 | ppl 1272.88 | wps 11631.7 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 1568 | lr 0.000196061 | gnorm 0.952 | loss_scale 32 | train_wall 8686 | gb_free 8.8 | wall 8891
2022-02-11 20:07:42 | INFO | fairseq.trainer | begin training epoch 2
2022-02-11 20:07:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 20:10:41 | INFO | train_inner | epoch 002:     32 / 1576 loss=9.475, nll_loss=8.045, ppl=264.09, wps=10910.4, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=1600, lr=0.00020006, gnorm=0.876, loss_scale=32, train_wall=550, gb_free=8.8, wall=9070
2022-02-11 20:20:00 | INFO | train_inner | epoch 002:    132 / 1576 loss=9.357, nll_loss=7.911, ppl=240.66, wps=11716.3, ups=0.18, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.849, loss_scale=32, train_wall=549, gb_free=8.8, wall=9630
2022-02-11 20:29:20 | INFO | train_inner | epoch 002:    232 / 1576 loss=9.273, nll_loss=7.817, ppl=225.45, wps=11696.3, ups=0.18, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.817, loss_scale=32, train_wall=550, gb_free=8.8, wall=10190
2022-02-11 20:30:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 20:38:46 | INFO | train_inner | epoch 002:    333 / 1576 loss=9.172, nll_loss=7.703, ppl=208.32, wps=11580.1, ups=0.18, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.841, loss_scale=32, train_wall=555, gb_free=8.8, wall=10756
2022-02-11 20:48:07 | INFO | train_inner | epoch 002:    433 / 1576 loss=9.093, nll_loss=7.613, ppl=195.78, wps=11693, ups=0.18, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.807, loss_scale=32, train_wall=550, gb_free=8.8, wall=11316
2022-02-11 20:56:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 20:57:32 | INFO | train_inner | epoch 002:    534 / 1576 loss=9.018, nll_loss=7.529, ppl=184.71, wps=11582.1, ups=0.18, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.798, loss_scale=32, train_wall=555, gb_free=8.8, wall=11882
2022-02-11 21:06:53 | INFO | train_inner | epoch 002:    634 / 1576 loss=8.944, nll_loss=7.446, ppl=174.32, wps=11696.1, ups=0.18, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.766, loss_scale=32, train_wall=550, gb_free=8.8, wall=12442
2022-02-11 21:16:13 | INFO | train_inner | epoch 002:    734 / 1576 loss=8.876, nll_loss=7.368, ppl=165.18, wps=11694.1, ups=0.18, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.76, loss_scale=32, train_wall=550, gb_free=8.8, wall=13003
2022-02-11 21:20:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 21:25:39 | INFO | train_inner | epoch 002:    835 / 1576 loss=8.794, nll_loss=7.276, ppl=155, wps=11578, ups=0.18, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.743, loss_scale=32, train_wall=555, gb_free=8.8, wall=13569
2022-02-11 21:31:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 21:35:05 | INFO | train_inner | epoch 002:    936 / 1576 loss=8.739, nll_loss=7.213, ppl=148.38, wps=11587.5, ups=0.18, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.752, loss_scale=16, train_wall=555, gb_free=8.8, wall=14134
2022-02-11 21:44:25 | INFO | train_inner | epoch 002:   1036 / 1576 loss=8.674, nll_loss=7.14, ppl=141.04, wps=11706.7, ups=0.18, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.717, loss_scale=16, train_wall=549, gb_free=8.8, wall=14694
2022-02-11 21:53:45 | INFO | train_inner | epoch 002:   1136 / 1576 loss=8.611, nll_loss=7.07, ppl=134.32, wps=11700.6, ups=0.18, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.717, loss_scale=16, train_wall=549, gb_free=8.8, wall=15254
2022-02-11 21:58:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 22:03:10 | INFO | train_inner | epoch 002:   1237 / 1576 loss=8.56, nll_loss=7.012, ppl=129.04, wps=11588.3, ups=0.18, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.689, loss_scale=16, train_wall=555, gb_free=8.8, wall=15820
2022-02-11 22:12:30 | INFO | train_inner | epoch 002:   1337 / 1576 loss=8.508, nll_loss=6.953, ppl=123.87, wps=11710.4, ups=0.18, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.694, loss_scale=16, train_wall=549, gb_free=8.8, wall=16380
2022-02-11 22:21:50 | INFO | train_inner | epoch 002:   1437 / 1576 loss=8.451, nll_loss=6.89, ppl=118.56, wps=11704.8, ups=0.18, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.676, loss_scale=16, train_wall=549, gb_free=8.8, wall=16940
2022-02-11 22:31:10 | INFO | train_inner | epoch 002:   1537 / 1576 loss=8.399, nll_loss=6.83, ppl=113.78, wps=11699.6, ups=0.18, wpb=65532.3, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.675, loss_scale=32, train_wall=550, gb_free=8.8, wall=17500
2022-02-11 22:34:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 22:34:51 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.195 | nll_loss 6.568 | ppl 94.86 | wps 32338.6 | wpb 1021.8 | bsz 2 | num_updates 3139 | best_loss 8.195
2022-02-11 22:34:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3139 updates
2022-02-11 22:34:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint2.pt
2022-02-11 22:35:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint2.pt
2022-02-11 22:35:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint2.pt (epoch 2 @ 3139 updates, score 8.195) (writing took 32.45434477645904 seconds)
2022-02-11 22:35:23 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-11 22:35:23 | INFO | train | epoch 002 | loss 8.832 | nll_loss 7.319 | ppl 159.68 | wps 11612 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 3139 | lr 0.000392397 | gnorm 0.754 | loss_scale 32 | train_wall 8655 | gb_free 8.8 | wall 17753
2022-02-11 22:35:23 | INFO | fairseq.trainer | begin training epoch 3
2022-02-11 22:35:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 22:35:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 22:41:10 | INFO | train_inner | epoch 003:     62 / 1576 loss=8.326, nll_loss=6.747, ppl=107.42, wps=10831.5, ups=0.17, wpb=64958.8, bsz=126.9, num_updates=3200, lr=0.00040002, gnorm=0.667, loss_scale=16, train_wall=550, gb_free=8.8, wall=18099
2022-02-11 22:50:29 | INFO | train_inner | epoch 003:    162 / 1576 loss=8.258, nll_loss=6.671, ppl=101.88, wps=11721.6, ups=0.18, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.654, loss_scale=16, train_wall=548, gb_free=8.8, wall=18658
2022-02-11 22:59:48 | INFO | train_inner | epoch 003:    262 / 1576 loss=8.238, nll_loss=6.649, ppl=100.36, wps=11723.1, ups=0.18, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.653, loss_scale=32, train_wall=548, gb_free=8.8, wall=19217
2022-02-11 23:07:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 23:09:13 | INFO | train_inner | epoch 003:    363 / 1576 loss=8.193, nll_loss=6.598, ppl=96.86, wps=11598.5, ups=0.18, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.629, loss_scale=16, train_wall=554, gb_free=8.8, wall=19783
2022-02-11 23:18:32 | INFO | train_inner | epoch 003:    463 / 1576 loss=8.152, nll_loss=6.552, ppl=93.83, wps=11722.8, ups=0.18, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.645, loss_scale=16, train_wall=548, gb_free=8.8, wall=20342
2022-02-11 23:27:51 | INFO | train_inner | epoch 003:    563 / 1576 loss=8.117, nll_loss=6.512, ppl=91.25, wps=11725, ups=0.18, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.615, loss_scale=16, train_wall=548, gb_free=8.8, wall=20901
2022-02-11 23:31:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 23:37:15 | INFO | train_inner | epoch 003:    664 / 1576 loss=8.102, nll_loss=6.496, ppl=90.26, wps=11607.6, ups=0.18, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.623, loss_scale=16, train_wall=554, gb_free=8.8, wall=21465
2022-02-11 23:46:34 | INFO | train_inner | epoch 003:    764 / 1576 loss=8.064, nll_loss=6.452, ppl=87.56, wps=11723.8, ups=0.18, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.622, loss_scale=16, train_wall=548, gb_free=8.8, wall=22024
2022-02-11 23:55:54 | INFO | train_inner | epoch 003:    864 / 1576 loss=8.037, nll_loss=6.423, ppl=85.81, wps=11721.2, ups=0.18, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.587, loss_scale=32, train_wall=548, gb_free=8.8, wall=22583
2022-02-11 23:57:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 00:05:18 | INFO | train_inner | epoch 003:    965 / 1576 loss=8.006, nll_loss=6.387, ppl=83.7, wps=11608, ups=0.18, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.6, loss_scale=16, train_wall=554, gb_free=8.8, wall=23148
2022-02-12 00:14:37 | INFO | train_inner | epoch 003:   1065 / 1576 loss=7.977, nll_loss=6.356, ppl=81.89, wps=11724.3, ups=0.18, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.579, loss_scale=16, train_wall=548, gb_free=8.8, wall=23707
2022-02-12 00:23:56 | INFO | train_inner | epoch 003:   1165 / 1576 loss=7.938, nll_loss=6.312, ppl=79.46, wps=11725, ups=0.18, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.548, loss_scale=32, train_wall=548, gb_free=8.8, wall=24266
2022-02-12 00:24:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 00:33:20 | INFO | train_inner | epoch 003:   1266 / 1576 loss=7.916, nll_loss=6.287, ppl=78.09, wps=11616.1, ups=0.18, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.565, loss_scale=16, train_wall=553, gb_free=8.8, wall=24830
2022-02-12 00:42:39 | INFO | train_inner | epoch 003:   1366 / 1576 loss=7.889, nll_loss=6.258, ppl=76.51, wps=11725.8, ups=0.18, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.533, loss_scale=16, train_wall=548, gb_free=8.8, wall=25389
2022-02-12 00:49:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 00:52:03 | INFO | train_inner | epoch 003:   1467 / 1576 loss=7.869, nll_loss=6.235, ppl=75.32, wps=11615.2, ups=0.18, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.537, loss_scale=16, train_wall=553, gb_free=8.8, wall=25953
2022-02-12 01:01:22 | INFO | train_inner | epoch 003:   1567 / 1576 loss=7.845, nll_loss=6.208, ppl=73.94, wps=11733, ups=0.18, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.518, loss_scale=16, train_wall=548, gb_free=8.8, wall=26512
2022-02-12 01:02:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 01:02:14 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.676 | nll_loss 6.008 | ppl 64.35 | wps 32279.5 | wpb 1021.8 | bsz 2 | num_updates 4709 | best_loss 7.676
2022-02-12 01:02:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4709 updates
2022-02-12 01:02:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint3.pt
2022-02-12 01:02:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint3.pt
2022-02-12 01:02:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint3.pt (epoch 3 @ 4709 updates, score 7.676) (writing took 30.510677307844162 seconds)
2022-02-12 01:02:45 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-12 01:02:45 | INFO | train | epoch 003 | loss 8.05 | nll_loss 6.437 | ppl 86.63 | wps 11630.5 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 4709 | lr 0.000460825 | gnorm 0.597 | loss_scale 16 | train_wall 8637 | gb_free 8.8 | wall 26594
2022-02-12 01:02:45 | INFO | fairseq.trainer | begin training epoch 4
2022-02-12 01:02:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 01:11:13 | INFO | train_inner | epoch 004:     91 / 1576 loss=7.747, nll_loss=6.098, ppl=68.5, wps=10985.7, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=4800, lr=0.000456435, gnorm=0.525, loss_scale=16, train_wall=543, gb_free=8.8, wall=27103
2022-02-12 01:18:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 01:20:38 | INFO | train_inner | epoch 004:    192 / 1576 loss=7.712, nll_loss=6.058, ppl=66.63, wps=11603.6, ups=0.18, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.514, loss_scale=16, train_wall=554, gb_free=8.8, wall=27668
2022-02-12 01:29:57 | INFO | train_inner | epoch 004:    292 / 1576 loss=7.703, nll_loss=6.047, ppl=66.13, wps=11729.7, ups=0.18, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.507, loss_scale=16, train_wall=548, gb_free=8.8, wall=28226
2022-02-12 01:39:16 | INFO | train_inner | epoch 004:    392 / 1576 loss=7.699, nll_loss=6.044, ppl=65.97, wps=11729.1, ups=0.18, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.51, loss_scale=16, train_wall=548, gb_free=8.8, wall=28785
2022-02-12 01:43:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 01:48:40 | INFO | train_inner | epoch 004:    493 / 1576 loss=7.682, nll_loss=6.025, ppl=65.1, wps=11615.1, ups=0.18, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.505, loss_scale=16, train_wall=554, gb_free=8.8, wall=29349
2022-02-12 01:57:59 | INFO | train_inner | epoch 004:    593 / 1576 loss=7.67, nll_loss=6.011, ppl=64.49, wps=11724.4, ups=0.18, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.498, loss_scale=16, train_wall=548, gb_free=8.8, wall=29908
2022-02-12 02:07:17 | INFO | train_inner | epoch 004:    693 / 1576 loss=7.65, nll_loss=5.989, ppl=63.51, wps=11730.7, ups=0.18, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.505, loss_scale=16, train_wall=548, gb_free=8.8, wall=30467
2022-02-12 02:07:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 02:16:42 | INFO | train_inner | epoch 004:    794 / 1576 loss=7.645, nll_loss=5.984, ppl=63.27, wps=11613.9, ups=0.18, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.493, loss_scale=16, train_wall=554, gb_free=8.8, wall=31031
2022-02-12 02:26:00 | INFO | train_inner | epoch 004:    894 / 1576 loss=7.649, nll_loss=5.988, ppl=63.47, wps=11730.1, ups=0.18, wpb=65532.3, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.504, loss_scale=16, train_wall=548, gb_free=8.8, wall=31590
2022-02-12 02:33:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 02:35:25 | INFO | train_inner | epoch 004:    995 / 1576 loss=7.63, nll_loss=5.967, ppl=62.56, wps=11613.9, ups=0.18, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.499, loss_scale=16, train_wall=554, gb_free=8.8, wall=32154
2022-02-12 02:44:43 | INFO | train_inner | epoch 004:   1095 / 1576 loss=7.616, nll_loss=5.951, ppl=61.87, wps=11731.9, ups=0.18, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.484, loss_scale=16, train_wall=548, gb_free=8.8, wall=32713
2022-02-12 02:54:02 | INFO | train_inner | epoch 004:   1195 / 1576 loss=7.607, nll_loss=5.941, ppl=61.43, wps=11734.8, ups=0.18, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.493, loss_scale=16, train_wall=548, gb_free=8.8, wall=33271
2022-02-12 02:58:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 03:03:26 | INFO | train_inner | epoch 004:   1296 / 1576 loss=7.6, nll_loss=5.934, ppl=61.13, wps=11616.9, ups=0.18, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.482, loss_scale=16, train_wall=553, gb_free=8.8, wall=33836
2022-02-12 03:12:45 | INFO | train_inner | epoch 004:   1396 / 1576 loss=7.59, nll_loss=5.923, ppl=60.69, wps=11731.5, ups=0.18, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.487, loss_scale=16, train_wall=548, gb_free=8.8, wall=34394
2022-02-12 03:22:03 | INFO | train_inner | epoch 004:   1496 / 1576 loss=7.568, nll_loss=5.898, ppl=59.62, wps=11731.4, ups=0.18, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.47, loss_scale=16, train_wall=548, gb_free=8.8, wall=34953
2022-02-12 03:23:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 03:29:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 03:29:32 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.432 | nll_loss 5.702 | ppl 52.06 | wps 32251.1 | wpb 1021.8 | bsz 2 | num_updates 6279 | best_loss 7.432
2022-02-12 03:29:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6279 updates
2022-02-12 03:29:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint4.pt
2022-02-12 03:29:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint4.pt
2022-02-12 03:30:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint4.pt (epoch 4 @ 6279 updates, score 7.432) (writing took 28.70386743079871 seconds)
2022-02-12 03:30:01 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-12 03:30:01 | INFO | train | epoch 004 | loss 7.646 | nll_loss 5.985 | ppl 63.32 | wps 11638.1 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 6279 | lr 0.000399075 | gnorm 0.498 | loss_scale 16 | train_wall 8633 | gb_free 8.8 | wall 35430
2022-02-12 03:30:01 | INFO | fairseq.trainer | begin training epoch 5
2022-02-12 03:30:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 03:31:58 | INFO | train_inner | epoch 005:     21 / 1576 loss=7.547, nll_loss=5.875, ppl=58.67, wps=10919.4, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=6300, lr=0.00039841, gnorm=0.493, loss_scale=16, train_wall=549, gb_free=8.8, wall=35548
2022-02-12 03:41:17 | INFO | train_inner | epoch 005:    121 / 1576 loss=7.458, nll_loss=5.773, ppl=54.69, wps=11731.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.476, loss_scale=16, train_wall=548, gb_free=8.8, wall=36106
2022-02-12 03:50:36 | INFO | train_inner | epoch 005:    221 / 1576 loss=7.461, nll_loss=5.777, ppl=54.83, wps=11727, ups=0.18, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.493, loss_scale=32, train_wall=548, gb_free=8.8, wall=36665
2022-02-12 03:52:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 04:00:00 | INFO | train_inner | epoch 005:    322 / 1576 loss=7.454, nll_loss=5.769, ppl=54.54, wps=11614.9, ups=0.18, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.497, loss_scale=16, train_wall=553, gb_free=8.8, wall=37229
2022-02-12 04:09:19 | INFO | train_inner | epoch 005:    422 / 1576 loss=7.45, nll_loss=5.765, ppl=54.39, wps=11727.4, ups=0.18, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.461, loss_scale=16, train_wall=548, gb_free=8.8, wall=37788
2022-02-12 04:18:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 04:18:43 | INFO | train_inner | epoch 005:    523 / 1576 loss=7.453, nll_loss=5.768, ppl=54.48, wps=11612, ups=0.18, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.475, loss_scale=16, train_wall=554, gb_free=8.8, wall=38353
2022-02-12 04:28:01 | INFO | train_inner | epoch 005:    623 / 1576 loss=7.449, nll_loss=5.764, ppl=54.34, wps=11735.7, ups=0.18, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.485, loss_scale=16, train_wall=548, gb_free=8.8, wall=38911
2022-02-12 04:37:20 | INFO | train_inner | epoch 005:    723 / 1576 loss=7.44, nll_loss=5.754, ppl=53.95, wps=11733.9, ups=0.18, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.483, loss_scale=16, train_wall=548, gb_free=8.8, wall=39470
2022-02-12 04:42:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 04:46:44 | INFO | train_inner | epoch 005:    824 / 1576 loss=7.433, nll_loss=5.746, ppl=53.68, wps=11616, ups=0.18, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.499, loss_scale=16, train_wall=553, gb_free=8.8, wall=40034
2022-02-12 04:56:03 | INFO | train_inner | epoch 005:    924 / 1576 loss=7.44, nll_loss=5.754, ppl=53.97, wps=11734.8, ups=0.18, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.481, loss_scale=16, train_wall=548, gb_free=8.8, wall=40592
2022-02-12 05:05:21 | INFO | train_inner | epoch 005:   1024 / 1576 loss=7.429, nll_loss=5.742, ppl=53.54, wps=11732.6, ups=0.18, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.469, loss_scale=16, train_wall=548, gb_free=8.8, wall=41151
2022-02-12 05:06:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 05:14:45 | INFO | train_inner | epoch 005:   1125 / 1576 loss=7.423, nll_loss=5.735, ppl=53.28, wps=11622.1, ups=0.18, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.487, loss_scale=16, train_wall=553, gb_free=8.8, wall=41715
2022-02-12 05:24:04 | INFO | train_inner | epoch 005:   1225 / 1576 loss=7.439, nll_loss=5.754, ppl=53.95, wps=11733.3, ups=0.18, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.476, loss_scale=16, train_wall=548, gb_free=8.8, wall=42273
2022-02-12 05:30:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 05:33:28 | INFO | train_inner | epoch 005:   1326 / 1576 loss=7.418, nll_loss=5.73, ppl=53.07, wps=11620.9, ups=0.18, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.467, loss_scale=16, train_wall=553, gb_free=8.8, wall=42837
2022-02-12 05:40:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 05:42:52 | INFO | train_inner | epoch 005:   1427 / 1576 loss=7.41, nll_loss=5.72, ppl=52.72, wps=11622, ups=0.18, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.513, loss_scale=8, train_wall=553, gb_free=8.8, wall=43401
2022-02-12 05:52:10 | INFO | train_inner | epoch 005:   1527 / 1576 loss=7.401, nll_loss=5.712, ppl=52.41, wps=11738.7, ups=0.18, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.45, loss_scale=8, train_wall=548, gb_free=8.8, wall=43959
2022-02-12 05:56:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 05:56:45 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.291 | nll_loss 5.566 | ppl 47.36 | wps 32267.8 | wpb 1021.8 | bsz 2 | num_updates 7849 | best_loss 7.291
2022-02-12 05:56:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7849 updates
2022-02-12 05:56:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint5.pt
2022-02-12 05:56:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint5.pt
2022-02-12 05:57:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint5.pt (epoch 5 @ 7849 updates, score 7.291) (writing took 28.209877696819603 seconds)
2022-02-12 05:57:14 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-12 05:57:14 | INFO | train | epoch 005 | loss 7.437 | nll_loss 5.75 | ppl 53.84 | wps 11642.1 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 7849 | lr 0.000356938 | gnorm 0.481 | loss_scale 8 | train_wall 8630 | gb_free 8.8 | wall 44263
2022-02-12 05:57:14 | INFO | fairseq.trainer | begin training epoch 6
2022-02-12 05:57:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 06:01:58 | INFO | train_inner | epoch 006:     51 / 1576 loss=7.354, nll_loss=5.658, ppl=50.48, wps=11035.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=7900, lr=0.000355784, gnorm=0.473, loss_scale=8, train_wall=543, gb_free=8.8, wall=44548
2022-02-12 06:11:17 | INFO | train_inner | epoch 006:    151 / 1576 loss=7.281, nll_loss=5.575, ppl=47.67, wps=11727.9, ups=0.18, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.496, loss_scale=16, train_wall=548, gb_free=8.8, wall=45107
2022-02-12 06:20:36 | INFO | train_inner | epoch 006:    251 / 1576 loss=7.315, nll_loss=5.613, ppl=48.94, wps=11728.4, ups=0.18, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.494, loss_scale=16, train_wall=548, gb_free=8.8, wall=45666
2022-02-12 06:28:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 06:29:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 06:30:06 | INFO | train_inner | epoch 006:    353 / 1576 loss=7.309, nll_loss=5.607, ppl=48.72, wps=11503.5, ups=0.18, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.478, loss_scale=8, train_wall=559, gb_free=8.8, wall=46235
2022-02-12 06:39:24 | INFO | train_inner | epoch 006:    453 / 1576 loss=7.325, nll_loss=5.625, ppl=49.37, wps=11731.5, ups=0.18, wpb=65532.3, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.486, loss_scale=8, train_wall=548, gb_free=8.8, wall=46794
2022-02-12 06:48:43 | INFO | train_inner | epoch 006:    553 / 1576 loss=7.318, nll_loss=5.617, ppl=49.08, wps=11733.8, ups=0.18, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.494, loss_scale=8, train_wall=548, gb_free=8.8, wall=47353
2022-02-12 06:58:02 | INFO | train_inner | epoch 006:    653 / 1576 loss=7.312, nll_loss=5.611, ppl=48.86, wps=11731.9, ups=0.18, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.464, loss_scale=16, train_wall=548, gb_free=8.8, wall=47911
2022-02-12 07:07:20 | INFO | train_inner | epoch 006:    753 / 1576 loss=7.318, nll_loss=5.618, ppl=49.11, wps=11728.9, ups=0.18, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.484, loss_scale=16, train_wall=548, gb_free=8.8, wall=48470
2022-02-12 07:16:39 | INFO | train_inner | epoch 006:    853 / 1576 loss=7.305, nll_loss=5.602, ppl=48.58, wps=11730.6, ups=0.18, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.456, loss_scale=16, train_wall=548, gb_free=8.8, wall=49029
2022-02-12 07:17:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 07:17:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 07:26:08 | INFO | train_inner | epoch 006:    955 / 1576 loss=7.321, nll_loss=5.62, ppl=49.19, wps=11508, ups=0.18, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.489, loss_scale=8, train_wall=559, gb_free=8.8, wall=49598
2022-02-12 07:35:27 | INFO | train_inner | epoch 006:   1055 / 1576 loss=7.304, nll_loss=5.602, ppl=48.56, wps=11731, ups=0.18, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.468, loss_scale=8, train_wall=548, gb_free=8.8, wall=50157
2022-02-12 07:44:45 | INFO | train_inner | epoch 006:   1155 / 1576 loss=7.31, nll_loss=5.609, ppl=48.8, wps=11737.3, ups=0.18, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.464, loss_scale=16, train_wall=548, gb_free=8.8, wall=50715
2022-02-12 07:54:04 | INFO | train_inner | epoch 006:   1255 / 1576 loss=7.301, nll_loss=5.599, ppl=48.46, wps=11735.4, ups=0.18, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.48, loss_scale=16, train_wall=548, gb_free=8.8, wall=51274
2022-02-12 07:59:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 08:03:28 | INFO | train_inner | epoch 006:   1356 / 1576 loss=7.305, nll_loss=5.603, ppl=48.61, wps=11618, ups=0.18, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.482, loss_scale=8, train_wall=553, gb_free=8.8, wall=51838
2022-02-12 08:12:47 | INFO | train_inner | epoch 006:   1456 / 1576 loss=7.297, nll_loss=5.594, ppl=48.29, wps=11733.5, ups=0.18, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.47, loss_scale=8, train_wall=548, gb_free=8.8, wall=52396
2022-02-12 08:22:05 | INFO | train_inner | epoch 006:   1556 / 1576 loss=7.307, nll_loss=5.606, ppl=48.7, wps=11734.9, ups=0.18, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.497, loss_scale=8, train_wall=548, gb_free=8.8, wall=52955
2022-02-12 08:23:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 08:23:59 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.213 | nll_loss 5.497 | ppl 45.15 | wps 32213 | wpb 1021.8 | bsz 2 | num_updates 9420 | best_loss 7.213
2022-02-12 08:23:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9420 updates
2022-02-12 08:23:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint6.pt
2022-02-12 08:24:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint6.pt
2022-02-12 08:24:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint6.pt (epoch 6 @ 9420 updates, score 7.213) (writing took 28.377183055505157 seconds)
2022-02-12 08:24:27 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-12 08:24:27 | INFO | train | epoch 006 | loss 7.308 | nll_loss 5.606 | ppl 48.72 | wps 11648.9 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 9420 | lr 0.000325818 | gnorm 0.48 | loss_scale 16 | train_wall 8630 | gb_free 8.8 | wall 53097
2022-02-12 08:24:27 | INFO | fairseq.trainer | begin training epoch 7
2022-02-12 08:24:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 08:31:54 | INFO | train_inner | epoch 007:     80 / 1576 loss=7.209, nll_loss=5.495, ppl=45.11, wps=11022.1, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=9500, lr=0.000324443, gnorm=0.479, loss_scale=16, train_wall=543, gb_free=8.8, wall=53544
2022-02-12 08:41:13 | INFO | train_inner | epoch 007:    180 / 1576 loss=7.202, nll_loss=5.487, ppl=44.85, wps=11724.1, ups=0.18, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.478, loss_scale=16, train_wall=548, gb_free=8.8, wall=54103
2022-02-12 08:47:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 08:50:38 | INFO | train_inner | epoch 007:    281 / 1576 loss=7.213, nll_loss=5.499, ppl=45.24, wps=11613.7, ups=0.18, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.48, loss_scale=8, train_wall=553, gb_free=8.8, wall=54667
2022-02-12 08:59:56 | INFO | train_inner | epoch 007:    381 / 1576 loss=7.214, nll_loss=5.5, ppl=45.26, wps=11733.5, ups=0.18, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.479, loss_scale=8, train_wall=548, gb_free=8.8, wall=55226
2022-02-12 09:09:14 | INFO | train_inner | epoch 007:    481 / 1576 loss=7.217, nll_loss=5.504, ppl=45.37, wps=11739.4, ups=0.18, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.474, loss_scale=8, train_wall=548, gb_free=8.8, wall=55784
2022-02-12 09:11:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 09:18:39 | INFO | train_inner | epoch 007:    582 / 1576 loss=7.216, nll_loss=5.502, ppl=45.31, wps=11616.8, ups=0.18, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.498, loss_scale=8, train_wall=553, gb_free=8.8, wall=56348
2022-02-12 09:27:57 | INFO | train_inner | epoch 007:    682 / 1576 loss=7.218, nll_loss=5.505, ppl=45.41, wps=11736.2, ups=0.18, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.478, loss_scale=8, train_wall=548, gb_free=8.8, wall=56907
2022-02-12 09:37:15 | INFO | train_inner | epoch 007:    782 / 1576 loss=7.218, nll_loss=5.505, ppl=45.41, wps=11735.2, ups=0.18, wpb=65536, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.492, loss_scale=16, train_wall=548, gb_free=8.8, wall=57465
2022-02-12 09:41:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 09:46:40 | INFO | train_inner | epoch 007:    883 / 1576 loss=7.227, nll_loss=5.516, ppl=45.75, wps=11617.8, ups=0.18, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.477, loss_scale=8, train_wall=553, gb_free=8.8, wall=58029
2022-02-12 09:55:58 | INFO | train_inner | epoch 007:    983 / 1576 loss=7.227, nll_loss=5.516, ppl=45.75, wps=11734.3, ups=0.18, wpb=65532.3, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.462, loss_scale=8, train_wall=548, gb_free=8.8, wall=58588
2022-02-12 10:05:17 | INFO | train_inner | epoch 007:   1083 / 1576 loss=7.218, nll_loss=5.506, ppl=45.44, wps=11734.2, ups=0.18, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.471, loss_scale=16, train_wall=548, gb_free=8.8, wall=59146
2022-02-12 10:11:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 10:14:41 | INFO | train_inner | epoch 007:   1184 / 1576 loss=7.234, nll_loss=5.524, ppl=46, wps=11614.3, ups=0.18, wpb=65536, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.475, loss_scale=8, train_wall=554, gb_free=8.8, wall=59710
2022-02-12 10:23:59 | INFO | train_inner | epoch 007:   1284 / 1576 loss=7.216, nll_loss=5.503, ppl=45.36, wps=11733, ups=0.18, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.473, loss_scale=8, train_wall=548, gb_free=8.8, wall=60269
2022-02-12 10:33:18 | INFO | train_inner | epoch 007:   1384 / 1576 loss=7.229, nll_loss=5.517, ppl=45.8, wps=11733.6, ups=0.18, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.484, loss_scale=8, train_wall=548, gb_free=8.8, wall=60828
2022-02-12 10:39:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 10:42:42 | INFO | train_inner | epoch 007:   1485 / 1576 loss=7.231, nll_loss=5.52, ppl=45.89, wps=11614.3, ups=0.18, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.467, loss_scale=8, train_wall=554, gb_free=8.8, wall=61392
2022-02-12 10:51:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 10:51:12 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.171 | nll_loss 5.385 | ppl 41.79 | wps 32147.1 | wpb 1021.8 | bsz 2 | num_updates 10991 | best_loss 7.171
2022-02-12 10:51:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10991 updates
2022-02-12 10:51:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint7.pt
2022-02-12 10:51:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint7.pt
2022-02-12 10:51:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint7.pt (epoch 7 @ 10991 updates, score 7.171) (writing took 28.33587173279375 seconds)
2022-02-12 10:51:41 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-12 10:51:41 | INFO | train | epoch 007 | loss 7.219 | nll_loss 5.506 | ppl 45.45 | wps 11648.5 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 10991 | lr 0.000301635 | gnorm 0.477 | loss_scale 8 | train_wall 8630 | gb_free 8.8 | wall 61930
2022-02-12 10:51:41 | INFO | fairseq.trainer | begin training epoch 8
2022-02-12 10:51:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 10:51:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 10:52:37 | INFO | train_inner | epoch 008:     10 / 1576 loss=7.222, nll_loss=5.51, ppl=45.56, wps=10929.6, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=11000, lr=0.000301511, gnorm=0.489, loss_scale=4, train_wall=548, gb_free=8.8, wall=61986
2022-02-12 11:01:55 | INFO | train_inner | epoch 008:    110 / 1576 loss=7.134, nll_loss=5.411, ppl=42.54, wps=11739.1, ups=0.18, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.475, loss_scale=4, train_wall=548, gb_free=8.8, wall=62545
2022-02-12 11:11:13 | INFO | train_inner | epoch 008:    210 / 1576 loss=7.116, nll_loss=5.391, ppl=41.95, wps=11738.7, ups=0.18, wpb=65532.3, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.458, loss_scale=4, train_wall=548, gb_free=8.8, wall=63103
2022-02-12 11:20:32 | INFO | train_inner | epoch 008:    310 / 1576 loss=7.137, nll_loss=5.414, ppl=42.63, wps=11733.5, ups=0.18, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.479, loss_scale=8, train_wall=548, gb_free=8.8, wall=63661
2022-02-12 11:29:50 | INFO | train_inner | epoch 008:    410 / 1576 loss=7.153, nll_loss=5.432, ppl=43.18, wps=11733.1, ups=0.18, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.475, loss_scale=8, train_wall=548, gb_free=8.8, wall=64220
2022-02-12 11:39:09 | INFO | train_inner | epoch 008:    510 / 1576 loss=7.146, nll_loss=5.424, ppl=42.94, wps=11738.4, ups=0.18, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.484, loss_scale=8, train_wall=548, gb_free=8.8, wall=64778
2022-02-12 11:41:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 11:48:33 | INFO | train_inner | epoch 008:    611 / 1576 loss=7.153, nll_loss=5.432, ppl=43.17, wps=11618.4, ups=0.18, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.482, loss_scale=8, train_wall=553, gb_free=8.8, wall=65342
2022-02-12 11:57:51 | INFO | train_inner | epoch 008:    711 / 1576 loss=7.153, nll_loss=5.432, ppl=43.17, wps=11733.3, ups=0.18, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.516, loss_scale=8, train_wall=548, gb_free=8.8, wall=65901
2022-02-12 12:07:10 | INFO | train_inner | epoch 008:    811 / 1576 loss=7.147, nll_loss=5.425, ppl=42.97, wps=11732.2, ups=0.18, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.488, loss_scale=16, train_wall=548, gb_free=8.8, wall=66459
2022-02-12 12:13:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 12:16:34 | INFO | train_inner | epoch 008:    912 / 1576 loss=7.162, nll_loss=5.443, ppl=43.49, wps=11611.6, ups=0.18, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.483, loss_scale=8, train_wall=554, gb_free=8.8, wall=67024
2022-02-12 12:25:53 | INFO | train_inner | epoch 008:   1012 / 1576 loss=7.159, nll_loss=5.439, ppl=43.37, wps=11730.8, ups=0.18, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.488, loss_scale=8, train_wall=548, gb_free=8.8, wall=67582
2022-02-12 12:35:11 | INFO | train_inner | epoch 008:   1112 / 1576 loss=7.162, nll_loss=5.442, ppl=43.47, wps=11732.7, ups=0.18, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.467, loss_scale=8, train_wall=548, gb_free=8.8, wall=68141
2022-02-12 12:41:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 12:44:35 | INFO | train_inner | epoch 008:   1213 / 1576 loss=7.16, nll_loss=5.44, ppl=43.42, wps=11617.7, ups=0.18, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.499, loss_scale=8, train_wall=553, gb_free=8.8, wall=68705
2022-02-12 12:53:54 | INFO | train_inner | epoch 008:   1313 / 1576 loss=7.16, nll_loss=5.44, ppl=43.42, wps=11738.1, ups=0.18, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.478, loss_scale=8, train_wall=548, gb_free=8.8, wall=69263
2022-02-12 13:03:12 | INFO | train_inner | epoch 008:   1413 / 1576 loss=7.163, nll_loss=5.444, ppl=43.52, wps=11734.1, ups=0.18, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.48, loss_scale=8, train_wall=548, gb_free=8.8, wall=69822
2022-02-12 13:08:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 13:12:36 | INFO | train_inner | epoch 008:   1514 / 1576 loss=7.165, nll_loss=5.446, ppl=43.58, wps=11618.6, ups=0.18, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.494, loss_scale=8, train_wall=553, gb_free=8.8, wall=70386
2022-02-12 13:18:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 13:18:25 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.115 | nll_loss 5.383 | ppl 41.72 | wps 32336.9 | wpb 1021.8 | bsz 2 | num_updates 12562 | best_loss 7.115
2022-02-12 13:18:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 12562 updates
2022-02-12 13:18:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint8.pt
2022-02-12 13:18:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint8.pt
2022-02-12 13:18:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint8.pt (epoch 8 @ 12562 updates, score 7.115) (writing took 28.32579157873988 seconds)
2022-02-12 13:18:53 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-12 13:18:53 | INFO | train | epoch 008 | loss 7.152 | nll_loss 5.431 | ppl 43.14 | wps 11650.5 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 12562 | lr 0.000282144 | gnorm 0.484 | loss_scale 8 | train_wall 8629 | gb_free 8.8 | wall 70763
2022-02-12 13:18:53 | INFO | fairseq.trainer | begin training epoch 9
2022-02-12 13:18:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 13:22:25 | INFO | train_inner | epoch 009:     38 / 1576 loss=7.126, nll_loss=5.402, ppl=42.28, wps=11030.8, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=12600, lr=0.000281718, gnorm=0.508, loss_scale=8, train_wall=543, gb_free=8.8, wall=70975
2022-02-12 13:31:44 | INFO | train_inner | epoch 009:    138 / 1576 loss=7.064, nll_loss=5.332, ppl=40.27, wps=11736.1, ups=0.18, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.471, loss_scale=8, train_wall=548, gb_free=8.8, wall=71533
2022-02-12 13:38:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 13:41:08 | INFO | train_inner | epoch 009:    239 / 1576 loss=7.066, nll_loss=5.334, ppl=40.32, wps=11606.8, ups=0.18, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.488, loss_scale=8, train_wall=554, gb_free=8.8, wall=72098
2022-02-12 13:50:27 | INFO | train_inner | epoch 009:    339 / 1576 loss=7.089, nll_loss=5.36, ppl=41.08, wps=11734.4, ups=0.18, wpb=65532.3, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.486, loss_scale=8, train_wall=548, gb_free=8.8, wall=72656
2022-02-12 13:59:45 | INFO | train_inner | epoch 009:    439 / 1576 loss=7.099, nll_loss=5.371, ppl=41.39, wps=11738.5, ups=0.18, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.486, loss_scale=8, train_wall=548, gb_free=8.8, wall=73215
2022-02-12 14:05:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 14:09:09 | INFO | train_inner | epoch 009:    540 / 1576 loss=7.092, nll_loss=5.363, ppl=41.16, wps=11615.4, ups=0.18, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.492, loss_scale=8, train_wall=553, gb_free=8.8, wall=73779
2022-02-12 14:18:28 | INFO | train_inner | epoch 009:    640 / 1576 loss=7.104, nll_loss=5.377, ppl=41.55, wps=11738, ups=0.18, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.506, loss_scale=8, train_wall=548, gb_free=8.8, wall=74337
2022-02-12 14:27:46 | INFO | train_inner | epoch 009:    740 / 1576 loss=7.094, nll_loss=5.366, ppl=41.25, wps=11731.5, ups=0.18, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.492, loss_scale=8, train_wall=548, gb_free=8.8, wall=74896
2022-02-12 14:29:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 14:37:10 | INFO | train_inner | epoch 009:    841 / 1576 loss=7.11, nll_loss=5.383, ppl=41.74, wps=11617.6, ups=0.18, wpb=65536, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.48, loss_scale=8, train_wall=553, gb_free=8.8, wall=75460
2022-02-12 14:46:29 | INFO | train_inner | epoch 009:    941 / 1576 loss=7.116, nll_loss=5.39, ppl=41.93, wps=11737.3, ups=0.18, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.511, loss_scale=8, train_wall=548, gb_free=8.8, wall=76018
2022-02-12 14:55:47 | INFO | train_inner | epoch 009:   1041 / 1576 loss=7.112, nll_loss=5.386, ppl=41.82, wps=11730.9, ups=0.18, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.455, loss_scale=16, train_wall=548, gb_free=8.8, wall=76577
2022-02-12 14:56:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 15:05:11 | INFO | train_inner | epoch 009:   1142 / 1576 loss=7.114, nll_loss=5.388, ppl=41.89, wps=11619.1, ups=0.18, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.487, loss_scale=8, train_wall=553, gb_free=8.8, wall=77141
2022-02-12 15:14:30 | INFO | train_inner | epoch 009:   1242 / 1576 loss=7.114, nll_loss=5.388, ppl=41.88, wps=11737.5, ups=0.18, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.494, loss_scale=8, train_wall=548, gb_free=8.8, wall=77699
2022-02-12 15:22:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 15:23:54 | INFO | train_inner | epoch 009:   1343 / 1576 loss=7.113, nll_loss=5.388, ppl=41.86, wps=11618, ups=0.18, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.477, loss_scale=8, train_wall=553, gb_free=8.8, wall=78264
2022-02-12 15:33:12 | INFO | train_inner | epoch 009:   1443 / 1576 loss=7.121, nll_loss=5.396, ppl=42.12, wps=11735.6, ups=0.18, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.473, loss_scale=8, train_wall=548, gb_free=8.8, wall=78822
2022-02-12 15:42:31 | INFO | train_inner | epoch 009:   1543 / 1576 loss=7.107, nll_loss=5.381, ppl=41.66, wps=11738, ups=0.18, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.48, loss_scale=8, train_wall=548, gb_free=8.8, wall=79380
2022-02-12 15:45:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 15:45:37 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.081 | nll_loss 5.323 | ppl 40.02 | wps 32492.2 | wpb 1021.8 | bsz 2 | num_updates 14133 | best_loss 7.081
2022-02-12 15:45:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 14133 updates
2022-02-12 15:45:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint9.pt
2022-02-12 15:45:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint9.pt
2022-02-12 15:46:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint9.pt (epoch 9 @ 14133 updates, score 7.081) (writing took 30.442148462869227 seconds)
2022-02-12 15:46:07 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-12 15:46:07 | INFO | train | epoch 009 | loss 7.1 | nll_loss 5.373 | ppl 41.43 | wps 11647.7 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 14133 | lr 0.000266001 | gnorm 0.487 | loss_scale 8 | train_wall 8629 | gb_free 8.8 | wall 79597
2022-02-12 15:46:07 | INFO | fairseq.trainer | begin training epoch 10
2022-02-12 15:46:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 15:47:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 15:52:27 | INFO | train_inner | epoch 010:     68 / 1576 loss=7.053, nll_loss=5.32, ppl=39.94, wps=10892.4, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=14200, lr=0.000265372, gnorm=0.504, loss_scale=8, train_wall=548, gb_free=8.8, wall=79977
2022-02-12 16:01:46 | INFO | train_inner | epoch 010:    168 / 1576 loss=7.027, nll_loss=5.29, ppl=39.12, wps=11733.5, ups=0.18, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.48, loss_scale=8, train_wall=548, gb_free=8.8, wall=80535
2022-02-12 16:11:04 | INFO | train_inner | epoch 010:    268 / 1576 loss=7.036, nll_loss=5.3, ppl=39.4, wps=11738.2, ups=0.18, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.476, loss_scale=8, train_wall=548, gb_free=8.8, wall=81094
2022-02-12 16:14:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 16:20:28 | INFO | train_inner | epoch 010:    369 / 1576 loss=7.043, nll_loss=5.307, ppl=39.59, wps=11619, ups=0.18, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.534, loss_scale=8, train_wall=553, gb_free=8.8, wall=81658
2022-02-12 16:29:47 | INFO | train_inner | epoch 010:    469 / 1576 loss=7.055, nll_loss=5.322, ppl=39.99, wps=11730.8, ups=0.18, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.471, loss_scale=8, train_wall=548, gb_free=8.8, wall=82216
2022-02-12 16:39:05 | INFO | train_inner | epoch 010:    569 / 1576 loss=7.058, nll_loss=5.325, ppl=40.08, wps=11733.9, ups=0.18, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.481, loss_scale=16, train_wall=548, gb_free=8.8, wall=82775
2022-02-12 16:39:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 16:48:29 | INFO | train_inner | epoch 010:    670 / 1576 loss=7.063, nll_loss=5.33, ppl=40.23, wps=11623.5, ups=0.18, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.498, loss_scale=8, train_wall=553, gb_free=8.8, wall=83339
2022-02-12 16:57:47 | INFO | train_inner | epoch 010:    770 / 1576 loss=7.054, nll_loss=5.321, ppl=39.97, wps=11738.8, ups=0.18, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.478, loss_scale=8, train_wall=548, gb_free=8.8, wall=83897
2022-02-12 17:05:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 17:07:11 | INFO | train_inner | epoch 010:    871 / 1576 loss=7.062, nll_loss=5.33, ppl=40.22, wps=11621.9, ups=0.18, wpb=65536, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.492, loss_scale=8, train_wall=553, gb_free=8.8, wall=84461
2022-02-12 17:16:30 | INFO | train_inner | epoch 010:    971 / 1576 loss=7.068, nll_loss=5.336, ppl=40.39, wps=11735.1, ups=0.18, wpb=65532.3, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.486, loss_scale=8, train_wall=548, gb_free=8.8, wall=85019
2022-02-12 17:25:48 | INFO | train_inner | epoch 010:   1071 / 1576 loss=7.065, nll_loss=5.333, ppl=40.32, wps=11738.4, ups=0.18, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.491, loss_scale=8, train_wall=548, gb_free=8.8, wall=85578
2022-02-12 17:29:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 17:35:12 | INFO | train_inner | epoch 010:   1172 / 1576 loss=7.072, nll_loss=5.341, ppl=40.52, wps=11621.5, ups=0.18, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.498, loss_scale=8, train_wall=553, gb_free=8.8, wall=86141
2022-02-12 17:44:30 | INFO | train_inner | epoch 010:   1272 / 1576 loss=7.072, nll_loss=5.341, ppl=40.54, wps=11734.7, ups=0.18, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.513, loss_scale=8, train_wall=548, gb_free=8.8, wall=86700
2022-02-12 17:53:49 | INFO | train_inner | epoch 010:   1372 / 1576 loss=7.07, nll_loss=5.339, ppl=40.48, wps=11733.7, ups=0.18, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.489, loss_scale=16, train_wall=548, gb_free=8.8, wall=87258
2022-02-12 17:54:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 18:03:13 | INFO | train_inner | epoch 010:   1473 / 1576 loss=7.067, nll_loss=5.336, ppl=40.38, wps=11619.7, ups=0.18, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.48, loss_scale=8, train_wall=553, gb_free=8.8, wall=87822
2022-02-12 18:12:31 | INFO | train_inner | epoch 010:   1573 / 1576 loss=7.076, nll_loss=5.346, ppl=40.68, wps=11737.9, ups=0.18, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.482, loss_scale=8, train_wall=548, gb_free=8.8, wall=88381
2022-02-12 18:12:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 18:12:50 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.061 | nll_loss 5.323 | ppl 40.04 | wps 32261.5 | wpb 1021.8 | bsz 2 | num_updates 15703 | best_loss 7.061
2022-02-12 18:12:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 15703 updates
2022-02-12 18:12:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint10.pt
2022-02-12 18:12:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint10.pt
2022-02-12 18:13:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint10.pt (epoch 10 @ 15703 updates, score 7.061) (writing took 28.748374247923493 seconds)
2022-02-12 18:13:19 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-12 18:13:19 | INFO | train | epoch 010 | loss 7.058 | nll_loss 5.325 | ppl 40.08 | wps 11644.2 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 15703 | lr 0.000252353 | gnorm 0.491 | loss_scale 8 | train_wall 8628 | gb_free 8.8 | wall 88428
2022-02-12 18:13:19 | INFO | fairseq.trainer | begin training epoch 11
2022-02-12 18:13:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 18:18:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 18:22:26 | INFO | train_inner | epoch 011:     98 / 1576 loss=6.987, nll_loss=5.245, ppl=37.91, wps=10916.7, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=15800, lr=0.000251577, gnorm=0.508, loss_scale=8, train_wall=549, gb_free=8.8, wall=88976
2022-02-12 18:31:45 | INFO | train_inner | epoch 011:    198 / 1576 loss=6.983, nll_loss=5.24, ppl=37.8, wps=11732.1, ups=0.18, wpb=65532.3, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.515, loss_scale=8, train_wall=548, gb_free=8.8, wall=89534
2022-02-12 18:41:04 | INFO | train_inner | epoch 011:    298 / 1576 loss=7.003, nll_loss=5.263, ppl=38.4, wps=11729.4, ups=0.18, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.478, loss_scale=8, train_wall=548, gb_free=8.8, wall=90093
2022-02-12 18:42:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 18:50:28 | INFO | train_inner | epoch 011:    399 / 1576 loss=7.009, nll_loss=5.269, ppl=38.57, wps=11618.4, ups=0.18, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.486, loss_scale=8, train_wall=553, gb_free=8.8, wall=90657
2022-02-12 18:59:46 | INFO | train_inner | epoch 011:    499 / 1576 loss=7.011, nll_loss=5.272, ppl=38.65, wps=11735.4, ups=0.18, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.504, loss_scale=8, train_wall=548, gb_free=8.8, wall=91216
2022-02-12 19:06:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 19:09:10 | INFO | train_inner | epoch 011:    600 / 1576 loss=7.029, nll_loss=5.292, ppl=39.17, wps=11616, ups=0.18, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.494, loss_scale=8, train_wall=553, gb_free=8.8, wall=91780
2022-02-12 19:18:29 | INFO | train_inner | epoch 011:    700 / 1576 loss=7.018, nll_loss=5.28, ppl=38.86, wps=11732.6, ups=0.18, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.486, loss_scale=8, train_wall=548, gb_free=8.8, wall=92338
2022-02-12 19:27:47 | INFO | train_inner | epoch 011:    800 / 1576 loss=7.028, nll_loss=5.291, ppl=39.15, wps=11731.4, ups=0.18, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.479, loss_scale=8, train_wall=548, gb_free=8.8, wall=92897
2022-02-12 19:30:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 19:37:12 | INFO | train_inner | epoch 011:    901 / 1576 loss=7.033, nll_loss=5.297, ppl=39.3, wps=11618.7, ups=0.18, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.484, loss_scale=8, train_wall=553, gb_free=8.8, wall=93461
2022-02-12 19:46:30 | INFO | train_inner | epoch 011:   1001 / 1576 loss=7.031, nll_loss=5.295, ppl=39.26, wps=11736.5, ups=0.18, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.484, loss_scale=8, train_wall=548, gb_free=8.8, wall=94020
2022-02-12 19:55:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 19:55:54 | INFO | train_inner | epoch 011:   1102 / 1576 loss=7.036, nll_loss=5.301, ppl=39.41, wps=11615.2, ups=0.18, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.508, loss_scale=8, train_wall=553, gb_free=8.8, wall=94584
2022-02-12 20:05:13 | INFO | train_inner | epoch 011:   1202 / 1576 loss=7.029, nll_loss=5.292, ppl=39.18, wps=11734.7, ups=0.18, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.472, loss_scale=8, train_wall=548, gb_free=8.8, wall=95142
2022-02-12 20:14:31 | INFO | train_inner | epoch 011:   1302 / 1576 loss=7.041, nll_loss=5.305, ppl=39.54, wps=11736.3, ups=0.18, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.499, loss_scale=8, train_wall=548, gb_free=8.8, wall=95701
2022-02-12 20:21:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 20:23:55 | INFO | train_inner | epoch 011:   1403 / 1576 loss=7.042, nll_loss=5.307, ppl=39.6, wps=11615.1, ups=0.18, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.495, loss_scale=8, train_wall=553, gb_free=8.8, wall=96265
2022-02-12 20:33:14 | INFO | train_inner | epoch 011:   1503 / 1576 loss=7.049, nll_loss=5.314, ppl=39.79, wps=11733.6, ups=0.18, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.495, loss_scale=8, train_wall=548, gb_free=8.8, wall=96823
2022-02-12 20:39:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 20:40:04 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.037 | nll_loss 5.275 | ppl 38.71 | wps 32155.9 | wpb 1021.8 | bsz 2 | num_updates 17273 | best_loss 7.037
2022-02-12 20:40:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 17273 updates
2022-02-12 20:40:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint11.pt
2022-02-12 20:40:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint11.pt
2022-02-12 20:40:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint11.pt (epoch 11 @ 17273 updates, score 7.037) (writing took 28.783966562710702 seconds)
2022-02-12 20:40:32 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-12 20:40:32 | INFO | train | epoch 011 | loss 7.023 | nll_loss 5.285 | ppl 38.99 | wps 11641 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 17273 | lr 0.000240611 | gnorm 0.492 | loss_scale 8 | train_wall 8630 | gb_free 8.8 | wall 97262
2022-02-12 20:40:32 | INFO | fairseq.trainer | begin training epoch 12
2022-02-12 20:40:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 20:43:03 | INFO | train_inner | epoch 012:     27 / 1576 loss=7.016, nll_loss=5.278, ppl=38.8, wps=11021, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=17300, lr=0.000240424, gnorm=0.498, loss_scale=8, train_wall=543, gb_free=8.8, wall=97413
2022-02-12 20:47:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 20:52:28 | INFO | train_inner | epoch 012:    128 / 1576 loss=6.947, nll_loss=5.199, ppl=36.73, wps=11609.8, ups=0.18, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.516, loss_scale=8, train_wall=554, gb_free=8.8, wall=97977
2022-02-12 21:01:47 | INFO | train_inner | epoch 012:    228 / 1576 loss=6.958, nll_loss=5.212, ppl=37.06, wps=11717.1, ups=0.18, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.499, loss_scale=8, train_wall=549, gb_free=8.8, wall=98537
2022-02-12 21:11:06 | INFO | train_inner | epoch 012:    328 / 1576 loss=6.969, nll_loss=5.224, ppl=37.39, wps=11731.5, ups=0.18, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.512, loss_scale=8, train_wall=548, gb_free=8.8, wall=99095
2022-02-12 21:12:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 21:20:30 | INFO | train_inner | epoch 012:    429 / 1576 loss=6.979, nll_loss=5.235, ppl=37.66, wps=11611.8, ups=0.18, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.473, loss_scale=8, train_wall=554, gb_free=8.8, wall=99660
2022-02-12 21:29:49 | INFO | train_inner | epoch 012:    529 / 1576 loss=6.986, nll_loss=5.243, ppl=37.87, wps=11731.5, ups=0.18, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.495, loss_scale=8, train_wall=548, gb_free=8.8, wall=100218
2022-02-12 21:36:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 21:39:13 | INFO | train_inner | epoch 012:    630 / 1576 loss=6.998, nll_loss=5.257, ppl=38.24, wps=11616.3, ups=0.18, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.497, loss_scale=8, train_wall=553, gb_free=8.8, wall=100783
2022-02-12 21:48:32 | INFO | train_inner | epoch 012:    730 / 1576 loss=6.991, nll_loss=5.249, ppl=38.02, wps=11730.1, ups=0.18, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.476, loss_scale=8, train_wall=548, gb_free=8.8, wall=101341
2022-02-12 21:53:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 21:57:56 | INFO | train_inner | epoch 012:    831 / 1576 loss=7.005, nll_loss=5.265, ppl=38.44, wps=11619.9, ups=0.18, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.505, loss_scale=4, train_wall=553, gb_free=8.8, wall=101905
2022-02-12 22:07:14 | INFO | train_inner | epoch 012:    931 / 1576 loss=6.994, nll_loss=5.253, ppl=38.14, wps=11734.9, ups=0.18, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.493, loss_scale=4, train_wall=548, gb_free=8.8, wall=102464
2022-02-12 22:16:33 | INFO | train_inner | epoch 012:   1031 / 1576 loss=7.011, nll_loss=5.272, ppl=38.65, wps=11731.7, ups=0.18, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.477, loss_scale=4, train_wall=548, gb_free=8.8, wall=103022
2022-02-12 22:25:51 | INFO | train_inner | epoch 012:   1131 / 1576 loss=7.002, nll_loss=5.261, ppl=38.35, wps=11732.6, ups=0.18, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.5, loss_scale=8, train_wall=548, gb_free=8.8, wall=103581
2022-02-12 22:35:10 | INFO | train_inner | epoch 012:   1231 / 1576 loss=7.011, nll_loss=5.272, ppl=38.63, wps=11735.3, ups=0.18, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.485, loss_scale=8, train_wall=548, gb_free=8.8, wall=104139
2022-02-12 22:44:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 22:44:34 | INFO | train_inner | epoch 012:   1332 / 1576 loss=7.011, nll_loss=5.272, ppl=38.65, wps=11608.8, ups=0.18, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.483, loss_scale=8, train_wall=554, gb_free=8.8, wall=104704
2022-02-12 22:53:53 | INFO | train_inner | epoch 012:   1432 / 1576 loss=7.011, nll_loss=5.272, ppl=38.63, wps=11732, ups=0.18, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.482, loss_scale=8, train_wall=548, gb_free=8.8, wall=105263
2022-02-12 23:03:11 | INFO | train_inner | epoch 012:   1532 / 1576 loss=7.025, nll_loss=5.288, ppl=39.07, wps=11732.1, ups=0.18, wpb=65532.3, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.475, loss_scale=8, train_wall=548, gb_free=8.8, wall=105821
2022-02-12 23:05:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-12 23:07:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 23:07:19 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.018 | nll_loss 5.274 | ppl 38.7 | wps 32267.6 | wpb 1021.8 | bsz 2 | num_updates 18843 | best_loss 7.018
2022-02-12 23:07:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 18843 updates
2022-02-12 23:07:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint12.pt
2022-02-12 23:07:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint12.pt
2022-02-12 23:07:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint12.pt (epoch 12 @ 18843 updates, score 7.018) (writing took 28.27422651834786 seconds)
2022-02-12 23:07:47 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-12 23:07:47 | INFO | train | epoch 012 | loss 6.993 | nll_loss 5.251 | ppl 38.09 | wps 11639.3 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 18843 | lr 0.000230369 | gnorm 0.494 | loss_scale 4 | train_wall 8632 | gb_free 8.8 | wall 106097
2022-02-12 23:07:48 | INFO | fairseq.trainer | begin training epoch 13
2022-02-12 23:07:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 23:13:06 | INFO | train_inner | epoch 013:     57 / 1576 loss=6.956, nll_loss=5.21, ppl=37.01, wps=10931.8, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=18900, lr=0.000230022, gnorm=0.544, loss_scale=4, train_wall=548, gb_free=8.8, wall=106415
2022-02-12 23:22:24 | INFO | train_inner | epoch 013:    157 / 1576 loss=6.921, nll_loss=5.17, ppl=36, wps=11732.2, ups=0.18, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.479, loss_scale=4, train_wall=548, gb_free=8.8, wall=106974
2022-02-12 23:31:43 | INFO | train_inner | epoch 013:    257 / 1576 loss=6.938, nll_loss=5.189, ppl=36.49, wps=11737, ups=0.18, wpb=65532.3, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.485, loss_scale=8, train_wall=548, gb_free=8.8, wall=107532
2022-02-12 23:41:01 | INFO | train_inner | epoch 013:    357 / 1576 loss=6.94, nll_loss=5.192, ppl=36.55, wps=11727.7, ups=0.18, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.495, loss_scale=8, train_wall=548, gb_free=8.8, wall=108091
2022-02-12 23:50:20 | INFO | train_inner | epoch 013:    457 / 1576 loss=6.949, nll_loss=5.202, ppl=36.81, wps=11729.9, ups=0.18, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.494, loss_scale=8, train_wall=548, gb_free=8.8, wall=108650
2022-02-12 23:53:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 23:59:44 | INFO | train_inner | epoch 013:    558 / 1576 loss=6.966, nll_loss=5.22, ppl=37.28, wps=11619.6, ups=0.18, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.487, loss_scale=8, train_wall=553, gb_free=8.8, wall=109214
2022-02-13 00:09:03 | INFO | train_inner | epoch 013:    658 / 1576 loss=6.961, nll_loss=5.215, ppl=37.15, wps=11731, ups=0.18, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.492, loss_scale=8, train_wall=548, gb_free=8.8, wall=109772
2022-02-13 00:17:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 00:18:27 | INFO | train_inner | epoch 013:    759 / 1576 loss=6.972, nll_loss=5.227, ppl=37.46, wps=11616.8, ups=0.18, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.491, loss_scale=8, train_wall=553, gb_free=8.8, wall=110337
2022-02-13 00:27:46 | INFO | train_inner | epoch 013:    859 / 1576 loss=6.98, nll_loss=5.237, ppl=37.71, wps=11731.3, ups=0.18, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.514, loss_scale=8, train_wall=548, gb_free=8.8, wall=110895
2022-02-13 00:37:04 | INFO | train_inner | epoch 013:    959 / 1576 loss=6.977, nll_loss=5.234, ppl=37.63, wps=11728.7, ups=0.18, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.498, loss_scale=8, train_wall=548, gb_free=8.8, wall=111454
2022-02-13 00:43:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 00:46:29 | INFO | train_inner | epoch 013:   1060 / 1576 loss=6.984, nll_loss=5.241, ppl=37.82, wps=11613.2, ups=0.18, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.511, loss_scale=8, train_wall=554, gb_free=8.8, wall=112018
2022-02-13 00:55:47 | INFO | train_inner | epoch 013:   1160 / 1576 loss=6.981, nll_loss=5.238, ppl=37.74, wps=11729, ups=0.18, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.479, loss_scale=8, train_wall=548, gb_free=8.8, wall=112577
2022-02-13 01:05:06 | INFO | train_inner | epoch 013:   1260 / 1576 loss=6.984, nll_loss=5.241, ppl=37.82, wps=11727.7, ups=0.18, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.49, loss_scale=8, train_wall=548, gb_free=8.8, wall=113136
2022-02-13 01:07:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 01:14:30 | INFO | train_inner | epoch 013:   1361 / 1576 loss=6.989, nll_loss=5.248, ppl=37.99, wps=11620.1, ups=0.18, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.489, loss_scale=8, train_wall=553, gb_free=8.8, wall=113700
2022-02-13 01:23:49 | INFO | train_inner | epoch 013:   1461 / 1576 loss=6.994, nll_loss=5.253, ppl=38.12, wps=11731.2, ups=0.18, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.501, loss_scale=8, train_wall=548, gb_free=8.8, wall=114259
2022-02-13 01:27:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 01:33:13 | INFO | train_inner | epoch 013:   1562 / 1576 loss=6.996, nll_loss=5.256, ppl=38.2, wps=11622.6, ups=0.18, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.514, loss_scale=4, train_wall=553, gb_free=8.8, wall=114822
2022-02-13 01:34:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 01:34:33 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.003 | nll_loss 5.248 | ppl 38.01 | wps 32096.5 | wpb 1021.8 | bsz 2 | num_updates 20414 | best_loss 7.003
2022-02-13 01:34:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 20414 updates
2022-02-13 01:34:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint13.pt
2022-02-13 01:34:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint13.pt
2022-02-13 01:35:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint13.pt (epoch 13 @ 20414 updates, score 7.003) (writing took 28.445169602520764 seconds)
2022-02-13 01:35:01 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-13 01:35:01 | INFO | train | epoch 013 | loss 6.967 | nll_loss 5.222 | ppl 37.33 | wps 11648.1 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 20414 | lr 0.000221328 | gnorm 0.496 | loss_scale 4 | train_wall 8631 | gb_free 8.8 | wall 114931
2022-02-13 01:35:02 | INFO | fairseq.trainer | begin training epoch 14
2022-02-13 01:35:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 01:43:02 | INFO | train_inner | epoch 014:     86 / 1576 loss=6.912, nll_loss=5.16, ppl=35.75, wps=11030.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=20500, lr=0.000220863, gnorm=0.53, loss_scale=4, train_wall=543, gb_free=8.8, wall=115411
2022-02-13 01:52:20 | INFO | train_inner | epoch 014:    186 / 1576 loss=6.909, nll_loss=5.157, ppl=35.67, wps=11731.1, ups=0.18, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.483, loss_scale=8, train_wall=548, gb_free=8.8, wall=115970
2022-02-13 02:01:39 | INFO | train_inner | epoch 014:    286 / 1576 loss=6.909, nll_loss=5.157, ppl=35.67, wps=11732.2, ups=0.18, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.521, loss_scale=8, train_wall=548, gb_free=8.8, wall=116529
2022-02-13 02:06:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 02:11:03 | INFO | train_inner | epoch 014:    387 / 1576 loss=6.925, nll_loss=5.174, ppl=36.11, wps=11617.9, ups=0.18, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.535, loss_scale=4, train_wall=553, gb_free=8.8, wall=117093
2022-02-13 02:20:22 | INFO | train_inner | epoch 014:    487 / 1576 loss=6.934, nll_loss=5.185, ppl=36.38, wps=11734.1, ups=0.18, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.476, loss_scale=4, train_wall=548, gb_free=8.8, wall=117651
2022-02-13 02:29:40 | INFO | train_inner | epoch 014:    587 / 1576 loss=6.944, nll_loss=5.195, ppl=36.64, wps=11730.4, ups=0.18, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.5, loss_scale=4, train_wall=548, gb_free=8.8, wall=118210
2022-02-13 02:38:59 | INFO | train_inner | epoch 014:    687 / 1576 loss=6.94, nll_loss=5.192, ppl=36.55, wps=11734, ups=0.18, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.507, loss_scale=8, train_wall=548, gb_free=8.8, wall=118768
2022-02-13 02:48:17 | INFO | train_inner | epoch 014:    787 / 1576 loss=6.946, nll_loss=5.198, ppl=36.72, wps=11730, ups=0.18, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.489, loss_scale=8, train_wall=548, gb_free=8.8, wall=119327
2022-02-13 02:54:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 02:57:42 | INFO | train_inner | epoch 014:    888 / 1576 loss=6.959, nll_loss=5.213, ppl=37.09, wps=11611.9, ups=0.18, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.501, loss_scale=8, train_wall=554, gb_free=8.8, wall=119892
2022-02-13 03:07:01 | INFO | train_inner | epoch 014:    988 / 1576 loss=6.96, nll_loss=5.214, ppl=37.11, wps=11727.3, ups=0.18, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.479, loss_scale=8, train_wall=548, gb_free=8.8, wall=120450
2022-02-13 03:16:19 | INFO | train_inner | epoch 014:   1088 / 1576 loss=6.959, nll_loss=5.213, ppl=37.1, wps=11734.2, ups=0.18, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.497, loss_scale=8, train_wall=548, gb_free=8.8, wall=121009
2022-02-13 03:19:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 03:25:44 | INFO | train_inner | epoch 014:   1189 / 1576 loss=6.953, nll_loss=5.206, ppl=36.92, wps=11613.7, ups=0.18, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.496, loss_scale=8, train_wall=554, gb_free=8.8, wall=121573
2022-02-13 03:35:02 | INFO | train_inner | epoch 014:   1289 / 1576 loss=6.963, nll_loss=5.218, ppl=37.22, wps=11728.4, ups=0.18, wpb=65532.3, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.486, loss_scale=8, train_wall=548, gb_free=8.8, wall=122132
2022-02-13 03:44:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 03:44:27 | INFO | train_inner | epoch 014:   1390 / 1576 loss=6.967, nll_loss=5.222, ppl=37.32, wps=11611.6, ups=0.18, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.49, loss_scale=8, train_wall=554, gb_free=8.8, wall=122696
2022-02-13 03:53:45 | INFO | train_inner | epoch 014:   1490 / 1576 loss=6.967, nll_loss=5.222, ppl=37.32, wps=11729.8, ups=0.18, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.505, loss_scale=8, train_wall=548, gb_free=8.8, wall=123255
2022-02-13 04:01:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 04:01:48 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.994 | nll_loss 5.235 | ppl 37.66 | wps 32366.9 | wpb 1021.8 | bsz 2 | num_updates 21986 | best_loss 6.994
2022-02-13 04:01:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 21986 updates
2022-02-13 04:01:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint14.pt
2022-02-13 04:01:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint14.pt
2022-02-13 04:02:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint14.pt (epoch 14 @ 21986 updates, score 6.994) (writing took 29.15853377338499 seconds)
2022-02-13 04:02:17 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-13 04:02:17 | INFO | train | epoch 014 | loss 6.944 | nll_loss 5.197 | ppl 36.67 | wps 11653.6 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 21986 | lr 0.000213269 | gnorm 0.5 | loss_scale 8 | train_wall 8632 | gb_free 8.8 | wall 123767
2022-02-13 04:02:17 | INFO | fairseq.trainer | begin training epoch 15
2022-02-13 04:02:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 04:03:35 | INFO | train_inner | epoch 015:     14 / 1576 loss=6.957, nll_loss=5.212, ppl=37.05, wps=11016.9, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=22000, lr=0.000213201, gnorm=0.505, loss_scale=8, train_wall=543, gb_free=8.8, wall=123845
2022-02-13 04:09:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 04:12:59 | INFO | train_inner | epoch 015:    115 / 1576 loss=6.882, nll_loss=5.126, ppl=34.91, wps=11620.5, ups=0.18, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.5, loss_scale=8, train_wall=553, gb_free=8.8, wall=124409
2022-02-13 04:22:18 | INFO | train_inner | epoch 015:    215 / 1576 loss=6.894, nll_loss=5.139, ppl=35.24, wps=11733, ups=0.18, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.49, loss_scale=8, train_wall=548, gb_free=8.8, wall=124967
2022-02-13 04:31:36 | INFO | train_inner | epoch 015:    315 / 1576 loss=6.894, nll_loss=5.14, ppl=35.25, wps=11734.3, ups=0.18, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.509, loss_scale=8, train_wall=548, gb_free=8.8, wall=125526
2022-02-13 04:33:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 04:41:00 | INFO | train_inner | epoch 015:    416 / 1576 loss=6.892, nll_loss=5.137, ppl=35.19, wps=11621.7, ups=0.18, wpb=65532.3, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.512, loss_scale=8, train_wall=553, gb_free=8.8, wall=126090
2022-02-13 04:50:18 | INFO | train_inner | epoch 015:    516 / 1576 loss=6.915, nll_loss=5.163, ppl=35.82, wps=11736, ups=0.18, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.509, loss_scale=8, train_wall=548, gb_free=8.8, wall=126648
2022-02-13 04:57:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 04:59:43 | INFO | train_inner | epoch 015:    617 / 1576 loss=6.917, nll_loss=5.166, ppl=35.89, wps=11616.3, ups=0.18, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.512, loss_scale=8, train_wall=553, gb_free=8.8, wall=127212
2022-02-13 05:09:01 | INFO | train_inner | epoch 015:    717 / 1576 loss=6.931, nll_loss=5.182, ppl=36.29, wps=11731.6, ups=0.18, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.487, loss_scale=8, train_wall=548, gb_free=8.8, wall=127771
2022-02-13 05:18:20 | INFO | train_inner | epoch 015:    817 / 1576 loss=6.94, nll_loss=5.191, ppl=36.53, wps=11733.6, ups=0.18, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.499, loss_scale=8, train_wall=548, gb_free=8.8, wall=128329
2022-02-13 05:22:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 05:27:44 | INFO | train_inner | epoch 015:    918 / 1576 loss=6.929, nll_loss=5.179, ppl=36.22, wps=11621.3, ups=0.18, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.486, loss_scale=8, train_wall=553, gb_free=8.8, wall=128893
2022-02-13 05:37:02 | INFO | train_inner | epoch 015:   1018 / 1576 loss=6.939, nll_loss=5.19, ppl=36.51, wps=11740.6, ups=0.18, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.518, loss_scale=8, train_wall=548, gb_free=8.8, wall=129451
2022-02-13 05:46:20 | INFO | train_inner | epoch 015:   1118 / 1576 loss=6.943, nll_loss=5.194, ppl=36.62, wps=11735.9, ups=0.18, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.506, loss_scale=16, train_wall=548, gb_free=8.8, wall=130010
2022-02-13 05:48:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 05:55:44 | INFO | train_inner | epoch 015:   1219 / 1576 loss=6.951, nll_loss=5.204, ppl=36.86, wps=11616.8, ups=0.18, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.499, loss_scale=8, train_wall=553, gb_free=8.8, wall=130574
2022-02-13 06:05:03 | INFO | train_inner | epoch 015:   1319 / 1576 loss=6.94, nll_loss=5.192, ppl=36.55, wps=11738.4, ups=0.18, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.491, loss_scale=8, train_wall=548, gb_free=8.8, wall=131132
2022-02-13 06:13:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 06:14:27 | INFO | train_inner | epoch 015:   1420 / 1576 loss=6.943, nll_loss=5.195, ppl=36.63, wps=11621.3, ups=0.18, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.513, loss_scale=8, train_wall=553, gb_free=8.8, wall=131696
2022-02-13 06:23:45 | INFO | train_inner | epoch 015:   1520 / 1576 loss=6.952, nll_loss=5.206, ppl=36.92, wps=11738.8, ups=0.18, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.501, loss_scale=8, train_wall=548, gb_free=8.8, wall=132255
2022-02-13 06:28:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 06:29:00 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.984 | nll_loss 5.215 | ppl 37.13 | wps 32252.1 | wpb 1021.8 | bsz 2 | num_updates 23556 | best_loss 6.984
2022-02-13 06:29:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 23556 updates
2022-02-13 06:29:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint15.pt
2022-02-13 06:29:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint15.pt
2022-02-13 06:29:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint15.pt (epoch 15 @ 23556 updates, score 6.984) (writing took 29.029610575176775 seconds)
2022-02-13 06:29:29 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-13 06:29:29 | INFO | train | epoch 015 | loss 6.925 | nll_loss 5.174 | ppl 36.11 | wps 11643.5 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 23556 | lr 0.000206039 | gnorm 0.501 | loss_scale 8 | train_wall 8629 | gb_free 8.8 | wall 132598
2022-02-13 06:29:29 | INFO | fairseq.trainer | begin training epoch 16
2022-02-13 06:29:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 06:33:34 | INFO | train_inner | epoch 016:     44 / 1576 loss=6.913, nll_loss=5.162, ppl=35.8, wps=11019.9, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=23600, lr=0.000205847, gnorm=0.49, loss_scale=8, train_wall=543, gb_free=8.8, wall=132844
2022-02-13 06:37:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 06:42:58 | INFO | train_inner | epoch 016:    145 / 1576 loss=6.86, nll_loss=5.101, ppl=34.33, wps=11623.7, ups=0.18, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.491, loss_scale=8, train_wall=553, gb_free=8.8, wall=133408
2022-02-13 06:52:17 | INFO | train_inner | epoch 016:    245 / 1576 loss=6.864, nll_loss=5.105, ppl=34.42, wps=11735, ups=0.18, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.507, loss_scale=8, train_wall=548, gb_free=8.8, wall=133966
2022-02-13 07:01:35 | INFO | train_inner | epoch 016:    345 / 1576 loss=6.887, nll_loss=5.132, ppl=35.06, wps=11732.8, ups=0.18, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.492, loss_scale=16, train_wall=548, gb_free=8.8, wall=134525
2022-02-13 07:02:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 07:03:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 07:11:04 | INFO | train_inner | epoch 016:    447 / 1576 loss=6.894, nll_loss=5.14, ppl=35.26, wps=11513.7, ups=0.18, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.505, loss_scale=4, train_wall=558, gb_free=8.8, wall=135094
2022-02-13 07:20:22 | INFO | train_inner | epoch 016:    547 / 1576 loss=6.894, nll_loss=5.14, ppl=35.26, wps=11745.9, ups=0.18, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.489, loss_scale=4, train_wall=547, gb_free=8.8, wall=135652
2022-02-13 07:29:40 | INFO | train_inner | epoch 016:    647 / 1576 loss=6.913, nll_loss=5.161, ppl=35.77, wps=11743.9, ups=0.18, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.503, loss_scale=8, train_wall=547, gb_free=8.8, wall=136210
2022-02-13 07:38:59 | INFO | train_inner | epoch 016:    747 / 1576 loss=6.913, nll_loss=5.161, ppl=35.79, wps=11738.3, ups=0.18, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.497, loss_scale=8, train_wall=548, gb_free=8.8, wall=136768
2022-02-13 07:48:17 | INFO | train_inner | epoch 016:    847 / 1576 loss=6.91, nll_loss=5.158, ppl=35.7, wps=11738.3, ups=0.18, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.507, loss_scale=8, train_wall=548, gb_free=8.8, wall=137327
2022-02-13 07:52:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 07:57:41 | INFO | train_inner | epoch 016:    948 / 1576 loss=6.92, nll_loss=5.169, ppl=35.99, wps=11622.5, ups=0.18, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.502, loss_scale=8, train_wall=553, gb_free=8.8, wall=137891
2022-02-13 08:06:59 | INFO | train_inner | epoch 016:   1048 / 1576 loss=6.929, nll_loss=5.18, ppl=36.25, wps=11737.3, ups=0.18, wpb=65532.3, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.496, loss_scale=8, train_wall=548, gb_free=8.8, wall=138449
2022-02-13 08:16:18 | INFO | train_inner | epoch 016:   1148 / 1576 loss=6.928, nll_loss=5.178, ppl=36.19, wps=11736, ups=0.18, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.508, loss_scale=8, train_wall=548, gb_free=8.8, wall=139007
2022-02-13 08:16:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 08:25:42 | INFO | train_inner | epoch 016:   1249 / 1576 loss=6.919, nll_loss=5.168, ppl=35.96, wps=11624.3, ups=0.18, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.501, loss_scale=8, train_wall=553, gb_free=8.8, wall=139571
2022-02-13 08:35:00 | INFO | train_inner | epoch 016:   1349 / 1576 loss=6.93, nll_loss=5.181, ppl=36.27, wps=11739.4, ups=0.18, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.505, loss_scale=8, train_wall=548, gb_free=8.8, wall=140129
2022-02-13 08:40:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 08:44:23 | INFO | train_inner | epoch 016:   1450 / 1576 loss=6.937, nll_loss=5.189, ppl=36.47, wps=11627.2, ups=0.18, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.503, loss_scale=8, train_wall=553, gb_free=8.8, wall=140693
2022-02-13 08:45:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 08:53:47 | INFO | train_inner | epoch 016:   1551 / 1576 loss=6.939, nll_loss=5.191, ppl=36.53, wps=11633.5, ups=0.18, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.545, loss_scale=4, train_wall=553, gb_free=8.8, wall=141256
2022-02-13 08:56:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 08:56:08 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.98 | nll_loss 5.193 | ppl 36.57 | wps 32327.2 | wpb 1021.8 | bsz 2 | num_updates 25125 | best_loss 6.98
2022-02-13 08:56:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 25125 updates
2022-02-13 08:56:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint16.pt
2022-02-13 08:56:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint16.pt
2022-02-13 08:56:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint16.pt (epoch 16 @ 25125 updates, score 6.98) (writing took 29.23040180467069 seconds)
2022-02-13 08:56:37 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-13 08:56:37 | INFO | train | epoch 016 | loss 6.908 | nll_loss 5.156 | ppl 35.64 | wps 11640.4 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 25125 | lr 0.000199502 | gnorm 0.503 | loss_scale 4 | train_wall 8625 | gb_free 8.8 | wall 141427
2022-02-13 08:56:37 | INFO | fairseq.trainer | begin training epoch 17
2022-02-13 08:56:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 09:03:36 | INFO | train_inner | epoch 017:     75 / 1576 loss=6.861, nll_loss=5.103, ppl=34.36, wps=11033.3, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=25200, lr=0.000199205, gnorm=0.509, loss_scale=4, train_wall=542, gb_free=8.8, wall=141845
2022-02-13 09:12:53 | INFO | train_inner | epoch 017:    175 / 1576 loss=6.847, nll_loss=5.086, ppl=33.97, wps=11754, ups=0.18, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.493, loss_scale=8, train_wall=547, gb_free=8.8, wall=142403
2022-02-13 09:22:11 | INFO | train_inner | epoch 017:    275 / 1576 loss=6.867, nll_loss=5.109, ppl=34.51, wps=11739.7, ups=0.18, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.493, loss_scale=8, train_wall=548, gb_free=8.8, wall=142961
2022-02-13 09:27:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 09:31:35 | INFO | train_inner | epoch 017:    376 / 1576 loss=6.87, nll_loss=5.112, ppl=34.59, wps=11628.8, ups=0.18, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.539, loss_scale=4, train_wall=553, gb_free=8.8, wall=143525
2022-02-13 09:40:52 | INFO | train_inner | epoch 017:    476 / 1576 loss=6.874, nll_loss=5.117, ppl=34.71, wps=11760.9, ups=0.18, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.511, loss_scale=4, train_wall=547, gb_free=8.8, wall=144082
2022-02-13 09:50:09 | INFO | train_inner | epoch 017:    576 / 1576 loss=6.895, nll_loss=5.141, ppl=35.28, wps=11759.9, ups=0.18, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.503, loss_scale=4, train_wall=547, gb_free=8.8, wall=144639
2022-02-13 09:56:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 09:59:33 | INFO | train_inner | epoch 017:    677 / 1576 loss=6.89, nll_loss=5.135, ppl=35.13, wps=11633.1, ups=0.18, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.501, loss_scale=4, train_wall=553, gb_free=8.8, wall=145202
2022-02-13 10:08:50 | INFO | train_inner | epoch 017:    777 / 1576 loss=6.899, nll_loss=5.146, ppl=35.4, wps=11759.7, ups=0.18, wpb=65536, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.496, loss_scale=4, train_wall=547, gb_free=8.8, wall=145760
2022-02-13 10:18:07 | INFO | train_inner | epoch 017:    877 / 1576 loss=6.899, nll_loss=5.145, ppl=35.39, wps=11759.8, ups=0.18, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.487, loss_scale=4, train_wall=547, gb_free=8.8, wall=146317
2022-02-13 10:27:25 | INFO | train_inner | epoch 017:    977 / 1576 loss=6.903, nll_loss=5.15, ppl=35.5, wps=11752.6, ups=0.18, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.516, loss_scale=8, train_wall=547, gb_free=8.8, wall=146875
2022-02-13 10:36:43 | INFO | train_inner | epoch 017:   1077 / 1576 loss=6.913, nll_loss=5.161, ppl=35.78, wps=11742.8, ups=0.18, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.519, loss_scale=8, train_wall=547, gb_free=8.8, wall=147433
2022-02-13 10:45:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 10:46:07 | INFO | train_inner | epoch 017:   1178 / 1576 loss=6.909, nll_loss=5.156, ppl=35.66, wps=11626.1, ups=0.18, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.501, loss_scale=8, train_wall=553, gb_free=8.8, wall=147996
2022-02-13 10:55:25 | INFO | train_inner | epoch 017:   1278 / 1576 loss=6.913, nll_loss=5.161, ppl=35.79, wps=11746.6, ups=0.18, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.517, loss_scale=8, train_wall=547, gb_free=8.8, wall=148554
2022-02-13 10:55:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 11:04:48 | INFO | train_inner | epoch 017:   1379 / 1576 loss=6.911, nll_loss=5.159, ppl=35.73, wps=11637.2, ups=0.18, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.53, loss_scale=4, train_wall=552, gb_free=8.8, wall=149118
2022-02-13 11:14:05 | INFO | train_inner | epoch 017:   1479 / 1576 loss=6.92, nll_loss=5.17, ppl=36.01, wps=11754.4, ups=0.18, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.502, loss_scale=4, train_wall=547, gb_free=8.8, wall=149675
2022-02-13 11:23:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 11:23:08 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.974 | nll_loss 5.187 | ppl 36.42 | wps 32258.8 | wpb 1021.8 | bsz 2 | num_updates 26697 | best_loss 6.974
2022-02-13 11:23:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 26697 updates
2022-02-13 11:23:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint17.pt
2022-02-13 11:23:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint17.pt
2022-02-13 11:23:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint17.pt (epoch 17 @ 26697 updates, score 6.974) (writing took 28.853803641162813 seconds)
2022-02-13 11:23:37 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-13 11:23:37 | INFO | train | epoch 017 | loss 6.892 | nll_loss 5.138 | ppl 35.21 | wps 11674.4 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 26697 | lr 0.000193539 | gnorm 0.508 | loss_scale 8 | train_wall 8616 | gb_free 8.8 | wall 150247
2022-02-13 11:23:37 | INFO | fairseq.trainer | begin training epoch 18
2022-02-13 11:23:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 11:23:54 | INFO | train_inner | epoch 018:      3 / 1576 loss=6.915, nll_loss=5.164, ppl=35.84, wps=11038.3, ups=0.17, wpb=64958.8, bsz=126.9, num_updates=26700, lr=0.000193528, gnorm=0.523, loss_scale=8, train_wall=542, gb_free=8.8, wall=150264
2022-02-13 11:33:11 | INFO | train_inner | epoch 018:    103 / 1576 loss=6.837, nll_loss=5.075, ppl=33.7, wps=11753.7, ups=0.18, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.504, loss_scale=8, train_wall=547, gb_free=8.8, wall=150821
2022-02-13 11:42:29 | INFO | train_inner | epoch 018:    203 / 1576 loss=6.842, nll_loss=5.08, ppl=33.84, wps=11750.6, ups=0.18, wpb=65536, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.511, loss_scale=8, train_wall=547, gb_free=8.8, wall=151379
2022-02-13 11:46:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 11:51:52 | INFO | train_inner | epoch 018:    304 / 1576 loss=6.859, nll_loss=5.1, ppl=34.3, wps=11637, ups=0.18, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.495, loss_scale=8, train_wall=552, gb_free=8.8, wall=151942
2022-02-13 12:01:10 | INFO | train_inner | epoch 018:    404 / 1576 loss=6.853, nll_loss=5.093, ppl=34.13, wps=11756.6, ups=0.18, wpb=65536, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.501, loss_scale=8, train_wall=547, gb_free=8.8, wall=152499
2022-02-13 12:10:27 | INFO | train_inner | epoch 018:    504 / 1576 loss=6.854, nll_loss=5.095, ppl=34.17, wps=11754.9, ups=0.18, wpb=65532.3, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.527, loss_scale=16, train_wall=547, gb_free=8.8, wall=153057
2022-02-13 12:10:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 12:19:50 | INFO | train_inner | epoch 018:    605 / 1576 loss=6.861, nll_loss=5.102, ppl=34.35, wps=11640.1, ups=0.18, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.507, loss_scale=8, train_wall=552, gb_free=8.8, wall=153620
2022-02-13 12:29:08 | INFO | train_inner | epoch 018:    705 / 1576 loss=6.877, nll_loss=5.121, ppl=34.8, wps=11755.4, ups=0.18, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.507, loss_scale=8, train_wall=547, gb_free=8.8, wall=154177
2022-02-13 12:35:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 12:38:31 | INFO | train_inner | epoch 018:    806 / 1576 loss=6.88, nll_loss=5.124, ppl=34.87, wps=11640.1, ups=0.18, wpb=65536, bsz=128, num_updates=27500, lr=0.000190693, gnorm=0.502, loss_scale=8, train_wall=552, gb_free=8.8, wall=154741
2022-02-13 12:47:48 | INFO | train_inner | epoch 018:    906 / 1576 loss=6.894, nll_loss=5.14, ppl=35.27, wps=11759.6, ups=0.18, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.493, loss_scale=8, train_wall=547, gb_free=8.8, wall=155298
2022-02-13 12:57:06 | INFO | train_inner | epoch 018:   1006 / 1576 loss=6.889, nll_loss=5.135, ppl=35.13, wps=11756.6, ups=0.18, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.522, loss_scale=8, train_wall=547, gb_free=8.8, wall=155855
2022-02-13 12:59:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 13:06:29 | INFO | train_inner | epoch 018:   1107 / 1576 loss=6.896, nll_loss=5.142, ppl=35.32, wps=11635, ups=0.18, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.513, loss_scale=8, train_wall=553, gb_free=8.8, wall=156419
2022-02-13 13:15:46 | INFO | train_inner | epoch 018:   1207 / 1576 loss=6.898, nll_loss=5.145, ppl=35.38, wps=11754.4, ups=0.18, wpb=65536, bsz=128, num_updates=27900, lr=0.000189321, gnorm=0.499, loss_scale=8, train_wall=547, gb_free=8.8, wall=156976
2022-02-13 13:23:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 13:25:09 | INFO | train_inner | epoch 018:   1308 / 1576 loss=6.898, nll_loss=5.144, ppl=35.37, wps=11640.1, ups=0.18, wpb=65536, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.515, loss_scale=8, train_wall=552, gb_free=8.8, wall=157539
2022-02-13 13:34:27 | INFO | train_inner | epoch 018:   1408 / 1576 loss=6.909, nll_loss=5.157, ppl=35.67, wps=11745.6, ups=0.18, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.522, loss_scale=8, train_wall=547, gb_free=8.8, wall=158097
2022-02-13 13:43:45 | INFO | train_inner | epoch 018:   1508 / 1576 loss=6.913, nll_loss=5.161, ppl=35.78, wps=11750.1, ups=0.18, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.507, loss_scale=8, train_wall=547, gb_free=8.8, wall=158655
2022-02-13 13:48:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 13:49:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 13:50:06 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.964 | nll_loss 5.179 | ppl 36.22 | wps 32465.6 | wpb 1021.8 | bsz 2 | num_updates 28267 | best_loss 6.964
2022-02-13 13:50:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 28267 updates
2022-02-13 13:50:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint18.pt
2022-02-13 13:50:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint18.pt
2022-02-13 13:50:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint18.pt (epoch 18 @ 28267 updates, score 6.964) (writing took 28.94460408296436 seconds)
2022-02-13 13:50:35 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-13 13:50:35 | INFO | train | epoch 018 | loss 6.879 | nll_loss 5.122 | ppl 34.83 | wps 11661.7 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 28267 | lr 0.000188088 | gnorm 0.508 | loss_scale 8 | train_wall 8616 | gb_free 8.8 | wall 159065
2022-02-13 13:50:35 | INFO | fairseq.trainer | begin training epoch 19
2022-02-13 13:50:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 13:53:39 | INFO | train_inner | epoch 019:     33 / 1576 loss=6.882, nll_loss=5.127, ppl=34.94, wps=10936.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=28300, lr=0.000187978, gnorm=0.507, loss_scale=8, train_wall=548, gb_free=8.8, wall=159249
2022-02-13 13:57:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 14:03:02 | INFO | train_inner | epoch 019:    134 / 1576 loss=6.829, nll_loss=5.066, ppl=33.5, wps=11643, ups=0.18, wpb=65536, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.5, loss_scale=4, train_wall=552, gb_free=8.8, wall=159812
2022-02-13 14:12:19 | INFO | train_inner | epoch 019:    234 / 1576 loss=6.826, nll_loss=5.063, ppl=33.42, wps=11762.8, ups=0.18, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.511, loss_scale=4, train_wall=547, gb_free=8.8, wall=160369
2022-02-13 14:21:37 | INFO | train_inner | epoch 019:    334 / 1576 loss=6.829, nll_loss=5.066, ppl=33.49, wps=11756.7, ups=0.18, wpb=65536, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.498, loss_scale=8, train_wall=547, gb_free=8.8, wall=160926
2022-02-13 14:30:54 | INFO | train_inner | epoch 019:    434 / 1576 loss=6.849, nll_loss=5.089, ppl=34.04, wps=11758.1, ups=0.18, wpb=65536, bsz=128, num_updates=28700, lr=0.000186663, gnorm=0.499, loss_scale=8, train_wall=547, gb_free=8.8, wall=161484
2022-02-13 14:34:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 14:40:17 | INFO | train_inner | epoch 019:    535 / 1576 loss=6.859, nll_loss=5.1, ppl=34.3, wps=11642.5, ups=0.18, wpb=65536, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.53, loss_scale=4, train_wall=552, gb_free=8.8, wall=162047
2022-02-13 14:49:34 | INFO | train_inner | epoch 019:    635 / 1576 loss=6.866, nll_loss=5.108, ppl=34.5, wps=11761.6, ups=0.18, wpb=65536, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.537, loss_scale=4, train_wall=547, gb_free=8.8, wall=162604
2022-02-13 14:58:51 | INFO | train_inner | epoch 019:    735 / 1576 loss=6.869, nll_loss=5.112, ppl=34.58, wps=11760.5, ups=0.18, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=0.519, loss_scale=8, train_wall=547, gb_free=8.8, wall=163161
2022-02-13 15:08:09 | INFO | train_inner | epoch 019:    835 / 1576 loss=6.874, nll_loss=5.117, ppl=34.7, wps=11758, ups=0.18, wpb=65536, bsz=128, num_updates=29100, lr=0.000185376, gnorm=0.509, loss_scale=8, train_wall=547, gb_free=8.8, wall=163718
2022-02-13 15:17:26 | INFO | train_inner | epoch 019:    935 / 1576 loss=6.889, nll_loss=5.135, ppl=35.14, wps=11756.4, ups=0.18, wpb=65536, bsz=128, num_updates=29200, lr=0.000185058, gnorm=0.506, loss_scale=8, train_wall=547, gb_free=8.8, wall=164276
2022-02-13 15:19:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 15:26:49 | INFO | train_inner | epoch 019:   1036 / 1576 loss=6.87, nll_loss=5.113, ppl=34.6, wps=11644.6, ups=0.18, wpb=65536, bsz=128, num_updates=29300, lr=0.000184742, gnorm=0.518, loss_scale=4, train_wall=552, gb_free=8.8, wall=164839
2022-02-13 15:36:06 | INFO | train_inner | epoch 019:   1136 / 1576 loss=6.878, nll_loss=5.122, ppl=34.82, wps=11758.6, ups=0.18, wpb=65532.3, bsz=128, num_updates=29400, lr=0.000184428, gnorm=0.509, loss_scale=4, train_wall=547, gb_free=8.8, wall=165396
2022-02-13 15:45:24 | INFO | train_inner | epoch 019:   1236 / 1576 loss=6.883, nll_loss=5.128, ppl=34.96, wps=11756.4, ups=0.18, wpb=65536, bsz=128, num_updates=29500, lr=0.000184115, gnorm=0.5, loss_scale=8, train_wall=547, gb_free=8.8, wall=165953
2022-02-13 15:46:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 15:54:47 | INFO | train_inner | epoch 019:   1337 / 1576 loss=6.897, nll_loss=5.143, ppl=35.35, wps=11639.1, ups=0.18, wpb=65536, bsz=128, num_updates=29600, lr=0.000183804, gnorm=0.513, loss_scale=4, train_wall=552, gb_free=8.8, wall=166516
2022-02-13 16:04:04 | INFO | train_inner | epoch 019:   1437 / 1576 loss=6.889, nll_loss=5.135, ppl=35.13, wps=11755.8, ups=0.18, wpb=65536, bsz=128, num_updates=29700, lr=0.000183494, gnorm=0.498, loss_scale=4, train_wall=547, gb_free=8.8, wall=167074
2022-02-13 16:13:22 | INFO | train_inner | epoch 019:   1537 / 1576 loss=6.899, nll_loss=5.146, ppl=35.4, wps=11754.8, ups=0.18, wpb=65536, bsz=128, num_updates=29800, lr=0.000183186, gnorm=0.507, loss_scale=8, train_wall=547, gb_free=8.8, wall=167631
2022-02-13 16:16:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 16:17:01 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.956 | nll_loss 5.178 | ppl 36.21 | wps 32174.1 | wpb 1021.8 | bsz 2 | num_updates 29839 | best_loss 6.956
2022-02-13 16:17:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 29839 updates
2022-02-13 16:17:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint19.pt
2022-02-13 16:17:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint19.pt
2022-02-13 16:17:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint19.pt (epoch 19 @ 29839 updates, score 6.956) (writing took 29.037384434603155 seconds)
2022-02-13 16:17:30 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-13 16:17:30 | INFO | train | epoch 019 | loss 6.867 | nll_loss 5.109 | ppl 34.52 | wps 11680.5 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 29839 | lr 0.000183066 | gnorm 0.511 | loss_scale 8 | train_wall 8613 | gb_free 8.8 | wall 167880
2022-02-13 16:17:30 | INFO | fairseq.trainer | begin training epoch 20
2022-02-13 16:17:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 16:23:10 | INFO | train_inner | epoch 020:     61 / 1576 loss=6.842, nll_loss=5.081, ppl=33.86, wps=11042.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=29900, lr=0.000182879, gnorm=0.523, loss_scale=8, train_wall=542, gb_free=8.8, wall=168220
2022-02-13 16:32:27 | INFO | train_inner | epoch 020:    161 / 1576 loss=6.81, nll_loss=5.045, ppl=33.01, wps=11757.9, ups=0.18, wpb=65536, bsz=128, num_updates=30000, lr=0.000182574, gnorm=0.494, loss_scale=8, train_wall=547, gb_free=8.8, wall=168777
2022-02-13 16:35:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 16:41:50 | INFO | train_inner | epoch 020:    262 / 1576 loss=6.826, nll_loss=5.062, ppl=33.41, wps=11641.6, ups=0.18, wpb=65536, bsz=128, num_updates=30100, lr=0.000182271, gnorm=0.506, loss_scale=8, train_wall=552, gb_free=8.8, wall=169340
2022-02-13 16:51:08 | INFO | train_inner | epoch 020:    362 / 1576 loss=6.824, nll_loss=5.061, ppl=33.38, wps=11756.8, ups=0.18, wpb=65536, bsz=128, num_updates=30200, lr=0.000181969, gnorm=0.506, loss_scale=8, train_wall=547, gb_free=8.8, wall=169898
2022-02-13 16:54:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 17:00:31 | INFO | train_inner | epoch 020:    463 / 1576 loss=6.838, nll_loss=5.076, ppl=33.73, wps=11641.9, ups=0.18, wpb=65536, bsz=128, num_updates=30300, lr=0.000181668, gnorm=0.506, loss_scale=4, train_wall=552, gb_free=8.8, wall=170460
2022-02-13 17:09:48 | INFO | train_inner | epoch 020:    563 / 1576 loss=6.846, nll_loss=5.085, ppl=33.94, wps=11761.9, ups=0.18, wpb=65536, bsz=128, num_updates=30400, lr=0.000181369, gnorm=0.527, loss_scale=4, train_wall=547, gb_free=8.8, wall=171018
2022-02-13 17:19:06 | INFO | train_inner | epoch 020:    663 / 1576 loss=6.853, nll_loss=5.093, ppl=34.14, wps=11753.5, ups=0.18, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=0.5, loss_scale=8, train_wall=547, gb_free=8.8, wall=171575
2022-02-13 17:28:23 | INFO | train_inner | epoch 020:    763 / 1576 loss=6.864, nll_loss=5.106, ppl=34.44, wps=11757, ups=0.18, wpb=65536, bsz=128, num_updates=30600, lr=0.000180775, gnorm=0.51, loss_scale=8, train_wall=547, gb_free=8.8, wall=172133
2022-02-13 17:33:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 17:37:46 | INFO | train_inner | epoch 020:    864 / 1576 loss=6.861, nll_loss=5.102, ppl=34.35, wps=11643.1, ups=0.18, wpb=65532.3, bsz=128, num_updates=30700, lr=0.000180481, gnorm=0.517, loss_scale=4, train_wall=552, gb_free=8.8, wall=172696
2022-02-13 17:47:03 | INFO | train_inner | epoch 020:    964 / 1576 loss=6.862, nll_loss=5.104, ppl=34.38, wps=11763.4, ups=0.18, wpb=65536, bsz=128, num_updates=30800, lr=0.000180187, gnorm=0.503, loss_scale=4, train_wall=547, gb_free=8.8, wall=173253
2022-02-13 17:57:45 | INFO | train_inner | epoch 020:   1064 / 1576 loss=6.874, nll_loss=5.118, ppl=34.73, wps=11768.3, ups=0.18, wpb=65536, bsz=128, num_updates=30900, lr=0.000179896, gnorm=0.503, loss_scale=4, train_wall=546, gb_free=8.8, wall=173894
2022-02-13 18:03:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 18:07:07 | INFO | train_inner | epoch 020:   1165 / 1576 loss=6.866, nll_loss=5.108, ppl=34.48, wps=11642.3, ups=0.18, wpb=65536, bsz=128, num_updates=31000, lr=0.000179605, gnorm=0.527, loss_scale=4, train_wall=552, gb_free=8.8, wall=174457
2022-02-13 18:16:25 | INFO | train_inner | epoch 020:   1265 / 1576 loss=6.877, nll_loss=5.121, ppl=34.81, wps=11753.5, ups=0.18, wpb=65536, bsz=128, num_updates=31100, lr=0.000179316, gnorm=0.511, loss_scale=4, train_wall=547, gb_free=8.8, wall=175015
2022-02-13 18:25:43 | INFO | train_inner | epoch 020:   1365 / 1576 loss=6.887, nll_loss=5.133, ppl=35.08, wps=11753.5, ups=0.18, wpb=65536, bsz=128, num_updates=31200, lr=0.000179029, gnorm=0.503, loss_scale=4, train_wall=547, gb_free=8.8, wall=175572
2022-02-13 18:35:00 | INFO | train_inner | epoch 020:   1465 / 1576 loss=6.88, nll_loss=5.124, ppl=34.88, wps=11754, ups=0.18, wpb=65536, bsz=128, num_updates=31300, lr=0.000178743, gnorm=0.502, loss_scale=8, train_wall=547, gb_free=8.8, wall=176130
2022-02-13 18:41:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 18:44:23 | INFO | train_inner | epoch 020:   1566 / 1576 loss=6.889, nll_loss=5.134, ppl=35.12, wps=11641.2, ups=0.18, wpb=65536, bsz=128, num_updates=31400, lr=0.000178458, gnorm=0.516, loss_scale=4, train_wall=552, gb_free=8.8, wall=176693
2022-02-13 18:45:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 18:45:21 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.949 | nll_loss 5.193 | ppl 36.59 | wps 32289.1 | wpb 1021.8 | bsz 2 | num_updates 31410 | best_loss 6.949
2022-02-13 18:45:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 31410 updates
2022-02-13 18:45:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint20.pt
2022-02-13 18:45:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint20.pt
2022-02-13 18:45:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint20.pt (epoch 20 @ 31410 updates, score 6.949) (writing took 29.119639948941767 seconds)
2022-02-13 18:45:50 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-13 18:45:50 | INFO | train | epoch 020 | loss 6.856 | nll_loss 5.096 | ppl 34.21 | wps 11562 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 31410 | lr 0.000178429 | gnorm 0.51 | loss_scale 4 | train_wall 8613 | gb_free 8.8 | wall 176780
2022-02-13 18:45:50 | INFO | fairseq.trainer | begin training epoch 21
2022-02-13 18:45:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 18:54:12 | INFO | train_inner | epoch 021:     90 / 1576 loss=6.796, nll_loss=5.03, ppl=32.66, wps=11034.9, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=31500, lr=0.000178174, gnorm=0.517, loss_scale=4, train_wall=542, gb_free=8.8, wall=177282
2022-02-13 19:03:29 | INFO | train_inner | epoch 021:    190 / 1576 loss=6.807, nll_loss=5.041, ppl=32.93, wps=11756.8, ups=0.18, wpb=65536, bsz=128, num_updates=31600, lr=0.000177892, gnorm=0.518, loss_scale=4, train_wall=547, gb_free=8.8, wall=177839
2022-02-13 19:05:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 19:12:52 | INFO | train_inner | epoch 021:    291 / 1576 loss=6.809, nll_loss=5.044, ppl=32.99, wps=11646.2, ups=0.18, wpb=65536, bsz=128, num_updates=31700, lr=0.000177611, gnorm=0.526, loss_scale=4, train_wall=552, gb_free=8.8, wall=178402
2022-02-13 19:22:10 | INFO | train_inner | epoch 021:    391 / 1576 loss=6.821, nll_loss=5.058, ppl=33.3, wps=11755.4, ups=0.18, wpb=65536, bsz=128, num_updates=31800, lr=0.000177332, gnorm=0.514, loss_scale=4, train_wall=547, gb_free=8.8, wall=178959
2022-02-13 19:31:27 | INFO | train_inner | epoch 021:    491 / 1576 loss=6.829, nll_loss=5.066, ppl=33.5, wps=11754.4, ups=0.18, wpb=65536, bsz=128, num_updates=31900, lr=0.000177054, gnorm=0.521, loss_scale=8, train_wall=547, gb_free=8.8, wall=179517
2022-02-13 19:40:44 | INFO | train_inner | epoch 021:    591 / 1576 loss=6.836, nll_loss=5.074, ppl=33.68, wps=11758.7, ups=0.18, wpb=65536, bsz=128, num_updates=32000, lr=0.000176777, gnorm=0.517, loss_scale=8, train_wall=547, gb_free=8.8, wall=180074
2022-02-13 19:50:02 | INFO | train_inner | epoch 021:    691 / 1576 loss=6.844, nll_loss=5.084, ppl=33.91, wps=11758.8, ups=0.18, wpb=65536, bsz=128, num_updates=32100, lr=0.000176501, gnorm=0.52, loss_scale=8, train_wall=547, gb_free=8.8, wall=180631
2022-02-13 19:53:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 19:59:25 | INFO | train_inner | epoch 021:    792 / 1576 loss=6.849, nll_loss=5.09, ppl=34.05, wps=11644.8, ups=0.18, wpb=65536, bsz=128, num_updates=32200, lr=0.000176227, gnorm=0.505, loss_scale=8, train_wall=552, gb_free=8.8, wall=181194
2022-02-13 20:08:42 | INFO | train_inner | epoch 021:    892 / 1576 loss=6.859, nll_loss=5.1, ppl=34.29, wps=11756.4, ups=0.18, wpb=65536, bsz=128, num_updates=32300, lr=0.000175954, gnorm=0.508, loss_scale=8, train_wall=547, gb_free=8.8, wall=181752
2022-02-13 20:17:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 20:18:05 | INFO | train_inner | epoch 021:    993 / 1576 loss=6.859, nll_loss=5.101, ppl=34.32, wps=11643.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=32400, lr=0.000175682, gnorm=0.508, loss_scale=8, train_wall=552, gb_free=8.8, wall=182315
2022-02-13 20:21:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 20:27:28 | INFO | train_inner | epoch 021:   1094 / 1576 loss=6.864, nll_loss=5.106, ppl=34.43, wps=11641.8, ups=0.18, wpb=65536, bsz=128, num_updates=32500, lr=0.000175412, gnorm=0.544, loss_scale=4, train_wall=552, gb_free=8.8, wall=182877
2022-02-13 20:36:45 | INFO | train_inner | epoch 021:   1194 / 1576 loss=6.868, nll_loss=5.111, ppl=34.57, wps=11763.3, ups=0.18, wpb=65536, bsz=128, num_updates=32600, lr=0.000175142, gnorm=0.528, loss_scale=4, train_wall=547, gb_free=8.8, wall=183435
2022-02-13 20:46:02 | INFO | train_inner | epoch 021:   1294 / 1576 loss=6.872, nll_loss=5.115, ppl=34.65, wps=11758.2, ups=0.18, wpb=65536, bsz=128, num_updates=32700, lr=0.000174874, gnorm=0.505, loss_scale=8, train_wall=547, gb_free=8.8, wall=183992
2022-02-13 20:46:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 20:55:25 | INFO | train_inner | epoch 021:   1395 / 1576 loss=6.876, nll_loss=5.121, ppl=34.79, wps=11640, ups=0.18, wpb=65536, bsz=128, num_updates=32800, lr=0.000174608, gnorm=0.511, loss_scale=4, train_wall=552, gb_free=8.8, wall=184555
2022-02-13 21:04:43 | INFO | train_inner | epoch 021:   1495 / 1576 loss=6.873, nll_loss=5.117, ppl=34.69, wps=11758, ups=0.18, wpb=65536, bsz=128, num_updates=32900, lr=0.000174342, gnorm=0.513, loss_scale=4, train_wall=547, gb_free=8.8, wall=185112
2022-02-13 21:12:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 21:12:16 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.947 | nll_loss 5.17 | ppl 35.99 | wps 32270.6 | wpb 1021.8 | bsz 2 | num_updates 32981 | best_loss 6.947
2022-02-13 21:12:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 32981 updates
2022-02-13 21:12:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint21.pt
2022-02-13 21:12:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint21.pt
2022-02-13 21:12:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint21.pt (epoch 21 @ 32981 updates, score 6.947) (writing took 29.210546666756272 seconds)
2022-02-13 21:12:45 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-13 21:12:45 | INFO | train | epoch 021 | loss 6.845 | nll_loss 5.085 | ppl 33.94 | wps 11672.9 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 32981 | lr 0.000174128 | gnorm 0.517 | loss_scale 8 | train_wall 8613 | gb_free 8.8 | wall 185595
2022-02-13 21:12:45 | INFO | fairseq.trainer | begin training epoch 22
2022-02-13 21:12:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 21:14:31 | INFO | train_inner | epoch 022:     19 / 1576 loss=6.86, nll_loss=5.101, ppl=34.33, wps=11038.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=33000, lr=0.000174078, gnorm=0.524, loss_scale=8, train_wall=542, gb_free=8.8, wall=185701
2022-02-13 21:16:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 21:23:54 | INFO | train_inner | epoch 022:    120 / 1576 loss=6.778, nll_loss=5.008, ppl=32.18, wps=11641.9, ups=0.18, wpb=65536, bsz=128, num_updates=33100, lr=0.000173814, gnorm=0.519, loss_scale=4, train_wall=552, gb_free=8.8, wall=186264
2022-02-13 21:33:12 | INFO | train_inner | epoch 022:    220 / 1576 loss=6.797, nll_loss=5.03, ppl=32.67, wps=11748.9, ups=0.18, wpb=65536, bsz=128, num_updates=33200, lr=0.000173553, gnorm=0.515, loss_scale=4, train_wall=547, gb_free=8.8, wall=186822
2022-02-13 21:42:29 | INFO | train_inner | epoch 022:    320 / 1576 loss=6.805, nll_loss=5.039, ppl=32.88, wps=11755.9, ups=0.18, wpb=65536, bsz=128, num_updates=33300, lr=0.000173292, gnorm=0.534, loss_scale=8, train_wall=547, gb_free=8.8, wall=187379
2022-02-13 21:48:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 21:51:53 | INFO | train_inner | epoch 022:    421 / 1576 loss=6.821, nll_loss=5.058, ppl=33.31, wps=11635.2, ups=0.18, wpb=65536, bsz=128, num_updates=33400, lr=0.000173032, gnorm=0.527, loss_scale=4, train_wall=553, gb_free=8.8, wall=187942
2022-02-13 22:01:10 | INFO | train_inner | epoch 022:    521 / 1576 loss=6.825, nll_loss=5.062, ppl=33.4, wps=11755.4, ups=0.18, wpb=65536, bsz=128, num_updates=33500, lr=0.000172774, gnorm=0.523, loss_scale=4, train_wall=547, gb_free=8.8, wall=188500
2022-02-13 22:10:28 | INFO | train_inner | epoch 022:    621 / 1576 loss=6.825, nll_loss=5.062, ppl=33.42, wps=11756.5, ups=0.18, wpb=65536, bsz=128, num_updates=33600, lr=0.000172516, gnorm=0.511, loss_scale=4, train_wall=547, gb_free=8.8, wall=189057
2022-02-13 22:14:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 22:19:51 | INFO | train_inner | epoch 022:    722 / 1576 loss=6.835, nll_loss=5.073, ppl=33.67, wps=11640.5, ups=0.18, wpb=65536, bsz=128, num_updates=33700, lr=0.00017226, gnorm=0.535, loss_scale=4, train_wall=552, gb_free=8.8, wall=189620
2022-02-13 22:29:08 | INFO | train_inner | epoch 022:    822 / 1576 loss=6.847, nll_loss=5.087, ppl=34, wps=11752.1, ups=0.18, wpb=65536, bsz=128, num_updates=33800, lr=0.000172005, gnorm=0.517, loss_scale=4, train_wall=547, gb_free=8.8, wall=190178
2022-02-13 22:38:26 | INFO | train_inner | epoch 022:    922 / 1576 loss=6.844, nll_loss=5.083, ppl=33.9, wps=11750.6, ups=0.18, wpb=65536, bsz=128, num_updates=33900, lr=0.000171751, gnorm=0.526, loss_scale=8, train_wall=547, gb_free=8.8, wall=190736
2022-02-13 22:46:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 22:47:49 | INFO | train_inner | epoch 022:   1023 / 1576 loss=6.844, nll_loss=5.084, ppl=33.92, wps=11634.5, ups=0.18, wpb=65536, bsz=128, num_updates=34000, lr=0.000171499, gnorm=0.517, loss_scale=4, train_wall=553, gb_free=8.8, wall=191299
2022-02-13 22:57:07 | INFO | train_inner | epoch 022:   1123 / 1576 loss=6.855, nll_loss=5.097, ppl=34.22, wps=11758.3, ups=0.18, wpb=65536, bsz=128, num_updates=34100, lr=0.000171247, gnorm=0.529, loss_scale=4, train_wall=547, gb_free=8.8, wall=191856
2022-02-13 23:06:24 | INFO | train_inner | epoch 022:   1223 / 1576 loss=6.859, nll_loss=5.1, ppl=34.3, wps=11756, ups=0.18, wpb=65536, bsz=128, num_updates=34200, lr=0.000170996, gnorm=0.514, loss_scale=4, train_wall=547, gb_free=8.8, wall=192414
2022-02-13 23:13:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-13 23:15:47 | INFO | train_inner | epoch 022:   1324 / 1576 loss=6.861, nll_loss=5.103, ppl=34.37, wps=11641, ups=0.18, wpb=65536, bsz=128, num_updates=34300, lr=0.000170747, gnorm=0.526, loss_scale=4, train_wall=552, gb_free=8.8, wall=192977
2022-02-13 23:25:05 | INFO | train_inner | epoch 022:   1424 / 1576 loss=6.862, nll_loss=5.104, ppl=34.39, wps=11756.9, ups=0.18, wpb=65536, bsz=128, num_updates=34400, lr=0.000170499, gnorm=0.494, loss_scale=4, train_wall=547, gb_free=8.8, wall=193534
2022-02-13 23:34:22 | INFO | train_inner | epoch 022:   1524 / 1576 loss=6.861, nll_loss=5.103, ppl=34.37, wps=11749.3, ups=0.18, wpb=65532.3, bsz=128, num_updates=34500, lr=0.000170251, gnorm=0.535, loss_scale=4, train_wall=547, gb_free=8.8, wall=194092
2022-02-13 23:39:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 23:39:14 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.941 | nll_loss 5.173 | ppl 36.08 | wps 32314.4 | wpb 1021.8 | bsz 2 | num_updates 34552 | best_loss 6.941
2022-02-13 23:39:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 34552 updates
2022-02-13 23:39:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint22.pt
2022-02-13 23:39:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint22.pt
2022-02-13 23:39:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint22.pt (epoch 22 @ 34552 updates, score 6.941) (writing took 29.26020563673228 seconds)
2022-02-13 23:39:44 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-13 23:39:44 | INFO | train | epoch 022 | loss 6.835 | nll_loss 5.074 | ppl 33.67 | wps 11668.7 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 34552 | lr 0.000170123 | gnorm 0.521 | loss_scale 8 | train_wall 8615 | gb_free 8.8 | wall 194413
2022-02-13 23:39:44 | INFO | fairseq.trainer | begin training epoch 23
2022-02-13 23:39:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 23:44:11 | INFO | train_inner | epoch 023:     48 / 1576 loss=6.817, nll_loss=5.053, ppl=33.2, wps=11026.9, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=34600, lr=0.000170005, gnorm=0.516, loss_scale=8, train_wall=542, gb_free=8.8, wall=194681
2022-02-13 23:53:29 | INFO | train_inner | epoch 023:    148 / 1576 loss=6.774, nll_loss=5.004, ppl=32.09, wps=11745.4, ups=0.18, wpb=65536, bsz=128, num_updates=34700, lr=0.00016976, gnorm=0.531, loss_scale=8, train_wall=547, gb_free=8.8, wall=195239
2022-02-14 00:01:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 00:02:53 | INFO | train_inner | epoch 023:    249 / 1576 loss=6.801, nll_loss=5.034, ppl=32.77, wps=11637.5, ups=0.18, wpb=65536, bsz=128, num_updates=34800, lr=0.000169516, gnorm=0.511, loss_scale=8, train_wall=552, gb_free=8.8, wall=195802
2022-02-14 00:03:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 00:12:15 | INFO | train_inner | epoch 023:    350 / 1576 loss=6.802, nll_loss=5.036, ppl=32.81, wps=11644, ups=0.18, wpb=65536, bsz=128, num_updates=34900, lr=0.000169273, gnorm=0.516, loss_scale=4, train_wall=552, gb_free=8.8, wall=196365
2022-02-14 00:21:33 | INFO | train_inner | epoch 023:    450 / 1576 loss=6.816, nll_loss=5.051, ppl=33.15, wps=11757.6, ups=0.18, wpb=65536, bsz=128, num_updates=35000, lr=0.000169031, gnorm=0.538, loss_scale=4, train_wall=547, gb_free=8.8, wall=196922
2022-02-14 00:30:50 | INFO | train_inner | epoch 023:    550 / 1576 loss=6.803, nll_loss=5.037, ppl=32.82, wps=11757.6, ups=0.18, wpb=65536, bsz=128, num_updates=35100, lr=0.00016879, gnorm=0.511, loss_scale=8, train_wall=547, gb_free=8.8, wall=197480
2022-02-14 00:38:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 00:40:13 | INFO | train_inner | epoch 023:    651 / 1576 loss=6.822, nll_loss=5.059, ppl=33.33, wps=11640, ups=0.18, wpb=65536, bsz=128, num_updates=35200, lr=0.00016855, gnorm=0.514, loss_scale=4, train_wall=552, gb_free=8.8, wall=198043
2022-02-14 00:49:30 | INFO | train_inner | epoch 023:    751 / 1576 loss=6.824, nll_loss=5.061, ppl=33.37, wps=11761.6, ups=0.18, wpb=65536, bsz=128, num_updates=35300, lr=0.000168311, gnorm=0.536, loss_scale=4, train_wall=547, gb_free=8.8, wall=198600
2022-02-14 00:58:48 | INFO | train_inner | epoch 023:    851 / 1576 loss=6.831, nll_loss=5.069, ppl=33.58, wps=11758.3, ups=0.18, wpb=65536, bsz=128, num_updates=35400, lr=0.000168073, gnorm=0.509, loss_scale=4, train_wall=547, gb_free=8.8, wall=199157
2022-02-14 01:08:05 | INFO | train_inner | epoch 023:    951 / 1576 loss=6.841, nll_loss=5.08, ppl=33.82, wps=11753.7, ups=0.18, wpb=65536, bsz=128, num_updates=35500, lr=0.000167836, gnorm=0.513, loss_scale=8, train_wall=547, gb_free=8.8, wall=199715
2022-02-14 01:14:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 01:17:28 | INFO | train_inner | epoch 023:   1052 / 1576 loss=6.843, nll_loss=5.082, ppl=33.87, wps=11636.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=35600, lr=0.0001676, gnorm=0.522, loss_scale=4, train_wall=553, gb_free=8.8, wall=200278
2022-02-14 01:26:46 | INFO | train_inner | epoch 023:   1152 / 1576 loss=6.843, nll_loss=5.083, ppl=33.89, wps=11754.6, ups=0.18, wpb=65536, bsz=128, num_updates=35700, lr=0.000167365, gnorm=0.53, loss_scale=4, train_wall=547, gb_free=8.8, wall=200836
2022-02-14 01:36:04 | INFO | train_inner | epoch 023:   1252 / 1576 loss=6.855, nll_loss=5.097, ppl=34.21, wps=11754.8, ups=0.18, wpb=65536, bsz=128, num_updates=35800, lr=0.000167132, gnorm=0.527, loss_scale=4, train_wall=547, gb_free=8.8, wall=201393
2022-02-14 01:41:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 01:45:27 | INFO | train_inner | epoch 023:   1353 / 1576 loss=6.851, nll_loss=5.092, ppl=34.11, wps=11641.5, ups=0.18, wpb=65536, bsz=128, num_updates=35900, lr=0.000166899, gnorm=0.515, loss_scale=4, train_wall=552, gb_free=8.8, wall=201956
2022-02-14 01:54:44 | INFO | train_inner | epoch 023:   1453 / 1576 loss=6.856, nll_loss=5.097, ppl=34.22, wps=11754.2, ups=0.18, wpb=65536, bsz=128, num_updates=36000, lr=0.000166667, gnorm=0.529, loss_scale=4, train_wall=547, gb_free=8.8, wall=202514
2022-02-14 02:04:02 | INFO | train_inner | epoch 023:   1553 / 1576 loss=6.857, nll_loss=5.098, ppl=34.25, wps=11755.3, ups=0.18, wpb=65536, bsz=128, num_updates=36100, lr=0.000166436, gnorm=0.525, loss_scale=4, train_wall=547, gb_free=8.8, wall=203071
2022-02-14 02:05:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 02:06:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 02:06:12 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.936 | nll_loss 5.154 | ppl 35.61 | wps 32190.4 | wpb 1021.8 | bsz 2 | num_updates 36122 | best_loss 6.936
2022-02-14 02:06:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 36122 updates
2022-02-14 02:06:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint23.pt
2022-02-14 02:06:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint23.pt
2022-02-14 02:06:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint23.pt (epoch 23 @ 36122 updates, score 6.936) (writing took 28.872624539770186 seconds)
2022-02-14 02:06:41 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-14 02:06:41 | INFO | train | epoch 023 | loss 6.826 | nll_loss 5.064 | ppl 33.44 | wps 11663 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 36122 | lr 0.000166385 | gnorm 0.523 | loss_scale 4 | train_wall 8615 | gb_free 8.8 | wall 203230
2022-02-14 02:06:41 | INFO | fairseq.trainer | begin training epoch 24
2022-02-14 02:06:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 02:13:55 | INFO | train_inner | epoch 024:     78 / 1576 loss=6.791, nll_loss=5.024, ppl=32.54, wps=10937.7, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=36200, lr=0.000166206, gnorm=0.539, loss_scale=4, train_wall=547, gb_free=8.8, wall=203665
2022-02-14 02:23:13 | INFO | train_inner | epoch 024:    178 / 1576 loss=6.775, nll_loss=5.005, ppl=32.11, wps=11758.1, ups=0.18, wpb=65536, bsz=128, num_updates=36300, lr=0.000165977, gnorm=0.518, loss_scale=4, train_wall=547, gb_free=8.8, wall=204223
2022-02-14 02:32:30 | INFO | train_inner | epoch 024:    278 / 1576 loss=6.785, nll_loss=5.017, ppl=32.38, wps=11757.9, ups=0.18, wpb=65536, bsz=128, num_updates=36400, lr=0.000165748, gnorm=0.513, loss_scale=8, train_wall=547, gb_free=8.8, wall=204780
2022-02-14 02:41:48 | INFO | train_inner | epoch 024:    378 / 1576 loss=6.789, nll_loss=5.021, ppl=32.47, wps=11752.5, ups=0.18, wpb=65536, bsz=128, num_updates=36500, lr=0.000165521, gnorm=0.523, loss_scale=8, train_wall=547, gb_free=8.8, wall=205338
2022-02-14 02:50:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 02:51:11 | INFO | train_inner | epoch 024:    479 / 1576 loss=6.801, nll_loss=5.035, ppl=32.78, wps=11638.3, ups=0.18, wpb=65536, bsz=128, num_updates=36600, lr=0.000165295, gnorm=0.515, loss_scale=4, train_wall=552, gb_free=8.8, wall=205901
2022-02-14 03:00:28 | INFO | train_inner | epoch 024:    579 / 1576 loss=6.817, nll_loss=5.053, ppl=33.19, wps=11756.2, ups=0.18, wpb=65536, bsz=128, num_updates=36700, lr=0.00016507, gnorm=0.533, loss_scale=4, train_wall=547, gb_free=8.8, wall=206458
2022-02-14 03:09:46 | INFO | train_inner | epoch 024:    679 / 1576 loss=6.817, nll_loss=5.052, ppl=33.18, wps=11757.8, ups=0.18, wpb=65536, bsz=128, num_updates=36800, lr=0.000164845, gnorm=0.52, loss_scale=4, train_wall=547, gb_free=8.8, wall=207015
2022-02-14 03:19:03 | INFO | train_inner | epoch 024:    779 / 1576 loss=6.82, nll_loss=5.057, ppl=33.28, wps=11754.5, ups=0.18, wpb=65532.3, bsz=128, num_updates=36900, lr=0.000164622, gnorm=0.521, loss_scale=8, train_wall=547, gb_free=8.8, wall=207573
2022-02-14 03:20:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 03:28:26 | INFO | train_inner | epoch 024:    880 / 1576 loss=6.819, nll_loss=5.055, ppl=33.24, wps=11639.8, ups=0.18, wpb=65536, bsz=128, num_updates=37000, lr=0.000164399, gnorm=0.552, loss_scale=4, train_wall=552, gb_free=8.8, wall=208136
2022-02-14 03:37:44 | INFO | train_inner | epoch 024:    980 / 1576 loss=6.829, nll_loss=5.066, ppl=33.5, wps=11758.3, ups=0.18, wpb=65536, bsz=128, num_updates=37100, lr=0.000164177, gnorm=0.514, loss_scale=4, train_wall=547, gb_free=8.8, wall=208693
2022-02-14 03:44:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 03:47:07 | INFO | train_inner | epoch 024:   1081 / 1576 loss=6.837, nll_loss=5.075, ppl=33.71, wps=11639.5, ups=0.18, wpb=65536, bsz=128, num_updates=37200, lr=0.000163956, gnorm=0.551, loss_scale=4, train_wall=552, gb_free=8.8, wall=209256
2022-02-14 03:56:24 | INFO | train_inner | epoch 024:   1181 / 1576 loss=6.847, nll_loss=5.087, ppl=34, wps=11753.6, ups=0.18, wpb=65536, bsz=128, num_updates=37300, lr=0.000163737, gnorm=0.515, loss_scale=4, train_wall=547, gb_free=8.8, wall=209814
2022-02-14 04:05:42 | INFO | train_inner | epoch 024:   1281 / 1576 loss=6.84, nll_loss=5.08, ppl=33.82, wps=11754.3, ups=0.18, wpb=65536, bsz=128, num_updates=37400, lr=0.000163517, gnorm=0.512, loss_scale=4, train_wall=547, gb_free=8.8, wall=210372
2022-02-14 04:14:59 | INFO | train_inner | epoch 024:   1381 / 1576 loss=6.841, nll_loss=5.08, ppl=33.82, wps=11753.8, ups=0.18, wpb=65536, bsz=128, num_updates=37500, lr=0.000163299, gnorm=0.524, loss_scale=8, train_wall=547, gb_free=8.8, wall=210929
2022-02-14 04:23:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 04:24:23 | INFO | train_inner | epoch 024:   1482 / 1576 loss=6.842, nll_loss=5.082, ppl=33.88, wps=11637.1, ups=0.18, wpb=65536, bsz=128, num_updates=37600, lr=0.000163082, gnorm=0.527, loss_scale=4, train_wall=553, gb_free=8.8, wall=211492
2022-02-14 04:33:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 04:33:08 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.928 | nll_loss 5.153 | ppl 35.59 | wps 32182 | wpb 1021.8 | bsz 2 | num_updates 37694 | best_loss 6.928
2022-02-14 04:33:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 37694 updates
2022-02-14 04:33:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint24.pt
2022-02-14 04:33:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint24.pt
2022-02-14 04:33:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint24.pt (epoch 24 @ 37694 updates, score 6.928) (writing took 28.920814773999155 seconds)
2022-02-14 04:33:37 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-14 04:33:37 | INFO | train | epoch 024 | loss 6.818 | nll_loss 5.054 | ppl 33.22 | wps 11678.4 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 37694 | lr 0.000162879 | gnorm 0.524 | loss_scale 4 | train_wall 8614 | gb_free 8.8 | wall 212047
2022-02-14 04:33:38 | INFO | fairseq.trainer | begin training epoch 25
2022-02-14 04:33:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 04:34:11 | INFO | train_inner | epoch 025:      6 / 1576 loss=6.84, nll_loss=5.08, ppl=33.82, wps=11042.1, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=37700, lr=0.000162866, gnorm=0.517, loss_scale=4, train_wall=542, gb_free=8.8, wall=212081
2022-02-14 04:43:28 | INFO | train_inner | epoch 025:    106 / 1576 loss=6.757, nll_loss=4.985, ppl=31.66, wps=11758.2, ups=0.18, wpb=65536, bsz=128, num_updates=37800, lr=0.00016265, gnorm=0.525, loss_scale=4, train_wall=547, gb_free=8.8, wall=212638
2022-02-14 04:52:46 | INFO | train_inner | epoch 025:    206 / 1576 loss=6.769, nll_loss=4.998, ppl=31.96, wps=11754.9, ups=0.18, wpb=65536, bsz=128, num_updates=37900, lr=0.000162435, gnorm=0.516, loss_scale=8, train_wall=547, gb_free=8.8, wall=213196
2022-02-14 04:53:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 05:02:09 | INFO | train_inner | epoch 025:    307 / 1576 loss=6.783, nll_loss=5.014, ppl=32.32, wps=11643.2, ups=0.18, wpb=65536, bsz=128, num_updates=38000, lr=0.000162221, gnorm=0.531, loss_scale=4, train_wall=552, gb_free=8.8, wall=213758
2022-02-14 05:11:26 | INFO | train_inner | epoch 025:    407 / 1576 loss=6.789, nll_loss=5.021, ppl=32.47, wps=11754.3, ups=0.18, wpb=65536, bsz=128, num_updates=38100, lr=0.000162008, gnorm=0.519, loss_scale=4, train_wall=547, gb_free=8.8, wall=214316
2022-02-14 05:20:44 | INFO | train_inner | epoch 025:    507 / 1576 loss=6.805, nll_loss=5.039, ppl=32.88, wps=11757.2, ups=0.18, wpb=65536, bsz=128, num_updates=38200, lr=0.000161796, gnorm=0.515, loss_scale=8, train_wall=547, gb_free=8.8, wall=214873
2022-02-14 05:30:01 | INFO | train_inner | epoch 025:    607 / 1576 loss=6.81, nll_loss=5.045, ppl=33.01, wps=11754.5, ups=0.18, wpb=65536, bsz=128, num_updates=38300, lr=0.000161585, gnorm=0.527, loss_scale=8, train_wall=547, gb_free=8.8, wall=215431
2022-02-14 05:39:19 | INFO | train_inner | epoch 025:    707 / 1576 loss=6.795, nll_loss=5.028, ppl=32.62, wps=11755.2, ups=0.18, wpb=65536, bsz=128, num_updates=38400, lr=0.000161374, gnorm=0.515, loss_scale=8, train_wall=547, gb_free=8.8, wall=215988
2022-02-14 05:41:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 05:48:42 | INFO | train_inner | epoch 025:    808 / 1576 loss=6.815, nll_loss=5.05, ppl=33.14, wps=11635.3, ups=0.18, wpb=65536, bsz=128, num_updates=38500, lr=0.000161165, gnorm=0.524, loss_scale=8, train_wall=553, gb_free=8.8, wall=216552
2022-02-14 05:54:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 05:58:05 | INFO | train_inner | epoch 025:    909 / 1576 loss=6.818, nll_loss=5.055, ppl=33.24, wps=11639, ups=0.18, wpb=65536, bsz=128, num_updates=38600, lr=0.000160956, gnorm=0.535, loss_scale=4, train_wall=552, gb_free=8.8, wall=217115
2022-02-14 06:07:23 | INFO | train_inner | epoch 025:   1009 / 1576 loss=6.825, nll_loss=5.062, ppl=33.41, wps=11752.9, ups=0.18, wpb=65536, bsz=128, num_updates=38700, lr=0.000160748, gnorm=0.527, loss_scale=4, train_wall=547, gb_free=8.8, wall=217672
2022-02-14 06:16:40 | INFO | train_inner | epoch 025:   1109 / 1576 loss=6.828, nll_loss=5.065, ppl=33.48, wps=11758.6, ups=0.18, wpb=65536, bsz=128, num_updates=38800, lr=0.00016054, gnorm=0.517, loss_scale=4, train_wall=547, gb_free=8.8, wall=218230
2022-02-14 06:25:58 | INFO | train_inner | epoch 025:   1209 / 1576 loss=6.832, nll_loss=5.07, ppl=33.59, wps=11753.1, ups=0.18, wpb=65536, bsz=128, num_updates=38900, lr=0.000160334, gnorm=0.523, loss_scale=8, train_wall=547, gb_free=8.8, wall=218787
2022-02-14 06:35:15 | INFO | train_inner | epoch 025:   1309 / 1576 loss=6.834, nll_loss=5.072, ppl=33.65, wps=11756.4, ups=0.18, wpb=65536, bsz=128, num_updates=39000, lr=0.000160128, gnorm=0.514, loss_scale=8, train_wall=547, gb_free=8.8, wall=219345
2022-02-14 06:42:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 06:44:38 | INFO | train_inner | epoch 025:   1410 / 1576 loss=6.845, nll_loss=5.085, ppl=33.93, wps=11641.5, ups=0.18, wpb=65532.3, bsz=128, num_updates=39100, lr=0.000159923, gnorm=0.525, loss_scale=8, train_wall=552, gb_free=8.8, wall=219908
2022-02-14 06:53:56 | INFO | train_inner | epoch 025:   1510 / 1576 loss=6.824, nll_loss=5.061, ppl=33.38, wps=11754.6, ups=0.18, wpb=65536, bsz=128, num_updates=39200, lr=0.000159719, gnorm=0.543, loss_scale=8, train_wall=547, gb_free=8.8, wall=220465
2022-02-14 06:59:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 07:00:05 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.931 | nll_loss 5.152 | ppl 35.55 | wps 32277.6 | wpb 1021.8 | bsz 2 | num_updates 39266 | best_loss 6.928
2022-02-14 07:00:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 39266 updates
2022-02-14 07:00:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint25.pt
2022-02-14 07:00:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint25.pt
2022-02-14 07:00:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint25.pt (epoch 25 @ 39266 updates, score 6.931) (writing took 19.162559632211924 seconds)
2022-02-14 07:00:24 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-14 07:00:25 | INFO | train | epoch 025 | loss 6.81 | nll_loss 5.045 | ppl 33 | wps 11691.1 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 39266 | lr 0.000159585 | gnorm 0.523 | loss_scale 8 | train_wall 8614 | gb_free 8.8 | wall 220854
2022-02-14 07:00:25 | INFO | fairseq.trainer | begin training epoch 26
2022-02-14 07:00:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 07:03:34 | INFO | train_inner | epoch 026:     34 / 1576 loss=6.807, nll_loss=5.041, ppl=32.93, wps=11225, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=39300, lr=0.000159516, gnorm=0.523, loss_scale=8, train_wall=542, gb_free=8.8, wall=221044
2022-02-14 07:06:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 07:12:58 | INFO | train_inner | epoch 026:    135 / 1576 loss=6.755, nll_loss=4.982, ppl=31.6, wps=11632.4, ups=0.18, wpb=65536, bsz=128, num_updates=39400, lr=0.000159313, gnorm=0.531, loss_scale=8, train_wall=553, gb_free=8.8, wall=221607
2022-02-14 07:18:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 07:22:21 | INFO | train_inner | epoch 026:    236 / 1576 loss=6.767, nll_loss=4.996, ppl=31.92, wps=11637.5, ups=0.18, wpb=65536, bsz=128, num_updates=39500, lr=0.000159111, gnorm=0.52, loss_scale=4, train_wall=552, gb_free=8.8, wall=222170
2022-02-14 07:31:38 | INFO | train_inner | epoch 026:    336 / 1576 loss=6.765, nll_loss=4.994, ppl=31.86, wps=11757.2, ups=0.18, wpb=65536, bsz=128, num_updates=39600, lr=0.00015891, gnorm=0.513, loss_scale=4, train_wall=547, gb_free=8.8, wall=222728
2022-02-14 07:40:56 | INFO | train_inner | epoch 026:    436 / 1576 loss=6.792, nll_loss=5.025, ppl=32.56, wps=11755.9, ups=0.18, wpb=65536, bsz=128, num_updates=39700, lr=0.00015871, gnorm=0.526, loss_scale=4, train_wall=547, gb_free=8.8, wall=223285
2022-02-14 07:50:14 | INFO | train_inner | epoch 026:    536 / 1576 loss=6.795, nll_loss=5.028, ppl=32.63, wps=11747.7, ups=0.18, wpb=65536, bsz=128, num_updates=39800, lr=0.000158511, gnorm=0.521, loss_scale=8, train_wall=547, gb_free=8.8, wall=223843
2022-02-14 07:59:31 | INFO | train_inner | epoch 026:    636 / 1576 loss=6.792, nll_loss=5.025, ppl=32.56, wps=11746.8, ups=0.18, wpb=65536, bsz=128, num_updates=39900, lr=0.000158312, gnorm=0.533, loss_scale=8, train_wall=547, gb_free=8.8, wall=224401
2022-02-14 08:07:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 08:08:55 | INFO | train_inner | epoch 026:    737 / 1576 loss=6.82, nll_loss=5.057, ppl=33.28, wps=11630.9, ups=0.18, wpb=65536, bsz=128, num_updates=40000, lr=0.000158114, gnorm=0.513, loss_scale=8, train_wall=553, gb_free=8.8, wall=224965
2022-02-14 08:18:13 | INFO | train_inner | epoch 026:    837 / 1576 loss=6.807, nll_loss=5.041, ppl=32.92, wps=11750.6, ups=0.18, wpb=65536, bsz=128, num_updates=40100, lr=0.000157917, gnorm=0.551, loss_scale=8, train_wall=547, gb_free=8.8, wall=225522
2022-02-14 08:27:30 | INFO | train_inner | epoch 026:    937 / 1576 loss=6.819, nll_loss=5.055, ppl=33.25, wps=11750.7, ups=0.18, wpb=65536, bsz=128, num_updates=40200, lr=0.00015772, gnorm=0.525, loss_scale=8, train_wall=547, gb_free=8.8, wall=226080
2022-02-14 08:33:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 08:36:54 | INFO | train_inner | epoch 026:   1038 / 1576 loss=6.804, nll_loss=5.038, ppl=32.86, wps=11630.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=40300, lr=0.000157524, gnorm=0.54, loss_scale=8, train_wall=553, gb_free=8.8, wall=226644
2022-02-14 08:46:12 | INFO | train_inner | epoch 026:   1138 / 1576 loss=6.815, nll_loss=5.05, ppl=33.13, wps=11751.6, ups=0.18, wpb=65536, bsz=128, num_updates=40400, lr=0.000157329, gnorm=0.531, loss_scale=8, train_wall=547, gb_free=8.8, wall=227201
2022-02-14 08:55:29 | INFO | train_inner | epoch 026:   1238 / 1576 loss=6.826, nll_loss=5.063, ppl=33.43, wps=11745.8, ups=0.18, wpb=65536, bsz=128, num_updates=40500, lr=0.000157135, gnorm=0.525, loss_scale=8, train_wall=547, gb_free=8.8, wall=227759
2022-02-14 08:57:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 09:04:53 | INFO | train_inner | epoch 026:   1339 / 1576 loss=6.818, nll_loss=5.055, ppl=33.24, wps=11633.2, ups=0.18, wpb=65536, bsz=128, num_updates=40600, lr=0.000156941, gnorm=0.52, loss_scale=8, train_wall=553, gb_free=8.8, wall=228322
2022-02-14 09:14:11 | INFO | train_inner | epoch 026:   1439 / 1576 loss=6.829, nll_loss=5.067, ppl=33.52, wps=11748.4, ups=0.18, wpb=65536, bsz=128, num_updates=40700, lr=0.000156748, gnorm=0.529, loss_scale=8, train_wall=547, gb_free=8.8, wall=228880
2022-02-14 09:22:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 09:23:34 | INFO | train_inner | epoch 026:   1540 / 1576 loss=6.828, nll_loss=5.066, ppl=33.49, wps=11631.9, ups=0.18, wpb=65536, bsz=128, num_updates=40800, lr=0.000156556, gnorm=0.532, loss_scale=8, train_wall=553, gb_free=8.8, wall=229444
2022-02-14 09:26:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 09:26:57 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.923 | nll_loss 5.154 | ppl 35.62 | wps 32343.2 | wpb 1021.8 | bsz 2 | num_updates 40836 | best_loss 6.923
2022-02-14 09:26:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 40836 updates
2022-02-14 09:26:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint26.pt
2022-02-14 09:27:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint26.pt
2022-02-14 09:27:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint26.pt (epoch 26 @ 40836 updates, score 6.923) (writing took 28.975596118718386 seconds)
2022-02-14 09:27:26 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-14 09:27:26 | INFO | train | epoch 026 | loss 6.802 | nll_loss 5.036 | ppl 32.8 | wps 11657.5 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 40836 | lr 0.000156487 | gnorm 0.528 | loss_scale 8 | train_wall 8619 | gb_free 8.8 | wall 229675
2022-02-14 09:27:26 | INFO | fairseq.trainer | begin training epoch 27
2022-02-14 09:27:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 09:33:23 | INFO | train_inner | epoch 027:     64 / 1576 loss=6.786, nll_loss=5.017, ppl=32.39, wps=11035.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=40900, lr=0.000156365, gnorm=0.529, loss_scale=8, train_wall=542, gb_free=8.8, wall=230032
2022-02-14 09:42:40 | INFO | train_inner | epoch 027:    164 / 1576 loss=6.753, nll_loss=4.98, ppl=31.56, wps=11751.3, ups=0.18, wpb=65536, bsz=128, num_updates=41000, lr=0.000156174, gnorm=0.526, loss_scale=8, train_wall=547, gb_free=8.8, wall=230590
2022-02-14 09:50:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 09:52:04 | INFO | train_inner | epoch 027:    265 / 1576 loss=6.766, nll_loss=4.996, ppl=31.9, wps=11631.2, ups=0.18, wpb=65536, bsz=128, num_updates=41100, lr=0.000155984, gnorm=0.516, loss_scale=8, train_wall=553, gb_free=8.8, wall=231154
2022-02-14 10:01:22 | INFO | train_inner | epoch 027:    365 / 1576 loss=6.769, nll_loss=4.998, ppl=31.96, wps=11749.7, ups=0.18, wpb=65536, bsz=128, num_updates=41200, lr=0.000155794, gnorm=0.522, loss_scale=8, train_wall=547, gb_free=8.8, wall=231711
2022-02-14 10:10:39 | INFO | train_inner | epoch 027:    465 / 1576 loss=6.77, nll_loss=5, ppl=32, wps=11749.2, ups=0.18, wpb=65536, bsz=128, num_updates=41300, lr=0.000155606, gnorm=0.529, loss_scale=8, train_wall=547, gb_free=8.8, wall=232269
2022-02-14 10:16:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 10:20:03 | INFO | train_inner | epoch 027:    566 / 1576 loss=6.781, nll_loss=5.012, ppl=32.26, wps=11632, ups=0.18, wpb=65536, bsz=128, num_updates=41400, lr=0.000155417, gnorm=0.526, loss_scale=8, train_wall=553, gb_free=8.8, wall=232833
2022-02-14 10:29:20 | INFO | train_inner | epoch 027:    666 / 1576 loss=6.79, nll_loss=5.023, ppl=32.51, wps=11752.8, ups=0.18, wpb=65536, bsz=128, num_updates=41500, lr=0.00015523, gnorm=0.526, loss_scale=8, train_wall=547, gb_free=8.8, wall=233390
2022-02-14 10:37:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 10:38:44 | INFO | train_inner | epoch 027:    767 / 1576 loss=6.795, nll_loss=5.028, ppl=32.62, wps=11638.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=41600, lr=0.000155043, gnorm=0.534, loss_scale=4, train_wall=552, gb_free=8.8, wall=233953
2022-02-14 10:48:01 | INFO | train_inner | epoch 027:    867 / 1576 loss=6.8, nll_loss=5.033, ppl=32.75, wps=11759, ups=0.18, wpb=65536, bsz=128, num_updates=41700, lr=0.000154857, gnorm=0.53, loss_scale=4, train_wall=547, gb_free=8.8, wall=234511
2022-02-14 10:57:18 | INFO | train_inner | epoch 027:    967 / 1576 loss=6.808, nll_loss=5.043, ppl=32.96, wps=11757.2, ups=0.18, wpb=65536, bsz=128, num_updates=41800, lr=0.000154672, gnorm=0.534, loss_scale=4, train_wall=547, gb_free=8.8, wall=235068
2022-02-14 11:06:36 | INFO | train_inner | epoch 027:   1067 / 1576 loss=6.814, nll_loss=5.05, ppl=33.12, wps=11754.8, ups=0.18, wpb=65536, bsz=128, num_updates=41900, lr=0.000154487, gnorm=0.525, loss_scale=8, train_wall=547, gb_free=8.8, wall=235625
2022-02-14 11:15:54 | INFO | train_inner | epoch 027:   1167 / 1576 loss=6.808, nll_loss=5.043, ppl=32.97, wps=11751.8, ups=0.18, wpb=65536, bsz=128, num_updates=42000, lr=0.000154303, gnorm=0.525, loss_scale=8, train_wall=547, gb_free=8.8, wall=236183
2022-02-14 11:25:11 | INFO | train_inner | epoch 027:   1267 / 1576 loss=6.811, nll_loss=5.046, ppl=33.05, wps=11755.2, ups=0.18, wpb=65536, bsz=128, num_updates=42100, lr=0.00015412, gnorm=0.531, loss_scale=8, train_wall=547, gb_free=8.8, wall=236741
2022-02-14 11:25:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 11:34:34 | INFO | train_inner | epoch 027:   1368 / 1576 loss=6.822, nll_loss=5.06, ppl=33.35, wps=11641, ups=0.18, wpb=65536, bsz=128, num_updates=42200, lr=0.000153937, gnorm=0.534, loss_scale=8, train_wall=552, gb_free=8.8, wall=237304
2022-02-14 11:43:50 | INFO | train_inner | epoch 027:   1468 / 1576 loss=6.828, nll_loss=5.065, ppl=33.48, wps=11785.6, ups=0.18, wpb=65536, bsz=128, num_updates=42300, lr=0.000153755, gnorm=0.545, loss_scale=8, train_wall=545, gb_free=8.8, wall=237860
2022-02-14 11:50:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 11:53:11 | INFO | train_inner | epoch 027:   1569 / 1576 loss=6.832, nll_loss=5.071, ppl=33.61, wps=11673.9, ups=0.18, wpb=65536, bsz=128, num_updates=42400, lr=0.000153574, gnorm=0.522, loss_scale=8, train_wall=551, gb_free=8.8, wall=238421
2022-02-14 11:53:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 11:53:52 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.92 | nll_loss 5.139 | ppl 35.24 | wps 32422.1 | wpb 1021.8 | bsz 2 | num_updates 42407 | best_loss 6.92
2022-02-14 11:53:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 42407 updates
2022-02-14 11:53:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint27.pt
2022-02-14 11:54:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint27.pt
2022-02-14 11:54:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint27.pt (epoch 27 @ 42407 updates, score 6.92) (writing took 29.022084584459662 seconds)
2022-02-14 11:54:21 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-14 11:54:21 | INFO | train | epoch 027 | loss 6.795 | nll_loss 5.028 | ppl 32.63 | wps 11672.4 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 42407 | lr 0.000153561 | gnorm 0.528 | loss_scale 8 | train_wall 8613 | gb_free 8.8 | wall 238491
2022-02-14 11:54:21 | INFO | fairseq.trainer | begin training epoch 28
2022-02-14 11:54:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 12:02:58 | INFO | train_inner | epoch 028:     93 / 1576 loss=6.754, nll_loss=4.981, ppl=31.58, wps=11071.5, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=42500, lr=0.000153393, gnorm=0.535, loss_scale=8, train_wall=540, gb_free=8.8, wall=239008
2022-02-14 12:12:14 | INFO | train_inner | epoch 028:    193 / 1576 loss=6.755, nll_loss=4.983, ppl=31.63, wps=11800.7, ups=0.18, wpb=65536, bsz=128, num_updates=42600, lr=0.000153213, gnorm=0.528, loss_scale=8, train_wall=545, gb_free=8.8, wall=239563
2022-02-14 12:14:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 12:21:35 | INFO | train_inner | epoch 028:    294 / 1576 loss=6.749, nll_loss=4.976, ppl=31.48, wps=11679.9, ups=0.18, wpb=65536, bsz=128, num_updates=42700, lr=0.000153033, gnorm=0.539, loss_scale=8, train_wall=550, gb_free=8.8, wall=240124
2022-02-14 12:30:50 | INFO | train_inner | epoch 028:    394 / 1576 loss=6.766, nll_loss=4.996, ppl=31.91, wps=11798.9, ups=0.18, wpb=65536, bsz=128, num_updates=42800, lr=0.000152854, gnorm=0.516, loss_scale=8, train_wall=545, gb_free=8.8, wall=240680
2022-02-14 12:40:05 | INFO | train_inner | epoch 028:    494 / 1576 loss=6.777, nll_loss=5.008, ppl=32.17, wps=11799.3, ups=0.18, wpb=65532.3, bsz=128, num_updates=42900, lr=0.000152676, gnorm=0.532, loss_scale=16, train_wall=545, gb_free=8.8, wall=241235
2022-02-14 12:41:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 12:49:26 | INFO | train_inner | epoch 028:    595 / 1576 loss=6.781, nll_loss=5.012, ppl=32.26, wps=11683.9, ups=0.18, wpb=65536, bsz=128, num_updates=43000, lr=0.000152499, gnorm=0.527, loss_scale=8, train_wall=550, gb_free=8.8, wall=241796
2022-02-14 12:58:42 | INFO | train_inner | epoch 028:    695 / 1576 loss=6.778, nll_loss=5.008, ppl=32.19, wps=11799.5, ups=0.18, wpb=65536, bsz=128, num_updates=43100, lr=0.000152322, gnorm=0.53, loss_scale=8, train_wall=545, gb_free=8.8, wall=242351
2022-02-14 13:06:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 13:08:03 | INFO | train_inner | epoch 028:    796 / 1576 loss=6.799, nll_loss=5.033, ppl=32.74, wps=11684.4, ups=0.18, wpb=65536, bsz=128, num_updates=43200, lr=0.000152145, gnorm=0.541, loss_scale=8, train_wall=550, gb_free=8.8, wall=242912
2022-02-14 13:17:18 | INFO | train_inner | epoch 028:    896 / 1576 loss=6.799, nll_loss=5.033, ppl=32.74, wps=11802, ups=0.18, wpb=65536, bsz=128, num_updates=43300, lr=0.000151969, gnorm=0.522, loss_scale=8, train_wall=545, gb_free=8.8, wall=243468
2022-02-14 13:26:34 | INFO | train_inner | epoch 028:    996 / 1576 loss=6.8, nll_loss=5.034, ppl=32.77, wps=11797.6, ups=0.18, wpb=65536, bsz=128, num_updates=43400, lr=0.000151794, gnorm=0.526, loss_scale=8, train_wall=545, gb_free=8.8, wall=244023
2022-02-14 13:30:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 13:35:54 | INFO | train_inner | epoch 028:   1097 / 1576 loss=6.798, nll_loss=5.032, ppl=32.71, wps=11683.3, ups=0.18, wpb=65536, bsz=128, num_updates=43500, lr=0.00015162, gnorm=0.529, loss_scale=8, train_wall=550, gb_free=8.8, wall=244584
2022-02-14 13:45:10 | INFO | train_inner | epoch 028:   1197 / 1576 loss=6.806, nll_loss=5.041, ppl=32.92, wps=11796.3, ups=0.18, wpb=65536, bsz=128, num_updates=43600, lr=0.000151446, gnorm=0.54, loss_scale=8, train_wall=545, gb_free=8.8, wall=245140
2022-02-14 13:54:25 | INFO | train_inner | epoch 028:   1297 / 1576 loss=6.806, nll_loss=5.041, ppl=32.93, wps=11798.2, ups=0.18, wpb=65536, bsz=128, num_updates=43700, lr=0.000151272, gnorm=0.523, loss_scale=8, train_wall=545, gb_free=8.8, wall=245695
2022-02-14 13:55:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 14:03:46 | INFO | train_inner | epoch 028:   1398 / 1576 loss=6.82, nll_loss=5.057, ppl=33.29, wps=11685, ups=0.18, wpb=65536, bsz=128, num_updates=43800, lr=0.000151099, gnorm=0.527, loss_scale=8, train_wall=550, gb_free=8.8, wall=246256
2022-02-14 14:13:02 | INFO | train_inner | epoch 028:   1498 / 1576 loss=6.814, nll_loss=5.05, ppl=33.13, wps=11802.1, ups=0.18, wpb=65536, bsz=128, num_updates=43900, lr=0.000150927, gnorm=0.526, loss_scale=8, train_wall=545, gb_free=8.8, wall=246811
2022-02-14 14:17:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 14:20:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 14:20:17 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.917 | nll_loss 5.146 | ppl 35.42 | wps 32558.5 | wpb 1021.8 | bsz 2 | num_updates 43977 | best_loss 6.917
2022-02-14 14:20:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 43977 updates
2022-02-14 14:20:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint28.pt
2022-02-14 14:20:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint28.pt
2022-02-14 14:20:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint28.pt (epoch 28 @ 43977 updates, score 6.917) (writing took 28.7997432583943 seconds)
2022-02-14 14:20:45 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-14 14:20:45 | INFO | train | epoch 028 | loss 6.788 | nll_loss 5.021 | ppl 32.46 | wps 11706.9 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 43977 | lr 0.000150795 | gnorm 0.531 | loss_scale 4 | train_wall 8583 | gb_free 8.8 | wall 247275
2022-02-14 14:20:45 | INFO | fairseq.trainer | begin training epoch 29
2022-02-14 14:20:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 14:22:53 | INFO | train_inner | epoch 029:     23 / 1576 loss=6.799, nll_loss=5.033, ppl=32.74, wps=10982.6, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=44000, lr=0.000150756, gnorm=0.566, loss_scale=4, train_wall=545, gb_free=8.8, wall=247403
2022-02-14 14:32:08 | INFO | train_inner | epoch 029:    123 / 1576 loss=6.741, nll_loss=4.966, ppl=31.26, wps=11807.4, ups=0.18, wpb=65536, bsz=128, num_updates=44100, lr=0.000150585, gnorm=0.521, loss_scale=4, train_wall=544, gb_free=8.8, wall=247958
2022-02-14 14:41:23 | INFO | train_inner | epoch 029:    223 / 1576 loss=6.744, nll_loss=4.97, ppl=31.35, wps=11805.6, ups=0.18, wpb=65536, bsz=128, num_updates=44200, lr=0.000150414, gnorm=0.531, loss_scale=8, train_wall=545, gb_free=8.8, wall=248513
2022-02-14 14:50:39 | INFO | train_inner | epoch 029:    323 / 1576 loss=6.75, nll_loss=4.978, ppl=31.51, wps=11796.7, ups=0.18, wpb=65536, bsz=128, num_updates=44300, lr=0.000150244, gnorm=0.527, loss_scale=8, train_wall=545, gb_free=8.8, wall=249069
2022-02-14 14:59:54 | INFO | train_inner | epoch 029:    423 / 1576 loss=6.763, nll_loss=4.992, ppl=31.82, wps=11800.8, ups=0.18, wpb=65536, bsz=128, num_updates=44400, lr=0.000150075, gnorm=0.538, loss_scale=8, train_wall=545, gb_free=8.8, wall=249624
2022-02-14 15:05:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 15:09:15 | INFO | train_inner | epoch 029:    524 / 1576 loss=6.771, nll_loss=5.001, ppl=32.03, wps=11681.8, ups=0.18, wpb=65532.3, bsz=128, num_updates=44500, lr=0.000149906, gnorm=0.537, loss_scale=8, train_wall=550, gb_free=8.8, wall=250185
2022-02-14 15:18:31 | INFO | train_inner | epoch 029:    624 / 1576 loss=6.77, nll_loss=5, ppl=31.99, wps=11797.2, ups=0.18, wpb=65536, bsz=128, num_updates=44600, lr=0.000149738, gnorm=0.525, loss_scale=8, train_wall=545, gb_free=8.8, wall=250740
2022-02-14 15:27:46 | INFO | train_inner | epoch 029:    724 / 1576 loss=6.786, nll_loss=5.018, ppl=32.41, wps=11795.3, ups=0.18, wpb=65536, bsz=128, num_updates=44700, lr=0.000149571, gnorm=0.534, loss_scale=8, train_wall=545, gb_free=8.8, wall=251296
2022-02-14 15:29:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 15:35:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 15:37:13 | INFO | train_inner | epoch 029:    826 / 1576 loss=6.792, nll_loss=5.025, ppl=32.56, wps=11569.2, ups=0.18, wpb=65536, bsz=128, num_updates=44800, lr=0.000149404, gnorm=0.546, loss_scale=4, train_wall=556, gb_free=8.8, wall=251862
2022-02-14 15:46:28 | INFO | train_inner | epoch 029:    926 / 1576 loss=6.79, nll_loss=5.023, ppl=32.52, wps=11804.4, ups=0.18, wpb=65536, bsz=128, num_updates=44900, lr=0.000149237, gnorm=0.533, loss_scale=4, train_wall=545, gb_free=8.8, wall=252418
2022-02-14 15:55:43 | INFO | train_inner | epoch 029:   1026 / 1576 loss=6.784, nll_loss=5.015, ppl=32.34, wps=11804.2, ups=0.18, wpb=65536, bsz=128, num_updates=45000, lr=0.000149071, gnorm=0.551, loss_scale=4, train_wall=545, gb_free=8.8, wall=252973
2022-02-14 16:04:59 | INFO | train_inner | epoch 029:   1126 / 1576 loss=6.794, nll_loss=5.027, ppl=32.62, wps=11800, ups=0.18, wpb=65536, bsz=128, num_updates=45100, lr=0.000148906, gnorm=0.517, loss_scale=8, train_wall=545, gb_free=8.8, wall=253528
2022-02-14 16:14:14 | INFO | train_inner | epoch 029:   1226 / 1576 loss=6.797, nll_loss=5.03, ppl=32.68, wps=11797.3, ups=0.18, wpb=65536, bsz=128, num_updates=45200, lr=0.000148741, gnorm=0.525, loss_scale=8, train_wall=545, gb_free=8.8, wall=254084
2022-02-14 16:23:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 16:23:35 | INFO | train_inner | epoch 029:   1327 / 1576 loss=6.811, nll_loss=5.046, ppl=33.04, wps=11680.9, ups=0.18, wpb=65536, bsz=128, num_updates=45300, lr=0.000148577, gnorm=0.531, loss_scale=8, train_wall=550, gb_free=8.8, wall=254645
2022-02-14 16:32:51 | INFO | train_inner | epoch 029:   1427 / 1576 loss=6.815, nll_loss=5.051, ppl=33.14, wps=11800.2, ups=0.18, wpb=65536, bsz=128, num_updates=45400, lr=0.000148413, gnorm=0.547, loss_scale=8, train_wall=545, gb_free=8.8, wall=255200
2022-02-14 16:42:06 | INFO | train_inner | epoch 029:   1527 / 1576 loss=6.817, nll_loss=5.054, ppl=33.21, wps=11800.2, ups=0.18, wpb=65536, bsz=128, num_updates=45500, lr=0.00014825, gnorm=0.534, loss_scale=8, train_wall=545, gb_free=8.8, wall=255756
2022-02-14 16:46:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 16:46:40 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.917 | nll_loss 5.132 | ppl 35.07 | wps 32600.7 | wpb 1021.8 | bsz 2 | num_updates 45549 | best_loss 6.917
2022-02-14 16:46:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 45549 updates
2022-02-14 16:46:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint29.pt
2022-02-14 16:46:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint29.pt
2022-02-14 16:47:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint29.pt (epoch 29 @ 45549 updates, score 6.917) (writing took 28.770518948324025 seconds)
2022-02-14 16:47:09 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-14 16:47:09 | INFO | train | epoch 029 | loss 6.782 | nll_loss 5.014 | ppl 32.3 | wps 11722.9 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 45549 | lr 0.00014817 | gnorm 0.534 | loss_scale 8 | train_wall 8582 | gb_free 8.8 | wall 256058
2022-02-14 16:47:09 | INFO | fairseq.trainer | begin training epoch 30
2022-02-14 16:47:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 16:47:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 16:51:57 | INFO | train_inner | epoch 030:     52 / 1576 loss=6.77, nll_loss=4.999, ppl=31.99, wps=10985.5, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=45600, lr=0.000148087, gnorm=0.546, loss_scale=8, train_wall=545, gb_free=8.8, wall=256347
2022-02-14 17:01:13 | INFO | train_inner | epoch 030:    152 / 1576 loss=6.736, nll_loss=4.962, ppl=31.16, wps=11800.2, ups=0.18, wpb=65536, bsz=128, num_updates=45700, lr=0.000147925, gnorm=0.523, loss_scale=8, train_wall=545, gb_free=8.8, wall=256902
2022-02-14 17:05:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 17:10:33 | INFO | train_inner | epoch 030:    253 / 1576 loss=6.742, nll_loss=4.968, ppl=31.29, wps=11689.2, ups=0.18, wpb=65536, bsz=128, num_updates=45800, lr=0.000147764, gnorm=0.543, loss_scale=4, train_wall=550, gb_free=8.8, wall=257463
2022-02-14 17:19:48 | INFO | train_inner | epoch 030:    353 / 1576 loss=6.756, nll_loss=4.984, ppl=31.64, wps=11804.1, ups=0.18, wpb=65536, bsz=128, num_updates=45900, lr=0.000147602, gnorm=0.539, loss_scale=4, train_wall=545, gb_free=8.8, wall=258018
2022-02-14 17:29:04 | INFO | train_inner | epoch 030:    453 / 1576 loss=6.76, nll_loss=4.988, ppl=31.73, wps=11805, ups=0.18, wpb=65536, bsz=128, num_updates=46000, lr=0.000147442, gnorm=0.541, loss_scale=8, train_wall=545, gb_free=8.8, wall=258573
2022-02-14 17:33:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 17:38:24 | INFO | train_inner | epoch 030:    554 / 1576 loss=6.767, nll_loss=4.997, ppl=31.92, wps=11686.8, ups=0.18, wpb=65536, bsz=128, num_updates=46100, lr=0.000147282, gnorm=0.541, loss_scale=4, train_wall=550, gb_free=8.8, wall=259134
2022-02-14 17:47:40 | INFO | train_inner | epoch 030:    654 / 1576 loss=6.774, nll_loss=5.004, ppl=32.08, wps=11805.7, ups=0.18, wpb=65536, bsz=128, num_updates=46200, lr=0.000147122, gnorm=0.539, loss_scale=4, train_wall=545, gb_free=8.8, wall=259689
2022-02-14 17:56:54 | INFO | train_inner | epoch 030:    754 / 1576 loss=6.787, nll_loss=5.02, ppl=32.44, wps=11809.9, ups=0.18, wpb=65536, bsz=128, num_updates=46300, lr=0.000146964, gnorm=0.535, loss_scale=8, train_wall=544, gb_free=8.8, wall=260244
2022-02-14 18:06:10 | INFO | train_inner | epoch 030:    854 / 1576 loss=6.783, nll_loss=5.015, ppl=32.34, wps=11798.7, ups=0.18, wpb=65536, bsz=128, num_updates=46400, lr=0.000146805, gnorm=0.535, loss_scale=8, train_wall=545, gb_free=8.8, wall=260800
2022-02-14 18:15:25 | INFO | train_inner | epoch 030:    954 / 1576 loss=6.788, nll_loss=5.02, ppl=32.45, wps=11799.3, ups=0.18, wpb=65536, bsz=128, num_updates=46500, lr=0.000146647, gnorm=0.545, loss_scale=8, train_wall=545, gb_free=8.8, wall=261355
2022-02-14 18:24:41 | INFO | train_inner | epoch 030:   1054 / 1576 loss=6.791, nll_loss=5.024, ppl=32.54, wps=11798.9, ups=0.18, wpb=65536, bsz=128, num_updates=46600, lr=0.00014649, gnorm=0.517, loss_scale=16, train_wall=545, gb_free=8.8, wall=261910
2022-02-14 18:24:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 18:34:02 | INFO | train_inner | epoch 030:   1155 / 1576 loss=6.787, nll_loss=5.019, ppl=32.42, wps=11675.1, ups=0.18, wpb=65532.3, bsz=128, num_updates=46700, lr=0.000146333, gnorm=0.533, loss_scale=8, train_wall=551, gb_free=8.8, wall=262472
2022-02-14 18:43:18 | INFO | train_inner | epoch 030:   1255 / 1576 loss=6.8, nll_loss=5.034, ppl=32.77, wps=11795.7, ups=0.18, wpb=65536, bsz=128, num_updates=46800, lr=0.000146176, gnorm=0.524, loss_scale=8, train_wall=545, gb_free=8.8, wall=263027
2022-02-14 18:50:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 18:52:39 | INFO | train_inner | epoch 030:   1356 / 1576 loss=6.798, nll_loss=5.032, ppl=32.71, wps=11684.3, ups=0.18, wpb=65536, bsz=128, num_updates=46900, lr=0.00014602, gnorm=0.529, loss_scale=8, train_wall=550, gb_free=8.8, wall=263588
2022-02-14 19:01:54 | INFO | train_inner | epoch 030:   1456 / 1576 loss=6.795, nll_loss=5.028, ppl=32.64, wps=11802.3, ups=0.18, wpb=65536, bsz=128, num_updates=47000, lr=0.000145865, gnorm=0.526, loss_scale=8, train_wall=545, gb_free=8.8, wall=264143
2022-02-14 19:11:09 | INFO | train_inner | epoch 030:   1556 / 1576 loss=6.801, nll_loss=5.035, ppl=32.78, wps=11800.2, ups=0.18, wpb=65536, bsz=128, num_updates=47100, lr=0.00014571, gnorm=0.532, loss_scale=8, train_wall=545, gb_free=8.8, wall=264699
2022-02-14 19:12:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 19:13:02 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.916 | nll_loss 5.146 | ppl 35.41 | wps 32531.9 | wpb 1021.8 | bsz 2 | num_updates 47120 | best_loss 6.916
2022-02-14 19:13:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 47120 updates
2022-02-14 19:13:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint30.pt
2022-02-14 19:13:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint30.pt
2022-02-14 19:13:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint30.pt (epoch 30 @ 47120 updates, score 6.916) (writing took 28.748610723763704 seconds)
2022-02-14 19:13:31 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-14 19:13:31 | INFO | train | epoch 030 | loss 6.776 | nll_loss 5.007 | ppl 32.16 | wps 11716.6 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 47120 | lr 0.000145679 | gnorm 0.534 | loss_scale 8 | train_wall 8582 | gb_free 8.8 | wall 264841
2022-02-14 19:13:31 | INFO | fairseq.trainer | begin training epoch 31
2022-02-14 19:13:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 19:17:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 19:21:01 | INFO | train_inner | epoch 031:     81 / 1576 loss=6.74, nll_loss=4.965, ppl=31.24, wps=10982.7, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=47200, lr=0.000145556, gnorm=0.534, loss_scale=8, train_wall=545, gb_free=8.8, wall=265290
2022-02-14 19:30:16 | INFO | train_inner | epoch 031:    181 / 1576 loss=6.723, nll_loss=4.947, ppl=30.84, wps=11800.8, ups=0.18, wpb=65536, bsz=128, num_updates=47300, lr=0.000145402, gnorm=0.521, loss_scale=8, train_wall=545, gb_free=8.8, wall=265846
2022-02-14 19:39:31 | INFO | train_inner | epoch 031:    281 / 1576 loss=6.748, nll_loss=4.975, ppl=31.44, wps=11800.8, ups=0.18, wpb=65532.3, bsz=128, num_updates=47400, lr=0.000145248, gnorm=0.536, loss_scale=8, train_wall=545, gb_free=8.8, wall=266401
2022-02-14 19:44:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 19:48:52 | INFO | train_inner | epoch 031:    382 / 1576 loss=6.743, nll_loss=4.97, ppl=31.33, wps=11683.9, ups=0.18, wpb=65536, bsz=128, num_updates=47500, lr=0.000145095, gnorm=0.538, loss_scale=8, train_wall=550, gb_free=8.8, wall=266962
2022-02-14 19:58:08 | INFO | train_inner | epoch 031:    482 / 1576 loss=6.754, nll_loss=4.982, ppl=31.61, wps=11798.4, ups=0.18, wpb=65536, bsz=128, num_updates=47600, lr=0.000144943, gnorm=0.53, loss_scale=8, train_wall=545, gb_free=8.8, wall=267517
2022-02-14 20:07:23 | INFO | train_inner | epoch 031:    582 / 1576 loss=6.765, nll_loss=4.994, ppl=31.87, wps=11798.9, ups=0.18, wpb=65536, bsz=128, num_updates=47700, lr=0.000144791, gnorm=0.538, loss_scale=8, train_wall=545, gb_free=8.8, wall=268073
2022-02-14 20:11:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 20:16:44 | INFO | train_inner | epoch 031:    683 / 1576 loss=6.767, nll_loss=4.996, ppl=31.92, wps=11683, ups=0.18, wpb=65536, bsz=128, num_updates=47800, lr=0.000144639, gnorm=0.521, loss_scale=8, train_wall=550, gb_free=8.8, wall=268634
2022-02-14 20:26:00 | INFO | train_inner | epoch 031:    783 / 1576 loss=6.775, nll_loss=5.006, ppl=32.12, wps=11780.2, ups=0.18, wpb=65536, bsz=128, num_updates=47900, lr=0.000144488, gnorm=0.54, loss_scale=8, train_wall=546, gb_free=8.8, wall=269190
2022-02-14 20:35:16 | INFO | train_inner | epoch 031:    883 / 1576 loss=6.771, nll_loss=5.001, ppl=32.03, wps=11789.4, ups=0.18, wpb=65536, bsz=128, num_updates=48000, lr=0.000144338, gnorm=0.529, loss_scale=16, train_wall=545, gb_free=8.8, wall=269746
2022-02-14 20:37:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 20:43:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 20:44:43 | INFO | train_inner | epoch 031:    985 / 1576 loss=6.78, nll_loss=5.011, ppl=32.25, wps=11556.8, ups=0.18, wpb=65536, bsz=128, num_updates=48100, lr=0.000144187, gnorm=0.549, loss_scale=4, train_wall=556, gb_free=8.8, wall=270313
2022-02-14 20:53:59 | INFO | train_inner | epoch 031:   1085 / 1576 loss=6.782, nll_loss=5.014, ppl=32.3, wps=11799.6, ups=0.18, wpb=65536, bsz=128, num_updates=48200, lr=0.000144038, gnorm=0.54, loss_scale=4, train_wall=545, gb_free=8.8, wall=270869
2022-02-14 21:03:15 | INFO | train_inner | epoch 031:   1185 / 1576 loss=6.796, nll_loss=5.029, ppl=32.66, wps=11785.6, ups=0.18, wpb=65536, bsz=128, num_updates=48300, lr=0.000143889, gnorm=0.527, loss_scale=4, train_wall=546, gb_free=8.8, wall=271425
2022-02-14 21:12:31 | INFO | train_inner | epoch 031:   1285 / 1576 loss=6.793, nll_loss=5.027, ppl=32.6, wps=11780.9, ups=0.18, wpb=65536, bsz=128, num_updates=48400, lr=0.00014374, gnorm=0.536, loss_scale=8, train_wall=546, gb_free=8.8, wall=271981
2022-02-14 21:21:48 | INFO | train_inner | epoch 031:   1385 / 1576 loss=6.802, nll_loss=5.036, ppl=32.81, wps=11772.8, ups=0.18, wpb=65536, bsz=128, num_updates=48500, lr=0.000143592, gnorm=0.533, loss_scale=8, train_wall=546, gb_free=8.8, wall=272538
2022-02-14 21:31:05 | INFO | train_inner | epoch 031:   1485 / 1576 loss=6.803, nll_loss=5.038, ppl=32.85, wps=11771.8, ups=0.18, wpb=65536, bsz=128, num_updates=48600, lr=0.000143444, gnorm=0.54, loss_scale=8, train_wall=546, gb_free=8.8, wall=273094
2022-02-14 21:38:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 21:39:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 21:39:33 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.911 | nll_loss 5.14 | ppl 35.26 | wps 32437.6 | wpb 1021.8 | bsz 2 | num_updates 48690 | best_loss 6.911
2022-02-14 21:39:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 48690 updates
2022-02-14 21:39:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint31.pt
2022-02-14 21:39:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint31.pt
2022-02-14 21:40:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint31.pt (epoch 31 @ 48690 updates, score 6.911) (writing took 29.11935491580516 seconds)
2022-02-14 21:40:02 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-14 21:40:02 | INFO | train | epoch 031 | loss 6.771 | nll_loss 5.001 | ppl 32.02 | wps 11697.5 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 48690 | lr 0.000143311 | gnorm 0.534 | loss_scale 8 | train_wall 8589 | gb_free 8.8 | wall 273632
2022-02-14 21:40:02 | INFO | fairseq.trainer | begin training epoch 32
2022-02-14 21:40:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 21:40:58 | INFO | train_inner | epoch 032:     10 / 1576 loss=6.799, nll_loss=5.033, ppl=32.74, wps=10951.6, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=48700, lr=0.000143296, gnorm=0.538, loss_scale=8, train_wall=547, gb_free=8.8, wall=273687
2022-02-14 21:50:14 | INFO | train_inner | epoch 032:    110 / 1576 loss=6.714, nll_loss=4.936, ppl=30.62, wps=11791.5, ups=0.18, wpb=65536, bsz=128, num_updates=48800, lr=0.00014315, gnorm=0.53, loss_scale=8, train_wall=545, gb_free=8.8, wall=274243
2022-02-14 21:59:29 | INFO | train_inner | epoch 032:    210 / 1576 loss=6.734, nll_loss=4.959, ppl=31.09, wps=11789.9, ups=0.18, wpb=65536, bsz=128, num_updates=48900, lr=0.000143003, gnorm=0.547, loss_scale=8, train_wall=545, gb_free=8.8, wall=274799
2022-02-14 22:02:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 22:08:51 | INFO | train_inner | epoch 032:    311 / 1576 loss=6.739, nll_loss=4.964, ppl=31.22, wps=11672.3, ups=0.18, wpb=65536, bsz=128, num_updates=49000, lr=0.000142857, gnorm=0.545, loss_scale=8, train_wall=551, gb_free=8.8, wall=275361
2022-02-14 22:18:07 | INFO | train_inner | epoch 032:    411 / 1576 loss=6.747, nll_loss=4.974, ppl=31.42, wps=11786.9, ups=0.18, wpb=65536, bsz=128, num_updates=49100, lr=0.000142712, gnorm=0.536, loss_scale=8, train_wall=545, gb_free=8.8, wall=275917
2022-02-14 22:26:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 22:27:28 | INFO | train_inner | epoch 032:    512 / 1576 loss=6.753, nll_loss=4.981, ppl=31.57, wps=11672.3, ups=0.18, wpb=65532.3, bsz=128, num_updates=49200, lr=0.000142566, gnorm=0.55, loss_scale=8, train_wall=551, gb_free=8.8, wall=276478
2022-02-14 22:36:44 | INFO | train_inner | epoch 032:    612 / 1576 loss=6.763, nll_loss=4.992, ppl=31.82, wps=11792.5, ups=0.18, wpb=65536, bsz=128, num_updates=49300, lr=0.000142422, gnorm=0.533, loss_scale=8, train_wall=545, gb_free=8.8, wall=277034
2022-02-14 22:46:00 | INFO | train_inner | epoch 032:    712 / 1576 loss=6.763, nll_loss=4.992, ppl=31.82, wps=11788.9, ups=0.18, wpb=65536, bsz=128, num_updates=49400, lr=0.000142278, gnorm=0.542, loss_scale=8, train_wall=545, gb_free=8.8, wall=277590
2022-02-14 22:51:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 22:55:22 | INFO | train_inner | epoch 032:    813 / 1576 loss=6.763, nll_loss=4.992, ppl=31.83, wps=11671, ups=0.18, wpb=65536, bsz=128, num_updates=49500, lr=0.000142134, gnorm=0.529, loss_scale=8, train_wall=551, gb_free=8.8, wall=278151
2022-02-14 23:04:38 | INFO | train_inner | epoch 032:    913 / 1576 loss=6.777, nll_loss=5.007, ppl=32.16, wps=11786.5, ups=0.18, wpb=65536, bsz=128, num_updates=49600, lr=0.00014199, gnorm=0.529, loss_scale=8, train_wall=545, gb_free=8.8, wall=278707
2022-02-14 23:13:54 | INFO | train_inner | epoch 032:   1013 / 1576 loss=6.77, nll_loss=5, ppl=32, wps=11783.7, ups=0.18, wpb=65536, bsz=128, num_updates=49700, lr=0.000141848, gnorm=0.544, loss_scale=8, train_wall=546, gb_free=8.8, wall=279263
2022-02-14 23:16:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 23:23:15 | INFO | train_inner | epoch 032:   1114 / 1576 loss=6.782, nll_loss=5.014, ppl=32.31, wps=11672.8, ups=0.18, wpb=65536, bsz=128, num_updates=49800, lr=0.000141705, gnorm=0.54, loss_scale=8, train_wall=551, gb_free=8.8, wall=279825
2022-02-14 23:32:31 | INFO | train_inner | epoch 032:   1214 / 1576 loss=6.789, nll_loss=5.021, ppl=32.47, wps=11789.1, ups=0.18, wpb=65536, bsz=128, num_updates=49900, lr=0.000141563, gnorm=0.536, loss_scale=8, train_wall=545, gb_free=8.8, wall=280381
2022-02-14 23:41:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 23:41:53 | INFO | train_inner | epoch 032:   1315 / 1576 loss=6.786, nll_loss=5.018, ppl=32.4, wps=11666.9, ups=0.18, wpb=65536, bsz=128, num_updates=50000, lr=0.000141421, gnorm=0.536, loss_scale=8, train_wall=551, gb_free=8.8, wall=280942
2022-02-14 23:41:53 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2022-02-14 23:41:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 23:42:00 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.91 | nll_loss 5.138 | ppl 35.22 | wps 32486.3 | wpb 1021.8 | bsz 2 | num_updates 50000 | best_loss 6.91
2022-02-14 23:42:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 50000 updates
2022-02-14 23:42:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-02-14 23:42:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-02-14 23:42:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 32 @ 50000 updates, score 6.91) (writing took 19.05227751750499 seconds)
2022-02-14 23:42:19 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-14 23:42:19 | INFO | train | epoch 032 | loss 6.76 | nll_loss 4.988 | ppl 31.74 | wps 11702 | ups 0.18 | wpb 65535.7 | bsz 128 | num_updates 50000 | lr 0.000141421 | gnorm 0.538 | loss_scale 8 | train_wall 7171 | gb_free 8.8 | wall 280968
2022-02-14 23:42:19 | INFO | fairseq_cli.train | done training in 280967.8 seconds
