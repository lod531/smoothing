Sender: LSF System <lsfadmin@eu-g2-09>
Subject: Job 208722285: <de_dropout_0.3_jelinek_0.01_0.04_0.95_#1> in cluster <euler> Done

Job <de_dropout_0.3_jelinek_0.01_0.04_0.95_#1> was submitted from host <eu-login-24> by user <andriusb> in cluster <euler> at Tue Mar 15 16:17:55 2022
Job was executed on host(s) <eu-g2-09>, in queue <gpu.120h>, as user <andriusb> in cluster <euler> at Tue Mar 15 16:32:50 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar 15 16:32:50 2022
Terminated at Wed Mar 16 11:10:41 2022
Results reported at Wed Mar 16 11:10:41 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/de --save-dir /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.3 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.01,0.04, 0.95)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --patience 3 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   70316.53 sec.
    Max Memory :                                 3744 MB
    Average Memory :                             2900.53 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               16256.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   67070 sec.
    Turnaround time :                            67966 sec.

The output (if any) follows:

2022-03-15 16:32:58 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.3, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/de', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.01,0.04, 0.95)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-15 16:32:58 | INFO | fairseq.tasks.language_modeling | dictionary: 34528 types
2022-03-15 16:32:59 | INFO | fairseq.data.data_utils | loaded 45,920 examples from: data-bin/de/train
Calculating frequency stats:
  0%|          | 0/45920 [00:00<?, ?it/s]  0%|          | 74/45920 [00:00<01:03, 726.35it/s]  0%|          | 163/45920 [00:00<00:55, 819.35it/s]  1%|          | 245/45920 [00:00<01:16, 596.30it/s]  1%|          | 327/45920 [00:00<01:12, 624.62it/s]  1%|          | 426/45920 [00:00<01:02, 730.78it/s]  1%|          | 504/45920 [00:00<01:01, 743.25it/s]  1%|▏         | 588/45920 [00:00<00:58, 770.82it/s]  1%|▏         | 671/45920 [00:00<00:59, 765.28it/s]  2%|▏         | 750/45920 [00:01<01:18, 576.84it/s]  2%|▏         | 822/45920 [00:01<01:13, 609.97it/s]  2%|▏         | 890/45920 [00:01<01:11, 625.52it/s]  2%|▏         | 982/45920 [00:01<01:03, 703.53it/s]  2%|▏         | 1076/45920 [00:01<00:58, 766.35it/s]  3%|▎         | 1160/45920 [00:01<00:57, 784.81it/s]  3%|▎         | 1260/45920 [00:01<00:53, 838.74it/s]  3%|▎         | 1351/45920 [00:01<00:52, 851.24it/s]  3%|▎         | 1446/45920 [00:01<00:50, 877.76it/s]  3%|▎         | 1539/45920 [00:02<00:50, 882.57it/s]  4%|▎         | 1629/45920 [00:02<00:55, 798.69it/s]  4%|▎         | 1711/45920 [00:02<00:57, 774.61it/s]  4%|▍         | 1816/45920 [00:02<00:51, 849.42it/s]  4%|▍         | 1912/45920 [00:02<00:50, 873.46it/s]  4%|▍         | 2001/45920 [00:02<00:50, 870.74it/s]  5%|▍         | 2089/45920 [00:02<00:52, 839.97it/s]  5%|▍         | 2174/45920 [00:02<00:54, 807.04it/s]  5%|▍         | 2258/45920 [00:02<00:53, 815.45it/s]  5%|▌         | 2341/45920 [00:03<01:01, 703.08it/s]  5%|▌         | 2422/45920 [00:03<00:59, 730.16it/s]  5%|▌         | 2514/45920 [00:03<00:55, 779.12it/s]  6%|▌         | 2595/45920 [00:03<00:58, 744.71it/s]  6%|▌         | 2695/45920 [00:03<00:53, 812.08it/s]  6%|▌         | 2779/45920 [00:03<00:56, 765.87it/s]  6%|▋         | 2887/45920 [00:03<00:50, 849.00it/s]  6%|▋         | 2978/45920 [00:03<00:49, 864.13it/s]  7%|▋         | 3066/45920 [00:03<00:56, 758.60it/s]  7%|▋         | 3146/45920 [00:04<00:58, 737.02it/s]  7%|▋         | 3231/45920 [00:04<00:55, 764.52it/s]  7%|▋         | 3311/45920 [00:04<00:55, 771.88it/s]  7%|▋         | 3393/45920 [00:04<00:54, 784.78it/s]  8%|▊         | 3473/45920 [00:04<00:54, 776.97it/s]  8%|▊         | 3552/45920 [00:04<00:56, 752.91it/s]  8%|▊         | 3628/45920 [00:04<01:06, 632.07it/s]  8%|▊         | 3707/45920 [00:04<01:02, 671.86it/s]  8%|▊         | 3792/45920 [00:05<01:04, 653.15it/s]  8%|▊         | 3880/45920 [00:05<00:59, 711.23it/s]  9%|▊         | 3954/45920 [00:05<01:04, 648.48it/s]  9%|▉         | 4022/45920 [00:05<01:04, 653.24it/s]  9%|▉         | 4090/45920 [00:05<01:06, 627.43it/s]  9%|▉         | 4155/45920 [00:05<01:08, 610.55it/s]  9%|▉         | 4240/45920 [00:05<01:01, 674.63it/s]  9%|▉         | 4309/45920 [00:05<01:16, 544.13it/s] 10%|▉         | 4395/45920 [00:06<01:07, 619.29it/s] 10%|▉         | 4482/45920 [00:06<01:00, 679.48it/s] 10%|▉         | 4579/45920 [00:06<00:55, 751.19it/s] 10%|█         | 4659/45920 [00:06<01:00, 681.42it/s] 10%|█         | 4756/45920 [00:06<00:54, 749.16it/s] 11%|█         | 4835/45920 [00:06<00:54, 751.66it/s] 11%|█         | 4930/45920 [00:06<00:51, 803.13it/s] 11%|█         | 5022/45920 [00:06<00:49, 834.53it/s] 11%|█         | 5108/45920 [00:06<00:53, 763.68it/s] 11%|█▏        | 5187/45920 [00:07<00:55, 735.43it/s] 11%|█▏        | 5265/45920 [00:07<00:54, 746.15it/s] 12%|█▏        | 5345/45920 [00:07<00:53, 759.59it/s] 12%|█▏        | 5451/45920 [00:07<00:48, 841.99it/s] 12%|█▏        | 5537/45920 [00:07<00:54, 738.38it/s] 12%|█▏        | 5614/45920 [00:07<00:54, 737.01it/s] 12%|█▏        | 5690/45920 [00:07<00:56, 710.55it/s] 13%|█▎        | 5763/45920 [00:07<00:57, 694.65it/s] 13%|█▎        | 5838/45920 [00:07<00:56, 709.62it/s] 13%|█▎        | 5919/45920 [00:08<00:55, 726.58it/s] 13%|█▎        | 5993/45920 [00:08<00:55, 717.16it/s] 13%|█▎        | 6066/45920 [00:08<00:59, 665.55it/s] 13%|█▎        | 6144/45920 [00:08<00:58, 684.41it/s] 14%|█▎        | 6218/45920 [00:08<00:57, 688.71it/s] 14%|█▎        | 6311/45920 [00:08<00:52, 756.21it/s] 14%|█▍        | 6401/45920 [00:08<00:49, 796.49it/s] 14%|█▍        | 6488/45920 [00:08<00:49, 789.70it/s] 14%|█▍        | 6588/45920 [00:08<00:46, 839.10it/s] 15%|█▍        | 6673/45920 [00:09<00:56, 690.21it/s] 15%|█▍        | 6761/45920 [00:09<00:53, 735.98it/s] 15%|█▍        | 6849/45920 [00:09<00:50, 773.83it/s] 15%|█▌        | 6930/45920 [00:09<00:51, 757.80it/s] 15%|█▌        | 7033/45920 [00:09<00:46, 832.19it/s] 16%|█▌        | 7121/45920 [00:09<00:45, 845.39it/s] 16%|█▌        | 7208/45920 [00:09<00:47, 809.24it/s] 16%|█▌        | 7291/45920 [00:09<00:50, 770.17it/s] 16%|█▌        | 7370/45920 [00:09<00:53, 723.57it/s] 16%|█▌        | 7453/45920 [00:10<00:51, 750.61it/s] 16%|█▋        | 7530/45920 [00:10<00:52, 733.79it/s] 17%|█▋        | 7605/45920 [00:10<01:03, 602.90it/s] 17%|█▋        | 7670/45920 [00:10<01:20, 475.03it/s] 17%|█▋        | 7747/45920 [00:10<01:11, 533.85it/s] 17%|█▋        | 7822/45920 [00:10<01:05, 581.92it/s] 17%|█▋        | 7913/45920 [00:10<00:57, 661.59it/s] 17%|█▋        | 7998/45920 [00:10<00:53, 707.86it/s] 18%|█▊        | 8074/45920 [00:11<00:53, 708.88it/s] 18%|█▊        | 8167/45920 [00:11<00:49, 767.48it/s] 18%|█▊        | 8247/45920 [00:11<00:52, 714.70it/s] 18%|█▊        | 8340/45920 [00:11<00:48, 772.10it/s] 18%|█▊        | 8438/45920 [00:11<00:45, 816.13it/s] 19%|█▊        | 8522/45920 [00:11<00:52, 706.27it/s] 19%|█▉        | 8613/45920 [00:11<00:49, 758.23it/s] 19%|█▉        | 8693/45920 [00:11<00:49, 754.61it/s] 19%|█▉        | 8771/45920 [00:11<00:50, 728.58it/s] 19%|█▉        | 8851/45920 [00:12<00:51, 725.51it/s] 19%|█▉        | 8925/45920 [00:12<00:52, 698.39it/s] 20%|█▉        | 9009/45920 [00:12<00:50, 735.84it/s] 20%|█▉        | 9101/45920 [00:12<00:46, 783.63it/s] 20%|█▉        | 9181/45920 [00:12<00:51, 715.32it/s] 20%|██        | 9278/45920 [00:12<00:47, 779.45it/s] 20%|██        | 9358/45920 [00:12<00:46, 785.13it/s] 21%|██        | 9438/45920 [00:12<00:48, 759.40it/s] 21%|██        | 9530/45920 [00:12<00:46, 786.97it/s] 21%|██        | 9610/45920 [00:13<00:50, 715.59it/s] 21%|██        | 9684/45920 [00:13<00:56, 643.53it/s] 21%|██▏       | 9761/45920 [00:13<00:53, 674.63it/s] 21%|██▏       | 9843/45920 [00:13<00:51, 701.95it/s] 22%|██▏       | 9915/45920 [00:13<00:54, 655.43it/s] 22%|██▏       | 10005/45920 [00:13<00:49, 719.63it/s] 22%|██▏       | 10079/45920 [00:13<00:50, 710.19it/s] 22%|██▏       | 10152/45920 [00:13<00:54, 651.44it/s] 22%|██▏       | 10240/45920 [00:14<00:50, 708.41it/s] 23%|██▎       | 10343/45920 [00:14<00:44, 794.46it/s] 23%|██▎       | 10425/45920 [00:14<00:46, 765.57it/s] 23%|██▎       | 10509/45920 [00:14<00:47, 742.98it/s] 23%|██▎       | 10590/45920 [00:14<00:46, 759.01it/s] 23%|██▎       | 10667/45920 [00:14<00:50, 698.90it/s] 23%|██▎       | 10764/45920 [00:14<00:45, 770.97it/s] 24%|██▎       | 10855/45920 [00:14<00:49, 713.96it/s] 24%|██▍       | 10929/45920 [00:14<00:49, 700.94it/s] 24%|██▍       | 11018/45920 [00:15<00:46, 750.32it/s] 24%|██▍       | 11095/45920 [00:15<00:48, 718.11it/s] 24%|██▍       | 11184/45920 [00:15<00:45, 763.56it/s] 25%|██▍       | 11266/45920 [00:15<00:44, 777.98it/s] 25%|██▍       | 11356/45920 [00:15<00:43, 790.40it/s] 25%|██▍       | 11436/45920 [00:15<00:48, 707.51it/s] 25%|██▌       | 11509/45920 [00:15<00:48, 703.43it/s] 25%|██▌       | 11611/45920 [00:15<00:43, 788.09it/s] 25%|██▌       | 11692/45920 [00:16<00:50, 681.90it/s] 26%|██▌       | 11764/45920 [00:16<00:55, 615.05it/s] 26%|██▌       | 11829/45920 [00:16<00:55, 611.85it/s] 26%|██▌       | 11912/45920 [00:16<00:51, 663.80it/s] 26%|██▌       | 11981/45920 [00:16<00:53, 629.10it/s] 26%|██▋       | 12054/45920 [00:16<00:51, 655.55it/s] 26%|██▋       | 12122/45920 [00:16<00:55, 604.71it/s] 27%|██▋       | 12210/45920 [00:16<00:50, 669.72it/s] 27%|██▋       | 12300/45920 [00:16<00:46, 727.37it/s] 27%|██▋       | 12402/45920 [00:17<00:41, 805.67it/s] 27%|██▋       | 12494/45920 [00:17<00:39, 836.97it/s] 27%|██▋       | 12580/45920 [00:17<00:41, 810.46it/s] 28%|██▊       | 12673/45920 [00:17<00:39, 842.94it/s] 28%|██▊       | 12759/45920 [00:17<00:43, 770.67it/s] 28%|██▊       | 12838/45920 [00:17<00:51, 643.76it/s] 28%|██▊       | 12936/45920 [00:17<00:45, 725.42it/s] 28%|██▊       | 13014/45920 [00:17<00:45, 725.05it/s] 29%|██▊       | 13090/45920 [00:17<00:45, 720.10it/s] 29%|██▊       | 13187/45920 [00:18<00:41, 786.19it/s] 29%|██▉       | 13273/45920 [00:18<00:40, 806.22it/s] 29%|██▉       | 13364/45920 [00:18<00:39, 827.95it/s] 29%|██▉       | 13449/45920 [00:18<00:42, 772.27it/s] 29%|██▉       | 13528/45920 [00:18<00:44, 729.63it/s] 30%|██▉       | 13622/45920 [00:18<00:42, 765.73it/s] 30%|██▉       | 13700/45920 [00:18<00:43, 744.91it/s] 30%|███       | 13791/45920 [00:18<00:40, 787.82it/s] 30%|███       | 13888/45920 [00:18<00:38, 837.82it/s] 30%|███       | 13973/45920 [00:19<00:42, 754.04it/s] 31%|███       | 14051/45920 [00:19<00:42, 742.48it/s] 31%|███       | 14127/45920 [00:19<00:49, 636.82it/s] 31%|███       | 14194/45920 [00:19<00:50, 624.78it/s] 31%|███       | 14265/45920 [00:19<00:49, 642.76it/s] 31%|███       | 14332/45920 [00:19<00:49, 639.79it/s] 31%|███▏      | 14413/45920 [00:19<00:46, 682.22it/s] 32%|███▏      | 14502/45920 [00:19<00:42, 739.09it/s] 32%|███▏      | 14578/45920 [00:20<00:46, 672.09it/s] 32%|███▏      | 14653/45920 [00:20<00:45, 691.72it/s] 32%|███▏      | 14724/45920 [00:20<00:47, 663.35it/s] 32%|███▏      | 14792/45920 [00:20<00:48, 647.84it/s] 32%|███▏      | 14875/45920 [00:20<00:44, 697.00it/s] 33%|███▎      | 14959/45920 [00:20<00:42, 731.16it/s] 33%|███▎      | 15033/45920 [00:20<00:43, 717.35it/s] 33%|███▎      | 15106/45920 [00:20<00:53, 574.94it/s] 33%|███▎      | 15205/45920 [00:20<00:45, 672.84it/s] 33%|███▎      | 15278/45920 [00:21<00:45, 667.63it/s] 33%|███▎      | 15349/45920 [00:21<00:45, 677.57it/s] 34%|███▎      | 15420/45920 [00:21<00:49, 619.58it/s] 34%|███▍      | 15500/45920 [00:21<00:45, 665.42it/s] 34%|███▍      | 15582/45920 [00:21<00:43, 705.52it/s] 34%|███▍      | 15655/45920 [00:21<00:46, 652.38it/s] 34%|███▍      | 15752/45920 [00:21<00:41, 732.85it/s] 34%|███▍      | 15828/45920 [00:21<00:44, 679.43it/s] 35%|███▍      | 15899/45920 [00:21<00:44, 680.47it/s] 35%|███▍      | 15981/45920 [00:22<00:41, 714.32it/s] 35%|███▍      | 16063/45920 [00:22<00:40, 742.62it/s] 35%|███▌      | 16139/45920 [00:22<00:42, 692.89it/s] 35%|███▌      | 16226/45920 [00:22<00:40, 739.39it/s] 36%|███▌      | 16302/45920 [00:22<00:47, 621.53it/s] 36%|███▌      | 16402/45920 [00:22<00:41, 715.86it/s] 36%|███▌      | 16479/45920 [00:22<00:40, 720.87it/s] 36%|███▌      | 16555/45920 [00:22<00:45, 642.61it/s] 36%|███▋      | 16664/45920 [00:23<00:39, 744.31it/s] 36%|███▋      | 16751/45920 [00:23<00:38, 767.48it/s] 37%|███▋      | 16831/45920 [00:23<00:46, 619.17it/s] 37%|███▋      | 16900/45920 [00:23<00:47, 616.03it/s] 37%|███▋      | 16966/45920 [00:23<00:47, 605.17it/s] 37%|███▋      | 17030/45920 [00:23<00:47, 603.22it/s] 37%|███▋      | 17135/45920 [00:23<00:40, 717.90it/s] 37%|███▋      | 17219/45920 [00:23<00:38, 750.16it/s] 38%|███▊      | 17297/45920 [00:24<00:39, 721.93it/s] 38%|███▊      | 17371/45920 [00:24<00:39, 719.67it/s] 38%|███▊      | 17459/45920 [00:24<00:37, 762.56it/s] 38%|███▊      | 17564/45920 [00:24<00:33, 836.54it/s] 38%|███▊      | 17652/45920 [00:24<00:33, 846.63it/s] 39%|███▊      | 17738/45920 [00:24<00:34, 819.27it/s] 39%|███▉      | 17821/45920 [00:24<00:34, 817.87it/s] 39%|███▉      | 17904/45920 [00:24<00:39, 702.34it/s] 39%|███▉      | 17978/45920 [00:24<00:40, 682.16it/s] 39%|███▉      | 18051/45920 [00:25<00:40, 694.37it/s] 40%|███▉      | 18150/45920 [00:25<00:35, 772.24it/s] 40%|███▉      | 18238/45920 [00:25<00:34, 798.96it/s] 40%|███▉      | 18337/45920 [00:25<00:32, 842.75it/s] 40%|████      | 18423/45920 [00:25<00:32, 839.88it/s] 40%|████      | 18508/45920 [00:25<00:40, 674.90it/s] 40%|████      | 18582/45920 [00:25<00:51, 534.03it/s] 41%|████      | 18649/45920 [00:25<00:48, 562.55it/s] 41%|████      | 18739/45920 [00:26<00:42, 640.49it/s] 41%|████      | 18812/45920 [00:26<00:40, 662.79it/s] 41%|████      | 18912/45920 [00:26<00:35, 750.46it/s] 41%|████▏     | 19007/45920 [00:26<00:33, 803.93it/s] 42%|████▏     | 19093/45920 [00:26<00:32, 818.61it/s] 42%|████▏     | 19178/45920 [00:26<00:34, 768.67it/s] 42%|████▏     | 19258/45920 [00:26<00:34, 776.55it/s] 42%|████▏     | 19338/45920 [00:26<00:37, 705.75it/s] 42%|████▏     | 19431/45920 [00:26<00:34, 765.43it/s] 42%|████▏     | 19510/45920 [00:27<00:35, 736.01it/s] 43%|████▎     | 19623/45920 [00:27<00:31, 841.56it/s] 43%|████▎     | 19710/45920 [00:27<00:33, 776.83it/s] 43%|████▎     | 19810/45920 [00:27<00:31, 833.94it/s] 43%|████▎     | 19896/45920 [00:27<00:36, 713.41it/s] 44%|████▎     | 19998/45920 [00:27<00:32, 789.11it/s] 44%|████▎     | 20082/45920 [00:27<00:34, 751.44it/s] 44%|████▍     | 20161/45920 [00:27<00:34, 745.21it/s] 44%|████▍     | 20248/45920 [00:27<00:33, 755.57it/s] 44%|████▍     | 20348/45920 [00:28<00:31, 820.50it/s] 44%|████▍     | 20432/45920 [00:28<00:32, 780.85it/s] 45%|████▍     | 20512/45920 [00:28<00:33, 756.60it/s] 45%|████▍     | 20601/45920 [00:28<00:34, 731.97it/s] 45%|████▌     | 20705/45920 [00:28<00:31, 811.31it/s] 45%|████▌     | 20798/45920 [00:28<00:30, 826.13it/s] 45%|████▌     | 20882/45920 [00:28<00:33, 753.58it/s] 46%|████▌     | 20961/45920 [00:28<00:32, 758.31it/s] 46%|████▌     | 21039/45920 [00:28<00:33, 741.68it/s] 46%|████▌     | 21115/45920 [00:29<00:39, 625.43it/s] 46%|████▌     | 21186/45920 [00:29<00:38, 639.66it/s] 46%|████▋     | 21267/45920 [00:29<00:36, 681.58it/s] 46%|████▋     | 21338/45920 [00:29<00:38, 637.83it/s] 47%|████▋     | 21431/45920 [00:29<00:34, 714.37it/s] 47%|████▋     | 21505/45920 [00:29<00:34, 701.74it/s] 47%|████▋     | 21605/45920 [00:29<00:31, 780.95it/s] 47%|████▋     | 21685/45920 [00:29<00:32, 748.62it/s] 47%|████▋     | 21762/45920 [00:30<00:34, 699.29it/s] 48%|████▊     | 21834/45920 [00:30<00:37, 640.00it/s] 48%|████▊     | 21900/45920 [00:30<00:38, 624.79it/s] 48%|████▊     | 21978/45920 [00:30<00:37, 635.55it/s] 48%|████▊     | 22047/45920 [00:30<00:36, 648.75it/s] 48%|████▊     | 22117/45920 [00:30<00:36, 658.51it/s] 48%|████▊     | 22223/45920 [00:30<00:30, 770.27it/s] 49%|████▊     | 22321/45920 [00:30<00:28, 830.06it/s] 49%|████▉     | 22406/45920 [00:30<00:28, 823.56it/s] 49%|████▉     | 22490/45920 [00:31<00:28, 826.69it/s] 49%|████▉     | 22576/45920 [00:31<00:28, 825.76it/s] 49%|████▉     | 22659/45920 [00:31<00:31, 731.90it/s] 50%|████▉     | 22735/45920 [00:31<00:31, 736.82it/s] 50%|████▉     | 22837/45920 [00:31<00:28, 812.78it/s] 50%|████▉     | 22925/45920 [00:31<00:27, 829.94it/s] 50%|█████     | 23015/45920 [00:31<00:27, 847.94it/s] 50%|█████     | 23101/45920 [00:31<00:27, 843.17it/s] 50%|█████     | 23186/45920 [00:31<00:29, 763.15it/s] 51%|█████     | 23267/45920 [00:32<00:29, 775.20it/s] 51%|█████     | 23358/45920 [00:32<00:27, 810.66it/s] 51%|█████     | 23441/45920 [00:32<00:31, 721.18it/s] 51%|█████     | 23516/45920 [00:32<00:32, 680.92it/s] 51%|█████▏    | 23587/45920 [00:32<00:35, 632.39it/s] 52%|█████▏    | 23662/45920 [00:32<00:34, 652.79it/s] 52%|█████▏    | 23750/45920 [00:32<00:31, 708.74it/s] 52%|█████▏    | 23823/45920 [00:32<00:31, 697.38it/s] 52%|█████▏    | 23911/45920 [00:32<00:29, 747.67it/s] 52%|█████▏    | 23987/45920 [00:33<00:30, 712.38it/s] 52%|█████▏    | 24073/45920 [00:33<00:29, 752.82it/s] 53%|█████▎    | 24150/45920 [00:33<00:30, 721.66it/s] 53%|█████▎    | 24224/45920 [00:33<00:34, 629.61it/s] 53%|█████▎    | 24290/45920 [00:33<00:34, 627.12it/s] 53%|█████▎    | 24381/45920 [00:33<00:30, 701.90it/s] 53%|█████▎    | 24471/45920 [00:33<00:28, 756.11it/s] 53%|█████▎    | 24565/45920 [00:33<00:26, 806.00it/s] 54%|█████▎    | 24671/45920 [00:33<00:24, 874.33it/s] 54%|█████▍    | 24760/45920 [00:34<00:25, 842.79it/s] 54%|█████▍    | 24846/45920 [00:34<00:26, 793.60it/s] 54%|█████▍    | 24931/45920 [00:34<00:26, 806.09it/s] 54%|█████▍    | 25013/45920 [00:34<00:30, 687.46it/s] 55%|█████▍    | 25093/45920 [00:34<00:29, 710.42it/s] 55%|█████▍    | 25168/45920 [00:34<00:28, 718.68it/s] 55%|█████▍    | 25252/45920 [00:34<00:27, 745.92it/s] 55%|█████▌    | 25343/45920 [00:34<00:26, 790.20it/s] 55%|█████▌    | 25424/45920 [00:34<00:25, 792.39it/s] 56%|█████▌    | 25516/45920 [00:35<00:24, 828.86it/s] 56%|█████▌    | 25600/45920 [00:35<00:26, 765.27it/s] 56%|█████▌    | 25679/45920 [00:35<00:29, 690.95it/s] 56%|█████▌    | 25756/45920 [00:35<00:28, 711.44it/s] 56%|█████▋    | 25861/45920 [00:35<00:24, 802.57it/s] 56%|█████▋    | 25944/45920 [00:35<00:24, 807.88it/s] 57%|█████▋    | 26029/45920 [00:35<00:24, 818.32it/s] 57%|█████▋    | 26112/45920 [00:35<00:26, 752.12it/s] 57%|█████▋    | 26189/45920 [00:36<00:30, 656.59it/s] 57%|█████▋    | 26289/45920 [00:36<00:26, 743.45it/s] 57%|█████▋    | 26368/45920 [00:36<00:26, 735.85it/s] 58%|█████▊    | 26446/45920 [00:36<00:26, 744.69it/s] 58%|█████▊    | 26525/45920 [00:36<00:26, 725.21it/s] 58%|█████▊    | 26609/45920 [00:36<00:25, 754.41it/s] 58%|█████▊    | 26693/45920 [00:36<00:24, 774.71it/s] 58%|█████▊    | 26784/45920 [00:36<00:23, 806.89it/s] 59%|█████▊    | 26866/45920 [00:36<00:24, 776.52it/s] 59%|█████▊    | 26945/45920 [00:37<00:27, 702.21it/s] 59%|█████▉    | 27034/45920 [00:37<00:25, 749.56it/s] 59%|█████▉    | 27116/45920 [00:37<00:24, 766.88it/s] 59%|█████▉    | 27195/45920 [00:37<00:34, 542.42it/s] 59%|█████▉    | 27273/45920 [00:37<00:31, 593.41it/s] 60%|█████▉    | 27359/45920 [00:37<00:28, 656.44it/s] 60%|█████▉    | 27434/45920 [00:37<00:27, 679.29it/s] 60%|█████▉    | 27508/45920 [00:37<00:27, 665.28it/s] 60%|██████    | 27579/45920 [00:37<00:28, 645.18it/s] 60%|██████    | 27657/45920 [00:38<00:27, 669.71it/s] 60%|██████    | 27727/45920 [00:38<00:27, 657.05it/s] 61%|██████    | 27796/45920 [00:38<00:27, 661.01it/s] 61%|██████    | 27864/45920 [00:38<00:28, 630.29it/s] 61%|██████    | 27928/45920 [00:38<00:35, 507.03it/s] 61%|██████    | 28022/45920 [00:38<00:29, 609.50it/s] 61%|██████    | 28098/45920 [00:38<00:27, 647.63it/s] 61%|██████▏   | 28168/45920 [00:38<00:28, 625.90it/s] 62%|██████▏   | 28243/45920 [00:39<00:26, 656.41it/s] 62%|██████▏   | 28319/45920 [00:39<00:25, 684.59it/s] 62%|██████▏   | 28405/45920 [00:39<00:23, 733.83it/s] 62%|██████▏   | 28488/45920 [00:39<00:22, 761.15it/s] 62%|██████▏   | 28566/45920 [00:39<00:23, 741.94it/s] 62%|██████▏   | 28642/45920 [00:39<00:24, 696.91it/s] 63%|██████▎   | 28720/45920 [00:39<00:24, 716.30it/s] 63%|██████▎   | 28797/45920 [00:39<00:23, 728.53it/s] 63%|██████▎   | 28871/45920 [00:39<00:23, 727.97it/s] 63%|██████▎   | 28951/45920 [00:39<00:22, 747.91it/s] 63%|██████▎   | 29042/45920 [00:40<00:22, 753.17it/s] 63%|██████▎   | 29118/45920 [00:40<00:22, 750.51it/s] 64%|██████▎   | 29194/45920 [00:40<00:22, 738.88it/s] 64%|██████▎   | 29269/45920 [00:40<00:27, 605.91it/s] 64%|██████▍   | 29343/45920 [00:40<00:27, 605.27it/s] 64%|██████▍   | 29407/45920 [00:40<00:27, 609.03it/s] 64%|██████▍   | 29489/45920 [00:40<00:25, 657.00it/s] 64%|██████▍   | 29558/45920 [00:40<00:24, 664.86it/s] 65%|██████▍   | 29637/45920 [00:41<00:23, 693.15it/s] 65%|██████▍   | 29716/45920 [00:41<00:22, 717.21it/s] 65%|██████▍   | 29789/45920 [00:41<00:25, 630.51it/s] 65%|██████▌   | 29859/45920 [00:41<00:25, 619.76it/s] 65%|██████▌   | 29932/45920 [00:41<00:24, 648.94it/s] 65%|██████▌   | 30011/45920 [00:41<00:23, 686.07it/s] 66%|██████▌   | 30082/45920 [00:41<00:23, 688.11it/s] 66%|██████▌   | 30166/45920 [00:41<00:21, 728.25it/s] 66%|██████▌   | 30240/45920 [00:41<00:25, 626.85it/s] 66%|██████▌   | 30326/45920 [00:42<00:22, 687.59it/s] 66%|██████▌   | 30398/45920 [00:42<00:23, 668.67it/s] 66%|██████▋   | 30492/45920 [00:42<00:20, 739.39it/s] 67%|██████▋   | 30571/45920 [00:42<00:20, 752.75it/s] 67%|██████▋   | 30656/45920 [00:42<00:19, 780.15it/s] 67%|██████▋   | 30736/45920 [00:42<00:19, 759.20it/s] 67%|██████▋   | 30813/45920 [00:42<00:19, 758.26it/s] 67%|██████▋   | 30899/45920 [00:42<00:20, 726.74it/s] 67%|██████▋   | 30978/45920 [00:42<00:20, 743.93it/s] 68%|██████▊   | 31076/45920 [00:43<00:18, 807.26it/s] 68%|██████▊   | 31166/45920 [00:43<00:17, 833.70it/s] 68%|██████▊   | 31251/45920 [00:43<00:18, 804.08it/s] 68%|██████▊   | 31333/45920 [00:43<00:18, 793.53it/s] 68%|██████▊   | 31413/45920 [00:43<00:18, 791.23it/s] 69%|██████▊   | 31493/45920 [00:43<00:24, 592.59it/s] 69%|██████▊   | 31560/45920 [00:43<00:25, 572.34it/s] 69%|██████▉   | 31623/45920 [00:43<00:24, 585.46it/s] 69%|██████▉   | 31695/45920 [00:44<00:22, 619.31it/s] 69%|██████▉   | 31761/45920 [00:44<00:23, 608.27it/s] 69%|██████▉   | 31825/45920 [00:44<00:24, 564.56it/s] 69%|██████▉   | 31893/45920 [00:44<00:25, 550.57it/s] 70%|██████▉   | 31983/45920 [00:44<00:21, 637.29it/s] 70%|██████▉   | 32068/45920 [00:44<00:19, 693.23it/s] 70%|██████▉   | 32140/45920 [00:44<00:19, 700.59it/s] 70%|███████   | 32212/45920 [00:44<00:20, 666.93it/s] 70%|███████   | 32310/45920 [00:44<00:18, 752.78it/s] 71%|███████   | 32388/45920 [00:45<00:18, 713.68it/s] 71%|███████   | 32466/45920 [00:45<00:18, 731.72it/s] 71%|███████   | 32551/45920 [00:45<00:17, 762.98it/s] 71%|███████   | 32637/45920 [00:45<00:17, 775.34it/s] 71%|███████   | 32716/45920 [00:45<00:19, 665.28it/s] 71%|███████▏  | 32800/45920 [00:45<00:18, 708.32it/s] 72%|███████▏  | 32888/45920 [00:45<00:17, 751.01it/s] 72%|███████▏  | 32973/45920 [00:45<00:16, 764.69it/s] 72%|███████▏  | 33055/45920 [00:45<00:16, 773.65it/s] 72%|███████▏  | 33134/45920 [00:46<00:17, 747.39it/s] 72%|███████▏  | 33224/45920 [00:46<00:16, 782.02it/s] 73%|███████▎  | 33309/45920 [00:46<00:15, 797.50it/s] 73%|███████▎  | 33402/45920 [00:46<00:14, 835.56it/s] 73%|███████▎  | 33488/45920 [00:46<00:14, 842.14it/s] 73%|███████▎  | 33575/45920 [00:46<00:14, 847.70it/s] 73%|███████▎  | 33661/45920 [00:46<00:15, 798.91it/s] 73%|███████▎  | 33742/45920 [00:46<00:16, 725.78it/s] 74%|███████▎  | 33823/45920 [00:46<00:16, 746.42it/s] 74%|███████▍  | 33909/45920 [00:46<00:15, 776.09it/s] 74%|███████▍  | 33988/45920 [00:47<00:15, 771.50it/s] 74%|███████▍  | 34072/45920 [00:47<00:15, 768.07it/s] 74%|███████▍  | 34150/45920 [00:47<00:16, 714.81it/s] 75%|███████▍  | 34223/45920 [00:47<00:16, 692.32it/s] 75%|███████▍  | 34293/45920 [00:47<00:17, 661.68it/s] 75%|███████▍  | 34376/45920 [00:47<00:16, 706.80it/s] 75%|███████▌  | 34448/45920 [00:47<00:16, 686.04it/s] 75%|███████▌  | 34527/45920 [00:47<00:15, 714.70it/s] 75%|███████▌  | 34629/45920 [00:47<00:14, 799.33it/s] 76%|███████▌  | 34710/45920 [00:48<00:15, 735.36it/s] 76%|███████▌  | 34812/45920 [00:48<00:13, 795.06it/s] 76%|███████▌  | 34893/45920 [00:48<00:15, 726.38it/s] 76%|███████▌  | 34968/45920 [00:48<00:15, 716.09it/s] 76%|███████▋  | 35041/45920 [00:48<00:17, 636.21it/s] 76%|███████▋  | 35107/45920 [00:48<00:17, 635.94it/s] 77%|███████▋  | 35174/45920 [00:48<00:16, 644.44it/s] 77%|███████▋  | 35262/45920 [00:48<00:15, 705.30it/s] 77%|███████▋  | 35361/45920 [00:49<00:13, 784.77it/s] 77%|███████▋  | 35441/45920 [00:49<00:16, 638.95it/s] 77%|███████▋  | 35520/45920 [00:49<00:15, 675.86it/s] 78%|███████▊  | 35593/45920 [00:49<00:14, 689.18it/s] 78%|███████▊  | 35685/45920 [00:49<00:13, 751.69it/s] 78%|███████▊  | 35764/45920 [00:49<00:15, 675.71it/s] 78%|███████▊  | 35840/45920 [00:49<00:14, 697.48it/s] 78%|███████▊  | 35928/45920 [00:49<00:14, 710.95it/s] 78%|███████▊  | 36010/45920 [00:49<00:13, 736.31it/s] 79%|███████▊  | 36089/45920 [00:50<00:13, 746.24it/s] 79%|███████▉  | 36165/45920 [00:50<00:13, 710.88it/s] 79%|███████▉  | 36244/45920 [00:50<00:13, 732.66it/s] 79%|███████▉  | 36319/45920 [00:50<00:13, 691.93it/s] 79%|███████▉  | 36390/45920 [00:50<00:14, 647.47it/s] 79%|███████▉  | 36456/45920 [00:50<00:15, 627.66it/s] 80%|███████▉  | 36545/45920 [00:50<00:13, 694.05it/s] 80%|███████▉  | 36616/45920 [00:50<00:13, 690.10it/s] 80%|███████▉  | 36686/45920 [00:50<00:14, 646.73it/s] 80%|████████  | 36772/45920 [00:51<00:13, 702.76it/s] 80%|████████  | 36844/45920 [00:51<00:15, 596.21it/s] 80%|████████  | 36936/45920 [00:51<00:13, 676.91it/s] 81%|████████  | 37008/45920 [00:51<00:13, 665.28it/s] 81%|████████  | 37102/45920 [00:51<00:12, 731.91it/s] 81%|████████  | 37183/45920 [00:51<00:11, 753.18it/s] 81%|████████  | 37271/45920 [00:51<00:10, 788.79it/s] 81%|████████▏ | 37352/45920 [00:51<00:11, 735.80it/s] 82%|████████▏ | 37430/45920 [00:52<00:11, 747.03it/s] 82%|████████▏ | 37507/45920 [00:52<00:12, 689.02it/s] 82%|████████▏ | 37581/45920 [00:52<00:11, 700.80it/s] 82%|████████▏ | 37653/45920 [00:52<00:11, 689.40it/s] 82%|████████▏ | 37726/45920 [00:52<00:11, 699.81it/s] 82%|████████▏ | 37815/45920 [00:52<00:10, 753.16it/s] 83%|████████▎ | 37896/45920 [00:52<00:10, 761.79it/s] 83%|████████▎ | 37990/45920 [00:52<00:09, 811.89it/s] 83%|████████▎ | 38072/45920 [00:52<00:12, 625.89it/s] 83%|████████▎ | 38142/45920 [00:53<00:12, 620.63it/s] 83%|████████▎ | 38224/45920 [00:53<00:11, 669.22it/s] 83%|████████▎ | 38299/45920 [00:53<00:11, 690.08it/s] 84%|████████▎ | 38379/45920 [00:53<00:10, 718.39it/s] 84%|████████▎ | 38454/45920 [00:53<00:10, 723.57it/s] 84%|████████▍ | 38544/45920 [00:53<00:09, 770.87it/s] 84%|████████▍ | 38623/45920 [00:53<00:11, 632.87it/s] 84%|████████▍ | 38692/45920 [00:53<00:11, 614.82it/s] 84%|████████▍ | 38757/45920 [00:53<00:11, 623.09it/s] 85%|████████▍ | 38823/45920 [00:54<00:11, 627.46it/s] 85%|████████▍ | 38902/45920 [00:54<00:10, 669.21it/s] 85%|████████▍ | 38971/45920 [00:54<00:10, 640.98it/s] 85%|████████▌ | 39048/45920 [00:54<00:10, 672.83it/s] 85%|████████▌ | 39117/45920 [00:54<00:10, 665.64it/s] 85%|████████▌ | 39201/45920 [00:54<00:09, 714.88it/s] 86%|████████▌ | 39274/45920 [00:54<00:10, 659.76it/s] 86%|████████▌ | 39342/45920 [00:54<00:11, 577.45it/s] 86%|████████▌ | 39413/45920 [00:54<00:10, 610.33it/s] 86%|████████▌ | 39502/45920 [00:55<00:09, 655.14it/s] 86%|████████▌ | 39588/45920 [00:55<00:08, 709.14it/s] 86%|████████▋ | 39661/45920 [00:55<00:09, 636.28it/s] 87%|████████▋ | 39728/45920 [00:55<00:10, 609.47it/s] 87%|████████▋ | 39791/45920 [00:55<00:10, 608.45it/s] 87%|████████▋ | 39869/45920 [00:55<00:09, 653.58it/s] 87%|████████▋ | 39973/45920 [00:55<00:07, 751.94it/s] 87%|████████▋ | 40050/45920 [00:55<00:08, 720.56it/s] 87%|████████▋ | 40124/45920 [00:56<00:08, 675.81it/s] 88%|████████▊ | 40205/45920 [00:56<00:08, 709.72it/s] 88%|████████▊ | 40278/45920 [00:56<00:08, 651.33it/s] 88%|████████▊ | 40345/45920 [00:56<00:09, 596.37it/s] 88%|████████▊ | 40430/45920 [00:56<00:08, 660.34it/s] 88%|████████▊ | 40509/45920 [00:56<00:07, 694.15it/s] 88%|████████▊ | 40594/45920 [00:56<00:07, 737.15it/s] 89%|████████▊ | 40683/45920 [00:56<00:06, 775.33it/s] 89%|████████▉ | 40763/45920 [00:56<00:06, 781.34it/s] 89%|████████▉ | 40843/45920 [00:57<00:07, 676.00it/s] 89%|████████▉ | 40931/45920 [00:57<00:06, 729.51it/s] 89%|████████▉ | 41007/45920 [00:57<00:06, 736.00it/s] 89%|████████▉ | 41095/45920 [00:57<00:06, 776.14it/s] 90%|████████▉ | 41175/45920 [00:57<00:06, 715.54it/s] 90%|████████▉ | 41255/45920 [00:57<00:06, 734.42it/s] 90%|█████████ | 41331/45920 [00:57<00:07, 638.31it/s] 90%|█████████ | 41401/45920 [00:57<00:06, 652.67it/s] 90%|█████████ | 41477/45920 [00:57<00:06, 681.07it/s] 90%|█████████ | 41548/45920 [00:58<00:06, 655.53it/s] 91%|█████████ | 41645/45920 [00:58<00:05, 739.66it/s] 91%|█████████ | 41734/45920 [00:58<00:05, 780.13it/s] 91%|█████████ | 41839/45920 [00:58<00:04, 854.71it/s] 91%|█████████▏| 41946/45920 [00:58<00:04, 915.57it/s] 92%|█████████▏| 42039/45920 [00:58<00:04, 895.88it/s] 92%|█████████▏| 42130/45920 [00:58<00:05, 703.52it/s] 92%|█████████▏| 42208/45920 [00:58<00:05, 718.56it/s] 92%|█████████▏| 42294/45920 [00:58<00:04, 754.81it/s] 92%|█████████▏| 42374/45920 [00:59<00:04, 737.11it/s] 92%|█████████▏| 42463/45920 [00:59<00:04, 776.92it/s] 93%|█████████▎| 42544/45920 [00:59<00:04, 745.74it/s] 93%|█████████▎| 42622/45920 [00:59<00:04, 746.72it/s] 93%|█████████▎| 42698/45920 [00:59<00:04, 745.35it/s] 93%|█████████▎| 42776/45920 [00:59<00:04, 752.20it/s] 93%|█████████▎| 42852/45920 [00:59<00:04, 743.91it/s] 94%|█████████▎| 42939/45920 [00:59<00:04, 649.72it/s] 94%|█████████▎| 43007/45920 [01:00<00:04, 655.23it/s] 94%|█████████▍| 43083/45920 [01:00<00:04, 682.53it/s] 94%|█████████▍| 43165/45920 [01:00<00:03, 718.90it/s] 94%|█████████▍| 43239/45920 [01:00<00:03, 707.06it/s] 94%|█████████▍| 43311/45920 [01:00<00:03, 662.34it/s] 95%|█████████▍| 43396/45920 [01:00<00:03, 711.34it/s] 95%|█████████▍| 43469/45920 [01:00<00:03, 677.34it/s] 95%|█████████▍| 43543/45920 [01:00<00:03, 693.83it/s] 95%|█████████▌| 43626/45920 [01:00<00:03, 728.51it/s] 95%|█████████▌| 43722/45920 [01:00<00:02, 790.02it/s] 95%|█████████▌| 43802/45920 [01:01<00:02, 759.56it/s] 96%|█████████▌| 43882/45920 [01:01<00:02, 758.62it/s] 96%|█████████▌| 43962/45920 [01:01<00:02, 768.23it/s] 96%|█████████▌| 44040/45920 [01:01<00:02, 771.10it/s] 96%|█████████▌| 44126/45920 [01:01<00:02, 795.06it/s] 96%|█████████▋| 44206/45920 [01:01<00:02, 597.72it/s] 96%|█████████▋| 44286/45920 [01:01<00:02, 646.09it/s] 97%|█████████▋| 44358/45920 [01:01<00:02, 664.76it/s] 97%|█████████▋| 44449/45920 [01:02<00:02, 729.30it/s] 97%|█████████▋| 44535/45920 [01:02<00:01, 755.24it/s] 97%|█████████▋| 44614/45920 [01:02<00:01, 731.64it/s] 97%|█████████▋| 44690/45920 [01:02<00:01, 650.16it/s] 97%|█████████▋| 44758/45920 [01:02<00:01, 637.44it/s] 98%|█████████▊| 44836/45920 [01:02<00:01, 674.61it/s] 98%|█████████▊| 44906/45920 [01:02<00:01, 646.57it/s] 98%|█████████▊| 44973/45920 [01:02<00:01, 625.30it/s] 98%|█████████▊| 45037/45920 [01:02<00:01, 603.24it/s] 98%|█████████▊| 45099/45920 [01:03<00:01, 577.58it/s] 98%|█████████▊| 45171/45920 [01:03<00:01, 615.72it/s] 99%|█████████▊| 45248/45920 [01:03<00:01, 648.35it/s] 99%|█████████▊| 45323/45920 [01:03<00:00, 658.99it/s] 99%|█████████▉| 45413/45920 [01:03<00:00, 723.89it/s] 99%|█████████▉| 45502/45920 [01:03<00:00, 769.55it/s] 99%|█████████▉| 45590/45920 [01:03<00:00, 797.24it/s] 99%|█████████▉| 45671/45920 [01:03<00:00, 663.02it/s]100%|█████████▉| 45747/45920 [01:03<00:00, 686.35it/s]100%|█████████▉| 45832/45920 [01:04<00:00, 729.98it/s]100%|█████████▉| 45908/45920 [01:04<00:00, 628.37it/s]100%|██████████| 45920/45920 [01:04<00:00, 714.89it/s]

gathering stats for n=1
  0%|          | 0/45920 [00:00<?, ?it/s]  1%|          | 286/45920 [00:00<00:15, 2859.83it/s]  1%|▏         | 603/45920 [00:00<00:14, 3032.98it/s]  2%|▏         | 907/45920 [00:00<00:16, 2709.09it/s]  3%|▎         | 1260/45920 [00:00<00:14, 3001.30it/s]  3%|▎         | 1604/45920 [00:00<00:14, 3150.73it/s]  4%|▍         | 1929/45920 [00:00<00:13, 3179.10it/s]  5%|▍         | 2250/45920 [00:00<00:13, 3122.36it/s]  6%|▌         | 2564/45920 [00:00<00:14, 3088.03it/s]  6%|▋         | 2894/45920 [00:00<00:13, 3150.11it/s]  7%|▋         | 3210/45920 [00:01<00:13, 3093.01it/s]  8%|▊         | 3521/45920 [00:01<00:14, 3023.77it/s]  8%|▊         | 3825/45920 [00:01<00:14, 2871.34it/s]  9%|▉         | 4114/45920 [00:01<00:14, 2804.33it/s] 10%|▉         | 4396/45920 [00:01<00:15, 2730.65it/s] 10%|█         | 4711/45920 [00:01<00:14, 2848.27it/s] 11%|█         | 5050/45920 [00:01<00:13, 3000.94it/s] 12%|█▏        | 5352/45920 [00:01<00:13, 2946.69it/s] 12%|█▏        | 5648/45920 [00:01<00:13, 2926.22it/s] 13%|█▎        | 5942/45920 [00:02<00:13, 2887.21it/s] 14%|█▎        | 6232/45920 [00:02<00:13, 2882.05it/s] 14%|█▍        | 6585/45920 [00:02<00:12, 3070.74it/s] 15%|█▌        | 6893/45920 [00:02<00:13, 2998.50it/s] 16%|█▌        | 7252/45920 [00:02<00:12, 3170.10it/s] 16%|█▋        | 7571/45920 [00:02<00:12, 2952.63it/s] 17%|█▋        | 7870/45920 [00:02<00:13, 2806.64it/s] 18%|█▊        | 8184/45920 [00:02<00:13, 2847.54it/s] 19%|█▊        | 8508/45920 [00:02<00:12, 2955.04it/s] 19%|█▉        | 8807/45920 [00:02<00:12, 2957.45it/s] 20%|█▉        | 9105/45920 [00:03<00:12, 2955.89it/s] 21%|██        | 9419/45920 [00:03<00:12, 3009.43it/s] 21%|██        | 9721/45920 [00:03<00:12, 2914.70it/s] 22%|██▏       | 10020/45920 [00:03<00:12, 2933.01it/s] 22%|██▏       | 10322/45920 [00:03<00:12, 2941.26it/s] 23%|██▎       | 10618/45920 [00:03<00:12, 2940.71it/s] 24%|██▍       | 10936/45920 [00:03<00:11, 3009.37it/s] 24%|██▍       | 11250/45920 [00:03<00:11, 3047.47it/s] 25%|██▌       | 11564/45920 [00:03<00:11, 3070.51it/s] 26%|██▌       | 11872/45920 [00:04<00:11, 2923.40it/s] 26%|██▋       | 12166/45920 [00:04<00:11, 2882.02it/s] 27%|██▋       | 12509/45920 [00:04<00:10, 3037.95it/s] 28%|██▊       | 12815/45920 [00:04<00:11, 2955.27it/s] 29%|██▊       | 13141/45920 [00:04<00:10, 3038.59it/s] 29%|██▉       | 13475/45920 [00:04<00:10, 3124.87it/s] 30%|███       | 13789/45920 [00:04<00:10, 3036.24it/s] 31%|███       | 14094/45920 [00:04<00:11, 2889.24it/s] 31%|███▏      | 14390/45920 [00:04<00:10, 2908.18it/s] 32%|███▏      | 14683/45920 [00:04<00:11, 2788.93it/s] 33%|███▎      | 14996/45920 [00:05<00:10, 2883.77it/s] 33%|███▎      | 15287/45920 [00:05<00:10, 2808.63it/s] 34%|███▍      | 15598/45920 [00:05<00:10, 2893.81it/s] 35%|███▍      | 15889/45920 [00:05<00:10, 2849.92it/s] 35%|███▌      | 16188/45920 [00:05<00:10, 2890.14it/s] 36%|███▌      | 16489/45920 [00:05<00:10, 2914.88it/s] 37%|███▋      | 16782/45920 [00:05<00:09, 2916.92it/s] 37%|███▋      | 17075/45920 [00:05<00:10, 2840.84it/s] 38%|███▊      | 17404/45920 [00:05<00:09, 2967.77it/s] 39%|███▊      | 17744/45920 [00:05<00:09, 3094.39it/s] 39%|███▉      | 18055/45920 [00:06<00:09, 2975.83it/s] 40%|████      | 18407/45920 [00:06<00:08, 3130.69it/s] 41%|████      | 18722/45920 [00:06<00:09, 2881.60it/s] 42%|████▏     | 19075/45920 [00:06<00:08, 3060.40it/s] 42%|████▏     | 19386/45920 [00:06<00:08, 3055.51it/s] 43%|████▎     | 19710/45920 [00:06<00:08, 3107.98it/s] 44%|████▎     | 20040/45920 [00:06<00:08, 3161.09it/s] 44%|████▍     | 20359/45920 [00:06<00:08, 3087.73it/s] 45%|████▌     | 20676/45920 [00:06<00:08, 3110.62it/s] 46%|████▌     | 20992/45920 [00:07<00:07, 3124.27it/s] 46%|████▋     | 21306/45920 [00:07<00:08, 2978.28it/s] 47%|████▋     | 21613/45920 [00:07<00:08, 2992.64it/s] 48%|████▊     | 21914/45920 [00:07<00:08, 2864.05it/s] 48%|████▊     | 22218/45920 [00:07<00:08, 2912.74it/s] 49%|████▉     | 22529/45920 [00:07<00:07, 2962.09it/s] 50%|████▉     | 22837/45920 [00:07<00:07, 2993.40it/s] 50%|█████     | 23140/45920 [00:07<00:07, 3002.92it/s] 51%|█████     | 23441/45920 [00:07<00:07, 2957.86it/s] 52%|█████▏    | 23738/45920 [00:08<00:07, 2816.08it/s] 52%|█████▏    | 24022/45920 [00:08<00:07, 2755.11it/s] 53%|█████▎    | 24300/45920 [00:08<00:07, 2760.54it/s] 54%|█████▎    | 24655/45920 [00:08<00:07, 2988.39it/s] 54%|█████▍    | 24968/45920 [00:08<00:06, 3025.03it/s] 55%|█████▌    | 25296/45920 [00:08<00:06, 3099.69it/s] 56%|█████▌    | 25632/45920 [00:08<00:06, 3174.20it/s] 57%|█████▋    | 25967/45920 [00:08<00:06, 3224.18it/s] 57%|█████▋    | 26290/45920 [00:08<00:06, 3161.53it/s] 58%|█████▊    | 26607/45920 [00:08<00:06, 3091.91it/s] 59%|█████▊    | 26917/45920 [00:09<00:06, 3062.04it/s] 59%|█████▉    | 27224/45920 [00:09<00:06, 2865.37it/s] 60%|█████▉    | 27532/45920 [00:09<00:06, 2921.89it/s] 61%|██████    | 27827/45920 [00:09<00:06, 2867.71it/s] 61%|██████    | 28116/45920 [00:09<00:06, 2844.05it/s] 62%|██████▏   | 28407/45920 [00:09<00:06, 2861.79it/s] 63%|██████▎   | 28716/45920 [00:09<00:05, 2927.35it/s] 63%|██████▎   | 29042/45920 [00:09<00:05, 2991.69it/s] 64%|██████▍   | 29342/45920 [00:09<00:05, 2940.90it/s] 65%|██████▍   | 29637/45920 [00:09<00:05, 2929.12it/s] 65%|██████▌   | 29941/45920 [00:10<00:05, 2959.30it/s] 66%|██████▌   | 30238/45920 [00:10<00:05, 2952.25it/s] 67%|██████▋   | 30560/45920 [00:10<00:05, 3029.15it/s] 67%|██████▋   | 30884/45920 [00:10<00:04, 3087.98it/s] 68%|██████▊   | 31226/45920 [00:10<00:04, 3183.25it/s] 69%|██████▊   | 31545/45920 [00:10<00:04, 2991.18it/s] 69%|██████▉   | 31847/45920 [00:10<00:04, 2943.65it/s] 70%|███████   | 32163/45920 [00:10<00:04, 2994.52it/s] 71%|███████   | 32501/45920 [00:10<00:04, 3105.55it/s] 71%|███████▏  | 32813/45920 [00:11<00:04, 3074.86it/s] 72%|███████▏  | 33134/45920 [00:11<00:04, 3102.44it/s] 73%|███████▎  | 33474/45920 [00:11<00:03, 3189.25it/s] 74%|███████▎  | 33801/45920 [00:11<00:03, 3207.96it/s] 74%|███████▍  | 34123/45920 [00:11<00:03, 3179.07it/s] 75%|███████▌  | 34442/45920 [00:11<00:03, 3071.15it/s] 76%|███████▌  | 34786/45920 [00:11<00:03, 3174.57it/s] 76%|███████▋  | 35105/45920 [00:11<00:03, 2980.28it/s] 77%|███████▋  | 35406/45920 [00:11<00:03, 2941.86it/s] 78%|███████▊  | 35727/45920 [00:11<00:03, 3014.88it/s] 78%|███████▊  | 36031/45920 [00:12<00:03, 2996.30it/s] 79%|███████▉  | 36332/45920 [00:12<00:03, 2931.04it/s] 80%|███████▉  | 36639/45920 [00:12<00:03, 2970.53it/s] 80%|████████  | 36937/45920 [00:12<00:03, 2926.46it/s] 81%|████████  | 37259/45920 [00:12<00:02, 3006.52it/s] 82%|████████▏ | 37561/45920 [00:12<00:02, 2976.34it/s] 82%|████████▏ | 37872/45920 [00:12<00:02, 3011.86it/s] 83%|████████▎ | 38174/45920 [00:12<00:02, 2942.22it/s] 84%|████████▍ | 38471/45920 [00:12<00:02, 2945.16it/s] 84%|████████▍ | 38766/45920 [00:13<00:02, 2846.84it/s] 85%|████████▌ | 39057/45920 [00:13<00:02, 2860.94it/s] 86%|████████▌ | 39344/45920 [00:13<00:02, 2820.86it/s] 86%|████████▋ | 39636/45920 [00:13<00:02, 2805.98it/s] 87%|████████▋ | 39953/45920 [00:13<00:02, 2908.82it/s] 88%|████████▊ | 40245/45920 [00:13<00:01, 2875.70it/s] 88%|████████▊ | 40539/45920 [00:13<00:01, 2892.60it/s] 89%|████████▉ | 40856/45920 [00:13<00:01, 2974.05it/s] 90%|████████▉ | 41164/45920 [00:13<00:01, 3001.02it/s] 90%|█████████ | 41465/45920 [00:13<00:01, 2942.95it/s] 91%|█████████ | 41795/45920 [00:14<00:01, 3045.74it/s] 92%|█████████▏| 42124/45920 [00:14<00:01, 3104.72it/s] 92%|█████████▏| 42435/45920 [00:14<00:01, 3071.88it/s] 93%|█████████▎| 42743/45920 [00:14<00:01, 3046.05it/s] 94%|█████████▎| 43048/45920 [00:14<00:00, 3024.56it/s] 94%|█████████▍| 43358/45920 [00:14<00:00, 3040.46it/s] 95%|█████████▌| 43663/45920 [00:14<00:00, 3026.06it/s] 96%|█████████▌| 44004/45920 [00:14<00:00, 3113.75it/s] 97%|█████████▋| 44316/45920 [00:14<00:00, 3026.74it/s] 97%|█████████▋| 44637/45920 [00:14<00:00, 3070.58it/s] 98%|█████████▊| 44945/45920 [00:15<00:00, 2930.26it/s] 99%|█████████▊| 45240/45920 [00:15<00:00, 2918.35it/s] 99%|█████████▉| 45580/45920 [00:15<00:00, 3054.33it/s]100%|█████████▉| 45896/45920 [00:15<00:00, 3083.91it/s]100%|██████████| 45920/45920 [00:15<00:00, 2982.48it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 145.05it/s]2022-03-15 16:34:23 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(34528, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=34528, bias=False)
  )
)
2022-03-15 16:34:23 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-15 16:34:23 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-15 16:34:23 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-15 16:34:23 | INFO | fairseq_cli.train | num. shared model params: 36,592,640 (num. trained: 36,592,640)
2022-03-15 16:34:23 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-15 16:34:23 | INFO | fairseq.data.data_utils | loaded 2,294 examples from: data-bin/de/valid
2022-03-15 16:34:23 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-15 16:34:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 16:34:23 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-15 16:34:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 16:34:23 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-15 16:34:23 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-15 16:34:23 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt
2022-03-15 16:34:23 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt
2022-03-15 16:34:23 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-15 16:34:23 | INFO | fairseq.data.data_utils | loaded 45,920 examples from: data-bin/de/train
2022-03-15 16:34:23 | INFO | fairseq.trainer | begin training epoch 1
2022-03-15 16:34:23 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-15 16:34:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-15 16:34:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:34:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 16:34:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-15 16:39:49 | INFO | train_inner | epoch 001:    104 / 392 loss=14.693, ppl=26487.3, wps=21662.5, ups=0.33, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.172, loss_scale=8, train_wall=301, gb_free=9.6, wall=327
2022-03-15 16:44:57 | INFO | train_inner | epoch 001:    204 / 392 loss=13.135, ppl=8993.36, wps=21325.3, ups=0.33, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=0.731, loss_scale=16, train_wall=283, gb_free=9.6, wall=634
2022-03-15 16:50:06 | INFO | train_inner | epoch 001:    304 / 392 loss=12.184, ppl=4652.05, wps=21197.6, ups=0.32, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.46, loss_scale=32, train_wall=285, gb_free=9.6, wall=943
2022-03-15 16:54:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 16:55:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.533 | ppl 2962.84 | wps 33239.1 | wpb 511.9 | bsz 1 | num_updates 388
2022-03-15 16:55:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-15 16:55:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 16:55:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 16:55:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 1 @ 388 updates, score 11.533) (writing took 2.3353116959915496 seconds)
2022-03-15 16:55:16 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-15 16:55:16 | INFO | train | epoch 001 | loss 12.97 | ppl 8025.61 | wps 20652.8 | ups 0.32 | wpb 65404.5 | bsz 127.7 | num_updates 388 | lr 4.85903e-05 | gnorm 0.955 | loss_scale 64 | train_wall 1116 | gb_free 9.6 | wall 1253
KL Stats: Epoch 1 Divergences: Uniform: 0.7068392610203652 Unigram: 0.8913920833725224
2022-03-15 16:55:16 | INFO | fairseq.trainer | begin training epoch 2
2022-03-15 16:55:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 16:55:52 | INFO | train_inner | epoch 002:     12 / 392 loss=11.694, ppl=3313.84, wps=18770.9, ups=0.29, wpb=65022.4, bsz=127, num_updates=400, lr=5.009e-05, gnorm=0.39, loss_scale=64, train_wall=281, gb_free=9.6, wall=1290
2022-03-15 16:59:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:01:06 | INFO | train_inner | epoch 002:    113 / 392 loss=11.462, ppl=2820.67, wps=20919.5, ups=0.32, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.376, loss_scale=32, train_wall=288, gb_free=9.6, wall=1603
2022-03-15 17:06:17 | INFO | train_inner | epoch 002:    213 / 392 loss=11.208, ppl=2365.54, wps=21050.4, ups=0.32, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.378, loss_scale=64, train_wall=287, gb_free=9.6, wall=1914
2022-03-15 17:10:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:11:29 | INFO | train_inner | epoch 002:    314 / 392 loss=10.913, ppl=1928.37, wps=21036.6, ups=0.32, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.439, loss_scale=32, train_wall=287, gb_free=9.6, wall=2226
2022-03-15 17:15:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:16:06 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.399 | ppl 1350.33 | wps 32685.6 | wpb 511.9 | bsz 1 | num_updates 778 | best_loss 10.399
2022-03-15 17:16:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 778 updates
2022-03-15 17:16:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 17:16:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 17:16:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 2 @ 778 updates, score 10.399) (writing took 2.2858850130578503 seconds)
2022-03-15 17:16:08 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-15 17:16:08 | INFO | train | epoch 002 | loss 11.101 | ppl 2195.87 | wps 20366 | ups 0.31 | wpb 65405.2 | bsz 127.7 | num_updates 778 | lr 9.73306e-05 | gnorm 0.405 | loss_scale 32 | train_wall 1115 | gb_free 9.6 | wall 2505
KL Stats: Epoch 2 Divergences: Uniform: 1.5259949895230975 Unigram: 0.5505648649442043
2022-03-15 17:16:08 | INFO | fairseq.trainer | begin training epoch 3
2022-03-15 17:16:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:17:18 | INFO | train_inner | epoch 003:     22 / 392 loss=10.63, ppl=1585.04, wps=18632.1, ups=0.29, wpb=65029.1, bsz=127, num_updates=800, lr=0.00010008, gnorm=0.437, loss_scale=32, train_wall=283, gb_free=9.6, wall=2575
2022-03-15 17:18:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:22:31 | INFO | train_inner | epoch 003:    123 / 392 loss=10.382, ppl=1334.26, wps=20873.8, ups=0.32, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.483, loss_scale=32, train_wall=289, gb_free=9.6, wall=2889
2022-03-15 17:25:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:27:47 | INFO | train_inner | epoch 003:    224 / 392 loss=10.172, ppl=1153.9, wps=20760.2, ups=0.32, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.508, loss_scale=32, train_wall=291, gb_free=9.6, wall=3204
2022-03-15 17:32:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:33:05 | INFO | train_inner | epoch 003:    325 / 392 loss=9.981, ppl=1010.49, wps=20626, ups=0.31, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.55, loss_scale=32, train_wall=293, gb_free=9.6, wall=3522
2022-03-15 17:36:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:37:11 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.624 | ppl 789.26 | wps 32670.2 | wpb 511.9 | bsz 1 | num_updates 1167 | best_loss 9.624
2022-03-15 17:37:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1167 updates
2022-03-15 17:37:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 17:37:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 17:37:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 3 @ 1167 updates, score 9.624) (writing took 2.299319527926855 seconds)
2022-03-15 17:37:13 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-15 17:37:13 | INFO | train | epoch 003 | loss 10.139 | ppl 1127.91 | wps 20111.8 | ups 0.31 | wpb 65404.8 | bsz 127.7 | num_updates 1167 | lr 0.000145946 | gnorm 0.521 | loss_scale 32 | train_wall 1127 | gb_free 9.6 | wall 3770
KL Stats: Epoch 3 Divergences: Uniform: 2.0858730081741395 Unigram: 1.2723894542992291
2022-03-15 17:37:13 | INFO | fairseq.trainer | begin training epoch 4
2022-03-15 17:37:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:38:58 | INFO | train_inner | epoch 004:     33 / 392 loss=9.808, ppl=896.33, wps=18440.5, ups=0.28, wpb=65025.8, bsz=127, num_updates=1200, lr=0.00015007, gnorm=0.574, loss_scale=32, train_wall=286, gb_free=9.6, wall=3875
2022-03-15 17:41:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:44:17 | INFO | train_inner | epoch 004:    134 / 392 loss=9.651, ppl=804.07, wps=20510.7, ups=0.31, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.604, loss_scale=32, train_wall=294, gb_free=9.6, wall=4194
2022-03-15 17:49:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:49:38 | INFO | train_inner | epoch 004:    235 / 392 loss=9.515, ppl=731.65, wps=20438.2, ups=0.31, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.63, loss_scale=32, train_wall=296, gb_free=9.6, wall=4515
2022-03-15 17:54:56 | INFO | train_inner | epoch 004:    335 / 392 loss=9.385, ppl=668.71, wps=20609.1, ups=0.31, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.645, loss_scale=32, train_wall=293, gb_free=9.6, wall=4833
2022-03-15 17:56:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:57:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:58:32 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.062 | ppl 534.51 | wps 33463.6 | wpb 511.9 | bsz 1 | num_updates 1556 | best_loss 9.062
2022-03-15 17:58:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1556 updates
2022-03-15 17:58:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 17:58:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 17:58:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 4 @ 1556 updates, score 9.062) (writing took 2.626563779078424 seconds)
2022-03-15 17:58:34 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-15 17:58:34 | INFO | train | epoch 004 | loss 9.503 | ppl 725.33 | wps 19856.3 | ups 0.3 | wpb 65404.8 | bsz 127.7 | num_updates 1556 | lr 0.000194561 | gnorm 0.63 | loss_scale 32 | train_wall 1143 | gb_free 9.6 | wall 5052
KL Stats: Epoch 4 Divergences: Uniform: 2.529594800518157 Unigram: 1.667209530066745
2022-03-15 17:58:34 | INFO | fairseq.trainer | begin training epoch 5
2022-03-15 17:58:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:00:50 | INFO | train_inner | epoch 005:     44 / 392 loss=9.237, ppl=603.4, wps=18347.3, ups=0.28, wpb=65025.8, bsz=127, num_updates=1600, lr=0.00020006, gnorm=0.689, loss_scale=32, train_wall=288, gb_free=9.6, wall=5187
2022-03-15 18:04:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:06:03 | INFO | train_inner | epoch 005:    145 / 392 loss=9.096, ppl=547.09, wps=20977.3, ups=0.32, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.66, loss_scale=32, train_wall=288, gb_free=9.6, wall=5500
2022-03-15 18:11:14 | INFO | train_inner | epoch 005:    245 / 392 loss=8.963, ppl=499.03, wps=21047, ups=0.32, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.683, loss_scale=64, train_wall=287, gb_free=9.6, wall=5811
2022-03-15 18:11:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:16:33 | INFO | train_inner | epoch 005:    346 / 392 loss=8.837, ppl=457.27, wps=20539.9, ups=0.31, wpb=65532.7, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.691, loss_scale=32, train_wall=294, gb_free=9.6, wall=6130
2022-03-15 18:18:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:19:35 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.518 | ppl 366.5 | wps 32677.9 | wpb 511.9 | bsz 1 | num_updates 1946 | best_loss 8.518
2022-03-15 18:19:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1946 updates
2022-03-15 18:19:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 18:19:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 18:19:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 5 @ 1946 updates, score 8.518) (writing took 2.5551923949969932 seconds)
2022-03-15 18:19:37 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-15 18:19:37 | INFO | train | epoch 005 | loss 8.964 | ppl 499.29 | wps 20195.4 | ups 0.31 | wpb 65405.2 | bsz 127.7 | num_updates 1946 | lr 0.000243301 | gnorm 0.68 | loss_scale 64 | train_wall 1124 | gb_free 9.6 | wall 6315
KL Stats: Epoch 5 Divergences: Uniform: 2.8733690689126763 Unigram: 1.9386009485161468
2022-03-15 18:19:38 | INFO | fairseq.trainer | begin training epoch 6
2022-03-15 18:19:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:19:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:22:28 | INFO | train_inner | epoch 006:     55 / 392 loss=8.695, ppl=414.33, wps=18314.7, ups=0.28, wpb=65029.1, bsz=127, num_updates=2000, lr=0.00025005, gnorm=0.681, loss_scale=32, train_wall=288, gb_free=9.6, wall=6485
2022-03-15 18:26:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:27:47 | INFO | train_inner | epoch 006:    156 / 392 loss=8.574, ppl=381.2, wps=20553, ups=0.31, wpb=65532.7, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.676, loss_scale=32, train_wall=294, gb_free=9.6, wall=6804
2022-03-15 18:29:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:33:10 | INFO | train_inner | epoch 006:    257 / 392 loss=8.476, ppl=356.01, wps=20292.3, ups=0.31, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.702, loss_scale=16, train_wall=298, gb_free=9.6, wall=7127
2022-03-15 18:38:26 | INFO | train_inner | epoch 006:    357 / 392 loss=8.379, ppl=332.94, wps=20751, ups=0.32, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.646, loss_scale=32, train_wall=291, gb_free=9.6, wall=7443
2022-03-15 18:40:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:40:52 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.125 | ppl 279.1 | wps 32642 | wpb 511.9 | bsz 1 | num_updates 2335 | best_loss 8.125
2022-03-15 18:40:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2335 updates
2022-03-15 18:40:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 18:40:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 18:40:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 6 @ 2335 updates, score 8.125) (writing took 2.3612830119673163 seconds)
2022-03-15 18:40:54 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-15 18:40:54 | INFO | train | epoch 006 | loss 8.488 | ppl 359.15 | wps 19923.7 | ups 0.3 | wpb 65404.8 | bsz 127.7 | num_updates 2335 | lr 0.000291917 | gnorm 0.677 | loss_scale 32 | train_wall 1138 | gb_free 9.6 | wall 7592
KL Stats: Epoch 6 Divergences: Uniform: 3.1963916550427416 Unigram: 2.1618643852506834
2022-03-15 18:40:55 | INFO | fairseq.trainer | begin training epoch 7
2022-03-15 18:40:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:43:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:44:20 | INFO | train_inner | epoch 007:     66 / 392 loss=8.274, ppl=309.63, wps=18356.4, ups=0.28, wpb=65029.1, bsz=127, num_updates=2400, lr=0.00030004, gnorm=0.678, loss_scale=32, train_wall=288, gb_free=9.6, wall=7797
2022-03-15 18:49:34 | INFO | train_inner | epoch 007:    166 / 392 loss=8.177, ppl=289.5, wps=20883.5, ups=0.32, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.636, loss_scale=32, train_wall=289, gb_free=9.6, wall=8111
2022-03-15 18:50:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:54:51 | INFO | train_inner | epoch 007:    267 / 392 loss=8.107, ppl=275.63, wps=20637.6, ups=0.31, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.641, loss_scale=16, train_wall=293, gb_free=9.6, wall=8429
2022-03-15 18:57:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:00:05 | INFO | train_inner | epoch 007:    368 / 392 loss=8.032, ppl=261.71, wps=20911.6, ups=0.32, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.629, loss_scale=16, train_wall=289, gb_free=9.6, wall=8742
2022-03-15 19:01:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:01:56 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.836 | ppl 228.55 | wps 32944.3 | wpb 511.9 | bsz 1 | num_updates 2724 | best_loss 7.836
2022-03-15 19:01:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2724 updates
2022-03-15 19:01:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 19:01:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 19:01:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 7 @ 2724 updates, score 7.836) (writing took 2.357885428937152 seconds)
2022-03-15 19:01:59 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-15 19:01:59 | INFO | train | epoch 007 | loss 8.123 | ppl 278.72 | wps 20126 | ups 0.31 | wpb 65404.8 | bsz 127.7 | num_updates 2724 | lr 0.000340532 | gnorm 0.643 | loss_scale 16 | train_wall 1126 | gb_free 9.6 | wall 8856
KL Stats: Epoch 7 Divergences: Uniform: 3.454687308052785 Unigram: 2.3292685519260865
2022-03-15 19:01:59 | INFO | fairseq.trainer | begin training epoch 8
2022-03-15 19:01:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:05:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:05:59 | INFO | train_inner | epoch 008:     77 / 392 loss=7.928, ppl=243.46, wps=18358.7, ups=0.28, wpb=65025.8, bsz=127, num_updates=2800, lr=0.00035003, gnorm=0.632, loss_scale=16, train_wall=288, gb_free=9.6, wall=9096
2022-03-15 19:11:18 | INFO | train_inner | epoch 008:    177 / 392 loss=7.875, ppl=234.73, wps=20536.8, ups=0.31, wpb=65532.7, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.606, loss_scale=16, train_wall=294, gb_free=9.6, wall=9415
2022-03-15 19:14:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:16:49 | INFO | train_inner | epoch 008:    278 / 392 loss=7.81, ppl=224.38, wps=19790, ups=0.3, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.613, loss_scale=16, train_wall=306, gb_free=9.6, wall=9746
2022-03-15 19:21:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:22:19 | INFO | train_inner | epoch 008:    379 / 392 loss=7.761, ppl=216.9, wps=19854.5, ups=0.3, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.609, loss_scale=16, train_wall=305, gb_free=9.6, wall=10076
2022-03-15 19:22:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:23:40 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.575 | ppl 190.73 | wps 31673.5 | wpb 511.9 | bsz 1 | num_updates 3113 | best_loss 7.575
2022-03-15 19:23:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3113 updates
2022-03-15 19:23:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 19:23:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 19:23:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 8 @ 3113 updates, score 7.575) (writing took 2.600633627967909 seconds)
2022-03-15 19:23:43 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-15 19:23:43 | INFO | train | epoch 008 | loss 7.83 | ppl 227.61 | wps 19511.7 | ups 0.3 | wpb 65404.8 | bsz 127.7 | num_updates 3113 | lr 0.000389147 | gnorm 0.613 | loss_scale 16 | train_wall 1162 | gb_free 9.6 | wall 10160
KL Stats: Epoch 8 Divergences: Uniform: 3.6400050102410546 Unigram: 2.446067938989859
2022-03-15 19:23:43 | INFO | fairseq.trainer | begin training epoch 9
2022-03-15 19:23:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:28:29 | INFO | train_inner | epoch 009:     87 / 392 loss=7.646, ppl=200.25, wps=17585.7, ups=0.27, wpb=65029.1, bsz=127, num_updates=3200, lr=0.00040002, gnorm=0.623, loss_scale=16, train_wall=301, gb_free=9.6, wall=10446
2022-03-15 19:29:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:33:58 | INFO | train_inner | epoch 009:    188 / 392 loss=7.611, ppl=195.54, wps=19921, ups=0.3, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.6, loss_scale=16, train_wall=303, gb_free=9.6, wall=10775
2022-03-15 19:39:27 | INFO | train_inner | epoch 009:    288 / 392 loss=7.561, ppl=188.82, wps=19890.4, ups=0.3, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.604, loss_scale=32, train_wall=304, gb_free=9.6, wall=11105
2022-03-15 19:39:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:45:02 | INFO | train_inner | epoch 009:    389 / 392 loss=7.5, ppl=181.07, wps=19602.8, ups=0.3, wpb=65532.7, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.6, loss_scale=16, train_wall=309, gb_free=9.6, wall=11439
2022-03-15 19:45:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:45:50 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.309 | ppl 158.54 | wps 31892 | wpb 511.9 | bsz 1 | num_updates 3503 | best_loss 7.309
2022-03-15 19:45:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3503 updates
2022-03-15 19:45:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 19:45:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 19:45:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 9 @ 3503 updates, score 7.309) (writing took 2.3192034530220553 seconds)
2022-03-15 19:45:52 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-15 19:45:52 | INFO | train | epoch 009 | loss 7.575 | ppl 190.62 | wps 19189.5 | ups 0.29 | wpb 65405.2 | bsz 127.7 | num_updates 3503 | lr 0.000437887 | gnorm 0.607 | loss_scale 16 | train_wall 1187 | gb_free 9.6 | wall 11489
KL Stats: Epoch 9 Divergences: Uniform: 3.7770077730869267 Unigram: 2.541568496837655
2022-03-15 19:45:52 | INFO | fairseq.trainer | begin training epoch 10
2022-03-15 19:45:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:48:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:51:11 | INFO | train_inner | epoch 010:     98 / 392 loss=7.384, ppl=166.98, wps=17623.2, ups=0.27, wpb=65029.1, bsz=127, num_updates=3600, lr=0.00045001, gnorm=0.597, loss_scale=16, train_wall=301, gb_free=9.6, wall=11808
2022-03-15 19:56:36 | INFO | train_inner | epoch 010:    198 / 392 loss=7.355, ppl=163.69, wps=20168.8, ups=0.31, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.6, loss_scale=32, train_wall=300, gb_free=9.6, wall=12133
2022-03-15 19:58:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:02:06 | INFO | train_inner | epoch 010:    299 / 392 loss=7.314, ppl=159.11, wps=19837, ups=0.3, wpb=65532.7, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.604, loss_scale=16, train_wall=305, gb_free=9.6, wall=12463
2022-03-15 20:07:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:07:49 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.1 | ppl 137.18 | wps 31576 | wpb 511.9 | bsz 1 | num_updates 3893 | best_loss 7.1
2022-03-15 20:07:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3893 updates
2022-03-15 20:07:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 20:07:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 20:07:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 10 @ 3893 updates, score 7.1) (writing took 2.3200784450164065 seconds)
2022-03-15 20:07:51 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-15 20:07:51 | INFO | train | epoch 010 | loss 7.332 | ppl 161.16 | wps 19336.8 | ups 0.3 | wpb 65405.2 | bsz 127.7 | num_updates 3893 | lr 0.000486628 | gnorm 0.602 | loss_scale 32 | train_wall 1177 | gb_free 9.6 | wall 12808
KL Stats: Epoch 10 Divergences: Uniform: 3.8960824953967435 Unigram: 2.624949981140318
2022-03-15 20:07:51 | INFO | fairseq.trainer | begin training epoch 11
2022-03-15 20:07:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:08:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:08:17 | INFO | train_inner | epoch 011:      8 / 392 loss=7.273, ppl=154.65, wps=17513.1, ups=0.27, wpb=65029.1, bsz=127, num_updates=3900, lr=0.000487503, gnorm=0.617, loss_scale=16, train_wall=303, gb_free=9.6, wall=12835
2022-03-15 20:13:47 | INFO | train_inner | epoch 011:    108 / 392 loss=7.168, ppl=143.84, wps=19873.9, ups=0.3, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.586, loss_scale=16, train_wall=304, gb_free=9.6, wall=13164
2022-03-15 20:15:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:19:19 | INFO | train_inner | epoch 011:    209 / 392 loss=7.144, ppl=141.39, wps=19721.5, ups=0.3, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.595, loss_scale=16, train_wall=307, gb_free=9.6, wall=13497
2022-03-15 20:23:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:24:51 | INFO | train_inner | epoch 011:    310 / 392 loss=7.123, ppl=139.35, wps=19779.6, ups=0.3, wpb=65532.7, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.586, loss_scale=16, train_wall=306, gb_free=9.6, wall=13828
2022-03-15 20:29:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:30:02 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.947 | ppl 123.42 | wps 31237 | wpb 511.9 | bsz 1 | num_updates 4282 | best_loss 6.947
2022-03-15 20:30:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4282 updates
2022-03-15 20:30:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 20:30:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 20:30:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 11 @ 4282 updates, score 6.947) (writing took 2.4797077890252694 seconds)
2022-03-15 20:30:05 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-15 20:30:05 | INFO | train | epoch 011 | loss 7.134 | ppl 140.44 | wps 19074.3 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 4282 | lr 0.000483255 | gnorm 0.59 | loss_scale 16 | train_wall 1191 | gb_free 9.6 | wall 14142
KL Stats: Epoch 11 Divergences: Uniform: 3.9827208818455695 Unigram: 2.6879896298952084
2022-03-15 20:30:05 | INFO | fairseq.trainer | begin training epoch 12
2022-03-15 20:30:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:31:05 | INFO | train_inner | epoch 012:     18 / 392 loss=7.074, ppl=134.71, wps=17365.5, ups=0.27, wpb=65029.1, bsz=127, num_updates=4300, lr=0.000482243, gnorm=0.588, loss_scale=16, train_wall=305, gb_free=9.6, wall=14202
2022-03-15 20:33:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:36:41 | INFO | train_inner | epoch 012:    119 / 392 loss=6.997, ppl=127.78, wps=19539.6, ups=0.3, wpb=65532.7, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.569, loss_scale=16, train_wall=310, gb_free=9.6, wall=14538
2022-03-15 20:41:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:42:16 | INFO | train_inner | epoch 012:    220 / 392 loss=6.984, ppl=126.57, wps=19570.6, ups=0.3, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.569, loss_scale=16, train_wall=309, gb_free=9.6, wall=14873
2022-03-15 20:47:45 | INFO | train_inner | epoch 012:    320 / 392 loss=6.957, ppl=124.28, wps=19860.9, ups=0.3, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.56, loss_scale=16, train_wall=305, gb_free=9.6, wall=15203
2022-03-15 20:48:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:51:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:52:21 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.834 | ppl 114.07 | wps 31478.3 | wpb 511.9 | bsz 1 | num_updates 4671 | best_loss 6.834
2022-03-15 20:52:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4671 updates
2022-03-15 20:52:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 20:52:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 20:52:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 12 @ 4671 updates, score 6.834) (writing took 2.4334957889514044 seconds)
2022-03-15 20:52:23 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-15 20:52:23 | INFO | train | epoch 012 | loss 6.975 | ppl 125.82 | wps 19011.2 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 4671 | lr 0.000462695 | gnorm 0.569 | loss_scale 16 | train_wall 1195 | gb_free 9.6 | wall 15480
KL Stats: Epoch 12 Divergences: Uniform: 4.0588342394713575 Unigram: 2.7395681066772135
2022-03-15 20:52:23 | INFO | fairseq.trainer | begin training epoch 13
2022-03-15 20:52:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:53:59 | INFO | train_inner | epoch 013:     29 / 392 loss=6.921, ppl=121.19, wps=17403.4, ups=0.27, wpb=65029.1, bsz=127, num_updates=4700, lr=0.000461266, gnorm=0.567, loss_scale=16, train_wall=305, gb_free=9.6, wall=15576
2022-03-15 20:57:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:59:33 | INFO | train_inner | epoch 013:    130 / 392 loss=6.871, ppl=117.07, wps=19640.7, ups=0.3, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.553, loss_scale=16, train_wall=308, gb_free=9.6, wall=15910
2022-03-15 21:05:03 | INFO | train_inner | epoch 013:    230 / 392 loss=6.852, ppl=115.55, wps=19822.3, ups=0.3, wpb=65532.7, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.551, loss_scale=32, train_wall=305, gb_free=9.6, wall=16241
2022-03-15 21:05:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:10:37 | INFO | train_inner | epoch 013:    331 / 392 loss=6.849, ppl=115.28, wps=19636.5, ups=0.3, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.554, loss_scale=16, train_wall=308, gb_free=9.6, wall=16574
2022-03-15 21:12:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:13:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:14:37 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.746 | ppl 107.34 | wps 31498.8 | wpb 511.9 | bsz 1 | num_updates 5060 | best_loss 6.746
2022-03-15 21:14:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5060 updates
2022-03-15 21:14:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 21:14:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 21:14:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 13 @ 5060 updates, score 6.746) (writing took 2.3672105600126088 seconds)
2022-03-15 21:14:39 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-15 21:14:39 | INFO | train | epoch 013 | loss 6.854 | ppl 115.66 | wps 19039.8 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 5060 | lr 0.000444554 | gnorm 0.553 | loss_scale 16 | train_wall 1193 | gb_free 9.6 | wall 16817
KL Stats: Epoch 13 Divergences: Uniform: 4.115083075650238 Unigram: 2.77939120478539
2022-03-15 21:14:39 | INFO | fairseq.trainer | begin training epoch 14
2022-03-15 21:14:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:16:52 | INFO | train_inner | epoch 014:     40 / 392 loss=6.8, ppl=111.42, wps=17355.1, ups=0.27, wpb=65025.8, bsz=127, num_updates=5100, lr=0.000442807, gnorm=0.561, loss_scale=16, train_wall=306, gb_free=9.6, wall=16949
2022-03-15 21:20:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:22:25 | INFO | train_inner | epoch 014:    141 / 392 loss=6.748, ppl=107.52, wps=19678, ups=0.3, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.539, loss_scale=16, train_wall=307, gb_free=9.6, wall=17282
2022-03-15 21:27:56 | INFO | train_inner | epoch 014:    241 / 392 loss=6.765, ppl=108.72, wps=19766.7, ups=0.3, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.558, loss_scale=32, train_wall=306, gb_free=9.6, wall=17614
2022-03-15 21:28:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:33:33 | INFO | train_inner | epoch 014:    342 / 392 loss=6.762, ppl=108.53, wps=19460.5, ups=0.3, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.542, loss_scale=16, train_wall=311, gb_free=9.6, wall=17950
2022-03-15 21:36:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:36:58 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.663 | ppl 101.32 | wps 31642.5 | wpb 511.9 | bsz 1 | num_updates 5450 | best_loss 6.663
2022-03-15 21:36:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5450 updates
2022-03-15 21:36:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 21:36:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 21:37:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 14 @ 5450 updates, score 6.663) (writing took 2.3270258069969714 seconds)
2022-03-15 21:37:00 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-15 21:37:00 | INFO | train | epoch 014 | loss 6.757 | ppl 108.13 | wps 19024.1 | ups 0.29 | wpb 65405.2 | bsz 127.7 | num_updates 5450 | lr 0.000428353 | gnorm 0.55 | loss_scale 32 | train_wall 1198 | gb_free 9.6 | wall 18157
KL Stats: Epoch 14 Divergences: Uniform: 4.1584074273356455 Unigram: 2.8070920178878667
2022-03-15 21:37:00 | INFO | fairseq.trainer | begin training epoch 15
2022-03-15 21:37:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:37:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:39:49 | INFO | train_inner | epoch 015:     51 / 392 loss=6.709, ppl=104.63, wps=17286.9, ups=0.27, wpb=65029.1, bsz=127, num_updates=5500, lr=0.000426401, gnorm=0.552, loss_scale=16, train_wall=307, gb_free=9.6, wall=18327
2022-03-15 21:44:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:45:26 | INFO | train_inner | epoch 015:    152 / 392 loss=6.681, ppl=102.6, wps=19486.8, ups=0.3, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.531, loss_scale=16, train_wall=310, gb_free=9.6, wall=18663
2022-03-15 21:50:59 | INFO | train_inner | epoch 015:    252 / 392 loss=6.68, ppl=102.54, wps=19661.9, ups=0.3, wpb=65532.7, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.543, loss_scale=16, train_wall=308, gb_free=9.6, wall=18996
2022-03-15 21:51:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:56:34 | INFO | train_inner | epoch 015:    353 / 392 loss=6.674, ppl=102.13, wps=19564.7, ups=0.3, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.538, loss_scale=16, train_wall=309, gb_free=9.6, wall=19331
2022-03-15 21:58:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:59:21 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.618 | ppl 98.21 | wps 31449.9 | wpb 511.9 | bsz 1 | num_updates 5839 | best_loss 6.618
2022-03-15 21:59:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5839 updates
2022-03-15 21:59:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 21:59:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 21:59:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 15 @ 5839 updates, score 6.618) (writing took 2.3459410450886935 seconds)
2022-03-15 21:59:24 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-15 21:59:24 | INFO | train | epoch 015 | loss 6.676 | ppl 102.25 | wps 18937.1 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 5839 | lr 0.000413838 | gnorm 0.541 | loss_scale 16 | train_wall 1200 | gb_free 9.6 | wall 19501
KL Stats: Epoch 15 Divergences: Uniform: 4.201633044684473 Unigram: 2.8398088144159446
2022-03-15 21:59:24 | INFO | fairseq.trainer | begin training epoch 16
2022-03-15 21:59:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:59:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:02:49 | INFO | train_inner | epoch 016:     62 / 392 loss=6.626, ppl=98.75, wps=17340.7, ups=0.27, wpb=65029.1, bsz=127, num_updates=5900, lr=0.000411693, gnorm=0.541, loss_scale=16, train_wall=306, gb_free=9.6, wall=19706
2022-03-15 22:07:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:08:21 | INFO | train_inner | epoch 016:    163 / 392 loss=6.609, ppl=97.62, wps=19708.7, ups=0.3, wpb=65532.7, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.541, loss_scale=16, train_wall=307, gb_free=9.6, wall=20039
2022-03-15 22:13:53 | INFO | train_inner | epoch 016:    263 / 392 loss=6.612, ppl=97.84, wps=19745.2, ups=0.3, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.542, loss_scale=16, train_wall=306, gb_free=9.6, wall=20371
2022-03-15 22:14:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:19:30 | INFO | train_inner | epoch 016:    364 / 392 loss=6.603, ppl=97.24, wps=19497.8, ups=0.3, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.536, loss_scale=16, train_wall=310, gb_free=9.6, wall=20707
2022-03-15 22:20:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:21:40 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.571 | ppl 95.11 | wps 31255.8 | wpb 511.9 | bsz 1 | num_updates 6228 | best_loss 6.571
2022-03-15 22:21:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6228 updates
2022-03-15 22:21:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 22:21:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 22:21:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 16 @ 6228 updates, score 6.571) (writing took 2.3799400029238313 seconds)
2022-03-15 22:21:43 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-15 22:21:43 | INFO | train | epoch 016 | loss 6.608 | ppl 97.56 | wps 19003.1 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 6228 | lr 0.000400706 | gnorm 0.539 | loss_scale 16 | train_wall 1196 | gb_free 9.6 | wall 20840
KL Stats: Epoch 16 Divergences: Uniform: 4.235207706251788 Unigram: 2.864358068856653
2022-03-15 22:21:43 | INFO | fairseq.trainer | begin training epoch 17
2022-03-15 22:21:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:22:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:25:45 | INFO | train_inner | epoch 017:     73 / 392 loss=6.562, ppl=94.47, wps=17302.4, ups=0.27, wpb=65025.8, bsz=127, num_updates=6300, lr=0.00039841, gnorm=0.54, loss_scale=16, train_wall=306, gb_free=9.6, wall=21083
2022-03-15 22:29:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:31:20 | INFO | train_inner | epoch 017:    174 / 392 loss=6.551, ppl=93.76, wps=19568.8, ups=0.3, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.542, loss_scale=16, train_wall=309, gb_free=9.6, wall=21417
2022-03-15 22:36:49 | INFO | train_inner | epoch 017:    274 / 392 loss=6.552, ppl=93.85, wps=19920.4, ups=0.3, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.535, loss_scale=16, train_wall=304, gb_free=9.6, wall=21746
2022-03-15 22:37:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:42:24 | INFO | train_inner | epoch 017:    375 / 392 loss=6.554, ppl=93.99, wps=19563.5, ups=0.3, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.533, loss_scale=16, train_wall=309, gb_free=9.6, wall=22081
2022-03-15 22:43:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:44:00 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.527 | ppl 92.2 | wps 31163.9 | wpb 511.9 | bsz 1 | num_updates 6617 | best_loss 6.527
2022-03-15 22:44:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6617 updates
2022-03-15 22:44:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 22:44:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 22:44:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 17 @ 6617 updates, score 6.527) (writing took 2.389682766981423 seconds)
2022-03-15 22:44:02 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-15 22:44:02 | INFO | train | epoch 017 | loss 6.55 | ppl 93.71 | wps 18992.8 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 6617 | lr 0.000388749 | gnorm 0.538 | loss_scale 16 | train_wall 1196 | gb_free 9.6 | wall 22179
KL Stats: Epoch 17 Divergences: Uniform: 4.265876409746491 Unigram: 2.8842228141337074
2022-03-15 22:44:02 | INFO | fairseq.trainer | begin training epoch 18
2022-03-15 22:44:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:45:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:48:42 | INFO | train_inner | epoch 018:     84 / 392 loss=6.492, ppl=90.03, wps=17195.3, ups=0.26, wpb=65029.1, bsz=127, num_updates=6700, lr=0.000386334, gnorm=0.545, loss_scale=16, train_wall=309, gb_free=9.6, wall=22460
2022-03-15 22:52:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:54:17 | INFO | train_inner | epoch 018:    185 / 392 loss=6.5, ppl=90.49, wps=19568.2, ups=0.3, wpb=65532.7, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.534, loss_scale=16, train_wall=309, gb_free=9.6, wall=22794
2022-03-15 22:59:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:59:53 | INFO | train_inner | epoch 018:    286 / 392 loss=6.506, ppl=90.9, wps=19521.2, ups=0.3, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.538, loss_scale=16, train_wall=310, gb_free=9.6, wall=23130
2022-03-15 23:05:22 | INFO | train_inner | epoch 018:    386 / 392 loss=6.507, ppl=90.96, wps=19926.3, ups=0.3, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.532, loss_scale=16, train_wall=304, gb_free=9.6, wall=23459
2022-03-15 23:05:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:06:20 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.501 | ppl 90.6 | wps 31309.5 | wpb 511.9 | bsz 1 | num_updates 7006 | best_loss 6.501
2022-03-15 23:06:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7006 updates
2022-03-15 23:06:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 23:06:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 23:06:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 18 @ 7006 updates, score 6.501) (writing took 2.409072143957019 seconds)
2022-03-15 23:06:23 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-15 23:06:23 | INFO | train | epoch 018 | loss 6.499 | ppl 90.47 | wps 18978.3 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 7006 | lr 0.000377803 | gnorm 0.537 | loss_scale 16 | train_wall 1197 | gb_free 9.6 | wall 23520
KL Stats: Epoch 18 Divergences: Uniform: 4.291301575074838 Unigram: 2.903000536471983
2022-03-15 23:06:23 | INFO | fairseq.trainer | begin training epoch 19
2022-03-15 23:06:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:07:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:11:38 | INFO | train_inner | epoch 019:     95 / 392 loss=6.438, ppl=86.67, wps=17290.8, ups=0.27, wpb=65029.1, bsz=127, num_updates=7100, lr=0.000375293, gnorm=0.535, loss_scale=16, train_wall=307, gb_free=9.6, wall=23835
2022-03-15 23:15:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:17:11 | INFO | train_inner | epoch 019:    196 / 392 loss=6.455, ppl=87.74, wps=19669.2, ups=0.3, wpb=65532.7, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.534, loss_scale=16, train_wall=308, gb_free=9.6, wall=24168
2022-03-15 23:22:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:22:41 | INFO | train_inner | epoch 019:    297 / 392 loss=6.462, ppl=88.16, wps=19891.5, ups=0.3, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.533, loss_scale=16, train_wall=304, gb_free=9.6, wall=24498
2022-03-15 23:27:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:28:33 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.463 | ppl 88.2 | wps 31387.9 | wpb 511.9 | bsz 1 | num_updates 7395 | best_loss 6.463
2022-03-15 23:28:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7395 updates
2022-03-15 23:28:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 23:28:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 23:28:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 19 @ 7395 updates, score 6.463) (writing took 2.3933612500550225 seconds)
2022-03-15 23:28:35 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-15 23:28:35 | INFO | train | epoch 019 | loss 6.455 | ppl 87.71 | wps 19098 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 7395 | lr 0.000367732 | gnorm 0.533 | loss_scale 16 | train_wall 1189 | gb_free 9.6 | wall 24852
KL Stats: Epoch 19 Divergences: Uniform: 4.313573453609475 Unigram: 2.918790606684314
2022-03-15 23:28:35 | INFO | fairseq.trainer | begin training epoch 20
2022-03-15 23:28:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:28:52 | INFO | train_inner | epoch 020:      5 / 392 loss=6.461, ppl=88.08, wps=17519.3, ups=0.27, wpb=65029.1, bsz=127, num_updates=7400, lr=0.000367607, gnorm=0.536, loss_scale=16, train_wall=302, gb_free=9.6, wall=24869
2022-03-15 23:30:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:34:23 | INFO | train_inner | epoch 020:    106 / 392 loss=6.407, ppl=84.86, wps=19782.6, ups=0.3, wpb=65532.7, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.528, loss_scale=16, train_wall=306, gb_free=9.6, wall=25200
2022-03-15 23:37:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:39:57 | INFO | train_inner | epoch 020:    207 / 392 loss=6.41, ppl=85.02, wps=19624.7, ups=0.3, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.528, loss_scale=16, train_wall=308, gb_free=9.6, wall=25534
2022-03-15 23:44:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:45:29 | INFO | train_inner | epoch 020:    308 / 392 loss=6.42, ppl=85.65, wps=19729.6, ups=0.3, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.532, loss_scale=16, train_wall=307, gb_free=9.6, wall=25866
2022-03-15 23:50:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:50:42 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.441 | ppl 86.86 | wps 31615.1 | wpb 511.9 | bsz 1 | num_updates 7784 | best_loss 6.441
2022-03-15 23:50:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7784 updates
2022-03-15 23:50:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 23:50:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-15 23:50:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 20 @ 7784 updates, score 6.441) (writing took 2.465176798054017 seconds)
2022-03-15 23:50:44 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-15 23:50:44 | INFO | train | epoch 020 | loss 6.414 | ppl 85.26 | wps 19138.8 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 7784 | lr 0.000358425 | gnorm 0.531 | loss_scale 16 | train_wall 1187 | gb_free 9.6 | wall 26182
KL Stats: Epoch 20 Divergences: Uniform: 4.337819842318461 Unigram: 2.935705460371279
2022-03-15 23:50:44 | INFO | fairseq.trainer | begin training epoch 21
2022-03-15 23:50:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:51:37 | INFO | train_inner | epoch 021:     16 / 392 loss=6.411, ppl=85.12, wps=17674, ups=0.27, wpb=65029.1, bsz=127, num_updates=7800, lr=0.000358057, gnorm=0.538, loss_scale=16, train_wall=300, gb_free=9.6, wall=26234
2022-03-15 23:52:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:54:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-15 23:57:12 | INFO | train_inner | epoch 021:    118 / 392 loss=6.368, ppl=82.59, wps=19557.7, ups=0.3, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.545, loss_scale=8, train_wall=309, gb_free=9.6, wall=26569
2022-03-16 00:02:40 | INFO | train_inner | epoch 021:    218 / 392 loss=6.369, ppl=82.67, wps=19973.9, ups=0.3, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.529, loss_scale=16, train_wall=303, gb_free=9.6, wall=26898
2022-03-16 00:08:12 | INFO | train_inner | epoch 021:    318 / 392 loss=6.386, ppl=83.64, wps=19768.5, ups=0.3, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.532, loss_scale=32, train_wall=306, gb_free=9.6, wall=27229
2022-03-16 00:08:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:12:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:12:55 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.407 | ppl 84.88 | wps 31691.3 | wpb 511.9 | bsz 1 | num_updates 8173 | best_loss 6.407
2022-03-16 00:12:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8173 updates
2022-03-16 00:12:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 00:12:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 00:12:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 21 @ 8173 updates, score 6.407) (writing took 2.3116167600965127 seconds)
2022-03-16 00:12:57 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-16 00:12:57 | INFO | train | epoch 021 | loss 6.378 | ppl 83.18 | wps 19090.7 | ups 0.29 | wpb 65404.8 | bsz 127.7 | num_updates 8173 | lr 0.000349791 | gnorm 0.537 | loss_scale 16 | train_wall 1190 | gb_free 9.6 | wall 27514
KL Stats: Epoch 21 Divergences: Uniform: 4.357853765556857 Unigram: 2.947141644866677
2022-03-16 00:12:57 | INFO | fairseq.trainer | begin training epoch 22
2022-03-16 00:12:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:14:25 | INFO | train_inner | epoch 022:     27 / 392 loss=6.373, ppl=82.91, wps=17412.4, ups=0.27, wpb=65025.8, bsz=127, num_updates=8200, lr=0.000349215, gnorm=0.538, loss_scale=16, train_wall=305, gb_free=9.6, wall=27603
2022-03-16 00:16:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:19:54 | INFO | train_inner | epoch 022:    128 / 392 loss=6.332, ppl=80.56, wps=19942.8, ups=0.3, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.531, loss_scale=16, train_wall=303, gb_free=9.6, wall=27931
2022-03-16 00:23:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:25:28 | INFO | train_inner | epoch 022:    229 / 392 loss=6.352, ppl=81.69, wps=19616.9, ups=0.3, wpb=65532.7, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.532, loss_scale=16, train_wall=308, gb_free=9.6, wall=28265
2022-03-16 00:30:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:31:01 | INFO | train_inner | epoch 022:    330 / 392 loss=6.353, ppl=81.72, wps=19684, ups=0.3, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.531, loss_scale=16, train_wall=307, gb_free=9.6, wall=28598
2022-03-16 00:34:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:34:46 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.392 | ppl 83.97 | wps 34002.1 | wpb 511.9 | bsz 1 | num_updates 8562 | best_loss 6.392
2022-03-16 00:34:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8562 updates
2022-03-16 00:34:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 00:34:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 00:34:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 22 @ 8562 updates, score 6.392) (writing took 2.3291841009631753 seconds)
2022-03-16 00:34:48 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-16 00:34:48 | INFO | train | epoch 022 | loss 6.345 | ppl 81.29 | wps 19405.5 | ups 0.3 | wpb 65404.8 | bsz 127.7 | num_updates 8562 | lr 0.000341753 | gnorm 0.532 | loss_scale 16 | train_wall 1172 | gb_free 9.6 | wall 28825
KL Stats: Epoch 22 Divergences: Uniform: 4.378000551204654 Unigram: 2.963214053489508
2022-03-16 00:34:48 | INFO | fairseq.trainer | begin training epoch 23
2022-03-16 00:34:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:36:45 | INFO | train_inner | epoch 023:     38 / 392 loss=6.329, ppl=80.38, wps=18921.5, ups=0.29, wpb=65029.1, bsz=127, num_updates=8600, lr=0.000340997, gnorm=0.535, loss_scale=16, train_wall=279, gb_free=9.6, wall=28942
2022-03-16 00:38:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:41:51 | INFO | train_inner | epoch 023:    139 / 392 loss=6.306, ppl=79.1, wps=21395.7, ups=0.33, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.531, loss_scale=16, train_wall=282, gb_free=9.6, wall=29248
2022-03-16 00:44:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:46:52 | INFO | train_inner | epoch 023:    240 / 392 loss=6.316, ppl=79.7, wps=21740.8, ups=0.33, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.533, loss_scale=16, train_wall=277, gb_free=9.6, wall=29550
2022-03-16 00:51:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:51:59 | INFO | train_inner | epoch 023:    341 / 392 loss=6.326, ppl=80.23, wps=21389.3, ups=0.33, wpb=65532.7, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.532, loss_scale=16, train_wall=282, gb_free=9.6, wall=29856
2022-03-16 00:54:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:55:10 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.37 | ppl 82.72 | wps 33758.2 | wpb 511.9 | bsz 1 | num_updates 8951 | best_loss 6.37
2022-03-16 00:55:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 8951 updates
2022-03-16 00:55:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 00:55:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 00:55:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 23 @ 8951 updates, score 6.37) (writing took 2.3594700309913605 seconds)
2022-03-16 00:55:12 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-16 00:55:12 | INFO | train | epoch 023 | loss 6.316 | ppl 79.65 | wps 20790.8 | ups 0.32 | wpb 65404.8 | bsz 127.7 | num_updates 8951 | lr 0.000334244 | gnorm 0.532 | loss_scale 16 | train_wall 1088 | gb_free 9.6 | wall 30049
KL Stats: Epoch 23 Divergences: Uniform: 4.39296354823677 Unigram: 2.973119754678312
2022-03-16 00:55:12 | INFO | fairseq.trainer | begin training epoch 24
2022-03-16 00:55:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:57:41 | INFO | train_inner | epoch 024:     49 / 392 loss=6.305, ppl=79.07, wps=18987.8, ups=0.29, wpb=65029.1, bsz=127, num_updates=9000, lr=0.000333333, gnorm=0.54, loss_scale=16, train_wall=278, gb_free=9.6, wall=30198
2022-03-16 00:59:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:02:44 | INFO | train_inner | epoch 024:    150 / 392 loss=6.275, ppl=77.43, wps=21660.6, ups=0.33, wpb=65532.7, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.529, loss_scale=16, train_wall=278, gb_free=9.6, wall=30501
2022-03-16 01:05:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:07:53 | INFO | train_inner | epoch 024:    251 / 392 loss=6.295, ppl=78.54, wps=21215.5, ups=0.32, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.535, loss_scale=16, train_wall=284, gb_free=9.6, wall=30810
2022-03-16 01:12:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:12:52 | INFO | train_inner | epoch 024:    352 / 392 loss=6.296, ppl=78.6, wps=21911.4, ups=0.33, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.536, loss_scale=16, train_wall=275, gb_free=9.6, wall=31109
2022-03-16 01:14:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:15:28 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.361 | ppl 82.19 | wps 33556.9 | wpb 511.9 | bsz 1 | num_updates 9340 | best_loss 6.361
2022-03-16 01:15:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9340 updates
2022-03-16 01:15:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 01:15:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 01:15:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 24 @ 9340 updates, score 6.361) (writing took 2.348605565028265 seconds)
2022-03-16 01:15:30 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-16 01:15:30 | INFO | train | epoch 024 | loss 6.288 | ppl 78.13 | wps 20882.9 | ups 0.32 | wpb 65404.8 | bsz 127.7 | num_updates 9340 | lr 0.00032721 | gnorm 0.539 | loss_scale 16 | train_wall 1082 | gb_free 9.6 | wall 31268
KL Stats: Epoch 24 Divergences: Uniform: 4.411335764651392 Unigram: 2.9864418784233853
2022-03-16 01:15:30 | INFO | fairseq.trainer | begin training epoch 25
2022-03-16 01:15:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:18:33 | INFO | train_inner | epoch 025:     60 / 392 loss=6.259, ppl=76.61, wps=19037.3, ups=0.29, wpb=65025.8, bsz=127, num_updates=9400, lr=0.000326164, gnorm=0.546, loss_scale=16, train_wall=276, gb_free=9.6, wall=31451
2022-03-16 01:20:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:23:41 | INFO | train_inner | epoch 025:    161 / 392 loss=6.257, ppl=76.47, wps=21313.4, ups=0.33, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.533, loss_scale=16, train_wall=283, gb_free=9.6, wall=31758
2022-03-16 01:26:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:28:39 | INFO | train_inner | epoch 025:    262 / 392 loss=6.272, ppl=77.27, wps=21948.4, ups=0.33, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.535, loss_scale=16, train_wall=274, gb_free=9.6, wall=32057
2022-03-16 01:33:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:33:47 | INFO | train_inner | epoch 025:    363 / 392 loss=6.273, ppl=77.31, wps=21309.3, ups=0.33, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.532, loss_scale=16, train_wall=283, gb_free=9.6, wall=32364
2022-03-16 01:35:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:35:51 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.34 | ppl 81 | wps 33661 | wpb 511.9 | bsz 1 | num_updates 9729 | best_loss 6.34
2022-03-16 01:35:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9729 updates
2022-03-16 01:35:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 01:35:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 01:35:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 25 @ 9729 updates, score 6.34) (writing took 2.3467538920231164 seconds)
2022-03-16 01:35:53 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-16 01:35:53 | INFO | train | epoch 025 | loss 6.263 | ppl 76.79 | wps 20800.3 | ups 0.32 | wpb 65404.8 | bsz 127.7 | num_updates 9729 | lr 0.000320602 | gnorm 0.534 | loss_scale 16 | train_wall 1087 | gb_free 9.6 | wall 32491
KL Stats: Epoch 25 Divergences: Uniform: 4.426354952799491 Unigram: 2.9955442574173237
2022-03-16 01:35:54 | INFO | fairseq.trainer | begin training epoch 26
2022-03-16 01:35:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:39:31 | INFO | train_inner | epoch 026:     71 / 392 loss=6.232, ppl=75.18, wps=18924, ups=0.29, wpb=65029.1, bsz=127, num_updates=9800, lr=0.000319438, gnorm=0.53, loss_scale=16, train_wall=279, gb_free=9.6, wall=32708
2022-03-16 01:40:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:42:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 01:44:10 | INFO | train_inner | epoch 026:    173 / 392 loss=6.234, ppl=75.29, wps=23473.8, ups=0.36, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.531, loss_scale=8, train_wall=255, gb_free=9.6, wall=32987
2022-03-16 01:48:34 | INFO | train_inner | epoch 026:    273 / 392 loss=6.245, ppl=75.83, wps=24777, ups=0.38, wpb=65532.7, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.53, loss_scale=16, train_wall=241, gb_free=9.6, wall=33251
2022-03-16 01:52:59 | INFO | train_inner | epoch 026:    373 / 392 loss=6.254, ppl=76.32, wps=24771.4, ups=0.38, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.529, loss_scale=16, train_wall=241, gb_free=9.6, wall=33516
2022-03-16 01:53:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:53:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:54:22 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.32 | ppl 79.91 | wps 36779.1 | wpb 511.9 | bsz 1 | num_updates 10118 | best_loss 6.32
2022-03-16 01:54:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10118 updates
2022-03-16 01:54:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 01:54:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 01:54:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 26 @ 10118 updates, score 6.32) (writing took 2.25544834905304 seconds)
2022-03-16 01:54:25 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-16 01:54:25 | INFO | train | epoch 026 | loss 6.239 | ppl 75.53 | wps 22897.6 | ups 0.35 | wpb 65404.8 | bsz 127.7 | num_updates 10118 | lr 0.000314378 | gnorm 0.531 | loss_scale 16 | train_wall 981 | gb_free 9.6 | wall 33602
KL Stats: Epoch 26 Divergences: Uniform: 4.441057039824707 Unigram: 3.004575224444011
2022-03-16 01:54:25 | INFO | fairseq.trainer | begin training epoch 27
2022-03-16 01:54:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:58:02 | INFO | train_inner | epoch 027:     82 / 392 loss=6.204, ppl=73.74, wps=21450.9, ups=0.33, wpb=65025.8, bsz=127, num_updates=10200, lr=0.000313112, gnorm=0.542, loss_scale=16, train_wall=242, gb_free=9.6, wall=33819
2022-03-16 02:00:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:00:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:02:32 | INFO | train_inner | epoch 027:    184 / 392 loss=6.208, ppl=73.93, wps=24293.3, ups=0.37, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.533, loss_scale=8, train_wall=246, gb_free=9.6, wall=34089
2022-03-16 02:06:56 | INFO | train_inner | epoch 027:    284 / 392 loss=6.229, ppl=74.99, wps=24756.3, ups=0.38, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.535, loss_scale=16, train_wall=241, gb_free=9.6, wall=34354
2022-03-16 02:10:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:11:24 | INFO | train_inner | epoch 027:    385 / 392 loss=6.236, ppl=75.37, wps=24513.5, ups=0.37, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.535, loss_scale=8, train_wall=244, gb_free=9.6, wall=34621
2022-03-16 02:11:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:12:15 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.31 | ppl 79.35 | wps 36903.2 | wpb 511.9 | bsz 1 | num_updates 10507 | best_loss 6.31
2022-03-16 02:12:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10507 updates
2022-03-16 02:12:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 02:12:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 02:12:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 27 @ 10507 updates, score 6.31) (writing took 2.2441351739689708 seconds)
2022-03-16 02:12:18 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-16 02:12:18 | INFO | train | epoch 027 | loss 6.217 | ppl 74.41 | wps 23712.3 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 10507 | lr 0.000308504 | gnorm 0.536 | loss_scale 8 | train_wall 944 | gb_free 9.6 | wall 34675
KL Stats: Epoch 27 Divergences: Uniform: 4.453503814617079 Unigram: 3.014751225816489
2022-03-16 02:12:18 | INFO | fairseq.trainer | begin training epoch 28
2022-03-16 02:12:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:16:24 | INFO | train_inner | epoch 028:     93 / 392 loss=6.177, ppl=72.36, wps=21686.9, ups=0.33, wpb=65025.8, bsz=127, num_updates=10600, lr=0.000307148, gnorm=0.541, loss_scale=8, train_wall=239, gb_free=9.6, wall=34921
2022-03-16 02:20:48 | INFO | train_inner | epoch 028:    193 / 392 loss=6.189, ppl=72.96, wps=24761.3, ups=0.38, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.537, loss_scale=16, train_wall=241, gb_free=9.6, wall=35186
2022-03-16 02:22:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:25:16 | INFO | train_inner | epoch 028:    294 / 392 loss=6.209, ppl=74, wps=24523.1, ups=0.37, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.537, loss_scale=8, train_wall=243, gb_free=9.6, wall=35453
2022-03-16 02:29:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:30:08 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.306 | ppl 79.15 | wps 36910.8 | wpb 511.9 | bsz 1 | num_updates 10898 | best_loss 6.306
2022-03-16 02:30:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10898 updates
2022-03-16 02:30:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 02:30:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 02:30:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 28 @ 10898 updates, score 6.306) (writing took 2.2718723070574924 seconds)
2022-03-16 02:30:10 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-16 02:30:10 | INFO | train | epoch 028 | loss 6.197 | ppl 73.35 | wps 23847.4 | ups 0.36 | wpb 65405.5 | bsz 127.7 | num_updates 10898 | lr 0.000302919 | gnorm 0.537 | loss_scale 16 | train_wall 943 | gb_free 9.6 | wall 35747
KL Stats: Epoch 28 Divergences: Uniform: 4.4700372264060855 Unigram: 3.0247090665779965
2022-03-16 02:30:10 | INFO | fairseq.trainer | begin training epoch 29
2022-03-16 02:30:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:30:15 | INFO | train_inner | epoch 029:      2 / 392 loss=6.211, ppl=74.08, wps=21689.8, ups=0.33, wpb=65029.1, bsz=127, num_updates=10900, lr=0.000302891, gnorm=0.536, loss_scale=16, train_wall=239, gb_free=9.6, wall=35753
2022-03-16 02:33:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:34:42 | INFO | train_inner | epoch 029:    103 / 392 loss=6.151, ppl=71.06, wps=24539, ups=0.37, wpb=65536, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.541, loss_scale=8, train_wall=243, gb_free=9.6, wall=36020
2022-03-16 02:39:07 | INFO | train_inner | epoch 029:    203 / 392 loss=6.174, ppl=72.21, wps=24781.8, ups=0.38, wpb=65532.7, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.532, loss_scale=8, train_wall=241, gb_free=9.6, wall=36284
2022-03-16 02:43:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:43:34 | INFO | train_inner | epoch 029:    304 / 392 loss=6.186, ppl=72.81, wps=24518, ups=0.37, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.539, loss_scale=8, train_wall=243, gb_free=9.6, wall=36551
2022-03-16 02:47:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:48:00 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.284 | ppl 77.95 | wps 36900.4 | wpb 511.9 | bsz 1 | num_updates 11288 | best_loss 6.284
2022-03-16 02:48:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11288 updates
2022-03-16 02:48:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 02:48:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 02:48:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 29 @ 11288 updates, score 6.284) (writing took 2.2565287119941786 seconds)
2022-03-16 02:48:02 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-16 02:48:02 | INFO | train | epoch 029 | loss 6.178 | ppl 72.38 | wps 23786.5 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 11288 | lr 0.00029764 | gnorm 0.538 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 36820
KL Stats: Epoch 29 Divergences: Uniform: 4.479800164623786 Unigram: 3.0314941125278
2022-03-16 02:48:02 | INFO | fairseq.trainer | begin training epoch 30
2022-03-16 02:48:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:48:34 | INFO | train_inner | epoch 030:     12 / 392 loss=6.195, ppl=73.24, wps=21672.3, ups=0.33, wpb=65029.1, bsz=127, num_updates=11300, lr=0.000297482, gnorm=0.544, loss_scale=8, train_wall=239, gb_free=9.6, wall=36851
2022-03-16 02:52:59 | INFO | train_inner | epoch 030:    112 / 392 loss=6.144, ppl=70.7, wps=24747.7, ups=0.38, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.528, loss_scale=16, train_wall=241, gb_free=9.6, wall=37116
2022-03-16 02:53:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:57:27 | INFO | train_inner | epoch 030:    213 / 392 loss=6.156, ppl=71.33, wps=24502.8, ups=0.37, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.548, loss_scale=8, train_wall=244, gb_free=9.6, wall=37384
2022-03-16 03:01:51 | INFO | train_inner | epoch 030:    313 / 392 loss=6.171, ppl=72.03, wps=24751.6, ups=0.38, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.528, loss_scale=16, train_wall=241, gb_free=9.6, wall=37649
2022-03-16 03:05:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:05:53 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.267 | ppl 76.99 | wps 36974.3 | wpb 511.9 | bsz 1 | num_updates 11679 | best_loss 6.267
2022-03-16 03:05:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11679 updates
2022-03-16 03:05:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 03:05:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 03:05:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 30 @ 11679 updates, score 6.267) (writing took 2.259645194048062 seconds)
2022-03-16 03:05:55 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-16 03:05:55 | INFO | train | epoch 030 | loss 6.159 | ppl 71.47 | wps 23832.5 | ups 0.36 | wpb 65405.5 | bsz 127.7 | num_updates 11679 | lr 0.000292615 | gnorm 0.535 | loss_scale 32 | train_wall 943 | gb_free 9.6 | wall 37893
KL Stats: Epoch 30 Divergences: Uniform: 4.490219956185984 Unigram: 3.0377113784164966
2022-03-16 03:05:55 | INFO | fairseq.trainer | begin training epoch 31
2022-03-16 03:05:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:06:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:06:54 | INFO | train_inner | epoch 031:     22 / 392 loss=6.157, ppl=71.36, wps=21500.9, ups=0.33, wpb=65025.8, bsz=127, num_updates=11700, lr=0.000292353, gnorm=0.539, loss_scale=16, train_wall=242, gb_free=9.6, wall=37951
2022-03-16 03:07:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:11:21 | INFO | train_inner | epoch 031:    123 / 392 loss=6.118, ppl=69.44, wps=24512.9, ups=0.37, wpb=65532.7, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.53, loss_scale=8, train_wall=244, gb_free=9.6, wall=38218
2022-03-16 03:15:46 | INFO | train_inner | epoch 031:    223 / 392 loss=6.155, ppl=71.28, wps=24741.1, ups=0.38, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.541, loss_scale=16, train_wall=241, gb_free=9.6, wall=38483
2022-03-16 03:16:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:20:13 | INFO | train_inner | epoch 031:    324 / 392 loss=6.148, ppl=70.91, wps=24511.7, ups=0.37, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.538, loss_scale=8, train_wall=244, gb_free=9.6, wall=38751
2022-03-16 03:22:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:23:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:23:46 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.262 | ppl 76.75 | wps 36919.8 | wpb 511.9 | bsz 1 | num_updates 12067 | best_loss 6.262
2022-03-16 03:23:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12067 updates
2022-03-16 03:23:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 03:23:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 03:23:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 31 @ 12067 updates, score 6.262) (writing took 2.2767799079883844 seconds)
2022-03-16 03:23:49 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-16 03:23:49 | INFO | train | epoch 031 | loss 6.142 | ppl 70.61 | wps 23644.7 | ups 0.36 | wpb 65404.5 | bsz 127.7 | num_updates 12067 | lr 0.000287873 | gnorm 0.538 | loss_scale 8 | train_wall 944 | gb_free 9.6 | wall 38966
KL Stats: Epoch 31 Divergences: Uniform: 4.501065818211195 Unigram: 3.0455577108953165
2022-03-16 03:23:49 | INFO | fairseq.trainer | begin training epoch 32
2022-03-16 03:23:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:25:16 | INFO | train_inner | epoch 032:     33 / 392 loss=6.143, ppl=70.65, wps=21468.4, ups=0.33, wpb=65029.1, bsz=127, num_updates=12100, lr=0.00028748, gnorm=0.543, loss_scale=8, train_wall=242, gb_free=9.6, wall=39053
2022-03-16 03:29:41 | INFO | train_inner | epoch 032:    133 / 392 loss=6.112, ppl=69.17, wps=24750, ups=0.38, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.538, loss_scale=16, train_wall=241, gb_free=9.6, wall=39318
2022-03-16 03:34:06 | INFO | train_inner | epoch 032:    233 / 392 loss=6.128, ppl=69.96, wps=24766.9, ups=0.38, wpb=65532.7, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.53, loss_scale=32, train_wall=241, gb_free=9.6, wall=39583
2022-03-16 03:34:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:38:33 | INFO | train_inner | epoch 032:    334 / 392 loss=6.13, ppl=70.04, wps=24525.3, ups=0.37, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.538, loss_scale=16, train_wall=243, gb_free=9.6, wall=39850
2022-03-16 03:39:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:41:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:41:39 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.256 | ppl 76.44 | wps 36956.8 | wpb 511.9 | bsz 1 | num_updates 12457 | best_loss 6.256
2022-03-16 03:41:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12457 updates
2022-03-16 03:41:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 03:41:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 03:41:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 32 @ 12457 updates, score 6.256) (writing took 2.2646876230137423 seconds)
2022-03-16 03:41:42 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-16 03:41:42 | INFO | train | epoch 032 | loss 6.126 | ppl 69.85 | wps 23775.3 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 12457 | lr 0.00028333 | gnorm 0.538 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 40039
KL Stats: Epoch 32 Divergences: Uniform: 4.5133071040054755 Unigram: 3.0534097495718684
2022-03-16 03:41:42 | INFO | fairseq.trainer | begin training epoch 33
2022-03-16 03:41:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:43:35 | INFO | train_inner | epoch 033:     43 / 392 loss=6.125, ppl=69.8, wps=21492.4, ups=0.33, wpb=65029.1, bsz=127, num_updates=12500, lr=0.000282843, gnorm=0.546, loss_scale=8, train_wall=242, gb_free=9.6, wall=40153
2022-03-16 03:48:00 | INFO | train_inner | epoch 033:    143 / 392 loss=6.092, ppl=68.22, wps=24777.2, ups=0.38, wpb=65532.7, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.537, loss_scale=16, train_wall=241, gb_free=9.6, wall=40417
2022-03-16 03:51:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:52:27 | INFO | train_inner | epoch 033:    244 / 392 loss=6.12, ppl=69.54, wps=24537.2, ups=0.37, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.54, loss_scale=16, train_wall=243, gb_free=9.6, wall=40684
2022-03-16 03:56:52 | INFO | train_inner | epoch 033:    344 / 392 loss=6.119, ppl=69.52, wps=24763.8, ups=0.38, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.538, loss_scale=16, train_wall=241, gb_free=9.6, wall=40949
2022-03-16 03:56:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:58:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:59:32 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.243 | ppl 75.72 | wps 36906.6 | wpb 511.9 | bsz 1 | num_updates 12847 | best_loss 6.243
2022-03-16 03:59:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12847 updates
2022-03-16 03:59:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 03:59:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 03:59:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 33 @ 12847 updates, score 6.243) (writing took 2.2320019580656663 seconds)
2022-03-16 03:59:34 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-16 03:59:34 | INFO | train | epoch 033 | loss 6.111 | ppl 69.12 | wps 23783.9 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 12847 | lr 0.000278997 | gnorm 0.539 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 41111
KL Stats: Epoch 33 Divergences: Uniform: 4.524129669956598 Unigram: 3.059640371517704
2022-03-16 03:59:34 | INFO | fairseq.trainer | begin training epoch 34
2022-03-16 03:59:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:01:55 | INFO | train_inner | epoch 034:     53 / 392 loss=6.099, ppl=68.56, wps=21471.8, ups=0.33, wpb=65029.1, bsz=127, num_updates=12900, lr=0.000278423, gnorm=0.541, loss_scale=8, train_wall=242, gb_free=9.6, wall=41252
2022-03-16 04:06:19 | INFO | train_inner | epoch 034:    153 / 392 loss=6.086, ppl=67.92, wps=24750.2, ups=0.38, wpb=65532.7, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.53, loss_scale=16, train_wall=241, gb_free=9.6, wall=41516
2022-03-16 04:08:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:09:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 04:10:49 | INFO | train_inner | epoch 034:    255 / 392 loss=6.103, ppl=68.72, wps=24296.5, ups=0.37, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.537, loss_scale=8, train_wall=246, gb_free=9.6, wall=41786
2022-03-16 04:15:13 | INFO | train_inner | epoch 034:    355 / 392 loss=6.108, ppl=68.99, wps=24790.9, ups=0.38, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.541, loss_scale=8, train_wall=241, gb_free=9.6, wall=42051
2022-03-16 04:16:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:17:24 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.24 | ppl 75.61 | wps 36911.3 | wpb 511.9 | bsz 1 | num_updates 13237 | best_loss 6.24
2022-03-16 04:17:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13237 updates
2022-03-16 04:17:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 04:17:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 04:17:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 34 @ 13237 updates, score 6.24) (writing took 2.2508483619894832 seconds)
2022-03-16 04:17:27 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-16 04:17:27 | INFO | train | epoch 034 | loss 6.096 | ppl 68.41 | wps 23781.2 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 13237 | lr 0.000274856 | gnorm 0.537 | loss_scale 16 | train_wall 943 | gb_free 9.6 | wall 42184
KL Stats: Epoch 34 Divergences: Uniform: 4.534638879987942 Unigram: 3.0674025478999587
2022-03-16 04:17:27 | INFO | fairseq.trainer | begin training epoch 35
2022-03-16 04:17:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:20:14 | INFO | train_inner | epoch 035:     63 / 392 loss=6.075, ppl=67.41, wps=21662.1, ups=0.33, wpb=65029.1, bsz=127, num_updates=13300, lr=0.000274204, gnorm=0.543, loss_scale=16, train_wall=239, gb_free=9.6, wall=42351
2022-03-16 04:21:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:24:40 | INFO | train_inner | epoch 035:    164 / 392 loss=6.07, ppl=67.16, wps=24556.9, ups=0.37, wpb=65532.7, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.532, loss_scale=16, train_wall=243, gb_free=9.6, wall=42618
2022-03-16 04:27:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:28:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 04:29:10 | INFO | train_inner | epoch 035:    266 / 392 loss=6.095, ppl=68.37, wps=24302.2, ups=0.37, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.539, loss_scale=8, train_wall=246, gb_free=9.6, wall=42887
2022-03-16 04:33:34 | INFO | train_inner | epoch 035:    366 / 392 loss=6.097, ppl=68.47, wps=24802.7, ups=0.38, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.539, loss_scale=8, train_wall=241, gb_free=9.6, wall=43152
2022-03-16 04:34:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:35:16 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.221 | ppl 74.61 | wps 36905.7 | wpb 511.9 | bsz 1 | num_updates 13626 | best_loss 6.221
2022-03-16 04:35:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13626 updates
2022-03-16 04:35:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 04:35:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 04:35:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 35 @ 13626 updates, score 6.221) (writing took 2.260479497956112 seconds)
2022-03-16 04:35:18 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-16 04:35:18 | INFO | train | epoch 035 | loss 6.083 | ppl 67.78 | wps 23737.2 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 13626 | lr 0.000270904 | gnorm 0.539 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 43256
KL Stats: Epoch 35 Divergences: Uniform: 4.543648776542069 Unigram: 3.07240770562348
2022-03-16 04:35:19 | INFO | fairseq.trainer | begin training epoch 36
2022-03-16 04:35:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:38:34 | INFO | train_inner | epoch 036:     74 / 392 loss=6.051, ppl=66.31, wps=21665.9, ups=0.33, wpb=65025.8, bsz=127, num_updates=13700, lr=0.000270172, gnorm=0.536, loss_scale=16, train_wall=239, gb_free=9.6, wall=43452
2022-03-16 04:40:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:43:02 | INFO | train_inner | epoch 036:    175 / 392 loss=6.066, ppl=67, wps=24516.1, ups=0.37, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.532, loss_scale=16, train_wall=244, gb_free=9.6, wall=43719
2022-03-16 04:45:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 04:47:29 | INFO | train_inner | epoch 036:    276 / 392 loss=6.079, ppl=67.62, wps=24519.5, ups=0.37, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.534, loss_scale=8, train_wall=243, gb_free=9.6, wall=43986
2022-03-16 04:51:54 | INFO | train_inner | epoch 036:    376 / 392 loss=6.09, ppl=68.1, wps=24767, ups=0.38, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.542, loss_scale=16, train_wall=241, gb_free=9.6, wall=44251
2022-03-16 04:52:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:53:09 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.223 | ppl 74.72 | wps 36875.6 | wpb 511.9 | bsz 1 | num_updates 14016 | best_loss 6.221
2022-03-16 04:53:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14016 updates
2022-03-16 04:53:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt
2022-03-16 04:53:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt
2022-03-16 04:53:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt (epoch 36 @ 14016 updates, score 6.223) (writing took 1.2666668789461255 seconds)
2022-03-16 04:53:10 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-16 04:53:10 | INFO | train | epoch 036 | loss 6.07 | ppl 67.19 | wps 23797.9 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 14016 | lr 0.000267109 | gnorm 0.537 | loss_scale 16 | train_wall 943 | gb_free 9.6 | wall 44328
KL Stats: Epoch 36 Divergences: Uniform: 4.553761194383495 Unigram: 3.080541442644707
2022-03-16 04:53:10 | INFO | fairseq.trainer | begin training epoch 37
2022-03-16 04:53:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:56:53 | INFO | train_inner | epoch 037:     84 / 392 loss=6.044, ppl=65.96, wps=21740.5, ups=0.33, wpb=65029.1, bsz=127, num_updates=14100, lr=0.000266312, gnorm=0.544, loss_scale=16, train_wall=239, gb_free=9.6, wall=44550
2022-03-16 04:57:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:01:20 | INFO | train_inner | epoch 037:    185 / 392 loss=6.05, ppl=66.27, wps=24522.6, ups=0.37, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.542, loss_scale=16, train_wall=243, gb_free=9.6, wall=44817
2022-03-16 05:04:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:05:47 | INFO | train_inner | epoch 037:    286 / 392 loss=6.064, ppl=66.91, wps=24518.5, ups=0.37, wpb=65532.7, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.534, loss_scale=16, train_wall=244, gb_free=9.6, wall=45085
2022-03-16 05:09:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:10:15 | INFO | train_inner | epoch 037:    387 / 392 loss=6.079, ppl=67.58, wps=24523.5, ups=0.37, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.541, loss_scale=16, train_wall=244, gb_free=9.6, wall=45352
2022-03-16 05:10:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:11:01 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.219 | ppl 74.5 | wps 36882.2 | wpb 511.9 | bsz 1 | num_updates 14405 | best_loss 6.219
2022-03-16 05:11:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14405 updates
2022-03-16 05:11:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 05:11:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 05:11:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 37 @ 14405 updates, score 6.219) (writing took 2.2643184210173786 seconds)
2022-03-16 05:11:03 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-16 05:11:03 | INFO | train | epoch 037 | loss 6.058 | ppl 66.62 | wps 23717.7 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 14405 | lr 0.000263477 | gnorm 0.539 | loss_scale 16 | train_wall 943 | gb_free 9.6 | wall 45400
KL Stats: Epoch 37 Divergences: Uniform: 4.562437050788854 Unigram: 3.0852761544127616
2022-03-16 05:11:03 | INFO | fairseq.trainer | begin training epoch 38
2022-03-16 05:11:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:11:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:15:17 | INFO | train_inner | epoch 038:     96 / 392 loss=6.026, ppl=65.16, wps=21478.4, ups=0.33, wpb=65029.1, bsz=127, num_updates=14500, lr=0.000262613, gnorm=0.551, loss_scale=8, train_wall=242, gb_free=9.6, wall=45655
2022-03-16 05:19:42 | INFO | train_inner | epoch 038:    196 / 392 loss=6.042, ppl=65.87, wps=24791.8, ups=0.38, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.542, loss_scale=16, train_wall=241, gb_free=9.6, wall=45919
2022-03-16 05:23:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:24:09 | INFO | train_inner | epoch 038:    297 / 392 loss=6.056, ppl=66.55, wps=24543.1, ups=0.37, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.535, loss_scale=16, train_wall=243, gb_free=9.6, wall=46186
2022-03-16 05:24:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:28:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:28:53 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.206 | ppl 73.85 | wps 36889.7 | wpb 511.9 | bsz 1 | num_updates 14794 | best_loss 6.206
2022-03-16 05:28:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14794 updates
2022-03-16 05:28:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 05:28:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 05:28:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 38 @ 14794 updates, score 6.206) (writing took 2.2399147329851985 seconds)
2022-03-16 05:28:55 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-16 05:28:55 | INFO | train | epoch 038 | loss 6.047 | ppl 66.1 | wps 23734.5 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 14794 | lr 0.00025999 | gnorm 0.541 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 46472
KL Stats: Epoch 38 Divergences: Uniform: 4.570032517311711 Unigram: 3.090085650963538
2022-03-16 05:28:55 | INFO | fairseq.trainer | begin training epoch 39
2022-03-16 05:28:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:29:11 | INFO | train_inner | epoch 039:      6 / 392 loss=6.061, ppl=66.78, wps=21509.2, ups=0.33, wpb=65025.8, bsz=127, num_updates=14800, lr=0.000259938, gnorm=0.541, loss_scale=8, train_wall=241, gb_free=9.6, wall=46488
2022-03-16 05:33:35 | INFO | train_inner | epoch 039:    106 / 392 loss=6.016, ppl=64.72, wps=24780.6, ups=0.38, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.538, loss_scale=16, train_wall=241, gb_free=9.6, wall=46753
2022-03-16 05:37:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:37:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:38:05 | INFO | train_inner | epoch 039:    208 / 392 loss=6.028, ppl=65.26, wps=24288.5, ups=0.37, wpb=65532.7, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.537, loss_scale=8, train_wall=246, gb_free=9.6, wall=47022
2022-03-16 05:42:30 | INFO | train_inner | epoch 039:    308 / 392 loss=6.048, ppl=66.16, wps=24778.1, ups=0.38, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.537, loss_scale=8, train_wall=241, gb_free=9.6, wall=47287
2022-03-16 05:45:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:46:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:46:45 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.199 | ppl 73.47 | wps 36901.7 | wpb 511.9 | bsz 1 | num_updates 15183 | best_loss 6.199
2022-03-16 05:46:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15183 updates
2022-03-16 05:46:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 05:46:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 05:46:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 39 @ 15183 updates, score 6.199) (writing took 2.248948212945834 seconds)
2022-03-16 05:46:47 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-16 05:46:47 | INFO | train | epoch 039 | loss 6.035 | ppl 65.59 | wps 23736.7 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 15183 | lr 0.000256638 | gnorm 0.539 | loss_scale 8 | train_wall 942 | gb_free 9.6 | wall 47544
KL Stats: Epoch 39 Divergences: Uniform: 4.579726319136178 Unigram: 3.095667255184039
2022-03-16 05:46:47 | INFO | fairseq.trainer | begin training epoch 40
2022-03-16 05:46:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:47:32 | INFO | train_inner | epoch 040:     17 / 392 loss=6.04, ppl=65.82, wps=21520, ups=0.33, wpb=65029.1, bsz=127, num_updates=15200, lr=0.000256495, gnorm=0.544, loss_scale=8, train_wall=241, gb_free=9.6, wall=47589
2022-03-16 05:51:56 | INFO | train_inner | epoch 040:    117 / 392 loss=6.004, ppl=64.17, wps=24796.7, ups=0.38, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.538, loss_scale=16, train_wall=241, gb_free=9.6, wall=47853
2022-03-16 05:56:21 | INFO | train_inner | epoch 040:    217 / 392 loss=6.025, ppl=65.13, wps=24795.6, ups=0.38, wpb=65532.7, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.54, loss_scale=16, train_wall=241, gb_free=9.6, wall=48118
2022-03-16 05:57:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:00:48 | INFO | train_inner | epoch 040:    318 / 392 loss=6.035, ppl=65.57, wps=24533.4, ups=0.37, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.537, loss_scale=16, train_wall=243, gb_free=9.6, wall=48385
2022-03-16 06:03:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:04:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:04:36 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.194 | ppl 73.22 | wps 36900.3 | wpb 511.9 | bsz 1 | num_updates 15573 | best_loss 6.194
2022-03-16 06:04:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15573 updates
2022-03-16 06:04:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 06:04:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 06:04:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 40 @ 15573 updates, score 6.194) (writing took 2.261423367075622 seconds)
2022-03-16 06:04:39 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-16 06:04:39 | INFO | train | epoch 040 | loss 6.025 | ppl 65.11 | wps 23800.9 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 15573 | lr 0.000253404 | gnorm 0.54 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 48616
KL Stats: Epoch 40 Divergences: Uniform: 4.586725133834383 Unigram: 3.0999648898710417
2022-03-16 06:04:39 | INFO | fairseq.trainer | begin training epoch 41
2022-03-16 06:04:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:05:50 | INFO | train_inner | epoch 041:     27 / 392 loss=6.031, ppl=65.38, wps=21495.3, ups=0.33, wpb=65029.1, bsz=127, num_updates=15600, lr=0.000253185, gnorm=0.546, loss_scale=16, train_wall=242, gb_free=9.6, wall=48687
2022-03-16 06:09:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:10:18 | INFO | train_inner | epoch 041:    128 / 392 loss=6.002, ppl=64.07, wps=24500.4, ups=0.37, wpb=65532.7, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.541, loss_scale=16, train_wall=244, gb_free=9.6, wall=48955
2022-03-16 06:14:42 | INFO | train_inner | epoch 041:    228 / 392 loss=6.016, ppl=64.73, wps=24770.6, ups=0.38, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.53, loss_scale=16, train_wall=241, gb_free=9.6, wall=49219
2022-03-16 06:15:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 06:19:09 | INFO | train_inner | epoch 041:    329 / 392 loss=6.025, ppl=65.13, wps=24531.5, ups=0.37, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.536, loss_scale=16, train_wall=243, gb_free=9.6, wall=49487
2022-03-16 06:20:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:21:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:22:29 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.194 | ppl 73.19 | wps 36931.8 | wpb 511.9 | bsz 1 | num_updates 15962 | best_loss 6.194
2022-03-16 06:22:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 15962 updates
2022-03-16 06:22:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 06:22:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 06:22:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 41 @ 15962 updates, score 6.194) (writing took 2.2102086370578036 seconds)
2022-03-16 06:22:31 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-16 06:22:31 | INFO | train | epoch 041 | loss 6.014 | ppl 64.64 | wps 23724.6 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 15962 | lr 0.000250297 | gnorm 0.542 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 49688
KL Stats: Epoch 41 Divergences: Uniform: 4.597564712608474 Unigram: 3.1068691578918766
2022-03-16 06:22:31 | INFO | fairseq.trainer | begin training epoch 42
2022-03-16 06:22:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:24:12 | INFO | train_inner | epoch 042:     38 / 392 loss=6.006, ppl=64.25, wps=21521, ups=0.33, wpb=65029.1, bsz=127, num_updates=16000, lr=0.00025, gnorm=0.561, loss_scale=8, train_wall=241, gb_free=9.6, wall=49789
2022-03-16 06:28:36 | INFO | train_inner | epoch 042:    138 / 392 loss=5.994, ppl=63.74, wps=24794.2, ups=0.38, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.537, loss_scale=16, train_wall=241, gb_free=9.6, wall=50053
2022-03-16 06:29:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:33:03 | INFO | train_inner | epoch 042:    239 / 392 loss=6.003, ppl=64.13, wps=24538.1, ups=0.37, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.538, loss_scale=8, train_wall=243, gb_free=9.6, wall=50320
2022-03-16 06:37:27 | INFO | train_inner | epoch 042:    339 / 392 loss=6.023, ppl=65.04, wps=24791.3, ups=0.38, wpb=65532.7, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.542, loss_scale=16, train_wall=241, gb_free=9.6, wall=50585
2022-03-16 06:39:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:40:20 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.186 | ppl 72.82 | wps 36947.4 | wpb 511.9 | bsz 1 | num_updates 16353 | best_loss 6.186
2022-03-16 06:40:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16353 updates
2022-03-16 06:40:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 06:40:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 06:40:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 42 @ 16353 updates, score 6.186) (writing took 2.2240523350192234 seconds)
2022-03-16 06:40:23 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-16 06:40:23 | INFO | train | epoch 042 | loss 6.005 | ppl 64.22 | wps 23862.6 | ups 0.36 | wpb 65405.5 | bsz 127.7 | num_updates 16353 | lr 0.000247287 | gnorm 0.542 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 50760
KL Stats: Epoch 42 Divergences: Uniform: 4.605377244803749 Unigram: 3.111100647642143
2022-03-16 06:40:23 | INFO | fairseq.trainer | begin training epoch 43
2022-03-16 06:40:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:40:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:42:30 | INFO | train_inner | epoch 043:     48 / 392 loss=5.997, ppl=63.86, wps=21494.6, ups=0.33, wpb=65029.1, bsz=127, num_updates=16400, lr=0.000246932, gnorm=0.552, loss_scale=8, train_wall=242, gb_free=9.6, wall=50887
2022-03-16 06:46:54 | INFO | train_inner | epoch 043:    148 / 392 loss=5.985, ppl=63.33, wps=24785.6, ups=0.38, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.548, loss_scale=16, train_wall=241, gb_free=9.6, wall=51151
2022-03-16 06:47:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:51:21 | INFO | train_inner | epoch 043:    249 / 392 loss=5.998, ppl=63.91, wps=24526.8, ups=0.37, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.545, loss_scale=8, train_wall=243, gb_free=9.6, wall=51419
2022-03-16 06:55:46 | INFO | train_inner | epoch 043:    349 / 392 loss=6.011, ppl=64.48, wps=24776, ups=0.38, wpb=65532.7, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.54, loss_scale=16, train_wall=241, gb_free=9.6, wall=51683
2022-03-16 06:56:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:57:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:58:13 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.177 | ppl 72.35 | wps 36844.6 | wpb 511.9 | bsz 1 | num_updates 16742 | best_loss 6.177
2022-03-16 06:58:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16742 updates
2022-03-16 06:58:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 06:58:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 06:58:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 43 @ 16742 updates, score 6.177) (writing took 2.265844031004235 seconds)
2022-03-16 06:58:15 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-16 06:58:15 | INFO | train | epoch 043 | loss 5.996 | ppl 63.81 | wps 23728.4 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 16742 | lr 0.000244397 | gnorm 0.546 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 51832
KL Stats: Epoch 43 Divergences: Uniform: 4.612221096146732 Unigram: 3.1162985655591577
2022-03-16 06:58:15 | INFO | fairseq.trainer | begin training epoch 44
2022-03-16 06:58:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:00:49 | INFO | train_inner | epoch 044:     58 / 392 loss=5.983, ppl=63.23, wps=21483.9, ups=0.33, wpb=65029.1, bsz=127, num_updates=16800, lr=0.000243975, gnorm=0.549, loss_scale=8, train_wall=242, gb_free=9.6, wall=51986
2022-03-16 07:05:13 | INFO | train_inner | epoch 044:    158 / 392 loss=5.971, ppl=62.73, wps=24747.3, ups=0.38, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.539, loss_scale=16, train_wall=241, gb_free=9.6, wall=52251
2022-03-16 07:08:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:09:41 | INFO | train_inner | epoch 044:    259 / 392 loss=5.999, ppl=63.94, wps=24534, ups=0.37, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.542, loss_scale=16, train_wall=243, gb_free=9.6, wall=52518
2022-03-16 07:14:05 | INFO | train_inner | epoch 044:    359 / 392 loss=5.998, ppl=63.9, wps=24767.6, ups=0.38, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.547, loss_scale=16, train_wall=241, gb_free=9.6, wall=52782
2022-03-16 07:14:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:15:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:16:05 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.179 | ppl 72.45 | wps 37005.2 | wpb 511.9 | bsz 1 | num_updates 17132 | best_loss 6.177
2022-03-16 07:16:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17132 updates
2022-03-16 07:16:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt
2022-03-16 07:16:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt
2022-03-16 07:16:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt (epoch 44 @ 17132 updates, score 6.179) (writing took 1.2496309980051592 seconds)
2022-03-16 07:16:07 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-16 07:16:07 | INFO | train | epoch 044 | loss 5.987 | ppl 63.4 | wps 23802.5 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 17132 | lr 0.000241599 | gnorm 0.544 | loss_scale 16 | train_wall 943 | gb_free 9.6 | wall 52904
KL Stats: Epoch 44 Divergences: Uniform: 4.6213874528802465 Unigram: 3.1226106069442356
2022-03-16 07:16:07 | INFO | fairseq.trainer | begin training epoch 45
2022-03-16 07:16:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:19:06 | INFO | train_inner | epoch 045:     68 / 392 loss=5.971, ppl=62.72, wps=21582.8, ups=0.33, wpb=65025.8, bsz=127, num_updates=17200, lr=0.000241121, gnorm=0.552, loss_scale=16, train_wall=241, gb_free=9.6, wall=53084
2022-03-16 07:20:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:21:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:23:36 | INFO | train_inner | epoch 045:    170 / 392 loss=5.962, ppl=62.32, wps=24301.4, ups=0.37, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.544, loss_scale=8, train_wall=246, gb_free=9.6, wall=53353
2022-03-16 07:28:01 | INFO | train_inner | epoch 045:    270 / 392 loss=5.989, ppl=63.5, wps=24761, ups=0.38, wpb=65532.7, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.548, loss_scale=16, train_wall=241, gb_free=9.6, wall=53618
2022-03-16 07:28:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:32:28 | INFO | train_inner | epoch 045:    371 / 392 loss=5.997, ppl=63.85, wps=24543.8, ups=0.37, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.542, loss_scale=8, train_wall=243, gb_free=9.6, wall=53885
2022-03-16 07:33:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:33:56 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.169 | ppl 71.97 | wps 36955.2 | wpb 511.9 | bsz 1 | num_updates 17521 | best_loss 6.169
2022-03-16 07:33:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17521 updates
2022-03-16 07:33:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 07:33:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 07:33:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 45 @ 17521 updates, score 6.169) (writing took 2.5185552180046216 seconds)
2022-03-16 07:33:59 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-16 07:33:59 | INFO | train | epoch 045 | loss 5.978 | ppl 63.03 | wps 23729.3 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 17521 | lr 0.000238902 | gnorm 0.547 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 53976
KL Stats: Epoch 45 Divergences: Uniform: 4.626325204894706 Unigram: 3.1248275792078704
2022-03-16 07:33:59 | INFO | fairseq.trainer | begin training epoch 46
2022-03-16 07:33:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:35:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:37:31 | INFO | train_inner | epoch 046:     80 / 392 loss=5.956, ppl=62.06, wps=21481.4, ups=0.33, wpb=65025.8, bsz=127, num_updates=17600, lr=0.000238366, gnorm=0.555, loss_scale=8, train_wall=242, gb_free=9.6, wall=54188
2022-03-16 07:41:55 | INFO | train_inner | epoch 046:    180 / 392 loss=5.967, ppl=62.53, wps=24776.9, ups=0.38, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.544, loss_scale=16, train_wall=241, gb_free=9.6, wall=54452
2022-03-16 07:46:20 | INFO | train_inner | epoch 046:    280 / 392 loss=5.976, ppl=62.95, wps=24774.5, ups=0.38, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.537, loss_scale=16, train_wall=241, gb_free=9.6, wall=54717
2022-03-16 07:47:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:48:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:50:49 | INFO | train_inner | epoch 046:    382 / 392 loss=5.983, ppl=63.24, wps=24295.3, ups=0.37, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.551, loss_scale=8, train_wall=246, gb_free=9.6, wall=54987
2022-03-16 07:51:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:51:49 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.163 | ppl 71.68 | wps 36930.1 | wpb 511.9 | bsz 1 | num_updates 17910 | best_loss 6.163
2022-03-16 07:51:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 17910 updates
2022-03-16 07:51:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 07:51:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 07:51:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 46 @ 17910 updates, score 6.163) (writing took 2.2449202159186825 seconds)
2022-03-16 07:51:51 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-16 07:51:51 | INFO | train | epoch 046 | loss 5.969 | ppl 62.66 | wps 23729.4 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 17910 | lr 0.000236294 | gnorm 0.547 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 55048
KL Stats: Epoch 46 Divergences: Uniform: 4.6340133615785515 Unigram: 3.1288888383445737
2022-03-16 07:51:51 | INFO | fairseq.trainer | begin training epoch 47
2022-03-16 07:51:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:55:49 | INFO | train_inner | epoch 047:     90 / 392 loss=5.943, ppl=61.5, wps=21698.9, ups=0.33, wpb=65029.1, bsz=127, num_updates=18000, lr=0.000235702, gnorm=0.553, loss_scale=16, train_wall=239, gb_free=9.6, wall=55286
2022-03-16 07:58:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:00:16 | INFO | train_inner | epoch 047:    191 / 392 loss=5.955, ppl=62.04, wps=24527.8, ups=0.37, wpb=65532.7, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.545, loss_scale=8, train_wall=243, gb_free=9.6, wall=55553
2022-03-16 08:04:41 | INFO | train_inner | epoch 047:    291 / 392 loss=5.969, ppl=62.66, wps=24744, ups=0.38, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.541, loss_scale=16, train_wall=241, gb_free=9.6, wall=55818
2022-03-16 08:08:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:09:07 | INFO | train_inner | epoch 047:    392 / 392 loss=5.984, ppl=63.28, wps=24435.2, ups=0.38, wpb=65029.1, bsz=127, num_updates=18300, lr=0.000233762, gnorm=0.547, loss_scale=8, train_wall=243, gb_free=9.6, wall=56084
2022-03-16 08:09:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:09:42 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 6.162 | ppl 71.59 | wps 36904.8 | wpb 511.9 | bsz 1 | num_updates 18300 | best_loss 6.162
2022-03-16 08:09:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18300 updates
2022-03-16 08:09:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 08:09:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 08:09:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 47 @ 18300 updates, score 6.162) (writing took 2.267417180002667 seconds)
2022-03-16 08:09:44 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-16 08:09:44 | INFO | train | epoch 047 | loss 5.962 | ppl 62.33 | wps 23762.7 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 18300 | lr 0.000233762 | gnorm 0.545 | loss_scale 8 | train_wall 944 | gb_free 9.6 | wall 56122
KL Stats: Epoch 47 Divergences: Uniform: 4.6398112123029955 Unigram: 3.1330103414257193
2022-03-16 08:09:44 | INFO | fairseq.trainer | begin training epoch 48
2022-03-16 08:09:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:14:09 | INFO | train_inner | epoch 048:    100 / 392 loss=5.929, ppl=60.92, wps=21703.5, ups=0.33, wpb=65532.7, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.543, loss_scale=8, train_wall=241, gb_free=9.6, wall=56386
2022-03-16 08:18:34 | INFO | train_inner | epoch 048:    200 / 392 loss=5.949, ppl=61.77, wps=24781, ups=0.38, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.546, loss_scale=16, train_wall=241, gb_free=9.6, wall=56651
2022-03-16 08:20:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:20:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:21:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-16 08:23:06 | INFO | train_inner | epoch 048:    303 / 392 loss=5.965, ppl=62.45, wps=24083.9, ups=0.37, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.544, loss_scale=4, train_wall=248, gb_free=9.6, wall=56923
2022-03-16 08:26:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:27:34 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 6.157 | ppl 71.35 | wps 36917.6 | wpb 511.9 | bsz 1 | num_updates 18689 | best_loss 6.157
2022-03-16 08:27:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18689 updates
2022-03-16 08:27:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 08:27:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 08:27:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 48 @ 18689 updates, score 6.157) (writing took 2.269618676044047 seconds)
2022-03-16 08:27:36 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-16 08:27:36 | INFO | train | epoch 048 | loss 5.954 | ppl 61.99 | wps 23733.2 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 18689 | lr 0.000231317 | gnorm 0.546 | loss_scale 4 | train_wall 942 | gb_free 9.6 | wall 57194
KL Stats: Epoch 48 Divergences: Uniform: 4.645375208980991 Unigram: 3.136189759751169
2022-03-16 08:27:37 | INFO | fairseq.trainer | begin training epoch 49
2022-03-16 08:27:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:28:06 | INFO | train_inner | epoch 049:     11 / 392 loss=5.969, ppl=62.63, wps=21674.7, ups=0.33, wpb=65029.1, bsz=127, num_updates=18700, lr=0.000231249, gnorm=0.555, loss_scale=8, train_wall=239, gb_free=9.6, wall=57223
2022-03-16 08:32:30 | INFO | train_inner | epoch 049:    111 / 392 loss=5.926, ppl=60.8, wps=24781.4, ups=0.38, wpb=65532.7, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.541, loss_scale=8, train_wall=241, gb_free=9.6, wall=57487
2022-03-16 08:35:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:36:57 | INFO | train_inner | epoch 049:    212 / 392 loss=5.946, ppl=61.67, wps=24516.6, ups=0.37, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.551, loss_scale=8, train_wall=243, gb_free=9.6, wall=57755
2022-03-16 08:41:22 | INFO | train_inner | epoch 049:    312 / 392 loss=5.957, ppl=62.12, wps=24747.2, ups=0.38, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.542, loss_scale=16, train_wall=241, gb_free=9.6, wall=58020
2022-03-16 08:44:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:45:27 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 6.16 | ppl 71.51 | wps 36963 | wpb 511.9 | bsz 1 | num_updates 19080 | best_loss 6.157
2022-03-16 08:45:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19080 updates
2022-03-16 08:45:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt
2022-03-16 08:45:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt
2022-03-16 08:45:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt (epoch 49 @ 19080 updates, score 6.16) (writing took 1.238843997940421 seconds)
2022-03-16 08:45:28 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-16 08:45:28 | INFO | train | epoch 049 | loss 5.947 | ppl 61.69 | wps 23860.9 | ups 0.36 | wpb 65405.5 | bsz 127.7 | num_updates 19080 | lr 0.000228934 | gnorm 0.547 | loss_scale 16 | train_wall 943 | gb_free 9.6 | wall 58265
KL Stats: Epoch 49 Divergences: Uniform: 4.653758231445338 Unigram: 3.1421814983355922
2022-03-16 08:45:28 | INFO | fairseq.trainer | begin training epoch 50
2022-03-16 08:45:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:46:21 | INFO | train_inner | epoch 050:     20 / 392 loss=5.952, ppl=61.89, wps=21745.9, ups=0.33, wpb=65029.1, bsz=127, num_updates=19100, lr=0.000228814, gnorm=0.552, loss_scale=16, train_wall=239, gb_free=9.6, wall=58319
2022-03-16 08:47:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:50:49 | INFO | train_inner | epoch 050:    121 / 392 loss=5.912, ppl=60.22, wps=24494.3, ups=0.37, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.549, loss_scale=16, train_wall=244, gb_free=9.6, wall=58586
2022-03-16 08:53:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:54:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:55:19 | INFO | train_inner | epoch 050:    223 / 392 loss=5.941, ppl=61.44, wps=24271.2, ups=0.37, wpb=65532.7, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.547, loss_scale=8, train_wall=246, gb_free=9.6, wall=58856
2022-03-16 08:59:43 | INFO | train_inner | epoch 050:    323 / 392 loss=5.952, ppl=61.91, wps=24777.1, ups=0.38, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.55, loss_scale=8, train_wall=241, gb_free=9.6, wall=59121
2022-03-16 09:02:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:02:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:03:19 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 6.154 | ppl 71.21 | wps 36949.3 | wpb 511.9 | bsz 1 | num_updates 19468 | best_loss 6.154
2022-03-16 09:03:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19468 updates
2022-03-16 09:03:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 09:03:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 09:03:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 50 @ 19468 updates, score 6.154) (writing took 2.2419399929931387 seconds)
2022-03-16 09:03:21 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-16 09:03:21 | INFO | train | epoch 050 | loss 5.94 | ppl 61.37 | wps 23650 | ups 0.36 | wpb 65404.5 | bsz 127.7 | num_updates 19468 | lr 0.000226641 | gnorm 0.551 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 59338
KL Stats: Epoch 50 Divergences: Uniform: 4.660046480132194 Unigram: 3.14557917958109
2022-03-16 09:03:21 | INFO | fairseq.trainer | begin training epoch 51
2022-03-16 09:03:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:04:46 | INFO | train_inner | epoch 051:     32 / 392 loss=5.951, ppl=61.87, wps=21486.8, ups=0.33, wpb=65029.1, bsz=127, num_updates=19500, lr=0.000226455, gnorm=0.555, loss_scale=8, train_wall=242, gb_free=9.6, wall=59423
2022-03-16 09:09:10 | INFO | train_inner | epoch 051:    132 / 392 loss=5.915, ppl=60.33, wps=24784.5, ups=0.38, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.546, loss_scale=16, train_wall=241, gb_free=9.6, wall=59688
2022-03-16 09:13:35 | INFO | train_inner | epoch 051:    232 / 392 loss=5.929, ppl=60.91, wps=24786, ups=0.38, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.541, loss_scale=16, train_wall=241, gb_free=9.6, wall=59952
2022-03-16 09:14:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:18:02 | INFO | train_inner | epoch 051:    333 / 392 loss=5.953, ppl=61.94, wps=24537.5, ups=0.37, wpb=65532.7, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.547, loss_scale=16, train_wall=243, gb_free=9.6, wall=60219
2022-03-16 09:20:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:20:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:21:11 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 6.15 | ppl 70.99 | wps 36938.8 | wpb 511.9 | bsz 1 | num_updates 19858 | best_loss 6.15
2022-03-16 09:21:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 19858 updates
2022-03-16 09:21:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 09:21:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 09:21:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 51 @ 19858 updates, score 6.15) (writing took 2.2424003599444404 seconds)
2022-03-16 09:21:13 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-16 09:21:13 | INFO | train | epoch 051 | loss 5.933 | ppl 61.09 | wps 23800.3 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 19858 | lr 0.000224405 | gnorm 0.547 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 60410
KL Stats: Epoch 51 Divergences: Uniform: 4.666832734719464 Unigram: 3.1499123923591386
2022-03-16 09:21:13 | INFO | fairseq.trainer | begin training epoch 52
2022-03-16 09:21:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:21:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:23:07 | INFO | train_inner | epoch 052:     43 / 392 loss=5.93, ppl=60.97, wps=21331.1, ups=0.33, wpb=65029.1, bsz=127, num_updates=19900, lr=0.000224168, gnorm=0.557, loss_scale=8, train_wall=244, gb_free=9.6, wall=60524
2022-03-16 09:27:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:27:36 | INFO | train_inner | epoch 052:    144 / 392 loss=5.91, ppl=60.12, wps=24310.4, ups=0.37, wpb=65532.7, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.548, loss_scale=8, train_wall=246, gb_free=9.6, wall=60794
2022-03-16 09:32:12 | INFO | train_inner | epoch 052:    244 / 392 loss=5.928, ppl=60.89, wps=23799.5, ups=0.36, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.551, loss_scale=8, train_wall=251, gb_free=9.6, wall=61069
2022-03-16 09:35:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:36:42 | INFO | train_inner | epoch 052:    345 / 392 loss=5.944, ppl=61.57, wps=24248.8, ups=0.37, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.545, loss_scale=8, train_wall=246, gb_free=9.6, wall=61339
2022-03-16 09:38:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:39:21 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 6.143 | ppl 70.69 | wps 36797.1 | wpb 511.9 | bsz 1 | num_updates 20247 | best_loss 6.143
2022-03-16 09:39:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20247 updates
2022-03-16 09:39:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 09:39:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 09:39:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 52 @ 20247 updates, score 6.143) (writing took 2.2535566900623962 seconds)
2022-03-16 09:39:24 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-16 09:39:24 | INFO | train | epoch 052 | loss 5.926 | ppl 60.8 | wps 23328.9 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 20247 | lr 0.000222239 | gnorm 0.549 | loss_scale 8 | train_wall 960 | gb_free 9.6 | wall 61501
KL Stats: Epoch 52 Divergences: Uniform: 4.67425672174493 Unigram: 3.1532705221669324
2022-03-16 09:39:24 | INFO | fairseq.trainer | begin training epoch 53
2022-03-16 09:39:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:41:46 | INFO | train_inner | epoch 053:     53 / 392 loss=5.911, ppl=60.16, wps=21401.9, ups=0.33, wpb=65029.1, bsz=127, num_updates=20300, lr=0.000221948, gnorm=0.559, loss_scale=8, train_wall=243, gb_free=9.6, wall=61643
2022-03-16 09:46:13 | INFO | train_inner | epoch 053:    153 / 392 loss=5.91, ppl=60.13, wps=24551, ups=0.37, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.553, loss_scale=16, train_wall=243, gb_free=9.6, wall=61910
2022-03-16 09:47:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:48:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:50:45 | INFO | train_inner | epoch 053:    255 / 392 loss=5.923, ppl=60.69, wps=24083.6, ups=0.37, wpb=65532.7, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.546, loss_scale=8, train_wall=248, gb_free=9.6, wall=62182
2022-03-16 09:55:11 | INFO | train_inner | epoch 053:    355 / 392 loss=5.936, ppl=61.21, wps=24623.5, ups=0.38, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.55, loss_scale=16, train_wall=243, gb_free=9.6, wall=62448
2022-03-16 09:56:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:57:23 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 6.141 | ppl 70.59 | wps 36578.7 | wpb 511.9 | bsz 1 | num_updates 20637 | best_loss 6.141
2022-03-16 09:57:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20637 updates
2022-03-16 09:57:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 09:57:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 09:57:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 53 @ 20637 updates, score 6.141) (writing took 2.1653476180508733 seconds)
2022-03-16 09:57:25 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-16 09:57:25 | INFO | train | epoch 053 | loss 5.92 | ppl 60.53 | wps 23586.6 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 20637 | lr 0.000220129 | gnorm 0.553 | loss_scale 16 | train_wall 951 | gb_free 9.6 | wall 62582
KL Stats: Epoch 53 Divergences: Uniform: 4.678197447204466 Unigram: 3.1555950160848996
2022-03-16 09:57:25 | INFO | fairseq.trainer | begin training epoch 54
2022-03-16 09:57:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:00:13 | INFO | train_inner | epoch 054:     63 / 392 loss=5.903, ppl=59.82, wps=21530.4, ups=0.33, wpb=65025.8, bsz=127, num_updates=20700, lr=0.000219793, gnorm=0.552, loss_scale=16, train_wall=241, gb_free=9.6, wall=62750
2022-03-16 10:00:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:01:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:04:44 | INFO | train_inner | epoch 054:    165 / 392 loss=5.906, ppl=59.95, wps=24146.6, ups=0.37, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.553, loss_scale=8, train_wall=247, gb_free=9.6, wall=63022
2022-03-16 10:08:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:09:14 | INFO | train_inner | epoch 054:    266 / 392 loss=5.922, ppl=60.65, wps=24344.3, ups=0.37, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.556, loss_scale=8, train_wall=245, gb_free=9.6, wall=63291
2022-03-16 10:13:41 | INFO | train_inner | epoch 054:    366 / 392 loss=5.929, ppl=60.94, wps=24490, ups=0.37, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.544, loss_scale=8, train_wall=244, gb_free=9.6, wall=63559
2022-03-16 10:14:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:15:24 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 6.135 | ppl 70.29 | wps 36795.6 | wpb 511.9 | bsz 1 | num_updates 21026 | best_loss 6.135
2022-03-16 10:15:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21026 updates
2022-03-16 10:15:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 10:15:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt
2022-03-16 10:15:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_best.pt (epoch 54 @ 21026 updates, score 6.135) (writing took 2.1816066650208086 seconds)
2022-03-16 10:15:26 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-16 10:15:26 | INFO | train | epoch 054 | loss 5.913 | ppl 60.27 | wps 23536.5 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 21026 | lr 0.000218083 | gnorm 0.55 | loss_scale 16 | train_wall 951 | gb_free 9.6 | wall 63663
KL Stats: Epoch 54 Divergences: Uniform: 4.684645884652584 Unigram: 3.1596481347242444
2022-03-16 10:15:26 | INFO | fairseq.trainer | begin training epoch 55
2022-03-16 10:15:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:18:43 | INFO | train_inner | epoch 055:     74 / 392 loss=5.892, ppl=59.39, wps=21523.8, ups=0.33, wpb=65029.1, bsz=127, num_updates=21100, lr=0.0002177, gnorm=0.548, loss_scale=16, train_wall=241, gb_free=9.6, wall=63861
2022-03-16 10:19:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:23:15 | INFO | train_inner | epoch 055:    175 / 392 loss=5.902, ppl=59.79, wps=24162.9, ups=0.37, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.548, loss_scale=8, train_wall=247, gb_free=9.6, wall=64132
2022-03-16 10:27:47 | INFO | train_inner | epoch 055:    275 / 392 loss=5.916, ppl=60.36, wps=24061.9, ups=0.37, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.543, loss_scale=16, train_wall=249, gb_free=9.6, wall=64404
2022-03-16 10:31:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:32:22 | INFO | train_inner | epoch 055:    376 / 392 loss=5.92, ppl=60.54, wps=23789.9, ups=0.36, wpb=65532.7, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.546, loss_scale=16, train_wall=252, gb_free=9.6, wall=64680
2022-03-16 10:33:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:33:41 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 6.139 | ppl 70.45 | wps 35414.8 | wpb 511.9 | bsz 1 | num_updates 21416 | best_loss 6.135
2022-03-16 10:33:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 21416 updates
2022-03-16 10:33:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt
2022-03-16 10:33:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt
2022-03-16 10:33:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt (epoch 55 @ 21416 updates, score 6.139) (writing took 1.2718875820282847 seconds)
2022-03-16 10:33:42 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-16 10:33:42 | INFO | train | epoch 055 | loss 5.907 | ppl 60.01 | wps 23268 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 21416 | lr 0.000216088 | gnorm 0.546 | loss_scale 16 | train_wall 966 | gb_free 9.6 | wall 64760
KL Stats: Epoch 55 Divergences: Uniform: 4.691576822866293 Unigram: 3.1630409771699335
2022-03-16 10:33:42 | INFO | fairseq.trainer | begin training epoch 56
2022-03-16 10:33:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:34:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:37:35 | INFO | train_inner | epoch 056:     85 / 392 loss=5.882, ppl=58.98, wps=20810, ups=0.32, wpb=65029.1, bsz=127, num_updates=21500, lr=0.000215666, gnorm=0.564, loss_scale=8, train_wall=251, gb_free=9.6, wall=64992
2022-03-16 10:42:09 | INFO | train_inner | epoch 056:    185 / 392 loss=5.895, ppl=59.5, wps=23951.5, ups=0.37, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.546, loss_scale=16, train_wall=250, gb_free=9.6, wall=65266
2022-03-16 10:43:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:46:45 | INFO | train_inner | epoch 056:    286 / 392 loss=5.91, ppl=60.14, wps=23745.5, ups=0.36, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.55, loss_scale=8, train_wall=252, gb_free=9.6, wall=65542
2022-03-16 10:51:18 | INFO | train_inner | epoch 056:    386 / 392 loss=5.922, ppl=60.62, wps=23954.4, ups=0.37, wpb=65532.7, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.55, loss_scale=16, train_wall=250, gb_free=9.6, wall=65815
2022-03-16 10:51:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:52:09 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 6.138 | ppl 70.42 | wps 35725.4 | wpb 511.9 | bsz 1 | num_updates 21806 | best_loss 6.135
2022-03-16 10:52:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 21806 updates
2022-03-16 10:52:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt
2022-03-16 10:52:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt
2022-03-16 10:52:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt (epoch 56 @ 21806 updates, score 6.138) (writing took 1.1412499629659578 seconds)
2022-03-16 10:52:10 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-16 10:52:10 | INFO | train | epoch 056 | loss 5.901 | ppl 59.77 | wps 23025.6 | ups 0.35 | wpb 65405.2 | bsz 127.7 | num_updates 21806 | lr 0.000214147 | gnorm 0.554 | loss_scale 16 | train_wall 978 | gb_free 9.6 | wall 65867
KL Stats: Epoch 56 Divergences: Uniform: 4.696948507668416 Unigram: 3.167098574937272
2022-03-16 10:52:10 | INFO | fairseq.trainer | begin training epoch 57
2022-03-16 10:52:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:54:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:56:30 | INFO | train_inner | epoch 057:     95 / 392 loss=5.869, ppl=58.45, wps=20846.3, ups=0.32, wpb=65029.1, bsz=127, num_updates=21900, lr=0.000213687, gnorm=0.563, loss_scale=8, train_wall=251, gb_free=9.6, wall=66127
2022-03-16 11:01:03 | INFO | train_inner | epoch 057:    195 / 392 loss=5.891, ppl=59.34, wps=23985.2, ups=0.37, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.558, loss_scale=16, train_wall=250, gb_free=9.6, wall=66401
2022-03-16 11:02:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 11:05:40 | INFO | train_inner | epoch 057:    296 / 392 loss=5.913, ppl=60.26, wps=23730.5, ups=0.36, wpb=65532.7, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.551, loss_scale=8, train_wall=252, gb_free=9.6, wall=66677
2022-03-16 11:09:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 11:10:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:10:37 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 6.136 | ppl 70.32 | wps 35686.6 | wpb 511.9 | bsz 1 | num_updates 22195 | best_loss 6.135
2022-03-16 11:10:37 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-16 11:10:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 22195 updates
2022-03-16 11:10:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt
2022-03-16 11:10:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt
2022-03-16 11:10:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.01_0.04_0.95/checkpoint_last.pt (epoch 57 @ 22195 updates, score 6.136) (writing took 1.2427364139584824 seconds)
2022-03-16 11:10:39 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-16 11:10:39 | INFO | train | epoch 057 | loss 5.895 | ppl 59.51 | wps 22953.8 | ups 0.35 | wpb 65404.8 | bsz 127.7 | num_updates 22195 | lr 0.000212262 | gnorm 0.555 | loss_scale 8 | train_wall 978 | gb_free 9.6 | wall 66976
2022-03-16 11:10:39 | INFO | fairseq_cli.train | done training in 66975.6 seconds
