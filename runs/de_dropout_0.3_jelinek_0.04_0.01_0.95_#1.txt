Sender: LSF System <lsfadmin@eu-g3-012>
Subject: Job 208722478: <de_dropout_0.3_jelinek_0.04_0.01_0.95_#1> in cluster <euler> Done

Job <de_dropout_0.3_jelinek_0.04_0.01_0.95_#1> was submitted from host <eu-login-24> by user <andriusb> in cluster <euler> at Tue Mar 15 16:18:25 2022
Job was executed on host(s) <eu-g3-012>, in queue <gpu.120h>, as user <andriusb> in cluster <euler> at Tue Mar 15 16:32:50 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar 15 16:32:50 2022
Terminated at Wed Mar 16 13:15:10 2022
Results reported at Wed Mar 16 13:15:10 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/de --save-dir /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.3 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.04,0.01, 0.95)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --patience 3 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   74490.63 sec.
    Max Memory :                                 3971 MB
    Average Memory :                             2999.16 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               16029.00 MB
    Max Swap :                                   229 MB
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   74539 sec.
    Turnaround time :                            75405 sec.

The output (if any) follows:

2022-03-15 16:33:06 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.3, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/de', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.04,0.01, 0.95)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-15 16:33:06 | INFO | fairseq.tasks.language_modeling | dictionary: 34528 types
2022-03-15 16:33:07 | INFO | fairseq.data.data_utils | loaded 45,920 examples from: data-bin/de/train
Calculating frequency stats:
  0%|          | 0/45920 [00:00<?, ?it/s]  0%|          | 57/45920 [00:00<01:22, 554.79it/s]  0%|          | 140/45920 [00:00<01:05, 695.09it/s]  0%|          | 210/45920 [00:00<01:26, 528.66it/s]  1%|          | 286/45920 [00:00<01:15, 601.09it/s]  1%|          | 352/45920 [00:00<01:13, 617.93it/s]  1%|          | 429/45920 [00:00<01:08, 662.28it/s]  1%|          | 500/45920 [00:00<01:07, 675.68it/s]  1%|▏         | 577/45920 [00:00<01:04, 699.96it/s]  1%|▏         | 654/45920 [00:00<01:04, 697.53it/s]  2%|▏         | 725/45920 [00:01<01:25, 530.18it/s]  2%|▏         | 785/45920 [00:01<01:24, 535.07it/s]  2%|▏         | 862/45920 [00:01<01:16, 587.32it/s]  2%|▏         | 927/45920 [00:01<01:14, 602.99it/s]  2%|▏         | 1011/45920 [00:01<01:07, 667.31it/s]  2%|▏         | 1099/45920 [00:01<01:01, 725.56it/s]  3%|▎         | 1174/45920 [00:01<01:01, 728.04it/s]  3%|▎         | 1260/45920 [00:01<00:58, 760.93it/s]  3%|▎         | 1338/45920 [00:02<00:58, 763.74it/s]  3%|▎         | 1419/45920 [00:02<00:57, 772.38it/s]  3%|▎         | 1512/45920 [00:02<00:54, 817.33it/s]  3%|▎         | 1595/45920 [00:02<00:54, 811.01it/s]  4%|▎         | 1677/45920 [00:02<01:03, 697.64it/s]  4%|▍         | 1765/45920 [00:02<00:59, 740.88it/s]  4%|▍         | 1854/45920 [00:02<00:56, 773.93it/s]  4%|▍         | 1934/45920 [00:02<00:57, 763.30it/s]  4%|▍         | 2014/45920 [00:02<00:56, 772.92it/s]  5%|▍         | 2093/45920 [00:03<00:57, 757.68it/s]  5%|▍         | 2170/45920 [00:03<01:00, 723.84it/s]  5%|▍         | 2244/45920 [00:03<01:00, 726.68it/s]  5%|▌         | 2318/45920 [00:03<01:06, 654.90it/s]  5%|▌         | 2390/45920 [00:03<01:04, 671.19it/s]  5%|▌         | 2459/45920 [00:03<01:05, 667.23it/s]  6%|▌         | 2533/45920 [00:03<01:03, 685.93it/s]  6%|▌         | 2603/45920 [00:03<01:04, 672.16it/s]  6%|▌         | 2700/45920 [00:03<00:58, 740.82it/s]  6%|▌         | 2775/45920 [00:04<01:01, 696.16it/s]  6%|▋         | 2870/45920 [00:04<00:56, 765.65it/s]  6%|▋         | 2957/45920 [00:04<00:54, 793.90it/s]  7%|▋         | 3038/45920 [00:04<01:04, 666.67it/s]  7%|▋         | 3110/45920 [00:04<01:02, 679.94it/s]  7%|▋         | 3181/45920 [00:04<01:04, 666.45it/s]  7%|▋         | 3262/45920 [00:04<01:00, 703.75it/s]  7%|▋         | 3335/45920 [00:04<01:00, 707.57it/s]  7%|▋         | 3408/45920 [00:04<00:59, 708.71it/s]  8%|▊         | 3480/45920 [00:05<01:01, 688.95it/s]  8%|▊         | 3550/45920 [00:05<01:01, 686.40it/s]  8%|▊         | 3620/45920 [00:05<01:14, 570.69it/s]  8%|▊         | 3695/45920 [00:05<01:09, 609.29it/s]  8%|▊         | 3767/45920 [00:05<01:06, 638.09it/s]  8%|▊         | 3834/45920 [00:05<01:09, 605.66it/s]  9%|▊         | 3910/45920 [00:05<01:07, 622.46it/s]  9%|▊         | 3974/45920 [00:05<01:12, 578.67it/s]  9%|▉         | 4039/45920 [00:05<01:10, 592.95it/s]  9%|▉         | 4100/45920 [00:06<01:12, 573.15it/s]  9%|▉         | 4159/45920 [00:06<01:15, 556.12it/s]  9%|▉         | 4237/45920 [00:06<01:07, 616.19it/s]  9%|▉         | 4300/45920 [00:06<01:21, 511.98it/s] 10%|▉         | 4372/45920 [00:06<01:13, 562.77it/s] 10%|▉         | 4441/45920 [00:06<01:10, 591.66it/s] 10%|▉         | 4533/45920 [00:06<01:00, 680.10it/s] 10%|█         | 4604/45920 [00:06<01:01, 671.65it/s] 10%|█         | 4674/45920 [00:07<01:07, 611.89it/s] 10%|█         | 4760/45920 [00:07<01:01, 673.93it/s] 11%|█         | 4834/45920 [00:07<00:59, 691.08it/s] 11%|█         | 4909/45920 [00:07<00:57, 707.30it/s] 11%|█         | 4997/45920 [00:07<00:54, 756.29it/s] 11%|█         | 5074/45920 [00:07<01:01, 669.24it/s] 11%|█         | 5152/45920 [00:07<00:58, 698.72it/s] 11%|█▏        | 5225/45920 [00:07<01:05, 623.29it/s] 12%|█▏        | 5291/45920 [00:07<01:04, 628.46it/s] 12%|█▏        | 5372/45920 [00:08<01:00, 675.71it/s] 12%|█▏        | 5463/45920 [00:08<00:55, 732.51it/s] 12%|█▏        | 5538/45920 [00:08<01:03, 636.24it/s] 12%|█▏        | 5605/45920 [00:08<01:04, 622.70it/s] 12%|█▏        | 5670/45920 [00:08<01:06, 606.73it/s] 13%|█▎        | 5741/45920 [00:08<01:04, 621.16it/s] 13%|█▎        | 5805/45920 [00:08<01:06, 599.05it/s] 13%|█▎        | 5892/45920 [00:08<01:00, 666.65it/s] 13%|█▎        | 5960/45920 [00:08<01:04, 621.85it/s] 13%|█▎        | 6026/45920 [00:09<01:04, 621.38it/s] 13%|█▎        | 6089/45920 [00:09<01:07, 590.33it/s] 13%|█▎        | 6149/45920 [00:09<01:07, 591.26it/s] 14%|█▎        | 6218/45920 [00:09<01:06, 596.48it/s] 14%|█▎        | 6297/45920 [00:09<01:01, 648.96it/s] 14%|█▍        | 6380/45920 [00:09<00:56, 697.22it/s] 14%|█▍        | 6463/45920 [00:09<00:53, 733.18it/s] 14%|█▍        | 6537/45920 [00:09<00:55, 709.22it/s] 14%|█▍        | 6609/45920 [00:09<01:02, 629.73it/s] 15%|█▍        | 6674/45920 [00:10<01:03, 621.04it/s] 15%|█▍        | 6749/45920 [00:10<00:59, 655.84it/s] 15%|█▍        | 6833/45920 [00:10<00:55, 702.52it/s] 15%|█▌        | 6905/45920 [00:10<00:57, 674.84it/s] 15%|█▌        | 6993/45920 [00:10<00:53, 731.14it/s] 15%|█▌        | 7068/45920 [00:10<00:52, 736.40it/s] 16%|█▌        | 7163/45920 [00:10<00:48, 797.31it/s] 16%|█▌        | 7244/45920 [00:10<00:55, 690.75it/s] 16%|█▌        | 7317/45920 [00:11<01:00, 639.73it/s] 16%|█▌        | 7384/45920 [00:11<01:01, 629.28it/s] 16%|█▌        | 7458/45920 [00:11<00:58, 657.93it/s] 16%|█▋        | 7526/45920 [00:11<01:00, 635.65it/s] 17%|█▋        | 7591/45920 [00:11<01:12, 530.88it/s] 17%|█▋        | 7648/45920 [00:11<01:32, 415.22it/s] 17%|█▋        | 7721/45920 [00:11<01:21, 469.04it/s] 17%|█▋        | 7774/45920 [00:11<01:20, 474.24it/s] 17%|█▋        | 7855/45920 [00:12<01:08, 554.14it/s] 17%|█▋        | 7944/45920 [00:12<00:59, 633.79it/s] 17%|█▋        | 8015/45920 [00:12<00:59, 640.79it/s] 18%|█▊        | 8083/45920 [00:12<00:58, 646.20it/s] 18%|█▊        | 8166/45920 [00:12<00:54, 696.49it/s] 18%|█▊        | 8238/45920 [00:12<00:57, 657.92it/s] 18%|█▊        | 8312/45920 [00:12<00:55, 679.33it/s] 18%|█▊        | 8397/45920 [00:12<00:51, 721.72it/s] 18%|█▊        | 8471/45920 [00:12<00:57, 645.85it/s] 19%|█▊        | 8538/45920 [00:13<00:59, 625.22it/s] 19%|█▉        | 8625/45920 [00:13<00:54, 688.86it/s] 19%|█▉        | 8696/45920 [00:13<00:56, 663.09it/s] 19%|█▉        | 8764/45920 [00:13<00:58, 638.31it/s] 19%|█▉        | 8843/45920 [00:13<00:54, 678.02it/s] 19%|█▉        | 8912/45920 [00:13<01:00, 613.31it/s] 20%|█▉        | 8978/45920 [00:13<00:59, 623.95it/s] 20%|█▉        | 9064/45920 [00:13<00:53, 683.41it/s] 20%|█▉        | 9134/45920 [00:13<00:56, 656.52it/s] 20%|██        | 9201/45920 [00:14<00:56, 644.22it/s] 20%|██        | 9285/45920 [00:14<00:54, 678.04it/s] 20%|██        | 9367/45920 [00:14<00:50, 716.92it/s] 21%|██        | 9440/45920 [00:14<00:54, 670.36it/s] 21%|██        | 9515/45920 [00:14<00:52, 691.65it/s] 21%|██        | 9586/45920 [00:14<00:59, 610.27it/s] 21%|██        | 9650/45920 [00:14<01:01, 593.88it/s] 21%|██        | 9711/45920 [00:14<01:02, 581.29it/s] 21%|██▏       | 9782/45920 [00:14<01:00, 601.19it/s] 21%|██▏       | 9858/45920 [00:15<00:56, 640.76it/s] 22%|██▏       | 9923/45920 [00:15<01:00, 595.67it/s] 22%|██▏       | 10002/45920 [00:15<00:55, 646.33it/s] 22%|██▏       | 10068/45920 [00:15<00:56, 632.23it/s] 22%|██▏       | 10133/45920 [00:15<01:01, 581.90it/s] 22%|██▏       | 10211/45920 [00:15<00:57, 625.88it/s] 22%|██▏       | 10300/45920 [00:15<00:51, 696.35it/s] 23%|██▎       | 10372/45920 [00:15<00:50, 699.63it/s] 23%|██▎       | 10453/45920 [00:15<00:48, 729.89it/s] 23%|██▎       | 10527/45920 [00:16<00:51, 687.38it/s] 23%|██▎       | 10597/45920 [00:16<00:51, 688.42it/s] 23%|██▎       | 10667/45920 [00:16<00:56, 627.12it/s] 23%|██▎       | 10752/45920 [00:16<00:51, 683.31it/s] 24%|██▎       | 10837/45920 [00:16<00:48, 729.16it/s] 24%|██▍       | 10912/45920 [00:16<00:52, 666.53it/s] 24%|██▍       | 10995/45920 [00:16<00:49, 709.74it/s] 24%|██▍       | 11068/45920 [00:16<00:56, 612.65it/s] 24%|██▍       | 11139/45920 [00:17<00:54, 637.20it/s] 24%|██▍       | 11225/45920 [00:17<00:49, 694.31it/s] 25%|██▍       | 11299/45920 [00:17<00:49, 705.54it/s] 25%|██▍       | 11372/45920 [00:17<00:52, 659.77it/s] 25%|██▍       | 11460/45920 [00:17<00:48, 717.03it/s] 25%|██▌       | 11534/45920 [00:17<00:50, 676.08it/s] 25%|██▌       | 11628/45920 [00:17<00:47, 719.97it/s] 25%|██▌       | 11702/45920 [00:17<00:58, 585.31it/s] 26%|██▌       | 11766/45920 [00:17<00:58, 582.57it/s] 26%|██▌       | 11828/45920 [00:18<00:59, 572.86it/s] 26%|██▌       | 11888/45920 [00:18<01:01, 551.55it/s] 26%|██▌       | 11945/45920 [00:18<01:01, 553.60it/s] 26%|██▌       | 12013/45920 [00:18<00:57, 586.60it/s] 26%|██▋       | 12081/45920 [00:18<00:55, 612.21it/s] 26%|██▋       | 12144/45920 [00:18<00:56, 602.76it/s] 27%|██▋       | 12215/45920 [00:18<00:53, 632.64it/s] 27%|██▋       | 12295/45920 [00:18<00:49, 680.22it/s] 27%|██▋       | 12386/45920 [00:18<00:44, 746.51it/s] 27%|██▋       | 12465/45920 [00:19<00:47, 705.06it/s] 27%|██▋       | 12537/45920 [00:19<00:47, 706.42it/s] 27%|██▋       | 12611/45920 [00:19<00:47, 707.60it/s] 28%|██▊       | 12701/45920 [00:19<00:44, 740.62it/s] 28%|██▊       | 12776/45920 [00:19<00:46, 714.75it/s] 28%|██▊       | 12848/45920 [00:19<00:50, 649.72it/s] 28%|██▊       | 12934/45920 [00:19<00:47, 701.62it/s] 28%|██▊       | 13006/45920 [00:19<00:51, 644.65it/s] 28%|██▊       | 13073/45920 [00:19<00:52, 629.68it/s] 29%|██▊       | 13164/45920 [00:20<00:46, 698.33it/s] 29%|██▉       | 13236/45920 [00:20<00:46, 695.89it/s] 29%|██▉       | 13322/45920 [00:20<00:43, 741.35it/s] 29%|██▉       | 13409/45920 [00:20<00:42, 773.84it/s] 29%|██▉       | 13488/45920 [00:20<00:43, 737.89it/s] 30%|██▉       | 13563/45920 [00:20<00:45, 708.72it/s] 30%|██▉       | 13635/45920 [00:20<00:48, 664.92it/s] 30%|██▉       | 13703/45920 [00:20<00:50, 644.22it/s] 30%|███       | 13783/45920 [00:20<00:46, 684.07it/s] 30%|███       | 13868/45920 [00:21<00:43, 730.16it/s] 30%|███       | 13942/45920 [00:21<00:47, 676.01it/s] 31%|███       | 14011/45920 [00:21<00:50, 633.81it/s] 31%|███       | 14076/45920 [00:21<00:57, 551.04it/s] 31%|███       | 14144/45920 [00:21<00:55, 577.06it/s] 31%|███       | 14204/45920 [00:21<00:55, 572.37it/s] 31%|███       | 14265/45920 [00:21<00:54, 581.18it/s] 31%|███       | 14325/45920 [00:21<00:55, 570.62it/s] 31%|███▏      | 14405/45920 [00:22<00:50, 624.30it/s] 32%|███▏      | 14480/45920 [00:22<00:47, 659.04it/s] 32%|███▏      | 14547/45920 [00:22<00:52, 600.20it/s] 32%|███▏      | 14611/45920 [00:22<00:51, 609.70it/s] 32%|███▏      | 14677/45920 [00:22<00:52, 589.78it/s] 32%|███▏      | 14746/45920 [00:22<00:50, 614.33it/s] 32%|███▏      | 14809/45920 [00:22<00:53, 582.87it/s] 32%|███▏      | 14891/45920 [00:22<00:48, 645.51it/s] 33%|███▎      | 14965/45920 [00:22<00:46, 670.74it/s] 33%|███▎      | 15034/45920 [00:23<00:47, 653.37it/s] 33%|███▎      | 15101/45920 [00:23<00:58, 525.31it/s] 33%|███▎      | 15184/45920 [00:23<00:51, 598.87it/s] 33%|███▎      | 15268/45920 [00:23<00:50, 610.83it/s] 33%|███▎      | 15333/45920 [00:23<00:49, 618.61it/s] 34%|███▎      | 15398/45920 [00:23<00:54, 564.70it/s] 34%|███▎      | 15464/45920 [00:23<00:51, 588.07it/s] 34%|███▍      | 15540/45920 [00:23<00:48, 632.07it/s] 34%|███▍      | 15620/45920 [00:24<00:48, 622.12it/s] 34%|███▍      | 15690/45920 [00:24<00:47, 642.31it/s] 34%|███▍      | 15771/45920 [00:24<00:43, 687.41it/s] 34%|███▍      | 15842/45920 [00:24<00:50, 593.17it/s] 35%|███▍      | 15923/45920 [00:24<00:46, 647.74it/s] 35%|███▍      | 15991/45920 [00:24<00:46, 645.76it/s] 35%|███▌      | 16072/45920 [00:24<00:43, 683.46it/s] 35%|███▌      | 16143/45920 [00:24<00:47, 629.75it/s] 35%|███▌      | 16224/45920 [00:24<00:43, 675.14it/s] 35%|███▌      | 16294/45920 [00:25<00:54, 546.74it/s] 36%|███▌      | 16385/45920 [00:25<00:46, 631.53it/s] 36%|███▌      | 16454/45920 [00:25<00:45, 642.66it/s] 36%|███▌      | 16523/45920 [00:25<00:48, 605.52it/s] 36%|███▌      | 16600/45920 [00:25<00:45, 646.76it/s] 36%|███▋      | 16675/45920 [00:25<00:43, 673.15it/s] 36%|███▋      | 16755/45920 [00:25<00:41, 707.45it/s] 37%|███▋      | 16828/45920 [00:25<00:50, 572.61it/s] 37%|███▋      | 16891/45920 [00:26<00:50, 569.74it/s] 37%|███▋      | 16952/45920 [00:26<00:51, 567.49it/s] 37%|███▋      | 17023/45920 [00:26<00:47, 603.76it/s] 37%|███▋      | 17122/45920 [00:26<00:40, 708.34it/s] 37%|███▋      | 17210/45920 [00:26<00:38, 751.81it/s] 38%|███▊      | 17288/45920 [00:26<00:43, 651.18it/s] 38%|███▊      | 17358/45920 [00:26<00:43, 662.01it/s] 38%|███▊      | 17431/45920 [00:26<00:41, 679.83it/s] 38%|███▊      | 17532/45920 [00:26<00:37, 765.33it/s] 38%|███▊      | 17624/45920 [00:27<00:35, 794.57it/s] 39%|███▊      | 17705/45920 [00:27<00:38, 742.23it/s] 39%|███▊      | 17781/45920 [00:27<00:38, 726.25it/s] 39%|███▉      | 17855/45920 [00:27<00:39, 711.81it/s] 39%|███▉      | 17930/45920 [00:27<00:39, 709.17it/s] 39%|███▉      | 18002/45920 [00:27<00:46, 595.41it/s] 39%|███▉      | 18088/45920 [00:27<00:42, 659.50it/s] 40%|███▉      | 18171/45920 [00:27<00:39, 702.97it/s] 40%|███▉      | 18245/45920 [00:27<00:39, 705.87it/s] 40%|███▉      | 18337/45920 [00:28<00:36, 755.12it/s] 40%|████      | 18415/45920 [00:28<00:36, 753.52it/s] 40%|████      | 18492/45920 [00:28<00:41, 666.18it/s] 40%|████      | 18562/45920 [00:28<00:46, 588.77it/s] 41%|████      | 18624/45920 [00:28<00:56, 479.40it/s] 41%|████      | 18704/45920 [00:28<00:49, 549.37it/s] 41%|████      | 18765/45920 [00:28<00:50, 539.46it/s] 41%|████      | 18857/45920 [00:28<00:42, 632.36it/s] 41%|████      | 18940/45920 [00:29<00:39, 681.60it/s] 41%|████▏     | 19032/45920 [00:29<00:36, 745.14it/s] 42%|████▏     | 19110/45920 [00:29<00:36, 736.49it/s] 42%|████▏     | 19187/45920 [00:29<00:38, 692.59it/s] 42%|████▏     | 19259/45920 [00:29<00:38, 695.60it/s] 42%|████▏     | 19331/45920 [00:29<00:41, 646.32it/s] 42%|████▏     | 19411/45920 [00:29<00:38, 687.12it/s] 42%|████▏     | 19482/45920 [00:29<00:41, 642.30it/s] 43%|████▎     | 19579/45920 [00:29<00:36, 729.77it/s] 43%|████▎     | 19655/45920 [00:30<00:35, 730.07it/s] 43%|████▎     | 19730/45920 [00:30<00:36, 723.58it/s] 43%|████▎     | 19814/45920 [00:30<00:34, 755.68it/s] 43%|████▎     | 19891/45920 [00:30<00:36, 704.75it/s] 43%|████▎     | 19972/45920 [00:30<00:35, 733.51it/s] 44%|████▎     | 20049/45920 [00:30<00:37, 685.53it/s] 44%|████▍     | 20119/45920 [00:30<00:41, 622.76it/s] 44%|████▍     | 20211/45920 [00:30<00:36, 699.16it/s] 44%|████▍     | 20284/45920 [00:31<00:36, 696.58it/s] 44%|████▍     | 20362/45920 [00:31<00:35, 718.74it/s] 45%|████▍     | 20436/45920 [00:31<00:36, 701.57it/s] 45%|████▍     | 20508/45920 [00:31<00:37, 674.57it/s] 45%|████▍     | 20589/45920 [00:31<00:35, 709.48it/s] 45%|████▌     | 20684/45920 [00:31<00:32, 776.70it/s] 45%|████▌     | 20763/45920 [00:31<00:34, 732.11it/s] 45%|████▌     | 20838/45920 [00:31<00:35, 702.35it/s] 46%|████▌     | 20910/45920 [00:31<00:36, 692.65it/s] 46%|████▌     | 20980/45920 [00:31<00:36, 675.26it/s] 46%|████▌     | 21048/45920 [00:32<00:37, 656.28it/s] 46%|████▌     | 21114/45920 [00:32<00:39, 630.83it/s] 46%|████▌     | 21178/45920 [00:32<00:41, 592.30it/s] 46%|████▋     | 21247/45920 [00:32<00:40, 614.71it/s] 46%|████▋     | 21309/45920 [00:32<00:41, 590.06it/s] 47%|████▋     | 21378/45920 [00:32<00:39, 615.92it/s] 47%|████▋     | 21448/45920 [00:32<00:38, 639.29it/s] 47%|████▋     | 21513/45920 [00:32<00:38, 628.68it/s] 47%|████▋     | 21603/45920 [00:32<00:34, 704.14it/s] 47%|████▋     | 21674/45920 [00:33<00:36, 656.72it/s] 47%|████▋     | 21741/45920 [00:33<00:39, 618.09it/s] 47%|████▋     | 21804/45920 [00:33<00:41, 577.03it/s] 48%|████▊     | 21871/45920 [00:33<00:42, 571.85it/s] 48%|████▊     | 21944/45920 [00:33<00:39, 610.53it/s] 48%|████▊     | 22012/45920 [00:33<00:39, 609.75it/s] 48%|████▊     | 22074/45920 [00:33<00:39, 598.92it/s] 48%|████▊     | 22139/45920 [00:33<00:38, 613.00it/s] 48%|████▊     | 22231/45920 [00:33<00:33, 699.13it/s] 49%|████▊     | 22324/45920 [00:34<00:30, 765.01it/s] 49%|████▉     | 22402/45920 [00:34<00:31, 746.18it/s] 49%|████▉     | 22478/45920 [00:34<00:31, 747.36it/s] 49%|████▉     | 22554/45920 [00:34<00:31, 745.69it/s] 49%|████▉     | 22633/45920 [00:34<00:30, 758.32it/s] 49%|████▉     | 22710/45920 [00:34<00:33, 683.35it/s] 50%|████▉     | 22796/45920 [00:34<00:31, 728.16it/s] 50%|████▉     | 22884/45920 [00:34<00:29, 768.32it/s] 50%|█████     | 22963/45920 [00:34<00:30, 753.53it/s] 50%|█████     | 23064/45920 [00:35<00:27, 821.65it/s] 50%|█████     | 23148/45920 [00:35<00:33, 670.41it/s] 51%|█████     | 23221/45920 [00:35<00:34, 658.26it/s] 51%|█████     | 23295/45920 [00:35<00:33, 674.50it/s] 51%|█████     | 23372/45920 [00:35<00:32, 692.99it/s] 51%|█████     | 23444/45920 [00:35<00:32, 699.20it/s] 51%|█████     | 23516/45920 [00:35<00:35, 637.29it/s] 51%|█████▏    | 23582/45920 [00:35<00:42, 524.29it/s] 52%|█████▏    | 23656/45920 [00:36<00:39, 559.70it/s] 52%|█████▏    | 23742/45920 [00:36<00:35, 630.01it/s] 52%|█████▏    | 23809/45920 [00:36<00:36, 608.18it/s] 52%|█████▏    | 23890/45920 [00:36<00:33, 659.68it/s] 52%|█████▏    | 23959/45920 [00:36<00:35, 617.21it/s] 52%|█████▏    | 24041/45920 [00:36<00:32, 668.54it/s] 53%|█████▎    | 24118/45920 [00:36<00:31, 693.00it/s] 53%|█████▎    | 24190/45920 [00:36<00:35, 616.15it/s] 53%|█████▎    | 24255/45920 [00:37<00:38, 562.96it/s] 53%|█████▎    | 24340/45920 [00:37<00:33, 634.82it/s] 53%|█████▎    | 24414/45920 [00:37<00:32, 661.36it/s] 53%|█████▎    | 24501/45920 [00:37<00:29, 717.22it/s] 54%|█████▎    | 24578/45920 [00:37<00:29, 729.64it/s] 54%|█████▎    | 24676/45920 [00:37<00:26, 800.82it/s] 54%|█████▍    | 24758/45920 [00:37<00:27, 773.08it/s] 54%|█████▍    | 24837/45920 [00:37<00:28, 749.29it/s] 54%|█████▍    | 24913/45920 [00:37<00:28, 737.61it/s] 54%|█████▍    | 24988/45920 [00:38<00:31, 660.69it/s] 55%|█████▍    | 25069/45920 [00:38<00:29, 699.58it/s] 55%|█████▍    | 25141/45920 [00:38<00:30, 672.81it/s] 55%|█████▍    | 25210/45920 [00:38<00:30, 668.95it/s] 55%|█████▌    | 25292/45920 [00:38<00:29, 710.77it/s] 55%|█████▌    | 25370/45920 [00:38<00:28, 722.31it/s] 55%|█████▌    | 25455/45920 [00:38<00:27, 757.40it/s] 56%|█████▌    | 25534/45920 [00:38<00:26, 766.66it/s] 56%|█████▌    | 25612/45920 [00:38<00:31, 653.29it/s] 56%|█████▌    | 25681/45920 [00:39<00:30, 661.33it/s] 56%|█████▌    | 25752/45920 [00:39<00:29, 672.70it/s] 56%|█████▋    | 25846/45920 [00:39<00:27, 737.08it/s] 56%|█████▋    | 25922/45920 [00:39<00:27, 736.38it/s] 57%|█████▋    | 26016/45920 [00:39<00:25, 788.95it/s] 57%|█████▋    | 26096/45920 [00:39<00:31, 631.42it/s] 57%|█████▋    | 26165/45920 [00:39<00:31, 624.68it/s] 57%|█████▋    | 26246/45920 [00:39<00:29, 669.13it/s] 57%|█████▋    | 26323/45920 [00:39<00:29, 671.03it/s] 57%|█████▊    | 26404/45920 [00:40<00:27, 707.62it/s] 58%|█████▊    | 26484/45920 [00:40<00:27, 717.30it/s] 58%|█████▊    | 26558/45920 [00:40<00:27, 700.78it/s] 58%|█████▊    | 26641/45920 [00:40<00:26, 735.98it/s] 58%|█████▊    | 26716/45920 [00:40<00:26, 714.31it/s] 58%|█████▊    | 26789/45920 [00:40<00:27, 686.26it/s] 58%|█████▊    | 26859/45920 [00:40<00:27, 682.59it/s] 59%|█████▊    | 26928/45920 [00:40<00:28, 660.35it/s] 59%|█████▉    | 26998/45920 [00:40<00:28, 669.61it/s] 59%|█████▉    | 27086/45920 [00:41<00:25, 728.29it/s] 59%|█████▉    | 27160/45920 [00:41<00:36, 511.66it/s] 59%|█████▉    | 27221/45920 [00:41<00:35, 528.23it/s] 59%|█████▉    | 27293/45920 [00:41<00:32, 573.88it/s] 60%|█████▉    | 27385/45920 [00:41<00:27, 662.13it/s] 60%|█████▉    | 27457/45920 [00:41<00:28, 652.93it/s] 60%|█████▉    | 27527/45920 [00:41<00:28, 648.52it/s] 60%|██████    | 27607/45920 [00:41<00:26, 688.73it/s] 60%|██████    | 27679/45920 [00:42<00:26, 684.59it/s] 60%|██████    | 27750/45920 [00:42<00:27, 670.12it/s] 61%|██████    | 27819/45920 [00:42<00:31, 579.41it/s] 61%|██████    | 27880/45920 [00:42<00:32, 563.48it/s] 61%|██████    | 27939/45920 [00:42<00:34, 525.09it/s] 61%|██████    | 28027/45920 [00:42<00:29, 613.66it/s] 61%|██████    | 28091/45920 [00:42<00:31, 569.84it/s] 61%|██████▏   | 28151/45920 [00:42<00:31, 569.28it/s] 61%|██████▏   | 28225/45920 [00:42<00:29, 608.91it/s] 62%|██████▏   | 28293/45920 [00:43<00:28, 626.46it/s] 62%|██████▏   | 28359/45920 [00:43<00:27, 635.35it/s] 62%|██████▏   | 28439/45920 [00:43<00:25, 674.24it/s] 62%|██████▏   | 28516/45920 [00:43<00:25, 692.61it/s] 62%|██████▏   | 28591/45920 [00:43<00:24, 708.37it/s] 62%|██████▏   | 28663/45920 [00:43<00:28, 615.40it/s] 63%|██████▎   | 28736/45920 [00:43<00:26, 636.78it/s] 63%|██████▎   | 28820/45920 [00:43<00:24, 690.44it/s] 63%|██████▎   | 28893/45920 [00:43<00:24, 687.23it/s] 63%|██████▎   | 28968/45920 [00:44<00:24, 704.59it/s] 63%|██████▎   | 29045/45920 [00:44<00:23, 723.27it/s] 63%|██████▎   | 29119/45920 [00:44<00:23, 709.31it/s] 64%|██████▎   | 29191/45920 [00:44<00:24, 696.37it/s] 64%|██████▎   | 29263/45920 [00:44<00:28, 584.85it/s] 64%|██████▍   | 29340/45920 [00:44<00:26, 630.63it/s] 64%|██████▍   | 29407/45920 [00:44<00:28, 586.16it/s] 64%|██████▍   | 29480/45920 [00:44<00:26, 622.18it/s] 64%|██████▍   | 29549/45920 [00:44<00:25, 640.34it/s] 65%|██████▍   | 29630/45920 [00:45<00:23, 687.39it/s] 65%|██████▍   | 29701/45920 [00:45<00:26, 612.02it/s] 65%|██████▍   | 29773/45920 [00:45<00:25, 636.81it/s] 65%|██████▍   | 29847/45920 [00:45<00:24, 662.77it/s] 65%|██████▌   | 29915/45920 [00:45<00:24, 641.14it/s] 65%|██████▌   | 29993/45920 [00:45<00:23, 666.33it/s] 65%|██████▌   | 30061/45920 [00:45<00:24, 651.77it/s] 66%|██████▌   | 30142/45920 [00:45<00:22, 691.79it/s] 66%|██████▌   | 30212/45920 [00:45<00:24, 645.59it/s] 66%|██████▌   | 30278/45920 [00:46<00:24, 626.74it/s] 66%|██████▌   | 30349/45920 [00:46<00:24, 644.59it/s] 66%|██████▌   | 30417/45920 [00:46<00:23, 653.54it/s] 66%|██████▋   | 30500/45920 [00:46<00:22, 696.66it/s] 67%|██████▋   | 30577/45920 [00:46<00:21, 712.68it/s] 67%|██████▋   | 30657/45920 [00:46<00:20, 737.42it/s] 67%|██████▋   | 30732/45920 [00:46<00:20, 724.85it/s] 67%|██████▋   | 30805/45920 [00:46<00:21, 710.33it/s] 67%|██████▋   | 30895/45920 [00:46<00:19, 764.62it/s] 67%|██████▋   | 30972/45920 [00:47<00:19, 752.81it/s] 68%|██████▊   | 31048/45920 [00:47<00:20, 722.11it/s] 68%|██████▊   | 31134/45920 [00:47<00:19, 757.20it/s] 68%|██████▊   | 31211/45920 [00:47<00:19, 737.51it/s] 68%|██████▊   | 31286/45920 [00:47<00:20, 714.42it/s] 68%|██████▊   | 31373/45920 [00:47<00:19, 757.41it/s] 68%|██████▊   | 31450/45920 [00:47<00:19, 760.89it/s] 69%|██████▊   | 31527/45920 [00:47<00:27, 514.17it/s] 69%|██████▉   | 31609/45920 [00:48<00:24, 580.90it/s] 69%|██████▉   | 31678/45920 [00:48<00:24, 572.48it/s] 69%|██████▉   | 31743/45920 [00:48<00:24, 569.56it/s] 69%|██████▉   | 31813/45920 [00:48<00:23, 602.16it/s] 69%|██████▉   | 31878/45920 [00:48<00:24, 584.68it/s] 70%|██████▉   | 31940/45920 [00:48<00:24, 573.96it/s] 70%|██████▉   | 32016/45920 [00:48<00:22, 618.92it/s] 70%|██████▉   | 32101/45920 [00:48<00:20, 678.64it/s] 70%|███████   | 32173/45920 [00:48<00:20, 687.33it/s] 70%|███████   | 32243/45920 [00:49<00:19, 686.01it/s] 70%|███████   | 32322/45920 [00:49<00:19, 688.94it/s] 71%|███████   | 32395/45920 [00:49<00:19, 699.41it/s] 71%|███████   | 32466/45920 [00:49<00:19, 696.27it/s] 71%|███████   | 32548/45920 [00:49<00:18, 728.53it/s] 71%|███████   | 32623/45920 [00:49<00:18, 734.04it/s] 71%|███████   | 32697/45920 [00:49<00:21, 618.28it/s] 71%|███████▏  | 32766/45920 [00:49<00:20, 635.91it/s] 72%|███████▏  | 32847/45920 [00:49<00:19, 682.48it/s] 72%|███████▏  | 32936/45920 [00:50<00:17, 739.82it/s] 72%|███████▏  | 33012/45920 [00:50<00:18, 714.77it/s] 72%|███████▏  | 33091/45920 [00:50<00:17, 734.16it/s] 72%|███████▏  | 33166/45920 [00:50<00:18, 704.82it/s] 72%|███████▏  | 33246/45920 [00:50<00:17, 731.16it/s] 73%|███████▎  | 33331/45920 [00:50<00:16, 764.70it/s] 73%|███████▎  | 33411/45920 [00:50<00:16, 773.84it/s] 73%|███████▎  | 33490/45920 [00:50<00:15, 778.51it/s] 73%|███████▎  | 33572/45920 [00:50<00:15, 786.52it/s] 73%|███████▎  | 33651/45920 [00:50<00:16, 741.79it/s] 73%|███████▎  | 33726/45920 [00:51<00:16, 728.66it/s] 74%|███████▎  | 33804/45920 [00:51<00:16, 729.16it/s] 74%|███████▍  | 33878/45920 [00:51<00:17, 685.09it/s] 74%|███████▍  | 33953/45920 [00:51<00:17, 701.77it/s] 74%|███████▍  | 34029/45920 [00:51<00:16, 716.13it/s] 74%|███████▍  | 34105/45920 [00:51<00:16, 724.80it/s] 74%|███████▍  | 34178/45920 [00:51<00:18, 638.47it/s] 75%|███████▍  | 34244/45920 [00:51<00:18, 643.53it/s] 75%|███████▍  | 34322/45920 [00:51<00:17, 680.52it/s] 75%|███████▍  | 34404/45920 [00:52<00:16, 715.31it/s] 75%|███████▌  | 34477/45920 [00:52<00:18, 610.12it/s] 75%|███████▌  | 34562/45920 [00:52<00:16, 671.79it/s] 75%|███████▌  | 34644/45920 [00:52<00:15, 710.05it/s] 76%|███████▌  | 34718/45920 [00:52<00:16, 680.92it/s] 76%|███████▌  | 34801/45920 [00:52<00:16, 677.80it/s] 76%|███████▌  | 34871/45920 [00:52<00:16, 679.60it/s] 76%|███████▌  | 34940/45920 [00:52<00:16, 664.69it/s] 76%|███████▌  | 35008/45920 [00:53<00:17, 617.76it/s] 76%|███████▋  | 35071/45920 [00:53<00:18, 591.78it/s] 77%|███████▋  | 35131/45920 [00:53<00:18, 573.87it/s] 77%|███████▋  | 35193/45920 [00:53<00:18, 582.53it/s] 77%|███████▋  | 35269/45920 [00:53<00:17, 623.86it/s] 77%|███████▋  | 35356/45920 [00:53<00:15, 691.62it/s] 77%|███████▋  | 35426/45920 [00:53<00:16, 633.35it/s] 77%|███████▋  | 35502/45920 [00:53<00:15, 666.64it/s] 77%|███████▋  | 35570/45920 [00:53<00:17, 591.37it/s] 78%|███████▊  | 35661/45920 [00:54<00:15, 674.15it/s] 78%|███████▊  | 35736/45920 [00:54<00:15, 661.68it/s] 78%|███████▊  | 35805/45920 [00:54<00:15, 665.45it/s] 78%|███████▊  | 35881/45920 [00:54<00:14, 691.12it/s] 78%|███████▊  | 35955/45920 [00:54<00:14, 704.68it/s] 78%|███████▊  | 36030/45920 [00:54<00:13, 716.40it/s] 79%|███████▊  | 36103/45920 [00:54<00:14, 659.41it/s] 79%|███████▉  | 36180/45920 [00:54<00:14, 688.07it/s] 79%|███████▉  | 36251/45920 [00:54<00:14, 688.81it/s] 79%|███████▉  | 36321/45920 [00:55<00:15, 632.95it/s] 79%|███████▉  | 36389/45920 [00:55<00:15, 620.60it/s] 79%|███████▉  | 36460/45920 [00:55<00:14, 640.69it/s] 80%|███████▉  | 36552/45920 [00:55<00:13, 715.02it/s] 80%|███████▉  | 36625/45920 [00:55<00:14, 642.01it/s] 80%|███████▉  | 36692/45920 [00:55<00:15, 602.03it/s] 80%|████████  | 36771/45920 [00:55<00:14, 648.93it/s] 80%|████████  | 36840/45920 [00:55<00:13, 656.60it/s] 80%|████████  | 36907/45920 [00:55<00:14, 602.43it/s] 81%|████████  | 36969/45920 [00:56<00:14, 597.53it/s] 81%|████████  | 37040/45920 [00:56<00:14, 625.40it/s] 81%|████████  | 37142/45920 [00:56<00:12, 730.64it/s] 81%|████████  | 37217/45920 [00:56<00:11, 726.26it/s] 81%|████████  | 37294/45920 [00:56<00:11, 738.29it/s] 81%|████████▏ | 37369/45920 [00:56<00:12, 697.17it/s] 82%|████████▏ | 37440/45920 [00:56<00:12, 695.63it/s] 82%|████████▏ | 37511/45920 [00:56<00:12, 695.53it/s] 82%|████████▏ | 37581/45920 [00:56<00:12, 645.13it/s] 82%|████████▏ | 37647/45920 [00:57<00:13, 619.90it/s] 82%|████████▏ | 37717/45920 [00:57<00:12, 641.74it/s] 82%|████████▏ | 37807/45920 [00:57<00:11, 713.44it/s] 82%|████████▏ | 37880/45920 [00:57<00:11, 692.37it/s] 83%|████████▎ | 37974/45920 [00:57<00:10, 761.19it/s] 83%|████████▎ | 38052/45920 [00:57<00:13, 595.26it/s] 83%|████████▎ | 38118/45920 [00:57<00:13, 595.63it/s] 83%|████████▎ | 38191/45920 [00:57<00:12, 629.38it/s] 83%|████████▎ | 38258/45920 [00:57<00:12, 619.59it/s] 83%|████████▎ | 38328/45920 [00:58<00:11, 637.02it/s] 84%|████████▎ | 38396/45920 [00:58<00:11, 648.25it/s] 84%|████████▍ | 38475/45920 [00:58<00:10, 685.97it/s] 84%|████████▍ | 38556/45920 [00:58<00:10, 716.03it/s] 84%|████████▍ | 38629/45920 [00:58<00:12, 596.31it/s] 84%|████████▍ | 38693/45920 [00:58<00:12, 583.09it/s] 84%|████████▍ | 38754/45920 [00:58<00:12, 584.63it/s] 85%|████████▍ | 38815/45920 [00:58<00:12, 580.31it/s] 85%|████████▍ | 38878/45920 [00:58<00:11, 590.99it/s] 85%|████████▍ | 38942/45920 [00:59<00:11, 589.15it/s] 85%|████████▍ | 39018/45920 [00:59<00:10, 631.87it/s] 85%|████████▌ | 39082/45920 [00:59<00:10, 633.17it/s] 85%|████████▌ | 39161/45920 [00:59<00:09, 676.65it/s] 85%|████████▌ | 39230/45920 [00:59<00:10, 644.49it/s] 86%|████████▌ | 39296/45920 [00:59<00:11, 570.03it/s] 86%|████████▌ | 39357/45920 [00:59<00:11, 578.86it/s] 86%|████████▌ | 39418/45920 [00:59<00:11, 587.12it/s] 86%|████████▌ | 39502/45920 [00:59<00:10, 620.86it/s] 86%|████████▌ | 39582/45920 [01:00<00:09, 665.54it/s] 86%|████████▋ | 39650/45920 [01:00<00:11, 558.42it/s] 87%|████████▋ | 39729/45920 [01:00<00:10, 615.12it/s] 87%|████████▋ | 39794/45920 [01:00<00:10, 610.95it/s] 87%|████████▋ | 39861/45920 [01:00<00:09, 626.00it/s] 87%|████████▋ | 39953/45920 [01:00<00:08, 705.01it/s] 87%|████████▋ | 40038/45920 [01:00<00:07, 740.60it/s] 87%|████████▋ | 40114/45920 [01:00<00:09, 634.50it/s] 88%|████████▊ | 40181/45920 [01:01<00:09, 635.89it/s] 88%|████████▊ | 40248/45920 [01:01<00:09, 602.02it/s] 88%|████████▊ | 40322/45920 [01:01<00:08, 637.34it/s] 88%|████████▊ | 40388/45920 [01:01<00:08, 626.85it/s] 88%|████████▊ | 40464/45920 [01:01<00:08, 660.12it/s] 88%|████████▊ | 40538/45920 [01:01<00:07, 682.08it/s] 88%|████████▊ | 40608/45920 [01:01<00:07, 673.37it/s] 89%|████████▊ | 40690/45920 [01:01<00:07, 709.43it/s] 89%|████████▉ | 40762/45920 [01:01<00:07, 710.98it/s] 89%|████████▉ | 40834/45920 [01:02<00:07, 658.40it/s] 89%|████████▉ | 40914/45920 [01:02<00:07, 695.30it/s] 89%|████████▉ | 40987/45920 [01:02<00:07, 703.78it/s] 89%|████████▉ | 41064/45920 [01:02<00:06, 712.09it/s] 90%|████████▉ | 41136/45920 [01:02<00:06, 700.34it/s] 90%|████████▉ | 41207/45920 [01:02<00:06, 680.34it/s] 90%|████████▉ | 41276/45920 [01:02<00:07, 623.95it/s] 90%|█████████ | 41340/45920 [01:02<00:07, 607.62it/s] 90%|█████████ | 41415/45920 [01:02<00:07, 639.22it/s] 90%|█████████ | 41484/45920 [01:02<00:06, 652.57it/s] 90%|█████████ | 41550/45920 [01:03<00:07, 566.76it/s] 91%|█████████ | 41640/45920 [01:03<00:06, 651.55it/s] 91%|█████████ | 41723/45920 [01:03<00:06, 695.64it/s] 91%|█████████ | 41810/45920 [01:03<00:05, 743.59it/s] 91%|█████████▏| 41916/45920 [01:03<00:04, 830.84it/s] 91%|█████████▏| 42001/45920 [01:03<00:04, 797.07it/s] 92%|█████████▏| 42083/45920 [01:03<00:04, 795.98it/s] 92%|█████████▏| 42164/45920 [01:03<00:05, 668.16it/s] 92%|█████████▏| 42235/45920 [01:04<00:05, 645.12it/s] 92%|█████████▏| 42311/45920 [01:04<00:05, 670.87it/s] 92%|█████████▏| 42381/45920 [01:04<00:05, 650.85it/s] 92%|█████████▏| 42462/45920 [01:04<00:04, 692.98it/s] 93%|█████████▎| 42549/45920 [01:04<00:04, 739.44it/s] 93%|█████████▎| 42625/45920 [01:04<00:04, 719.12it/s] 93%|█████████▎| 42698/45920 [01:04<00:04, 717.38it/s] 93%|█████████▎| 42771/45920 [01:04<00:04, 663.17it/s] 93%|█████████▎| 42839/45920 [01:04<00:04, 660.52it/s] 93%|█████████▎| 42912/45920 [01:05<00:04, 679.81it/s] 94%|█████████▎| 42981/45920 [01:05<00:04, 654.74it/s] 94%|█████████▎| 43048/45920 [01:05<00:04, 642.64it/s] 94%|█████████▍| 43113/45920 [01:05<00:04, 593.35it/s] 94%|█████████▍| 43182/45920 [01:05<00:04, 619.25it/s] 94%|█████████▍| 43253/45920 [01:05<00:04, 642.43it/s] 94%|█████████▍| 43329/45920 [01:05<00:03, 665.42it/s] 95%|█████████▍| 43403/45920 [01:05<00:03, 676.68it/s] 95%|█████████▍| 43472/45920 [01:05<00:03, 647.75it/s] 95%|█████████▍| 43540/45920 [01:06<00:03, 650.10it/s] 95%|█████████▍| 43606/45920 [01:06<00:03, 619.62it/s] 95%|█████████▌| 43689/45920 [01:06<00:03, 677.48it/s] 95%|█████████▌| 43774/45920 [01:06<00:03, 680.45it/s] 96%|█████████▌| 43863/45920 [01:06<00:02, 737.17it/s] 96%|█████████▌| 43942/45920 [01:06<00:02, 751.34it/s] 96%|█████████▌| 44018/45920 [01:06<00:02, 727.07it/s] 96%|█████████▌| 44105/45920 [01:06<00:02, 767.45it/s] 96%|█████████▌| 44183/45920 [01:06<00:02, 581.67it/s] 96%|█████████▋| 44263/45920 [01:07<00:02, 632.22it/s] 97%|█████████▋| 44338/45920 [01:07<00:02, 657.41it/s] 97%|█████████▋| 44416/45920 [01:07<00:02, 682.65it/s] 97%|█████████▋| 44503/45920 [01:07<00:01, 732.75it/s] 97%|█████████▋| 44580/45920 [01:07<00:01, 730.64it/s] 97%|█████████▋| 44656/45920 [01:07<00:01, 695.50it/s] 97%|█████████▋| 44728/45920 [01:07<00:01, 698.29it/s] 98%|█████████▊| 44800/45920 [01:07<00:01, 620.81it/s] 98%|█████████▊| 44865/45920 [01:07<00:01, 581.02it/s] 98%|█████████▊| 44933/45920 [01:08<00:01, 603.13it/s] 98%|█████████▊| 45002/45920 [01:08<00:01, 621.50it/s] 98%|█████████▊| 45066/45920 [01:08<00:01, 524.88it/s] 98%|█████████▊| 45143/45920 [01:08<00:01, 581.88it/s] 98%|█████████▊| 45223/45920 [01:08<00:01, 636.97it/s] 99%|█████████▊| 45290/45920 [01:08<00:01, 604.00it/s] 99%|█████████▉| 45369/45920 [01:08<00:00, 645.94it/s] 99%|█████████▉| 45456/45920 [01:08<00:00, 706.66it/s] 99%|█████████▉| 45545/45920 [01:09<00:00, 732.97it/s] 99%|█████████▉| 45620/45920 [01:09<00:00, 585.27it/s]100%|█████████▉| 45698/45920 [01:09<00:00, 630.23it/s]100%|█████████▉| 45786/45920 [01:09<00:00, 693.36it/s]100%|█████████▉| 45862/45920 [01:09<00:00, 706.84it/s]100%|██████████| 45920/45920 [01:09<00:00, 659.43it/s]

gathering stats for n=1
  0%|          | 0/45920 [00:00<?, ?it/s]  0%|          | 195/45920 [00:00<00:23, 1939.39it/s]  1%|          | 419/45920 [00:00<00:21, 2112.10it/s]  1%|▏         | 654/45920 [00:00<00:20, 2198.86it/s]  2%|▏         | 874/45920 [00:00<00:22, 1990.48it/s]  2%|▏         | 1117/45920 [00:00<00:20, 2137.60it/s]  3%|▎         | 1385/45920 [00:00<00:19, 2313.53it/s]  4%|▎         | 1635/45920 [00:00<00:18, 2369.06it/s]  4%|▍         | 1889/45920 [00:00<00:18, 2421.98it/s]  5%|▍         | 2133/45920 [00:00<00:18, 2370.21it/s]  5%|▌         | 2372/45920 [00:01<00:18, 2362.03it/s]  6%|▌         | 2609/45920 [00:01<00:18, 2304.46it/s]  6%|▌         | 2856/45920 [00:01<00:18, 2350.83it/s]  7%|▋         | 3092/45920 [00:01<00:18, 2335.20it/s]  7%|▋         | 3326/45920 [00:01<00:18, 2316.10it/s]  8%|▊         | 3558/45920 [00:01<00:18, 2261.50it/s]  8%|▊         | 3785/45920 [00:01<00:19, 2198.47it/s]  9%|▊         | 4006/45920 [00:01<00:19, 2117.88it/s]  9%|▉         | 4219/45920 [00:01<00:20, 2062.01it/s] 10%|▉         | 4426/45920 [00:02<00:20, 2001.03it/s] 10%|█         | 4672/45920 [00:02<00:19, 2125.32it/s] 11%|█         | 4929/45920 [00:02<00:18, 2250.88it/s] 11%|█         | 5157/45920 [00:02<00:18, 2256.74it/s] 12%|█▏        | 5388/45920 [00:02<00:17, 2270.80it/s] 12%|█▏        | 5616/45920 [00:02<00:18, 2201.04it/s] 13%|█▎        | 5838/45920 [00:02<00:18, 2206.48it/s] 13%|█▎        | 6060/45920 [00:02<00:18, 2194.23it/s] 14%|█▎        | 6280/45920 [00:02<00:18, 2172.37it/s] 14%|█▍        | 6537/45920 [00:02<00:17, 2287.16it/s] 15%|█▍        | 6767/45920 [00:03<00:17, 2263.76it/s] 15%|█▌        | 7013/45920 [00:03<00:16, 2320.75it/s] 16%|█▌        | 7265/45920 [00:03<00:16, 2365.87it/s] 16%|█▋        | 7502/45920 [00:03<00:17, 2247.97it/s] 17%|█▋        | 7729/45920 [00:03<00:19, 1985.57it/s] 17%|█▋        | 7994/45920 [00:03<00:17, 2154.52it/s] 18%|█▊        | 8216/45920 [00:03<00:17, 2144.93it/s] 18%|█▊        | 8465/45920 [00:03<00:16, 2241.03it/s] 19%|█▉        | 8693/45920 [00:03<00:16, 2232.21it/s] 19%|█▉        | 8919/45920 [00:04<00:17, 2148.64it/s] 20%|█▉        | 9167/45920 [00:04<00:16, 2239.57it/s] 20%|██        | 9406/45920 [00:04<00:16, 2275.96it/s] 21%|██        | 9636/45920 [00:04<00:16, 2198.26it/s] 21%|██▏       | 9858/45920 [00:04<00:16, 2165.21it/s] 22%|██▏       | 10076/45920 [00:04<00:16, 2148.41it/s] 22%|██▏       | 10294/45920 [00:04<00:16, 2156.66it/s] 23%|██▎       | 10553/45920 [00:04<00:15, 2275.56it/s] 23%|██▎       | 10782/45920 [00:04<00:15, 2277.18it/s] 24%|██▍       | 11018/45920 [00:04<00:15, 2297.91it/s] 24%|██▍       | 11249/45920 [00:05<00:15, 2285.77it/s] 25%|██▌       | 11482/45920 [00:05<00:15, 2294.62it/s] 26%|██▌       | 11712/45920 [00:05<00:15, 2229.30it/s] 26%|██▌       | 11936/45920 [00:05<00:15, 2170.60it/s] 26%|██▋       | 12154/45920 [00:05<00:15, 2165.31it/s] 27%|██▋       | 12414/45920 [00:05<00:14, 2288.78it/s] 28%|██▊       | 12664/45920 [00:05<00:14, 2349.29it/s] 28%|██▊       | 12900/45920 [00:05<00:14, 2306.89it/s] 29%|██▊       | 13141/45920 [00:05<00:14, 2325.77it/s] 29%|██▉       | 13409/45920 [00:05<00:13, 2425.82it/s] 30%|██▉       | 13653/45920 [00:06<00:13, 2361.23it/s] 30%|███       | 13903/45920 [00:06<00:13, 2400.30it/s] 31%|███       | 14144/45920 [00:06<00:14, 2247.04it/s] 31%|███▏      | 14371/45920 [00:06<00:14, 2237.60it/s] 32%|███▏      | 14597/45920 [00:06<00:14, 2178.83it/s] 32%|███▏      | 14817/45920 [00:06<00:14, 2179.54it/s] 33%|███▎      | 15041/45920 [00:06<00:14, 2192.21it/s] 33%|███▎      | 15268/45920 [00:06<00:13, 2193.17it/s] 34%|███▎      | 15488/45920 [00:06<00:13, 2188.67it/s] 34%|███▍      | 15724/45920 [00:07<00:13, 2238.00it/s] 35%|███▍      | 15949/45920 [00:07<00:13, 2215.13it/s] 35%|███▌      | 16181/45920 [00:07<00:13, 2245.13it/s] 36%|███▌      | 16409/45920 [00:07<00:13, 2244.79it/s] 36%|███▌      | 16634/45920 [00:07<00:13, 2242.91it/s] 37%|███▋      | 16859/45920 [00:07<00:13, 2211.71it/s] 37%|███▋      | 17081/45920 [00:07<00:13, 2188.05it/s] 38%|███▊      | 17333/45920 [00:07<00:12, 2278.65it/s] 38%|███▊      | 17617/45920 [00:07<00:11, 2443.95it/s] 39%|███▉      | 17862/45920 [00:07<00:11, 2378.65it/s] 39%|███▉      | 18101/45920 [00:08<00:12, 2311.55it/s] 40%|████      | 18369/45920 [00:08<00:11, 2414.62it/s] 41%|████      | 18612/45920 [00:08<00:12, 2142.87it/s] 41%|████      | 18882/45920 [00:08<00:11, 2290.86it/s] 42%|████▏     | 19138/45920 [00:08<00:11, 2359.05it/s] 42%|████▏     | 19379/45920 [00:08<00:11, 2355.45it/s] 43%|████▎     | 19639/45920 [00:08<00:10, 2408.95it/s] 43%|████▎     | 19883/45920 [00:08<00:10, 2408.11it/s] 44%|████▍     | 20126/45920 [00:08<00:11, 2342.38it/s] 44%|████▍     | 20384/45920 [00:09<00:10, 2407.56it/s] 45%|████▍     | 20627/45920 [00:09<00:10, 2408.56it/s] 45%|████▌     | 20879/45920 [00:09<00:10, 2440.09it/s] 46%|████▌     | 21124/45920 [00:09<00:10, 2335.69it/s] 47%|████▋     | 21359/45920 [00:09<00:10, 2267.91it/s] 47%|████▋     | 21612/45920 [00:09<00:10, 2340.84it/s] 48%|████▊     | 21848/45920 [00:09<00:10, 2253.45it/s] 48%|████▊     | 22075/45920 [00:09<00:10, 2243.38it/s] 49%|████▊     | 22337/45920 [00:09<00:10, 2344.98it/s] 49%|████▉     | 22592/45920 [00:09<00:09, 2404.45it/s] 50%|████▉     | 22846/45920 [00:10<00:09, 2441.99it/s] 50%|█████     | 23104/45920 [00:10<00:09, 2475.77it/s] 51%|█████     | 23353/45920 [00:10<00:09, 2418.04it/s] 51%|█████▏    | 23596/45920 [00:10<00:09, 2242.22it/s] 52%|█████▏    | 23829/45920 [00:10<00:09, 2266.16it/s] 52%|█████▏    | 24065/45920 [00:10<00:09, 2289.97it/s] 53%|█████▎    | 24296/45920 [00:10<00:09, 2226.07it/s] 53%|█████▎    | 24565/45920 [00:10<00:09, 2355.79it/s] 54%|█████▍    | 24827/45920 [00:10<00:08, 2430.84it/s] 55%|█████▍    | 25072/45920 [00:11<00:08, 2400.32it/s] 55%|█████▌    | 25333/45920 [00:11<00:08, 2458.50it/s] 56%|█████▌    | 25586/45920 [00:11<00:08, 2468.85it/s] 56%|█████▋    | 25839/45920 [00:11<00:08, 2486.47it/s] 57%|█████▋    | 26089/45920 [00:11<00:08, 2450.09it/s] 57%|█████▋    | 26335/45920 [00:11<00:08, 2424.54it/s] 58%|█████▊    | 26582/45920 [00:11<00:08, 2402.63it/s] 58%|█████▊    | 26840/45920 [00:11<00:07, 2454.00it/s] 59%|█████▉    | 27086/45920 [00:11<00:07, 2445.76it/s] 60%|█████▉    | 27331/45920 [00:11<00:08, 2245.27it/s] 60%|██████    | 27559/45920 [00:12<00:08, 2253.88it/s] 61%|██████    | 27787/45920 [00:12<00:08, 2260.56it/s] 61%|██████    | 28015/45920 [00:12<00:08, 2157.60it/s] 61%|██████▏   | 28234/45920 [00:12<00:08, 2164.65it/s] 62%|██████▏   | 28481/45920 [00:12<00:07, 2249.84it/s] 63%|██████▎   | 28720/45920 [00:12<00:07, 2287.17it/s] 63%|██████▎   | 28963/45920 [00:12<00:07, 2328.35it/s] 64%|██████▎   | 29203/45920 [00:12<00:07, 2339.66it/s] 64%|██████▍   | 29438/45920 [00:12<00:07, 2229.85it/s] 65%|██████▍   | 29678/45920 [00:13<00:07, 2278.02it/s] 65%|██████▌   | 29907/45920 [00:13<00:07, 2271.36it/s] 66%|██████▌   | 30143/45920 [00:13<00:06, 2296.72it/s] 66%|██████▌   | 30374/45920 [00:13<00:06, 2278.49it/s] 67%|██████▋   | 30635/45920 [00:13<00:06, 2374.90it/s] 67%|██████▋   | 30887/45920 [00:13<00:06, 2417.39it/s] 68%|██████▊   | 31159/45920 [00:13<00:05, 2506.70it/s] 68%|██████▊   | 31411/45920 [00:13<00:05, 2472.38it/s] 69%|██████▉   | 31659/45920 [00:13<00:06, 2247.66it/s] 69%|██████▉   | 31888/45920 [00:13<00:06, 2231.92it/s] 70%|██████▉   | 32132/45920 [00:14<00:06, 2280.01it/s] 71%|███████   | 32388/45920 [00:14<00:05, 2358.09it/s] 71%|███████   | 32637/45920 [00:14<00:05, 2393.39it/s] 72%|███████▏  | 32878/45920 [00:14<00:05, 2390.40it/s] 72%|███████▏  | 33124/45920 [00:14<00:05, 2409.65it/s] 73%|███████▎  | 33386/45920 [00:14<00:05, 2470.48it/s] 73%|███████▎  | 33635/45920 [00:14<00:04, 2475.68it/s] 74%|███████▍  | 33890/45920 [00:14<00:04, 2496.43it/s] 74%|███████▍  | 34140/45920 [00:14<00:04, 2434.97it/s] 75%|███████▍  | 34385/45920 [00:14<00:04, 2399.43it/s] 75%|███████▌  | 34630/45920 [00:15<00:04, 2404.89it/s] 76%|███████▌  | 34880/45920 [00:15<00:04, 2429.32it/s] 76%|███████▋  | 35124/45920 [00:15<00:04, 2205.66it/s] 77%|███████▋  | 35390/45920 [00:15<00:04, 2259.57it/s] 78%|███████▊  | 35643/45920 [00:15<00:04, 2334.39it/s] 78%|███████▊  | 35880/45920 [00:15<00:04, 2339.71it/s] 79%|███████▊  | 36132/45920 [00:15<00:04, 2391.14it/s] 79%|███████▉  | 36373/45920 [00:15<00:04, 2306.60it/s] 80%|███████▉  | 36611/45920 [00:15<00:04, 2323.95it/s] 80%|████████  | 36845/45920 [00:16<00:04, 2247.32it/s] 81%|████████  | 37102/45920 [00:16<00:03, 2331.75it/s] 81%|████████▏ | 37349/45920 [00:16<00:03, 2346.99it/s] 82%|████████▏ | 37591/45920 [00:16<00:03, 2359.95it/s] 82%|████████▏ | 37828/45920 [00:16<00:03, 2361.85it/s] 83%|████████▎ | 38065/45920 [00:16<00:03, 2268.14it/s] 83%|████████▎ | 38293/45920 [00:16<00:03, 2214.90it/s] 84%|████████▍ | 38537/45920 [00:16<00:03, 2275.04it/s] 84%|████████▍ | 38766/45920 [00:16<00:03, 2150.71it/s] 85%|████████▍ | 38988/45920 [00:17<00:03, 2169.81it/s] 85%|████████▌ | 39217/45920 [00:17<00:03, 2192.94it/s] 86%|████████▌ | 39438/45920 [00:17<00:02, 2166.69it/s] 86%|████████▋ | 39656/45920 [00:17<00:02, 2150.71it/s] 87%|████████▋ | 39891/45920 [00:17<00:02, 2208.46it/s] 87%|████████▋ | 40138/45920 [00:17<00:02, 2285.04it/s] 88%|████████▊ | 40368/45920 [00:17<00:02, 2205.04it/s] 88%|████████▊ | 40629/45920 [00:17<00:02, 2316.03it/s] 89%|████████▉ | 40863/45920 [00:17<00:02, 2322.71it/s] 90%|████████▉ | 41112/45920 [00:17<00:02, 2371.30it/s] 90%|█████████ | 41350/45920 [00:18<00:02, 2267.89it/s] 91%|█████████ | 41583/45920 [00:18<00:01, 2285.29it/s] 91%|█████████ | 41873/45920 [00:18<00:01, 2461.91it/s] 92%|█████████▏| 42121/45920 [00:18<00:01, 2452.96it/s] 92%|█████████▏| 42368/45920 [00:18<00:01, 2369.52it/s] 93%|█████████▎| 42629/45920 [00:18<00:01, 2438.20it/s] 93%|█████████▎| 42874/45920 [00:18<00:01, 2424.39it/s] 94%|█████████▍| 43118/45920 [00:18<00:01, 2334.65it/s] 94%|█████████▍| 43362/45920 [00:18<00:01, 2364.35it/s] 95%|█████████▍| 43600/45920 [00:18<00:01, 2290.32it/s] 96%|█████████▌| 43865/45920 [00:19<00:00, 2392.17it/s] 96%|█████████▌| 44122/45920 [00:19<00:00, 2443.49it/s] 97%|█████████▋| 44368/45920 [00:19<00:00, 2310.82it/s] 97%|█████████▋| 44620/45920 [00:19<00:00, 2368.71it/s] 98%|█████████▊| 44859/45920 [00:19<00:00, 2271.31it/s] 98%|█████████▊| 45088/45920 [00:19<00:00, 2184.83it/s] 99%|█████████▊| 45317/45920 [00:19<00:00, 2213.71it/s] 99%|█████████▉| 45580/45920 [00:19<00:00, 2329.30it/s]100%|█████████▉| 45815/45920 [00:19<00:00, 2310.89it/s]100%|██████████| 45920/45920 [00:20<00:00, 2295.29it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 55.25it/s]2022-03-15 16:34:46 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(34528, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=34528, bias=False)
  )
)
2022-03-15 16:34:46 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-15 16:34:46 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-15 16:34:46 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-15 16:34:46 | INFO | fairseq_cli.train | num. shared model params: 36,592,640 (num. trained: 36,592,640)
2022-03-15 16:34:46 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-15 16:34:46 | INFO | fairseq.data.data_utils | loaded 2,294 examples from: data-bin/de/valid
2022-03-15 16:34:46 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-15 16:34:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 16:34:46 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-15 16:34:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-15 16:34:46 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-15 16:34:46 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-15 16:34:46 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-15 16:34:46 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-15 16:34:46 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-15 16:34:46 | INFO | fairseq.data.data_utils | loaded 45,920 examples from: data-bin/de/train
2022-03-15 16:34:47 | INFO | fairseq.trainer | begin training epoch 1
2022-03-15 16:34:47 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-15 16:34:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-15 16:34:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:35:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 16:35:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-15 16:39:34 | INFO | train_inner | epoch 001:    104 / 392 loss=14.73, ppl=27168, wps=24731.9, ups=0.38, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.108, loss_scale=8, train_wall=265, gb_free=9.6, wall=288
2022-03-15 16:43:59 | INFO | train_inner | epoch 001:    204 / 392 loss=13.219, ppl=9537.87, wps=24759.5, ups=0.38, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=0.705, loss_scale=16, train_wall=243, gb_free=9.6, wall=553
2022-03-15 16:48:23 | INFO | train_inner | epoch 001:    304 / 392 loss=12.304, ppl=5057.37, wps=24766.5, ups=0.38, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.446, loss_scale=32, train_wall=243, gb_free=9.6, wall=817
2022-03-15 16:52:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 16:52:49 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.672 | ppl 3262.79 | wps 37498.5 | wpb 511.9 | bsz 1 | num_updates 388
2022-03-15 16:52:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-15 16:52:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 16:52:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 16:52:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 1 @ 388 updates, score 11.672) (writing took 1.873856296762824 seconds)
2022-03-15 16:52:51 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-15 16:52:51 | INFO | train | epoch 001 | loss 13.064 | ppl 8564.33 | wps 23890.7 | ups 0.37 | wpb 65404.5 | bsz 127.7 | num_updates 388 | lr 4.85903e-05 | gnorm 0.928 | loss_scale 64 | train_wall 964 | gb_free 9.6 | wall 1085
KL Stats: Epoch 1 Divergences: Uniform: 0.6811918607592269 Unigram: 0.8978580778934139
2022-03-15 16:52:51 | INFO | fairseq.trainer | begin training epoch 2
2022-03-15 16:52:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 16:53:23 | INFO | train_inner | epoch 002:     12 / 392 loss=11.833, ppl=3648.6, wps=21680.6, ups=0.33, wpb=65022.4, bsz=127, num_updates=400, lr=5.009e-05, gnorm=0.388, loss_scale=64, train_wall=242, gb_free=9.6, wall=1117
2022-03-15 16:56:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 16:57:51 | INFO | train_inner | epoch 002:    113 / 392 loss=11.599, ppl=3101.04, wps=24510.9, ups=0.37, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.375, loss_scale=32, train_wall=245, gb_free=9.6, wall=1384
2022-03-15 17:02:15 | INFO | train_inner | epoch 002:    213 / 392 loss=11.339, ppl=2589.62, wps=24779.7, ups=0.38, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.376, loss_scale=64, train_wall=243, gb_free=9.6, wall=1649
2022-03-15 17:05:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:06:43 | INFO | train_inner | epoch 002:    314 / 392 loss=11.039, ppl=2104.54, wps=24501.6, ups=0.37, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.443, loss_scale=32, train_wall=246, gb_free=9.6, wall=1916
2022-03-15 17:10:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:10:42 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.517 | ppl 1464.91 | wps 37154.2 | wpb 511.9 | bsz 1 | num_updates 778 | best_loss 10.517
2022-03-15 17:10:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 778 updates
2022-03-15 17:10:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 17:10:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 17:10:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 2 @ 778 updates, score 10.517) (writing took 1.7910077441483736 seconds)
2022-03-15 17:10:44 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-15 17:10:44 | INFO | train | epoch 002 | loss 11.23 | ppl 2402.28 | wps 23783.4 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 778 | lr 9.73306e-05 | gnorm 0.404 | loss_scale 32 | train_wall 952 | gb_free 9.6 | wall 2158
KL Stats: Epoch 2 Divergences: Uniform: 1.3668548578558053 Unigram: 0.6105733482741085
2022-03-15 17:10:44 | INFO | fairseq.trainer | begin training epoch 3
2022-03-15 17:10:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:11:42 | INFO | train_inner | epoch 003:     22 / 392 loss=10.752, ppl=1724.63, wps=21728.2, ups=0.33, wpb=65029.1, bsz=127, num_updates=800, lr=0.00010008, gnorm=0.436, loss_scale=32, train_wall=241, gb_free=9.6, wall=2216
2022-03-15 17:12:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:13:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 17:16:12 | INFO | train_inner | epoch 003:    124 / 392 loss=10.503, ppl=1451.19, wps=24247.2, ups=0.37, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.485, loss_scale=16, train_wall=248, gb_free=9.6, wall=2486
2022-03-15 17:20:37 | INFO | train_inner | epoch 003:    224 / 392 loss=10.294, ppl=1255.71, wps=24740.2, ups=0.38, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.524, loss_scale=32, train_wall=243, gb_free=9.6, wall=2751
2022-03-15 17:25:02 | INFO | train_inner | epoch 003:    324 / 392 loss=10.105, ppl=1101.01, wps=24726, ups=0.38, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.548, loss_scale=64, train_wall=243, gb_free=9.6, wall=3016
2022-03-15 17:25:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:28:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:28:35 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.748 | ppl 859.79 | wps 37116.4 | wpb 511.9 | bsz 1 | num_updates 1167 | best_loss 9.748
2022-03-15 17:28:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1167 updates
2022-03-15 17:28:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 17:28:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 17:28:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 3 @ 1167 updates, score 9.748) (writing took 3.114812145009637 seconds)
2022-03-15 17:28:38 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-15 17:28:38 | INFO | train | epoch 003 | loss 10.262 | ppl 1228.14 | wps 23681 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 1167 | lr 0.000145946 | gnorm 0.527 | loss_scale 32 | train_wall 952 | gb_free 9.6 | wall 3232
KL Stats: Epoch 3 Divergences: Uniform: 1.8352986973613012 Unigram: 1.4189245414033793
2022-03-15 17:28:38 | INFO | fairseq.trainer | begin training epoch 4
2022-03-15 17:28:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:30:06 | INFO | train_inner | epoch 004:     33 / 392 loss=9.934, ppl=978.4, wps=21424, ups=0.33, wpb=65025.8, bsz=127, num_updates=1200, lr=0.00015007, gnorm=0.581, loss_scale=32, train_wall=244, gb_free=9.6, wall=3319
2022-03-15 17:34:31 | INFO | train_inner | epoch 004:    133 / 392 loss=9.78, ppl=879.02, wps=24677.6, ups=0.38, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.618, loss_scale=64, train_wall=244, gb_free=9.6, wall=3585
2022-03-15 17:35:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:38:59 | INFO | train_inner | epoch 004:    234 / 392 loss=9.644, ppl=800.12, wps=24472.8, ups=0.37, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.619, loss_scale=32, train_wall=246, gb_free=9.6, wall=3853
2022-03-15 17:42:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:43:27 | INFO | train_inner | epoch 004:    335 / 392 loss=9.516, ppl=732, wps=24505.6, ups=0.37, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.65, loss_scale=32, train_wall=246, gb_free=9.6, wall=4120
2022-03-15 17:45:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 17:46:30 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.17 | ppl 576.09 | wps 37045.4 | wpb 511.9 | bsz 1 | num_updates 1557 | best_loss 9.17
2022-03-15 17:46:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1557 updates
2022-03-15 17:46:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 17:46:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 17:46:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 4 @ 1557 updates, score 9.17) (writing took 1.8553099017590284 seconds)
2022-03-15 17:46:32 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-15 17:46:32 | INFO | train | epoch 004 | loss 9.631 | ppl 793.01 | wps 23750.1 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 1557 | lr 0.000194686 | gnorm 0.635 | loss_scale 32 | train_wall 953 | gb_free 9.6 | wall 4306
KL Stats: Epoch 4 Divergences: Uniform: 2.2174757002994965 Unigram: 1.8511749358091636
2022-03-15 17:46:32 | INFO | fairseq.trainer | begin training epoch 5
2022-03-15 17:46:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 17:48:26 | INFO | train_inner | epoch 005:     43 / 392 loss=9.367, ppl=660.35, wps=21695.1, ups=0.33, wpb=65025.8, bsz=127, num_updates=1600, lr=0.00020006, gnorm=0.672, loss_scale=64, train_wall=242, gb_free=9.6, wall=4420
2022-03-15 17:48:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:52:54 | INFO | train_inner | epoch 005:    144 / 392 loss=9.226, ppl=598.78, wps=24506.2, ups=0.37, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.686, loss_scale=32, train_wall=246, gb_free=9.6, wall=4687
2022-03-15 17:54:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 17:57:21 | INFO | train_inner | epoch 005:    245 / 392 loss=9.094, ppl=546.65, wps=24476.5, ups=0.37, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.694, loss_scale=32, train_wall=246, gb_free=9.6, wall=4955
2022-03-15 18:00:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:01:49 | INFO | train_inner | epoch 005:    346 / 392 loss=8.97, ppl=501.36, wps=24484.3, ups=0.37, wpb=65532.7, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.688, loss_scale=32, train_wall=246, gb_free=9.6, wall=5223
2022-03-15 18:03:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:04:23 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.65 | ppl 401.57 | wps 37138.4 | wpb 511.9 | bsz 1 | num_updates 1946 | best_loss 8.65
2022-03-15 18:04:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1946 updates
2022-03-15 18:04:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 18:04:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 18:04:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 5 @ 1946 updates, score 8.65) (writing took 2.162889168597758 seconds)
2022-03-15 18:04:26 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-15 18:04:26 | INFO | train | epoch 005 | loss 9.095 | ppl 546.66 | wps 23699.8 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 1946 | lr 0.000243301 | gnorm 0.682 | loss_scale 32 | train_wall 952 | gb_free 9.6 | wall 5379
KL Stats: Epoch 5 Divergences: Uniform: 2.504430467199553 Unigram: 2.1384216095457584
2022-03-15 18:04:26 | INFO | fairseq.trainer | begin training epoch 6
2022-03-15 18:04:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:06:49 | INFO | train_inner | epoch 006:     54 / 392 loss=8.828, ppl=454.42, wps=21655.8, ups=0.33, wpb=65029.1, bsz=127, num_updates=2000, lr=0.00025005, gnorm=0.681, loss_scale=64, train_wall=242, gb_free=9.6, wall=5523
2022-03-15 18:07:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:11:17 | INFO | train_inner | epoch 006:    155 / 392 loss=8.709, ppl=418.51, wps=24452.2, ups=0.37, wpb=65532.7, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.69, loss_scale=32, train_wall=246, gb_free=9.6, wall=5791
2022-03-15 18:13:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:15:46 | INFO | train_inner | epoch 006:    256 / 392 loss=8.61, ppl=390.65, wps=24439.5, ups=0.37, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.687, loss_scale=32, train_wall=246, gb_free=9.6, wall=6059
2022-03-15 18:19:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:20:18 | INFO | train_inner | epoch 006:    357 / 392 loss=8.516, ppl=366.04, wps=24019.1, ups=0.37, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.657, loss_scale=32, train_wall=251, gb_free=9.6, wall=6332
2022-03-15 18:21:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:22:26 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.258 | ppl 306.05 | wps 36796.9 | wpb 511.9 | bsz 1 | num_updates 2335 | best_loss 8.258
2022-03-15 18:22:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2335 updates
2022-03-15 18:22:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 18:22:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 18:22:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 6 @ 2335 updates, score 8.258) (writing took 1.8541100984439254 seconds)
2022-03-15 18:22:28 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-15 18:22:28 | INFO | train | epoch 006 | loss 8.623 | ppl 394.36 | wps 23510.9 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 2335 | lr 0.000291917 | gnorm 0.677 | loss_scale 32 | train_wall 960 | gb_free 9.6 | wall 6462
KL Stats: Epoch 6 Divergences: Uniform: 2.7732354261762424 Unigram: 2.375586081758134
2022-03-15 18:22:28 | INFO | fairseq.trainer | begin training epoch 7
2022-03-15 18:22:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:25:22 | INFO | train_inner | epoch 007:     65 / 392 loss=8.409, ppl=340, wps=21396, ups=0.33, wpb=65029.1, bsz=127, num_updates=2400, lr=0.00030004, gnorm=0.668, loss_scale=32, train_wall=245, gb_free=9.6, wall=6636
2022-03-15 18:25:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:28:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:29:52 | INFO | train_inner | epoch 007:    167 / 392 loss=8.315, ppl=318.5, wps=24261.9, ups=0.37, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.644, loss_scale=16, train_wall=248, gb_free=9.6, wall=6906
2022-03-15 18:34:15 | INFO | train_inner | epoch 007:    267 / 392 loss=8.244, ppl=303.24, wps=25002.9, ups=0.38, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.646, loss_scale=16, train_wall=241, gb_free=9.6, wall=7168
2022-03-15 18:35:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:38:39 | INFO | train_inner | epoch 007:    368 / 392 loss=8.169, ppl=287.8, wps=24756.2, ups=0.38, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.629, loss_scale=16, train_wall=243, gb_free=9.6, wall=7433
2022-03-15 18:39:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:40:14 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.966 | ppl 249.98 | wps 37826.3 | wpb 511.9 | bsz 1 | num_updates 2724 | best_loss 7.966
2022-03-15 18:40:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2724 updates
2022-03-15 18:40:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 18:40:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 18:40:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 7 @ 2724 updates, score 7.966) (writing took 2.2764490442350507 seconds)
2022-03-15 18:40:17 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-15 18:40:17 | INFO | train | epoch 007 | loss 8.26 | ppl 306.48 | wps 23804.4 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 2724 | lr 0.000340532 | gnorm 0.645 | loss_scale 16 | train_wall 948 | gb_free 9.6 | wall 7530
KL Stats: Epoch 7 Divergences: Uniform: 2.978397089623997 Unigram: 2.557672400310504
2022-03-15 18:40:17 | INFO | fairseq.trainer | begin training epoch 8
2022-03-15 18:40:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 18:43:36 | INFO | train_inner | epoch 008:     76 / 392 loss=8.067, ppl=268.09, wps=21892.9, ups=0.34, wpb=65025.8, bsz=127, num_updates=2800, lr=0.00035003, gnorm=0.632, loss_scale=32, train_wall=239, gb_free=9.6, wall=7730
2022-03-15 18:46:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-15 18:48:01 | INFO | train_inner | epoch 008:    177 / 392 loss=8.014, ppl=258.42, wps=24733.2, ups=0.38, wpb=65532.7, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.616, loss_scale=32, train_wall=243, gb_free=9.6, wall=7995
2022-03-15 18:50:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 18:52:26 | INFO | train_inner | epoch 008:    278 / 392 loss=7.949, ppl=247.05, wps=24747.5, ups=0.38, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.603, loss_scale=16, train_wall=243, gb_free=9.6, wall=8260
2022-03-15 18:56:48 | INFO | train_inner | epoch 008:    378 / 392 loss=7.901, ppl=238.98, wps=24994.7, ups=0.38, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.607, loss_scale=32, train_wall=241, gb_free=9.6, wall=8522
2022-03-15 18:57:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 18:57:57 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.707 | ppl 208.98 | wps 37777.2 | wpb 511.9 | bsz 1 | num_updates 3114 | best_loss 7.707
2022-03-15 18:57:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3114 updates
2022-03-15 18:57:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 18:57:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 18:57:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 8 @ 3114 updates, score 7.707) (writing took 1.9895324567332864 seconds)
2022-03-15 18:57:59 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-15 18:57:59 | INFO | train | epoch 008 | loss 7.969 | ppl 250.6 | wps 24008.1 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 3114 | lr 0.000389272 | gnorm 0.612 | loss_scale 32 | train_wall 943 | gb_free 9.6 | wall 8593
KL Stats: Epoch 8 Divergences: Uniform: 3.121553705010958 Unigram: 2.68615508231048
2022-03-15 18:57:59 | INFO | fairseq.trainer | begin training epoch 9
2022-03-15 18:57:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:00:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:01:47 | INFO | train_inner | epoch 009:     87 / 392 loss=7.784, ppl=220.36, wps=21749, ups=0.33, wpb=65029.1, bsz=127, num_updates=3200, lr=0.00040002, gnorm=0.609, loss_scale=16, train_wall=241, gb_free=9.6, wall=8821
2022-03-15 19:06:09 | INFO | train_inner | epoch 009:    187 / 392 loss=7.751, ppl=215.39, wps=25012.1, ups=0.38, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.6, loss_scale=32, train_wall=241, gb_free=9.6, wall=9083
2022-03-15 19:07:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:10:34 | INFO | train_inner | epoch 009:    288 / 392 loss=7.701, ppl=208.04, wps=24779.7, ups=0.38, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.595, loss_scale=16, train_wall=243, gb_free=9.6, wall=9347
2022-03-15 19:13:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:14:59 | INFO | train_inner | epoch 009:    389 / 392 loss=7.641, ppl=199.62, wps=24696.4, ups=0.38, wpb=65532.7, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.595, loss_scale=16, train_wall=244, gb_free=9.6, wall=9613
2022-03-15 19:15:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:15:39 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.451 | ppl 174.98 | wps 37845.7 | wpb 511.9 | bsz 1 | num_updates 3503 | best_loss 7.451
2022-03-15 19:15:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3503 updates
2022-03-15 19:15:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 19:15:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 19:15:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 9 @ 3503 updates, score 7.451) (writing took 1.9244630187749863 seconds)
2022-03-15 19:15:41 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-15 19:15:41 | INFO | train | epoch 009 | loss 7.714 | ppl 209.95 | wps 23959.7 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 3503 | lr 0.000437887 | gnorm 0.601 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 9655
KL Stats: Epoch 9 Divergences: Uniform: 3.2272361029114793 Unigram: 2.7886206406172684
2022-03-15 19:15:41 | INFO | fairseq.trainer | begin training epoch 10
2022-03-15 19:15:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:19:56 | INFO | train_inner | epoch 010:     97 / 392 loss=7.525, ppl=184.15, wps=21885.9, ups=0.34, wpb=65029.1, bsz=127, num_updates=3600, lr=0.00045001, gnorm=0.599, loss_scale=32, train_wall=240, gb_free=9.6, wall=9910
2022-03-15 19:21:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:24:21 | INFO | train_inner | epoch 010:    198 / 392 loss=7.496, ppl=180.57, wps=24709.6, ups=0.38, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.598, loss_scale=16, train_wall=244, gb_free=9.6, wall=10175
2022-03-15 19:28:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:28:47 | INFO | train_inner | epoch 010:    299 / 392 loss=7.455, ppl=175.52, wps=24695.1, ups=0.38, wpb=65532.7, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.601, loss_scale=16, train_wall=244, gb_free=9.6, wall=10441
2022-03-15 19:32:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:33:23 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.237 | ppl 150.83 | wps 37822.1 | wpb 511.9 | bsz 1 | num_updates 3893 | best_loss 7.237
2022-03-15 19:33:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3893 updates
2022-03-15 19:33:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 19:33:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 19:33:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 10 @ 3893 updates, score 7.237) (writing took 1.9041117960587144 seconds)
2022-03-15 19:33:25 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-15 19:33:25 | INFO | train | epoch 010 | loss 7.474 | ppl 177.78 | wps 23969.7 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 3893 | lr 0.000486628 | gnorm 0.601 | loss_scale 16 | train_wall 944 | gb_free 9.6 | wall 10719
KL Stats: Epoch 10 Divergences: Uniform: 3.3229187468930643 Unigram: 2.8783424610196993
2022-03-15 19:33:25 | INFO | fairseq.trainer | begin training epoch 11
2022-03-15 19:33:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:33:44 | INFO | train_inner | epoch 011:      7 / 392 loss=7.414, ppl=170.55, wps=21901.9, ups=0.34, wpb=65029.1, bsz=127, num_updates=3900, lr=0.000487503, gnorm=0.614, loss_scale=16, train_wall=240, gb_free=9.6, wall=10737
2022-03-15 19:34:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:38:09 | INFO | train_inner | epoch 011:    108 / 392 loss=7.31, ppl=158.66, wps=24689.5, ups=0.38, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.59, loss_scale=16, train_wall=244, gb_free=9.6, wall=11003
2022-03-15 19:40:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:42:35 | INFO | train_inner | epoch 011:    209 / 392 loss=7.284, ppl=155.84, wps=24663.7, ups=0.38, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.598, loss_scale=16, train_wall=244, gb_free=9.6, wall=11269
2022-03-15 19:46:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:47:01 | INFO | train_inner | epoch 011:    310 / 392 loss=7.263, ppl=153.56, wps=24674.6, ups=0.38, wpb=65532.7, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.571, loss_scale=16, train_wall=244, gb_free=9.6, wall=11534
2022-03-15 19:50:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 19:51:09 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.085 | ppl 135.8 | wps 37723.3 | wpb 511.9 | bsz 1 | num_updates 4282 | best_loss 7.085
2022-03-15 19:51:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4282 updates
2022-03-15 19:51:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 19:51:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 19:51:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 11 @ 4282 updates, score 7.085) (writing took 1.8973919013515115 seconds)
2022-03-15 19:51:11 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-15 19:51:11 | INFO | train | epoch 011 | loss 7.274 | ppl 154.78 | wps 23881.1 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 4282 | lr 0.000483255 | gnorm 0.587 | loss_scale 16 | train_wall 945 | gb_free 9.6 | wall 11784
KL Stats: Epoch 11 Divergences: Uniform: 3.391623893062629 Unigram: 2.945353664964076
2022-03-15 19:51:11 | INFO | fairseq.trainer | begin training epoch 12
2022-03-15 19:51:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 19:51:58 | INFO | train_inner | epoch 012:     18 / 392 loss=7.214, ppl=148.49, wps=21856.5, ups=0.34, wpb=65029.1, bsz=127, num_updates=4300, lr=0.000482243, gnorm=0.588, loss_scale=16, train_wall=240, gb_free=9.6, wall=11832
2022-03-15 19:53:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 19:56:24 | INFO | train_inner | epoch 012:    119 / 392 loss=7.137, ppl=140.77, wps=24667.7, ups=0.38, wpb=65532.7, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.564, loss_scale=16, train_wall=244, gb_free=9.6, wall=12097
2022-03-15 19:59:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:00:50 | INFO | train_inner | epoch 012:    220 / 392 loss=7.124, ppl=139.47, wps=24645.7, ups=0.38, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.565, loss_scale=16, train_wall=244, gb_free=9.6, wall=12363
2022-03-15 20:05:13 | INFO | train_inner | epoch 012:    320 / 392 loss=7.097, ppl=136.94, wps=24909.1, ups=0.38, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.558, loss_scale=32, train_wall=241, gb_free=9.6, wall=12626
2022-03-15 20:05:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:08:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:08:54 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.958 | ppl 124.34 | wps 37770.1 | wpb 511.9 | bsz 1 | num_updates 4671 | best_loss 6.958
2022-03-15 20:08:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4671 updates
2022-03-15 20:08:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 20:08:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 20:08:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 12 @ 4671 updates, score 6.958) (writing took 1.8012019470334053 seconds)
2022-03-15 20:08:56 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-15 20:08:56 | INFO | train | epoch 012 | loss 7.115 | ppl 138.61 | wps 23876.1 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 4671 | lr 0.000462695 | gnorm 0.566 | loss_scale 16 | train_wall 945 | gb_free 9.6 | wall 12850
KL Stats: Epoch 12 Divergences: Uniform: 3.4485015904647454 Unigram: 3.002591217250541
2022-03-15 20:08:56 | INFO | fairseq.trainer | begin training epoch 13
2022-03-15 20:08:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:10:13 | INFO | train_inner | epoch 013:     29 / 392 loss=7.06, ppl=133.42, wps=21659.8, ups=0.33, wpb=65029.1, bsz=127, num_updates=4700, lr=0.000461266, gnorm=0.566, loss_scale=16, train_wall=242, gb_free=9.6, wall=12927
2022-03-15 20:14:37 | INFO | train_inner | epoch 013:    129 / 392 loss=7.01, ppl=128.91, wps=24822.8, ups=0.38, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.554, loss_scale=32, train_wall=242, gb_free=9.6, wall=13191
2022-03-15 20:15:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:19:03 | INFO | train_inner | epoch 013:    230 / 392 loss=6.992, ppl=127.32, wps=24588.3, ups=0.38, wpb=65532.7, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.552, loss_scale=16, train_wall=245, gb_free=9.6, wall=13457
2022-03-15 20:21:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:23:31 | INFO | train_inner | epoch 013:    331 / 392 loss=6.989, ppl=127.05, wps=24515.9, ups=0.37, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.551, loss_scale=16, train_wall=245, gb_free=9.6, wall=13724
2022-03-15 20:26:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:26:44 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.879 | ppl 117.69 | wps 37762.9 | wpb 511.9 | bsz 1 | num_updates 5061 | best_loss 6.879
2022-03-15 20:26:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5061 updates
2022-03-15 20:26:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 20:26:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 20:26:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 13 @ 5061 updates, score 6.879) (writing took 1.8062000405043364 seconds)
2022-03-15 20:26:46 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-15 20:26:46 | INFO | train | epoch 013 | loss 6.993 | ppl 127.4 | wps 23850.1 | ups 0.36 | wpb 65405.2 | bsz 127.7 | num_updates 5061 | lr 0.00044451 | gnorm 0.552 | loss_scale 16 | train_wall 949 | gb_free 9.6 | wall 13919
KL Stats: Epoch 13 Divergences: Uniform: 3.4902355241337806 Unigram: 3.047398909837127
2022-03-15 20:26:46 | INFO | fairseq.trainer | begin training epoch 14
2022-03-15 20:26:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:27:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:28:31 | INFO | train_inner | epoch 014:     40 / 392 loss=6.939, ppl=122.69, wps=21645.6, ups=0.33, wpb=65025.8, bsz=127, num_updates=5100, lr=0.000442807, gnorm=0.555, loss_scale=16, train_wall=243, gb_free=9.6, wall=14025
2022-03-15 20:32:55 | INFO | train_inner | epoch 014:    140 / 392 loss=6.888, ppl=118.44, wps=24883.5, ups=0.38, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.538, loss_scale=16, train_wall=242, gb_free=9.6, wall=14288
2022-03-15 20:34:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:37:20 | INFO | train_inner | epoch 014:    241 / 392 loss=6.902, ppl=119.63, wps=24646.3, ups=0.38, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.545, loss_scale=16, train_wall=244, gb_free=9.6, wall=14554
2022-03-15 20:39:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:41:46 | INFO | train_inner | epoch 014:    342 / 392 loss=6.9, ppl=119.46, wps=24640.3, ups=0.38, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.539, loss_scale=16, train_wall=244, gb_free=9.6, wall=14820
2022-03-15 20:43:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 20:44:30 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.802 | ppl 111.61 | wps 37824 | wpb 511.9 | bsz 1 | num_updates 5450 | best_loss 6.802
2022-03-15 20:44:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5450 updates
2022-03-15 20:44:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 20:44:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 20:44:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 14 @ 5450 updates, score 6.802) (writing took 1.874969488941133 seconds)
2022-03-15 20:44:32 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-15 20:44:32 | INFO | train | epoch 014 | loss 6.895 | ppl 119.04 | wps 23863.2 | ups 0.36 | wpb 65404.8 | bsz 127.7 | num_updates 5450 | lr 0.000428353 | gnorm 0.545 | loss_scale 16 | train_wall 946 | gb_free 9.6 | wall 14986
KL Stats: Epoch 14 Divergences: Uniform: 3.5193557928674783 Unigram: 3.0806837588213685
2022-03-15 20:44:32 | INFO | fairseq.trainer | begin training epoch 15
2022-03-15 20:44:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 20:46:44 | INFO | train_inner | epoch 015:     50 / 392 loss=6.849, ppl=115.25, wps=21890.6, ups=0.34, wpb=65029.1, bsz=127, num_updates=5500, lr=0.000426401, gnorm=0.548, loss_scale=32, train_wall=240, gb_free=9.6, wall=15117
2022-03-15 20:46:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:51:09 | INFO | train_inner | epoch 015:    151 / 392 loss=6.821, ppl=113.06, wps=24675.8, ups=0.38, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.535, loss_scale=16, train_wall=244, gb_free=9.6, wall=15383
2022-03-15 20:52:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 20:55:35 | INFO | train_inner | epoch 015:    252 / 392 loss=6.82, ppl=112.97, wps=24684.6, ups=0.38, wpb=65532.7, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.531, loss_scale=16, train_wall=244, gb_free=9.6, wall=15648
2022-03-15 20:58:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:00:00 | INFO | train_inner | epoch 015:    353 / 392 loss=6.814, ppl=112.49, wps=24693.9, ups=0.38, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.536, loss_scale=16, train_wall=244, gb_free=9.6, wall=15914
2022-03-15 21:01:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:02:14 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.755 | ppl 108.04 | wps 37922.2 | wpb 511.9 | bsz 1 | num_updates 5839 | best_loss 6.755
2022-03-15 21:02:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5839 updates
2022-03-15 21:02:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 21:02:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 21:02:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 15 @ 5839 updates, score 6.755) (writing took 2.1279587550088763 seconds)
2022-03-15 21:02:17 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-15 21:02:17 | INFO | train | epoch 015 | loss 6.815 | ppl 112.63 | wps 23897.2 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 5839 | lr 0.000413838 | gnorm 0.536 | loss_scale 16 | train_wall 944 | gb_free 9.6 | wall 16050
KL Stats: Epoch 15 Divergences: Uniform: 3.551356391187555 Unigram: 3.118620292637298
2022-03-15 21:02:17 | INFO | fairseq.trainer | begin training epoch 16
2022-03-15 21:02:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:04:57 | INFO | train_inner | epoch 016:     61 / 392 loss=6.765, ppl=108.74, wps=21898.3, ups=0.34, wpb=65029.1, bsz=127, num_updates=5900, lr=0.000411693, gnorm=0.541, loss_scale=16, train_wall=239, gb_free=9.6, wall=16211
2022-03-15 21:05:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:09:22 | INFO | train_inner | epoch 016:    162 / 392 loss=6.749, ppl=107.6, wps=24760.8, ups=0.38, wpb=65532.7, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.533, loss_scale=16, train_wall=243, gb_free=9.6, wall=16475
2022-03-15 21:12:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:13:46 | INFO | train_inner | epoch 016:    263 / 392 loss=6.752, ppl=107.81, wps=24787.1, ups=0.38, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.544, loss_scale=16, train_wall=243, gb_free=9.6, wall=16740
2022-03-15 21:18:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:18:10 | INFO | train_inner | epoch 016:    364 / 392 loss=6.743, ppl=107.14, wps=24779.2, ups=0.38, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.536, loss_scale=16, train_wall=243, gb_free=9.6, wall=17004
2022-03-15 21:19:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:19:56 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.709 | ppl 104.63 | wps 37878.2 | wpb 511.9 | bsz 1 | num_updates 6228 | best_loss 6.709
2022-03-15 21:19:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6228 updates
2022-03-15 21:19:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 21:19:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 21:19:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 16 @ 6228 updates, score 6.709) (writing took 1.920951534062624 seconds)
2022-03-15 21:19:58 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-15 21:19:58 | INFO | train | epoch 016 | loss 6.748 | ppl 107.5 | wps 23971.5 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 6228 | lr 0.000400706 | gnorm 0.537 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 17112
KL Stats: Epoch 16 Divergences: Uniform: 3.5755468205099272 Unigram: 3.1447668294516284
2022-03-15 21:19:58 | INFO | fairseq.trainer | begin training epoch 17
2022-03-15 21:19:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:23:07 | INFO | train_inner | epoch 017:     72 / 392 loss=6.702, ppl=104.12, wps=21931.4, ups=0.34, wpb=65025.8, bsz=127, num_updates=6300, lr=0.00039841, gnorm=0.529, loss_scale=16, train_wall=239, gb_free=9.6, wall=17301
2022-03-15 21:24:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:27:32 | INFO | train_inner | epoch 017:    173 / 392 loss=6.69, ppl=103.22, wps=24754.7, ups=0.38, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.532, loss_scale=16, train_wall=243, gb_free=9.6, wall=17565
2022-03-15 21:31:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:31:56 | INFO | train_inner | epoch 017:    274 / 392 loss=6.692, ppl=103.37, wps=24752.6, ups=0.38, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.532, loss_scale=16, train_wall=243, gb_free=9.6, wall=17830
2022-03-15 21:36:19 | INFO | train_inner | epoch 017:    374 / 392 loss=6.694, ppl=103.54, wps=24999.9, ups=0.38, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.528, loss_scale=16, train_wall=241, gb_free=9.6, wall=18092
2022-03-15 21:37:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:37:38 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.658 | ppl 100.96 | wps 38074.5 | wpb 511.9 | bsz 1 | num_updates 6618 | best_loss 6.658
2022-03-15 21:37:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6618 updates
2022-03-15 21:37:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 21:37:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 21:37:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 17 @ 6618 updates, score 6.658) (writing took 1.9337154440581799 seconds)
2022-03-15 21:37:40 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-15 21:37:40 | INFO | train | epoch 017 | loss 6.689 | ppl 103.21 | wps 24023.5 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 6618 | lr 0.00038872 | gnorm 0.53 | loss_scale 32 | train_wall 942 | gb_free 9.6 | wall 18173
KL Stats: Epoch 17 Divergences: Uniform: 3.5956897526661593 Unigram: 3.1666927809561187
2022-03-15 21:37:40 | INFO | fairseq.trainer | begin training epoch 18
2022-03-15 21:37:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:37:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:41:18 | INFO | train_inner | epoch 018:     83 / 392 loss=6.631, ppl=99.15, wps=21756, ups=0.33, wpb=65029.1, bsz=127, num_updates=6700, lr=0.000386334, gnorm=0.539, loss_scale=16, train_wall=241, gb_free=9.6, wall=18391
2022-03-15 21:44:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:45:42 | INFO | train_inner | epoch 018:    184 / 392 loss=6.639, ppl=99.66, wps=24767.3, ups=0.38, wpb=65532.7, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.524, loss_scale=16, train_wall=243, gb_free=9.6, wall=18656
2022-03-15 21:49:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:50:07 | INFO | train_inner | epoch 018:    285 / 392 loss=6.644, ppl=100.02, wps=24762.9, ups=0.38, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.527, loss_scale=16, train_wall=243, gb_free=9.6, wall=18920
2022-03-15 21:54:29 | INFO | train_inner | epoch 018:    385 / 392 loss=6.647, ppl=100.21, wps=25007.6, ups=0.38, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.528, loss_scale=16, train_wall=241, gb_free=9.6, wall=19183
2022-03-15 21:54:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 21:55:19 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.632 | ppl 99.19 | wps 37921.6 | wpb 511.9 | bsz 1 | num_updates 7007 | best_loss 6.632
2022-03-15 21:55:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7007 updates
2022-03-15 21:55:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 21:55:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 21:55:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 18 @ 7007 updates, score 6.632) (writing took 1.855473479256034 seconds)
2022-03-15 21:55:21 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-15 21:55:21 | INFO | train | epoch 018 | loss 6.638 | ppl 99.62 | wps 23971.7 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 7007 | lr 0.000377776 | gnorm 0.53 | loss_scale 16 | train_wall 941 | gb_free 9.6 | wall 19235
KL Stats: Epoch 18 Divergences: Uniform: 3.611996689010918 Unigram: 3.1865307316564686
2022-03-15 21:55:21 | INFO | fairseq.trainer | begin training epoch 19
2022-03-15 21:55:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 21:57:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 21:59:28 | INFO | train_inner | epoch 019:     94 / 392 loss=6.576, ppl=95.43, wps=21752.5, ups=0.33, wpb=65029.1, bsz=127, num_updates=7100, lr=0.000375293, gnorm=0.53, loss_scale=16, train_wall=241, gb_free=9.6, wall=19482
2022-03-15 22:02:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:03:53 | INFO | train_inner | epoch 019:    195 / 392 loss=6.594, ppl=96.6, wps=24747.2, ups=0.38, wpb=65532.7, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.521, loss_scale=16, train_wall=243, gb_free=9.6, wall=19746
2022-03-15 22:08:15 | INFO | train_inner | epoch 019:    295 / 392 loss=6.601, ppl=97.1, wps=25000.4, ups=0.38, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.524, loss_scale=16, train_wall=240, gb_free=9.6, wall=20008
2022-03-15 22:08:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:12:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:13:01 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.599 | ppl 96.92 | wps 37939 | wpb 511.9 | bsz 1 | num_updates 7396 | best_loss 6.599
2022-03-15 22:13:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7396 updates
2022-03-15 22:13:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 22:13:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 22:13:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 19 @ 7396 updates, score 6.599) (writing took 1.910062505863607 seconds)
2022-03-15 22:13:03 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-15 22:13:03 | INFO | train | epoch 019 | loss 6.593 | ppl 96.53 | wps 23959 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 7396 | lr 0.000367707 | gnorm 0.525 | loss_scale 16 | train_wall 941 | gb_free 9.6 | wall 20297
KL Stats: Epoch 19 Divergences: Uniform: 3.6298980411802244 Unigram: 3.2071812650475042
2022-03-15 22:13:03 | INFO | fairseq.trainer | begin training epoch 20
2022-03-15 22:13:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:13:14 | INFO | train_inner | epoch 020:      4 / 392 loss=6.598, ppl=96.89, wps=21758.2, ups=0.33, wpb=65029.1, bsz=127, num_updates=7400, lr=0.000367607, gnorm=0.53, loss_scale=16, train_wall=241, gb_free=9.6, wall=20307
2022-03-15 22:14:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:17:39 | INFO | train_inner | epoch 020:    105 / 392 loss=6.547, ppl=93.5, wps=24740.9, ups=0.38, wpb=65532.7, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.528, loss_scale=16, train_wall=243, gb_free=9.6, wall=20572
2022-03-15 22:20:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:22:03 | INFO | train_inner | epoch 020:    206 / 392 loss=6.551, ppl=93.8, wps=24750.8, ups=0.38, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.531, loss_scale=16, train_wall=243, gb_free=9.6, wall=20837
2022-03-15 22:26:25 | INFO | train_inner | epoch 020:    306 / 392 loss=6.561, ppl=94.39, wps=25013.1, ups=0.38, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.524, loss_scale=32, train_wall=241, gb_free=9.6, wall=21099
2022-03-15 22:26:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:30:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:30:43 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.572 | ppl 95.13 | wps 38013.4 | wpb 511.9 | bsz 1 | num_updates 7785 | best_loss 6.572
2022-03-15 22:30:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7785 updates
2022-03-15 22:30:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 22:30:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 22:30:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 20 @ 7785 updates, score 6.572) (writing took 1.8474428644403815 seconds)
2022-03-15 22:30:45 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-15 22:30:45 | INFO | train | epoch 020 | loss 6.554 | ppl 93.96 | wps 23968.7 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 7785 | lr 0.000358402 | gnorm 0.528 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 21358
KL Stats: Epoch 20 Divergences: Uniform: 3.6460577536360264 Unigram: 3.223163173681112
2022-03-15 22:30:45 | INFO | fairseq.trainer | begin training epoch 21
2022-03-15 22:30:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:31:24 | INFO | train_inner | epoch 021:     15 / 392 loss=6.55, ppl=93.73, wps=21776, ups=0.33, wpb=65029.1, bsz=127, num_updates=7800, lr=0.000358057, gnorm=0.536, loss_scale=16, train_wall=241, gb_free=9.6, wall=21398
2022-03-15 22:32:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:35:49 | INFO | train_inner | epoch 021:    116 / 392 loss=6.508, ppl=91.04, wps=24740.4, ups=0.38, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.523, loss_scale=16, train_wall=243, gb_free=9.6, wall=21663
2022-03-15 22:38:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:40:14 | INFO | train_inner | epoch 021:    217 / 392 loss=6.508, ppl=91.02, wps=24752.2, ups=0.38, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.53, loss_scale=16, train_wall=243, gb_free=9.6, wall=21927
2022-03-15 22:44:36 | INFO | train_inner | epoch 021:    317 / 392 loss=6.525, ppl=92.09, wps=24989.3, ups=0.38, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.533, loss_scale=32, train_wall=241, gb_free=9.6, wall=22190
2022-03-15 22:44:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:47:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 22:48:24 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.542 | ppl 93.19 | wps 37926.4 | wpb 511.9 | bsz 1 | num_updates 8174 | best_loss 6.542
2022-03-15 22:48:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8174 updates
2022-03-15 22:48:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 22:48:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 22:48:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 21 @ 8174 updates, score 6.542) (writing took 1.9152898453176022 seconds)
2022-03-15 22:48:26 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-15 22:48:26 | INFO | train | epoch 021 | loss 6.518 | ppl 91.63 | wps 23959.4 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 8174 | lr 0.00034977 | gnorm 0.532 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 22420
KL Stats: Epoch 21 Divergences: Uniform: 3.6600908466493776 Unigram: 3.2380024462177346
2022-03-15 22:48:26 | INFO | fairseq.trainer | begin training epoch 22
2022-03-15 22:48:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 22:49:35 | INFO | train_inner | epoch 022:     26 / 392 loss=6.513, ppl=91.36, wps=21754.7, ups=0.33, wpb=65025.8, bsz=127, num_updates=8200, lr=0.000349215, gnorm=0.534, loss_scale=16, train_wall=241, gb_free=9.6, wall=22488
2022-03-15 22:51:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:54:00 | INFO | train_inner | epoch 022:    127 / 392 loss=6.47, ppl=88.66, wps=24744.5, ups=0.38, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.52, loss_scale=16, train_wall=243, gb_free=9.6, wall=22753
2022-03-15 22:57:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 22:58:24 | INFO | train_inner | epoch 022:    228 / 392 loss=6.493, ppl=90.06, wps=24762.6, ups=0.38, wpb=65532.7, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.527, loss_scale=16, train_wall=243, gb_free=9.6, wall=23018
2022-03-15 23:02:46 | INFO | train_inner | epoch 022:    328 / 392 loss=6.493, ppl=90.04, wps=25009.6, ups=0.38, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.534, loss_scale=16, train_wall=241, gb_free=9.6, wall=23280
2022-03-15 23:03:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:05:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:06:06 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.52 | ppl 91.76 | wps 37871.3 | wpb 511.9 | bsz 1 | num_updates 8563 | best_loss 6.52
2022-03-15 23:06:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8563 updates
2022-03-15 23:06:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 23:06:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 23:06:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 22 @ 8563 updates, score 6.52) (writing took 1.8698061937466264 seconds)
2022-03-15 23:06:08 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-15 23:06:08 | INFO | train | epoch 022 | loss 6.484 | ppl 89.54 | wps 23968.2 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 8563 | lr 0.000341733 | gnorm 0.527 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 23482
KL Stats: Epoch 22 Divergences: Uniform: 3.673907907945063 Unigram: 3.25384168623343
2022-03-15 23:06:08 | INFO | fairseq.trainer | begin training epoch 23
2022-03-15 23:06:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:07:45 | INFO | train_inner | epoch 023:     37 / 392 loss=6.468, ppl=88.55, wps=21767.5, ups=0.33, wpb=65029.1, bsz=127, num_updates=8600, lr=0.000340997, gnorm=0.532, loss_scale=16, train_wall=241, gb_free=9.6, wall=23579
2022-03-15 23:09:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:12:10 | INFO | train_inner | epoch 023:    138 / 392 loss=6.443, ppl=87.01, wps=24752.9, ups=0.38, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.527, loss_scale=16, train_wall=243, gb_free=9.6, wall=23843
2022-03-15 23:15:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:16:35 | INFO | train_inner | epoch 023:    239 / 392 loss=6.456, ppl=87.79, wps=24750, ups=0.38, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.528, loss_scale=16, train_wall=243, gb_free=9.6, wall=24108
2022-03-15 23:20:56 | INFO | train_inner | epoch 023:    339 / 392 loss=6.466, ppl=88.39, wps=25039.3, ups=0.38, wpb=65532.7, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.531, loss_scale=16, train_wall=240, gb_free=9.6, wall=24370
2022-03-15 23:21:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:23:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:23:47 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.5 | ppl 90.5 | wps 38077.8 | wpb 511.9 | bsz 1 | num_updates 8952 | best_loss 6.5
2022-03-15 23:23:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 8952 updates
2022-03-15 23:23:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 23:23:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 23:23:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 23 @ 8952 updates, score 6.5) (writing took 2.004857610911131 seconds)
2022-03-15 23:23:49 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-15 23:23:49 | INFO | train | epoch 023 | loss 6.454 | ppl 87.68 | wps 23979.2 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 8952 | lr 0.000334226 | gnorm 0.531 | loss_scale 16 | train_wall 941 | gb_free 9.6 | wall 24543
KL Stats: Epoch 23 Divergences: Uniform: 3.684531736503882 Unigram: 3.265698078632601
2022-03-15 23:23:49 | INFO | fairseq.trainer | begin training epoch 24
2022-03-15 23:23:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:25:55 | INFO | train_inner | epoch 024:     48 / 392 loss=6.443, ppl=87, wps=21791.9, ups=0.34, wpb=65029.1, bsz=127, num_updates=9000, lr=0.000333333, gnorm=0.539, loss_scale=16, train_wall=241, gb_free=9.6, wall=24668
2022-03-15 23:28:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:30:19 | INFO | train_inner | epoch 024:    149 / 392 loss=6.415, ppl=85.33, wps=24801.7, ups=0.38, wpb=65532.7, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.527, loss_scale=16, train_wall=243, gb_free=9.6, wall=24933
2022-03-15 23:34:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:34:43 | INFO | train_inner | epoch 024:    250 / 392 loss=6.434, ppl=86.49, wps=24802.8, ups=0.38, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.525, loss_scale=16, train_wall=243, gb_free=9.6, wall=25197
2022-03-15 23:37:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-15 23:39:07 | INFO | train_inner | epoch 024:    351 / 392 loss=6.436, ppl=86.6, wps=24810.5, ups=0.38, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.533, loss_scale=8, train_wall=243, gb_free=9.6, wall=25461
2022-03-15 23:40:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:41:26 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.489 | ppl 89.8 | wps 38088.2 | wpb 511.9 | bsz 1 | num_updates 9341 | best_loss 6.489
2022-03-15 23:41:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9341 updates
2022-03-15 23:41:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 23:41:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 23:41:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 24 @ 9341 updates, score 6.489) (writing took 1.8629977758973837 seconds)
2022-03-15 23:41:28 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-15 23:41:28 | INFO | train | epoch 024 | loss 6.427 | ppl 86.06 | wps 24015.5 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 9341 | lr 0.000327192 | gnorm 0.531 | loss_scale 8 | train_wall 940 | gb_free 9.6 | wall 25602
KL Stats: Epoch 24 Divergences: Uniform: 3.697703679943403 Unigram: 3.2802620947854106
2022-03-15 23:41:28 | INFO | fairseq.trainer | begin training epoch 25
2022-03-15 23:41:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-15 23:44:03 | INFO | train_inner | epoch 025:     59 / 392 loss=6.399, ppl=84.38, wps=22014, ups=0.34, wpb=65025.8, bsz=127, num_updates=9400, lr=0.000326164, gnorm=0.53, loss_scale=16, train_wall=238, gb_free=9.6, wall=25756
2022-03-15 23:48:25 | INFO | train_inner | epoch 025:    159 / 392 loss=6.396, ppl=84.24, wps=25027.7, ups=0.38, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.526, loss_scale=16, train_wall=240, gb_free=9.6, wall=26018
2022-03-15 23:49:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:52:49 | INFO | train_inner | epoch 025:    260 / 392 loss=6.41, ppl=85.05, wps=24758.4, ups=0.38, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.53, loss_scale=16, train_wall=243, gb_free=9.6, wall=26283
2022-03-15 23:55:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-15 23:57:14 | INFO | train_inner | epoch 025:    361 / 392 loss=6.411, ppl=85.07, wps=24767, ups=0.38, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.527, loss_scale=16, train_wall=243, gb_free=9.6, wall=26548
2022-03-15 23:58:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-15 23:59:07 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.473 | ppl 88.84 | wps 37902.6 | wpb 511.9 | bsz 1 | num_updates 9731 | best_loss 6.473
2022-03-15 23:59:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9731 updates
2022-03-15 23:59:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 23:59:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-15 23:59:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 25 @ 9731 updates, score 6.473) (writing took 1.905580154620111 seconds)
2022-03-15 23:59:09 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-15 23:59:09 | INFO | train | epoch 025 | loss 6.402 | ppl 84.56 | wps 24050.5 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 9731 | lr 0.000320569 | gnorm 0.527 | loss_scale 16 | train_wall 941 | gb_free 9.6 | wall 26663
KL Stats: Epoch 25 Divergences: Uniform: 3.708056834643466 Unigram: 3.291129488935413
2022-03-15 23:59:09 | INFO | fairseq.trainer | begin training epoch 26
2022-03-15 23:59:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:01:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:02:13 | INFO | train_inner | epoch 026:     70 / 392 loss=6.375, ppl=83, wps=21767.2, ups=0.33, wpb=65029.1, bsz=127, num_updates=9800, lr=0.000319438, gnorm=0.532, loss_scale=16, train_wall=241, gb_free=9.6, wall=26846
2022-03-16 00:06:35 | INFO | train_inner | epoch 026:    170 / 392 loss=6.374, ppl=82.95, wps=25009, ups=0.38, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.53, loss_scale=16, train_wall=241, gb_free=9.6, wall=27108
2022-03-16 00:08:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:10:59 | INFO | train_inner | epoch 026:    271 / 392 loss=6.381, ppl=83.36, wps=24749.8, ups=0.38, wpb=65532.7, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.529, loss_scale=16, train_wall=243, gb_free=9.6, wall=27373
2022-03-16 00:13:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 00:15:24 | INFO | train_inner | epoch 026:    372 / 392 loss=6.392, ppl=83.97, wps=24751.7, ups=0.38, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.531, loss_scale=8, train_wall=243, gb_free=9.6, wall=27638
2022-03-16 00:16:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:16:49 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.454 | ppl 87.65 | wps 37923.5 | wpb 511.9 | bsz 1 | num_updates 10120 | best_loss 6.454
2022-03-16 00:16:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10120 updates
2022-03-16 00:16:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 00:16:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 00:16:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 26 @ 10120 updates, score 6.454) (writing took 2.0870597679167986 seconds)
2022-03-16 00:16:51 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-16 00:16:51 | INFO | train | epoch 026 | loss 6.378 | ppl 83.18 | wps 23958.5 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 10120 | lr 0.000314347 | gnorm 0.532 | loss_scale 8 | train_wall 942 | gb_free 9.6 | wall 27725
KL Stats: Epoch 26 Divergences: Uniform: 3.718216786412868 Unigram: 3.3013904484530423
2022-03-16 00:16:51 | INFO | fairseq.trainer | begin training epoch 27
2022-03-16 00:16:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:20:21 | INFO | train_inner | epoch 027:     80 / 392 loss=6.346, ppl=81.37, wps=21927, ups=0.34, wpb=65025.8, bsz=127, num_updates=10200, lr=0.000313112, gnorm=0.545, loss_scale=16, train_wall=239, gb_free=9.6, wall=27934
2022-03-16 00:24:43 | INFO | train_inner | epoch 027:    180 / 392 loss=6.347, ppl=81.39, wps=24993.1, ups=0.38, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.53, loss_scale=16, train_wall=241, gb_free=9.6, wall=28197
2022-03-16 00:24:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 00:29:08 | INFO | train_inner | epoch 027:    281 / 392 loss=6.365, ppl=82.45, wps=24772, ups=0.38, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.524, loss_scale=8, train_wall=243, gb_free=9.6, wall=28461
2022-03-16 00:33:29 | INFO | train_inner | epoch 027:    381 / 392 loss=6.375, ppl=83, wps=25025.6, ups=0.38, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.544, loss_scale=16, train_wall=241, gb_free=9.6, wall=28723
2022-03-16 00:33:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:34:30 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.435 | ppl 86.55 | wps 37929.1 | wpb 511.9 | bsz 1 | num_updates 10511 | best_loss 6.435
2022-03-16 00:34:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10511 updates
2022-03-16 00:34:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 00:34:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 00:34:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 27 @ 10511 updates, score 6.435) (writing took 1.9619827484712005 seconds)
2022-03-16 00:34:32 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-16 00:34:32 | INFO | train | epoch 027 | loss 6.356 | ppl 81.93 | wps 24095.1 | ups 0.37 | wpb 65405.5 | bsz 127.7 | num_updates 10511 | lr 0.000308445 | gnorm 0.535 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 28786
KL Stats: Epoch 27 Divergences: Uniform: 3.7260914587396026 Unigram: 3.312146704191247
2022-03-16 00:34:32 | INFO | fairseq.trainer | begin training epoch 28
2022-03-16 00:34:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:37:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:38:28 | INFO | train_inner | epoch 028:     90 / 392 loss=6.319, ppl=79.83, wps=21772.2, ups=0.33, wpb=65025.8, bsz=127, num_updates=10600, lr=0.000307148, gnorm=0.535, loss_scale=16, train_wall=241, gb_free=9.6, wall=29022
2022-03-16 00:42:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 00:42:53 | INFO | train_inner | epoch 028:    191 / 392 loss=6.328, ppl=80.33, wps=24748.6, ups=0.38, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.531, loss_scale=8, train_wall=243, gb_free=9.6, wall=29287
2022-03-16 00:47:15 | INFO | train_inner | epoch 028:    291 / 392 loss=6.346, ppl=81.37, wps=24992.6, ups=0.38, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.527, loss_scale=8, train_wall=241, gb_free=9.6, wall=29549
2022-03-16 00:51:37 | INFO | train_inner | epoch 028:    391 / 392 loss=6.352, ppl=81.67, wps=25005.3, ups=0.38, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.531, loss_scale=16, train_wall=241, gb_free=9.6, wall=29811
2022-03-16 00:51:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 00:52:12 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.43 | ppl 86.2 | wps 37929.3 | wpb 511.9 | bsz 1 | num_updates 10901 | best_loss 6.43
2022-03-16 00:52:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10901 updates
2022-03-16 00:52:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 00:52:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 00:52:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 28 @ 10901 updates, score 6.43) (writing took 1.9998517595231533 seconds)
2022-03-16 00:52:14 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-16 00:52:14 | INFO | train | epoch 028 | loss 6.336 | ppl 80.76 | wps 24026.7 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 10901 | lr 0.000302877 | gnorm 0.531 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 29848
KL Stats: Epoch 28 Divergences: Uniform: 3.7372757709160256 Unigram: 3.321616859274546
2022-03-16 00:52:14 | INFO | fairseq.trainer | begin training epoch 29
2022-03-16 00:52:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 00:54:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 00:56:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 00:56:39 | INFO | train_inner | epoch 029:    101 / 392 loss=6.289, ppl=78.18, wps=21560.3, ups=0.33, wpb=65029.1, bsz=127, num_updates=11000, lr=0.000301511, gnorm=0.537, loss_scale=8, train_wall=244, gb_free=9.6, wall=30113
2022-03-16 01:01:01 | INFO | train_inner | epoch 029:    201 / 392 loss=6.312, ppl=79.43, wps=24994.1, ups=0.38, wpb=65532.7, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.531, loss_scale=8, train_wall=241, gb_free=9.6, wall=30375
2022-03-16 01:05:23 | INFO | train_inner | epoch 029:    301 / 392 loss=6.324, ppl=80.12, wps=24999.3, ups=0.38, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.526, loss_scale=16, train_wall=241, gb_free=9.6, wall=30637
2022-03-16 01:07:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:09:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:09:54 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.408 | ppl 84.94 | wps 37934.4 | wpb 511.9 | bsz 1 | num_updates 11290 | best_loss 6.408
2022-03-16 01:09:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11290 updates
2022-03-16 01:09:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 01:09:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 01:09:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 29 @ 11290 updates, score 6.408) (writing took 1.8963650548830628 seconds)
2022-03-16 01:09:56 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-16 01:09:56 | INFO | train | epoch 029 | loss 6.316 | ppl 79.67 | wps 23963.9 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 11290 | lr 0.000297614 | gnorm 0.533 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 30909
KL Stats: Epoch 29 Divergences: Uniform: 3.743978726516401 Unigram: 3.330104852987519
2022-03-16 01:09:56 | INFO | fairseq.trainer | begin training epoch 30
2022-03-16 01:09:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:10:22 | INFO | train_inner | epoch 030:     10 / 392 loss=6.334, ppl=80.69, wps=21765.5, ups=0.33, wpb=65029.1, bsz=127, num_updates=11300, lr=0.000297482, gnorm=0.542, loss_scale=16, train_wall=241, gb_free=9.6, wall=30936
2022-03-16 01:13:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:14:47 | INFO | train_inner | epoch 030:    111 / 392 loss=6.282, ppl=77.81, wps=24745.5, ups=0.38, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.53, loss_scale=16, train_wall=243, gb_free=9.6, wall=31200
2022-03-16 01:19:09 | INFO | train_inner | epoch 030:    211 / 392 loss=6.294, ppl=78.48, wps=24989, ups=0.38, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.534, loss_scale=16, train_wall=241, gb_free=9.6, wall=31463
2022-03-16 01:19:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:23:34 | INFO | train_inner | epoch 030:    312 / 392 loss=6.311, ppl=79.38, wps=24747.7, ups=0.38, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.536, loss_scale=16, train_wall=243, gb_free=9.6, wall=31728
2022-03-16 01:25:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:27:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:27:36 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.396 | ppl 84.22 | wps 37966.9 | wpb 511.9 | bsz 1 | num_updates 11679 | best_loss 6.396
2022-03-16 01:27:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11679 updates
2022-03-16 01:27:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 01:27:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 01:27:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 30 @ 11679 updates, score 6.396) (writing took 1.8528052316978574 seconds)
2022-03-16 01:27:37 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-16 01:27:37 | INFO | train | epoch 030 | loss 6.298 | ppl 78.69 | wps 23960.8 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 11679 | lr 0.000292615 | gnorm 0.534 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 31971
KL Stats: Epoch 30 Divergences: Uniform: 3.7520888753102293 Unigram: 3.3378079039572803
2022-03-16 01:27:38 | INFO | fairseq.trainer | begin training epoch 31
2022-03-16 01:27:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:28:33 | INFO | train_inner | epoch 031:     21 / 392 loss=6.296, ppl=78.55, wps=21762.4, ups=0.33, wpb=65025.8, bsz=127, num_updates=11700, lr=0.000292353, gnorm=0.535, loss_scale=16, train_wall=241, gb_free=9.6, wall=32026
2022-03-16 01:31:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:32:57 | INFO | train_inner | epoch 031:    122 / 392 loss=6.257, ppl=76.49, wps=24754.9, ups=0.38, wpb=65532.7, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.532, loss_scale=16, train_wall=243, gb_free=9.6, wall=32291
2022-03-16 01:37:20 | INFO | train_inner | epoch 031:    222 / 392 loss=6.295, ppl=78.5, wps=25000, ups=0.38, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.53, loss_scale=32, train_wall=241, gb_free=9.6, wall=32553
2022-03-16 01:37:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:40:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 01:41:47 | INFO | train_inner | epoch 031:    324 / 392 loss=6.287, ppl=78.1, wps=24519.3, ups=0.37, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.538, loss_scale=8, train_wall=245, gb_free=9.6, wall=32821
2022-03-16 01:44:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 01:45:17 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.386 | ppl 83.65 | wps 37959.9 | wpb 511.9 | bsz 1 | num_updates 12068 | best_loss 6.386
2022-03-16 01:45:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12068 updates
2022-03-16 01:45:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 01:45:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 01:45:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 31 @ 12068 updates, score 6.386) (writing took 1.9840090414509177 seconds)
2022-03-16 01:45:19 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-16 01:45:19 | INFO | train | epoch 031 | loss 6.281 | ppl 77.76 | wps 23964.7 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 12068 | lr 0.000287861 | gnorm 0.535 | loss_scale 8 | train_wall 942 | gb_free 9.6 | wall 33033
KL Stats: Epoch 31 Divergences: Uniform: 3.7593214724065684 Unigram: 3.3469368893950007
2022-03-16 01:45:19 | INFO | fairseq.trainer | begin training epoch 32
2022-03-16 01:45:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 01:46:43 | INFO | train_inner | epoch 032:     32 / 392 loss=6.284, ppl=77.9, wps=21949, ups=0.34, wpb=65029.1, bsz=127, num_updates=12100, lr=0.00028748, gnorm=0.544, loss_scale=16, train_wall=239, gb_free=9.6, wall=33117
2022-03-16 01:51:05 | INFO | train_inner | epoch 032:    132 / 392 loss=6.252, ppl=76.19, wps=25005.7, ups=0.38, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.531, loss_scale=16, train_wall=241, gb_free=9.6, wall=33379
2022-03-16 01:52:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 01:53:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 01:55:33 | INFO | train_inner | epoch 032:    234 / 392 loss=6.264, ppl=76.87, wps=24502.2, ups=0.37, wpb=65532.7, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.523, loss_scale=8, train_wall=246, gb_free=9.6, wall=33646
2022-03-16 01:59:55 | INFO | train_inner | epoch 032:    334 / 392 loss=6.269, ppl=77.11, wps=24997.9, ups=0.38, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.533, loss_scale=16, train_wall=241, gb_free=9.6, wall=33909
2022-03-16 02:02:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:02:59 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.381 | ppl 83.32 | wps 37940.5 | wpb 511.9 | bsz 1 | num_updates 12458 | best_loss 6.381
2022-03-16 02:02:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12458 updates
2022-03-16 02:02:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 02:03:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 02:03:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 32 @ 12458 updates, score 6.381) (writing took 1.8647870309650898 seconds)
2022-03-16 02:03:01 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-16 02:03:01 | INFO | train | epoch 032 | loss 6.265 | ppl 76.89 | wps 24026.7 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 12458 | lr 0.000283319 | gnorm 0.532 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 34094
KL Stats: Epoch 32 Divergences: Uniform: 3.7677506513606276 Unigram: 3.3546846140607745
2022-03-16 02:03:01 | INFO | fairseq.trainer | begin training epoch 33
2022-03-16 02:03:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:04:51 | INFO | train_inner | epoch 033:     42 / 392 loss=6.263, ppl=76.81, wps=21951.1, ups=0.34, wpb=65029.1, bsz=127, num_updates=12500, lr=0.000282843, gnorm=0.537, loss_scale=16, train_wall=239, gb_free=9.6, wall=34205
2022-03-16 02:05:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:08:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:09:18 | INFO | train_inner | epoch 033:    144 / 392 loss=6.232, ppl=75.15, wps=24519.2, ups=0.37, wpb=65532.7, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.531, loss_scale=8, train_wall=245, gb_free=9.6, wall=34472
2022-03-16 02:13:40 | INFO | train_inner | epoch 033:    244 / 392 loss=6.258, ppl=76.55, wps=24999.2, ups=0.38, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.537, loss_scale=16, train_wall=241, gb_free=9.6, wall=34734
2022-03-16 02:18:03 | INFO | train_inner | epoch 033:    344 / 392 loss=6.258, ppl=76.54, wps=25007.7, ups=0.38, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.528, loss_scale=16, train_wall=241, gb_free=9.6, wall=34996
2022-03-16 02:19:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 02:19:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:20:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:20:41 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.376 | ppl 83.03 | wps 37937.4 | wpb 511.9 | bsz 1 | num_updates 12846 | best_loss 6.376
2022-03-16 02:20:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12846 updates
2022-03-16 02:20:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 02:20:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 02:20:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 33 @ 12846 updates, score 6.376) (writing took 1.9497054908424616 seconds)
2022-03-16 02:20:43 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-16 02:20:43 | INFO | train | epoch 033 | loss 6.25 | ppl 76.1 | wps 23897.1 | ups 0.37 | wpb 65404.5 | bsz 127.7 | num_updates 12846 | lr 0.000279008 | gnorm 0.534 | loss_scale 8 | train_wall 942 | gb_free 9.6 | wall 35156
KL Stats: Epoch 33 Divergences: Uniform: 3.7780252048504104 Unigram: 3.3649360930694483
2022-03-16 02:20:43 | INFO | fairseq.trainer | begin training epoch 34
2022-03-16 02:20:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:23:04 | INFO | train_inner | epoch 034:     54 / 392 loss=6.238, ppl=75.45, wps=21544.5, ups=0.33, wpb=65029.1, bsz=127, num_updates=12900, lr=0.000278423, gnorm=0.533, loss_scale=8, train_wall=244, gb_free=9.6, wall=35298
2022-03-16 02:27:27 | INFO | train_inner | epoch 034:    154 / 392 loss=6.225, ppl=74.8, wps=24920.8, ups=0.38, wpb=65532.7, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.529, loss_scale=16, train_wall=241, gb_free=9.6, wall=35561
2022-03-16 02:28:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:31:54 | INFO | train_inner | epoch 034:    255 / 392 loss=6.244, ppl=75.8, wps=24601.9, ups=0.38, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.532, loss_scale=8, train_wall=245, gb_free=9.6, wall=35827
2022-03-16 02:36:16 | INFO | train_inner | epoch 034:    355 / 392 loss=6.248, ppl=75.99, wps=25012.3, ups=0.38, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.529, loss_scale=16, train_wall=241, gb_free=9.6, wall=36089
2022-03-16 02:36:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:37:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:38:25 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.36 | ppl 82.16 | wps 37952.9 | wpb 511.9 | bsz 1 | num_updates 13236 | best_loss 6.36
2022-03-16 02:38:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13236 updates
2022-03-16 02:38:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 02:38:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 02:38:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 34 @ 13236 updates, score 6.36) (writing took 1.8383465185761452 seconds)
2022-03-16 02:38:27 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-16 02:38:27 | INFO | train | epoch 034 | loss 6.236 | ppl 75.37 | wps 23976.6 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 13236 | lr 0.000274866 | gnorm 0.53 | loss_scale 8 | train_wall 944 | gb_free 9.6 | wall 36220
KL Stats: Epoch 34 Divergences: Uniform: 3.781977348362931 Unigram: 3.3699164005420634
2022-03-16 02:38:27 | INFO | fairseq.trainer | begin training epoch 35
2022-03-16 02:38:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:41:14 | INFO | train_inner | epoch 035:     64 / 392 loss=6.212, ppl=74.15, wps=21769.3, ups=0.33, wpb=65029.1, bsz=127, num_updates=13300, lr=0.000274204, gnorm=0.536, loss_scale=8, train_wall=241, gb_free=9.6, wall=36388
2022-03-16 02:45:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:45:39 | INFO | train_inner | epoch 035:    165 / 392 loss=6.21, ppl=74.02, wps=24761, ups=0.38, wpb=65532.7, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.536, loss_scale=8, train_wall=243, gb_free=9.6, wall=36653
2022-03-16 02:50:01 | INFO | train_inner | epoch 035:    265 / 392 loss=6.234, ppl=75.28, wps=25003.4, ups=0.38, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.536, loss_scale=8, train_wall=241, gb_free=9.6, wall=36915
2022-03-16 02:54:23 | INFO | train_inner | epoch 035:    365 / 392 loss=6.237, ppl=75.45, wps=24991.2, ups=0.38, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.548, loss_scale=16, train_wall=241, gb_free=9.6, wall=37177
2022-03-16 02:54:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 02:55:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 02:56:06 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.348 | ppl 81.48 | wps 37858.5 | wpb 511.9 | bsz 1 | num_updates 13626 | best_loss 6.348
2022-03-16 02:56:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13626 updates
2022-03-16 02:56:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 02:56:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 02:56:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 35 @ 13626 updates, score 6.348) (writing took 1.8868882963433862 seconds)
2022-03-16 02:56:08 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-16 02:56:08 | INFO | train | epoch 035 | loss 6.222 | ppl 74.64 | wps 24025.6 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 13626 | lr 0.000270904 | gnorm 0.54 | loss_scale 8 | train_wall 942 | gb_free 9.6 | wall 37282
KL Stats: Epoch 35 Divergences: Uniform: 3.7890429715392298 Unigram: 3.3765953646422453
2022-03-16 02:56:08 | INFO | fairseq.trainer | begin training epoch 36
2022-03-16 02:56:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 02:59:23 | INFO | train_inner | epoch 036:     74 / 392 loss=6.19, ppl=73, wps=21736.5, ups=0.33, wpb=65025.8, bsz=127, num_updates=13700, lr=0.000270172, gnorm=0.536, loss_scale=8, train_wall=241, gb_free=9.6, wall=37476
2022-03-16 03:03:45 | INFO | train_inner | epoch 036:    174 / 392 loss=6.205, ppl=73.76, wps=24984.9, ups=0.38, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.537, loss_scale=16, train_wall=241, gb_free=9.6, wall=37739
2022-03-16 03:06:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:08:10 | INFO | train_inner | epoch 036:    275 / 392 loss=6.218, ppl=74.42, wps=24746.1, ups=0.38, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.531, loss_scale=16, train_wall=243, gb_free=9.6, wall=38003
2022-03-16 03:12:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:12:35 | INFO | train_inner | epoch 036:    376 / 392 loss=6.228, ppl=74.94, wps=24751.7, ups=0.38, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.54, loss_scale=16, train_wall=243, gb_free=9.6, wall=38268
2022-03-16 03:12:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:13:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:13:49 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.353 | ppl 81.74 | wps 37936.1 | wpb 511.9 | bsz 1 | num_updates 14015 | best_loss 6.348
2022-03-16 03:13:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14015 updates
2022-03-16 03:13:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 03:13:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 03:13:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt (epoch 36 @ 14015 updates, score 6.353) (writing took 0.8859329782426357 seconds)
2022-03-16 03:13:49 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-16 03:13:49 | INFO | train | epoch 036 | loss 6.209 | ppl 73.96 | wps 23975.7 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 14015 | lr 0.000267118 | gnorm 0.536 | loss_scale 8 | train_wall 942 | gb_free 9.6 | wall 38343
KL Stats: Epoch 36 Divergences: Uniform: 3.796676038587152 Unigram: 3.387200988527323
2022-03-16 03:13:49 | INFO | fairseq.trainer | begin training epoch 37
2022-03-16 03:13:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:17:32 | INFO | train_inner | epoch 037:     85 / 392 loss=6.182, ppl=72.59, wps=21826.1, ups=0.34, wpb=65029.1, bsz=127, num_updates=14100, lr=0.000266312, gnorm=0.543, loss_scale=8, train_wall=241, gb_free=9.6, wall=38566
2022-03-16 03:21:55 | INFO | train_inner | epoch 037:    185 / 392 loss=6.19, ppl=72.99, wps=24997.8, ups=0.38, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.533, loss_scale=16, train_wall=241, gb_free=9.6, wall=38828
2022-03-16 03:25:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:26:19 | INFO | train_inner | epoch 037:    286 / 392 loss=6.204, ppl=73.7, wps=24756.2, ups=0.38, wpb=65532.7, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.53, loss_scale=16, train_wall=243, gb_free=9.6, wall=39093
2022-03-16 03:29:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:30:44 | INFO | train_inner | epoch 037:    387 / 392 loss=6.216, ppl=74.33, wps=24761.9, ups=0.38, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.543, loss_scale=8, train_wall=243, gb_free=9.6, wall=39358
2022-03-16 03:30:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:31:29 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.345 | ppl 81.3 | wps 37959 | wpb 511.9 | bsz 1 | num_updates 14405 | best_loss 6.345
2022-03-16 03:31:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14405 updates
2022-03-16 03:31:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 03:31:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 03:31:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 37 @ 14405 updates, score 6.345) (writing took 1.913713208399713 seconds)
2022-03-16 03:31:31 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-16 03:31:31 | INFO | train | epoch 037 | loss 6.197 | ppl 73.35 | wps 24027.2 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 14405 | lr 0.000263477 | gnorm 0.537 | loss_scale 8 | train_wall 942 | gb_free 9.6 | wall 39405
KL Stats: Epoch 37 Divergences: Uniform: 3.8022079688073305 Unigram: 3.3921515863455163
2022-03-16 03:31:31 | INFO | fairseq.trainer | begin training epoch 38
2022-03-16 03:31:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:35:40 | INFO | train_inner | epoch 038:     95 / 392 loss=6.164, ppl=71.7, wps=21948.3, ups=0.34, wpb=65029.1, bsz=127, num_updates=14500, lr=0.000262613, gnorm=0.554, loss_scale=8, train_wall=239, gb_free=9.6, wall=39654
2022-03-16 03:37:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:40:05 | INFO | train_inner | epoch 038:    196 / 392 loss=6.18, ppl=72.53, wps=24745.4, ups=0.38, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.534, loss_scale=8, train_wall=243, gb_free=9.6, wall=39919
2022-03-16 03:44:27 | INFO | train_inner | epoch 038:    296 / 392 loss=6.196, ppl=73.3, wps=24994.9, ups=0.38, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.534, loss_scale=16, train_wall=241, gb_free=9.6, wall=40181
2022-03-16 03:48:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 03:49:11 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.332 | ppl 80.59 | wps 37843.3 | wpb 511.9 | bsz 1 | num_updates 14796 | best_loss 6.332
2022-03-16 03:49:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14796 updates
2022-03-16 03:49:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 03:49:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 03:49:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 38 @ 14796 updates, score 6.332) (writing took 1.8982289787381887 seconds)
2022-03-16 03:49:13 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-16 03:49:13 | INFO | train | epoch 038 | loss 6.185 | ppl 72.76 | wps 24080.7 | ups 0.37 | wpb 65405.5 | bsz 127.7 | num_updates 14796 | lr 0.000259973 | gnorm 0.538 | loss_scale 32 | train_wall 942 | gb_free 9.6 | wall 40467
KL Stats: Epoch 38 Divergences: Uniform: 3.8086676011242746 Unigram: 3.397337915554642
2022-03-16 03:49:13 | INFO | fairseq.trainer | begin training epoch 39
2022-03-16 03:49:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 03:49:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:49:26 | INFO | train_inner | epoch 039:      5 / 392 loss=6.2, ppl=73.54, wps=21752.9, ups=0.33, wpb=65025.8, bsz=127, num_updates=14800, lr=0.000259938, gnorm=0.534, loss_scale=16, train_wall=241, gb_free=9.6, wall=40480
2022-03-16 03:53:48 | INFO | train_inner | epoch 039:    105 / 392 loss=6.153, ppl=71.18, wps=25000.1, ups=0.38, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.543, loss_scale=16, train_wall=241, gb_free=9.6, wall=40742
2022-03-16 03:54:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 03:54:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 03:58:16 | INFO | train_inner | epoch 039:    207 / 392 loss=6.166, ppl=71.81, wps=24521.7, ups=0.37, wpb=65532.7, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.533, loss_scale=8, train_wall=245, gb_free=9.6, wall=41009
2022-03-16 04:02:38 | INFO | train_inner | epoch 039:    307 / 392 loss=6.188, ppl=72.89, wps=25012, ups=0.38, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.541, loss_scale=16, train_wall=241, gb_free=9.6, wall=41271
2022-03-16 04:05:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 04:06:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:06:52 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.326 | ppl 80.22 | wps 38042.2 | wpb 511.9 | bsz 1 | num_updates 15184 | best_loss 6.326
2022-03-16 04:06:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15184 updates
2022-03-16 04:06:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 04:06:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 04:06:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 39 @ 15184 updates, score 6.326) (writing took 1.8110083164647222 seconds)
2022-03-16 04:06:54 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-16 04:06:54 | INFO | train | epoch 039 | loss 6.174 | ppl 72.22 | wps 23914.8 | ups 0.37 | wpb 65404.5 | bsz 127.7 | num_updates 15184 | lr 0.00025663 | gnorm 0.538 | loss_scale 8 | train_wall 941 | gb_free 9.6 | wall 41528
KL Stats: Epoch 39 Divergences: Uniform: 3.814360663673888 Unigram: 3.4019684875027734
2022-03-16 04:06:54 | INFO | fairseq.trainer | begin training epoch 40
2022-03-16 04:06:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:07:36 | INFO | train_inner | epoch 040:     16 / 392 loss=6.18, ppl=72.49, wps=21775.9, ups=0.33, wpb=65029.1, bsz=127, num_updates=15200, lr=0.000256495, gnorm=0.534, loss_scale=8, train_wall=241, gb_free=9.6, wall=41570
2022-03-16 04:11:59 | INFO | train_inner | epoch 040:    116 / 392 loss=6.143, ppl=70.66, wps=24985.4, ups=0.38, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.54, loss_scale=16, train_wall=241, gb_free=9.6, wall=41832
2022-03-16 04:16:21 | INFO | train_inner | epoch 040:    216 / 392 loss=6.164, ppl=71.72, wps=24986.8, ups=0.38, wpb=65532.7, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.535, loss_scale=16, train_wall=241, gb_free=9.6, wall=42095
2022-03-16 04:17:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:20:46 | INFO | train_inner | epoch 040:    317 / 392 loss=6.174, ppl=72.23, wps=24715.6, ups=0.38, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.542, loss_scale=16, train_wall=244, gb_free=9.6, wall=42360
2022-03-16 04:21:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 04:24:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:24:35 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.317 | ppl 79.7 | wps 38005.3 | wpb 511.9 | bsz 1 | num_updates 15574 | best_loss 6.317
2022-03-16 04:24:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15574 updates
2022-03-16 04:24:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 04:24:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 04:24:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 40 @ 15574 updates, score 6.317) (writing took 1.7917320812121034 seconds)
2022-03-16 04:24:37 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-16 04:24:37 | INFO | train | epoch 040 | loss 6.164 | ppl 71.69 | wps 24008.7 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 15574 | lr 0.000253396 | gnorm 0.542 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 42590
KL Stats: Epoch 40 Divergences: Uniform: 3.819318968767604 Unigram: 3.4083738567437654
2022-03-16 04:24:37 | INFO | fairseq.trainer | begin training epoch 41
2022-03-16 04:24:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:25:45 | INFO | train_inner | epoch 041:     26 / 392 loss=6.168, ppl=71.91, wps=21762.8, ups=0.33, wpb=65029.1, bsz=127, num_updates=15600, lr=0.000253185, gnorm=0.552, loss_scale=8, train_wall=242, gb_free=9.6, wall=42659
2022-03-16 04:30:07 | INFO | train_inner | epoch 041:    126 / 392 loss=6.14, ppl=70.51, wps=25015.8, ups=0.38, wpb=65532.7, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.539, loss_scale=16, train_wall=241, gb_free=9.6, wall=42921
2022-03-16 04:33:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 04:33:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 04:34:34 | INFO | train_inner | epoch 041:    228 / 392 loss=6.156, ppl=71.33, wps=24510, ups=0.37, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.535, loss_scale=8, train_wall=246, gb_free=9.6, wall=43188
2022-03-16 04:38:56 | INFO | train_inner | epoch 041:    328 / 392 loss=6.164, ppl=71.71, wps=25007.2, ups=0.38, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.542, loss_scale=8, train_wall=241, gb_free=9.6, wall=43450
2022-03-16 04:39:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 04:41:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:42:16 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.318 | ppl 79.8 | wps 37972.6 | wpb 511.9 | bsz 1 | num_updates 15963 | best_loss 6.317
2022-03-16 04:42:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 15963 updates
2022-03-16 04:42:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 04:42:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 04:42:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt (epoch 41 @ 15963 updates, score 6.318) (writing took 0.8890622770413756 seconds)
2022-03-16 04:42:17 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-16 04:42:17 | INFO | train | epoch 041 | loss 6.154 | ppl 71.19 | wps 23997.9 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 15963 | lr 0.00025029 | gnorm 0.543 | loss_scale 8 | train_wall 941 | gb_free 9.6 | wall 43651
KL Stats: Epoch 41 Divergences: Uniform: 3.8262838164554367 Unigram: 3.4149524613826285
2022-03-16 04:42:17 | INFO | fairseq.trainer | begin training epoch 42
2022-03-16 04:42:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 04:43:54 | INFO | train_inner | epoch 042:     37 / 392 loss=6.146, ppl=70.8, wps=21845.6, ups=0.34, wpb=65029.1, bsz=127, num_updates=16000, lr=0.00025, gnorm=0.556, loss_scale=8, train_wall=241, gb_free=9.6, wall=43748
2022-03-16 04:47:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 04:48:19 | INFO | train_inner | epoch 042:    138 / 392 loss=6.134, ppl=70.24, wps=24766.2, ups=0.38, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.541, loss_scale=8, train_wall=243, gb_free=9.6, wall=44012
2022-03-16 04:52:41 | INFO | train_inner | epoch 042:    238 / 392 loss=6.141, ppl=70.58, wps=24977.3, ups=0.38, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.542, loss_scale=8, train_wall=241, gb_free=9.6, wall=44275
2022-03-16 04:57:03 | INFO | train_inner | epoch 042:    338 / 392 loss=6.161, ppl=71.57, wps=25009.2, ups=0.38, wpb=65532.7, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.534, loss_scale=16, train_wall=241, gb_free=9.6, wall=44537
2022-03-16 04:59:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 04:59:57 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.314 | ppl 79.57 | wps 37886.8 | wpb 511.9 | bsz 1 | num_updates 16354 | best_loss 6.314
2022-03-16 04:59:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16354 updates
2022-03-16 04:59:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 04:59:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 04:59:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 42 @ 16354 updates, score 6.314) (writing took 1.7699339631944895 seconds)
2022-03-16 04:59:58 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-16 04:59:58 | INFO | train | epoch 042 | loss 6.144 | ppl 70.71 | wps 24090.7 | ups 0.37 | wpb 65405.5 | bsz 127.7 | num_updates 16354 | lr 0.000247279 | gnorm 0.542 | loss_scale 32 | train_wall 942 | gb_free 9.6 | wall 44712
KL Stats: Epoch 42 Divergences: Uniform: 3.8315955219608173 Unigram: 3.4206115912477815
2022-03-16 04:59:58 | INFO | fairseq.trainer | begin training epoch 43
2022-03-16 04:59:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:00:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:02:02 | INFO | train_inner | epoch 043:     47 / 392 loss=6.136, ppl=70.33, wps=21764.2, ups=0.33, wpb=65029.1, bsz=127, num_updates=16400, lr=0.000246932, gnorm=0.546, loss_scale=16, train_wall=241, gb_free=9.6, wall=44835
2022-03-16 05:05:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:06:26 | INFO | train_inner | epoch 043:    148 / 392 loss=6.123, ppl=69.7, wps=24766.8, ups=0.38, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.538, loss_scale=16, train_wall=243, gb_free=9.6, wall=45100
2022-03-16 05:06:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:10:51 | INFO | train_inner | epoch 043:    249 / 392 loss=6.136, ppl=70.33, wps=24761.5, ups=0.38, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.543, loss_scale=8, train_wall=243, gb_free=9.6, wall=45365
2022-03-16 05:15:13 | INFO | train_inner | epoch 043:    349 / 392 loss=6.149, ppl=70.97, wps=25009.3, ups=0.38, wpb=65532.7, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.533, loss_scale=16, train_wall=241, gb_free=9.6, wall=45627
2022-03-16 05:17:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:17:38 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.304 | ppl 79.04 | wps 37983.7 | wpb 511.9 | bsz 1 | num_updates 16743 | best_loss 6.304
2022-03-16 05:17:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16743 updates
2022-03-16 05:17:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 05:17:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 05:17:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 43 @ 16743 updates, score 6.304) (writing took 1.8297864189371467 seconds)
2022-03-16 05:17:40 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-16 05:17:40 | INFO | train | epoch 043 | loss 6.134 | ppl 70.23 | wps 23974.7 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 16743 | lr 0.00024439 | gnorm 0.539 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 45773
KL Stats: Epoch 43 Divergences: Uniform: 3.8377588825595375 Unigram: 3.4276551931641164
2022-03-16 05:17:40 | INFO | fairseq.trainer | begin training epoch 44
2022-03-16 05:17:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:17:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:20:12 | INFO | train_inner | epoch 044:     58 / 392 loss=6.122, ppl=69.64, wps=21768.5, ups=0.33, wpb=65029.1, bsz=127, num_updates=16800, lr=0.000243975, gnorm=0.545, loss_scale=8, train_wall=241, gb_free=9.6, wall=45926
2022-03-16 05:23:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:24:36 | INFO | train_inner | epoch 044:    159 / 392 loss=6.109, ppl=69.04, wps=24762.4, ups=0.38, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.545, loss_scale=8, train_wall=243, gb_free=9.6, wall=46190
2022-03-16 05:28:58 | INFO | train_inner | epoch 044:    259 / 392 loss=6.139, ppl=70.47, wps=25018.4, ups=0.38, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.54, loss_scale=8, train_wall=241, gb_free=9.6, wall=46452
2022-03-16 05:29:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 05:33:23 | INFO | train_inner | epoch 044:    360 / 392 loss=6.136, ppl=70.33, wps=24776.3, ups=0.38, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.538, loss_scale=8, train_wall=243, gb_free=9.6, wall=46717
2022-03-16 05:34:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:35:19 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.304 | ppl 79 | wps 37968.5 | wpb 511.9 | bsz 1 | num_updates 17132 | best_loss 6.304
2022-03-16 05:35:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17132 updates
2022-03-16 05:35:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 05:35:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 05:35:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 44 @ 17132 updates, score 6.304) (writing took 1.7981362389400601 seconds)
2022-03-16 05:35:21 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-16 05:35:21 | INFO | train | epoch 044 | loss 6.125 | ppl 69.82 | wps 23982.5 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 17132 | lr 0.000241599 | gnorm 0.543 | loss_scale 8 | train_wall 941 | gb_free 9.6 | wall 46834
KL Stats: Epoch 44 Divergences: Uniform: 3.8435335625003755 Unigram: 3.432742204060974
2022-03-16 05:35:21 | INFO | fairseq.trainer | begin training epoch 45
2022-03-16 05:35:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:38:19 | INFO | train_inner | epoch 045:     68 / 392 loss=6.109, ppl=69.03, wps=21976.4, ups=0.34, wpb=65025.8, bsz=127, num_updates=17200, lr=0.000241121, gnorm=0.542, loss_scale=16, train_wall=239, gb_free=9.6, wall=47013
2022-03-16 05:41:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:42:43 | INFO | train_inner | epoch 045:    169 / 392 loss=6.103, ppl=68.71, wps=24768.8, ups=0.38, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.541, loss_scale=16, train_wall=243, gb_free=9.6, wall=47277
2022-03-16 05:47:05 | INFO | train_inner | epoch 045:    269 / 392 loss=6.127, ppl=69.9, wps=25010.5, ups=0.38, wpb=65532.7, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.545, loss_scale=32, train_wall=241, gb_free=9.6, wall=47539
2022-03-16 05:47:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:51:30 | INFO | train_inner | epoch 045:    370 / 392 loss=6.136, ppl=70.34, wps=24760, ups=0.38, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.54, loss_scale=16, train_wall=243, gb_free=9.6, wall=47804
2022-03-16 05:52:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 05:53:00 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.299 | ppl 78.75 | wps 37956.4 | wpb 511.9 | bsz 1 | num_updates 17522 | best_loss 6.299
2022-03-16 05:53:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17522 updates
2022-03-16 05:53:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 05:53:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 05:53:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 45 @ 17522 updates, score 6.299) (writing took 1.7921245619654655 seconds)
2022-03-16 05:53:02 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-16 05:53:02 | INFO | train | epoch 045 | loss 6.117 | ppl 69.43 | wps 24038.3 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 17522 | lr 0.000238896 | gnorm 0.542 | loss_scale 16 | train_wall 941 | gb_free 9.6 | wall 47895
KL Stats: Epoch 45 Divergences: Uniform: 3.8480897215184937 Unigram: 3.4364797526549826
2022-03-16 05:53:02 | INFO | fairseq.trainer | begin training epoch 46
2022-03-16 05:53:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 05:53:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 05:56:29 | INFO | train_inner | epoch 046:     79 / 392 loss=6.095, ppl=68.37, wps=21770.5, ups=0.33, wpb=65025.8, bsz=127, num_updates=17600, lr=0.000238366, gnorm=0.55, loss_scale=16, train_wall=241, gb_free=9.6, wall=48103
2022-03-16 05:58:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:00:53 | INFO | train_inner | epoch 046:    180 / 392 loss=6.104, ppl=68.79, wps=24765, ups=0.38, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.543, loss_scale=8, train_wall=243, gb_free=9.6, wall=48367
2022-03-16 06:04:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:05:18 | INFO | train_inner | epoch 046:    281 / 392 loss=6.116, ppl=69.38, wps=24748.8, ups=0.38, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.542, loss_scale=8, train_wall=243, gb_free=9.6, wall=48632
2022-03-16 06:09:40 | INFO | train_inner | epoch 046:    381 / 392 loss=6.121, ppl=69.62, wps=24996.5, ups=0.38, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.549, loss_scale=8, train_wall=241, gb_free=9.6, wall=48894
2022-03-16 06:10:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:10:41 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.289 | ppl 78.17 | wps 37929.2 | wpb 511.9 | bsz 1 | num_updates 17911 | best_loss 6.289
2022-03-16 06:10:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 17911 updates
2022-03-16 06:10:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 06:10:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 06:10:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 46 @ 17911 updates, score 6.289) (writing took 1.8890453660860658 seconds)
2022-03-16 06:10:43 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-16 06:10:43 | INFO | train | epoch 046 | loss 6.108 | ppl 69 | wps 23967.2 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 17911 | lr 0.000236287 | gnorm 0.546 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 48957
KL Stats: Epoch 46 Divergences: Uniform: 3.8520219186141915 Unigram: 3.440022608634064
2022-03-16 06:10:43 | INFO | fairseq.trainer | begin training epoch 47
2022-03-16 06:10:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:14:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:14:39 | INFO | train_inner | epoch 047:     90 / 392 loss=6.083, ppl=67.78, wps=21757, ups=0.33, wpb=65029.1, bsz=127, num_updates=18000, lr=0.000235702, gnorm=0.544, loss_scale=8, train_wall=241, gb_free=9.6, wall=49193
2022-03-16 06:19:01 | INFO | train_inner | epoch 047:    190 / 392 loss=6.094, ppl=68.31, wps=25011, ups=0.38, wpb=65532.7, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.543, loss_scale=8, train_wall=241, gb_free=9.6, wall=49455
2022-03-16 06:23:24 | INFO | train_inner | epoch 047:    290 / 392 loss=6.109, ppl=69.05, wps=24990.3, ups=0.38, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.543, loss_scale=16, train_wall=241, gb_free=9.6, wall=49717
2022-03-16 06:23:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:27:49 | INFO | train_inner | epoch 047:    391 / 392 loss=6.122, ppl=69.66, wps=24724, ups=0.38, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.545, loss_scale=8, train_wall=244, gb_free=9.6, wall=49982
2022-03-16 06:27:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:28:23 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 6.287 | ppl 78.07 | wps 37893.6 | wpb 511.9 | bsz 1 | num_updates 18301 | best_loss 6.287
2022-03-16 06:28:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18301 updates
2022-03-16 06:28:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 06:28:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 06:28:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 47 @ 18301 updates, score 6.287) (writing took 1.798554789274931 seconds)
2022-03-16 06:28:25 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-16 06:28:25 | INFO | train | epoch 047 | loss 6.101 | ppl 68.66 | wps 24019.3 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 18301 | lr 0.000233756 | gnorm 0.544 | loss_scale 8 | train_wall 942 | gb_free 9.6 | wall 50019
KL Stats: Epoch 47 Divergences: Uniform: 3.856903578351529 Unigram: 3.4455438608687268
2022-03-16 06:28:25 | INFO | fairseq.trainer | begin training epoch 48
2022-03-16 06:28:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:32:45 | INFO | train_inner | epoch 048:     99 / 392 loss=6.068, ppl=67.08, wps=21956.8, ups=0.34, wpb=65025.8, bsz=127, num_updates=18400, lr=0.000233126, gnorm=0.547, loss_scale=16, train_wall=239, gb_free=9.6, wall=50279
2022-03-16 06:33:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:37:09 | INFO | train_inner | epoch 048:    200 / 392 loss=6.089, ppl=68.06, wps=24765, ups=0.38, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.546, loss_scale=8, train_wall=243, gb_free=9.6, wall=50543
2022-03-16 06:39:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:41:34 | INFO | train_inner | epoch 048:    301 / 392 loss=6.104, ppl=68.78, wps=24762.2, ups=0.38, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.544, loss_scale=8, train_wall=243, gb_free=9.6, wall=50808
2022-03-16 06:45:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 06:46:05 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 6.283 | ppl 77.88 | wps 38008.6 | wpb 511.9 | bsz 1 | num_updates 18691 | best_loss 6.283
2022-03-16 06:46:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18691 updates
2022-03-16 06:46:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 06:46:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 06:46:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 48 @ 18691 updates, score 6.283) (writing took 1.9165271781384945 seconds)
2022-03-16 06:46:07 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-16 06:46:07 | INFO | train | epoch 048 | loss 6.093 | ppl 68.28 | wps 24033.9 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 18691 | lr 0.000231304 | gnorm 0.545 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 51080
KL Stats: Epoch 48 Divergences: Uniform: 3.861101845440565 Unigram: 3.4495486994969786
2022-03-16 06:46:07 | INFO | fairseq.trainer | begin training epoch 49
2022-03-16 06:46:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 06:46:30 | INFO | train_inner | epoch 049:      9 / 392 loss=6.107, ppl=68.95, wps=21962.3, ups=0.34, wpb=65029.1, bsz=127, num_updates=18700, lr=0.000231249, gnorm=0.55, loss_scale=16, train_wall=239, gb_free=9.6, wall=51104
2022-03-16 06:46:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:50:55 | INFO | train_inner | epoch 049:    110 / 392 loss=6.066, ppl=67, wps=24753.4, ups=0.38, wpb=65532.7, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.537, loss_scale=8, train_wall=243, gb_free=9.6, wall=51369
2022-03-16 06:54:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 06:55:20 | INFO | train_inner | epoch 049:    211 / 392 loss=6.085, ppl=67.88, wps=24760.6, ups=0.38, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.546, loss_scale=8, train_wall=243, gb_free=9.6, wall=51633
2022-03-16 06:59:42 | INFO | train_inner | epoch 049:    311 / 392 loss=6.095, ppl=68.37, wps=25004.5, ups=0.38, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.541, loss_scale=8, train_wall=241, gb_free=9.6, wall=51895
2022-03-16 07:03:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:03:46 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 6.286 | ppl 78.01 | wps 37922.4 | wpb 511.9 | bsz 1 | num_updates 19081 | best_loss 6.283
2022-03-16 07:03:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19081 updates
2022-03-16 07:03:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 07:03:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 07:03:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt (epoch 49 @ 19081 updates, score 6.286) (writing took 0.8328563906252384 seconds)
2022-03-16 07:03:47 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-16 07:03:47 | INFO | train | epoch 049 | loss 6.086 | ppl 67.92 | wps 24056.7 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 19081 | lr 0.000228928 | gnorm 0.543 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 52141
KL Stats: Epoch 49 Divergences: Uniform: 3.8670571367431115 Unigram: 3.455813712533685
2022-03-16 07:03:47 | INFO | fairseq.trainer | begin training epoch 50
2022-03-16 07:03:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:04:37 | INFO | train_inner | epoch 050:     19 / 392 loss=6.092, ppl=68.2, wps=22043, ups=0.34, wpb=65029.1, bsz=127, num_updates=19100, lr=0.000228814, gnorm=0.546, loss_scale=16, train_wall=239, gb_free=9.6, wall=52190
2022-03-16 07:06:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:09:01 | INFO | train_inner | epoch 050:    120 / 392 loss=6.052, ppl=66.34, wps=24773.2, ups=0.38, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.549, loss_scale=16, train_wall=243, gb_free=9.6, wall=52455
2022-03-16 07:11:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:13:26 | INFO | train_inner | epoch 050:    221 / 392 loss=6.079, ppl=67.62, wps=24772.4, ups=0.38, wpb=65532.7, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.539, loss_scale=16, train_wall=243, gb_free=9.6, wall=52720
2022-03-16 07:17:48 | INFO | train_inner | epoch 050:    321 / 392 loss=6.093, ppl=68.25, wps=25023, ups=0.38, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.544, loss_scale=32, train_wall=241, gb_free=9.6, wall=52981
2022-03-16 07:18:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 07:20:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:20:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:21:26 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 6.28 | ppl 77.73 | wps 37909.7 | wpb 511.9 | bsz 1 | num_updates 19469 | best_loss 6.28
2022-03-16 07:21:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19469 updates
2022-03-16 07:21:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 07:21:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 07:21:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 50 @ 19469 updates, score 6.28) (writing took 1.8769295252859592 seconds)
2022-03-16 07:21:28 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-16 07:21:28 | INFO | train | epoch 050 | loss 6.079 | ppl 67.61 | wps 23923.2 | ups 0.37 | wpb 65404.5 | bsz 127.7 | num_updates 19469 | lr 0.000226636 | gnorm 0.545 | loss_scale 8 | train_wall 941 | gb_free 9.6 | wall 53201
KL Stats: Epoch 50 Divergences: Uniform: 3.871312509170186 Unigram: 3.460297828859527
2022-03-16 07:21:28 | INFO | fairseq.trainer | begin training epoch 51
2022-03-16 07:21:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:22:49 | INFO | train_inner | epoch 051:     31 / 392 loss=6.091, ppl=68.18, wps=21597.2, ups=0.33, wpb=65029.1, bsz=127, num_updates=19500, lr=0.000226455, gnorm=0.548, loss_scale=8, train_wall=243, gb_free=9.6, wall=53283
2022-03-16 07:27:11 | INFO | train_inner | epoch 051:    131 / 392 loss=6.053, ppl=66.41, wps=25037.9, ups=0.38, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.547, loss_scale=16, train_wall=240, gb_free=9.6, wall=53544
2022-03-16 07:31:33 | INFO | train_inner | epoch 051:    231 / 392 loss=6.068, ppl=67.1, wps=25016.1, ups=0.38, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.546, loss_scale=16, train_wall=240, gb_free=9.6, wall=53806
2022-03-16 07:31:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:35:57 | INFO | train_inner | epoch 051:    332 / 392 loss=6.092, ppl=68.22, wps=24791, ups=0.38, wpb=65532.7, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.548, loss_scale=8, train_wall=243, gb_free=9.6, wall=54071
2022-03-16 07:38:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:39:06 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 6.279 | ppl 77.65 | wps 38005.7 | wpb 511.9 | bsz 1 | num_updates 19860 | best_loss 6.279
2022-03-16 07:39:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 19860 updates
2022-03-16 07:39:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 07:39:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 07:39:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 51 @ 19860 updates, score 6.279) (writing took 1.8565638409927487 seconds)
2022-03-16 07:39:08 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-16 07:39:08 | INFO | train | epoch 051 | loss 6.072 | ppl 67.28 | wps 24122.8 | ups 0.37 | wpb 65405.5 | bsz 127.7 | num_updates 19860 | lr 0.000224394 | gnorm 0.548 | loss_scale 16 | train_wall 940 | gb_free 9.6 | wall 54261
KL Stats: Epoch 51 Divergences: Uniform: 3.875013614750391 Unigram: 3.4646706905813582
2022-03-16 07:39:08 | INFO | fairseq.trainer | begin training epoch 52
2022-03-16 07:39:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:40:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:40:55 | INFO | train_inner | epoch 052:     41 / 392 loss=6.071, ppl=67.21, wps=21805.1, ups=0.34, wpb=65029.1, bsz=127, num_updates=19900, lr=0.000224168, gnorm=0.546, loss_scale=8, train_wall=241, gb_free=9.6, wall=54369
2022-03-16 07:45:17 | INFO | train_inner | epoch 052:    141 / 392 loss=6.048, ppl=66.15, wps=25016.4, ups=0.38, wpb=65532.7, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.554, loss_scale=8, train_wall=241, gb_free=9.6, wall=54631
2022-03-16 07:49:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 07:49:42 | INFO | train_inner | epoch 052:    242 / 392 loss=6.066, ppl=67.01, wps=24765.7, ups=0.38, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.544, loss_scale=8, train_wall=243, gb_free=9.6, wall=54895
2022-03-16 07:54:04 | INFO | train_inner | epoch 052:    342 / 392 loss=6.085, ppl=67.87, wps=25022.8, ups=0.38, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.544, loss_scale=8, train_wall=241, gb_free=9.6, wall=55157
2022-03-16 07:56:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 07:56:47 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 6.274 | ppl 77.39 | wps 37944.5 | wpb 511.9 | bsz 1 | num_updates 20250 | best_loss 6.274
2022-03-16 07:56:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20250 updates
2022-03-16 07:56:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 07:56:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 07:56:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 52 @ 20250 updates, score 6.274) (writing took 1.8439543386921287 seconds)
2022-03-16 07:56:48 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-16 07:56:48 | INFO | train | epoch 052 | loss 6.065 | ppl 66.96 | wps 24050.3 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 20250 | lr 0.000222222 | gnorm 0.549 | loss_scale 16 | train_wall 941 | gb_free 9.6 | wall 55322
KL Stats: Epoch 52 Divergences: Uniform: 3.8800510972079207 Unigram: 3.4688534424853135
2022-03-16 07:56:48 | INFO | fairseq.trainer | begin training epoch 53
2022-03-16 07:56:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 07:58:59 | INFO | train_inner | epoch 053:     50 / 392 loss=6.05, ppl=66.26, wps=21981.1, ups=0.34, wpb=65029.1, bsz=127, num_updates=20300, lr=0.000221948, gnorm=0.554, loss_scale=16, train_wall=239, gb_free=9.6, wall=55453
2022-03-16 08:01:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:03:24 | INFO | train_inner | epoch 053:    151 / 392 loss=6.049, ppl=66.22, wps=24778.4, ups=0.38, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.546, loss_scale=16, train_wall=243, gb_free=9.6, wall=55718
2022-03-16 08:06:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:07:49 | INFO | train_inner | epoch 053:    252 / 392 loss=6.066, ppl=66.99, wps=24770, ups=0.38, wpb=65532.7, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.548, loss_scale=16, train_wall=243, gb_free=9.6, wall=55982
2022-03-16 08:09:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:12:13 | INFO | train_inner | epoch 053:    353 / 392 loss=6.075, ppl=67.41, wps=24749.5, ups=0.38, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.546, loss_scale=8, train_wall=243, gb_free=9.6, wall=56247
2022-03-16 08:13:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:14:27 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 6.267 | ppl 77.04 | wps 37848.3 | wpb 511.9 | bsz 1 | num_updates 20639 | best_loss 6.267
2022-03-16 08:14:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20639 updates
2022-03-16 08:14:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 08:14:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 08:14:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 53 @ 20639 updates, score 6.267) (writing took 1.8294242341071367 seconds)
2022-03-16 08:14:29 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-16 08:14:29 | INFO | train | epoch 053 | loss 6.06 | ppl 66.7 | wps 23981.1 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 20639 | lr 0.000220118 | gnorm 0.548 | loss_scale 8 | train_wall 941 | gb_free 9.6 | wall 56383
KL Stats: Epoch 53 Divergences: Uniform: 3.8825419126609453 Unigram: 3.470637769044703
2022-03-16 08:14:29 | INFO | fairseq.trainer | begin training epoch 54
2022-03-16 08:14:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:17:09 | INFO | train_inner | epoch 054:     61 / 392 loss=6.043, ppl=65.93, wps=21959.7, ups=0.34, wpb=65025.8, bsz=127, num_updates=20700, lr=0.000219793, gnorm=0.548, loss_scale=16, train_wall=239, gb_free=9.6, wall=56543
2022-03-16 08:21:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:21:34 | INFO | train_inner | epoch 054:    162 / 392 loss=6.042, ppl=65.89, wps=24773, ups=0.38, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.542, loss_scale=16, train_wall=243, gb_free=9.6, wall=56808
2022-03-16 08:21:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:25:59 | INFO | train_inner | epoch 054:    263 / 392 loss=6.064, ppl=66.89, wps=24760.6, ups=0.38, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.545, loss_scale=8, train_wall=243, gb_free=9.6, wall=57072
2022-03-16 08:30:21 | INFO | train_inner | epoch 054:    363 / 392 loss=6.069, ppl=67.15, wps=24986.6, ups=0.38, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.547, loss_scale=16, train_wall=241, gb_free=9.6, wall=57335
2022-03-16 08:31:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:32:09 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 6.263 | ppl 76.78 | wps 37885.5 | wpb 511.9 | bsz 1 | num_updates 21029 | best_loss 6.263
2022-03-16 08:32:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21029 updates
2022-03-16 08:32:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 08:32:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 08:32:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 54 @ 21029 updates, score 6.263) (writing took 1.8590990025550127 seconds)
2022-03-16 08:32:11 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-16 08:32:11 | INFO | train | epoch 054 | loss 6.053 | ppl 66.39 | wps 24024 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 21029 | lr 0.000218067 | gnorm 0.545 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 57445
KL Stats: Epoch 54 Divergences: Uniform: 3.8876790203942417 Unigram: 3.4756474563061674
2022-03-16 08:32:11 | INFO | fairseq.trainer | begin training epoch 55
2022-03-16 08:32:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:32:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:35:20 | INFO | train_inner | epoch 055:     72 / 392 loss=6.032, ppl=65.45, wps=21757, ups=0.33, wpb=65029.1, bsz=127, num_updates=21100, lr=0.0002177, gnorm=0.555, loss_scale=8, train_wall=241, gb_free=9.6, wall=57634
2022-03-16 08:39:42 | INFO | train_inner | epoch 055:    172 / 392 loss=6.04, ppl=65.79, wps=25024.8, ups=0.38, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.544, loss_scale=16, train_wall=240, gb_free=9.6, wall=57895
2022-03-16 08:44:04 | INFO | train_inner | epoch 055:    272 / 392 loss=6.057, ppl=66.6, wps=25016.8, ups=0.38, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.54, loss_scale=32, train_wall=241, gb_free=9.6, wall=58157
2022-03-16 08:44:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:48:29 | INFO | train_inner | epoch 055:    373 / 392 loss=6.059, ppl=66.67, wps=24739.5, ups=0.38, wpb=65532.7, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.547, loss_scale=16, train_wall=243, gb_free=9.6, wall=58422
2022-03-16 08:49:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 08:49:50 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 6.267 | ppl 76.99 | wps 37909.9 | wpb 511.9 | bsz 1 | num_updates 21419 | best_loss 6.263
2022-03-16 08:49:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 21419 updates
2022-03-16 08:49:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 08:49:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 08:49:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt (epoch 55 @ 21419 updates, score 6.267) (writing took 0.848540659993887 seconds)
2022-03-16 08:49:51 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-16 08:49:51 | INFO | train | epoch 055 | loss 6.047 | ppl 66.13 | wps 24061.4 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 21419 | lr 0.000216073 | gnorm 0.546 | loss_scale 16 | train_wall 941 | gb_free 9.6 | wall 58505
KL Stats: Epoch 55 Divergences: Uniform: 3.8913643540862193 Unigram: 3.4794725447827224
2022-03-16 08:49:51 | INFO | fairseq.trainer | begin training epoch 56
2022-03-16 08:49:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 08:50:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 08:51:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 08:53:29 | INFO | train_inner | epoch 056:     83 / 392 loss=6.021, ppl=64.94, wps=21661.6, ups=0.33, wpb=65029.1, bsz=127, num_updates=21500, lr=0.000215666, gnorm=0.556, loss_scale=8, train_wall=243, gb_free=9.6, wall=58722
2022-03-16 08:57:51 | INFO | train_inner | epoch 056:    183 / 392 loss=6.034, ppl=65.55, wps=25014.2, ups=0.38, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.552, loss_scale=16, train_wall=241, gb_free=9.6, wall=58984
2022-03-16 09:01:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:02:16 | INFO | train_inner | epoch 056:    284 / 392 loss=6.05, ppl=66.26, wps=24745.6, ups=0.38, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.546, loss_scale=8, train_wall=243, gb_free=9.6, wall=59249
2022-03-16 09:06:37 | INFO | train_inner | epoch 056:    384 / 392 loss=6.063, ppl=66.87, wps=25025.7, ups=0.38, wpb=65532.7, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.553, loss_scale=8, train_wall=240, gb_free=9.6, wall=59511
2022-03-16 09:06:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:07:30 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 6.26 | ppl 76.66 | wps 37932 | wpb 511.9 | bsz 1 | num_updates 21808 | best_loss 6.26
2022-03-16 09:07:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 21808 updates
2022-03-16 09:07:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 09:07:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 09:07:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 56 @ 21808 updates, score 6.26) (writing took 1.959904657676816 seconds)
2022-03-16 09:07:32 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-16 09:07:32 | INFO | train | epoch 056 | loss 6.041 | ppl 65.84 | wps 23974.8 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 21808 | lr 0.000214137 | gnorm 0.552 | loss_scale 16 | train_wall 941 | gb_free 9.6 | wall 59566
KL Stats: Epoch 56 Divergences: Uniform: 3.895182741335201 Unigram: 3.4833353450556928
2022-03-16 09:07:32 | INFO | fairseq.trainer | begin training epoch 57
2022-03-16 09:07:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:11:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:11:36 | INFO | train_inner | epoch 057:     93 / 392 loss=6.009, ppl=64.39, wps=21760.3, ups=0.33, wpb=65029.1, bsz=127, num_updates=21900, lr=0.000213687, gnorm=0.558, loss_scale=8, train_wall=241, gb_free=9.6, wall=59810
2022-03-16 09:15:58 | INFO | train_inner | epoch 057:    193 / 392 loss=6.03, ppl=65.34, wps=25019.7, ups=0.38, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.548, loss_scale=8, train_wall=240, gb_free=9.6, wall=60072
2022-03-16 09:18:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:20:23 | INFO | train_inner | epoch 057:    294 / 392 loss=6.053, ppl=66.42, wps=24774.6, ups=0.38, wpb=65532.7, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.552, loss_scale=8, train_wall=243, gb_free=9.6, wall=60336
2022-03-16 09:24:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:24:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:25:12 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 6.26 | ppl 76.62 | wps 37963 | wpb 511.9 | bsz 1 | num_updates 22197 | best_loss 6.26
2022-03-16 09:25:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 22197 updates
2022-03-16 09:25:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 09:25:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 09:25:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 57 @ 22197 updates, score 6.26) (writing took 1.7880392847582698 seconds)
2022-03-16 09:25:13 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-16 09:25:13 | INFO | train | epoch 057 | loss 6.035 | ppl 65.59 | wps 23981.1 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 22197 | lr 0.000212253 | gnorm 0.553 | loss_scale 8 | train_wall 941 | gb_free 9.6 | wall 60627
KL Stats: Epoch 57 Divergences: Uniform: 3.8990230025514427 Unigram: 3.487024856678483
2022-03-16 09:25:13 | INFO | fairseq.trainer | begin training epoch 58
2022-03-16 09:25:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:25:21 | INFO | train_inner | epoch 058:      3 / 392 loss=6.05, ppl=66.27, wps=21782.3, ups=0.33, wpb=65029.1, bsz=127, num_updates=22200, lr=0.000212238, gnorm=0.561, loss_scale=8, train_wall=241, gb_free=9.6, wall=60635
2022-03-16 09:29:43 | INFO | train_inner | epoch 058:    103 / 392 loss=6.004, ppl=64.2, wps=25051, ups=0.38, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.547, loss_scale=8, train_wall=240, gb_free=9.6, wall=60897
2022-03-16 09:32:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:34:07 | INFO | train_inner | epoch 058:    204 / 392 loss=6.024, ppl=65.07, wps=24798.6, ups=0.38, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.551, loss_scale=8, train_wall=243, gb_free=9.6, wall=61161
2022-03-16 09:38:29 | INFO | train_inner | epoch 058:    304 / 392 loss=6.039, ppl=65.76, wps=25036.4, ups=0.38, wpb=65532.7, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.554, loss_scale=16, train_wall=240, gb_free=9.6, wall=61423
2022-03-16 09:42:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 09:42:51 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 6.251 | ppl 76.14 | wps 38062.2 | wpb 511.9 | bsz 1 | num_updates 22588 | best_loss 6.251
2022-03-16 09:42:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 22588 updates
2022-03-16 09:42:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 09:42:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 09:42:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 58 @ 22588 updates, score 6.251) (writing took 1.7783044623211026 seconds)
2022-03-16 09:42:53 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-16 09:42:53 | INFO | train | epoch 058 | loss 6.03 | ppl 65.34 | wps 24134.6 | ups 0.37 | wpb 65405.5 | bsz 127.7 | num_updates 22588 | lr 0.000210407 | gnorm 0.551 | loss_scale 16 | train_wall 940 | gb_free 9.6 | wall 61687
KL Stats: Epoch 58 Divergences: Uniform: 3.900214742723928 Unigram: 3.488246719449914
2022-03-16 09:42:53 | INFO | fairseq.trainer | begin training epoch 59
2022-03-16 09:42:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 09:43:25 | INFO | train_inner | epoch 059:     12 / 392 loss=6.049, ppl=66.22, wps=22003.2, ups=0.34, wpb=65029.1, bsz=127, num_updates=22600, lr=0.000210352, gnorm=0.553, loss_scale=16, train_wall=239, gb_free=9.6, wall=61718
2022-03-16 09:43:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 09:47:49 | INFO | train_inner | epoch 059:    113 / 392 loss=6.005, ppl=64.22, wps=24784.8, ups=0.38, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.542, loss_scale=16, train_wall=243, gb_free=9.6, wall=61983
2022-03-16 09:47:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:52:13 | INFO | train_inner | epoch 059:    214 / 392 loss=6.023, ppl=65.02, wps=24781.8, ups=0.38, wpb=65532.7, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.549, loss_scale=8, train_wall=243, gb_free=9.6, wall=62247
2022-03-16 09:55:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 09:56:38 | INFO | train_inner | epoch 059:    315 / 392 loss=6.036, ppl=65.62, wps=24751, ups=0.38, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.543, loss_scale=8, train_wall=243, gb_free=9.6, wall=62512
2022-03-16 09:59:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:00:32 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 6.251 | ppl 76.16 | wps 37921.9 | wpb 511.9 | bsz 1 | num_updates 22977 | best_loss 6.251
2022-03-16 10:00:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 22977 updates
2022-03-16 10:00:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 10:00:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 10:00:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 59 @ 22977 updates, score 6.251) (writing took 1.8858879385516047 seconds)
2022-03-16 10:00:34 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-16 10:00:34 | INFO | train | epoch 059 | loss 6.024 | ppl 65.06 | wps 23986.4 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 22977 | lr 0.000208619 | gnorm 0.547 | loss_scale 8 | train_wall 941 | gb_free 9.6 | wall 62747
KL Stats: Epoch 59 Divergences: Uniform: 3.9053789434612827 Unigram: 3.49337855540183
2022-03-16 10:00:34 | INFO | fairseq.trainer | begin training epoch 60
2022-03-16 10:00:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:01:34 | INFO | train_inner | epoch 060:     23 / 392 loss=6.024, ppl=65.09, wps=21980.9, ups=0.34, wpb=65029.1, bsz=127, num_updates=23000, lr=0.000208514, gnorm=0.553, loss_scale=16, train_wall=239, gb_free=9.6, wall=62808
2022-03-16 10:05:56 | INFO | train_inner | epoch 060:    123 / 392 loss=6, ppl=64, wps=25020.9, ups=0.38, wpb=65532.7, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.549, loss_scale=16, train_wall=240, gb_free=9.6, wall=63070
2022-03-16 10:07:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:10:20 | INFO | train_inner | epoch 060:    224 / 392 loss=6.023, ppl=65.01, wps=24791, ups=0.38, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.546, loss_scale=16, train_wall=243, gb_free=9.6, wall=63334
2022-03-16 10:13:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:13:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:14:47 | INFO | train_inner | epoch 060:    326 / 392 loss=6.032, ppl=65.42, wps=24547.9, ups=0.37, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.549, loss_scale=8, train_wall=245, gb_free=9.6, wall=63601
2022-03-16 10:17:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:18:12 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 6.252 | ppl 76.21 | wps 38040.5 | wpb 511.9 | bsz 1 | num_updates 23366 | best_loss 6.251
2022-03-16 10:18:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 23366 updates
2022-03-16 10:18:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 10:18:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 10:18:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt (epoch 60 @ 23366 updates, score 6.252) (writing took 0.8386346008628607 seconds)
2022-03-16 10:18:13 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-16 10:18:13 | INFO | train | epoch 060 | loss 6.019 | ppl 64.86 | wps 24023 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 23366 | lr 0.000206875 | gnorm 0.549 | loss_scale 8 | train_wall 940 | gb_free 9.6 | wall 63806
KL Stats: Epoch 60 Divergences: Uniform: 3.9098372314426704 Unigram: 3.497102462596627
2022-03-16 10:18:13 | INFO | fairseq.trainer | begin training epoch 61
2022-03-16 10:18:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:19:42 | INFO | train_inner | epoch 061:     34 / 392 loss=6.016, ppl=64.7, wps=22065.1, ups=0.34, wpb=65029.1, bsz=127, num_updates=23400, lr=0.000206725, gnorm=0.551, loss_scale=8, train_wall=239, gb_free=9.6, wall=63896
2022-03-16 10:21:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:24:07 | INFO | train_inner | epoch 061:    135 / 392 loss=5.997, ppl=63.87, wps=24760.8, ups=0.38, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.553, loss_scale=8, train_wall=243, gb_free=9.6, wall=64160
2022-03-16 10:28:29 | INFO | train_inner | epoch 061:    235 / 392 loss=6.015, ppl=64.66, wps=24993.8, ups=0.38, wpb=65532.7, bsz=128, num_updates=23600, lr=0.000205847, gnorm=0.551, loss_scale=16, train_wall=241, gb_free=9.6, wall=64423
2022-03-16 10:32:52 | INFO | train_inner | epoch 061:    335 / 392 loss=6.026, ppl=65.16, wps=24936.6, ups=0.38, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.549, loss_scale=16, train_wall=241, gb_free=9.6, wall=64685
2022-03-16 10:32:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 10:32:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:35:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:35:54 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 6.245 | ppl 75.85 | wps 37928.5 | wpb 511.9 | bsz 1 | num_updates 23755 | best_loss 6.245
2022-03-16 10:35:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 23755 updates
2022-03-16 10:35:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 10:35:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 10:35:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 61 @ 23755 updates, score 6.245) (writing took 1.8681188179180026 seconds)
2022-03-16 10:35:55 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-16 10:35:55 | INFO | train | epoch 061 | loss 6.014 | ppl 64.63 | wps 23943.2 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 23755 | lr 0.000205174 | gnorm 0.553 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 64869
KL Stats: Epoch 61 Divergences: Uniform: 3.9122012507501784 Unigram: 3.500188968390652
2022-03-16 10:35:55 | INFO | fairseq.trainer | begin training epoch 62
2022-03-16 10:35:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:37:54 | INFO | train_inner | epoch 062:     45 / 392 loss=6.021, ppl=64.92, wps=21537.6, ups=0.33, wpb=65029.1, bsz=127, num_updates=23800, lr=0.00020498, gnorm=0.557, loss_scale=8, train_wall=244, gb_free=9.6, wall=64987
2022-03-16 10:42:16 | INFO | train_inner | epoch 062:    145 / 392 loss=5.994, ppl=63.75, wps=24974.2, ups=0.38, wpb=65532.7, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.551, loss_scale=16, train_wall=241, gb_free=9.6, wall=65250
2022-03-16 10:44:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:46:41 | INFO | train_inner | epoch 062:    246 / 392 loss=6.007, ppl=64.31, wps=24727.2, ups=0.38, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.548, loss_scale=8, train_wall=243, gb_free=9.6, wall=65515
2022-03-16 10:51:03 | INFO | train_inner | epoch 062:    346 / 392 loss=6.028, ppl=65.23, wps=24978.7, ups=0.38, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.552, loss_scale=16, train_wall=241, gb_free=9.6, wall=65777
2022-03-16 10:52:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 10:53:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 10:53:36 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 6.241 | ppl 75.62 | wps 38039.8 | wpb 511.9 | bsz 1 | num_updates 24145 | best_loss 6.241
2022-03-16 10:53:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 24145 updates
2022-03-16 10:53:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 10:53:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 10:53:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 62 @ 24145 updates, score 6.241) (writing took 1.8428207701072097 seconds)
2022-03-16 10:53:38 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-16 10:53:38 | INFO | train | epoch 062 | loss 6.01 | ppl 64.44 | wps 24008.1 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 24145 | lr 0.00020351 | gnorm 0.552 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 65932
KL Stats: Epoch 62 Divergences: Uniform: 3.9138680782546844 Unigram: 3.500018303356204
2022-03-16 10:53:38 | INFO | fairseq.trainer | begin training epoch 63
2022-03-16 10:53:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 10:56:02 | INFO | train_inner | epoch 063:     55 / 392 loss=5.995, ppl=63.78, wps=21761.3, ups=0.33, wpb=65029.1, bsz=127, num_updates=24200, lr=0.000203279, gnorm=0.56, loss_scale=8, train_wall=241, gb_free=9.6, wall=66076
2022-03-16 11:00:24 | INFO | train_inner | epoch 063:    155 / 392 loss=5.994, ppl=63.75, wps=25015.5, ups=0.38, wpb=65532.7, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.549, loss_scale=16, train_wall=241, gb_free=9.6, wall=66338
2022-03-16 11:04:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:04:49 | INFO | train_inner | epoch 063:    256 / 392 loss=6.013, ppl=64.6, wps=24760.7, ups=0.38, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.55, loss_scale=16, train_wall=243, gb_free=9.6, wall=66603
2022-03-16 11:09:11 | INFO | train_inner | epoch 063:    356 / 392 loss=6.022, ppl=65, wps=24975, ups=0.38, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.548, loss_scale=16, train_wall=241, gb_free=9.6, wall=66865
2022-03-16 11:09:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 11:10:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:11:18 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 6.242 | ppl 75.67 | wps 37841.4 | wpb 511.9 | bsz 1 | num_updates 24535 | best_loss 6.241
2022-03-16 11:11:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 24535 updates
2022-03-16 11:11:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 11:11:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 11:11:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt (epoch 63 @ 24535 updates, score 6.242) (writing took 0.8535992680117488 seconds)
2022-03-16 11:11:19 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-16 11:11:19 | INFO | train | epoch 063 | loss 6.005 | ppl 64.22 | wps 24043.6 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 24535 | lr 0.000201886 | gnorm 0.552 | loss_scale 8 | train_wall 942 | gb_free 9.6 | wall 66992
KL Stats: Epoch 63 Divergences: Uniform: 3.9189387473226245 Unigram: 3.50668952034703
2022-03-16 11:11:19 | INFO | fairseq.trainer | begin training epoch 64
2022-03-16 11:11:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:14:10 | INFO | train_inner | epoch 064:     65 / 392 loss=5.99, ppl=63.57, wps=21802.9, ups=0.34, wpb=65025.8, bsz=127, num_updates=24600, lr=0.000201619, gnorm=0.571, loss_scale=8, train_wall=242, gb_free=9.6, wall=67163
2022-03-16 11:17:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 11:18:34 | INFO | train_inner | epoch 064:    166 / 392 loss=5.982, ppl=63.21, wps=24747.1, ups=0.38, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.562, loss_scale=8, train_wall=243, gb_free=9.6, wall=67428
2022-03-16 11:22:56 | INFO | train_inner | epoch 064:    266 / 392 loss=6.008, ppl=64.35, wps=25008.4, ups=0.38, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.557, loss_scale=16, train_wall=241, gb_free=9.6, wall=67690
2022-03-16 11:27:19 | INFO | train_inner | epoch 064:    366 / 392 loss=6.019, ppl=64.84, wps=25003, ups=0.38, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.547, loss_scale=16, train_wall=241, gb_free=9.6, wall=67952
2022-03-16 11:28:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:28:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:28:59 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 6.237 | ppl 75.42 | wps 37898.2 | wpb 511.9 | bsz 1 | num_updates 24925 | best_loss 6.237
2022-03-16 11:28:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 24925 updates
2022-03-16 11:28:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 11:28:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 11:29:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 64 @ 24925 updates, score 6.237) (writing took 1.8440492199733853 seconds)
2022-03-16 11:29:00 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-16 11:29:00 | INFO | train | epoch 064 | loss 6 | ppl 63.98 | wps 24025.3 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 24925 | lr 0.000200301 | gnorm 0.559 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 68054
KL Stats: Epoch 64 Divergences: Uniform: 3.9211473917243675 Unigram: 3.509026872247357
2022-03-16 11:29:01 | INFO | fairseq.trainer | begin training epoch 65
2022-03-16 11:29:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:32:17 | INFO | train_inner | epoch 065:     75 / 392 loss=5.986, ppl=63.4, wps=21799.4, ups=0.34, wpb=65029.1, bsz=127, num_updates=25000, lr=0.0002, gnorm=0.556, loss_scale=16, train_wall=241, gb_free=9.6, wall=68251
2022-03-16 11:34:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:36:41 | INFO | train_inner | epoch 065:    176 / 392 loss=5.986, ppl=63.38, wps=24763.6, ups=0.38, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.547, loss_scale=16, train_wall=243, gb_free=9.6, wall=68515
2022-03-16 11:40:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:41:06 | INFO | train_inner | epoch 065:    277 / 392 loss=5.998, ppl=63.93, wps=24733, ups=0.38, wpb=65536, bsz=128, num_updates=25200, lr=0.000199205, gnorm=0.551, loss_scale=16, train_wall=243, gb_free=9.6, wall=68780
2022-03-16 11:41:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 11:45:31 | INFO | train_inner | epoch 065:    378 / 392 loss=6.018, ppl=64.8, wps=24743.6, ups=0.38, wpb=65532.7, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.553, loss_scale=8, train_wall=243, gb_free=9.6, wall=69045
2022-03-16 11:46:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 11:46:40 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 6.238 | ppl 75.48 | wps 37917.1 | wpb 511.9 | bsz 1 | num_updates 25314 | best_loss 6.237
2022-03-16 11:46:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 25314 updates
2022-03-16 11:46:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 11:46:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 11:46:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt (epoch 65 @ 25314 updates, score 6.238) (writing took 0.9249934498220682 seconds)
2022-03-16 11:46:41 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-16 11:46:41 | INFO | train | epoch 065 | loss 5.996 | ppl 63.82 | wps 23992.4 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 25314 | lr 0.000198756 | gnorm 0.552 | loss_scale 8 | train_wall 942 | gb_free 9.6 | wall 69115
KL Stats: Epoch 65 Divergences: Uniform: 3.9244041299951293 Unigram: 3.511359797299613
2022-03-16 11:46:41 | INFO | fairseq.trainer | begin training epoch 66
2022-03-16 11:46:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 11:50:27 | INFO | train_inner | epoch 066:     86 / 392 loss=5.973, ppl=62.81, wps=22025.5, ups=0.34, wpb=65029.1, bsz=127, num_updates=25400, lr=0.000198419, gnorm=0.563, loss_scale=16, train_wall=239, gb_free=9.6, wall=69340
2022-03-16 11:53:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 11:54:51 | INFO | train_inner | epoch 066:    187 / 392 loss=5.989, ppl=63.49, wps=24738, ups=0.38, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.554, loss_scale=16, train_wall=243, gb_free=9.6, wall=69605
2022-03-16 11:57:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 11:59:16 | INFO | train_inner | epoch 066:    288 / 392 loss=6.002, ppl=64.07, wps=24737.3, ups=0.38, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.551, loss_scale=8, train_wall=243, gb_free=9.6, wall=69870
2022-03-16 12:03:39 | INFO | train_inner | epoch 066:    388 / 392 loss=6.002, ppl=64.09, wps=24983, ups=0.38, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.546, loss_scale=16, train_wall=241, gb_free=9.6, wall=70132
2022-03-16 12:03:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:04:21 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 6.236 | ppl 75.39 | wps 37886.2 | wpb 511.9 | bsz 1 | num_updates 25704 | best_loss 6.236
2022-03-16 12:04:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 25704 updates
2022-03-16 12:04:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 12:04:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 12:04:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 66 @ 25704 updates, score 6.236) (writing took 2.022315990179777 seconds)
2022-03-16 12:04:23 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-16 12:04:23 | INFO | train | epoch 066 | loss 5.991 | ppl 63.6 | wps 24010 | ups 0.37 | wpb 65405.2 | bsz 127.7 | num_updates 25704 | lr 0.000197242 | gnorm 0.553 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 70177
KL Stats: Epoch 66 Divergences: Uniform: 3.926351451180253 Unigram: 3.5136956679067524
2022-03-16 12:04:23 | INFO | fairseq.trainer | begin training epoch 67
2022-03-16 12:04:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:06:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 12:08:38 | INFO | train_inner | epoch 067:     97 / 392 loss=5.965, ppl=62.48, wps=21726.9, ups=0.33, wpb=65025.8, bsz=127, num_updates=25800, lr=0.000196875, gnorm=0.558, loss_scale=8, train_wall=241, gb_free=9.6, wall=70432
2022-03-16 12:13:01 | INFO | train_inner | epoch 067:    197 / 392 loss=5.983, ppl=63.25, wps=24959.9, ups=0.38, wpb=65532.7, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.555, loss_scale=16, train_wall=241, gb_free=9.6, wall=70694
2022-03-16 12:17:23 | INFO | train_inner | epoch 067:    297 / 392 loss=5.996, ppl=63.82, wps=24965.7, ups=0.38, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.553, loss_scale=16, train_wall=241, gb_free=9.6, wall=70957
2022-03-16 12:17:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:18:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 12:21:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:22:04 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 6.226 | ppl 74.84 | wps 37994.5 | wpb 511.9 | bsz 1 | num_updates 26093 | best_loss 6.226
2022-03-16 12:22:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 26093 updates
2022-03-16 12:22:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 12:22:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt
2022-03-16 12:22:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_best.pt (epoch 67 @ 26093 updates, score 6.226) (writing took 1.8979929760098457 seconds)
2022-03-16 12:22:06 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-16 12:22:06 | INFO | train | epoch 067 | loss 5.987 | ppl 63.42 | wps 23939.3 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 26093 | lr 0.000195766 | gnorm 0.556 | loss_scale 8 | train_wall 943 | gb_free 9.6 | wall 71240
KL Stats: Epoch 67 Divergences: Uniform: 3.929524651236724 Unigram: 3.515588105357023
2022-03-16 12:22:06 | INFO | fairseq.trainer | begin training epoch 68
2022-03-16 12:22:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:22:25 | INFO | train_inner | epoch 068:      7 / 392 loss=6.001, ppl=64.04, wps=21567.2, ups=0.33, wpb=65029.1, bsz=127, num_updates=26100, lr=0.00019574, gnorm=0.567, loss_scale=8, train_wall=244, gb_free=9.6, wall=71258
2022-03-16 12:26:47 | INFO | train_inner | epoch 068:    107 / 392 loss=5.963, ppl=62.37, wps=24948.7, ups=0.38, wpb=65532.7, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.554, loss_scale=16, train_wall=241, gb_free=9.6, wall=71521
2022-03-16 12:28:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 12:31:12 | INFO | train_inner | epoch 068:    208 / 392 loss=5.979, ppl=63.1, wps=24734.6, ups=0.38, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.548, loss_scale=8, train_wall=243, gb_free=9.6, wall=71786
2022-03-16 12:35:34 | INFO | train_inner | epoch 068:    308 / 392 loss=5.986, ppl=63.39, wps=24996.7, ups=0.38, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.554, loss_scale=16, train_wall=241, gb_free=9.6, wall=72048
2022-03-16 12:39:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:39:46 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 6.235 | ppl 75.33 | wps 38068.2 | wpb 511.9 | bsz 1 | num_updates 26484 | best_loss 6.226
2022-03-16 12:39:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 26484 updates
2022-03-16 12:39:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 12:39:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 12:39:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt (epoch 68 @ 26484 updates, score 6.235) (writing took 0.9950490025803447 seconds)
2022-03-16 12:39:47 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-16 12:39:47 | INFO | train | epoch 068 | loss 5.983 | ppl 63.24 | wps 24094.8 | ups 0.37 | wpb 65405.5 | bsz 127.7 | num_updates 26484 | lr 0.000194316 | gnorm 0.555 | loss_scale 16 | train_wall 943 | gb_free 9.6 | wall 72301
KL Stats: Epoch 68 Divergences: Uniform: 3.933711902715982 Unigram: 3.5221815819395865
2022-03-16 12:39:48 | INFO | fairseq.trainer | begin training epoch 69
2022-03-16 12:39:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:40:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:40:32 | INFO | train_inner | epoch 069:     17 / 392 loss=5.998, ppl=63.9, wps=21830.3, ups=0.34, wpb=65029.1, bsz=127, num_updates=26500, lr=0.000194257, gnorm=0.559, loss_scale=16, train_wall=241, gb_free=9.6, wall=72346
2022-03-16 12:44:55 | INFO | train_inner | epoch 069:    117 / 392 loss=5.959, ppl=62.22, wps=24981.4, ups=0.38, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.552, loss_scale=16, train_wall=241, gb_free=9.6, wall=72608
2022-03-16 12:46:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:49:19 | INFO | train_inner | epoch 069:    218 / 392 loss=5.973, ppl=62.81, wps=24760.5, ups=0.38, wpb=65536, bsz=128, num_updates=26700, lr=0.000193528, gnorm=0.546, loss_scale=16, train_wall=243, gb_free=9.6, wall=72873
2022-03-16 12:51:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 12:53:44 | INFO | train_inner | epoch 069:    319 / 392 loss=5.992, ppl=63.65, wps=24756.7, ups=0.38, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.559, loss_scale=16, train_wall=243, gb_free=9.6, wall=73138
2022-03-16 12:56:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 12:57:27 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 6.23 | ppl 75.05 | wps 37887.1 | wpb 511.9 | bsz 1 | num_updates 26873 | best_loss 6.226
2022-03-16 12:57:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 26873 updates
2022-03-16 12:57:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 12:57:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 12:57:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt (epoch 69 @ 26873 updates, score 6.23) (writing took 0.8515626005828381 seconds)
2022-03-16 12:57:28 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-16 12:57:28 | INFO | train | epoch 069 | loss 5.978 | ppl 63.04 | wps 23994.1 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 26873 | lr 0.000192904 | gnorm 0.554 | loss_scale 16 | train_wall 942 | gb_free 9.6 | wall 73362
KL Stats: Epoch 69 Divergences: Uniform: 3.935986323728636 Unigram: 3.5241581929440398
2022-03-16 12:57:28 | INFO | fairseq.trainer | begin training epoch 70
2022-03-16 12:57:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-16 12:57:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 12:58:41 | INFO | train_inner | epoch 070:     28 / 392 loss=5.988, ppl=63.47, wps=21881.6, ups=0.34, wpb=65025.8, bsz=127, num_updates=26900, lr=0.000192807, gnorm=0.563, loss_scale=8, train_wall=241, gb_free=9.6, wall=73435
2022-03-16 13:03:03 | INFO | train_inner | epoch 070:    128 / 392 loss=5.96, ppl=62.27, wps=25030.3, ups=0.38, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.556, loss_scale=8, train_wall=240, gb_free=9.6, wall=73697
2022-03-16 13:07:25 | INFO | train_inner | epoch 070:    228 / 392 loss=5.978, ppl=63.04, wps=25023.9, ups=0.38, wpb=65536, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.556, loss_scale=16, train_wall=241, gb_free=9.6, wall=73959
2022-03-16 13:09:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-16 13:09:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-16 13:11:52 | INFO | train_inner | epoch 070:    330 / 392 loss=5.985, ppl=63.32, wps=24513.7, ups=0.37, wpb=65532.7, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.56, loss_scale=8, train_wall=246, gb_free=9.6, wall=74226
2022-03-16 13:14:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-16 13:15:07 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 6.228 | ppl 74.95 | wps 37952.2 | wpb 511.9 | bsz 1 | num_updates 27262 | best_loss 6.226
2022-03-16 13:15:07 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-16 13:15:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 27262 updates
2022-03-16 13:15:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 13:15:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt
2022-03-16 13:15:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_dropout_0.3_#1_jelinek_0.04_0.01_0.95/checkpoint_last.pt (epoch 70 @ 27262 updates, score 6.228) (writing took 0.9151177294552326 seconds)
2022-03-16 13:15:08 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-16 13:15:08 | INFO | train | epoch 070 | loss 5.974 | ppl 62.87 | wps 24008.4 | ups 0.37 | wpb 65404.8 | bsz 127.7 | num_updates 27262 | lr 0.000191523 | gnorm 0.559 | loss_scale 8 | train_wall 941 | gb_free 9.6 | wall 74421
2022-03-16 13:15:08 | INFO | fairseq_cli.train | done training in 74421.1 seconds
