Sender: LSF System <lsfadmin@eu-g3-045>
Subject: Job 207129665: <w103_size_0.125_fp16_cross_entropy_#1> in cluster <euler> Exited

Job <w103_size_0.125_fp16_cross_entropy_#1> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:14:08 2022
Job was executed on host(s) <eu-g3-045>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 09:14:13 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 09:14:13 2022
Terminated at Sat Mar  5 11:25:20 2022
Results reported at Sat Mar  5 11:25:20 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   94184.45 sec.
    Max Memory :                                 7429 MB
    Average Memory :                             1853.57 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               12571.00 MB
    Max Swap :                                   2647 MB
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   94266 sec.
    Turnaround time :                            94272 sec.

The output (if any) follows:

2022-03-04 09:14:26 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 09:14:27 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-04 09:14:29 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-04 09:14:29 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 09:14:29 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 09:14:29 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-04 09:14:29 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-04 09:14:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 09:14:29 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-04 09:14:32 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 09:14:32 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:14:32 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-04 09:14:32 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 09:14:32 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 09:14:32 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 09:14:32 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 09:14:32 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 09:14:32 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 09:14:32 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-04 09:14:32 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 09:14:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:14:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 09:14:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:14:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 09:14:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:17:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-04 09:20:22 | INFO | train_inner | epoch 001:    105 / 196 loss=16.35, ppl=83507.2, wps=20442.1, ups=0.31, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.549, loss_scale=4, train_wall=326, gb_free=7.2, wall=350
2022-03-04 09:25:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:25:15 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.007 | ppl 8233.62 | wps 41793.3 | wpb 510.9 | bsz 1 | num_updates 191
2022-03-04 09:25:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-04 09:25:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 09:25:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 09:25:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.007) (writing took 6.576285258866847 seconds)
2022-03-04 09:25:22 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 09:25:22 | INFO | train | epoch 001 | loss 15.246 | ppl 38857.9 | wps 20137.6 | ups 0.31 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.606 | loss_scale 8 | train_wall 594 | gb_free 7.2 | wall 650
2022-03-04 09:25:22 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 09:25:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:25:51 | INFO | train_inner | epoch 002:      9 / 196 loss=13.937, ppl=15688.4, wps=19885, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.544, loss_scale=8, train_wall=295, gb_free=7.2, wall=679
2022-03-04 09:31:08 | INFO | train_inner | epoch 002:    109 / 196 loss=11.96, ppl=3985.13, wps=20658.9, ups=0.32, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.963, loss_scale=16, train_wall=295, gb_free=7.2, wall=996
2022-03-04 09:35:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:35:48 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.315 | ppl 1273.71 | wps 41955.2 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 10.315
2022-03-04 09:35:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-04 09:35:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 09:35:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 09:35:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 2 @ 387 updates, score 10.315) (writing took 6.515595591627061 seconds)
2022-03-04 09:35:55 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 09:35:55 | INFO | train | epoch 002 | loss 11.436 | ppl 2771.33 | wps 20269 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.822 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 1283
2022-03-04 09:35:55 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 09:35:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:36:36 | INFO | train_inner | epoch 003:     13 / 196 loss=10.618, ppl=1571.07, wps=19902.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.592, loss_scale=16, train_wall=295, gb_free=7.2, wall=1324
2022-03-04 09:41:55 | INFO | train_inner | epoch 003:    113 / 196 loss=10.022, ppl=1039.52, wps=20587.3, ups=0.31, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.506, loss_scale=32, train_wall=296, gb_free=7.2, wall=1643
2022-03-04 09:44:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 09:46:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:46:23 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.577 | ppl 763.95 | wps 41541.8 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 9.577
2022-03-04 09:46:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 582 updates
2022-03-04 09:46:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 09:46:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 09:46:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 3 @ 582 updates, score 9.577) (writing took 6.42107398621738 seconds)
2022-03-04 09:46:30 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 09:46:30 | INFO | train | epoch 003 | loss 9.908 | ppl 961.01 | wps 20107.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 582 | lr 7.28355e-05 | gnorm 0.539 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 1918
2022-03-04 09:46:30 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 09:46:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:47:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 09:47:30 | INFO | train_inner | epoch 004:     19 / 196 loss=9.678, ppl=819.42, wps=19485.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.61, loss_scale=16, train_wall=301, gb_free=7.2, wall=1978
2022-03-04 09:52:48 | INFO | train_inner | epoch 004:    119 / 196 loss=9.38, ppl=666.06, wps=20591.9, ups=0.31, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.624, loss_scale=16, train_wall=296, gb_free=7.2, wall=2297
2022-03-04 09:56:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:56:58 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.099 | ppl 548.48 | wps 41516.9 | wpb 510.9 | bsz 1 | num_updates 777 | best_loss 9.099
2022-03-04 09:56:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 777 updates
2022-03-04 09:56:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 09:57:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 09:57:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 4 @ 777 updates, score 9.099) (writing took 6.517030988819897 seconds)
2022-03-04 09:57:04 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 09:57:04 | INFO | train | epoch 004 | loss 9.315 | ppl 636.82 | wps 20103.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 777 | lr 9.72056e-05 | gnorm 0.681 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 2553
2022-03-04 09:57:04 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 09:57:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:58:18 | INFO | train_inner | epoch 005:     23 / 196 loss=9.144, ppl=565.84, wps=19847.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.758, loss_scale=32, train_wall=295, gb_free=7.2, wall=2626
2022-03-04 10:01:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:02:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 10:03:43 | INFO | train_inner | epoch 005:    125 / 196 loss=8.885, ppl=472.73, wps=20169.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.931, loss_scale=16, train_wall=302, gb_free=7.2, wall=2951
2022-03-04 10:07:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:07:33 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.697 | ppl 415.1 | wps 41335.7 | wpb 510.9 | bsz 1 | num_updates 971 | best_loss 8.697
2022-03-04 10:07:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 971 updates
2022-03-04 10:07:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 10:07:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 10:07:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 5 @ 971 updates, score 8.697) (writing took 6.707141174003482 seconds)
2022-03-04 10:07:40 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 10:07:40 | INFO | train | epoch 005 | loss 8.843 | ppl 459.29 | wps 19984.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 971 | lr 0.000121451 | gnorm 0.885 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 3188
2022-03-04 10:07:40 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 10:07:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:09:12 | INFO | train_inner | epoch 006:     29 / 196 loss=8.683, ppl=410.95, wps=19834.6, ups=0.3, wpb=65367, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.89, loss_scale=16, train_wall=295, gb_free=7.2, wall=3280
2022-03-04 10:14:31 | INFO | train_inner | epoch 006:    129 / 196 loss=8.477, ppl=356.38, wps=20568.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.938, loss_scale=32, train_wall=296, gb_free=7.2, wall=3599
2022-03-04 10:17:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:18:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:18:09 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.37 | ppl 330.95 | wps 41259.8 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 8.37
2022-03-04 10:18:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1166 updates
2022-03-04 10:18:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 10:18:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 10:18:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 6 @ 1166 updates, score 8.37) (writing took 6.547104292549193 seconds)
2022-03-04 10:18:15 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 10:18:15 | INFO | train | epoch 006 | loss 8.451 | ppl 349.87 | wps 20088.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1166 | lr 0.000145821 | gnorm 0.947 | loss_scale 32 | train_wall 580 | gb_free 7.2 | wall 3823
2022-03-04 10:18:15 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 10:18:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:20:03 | INFO | train_inner | epoch 007:     34 / 196 loss=8.31, ppl=317.44, wps=19654.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.97, loss_scale=32, train_wall=298, gb_free=7.2, wall=3932
2022-03-04 10:24:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:25:25 | INFO | train_inner | epoch 007:    135 / 196 loss=8.138, ppl=281.61, wps=20378, ups=0.31, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.968, loss_scale=32, train_wall=299, gb_free=7.2, wall=4253
2022-03-04 10:28:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:28:44 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.1 | ppl 274.32 | wps 41434.5 | wpb 510.9 | bsz 1 | num_updates 1361 | best_loss 8.1
2022-03-04 10:28:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1361 updates
2022-03-04 10:28:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 10:28:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 10:28:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 7 @ 1361 updates, score 8.1) (writing took 6.58151816483587 seconds)
2022-03-04 10:28:50 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 10:28:50 | INFO | train | epoch 007 | loss 8.118 | ppl 277.83 | wps 20094.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1361 | lr 0.000170191 | gnorm 0.988 | loss_scale 32 | train_wall 580 | gb_free 7.2 | wall 4458
2022-03-04 10:28:50 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 10:28:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:30:54 | INFO | train_inner | epoch 008:     39 / 196 loss=7.978, ppl=252.07, wps=19838.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.963, loss_scale=32, train_wall=295, gb_free=7.2, wall=4583
2022-03-04 10:31:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:36:16 | INFO | train_inner | epoch 008:    140 / 196 loss=7.831, ppl=227.65, wps=20383.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=1500, lr=0.000187563, gnorm=1.02, loss_scale=32, train_wall=299, gb_free=7.2, wall=4904
2022-03-04 10:39:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:39:19 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.864 | ppl 232.96 | wps 41456.8 | wpb 510.9 | bsz 1 | num_updates 1556 | best_loss 7.864
2022-03-04 10:39:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1556 updates
2022-03-04 10:39:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 10:39:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 10:39:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 8 @ 1556 updates, score 7.864) (writing took 6.431986473500729 seconds)
2022-03-04 10:39:25 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 10:39:25 | INFO | train | epoch 008 | loss 7.82 | ppl 225.96 | wps 20094.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1556 | lr 0.000194561 | gnorm 0.973 | loss_scale 64 | train_wall 580 | gb_free 7.2 | wall 5093
2022-03-04 10:39:25 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 10:39:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:40:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:41:49 | INFO | train_inner | epoch 009:     45 / 196 loss=7.681, ppl=205.28, wps=19652.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.971, loss_scale=32, train_wall=298, gb_free=7.2, wall=5237
2022-03-04 10:47:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:47:10 | INFO | train_inner | epoch 009:    146 / 196 loss=7.543, ppl=186.55, wps=20386.9, ups=0.31, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.982, loss_scale=32, train_wall=299, gb_free=7.2, wall=5558
2022-03-04 10:49:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:49:54 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.657 | ppl 201.87 | wps 41137.2 | wpb 510.9 | bsz 1 | num_updates 1750 | best_loss 7.657
2022-03-04 10:49:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1750 updates
2022-03-04 10:49:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 10:49:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 10:50:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 9 @ 1750 updates, score 7.657) (writing took 6.5551573652774096 seconds)
2022-03-04 10:50:00 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 10:50:00 | INFO | train | epoch 009 | loss 7.543 | ppl 186.44 | wps 19994.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1750 | lr 0.000218806 | gnorm 0.986 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 5728
2022-03-04 10:50:00 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 10:50:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:52:39 | INFO | train_inner | epoch 010:     50 / 196 loss=7.413, ppl=170.47, wps=19838.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.976, loss_scale=32, train_wall=295, gb_free=7.2, wall=5888
2022-03-04 10:54:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 10:58:01 | INFO | train_inner | epoch 010:    151 / 196 loss=7.284, ppl=155.82, wps=20377.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.968, loss_scale=32, train_wall=299, gb_free=7.2, wall=6209
2022-03-04 11:00:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:00:29 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.477 | ppl 178.21 | wps 41262.2 | wpb 510.9 | bsz 1 | num_updates 1945 | best_loss 7.477
2022-03-04 11:00:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1945 updates
2022-03-04 11:00:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 11:00:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 11:00:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 10 @ 1945 updates, score 7.477) (writing took 6.614152016118169 seconds)
2022-03-04 11:00:35 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 11:00:35 | INFO | train | epoch 010 | loss 7.283 | ppl 155.76 | wps 20092.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1945 | lr 0.000243176 | gnorm 0.962 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 6364
2022-03-04 11:00:35 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 11:00:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:01:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:03:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:03:37 | INFO | train_inner | epoch 011:     57 / 196 loss=7.14, ppl=141.04, wps=19453.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.95, loss_scale=16, train_wall=301, gb_free=7.2, wall=6545
2022-03-04 11:08:55 | INFO | train_inner | epoch 011:    157 / 196 loss=7.039, ppl=131.54, wps=20583.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.944, loss_scale=16, train_wall=296, gb_free=7.2, wall=6864
2022-03-04 11:10:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:11:04 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.337 | ppl 161.63 | wps 41421.8 | wpb 510.9 | bsz 1 | num_updates 2139 | best_loss 7.337
2022-03-04 11:11:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2139 updates
2022-03-04 11:11:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 11:11:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 11:11:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 11 @ 2139 updates, score 7.337) (writing took 6.542736362665892 seconds)
2022-03-04 11:11:11 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 11:11:11 | INFO | train | epoch 011 | loss 7.041 | ppl 131.72 | wps 19986.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 2139 | lr 0.000267422 | gnorm 0.953 | loss_scale 32 | train_wall 580 | gb_free 7.2 | wall 6999
2022-03-04 11:11:11 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 11:11:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:14:25 | INFO | train_inner | epoch 012:     61 / 196 loss=6.909, ppl=120.21, wps=19844.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.938, loss_scale=32, train_wall=295, gb_free=7.2, wall=7193
2022-03-04 11:15:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:19:46 | INFO | train_inner | epoch 012:    162 / 196 loss=6.808, ppl=112.02, wps=20389.3, ups=0.31, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.943, loss_scale=16, train_wall=299, gb_free=7.2, wall=7514
2022-03-04 11:21:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:21:39 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.204 | ppl 147.47 | wps 41476.8 | wpb 510.9 | bsz 1 | num_updates 2334 | best_loss 7.204
2022-03-04 11:21:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2334 updates
2022-03-04 11:21:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 11:21:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 11:21:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 12 @ 2334 updates, score 7.204) (writing took 6.437575550749898 seconds)
2022-03-04 11:21:45 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 11:21:45 | INFO | train | epoch 012 | loss 6.817 | ppl 112.71 | wps 20109 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2334 | lr 0.000291792 | gnorm 0.921 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 7634
2022-03-04 11:21:45 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 11:21:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:25:16 | INFO | train_inner | epoch 013:     66 / 196 loss=6.671, ppl=101.89, wps=19845.9, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.902, loss_scale=32, train_wall=295, gb_free=7.2, wall=7844
2022-03-04 11:30:34 | INFO | train_inner | epoch 013:    166 / 196 loss=6.613, ppl=97.87, wps=20580, ups=0.31, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.924, loss_scale=64, train_wall=296, gb_free=7.2, wall=8162
2022-03-04 11:30:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:32:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:32:14 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.109 | ppl 138.07 | wps 41395.1 | wpb 510.9 | bsz 1 | num_updates 2529 | best_loss 7.109
2022-03-04 11:32:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2529 updates
2022-03-04 11:32:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 11:32:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 11:32:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 13 @ 2529 updates, score 7.109) (writing took 6.578097285702825 seconds)
2022-03-04 11:32:21 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 11:32:21 | INFO | train | epoch 013 | loss 6.608 | ppl 97.54 | wps 20086.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2529 | lr 0.000316162 | gnorm 0.913 | loss_scale 32 | train_wall 580 | gb_free 7.2 | wall 8269
2022-03-04 11:32:21 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 11:32:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:36:07 | INFO | train_inner | epoch 014:     71 / 196 loss=6.466, ppl=88.4, wps=19643.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.892, loss_scale=32, train_wall=298, gb_free=7.2, wall=8495
2022-03-04 11:37:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:41:28 | INFO | train_inner | epoch 014:    172 / 196 loss=6.41, ppl=85.03, wps=20380, ups=0.31, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.899, loss_scale=32, train_wall=299, gb_free=7.2, wall=8817
2022-03-04 11:42:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:42:49 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.036 | ppl 131.26 | wps 41430.2 | wpb 510.9 | bsz 1 | num_updates 2724 | best_loss 7.036
2022-03-04 11:42:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2724 updates
2022-03-04 11:42:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 11:42:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 11:42:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 14 @ 2724 updates, score 7.036) (writing took 6.670683939009905 seconds)
2022-03-04 11:42:56 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 11:42:56 | INFO | train | epoch 014 | loss 6.415 | ppl 85.33 | wps 20087.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2724 | lr 0.000340532 | gnorm 0.907 | loss_scale 32 | train_wall 580 | gb_free 7.2 | wall 8904
2022-03-04 11:42:56 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 11:42:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:45:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 11:47:01 | INFO | train_inner | epoch 015:     77 / 196 loss=6.271, ppl=77.22, wps=19640.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.901, loss_scale=32, train_wall=298, gb_free=7.2, wall=9149
2022-03-04 11:47:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:52:23 | INFO | train_inner | epoch 015:    178 / 196 loss=6.242, ppl=75.69, wps=20390.6, ups=0.31, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.91, loss_scale=16, train_wall=299, gb_free=7.2, wall=9471
2022-03-04 11:53:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:53:24 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.013 | ppl 129.17 | wps 41496.2 | wpb 510.9 | bsz 1 | num_updates 2918 | best_loss 7.013
2022-03-04 11:53:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2918 updates
2022-03-04 11:53:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 11:53:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 11:53:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 15 @ 2918 updates, score 7.013) (writing took 6.627130287699401 seconds)
2022-03-04 11:53:31 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 11:53:31 | INFO | train | epoch 015 | loss 6.237 | ppl 75.45 | wps 19997.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 2918 | lr 0.000364777 | gnorm 0.908 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 9539
2022-03-04 11:53:31 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 11:53:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:57:52 | INFO | train_inner | epoch 016:     82 / 196 loss=6.1, ppl=68.59, wps=19834.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.892, loss_scale=32, train_wall=295, gb_free=7.2, wall=9800
2022-03-04 12:01:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:03:14 | INFO | train_inner | epoch 016:    183 / 196 loss=6.077, ppl=67.51, wps=20378.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.881, loss_scale=32, train_wall=299, gb_free=7.2, wall=10122
2022-03-04 12:03:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:04:00 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.966 | ppl 125.01 | wps 41306 | wpb 510.9 | bsz 1 | num_updates 3113 | best_loss 6.966
2022-03-04 12:04:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3113 updates
2022-03-04 12:04:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 12:04:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 12:04:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 16 @ 3113 updates, score 6.966) (writing took 6.575332071632147 seconds)
2022-03-04 12:04:06 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 12:04:06 | INFO | train | epoch 016 | loss 6.072 | ppl 67.26 | wps 20090.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3113 | lr 0.000389147 | gnorm 0.876 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 10174
2022-03-04 12:04:06 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 12:04:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:08:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:08:46 | INFO | train_inner | epoch 017:     88 / 196 loss=5.922, ppl=60.62, wps=19647.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.882, loss_scale=32, train_wall=298, gb_free=7.2, wall=10455
2022-03-04 12:12:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:14:08 | INFO | train_inner | epoch 017:    189 / 196 loss=5.929, ppl=60.91, wps=20386.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.878, loss_scale=16, train_wall=299, gb_free=7.2, wall=10776
2022-03-04 12:14:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:14:35 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.944 | ppl 123.14 | wps 41410.7 | wpb 510.9 | bsz 1 | num_updates 3307 | best_loss 6.944
2022-03-04 12:14:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3307 updates
2022-03-04 12:14:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 12:14:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt
2022-03-04 12:14:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_best.pt (epoch 17 @ 3307 updates, score 6.944) (writing took 6.567687465809286 seconds)
2022-03-04 12:14:41 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 12:14:41 | INFO | train | epoch 017 | loss 5.917 | ppl 60.42 | wps 19995 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 3307 | lr 0.000413392 | gnorm 0.881 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 10809
2022-03-04 12:14:41 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 12:14:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:19:37 | INFO | train_inner | epoch 018:     93 / 196 loss=5.769, ppl=54.51, wps=19838.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.886, loss_scale=32, train_wall=295, gb_free=7.2, wall=11106
2022-03-04 12:24:56 | INFO | train_inner | epoch 018:    193 / 196 loss=5.784, ppl=55.09, wps=20574.6, ups=0.31, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.869, loss_scale=32, train_wall=296, gb_free=7.2, wall=11424
2022-03-04 12:25:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:25:10 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.029 | ppl 130.59 | wps 41350.6 | wpb 510.9 | bsz 1 | num_updates 3503 | best_loss 6.944
2022-03-04 12:25:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3503 updates
2022-03-04 12:25:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 12:25:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 12:25:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 18 @ 3503 updates, score 7.029) (writing took 2.9782119123265147 seconds)
2022-03-04 12:25:13 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 12:25:13 | INFO | train | epoch 018 | loss 5.77 | ppl 54.56 | wps 20306.7 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 3503 | lr 0.000437887 | gnorm 0.886 | loss_scale 32 | train_wall 580 | gb_free 7.2 | wall 11441
2022-03-04 12:25:13 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 12:25:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:26:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:30:25 | INFO | train_inner | epoch 019:     98 / 196 loss=5.608, ppl=48.76, wps=19862.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.883, loss_scale=32, train_wall=298, gb_free=7.2, wall=11753
2022-03-04 12:31:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:35:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:35:41 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.978 | ppl 126.06 | wps 41317.3 | wpb 510.9 | bsz 1 | num_updates 3697 | best_loss 6.944
2022-03-04 12:35:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3697 updates
2022-03-04 12:35:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 12:35:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 12:35:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 19 @ 3697 updates, score 6.978) (writing took 3.0633401600643992 seconds)
2022-03-04 12:35:44 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 12:35:44 | INFO | train | epoch 019 | loss 5.628 | ppl 49.46 | wps 20106.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 3697 | lr 0.000462133 | gnorm 0.878 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 12073
2022-03-04 12:35:44 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 12:35:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:35:54 | INFO | train_inner | epoch 020:      3 / 196 loss=5.648, ppl=50.16, wps=19864, ups=0.3, wpb=65367, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.885, loss_scale=16, train_wall=298, gb_free=7.2, wall=12082
2022-03-04 12:41:12 | INFO | train_inner | epoch 020:    103 / 196 loss=5.469, ppl=44.3, wps=20595.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.873, loss_scale=32, train_wall=296, gb_free=7.2, wall=12400
2022-03-04 12:44:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 12:46:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:46:13 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.012 | ppl 129.08 | wps 41558 | wpb 510.9 | bsz 1 | num_updates 3892 | best_loss 6.944
2022-03-04 12:46:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3892 updates
2022-03-04 12:46:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 12:46:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 12:46:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 20 @ 3892 updates, score 7.012) (writing took 3.039151226170361 seconds)
2022-03-04 12:46:16 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 12:46:16 | INFO | train | epoch 020 | loss 5.498 | ppl 45.21 | wps 20212 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3892 | lr 0.000486503 | gnorm 0.889 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 12704
2022-03-04 12:46:16 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 12:46:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:46:41 | INFO | train_inner | epoch 021:      8 / 196 loss=5.513, ppl=45.65, wps=19856.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=3900, lr=0.000487503, gnorm=0.907, loss_scale=32, train_wall=298, gb_free=7.2, wall=12730
2022-03-04 12:48:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:52:03 | INFO | train_inner | epoch 021:    109 / 196 loss=5.346, ppl=40.67, wps=20395.4, ups=0.31, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.912, loss_scale=16, train_wall=299, gb_free=7.2, wall=13051
2022-03-04 12:56:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:56:44 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.065 | ppl 133.89 | wps 41264.6 | wpb 510.9 | bsz 1 | num_updates 4087 | best_loss 6.944
2022-03-04 12:56:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4087 updates
2022-03-04 12:56:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 12:56:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 12:56:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 21 @ 4087 updates, score 7.065) (writing took 3.060098009184003 seconds)
2022-03-04 12:56:47 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 12:56:47 | INFO | train | epoch 021 | loss 5.37 | ppl 41.35 | wps 20209.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4087 | lr 0.00049465 | gnorm 0.877 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 13336
2022-03-04 12:56:47 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 12:56:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:57:29 | INFO | train_inner | epoch 022:     13 / 196 loss=5.374, ppl=41.47, wps=20049, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.845, loss_scale=32, train_wall=295, gb_free=7.2, wall=13377
2022-03-04 12:59:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:02:50 | INFO | train_inner | epoch 022:    114 / 196 loss=5.212, ppl=37.05, wps=20379.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.852, loss_scale=16, train_wall=299, gb_free=7.2, wall=13699
2022-03-04 13:07:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:07:16 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.089 | ppl 136.11 | wps 41598.3 | wpb 510.9 | bsz 1 | num_updates 4282 | best_loss 6.944
2022-03-04 13:07:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4282 updates
2022-03-04 13:07:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 13:07:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 13:07:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 22 @ 4282 updates, score 7.089) (writing took 2.937864374369383 seconds)
2022-03-04 13:07:19 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 13:07:19 | INFO | train | epoch 022 | loss 5.235 | ppl 37.67 | wps 20213.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4282 | lr 0.000483255 | gnorm 0.86 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 13967
2022-03-04 13:07:19 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 13:07:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:08:16 | INFO | train_inner | epoch 023:     18 / 196 loss=5.229, ppl=37.5, wps=20066.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=4300, lr=0.000482243, gnorm=0.847, loss_scale=32, train_wall=295, gb_free=7.2, wall=14024
2022-03-04 13:09:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:13:38 | INFO | train_inner | epoch 023:    119 / 196 loss=5.083, ppl=33.89, wps=20380.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.844, loss_scale=16, train_wall=299, gb_free=7.2, wall=14346
2022-03-04 13:17:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:17:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:17:47 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.144 | ppl 141.4 | wps 41400.5 | wpb 510.9 | bsz 1 | num_updates 4476 | best_loss 6.944
2022-03-04 13:17:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4476 updates
2022-03-04 13:17:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 13:17:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 13:17:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 23 @ 4476 updates, score 7.144) (writing took 2.896994236856699 seconds)
2022-03-04 13:17:50 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 13:17:50 | INFO | train | epoch 023 | loss 5.103 | ppl 34.38 | wps 20109.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 4476 | lr 0.000472667 | gnorm 0.833 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 14598
2022-03-04 13:17:50 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 13:17:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:19:06 | INFO | train_inner | epoch 024:     24 / 196 loss=5.088, ppl=34, wps=19878, ups=0.3, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.828, loss_scale=16, train_wall=298, gb_free=7.2, wall=14675
2022-03-04 13:24:25 | INFO | train_inner | epoch 024:    124 / 196 loss=4.971, ppl=31.36, wps=20594.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.841, loss_scale=32, train_wall=296, gb_free=7.2, wall=14993
2022-03-04 13:27:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:28:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:28:18 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.194 | ppl 146.41 | wps 41439.8 | wpb 510.9 | bsz 1 | num_updates 4671 | best_loss 6.944
2022-03-04 13:28:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4671 updates
2022-03-04 13:28:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 13:28:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 13:28:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 24 @ 4671 updates, score 7.194) (writing took 3.1724209571257234 seconds)
2022-03-04 13:28:22 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 13:28:22 | INFO | train | epoch 024 | loss 4.984 | ppl 31.65 | wps 20212.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4671 | lr 0.000462695 | gnorm 0.841 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 15230
2022-03-04 13:28:22 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 13:28:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:29:54 | INFO | train_inner | epoch 025:     29 / 196 loss=4.965, ppl=31.24, wps=19858.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.849, loss_scale=16, train_wall=298, gb_free=7.2, wall=15322
2022-03-04 13:35:12 | INFO | train_inner | epoch 025:    129 / 196 loss=4.858, ppl=29, wps=20596.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.818, loss_scale=32, train_wall=296, gb_free=7.2, wall=15640
2022-03-04 13:38:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:38:50 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.263 | ppl 153.6 | wps 41334.7 | wpb 510.9 | bsz 1 | num_updates 4867 | best_loss 6.944
2022-03-04 13:38:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4867 updates
2022-03-04 13:38:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 13:38:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 13:38:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 25 @ 4867 updates, score 7.263) (writing took 3.2434820672497153 seconds)
2022-03-04 13:38:53 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 13:38:53 | INFO | train | epoch 025 | loss 4.87 | ppl 29.23 | wps 20313.6 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 4867 | lr 0.000453283 | gnorm 0.827 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 15861
2022-03-04 13:38:53 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 13:38:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:40:38 | INFO | train_inner | epoch 026:     33 / 196 loss=4.843, ppl=28.69, wps=20039.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.817, loss_scale=32, train_wall=295, gb_free=7.2, wall=15966
2022-03-04 13:41:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:46:00 | INFO | train_inner | epoch 026:    134 / 196 loss=4.754, ppl=26.98, wps=20378.1, ups=0.31, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.833, loss_scale=32, train_wall=299, gb_free=7.2, wall=16288
2022-03-04 13:48:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:49:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:49:22 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.43 | ppl 172.41 | wps 41432.5 | wpb 510.9 | bsz 1 | num_updates 5061 | best_loss 6.944
2022-03-04 13:49:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5061 updates
2022-03-04 13:49:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 13:49:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 13:49:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 26 @ 5061 updates, score 7.43) (writing took 3.259181789122522 seconds)
2022-03-04 13:49:25 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 13:49:25 | INFO | train | epoch 026 | loss 4.761 | ppl 27.11 | wps 20094.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 5061 | lr 0.00044451 | gnorm 0.825 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 16493
2022-03-04 13:49:25 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 13:49:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:51:29 | INFO | train_inner | epoch 027:     39 / 196 loss=4.721, ppl=26.37, wps=19856.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.824, loss_scale=32, train_wall=298, gb_free=7.2, wall=16617
2022-03-04 13:55:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:56:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:56:54 | INFO | train_inner | epoch 027:    141 / 196 loss=4.662, ppl=25.31, wps=20194.1, ups=0.31, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.839, loss_scale=16, train_wall=302, gb_free=7.2, wall=16942
2022-03-04 13:59:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:59:53 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.426 | ppl 172.02 | wps 41484.4 | wpb 510.9 | bsz 1 | num_updates 5255 | best_loss 6.944
2022-03-04 13:59:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5255 updates
2022-03-04 13:59:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 13:59:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 13:59:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 27 @ 5255 updates, score 7.426) (writing took 3.3217158410698175 seconds)
2022-03-04 13:59:56 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 13:59:56 | INFO | train | epoch 027 | loss 4.661 | ppl 25.29 | wps 20107.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 5255 | lr 0.000436228 | gnorm 0.827 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 17124
2022-03-04 13:59:56 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 13:59:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:02:20 | INFO | train_inner | epoch 028:     45 / 196 loss=4.609, ppl=24.41, wps=20046.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.803, loss_scale=16, train_wall=295, gb_free=7.2, wall=17268
2022-03-04 14:07:38 | INFO | train_inner | epoch 028:    145 / 196 loss=4.575, ppl=23.84, wps=20590.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.85, loss_scale=32, train_wall=296, gb_free=7.2, wall=17586
2022-03-04 14:09:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:10:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:10:25 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.471 | ppl 177.46 | wps 41367.3 | wpb 510.9 | bsz 1 | num_updates 5450 | best_loss 6.944
2022-03-04 14:10:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5450 updates
2022-03-04 14:10:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 14:10:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 14:10:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 28 @ 5450 updates, score 7.471) (writing took 3.2994167041033506 seconds)
2022-03-04 14:10:28 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 14:10:28 | INFO | train | epoch 028 | loss 4.566 | ppl 23.69 | wps 20204 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5450 | lr 0.000428353 | gnorm 0.83 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 17756
2022-03-04 14:10:28 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 14:10:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:13:07 | INFO | train_inner | epoch 029:     50 / 196 loss=4.508, ppl=22.75, wps=19847.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=5500, lr=0.000426401, gnorm=0.84, loss_scale=32, train_wall=298, gb_free=7.2, wall=17915
2022-03-04 14:17:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:18:29 | INFO | train_inner | epoch 029:    151 / 196 loss=4.483, ppl=22.36, wps=20390.1, ups=0.31, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.841, loss_scale=32, train_wall=299, gb_free=7.2, wall=18237
2022-03-04 14:20:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:20:56 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.566 | ppl 189.5 | wps 41302.1 | wpb 510.9 | bsz 1 | num_updates 5645 | best_loss 6.944
2022-03-04 14:20:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5645 updates
2022-03-04 14:20:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 14:20:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 14:21:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 29 @ 5645 updates, score 7.566) (writing took 3.2785646971315145 seconds)
2022-03-04 14:21:00 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 14:21:00 | INFO | train | epoch 029 | loss 4.475 | ppl 22.25 | wps 20205.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5645 | lr 0.000420889 | gnorm 0.847 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 18388
2022-03-04 14:21:00 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 14:21:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:23:55 | INFO | train_inner | epoch 030:     55 / 196 loss=4.422, ppl=21.44, wps=20035.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.834, loss_scale=32, train_wall=295, gb_free=7.2, wall=18563
2022-03-04 14:24:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:29:17 | INFO | train_inner | epoch 030:    156 / 196 loss=4.4, ppl=21.11, wps=20363.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.846, loss_scale=32, train_wall=299, gb_free=7.2, wall=18885
2022-03-04 14:31:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:31:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:31:29 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.638 | ppl 199.25 | wps 40709.9 | wpb 510.9 | bsz 1 | num_updates 5839 | best_loss 6.944
2022-03-04 14:31:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5839 updates
2022-03-04 14:31:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 14:31:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 14:31:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 30 @ 5839 updates, score 7.638) (writing took 3.2574508152902126 seconds)
2022-03-04 14:31:32 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 14:31:32 | INFO | train | epoch 030 | loss 4.387 | ppl 20.93 | wps 20081.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 5839 | lr 0.000413838 | gnorm 0.836 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 19020
2022-03-04 14:31:32 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 14:31:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:34:46 | INFO | train_inner | epoch 031:     61 / 196 loss=4.322, ppl=20, wps=19840.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.837, loss_scale=32, train_wall=298, gb_free=7.2, wall=19214
2022-03-04 14:38:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:40:08 | INFO | train_inner | epoch 031:    162 / 196 loss=4.328, ppl=20.08, wps=20391, ups=0.31, wpb=65532.4, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.847, loss_scale=32, train_wall=299, gb_free=7.2, wall=19536
2022-03-04 14:41:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:42:00 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.655 | ppl 201.56 | wps 41490 | wpb 510.9 | bsz 1 | num_updates 6034 | best_loss 6.944
2022-03-04 14:42:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6034 updates
2022-03-04 14:42:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 14:42:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 14:42:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 31 @ 6034 updates, score 7.655) (writing took 3.3061477737501264 seconds)
2022-03-04 14:42:04 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 14:42:04 | INFO | train | epoch 031 | loss 4.307 | ppl 19.8 | wps 20203.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6034 | lr 0.000407096 | gnorm 0.851 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 19652
2022-03-04 14:42:04 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 14:42:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:45:34 | INFO | train_inner | epoch 032:     66 / 196 loss=4.234, ppl=18.82, wps=20042.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.859, loss_scale=64, train_wall=295, gb_free=7.2, wall=19862
2022-03-04 14:45:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:50:55 | INFO | train_inner | epoch 032:    167 / 196 loss=4.25, ppl=19.02, wps=20394, ups=0.31, wpb=65532.4, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.853, loss_scale=32, train_wall=299, gb_free=7.2, wall=20183
2022-03-04 14:52:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:52:32 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.77 | ppl 218.26 | wps 41426.1 | wpb 510.9 | bsz 1 | num_updates 6229 | best_loss 6.944
2022-03-04 14:52:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6229 updates
2022-03-04 14:52:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 14:52:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 14:52:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 32 @ 6229 updates, score 7.77) (writing took 3.4776375191286206 seconds)
2022-03-04 14:52:35 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 14:52:35 | INFO | train | epoch 032 | loss 4.229 | ppl 18.75 | wps 20202.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6229 | lr 0.000400674 | gnorm 0.845 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 20283
2022-03-04 14:52:35 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 14:52:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:52:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:56:24 | INFO | train_inner | epoch 033:     72 / 196 loss=4.151, ppl=17.77, wps=19848.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.843, loss_scale=32, train_wall=298, gb_free=7.2, wall=20512
2022-03-04 14:59:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:01:46 | INFO | train_inner | epoch 033:    173 / 196 loss=4.182, ppl=18.16, wps=20402.1, ups=0.31, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.862, loss_scale=32, train_wall=299, gb_free=7.2, wall=20834
2022-03-04 15:02:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:02:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:03:03 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.827 | ppl 227 | wps 41356.5 | wpb 510.9 | bsz 1 | num_updates 6422 | best_loss 6.944
2022-03-04 15:03:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6422 updates
2022-03-04 15:03:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 15:03:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 15:03:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 33 @ 6422 updates, score 7.827) (writing took 3.235782806761563 seconds)
2022-03-04 15:03:06 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 15:03:06 | INFO | train | epoch 033 | loss 4.154 | ppl 17.8 | wps 20010.4 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 6422 | lr 0.000394607 | gnorm 0.867 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 20915
2022-03-04 15:03:06 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 15:03:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:07:15 | INFO | train_inner | epoch 034:     78 / 196 loss=4.076, ppl=16.86, wps=19862.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.871, loss_scale=16, train_wall=298, gb_free=7.2, wall=21163
2022-03-04 15:12:33 | INFO | train_inner | epoch 034:    178 / 196 loss=4.112, ppl=17.29, wps=20609.2, ups=0.31, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.853, loss_scale=32, train_wall=296, gb_free=7.2, wall=21481
2022-03-04 15:13:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:13:34 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.906 | ppl 239.77 | wps 41391.5 | wpb 510.9 | bsz 1 | num_updates 6618 | best_loss 6.944
2022-03-04 15:13:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6618 updates
2022-03-04 15:13:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 15:13:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 15:13:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 34 @ 6618 updates, score 7.906) (writing took 3.1760966293513775 seconds)
2022-03-04 15:13:38 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 15:13:38 | INFO | train | epoch 034 | loss 4.084 | ppl 16.96 | wps 20325.5 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 6618 | lr 0.00038872 | gnorm 0.863 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 21546
2022-03-04 15:13:38 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 15:13:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:16:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:18:02 | INFO | train_inner | epoch 035:     83 / 196 loss=3.994, ppl=15.94, wps=19859.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.875, loss_scale=32, train_wall=298, gb_free=7.2, wall=21810
2022-03-04 15:21:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:23:23 | INFO | train_inner | epoch 035:    184 / 196 loss=4.051, ppl=16.58, wps=20406.2, ups=0.31, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.874, loss_scale=16, train_wall=299, gb_free=7.2, wall=22131
2022-03-04 15:24:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:24:06 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.994 | ppl 254.96 | wps 41398.6 | wpb 510.9 | bsz 1 | num_updates 6812 | best_loss 6.944
2022-03-04 15:24:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6812 updates
2022-03-04 15:24:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 15:24:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 15:24:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 35 @ 6812 updates, score 7.994) (writing took 3.4823473999276757 seconds)
2022-03-04 15:24:09 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 15:24:09 | INFO | train | epoch 035 | loss 4.016 | ppl 16.18 | wps 20107.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 6812 | lr 0.000383145 | gnorm 0.872 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 22177
2022-03-04 15:24:09 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 15:24:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:28:49 | INFO | train_inner | epoch 036:     88 / 196 loss=3.921, ppl=15.14, wps=20048.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.871, loss_scale=32, train_wall=295, gb_free=7.2, wall=22457
2022-03-04 15:29:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:34:10 | INFO | train_inner | epoch 036:    189 / 196 loss=3.998, ppl=15.98, wps=20402.5, ups=0.31, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.879, loss_scale=16, train_wall=299, gb_free=7.2, wall=22778
2022-03-04 15:34:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:34:37 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.055 | ppl 265.97 | wps 41422 | wpb 510.9 | bsz 1 | num_updates 7007 | best_loss 6.944
2022-03-04 15:34:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7007 updates
2022-03-04 15:34:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 15:34:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 15:34:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 36 @ 7007 updates, score 8.055) (writing took 3.2450136924162507 seconds)
2022-03-04 15:34:40 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 15:34:40 | INFO | train | epoch 036 | loss 3.951 | ppl 15.47 | wps 20221.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7007 | lr 0.000377776 | gnorm 0.87 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 22808
2022-03-04 15:34:40 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 15:34:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:39:36 | INFO | train_inner | epoch 037:     93 / 196 loss=3.851, ppl=14.43, wps=20062.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.863, loss_scale=32, train_wall=295, gb_free=7.2, wall=23104
2022-03-04 15:43:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:44:57 | INFO | train_inner | epoch 037:    194 / 196 loss=3.935, ppl=15.29, wps=20405.9, ups=0.31, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.882, loss_scale=32, train_wall=299, gb_free=7.2, wall=23425
2022-03-04 15:45:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:45:08 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.13 | ppl 280.16 | wps 41547.4 | wpb 510.9 | bsz 1 | num_updates 7202 | best_loss 6.944
2022-03-04 15:45:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7202 updates
2022-03-04 15:45:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 15:45:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 15:45:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 37 @ 7202 updates, score 8.13) (writing took 3.2516375305131078 seconds)
2022-03-04 15:45:11 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 15:45:11 | INFO | train | epoch 037 | loss 3.889 | ppl 14.82 | wps 20223.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7202 | lr 0.000372626 | gnorm 0.872 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 23439
2022-03-04 15:45:11 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 15:45:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:50:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:50:26 | INFO | train_inner | epoch 038:     99 / 196 loss=3.782, ppl=13.76, wps=19861.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.873, loss_scale=32, train_wall=298, gb_free=7.2, wall=23754
2022-03-04 15:53:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:55:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:55:39 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.145 | ppl 283.09 | wps 41643.6 | wpb 510.9 | bsz 1 | num_updates 7396 | best_loss 6.944
2022-03-04 15:55:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7396 updates
2022-03-04 15:55:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 15:55:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 15:55:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 38 @ 7396 updates, score 8.145) (writing took 3.3757049599662423 seconds)
2022-03-04 15:55:43 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 15:55:43 | INFO | train | epoch 038 | loss 3.828 | ppl 14.21 | wps 20109.2 | ups 0.31 | wpb 65448.9 | bsz 127.8 | num_updates 7396 | lr 0.000367707 | gnorm 0.883 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 24071
2022-03-04 15:55:43 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 15:55:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:55:55 | INFO | train_inner | epoch 039:      4 / 196 loss=3.871, ppl=14.63, wps=19856, ups=0.3, wpb=65367, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.892, loss_scale=16, train_wall=298, gb_free=7.2, wall=24084
2022-03-04 16:01:13 | INFO | train_inner | epoch 039:    104 / 196 loss=3.726, ppl=13.23, wps=20616.1, ups=0.31, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.888, loss_scale=32, train_wall=296, gb_free=7.2, wall=24401
2022-03-04 16:06:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:06:10 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.295 | ppl 314.11 | wps 41364.3 | wpb 510.9 | bsz 1 | num_updates 7592 | best_loss 6.944
2022-03-04 16:06:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7592 updates
2022-03-04 16:06:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 16:06:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 16:06:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 39 @ 7592 updates, score 8.295) (writing took 3.1851655673235655 seconds)
2022-03-04 16:06:13 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 16:06:13 | INFO | train | epoch 039 | loss 3.773 | ppl 13.67 | wps 20334.3 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 7592 | lr 0.000362929 | gnorm 0.898 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 24702
2022-03-04 16:06:13 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 16:06:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:06:39 | INFO | train_inner | epoch 040:      8 / 196 loss=3.812, ppl=14.05, wps=20068.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7600, lr=0.000362738, gnorm=0.911, loss_scale=32, train_wall=295, gb_free=7.2, wall=24727
2022-03-04 16:07:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 16:10:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:12:03 | INFO | train_inner | epoch 040:    110 / 196 loss=3.68, ppl=12.82, wps=20203.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.885, loss_scale=16, train_wall=302, gb_free=7.2, wall=25052
2022-03-04 16:16:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:16:41 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.382 | ppl 333.63 | wps 41400.8 | wpb 510.9 | bsz 1 | num_updates 7786 | best_loss 6.944
2022-03-04 16:16:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7786 updates
2022-03-04 16:16:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 16:16:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 16:16:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 40 @ 7786 updates, score 8.382) (writing took 3.3467294685542583 seconds)
2022-03-04 16:16:45 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 16:16:45 | INFO | train | epoch 040 | loss 3.716 | ppl 13.14 | wps 20116.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7786 | lr 0.000358379 | gnorm 0.89 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 25333
2022-03-04 16:16:45 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 16:16:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:17:29 | INFO | train_inner | epoch 041:     14 / 196 loss=3.738, ppl=13.34, wps=20057.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.903, loss_scale=16, train_wall=295, gb_free=7.2, wall=25377
2022-03-04 16:20:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:22:50 | INFO | train_inner | epoch 041:    115 / 196 loss=3.631, ppl=12.39, wps=20412.9, ups=0.31, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.901, loss_scale=16, train_wall=298, gb_free=7.2, wall=25698
2022-03-04 16:27:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:27:12 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.418 | ppl 342.06 | wps 40725.2 | wpb 510.9 | bsz 1 | num_updates 7981 | best_loss 6.944
2022-03-04 16:27:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 7981 updates
2022-03-04 16:27:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 16:27:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 16:27:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 41 @ 7981 updates, score 8.418) (writing took 3.3100510695949197 seconds)
2022-03-04 16:27:16 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 16:27:16 | INFO | train | epoch 041 | loss 3.666 | ppl 12.69 | wps 20221.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7981 | lr 0.000353974 | gnorm 0.915 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 25964
2022-03-04 16:27:16 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 16:27:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:28:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:28:19 | INFO | train_inner | epoch 042:     20 / 196 loss=3.686, ppl=12.87, wps=19863.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=8000, lr=0.000353553, gnorm=0.914, loss_scale=16, train_wall=298, gb_free=7.2, wall=26028
2022-03-04 16:33:37 | INFO | train_inner | epoch 042:    120 / 196 loss=3.586, ppl=12.01, wps=20613.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.906, loss_scale=16, train_wall=296, gb_free=7.2, wall=26345
2022-03-04 16:35:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:37:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:37:43 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.554 | ppl 375.8 | wps 41502.2 | wpb 510.9 | bsz 1 | num_updates 8175 | best_loss 6.944
2022-03-04 16:37:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8175 updates
2022-03-04 16:37:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 16:37:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 16:37:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 42 @ 8175 updates, score 8.554) (writing took 3.222805810160935 seconds)
2022-03-04 16:37:47 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 16:37:47 | INFO | train | epoch 042 | loss 3.614 | ppl 12.24 | wps 20126.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 8175 | lr 0.000349749 | gnorm 0.907 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 26595
2022-03-04 16:37:47 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 16:37:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:39:06 | INFO | train_inner | epoch 043:     25 / 196 loss=3.624, ppl=12.33, wps=19876.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.906, loss_scale=16, train_wall=298, gb_free=7.2, wall=26674
2022-03-04 16:44:24 | INFO | train_inner | epoch 043:    125 / 196 loss=3.547, ppl=11.69, wps=20606.9, ups=0.31, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.925, loss_scale=32, train_wall=296, gb_free=7.2, wall=26992
2022-03-04 16:48:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:48:14 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.517 | ppl 366.21 | wps 41653.8 | wpb 510.9 | bsz 1 | num_updates 8371 | best_loss 6.944
2022-03-04 16:48:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8371 updates
2022-03-04 16:48:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 16:48:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 16:48:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 43 @ 8371 updates, score 8.517) (writing took 3.178975336253643 seconds)
2022-03-04 16:48:18 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 16:48:18 | INFO | train | epoch 043 | loss 3.567 | ppl 11.85 | wps 20331 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 8371 | lr 0.00034563 | gnorm 0.918 | loss_scale 32 | train_wall 579 | gb_free 7.2 | wall 27226
2022-03-04 16:48:18 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 16:48:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:49:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:49:53 | INFO | train_inner | epoch 044:     30 / 196 loss=3.569, ppl=11.87, wps=19885.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=8400, lr=0.000345033, gnorm=0.91, loss_scale=16, train_wall=298, gb_free=7.2, wall=27321
2022-03-04 16:55:11 | INFO | train_inner | epoch 044:    130 / 196 loss=3.502, ppl=11.33, wps=20630.9, ups=0.31, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.929, loss_scale=16, train_wall=295, gb_free=7.2, wall=27639
2022-03-04 16:56:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:58:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:58:45 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.604 | ppl 389.15 | wps 41394 | wpb 510.9 | bsz 1 | num_updates 8565 | best_loss 6.944
2022-03-04 16:58:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8565 updates
2022-03-04 16:58:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 16:58:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 16:58:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 44 @ 8565 updates, score 8.604) (writing took 3.220894400961697 seconds)
2022-03-04 16:58:48 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 16:58:48 | INFO | train | epoch 044 | loss 3.517 | ppl 11.45 | wps 20140.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 8565 | lr 0.000341693 | gnorm 0.924 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 27856
2022-03-04 16:58:48 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 16:58:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:00:39 | INFO | train_inner | epoch 045:     35 / 196 loss=3.519, ppl=11.47, wps=19880.2, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.933, loss_scale=16, train_wall=298, gb_free=7.2, wall=27967
2022-03-04 17:03:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:06:00 | INFO | train_inner | epoch 045:    136 / 196 loss=3.464, ppl=11.03, wps=20415.2, ups=0.31, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.934, loss_scale=16, train_wall=298, gb_free=7.2, wall=28289
2022-03-04 17:09:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:09:15 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.657 | ppl 403.65 | wps 41392.7 | wpb 510.9 | bsz 1 | num_updates 8760 | best_loss 6.944
2022-03-04 17:09:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8760 updates
2022-03-04 17:09:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 17:09:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 17:09:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 45 @ 8760 updates, score 8.657) (writing took 3.3076779441908 seconds)
2022-03-04 17:09:19 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 17:09:19 | INFO | train | epoch 045 | loss 3.472 | ppl 11.1 | wps 20236.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 8760 | lr 0.000337869 | gnorm 0.93 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 28487
2022-03-04 17:09:19 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 17:09:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:11:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:11:29 | INFO | train_inner | epoch 046:     41 / 196 loss=3.454, ppl=10.96, wps=19891.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=8800, lr=0.0003371, gnorm=0.921, loss_scale=16, train_wall=298, gb_free=7.2, wall=28617
2022-03-04 17:16:46 | INFO | train_inner | epoch 046:    141 / 196 loss=3.426, ppl=10.74, wps=20642.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.934, loss_scale=16, train_wall=295, gb_free=7.2, wall=28935
2022-03-04 17:18:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:19:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:19:46 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.728 | ppl 424.14 | wps 41383.2 | wpb 510.9 | bsz 1 | num_updates 8954 | best_loss 6.944
2022-03-04 17:19:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8954 updates
2022-03-04 17:19:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 17:19:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 17:19:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 46 @ 8954 updates, score 8.728) (writing took 3.2460548654198647 seconds)
2022-03-04 17:19:49 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 17:19:49 | INFO | train | epoch 046 | loss 3.428 | ppl 10.76 | wps 20144.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 8954 | lr 0.000334188 | gnorm 0.929 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 29117
2022-03-04 17:19:49 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 17:19:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:22:15 | INFO | train_inner | epoch 047:     46 / 196 loss=3.408, ppl=10.61, wps=19891.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.93, loss_scale=16, train_wall=298, gb_free=7.2, wall=29263
2022-03-04 17:26:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:27:36 | INFO | train_inner | epoch 047:    147 / 196 loss=3.392, ppl=10.49, wps=20442.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.954, loss_scale=16, train_wall=298, gb_free=7.2, wall=29584
2022-03-04 17:30:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:30:16 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.851 | ppl 461.62 | wps 41625.7 | wpb 510.9 | bsz 1 | num_updates 9149 | best_loss 6.944
2022-03-04 17:30:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 9149 updates
2022-03-04 17:30:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 17:30:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 17:30:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 47 @ 9149 updates, score 8.851) (writing took 3.2018841383978724 seconds)
2022-03-04 17:30:19 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 17:30:19 | INFO | train | epoch 047 | loss 3.387 | ppl 10.46 | wps 20261.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 9149 | lr 0.000330608 | gnorm 0.946 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 29747
2022-03-04 17:30:19 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 17:30:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:33:01 | INFO | train_inner | epoch 048:     51 / 196 loss=3.362, ppl=10.28, wps=20093.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=9200, lr=0.00032969, gnorm=0.935, loss_scale=16, train_wall=295, gb_free=7.2, wall=29909
2022-03-04 17:34:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:38:21 | INFO | train_inner | epoch 048:    152 / 196 loss=3.353, ppl=10.22, wps=20445.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.96, loss_scale=16, train_wall=298, gb_free=7.2, wall=30230
2022-03-04 17:40:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:40:45 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.917 | ppl 483.49 | wps 41520.5 | wpb 510.9 | bsz 1 | num_updates 9344 | best_loss 6.944
2022-03-04 17:40:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 9344 updates
2022-03-04 17:40:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 17:40:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 17:40:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 48 @ 9344 updates, score 8.917) (writing took 3.269937695004046 seconds)
2022-03-04 17:40:49 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 17:40:49 | INFO | train | epoch 048 | loss 3.347 | ppl 10.17 | wps 20259.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 9344 | lr 0.00032714 | gnorm 0.953 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 30377
2022-03-04 17:40:49 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 17:40:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:41:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:43:50 | INFO | train_inner | epoch 049:     57 / 196 loss=3.313, ppl=9.93, wps=19907.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.945, loss_scale=16, train_wall=297, gb_free=7.2, wall=30558
2022-03-04 17:48:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:49:10 | INFO | train_inner | epoch 049:    158 / 196 loss=3.318, ppl=9.97, wps=20445.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.96, loss_scale=16, train_wall=298, gb_free=7.2, wall=30879
2022-03-04 17:51:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:51:15 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.969 | ppl 501.27 | wps 41529.3 | wpb 510.9 | bsz 1 | num_updates 9538 | best_loss 6.944
2022-03-04 17:51:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 9538 updates
2022-03-04 17:51:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 17:51:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 17:51:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 49 @ 9538 updates, score 8.969) (writing took 3.3229703521355987 seconds)
2022-03-04 17:51:19 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 17:51:19 | INFO | train | epoch 049 | loss 3.305 | ppl 9.88 | wps 20154.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 9538 | lr 0.000323796 | gnorm 0.952 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 31007
2022-03-04 17:51:19 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 17:51:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:54:36 | INFO | train_inner | epoch 050:     62 / 196 loss=3.268, ppl=9.63, wps=20096.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=9600, lr=0.000322749, gnorm=0.957, loss_scale=16, train_wall=295, gb_free=7.2, wall=31204
2022-03-04 17:55:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:59:56 | INFO | train_inner | epoch 050:    163 / 196 loss=3.287, ppl=9.76, wps=20443.7, ups=0.31, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.95, loss_scale=16, train_wall=298, gb_free=7.2, wall=31524
2022-03-04 18:01:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:01:45 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.105 | ppl 550.73 | wps 41593.6 | wpb 510.9 | bsz 1 | num_updates 9733 | best_loss 6.944
2022-03-04 18:01:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 9733 updates
2022-03-04 18:01:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 18:01:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 18:01:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 50 @ 9733 updates, score 9.105) (writing took 3.212167982943356 seconds)
2022-03-04 18:01:49 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 18:01:49 | INFO | train | epoch 050 | loss 3.268 | ppl 9.63 | wps 20261.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 9733 | lr 0.000320536 | gnorm 0.958 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 31637
2022-03-04 18:01:49 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 18:01:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:02:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:05:25 | INFO | train_inner | epoch 051:     68 / 196 loss=3.22, ppl=9.32, wps=19890.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.975, loss_scale=16, train_wall=298, gb_free=7.2, wall=31853
2022-03-04 18:10:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:10:45 | INFO | train_inner | epoch 051:    169 / 196 loss=3.251, ppl=9.52, wps=20436.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.954, loss_scale=16, train_wall=298, gb_free=7.2, wall=32174
2022-03-04 18:12:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:12:16 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.132 | ppl 561.04 | wps 41613.1 | wpb 510.9 | bsz 1 | num_updates 9927 | best_loss 6.944
2022-03-04 18:12:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 9927 updates
2022-03-04 18:12:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 18:12:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 18:12:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 51 @ 9927 updates, score 9.132) (writing took 3.5450566560029984 seconds)
2022-03-04 18:12:19 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 18:12:19 | INFO | train | epoch 051 | loss 3.23 | ppl 9.38 | wps 20128.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 9927 | lr 0.000317388 | gnorm 0.967 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 32268
2022-03-04 18:12:19 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 18:12:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:16:13 | INFO | train_inner | epoch 052:     73 / 196 loss=3.185, ppl=9.09, wps=19983.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10000, lr=0.000316228, gnorm=0.975, loss_scale=16, train_wall=295, gb_free=7.2, wall=32501
2022-03-04 18:17:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:21:33 | INFO | train_inner | epoch 052:    174 / 196 loss=3.223, ppl=9.34, wps=20437.9, ups=0.31, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.973, loss_scale=16, train_wall=298, gb_free=7.2, wall=32821
2022-03-04 18:22:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:22:47 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.173 | ppl 577.19 | wps 41607.1 | wpb 510.9 | bsz 1 | num_updates 10122 | best_loss 6.944
2022-03-04 18:22:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 10122 updates
2022-03-04 18:22:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 18:22:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 18:22:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 52 @ 10122 updates, score 9.173) (writing took 3.3316664323210716 seconds)
2022-03-04 18:22:51 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 18:22:51 | INFO | train | epoch 052 | loss 3.195 | ppl 9.16 | wps 20210.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10122 | lr 0.000314316 | gnorm 0.972 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 32899
2022-03-04 18:22:51 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 18:22:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:25:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:27:02 | INFO | train_inner | epoch 053:     79 / 196 loss=3.136, ppl=8.79, wps=19900, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.964, loss_scale=16, train_wall=297, gb_free=7.2, wall=33150
2022-03-04 18:32:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:32:22 | INFO | train_inner | epoch 053:    180 / 196 loss=3.199, ppl=9.19, wps=20436.8, ups=0.31, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.993, loss_scale=16, train_wall=298, gb_free=7.2, wall=33471
2022-03-04 18:33:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:33:18 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.189 | ppl 583.7 | wps 41399.1 | wpb 510.9 | bsz 1 | num_updates 10316 | best_loss 6.944
2022-03-04 18:33:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 10316 updates
2022-03-04 18:33:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 18:33:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 18:33:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 53 @ 10316 updates, score 9.189) (writing took 3.200049726292491 seconds)
2022-03-04 18:33:21 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 18:33:21 | INFO | train | epoch 053 | loss 3.161 | ppl 8.94 | wps 20156.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 10316 | lr 0.000311347 | gnorm 0.976 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 33529
2022-03-04 18:33:21 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 18:33:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:37:48 | INFO | train_inner | epoch 054:     84 / 196 loss=3.1, ppl=8.57, wps=20092.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=10400, lr=0.000310087, gnorm=0.982, loss_scale=16, train_wall=295, gb_free=7.2, wall=33796
2022-03-04 18:39:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:43:08 | INFO | train_inner | epoch 054:    185 / 196 loss=3.161, ppl=8.94, wps=20439, ups=0.31, wpb=65532.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.98, loss_scale=16, train_wall=298, gb_free=7.2, wall=34116
2022-03-04 18:43:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:43:48 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.296 | ppl 628.5 | wps 41410 | wpb 510.9 | bsz 1 | num_updates 10511 | best_loss 6.944
2022-03-04 18:43:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 10511 updates
2022-03-04 18:43:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 18:43:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 18:43:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 54 @ 10511 updates, score 9.296) (writing took 3.3986114086583257 seconds)
2022-03-04 18:43:51 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 18:43:51 | INFO | train | epoch 054 | loss 3.127 | ppl 8.73 | wps 20247.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10511 | lr 0.000308445 | gnorm 0.981 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 34159
2022-03-04 18:43:51 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 18:43:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:46:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:48:37 | INFO | train_inner | epoch 055:     90 / 196 loss=3.061, ppl=8.34, wps=19882.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.976, loss_scale=16, train_wall=298, gb_free=7.2, wall=34445
2022-03-04 18:53:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:53:58 | INFO | train_inner | epoch 055:    191 / 196 loss=3.134, ppl=8.78, wps=20448, ups=0.31, wpb=65532.4, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.99, loss_scale=16, train_wall=298, gb_free=7.2, wall=34766
2022-03-04 18:54:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:54:18 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.263 | ppl 614.33 | wps 41586.5 | wpb 510.9 | bsz 1 | num_updates 10705 | best_loss 6.944
2022-03-04 18:54:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 10705 updates
2022-03-04 18:54:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 18:54:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 18:54:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 55 @ 10705 updates, score 9.263) (writing took 3.4057036666199565 seconds)
2022-03-04 18:54:21 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 18:54:21 | INFO | train | epoch 055 | loss 3.094 | ppl 8.54 | wps 20144.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 10705 | lr 0.000305638 | gnorm 0.983 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 34790
2022-03-04 18:54:21 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 18:54:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:59:23 | INFO | train_inner | epoch 056:     95 / 196 loss=3.02, ppl=8.11, wps=20069.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=10800, lr=0.00030429, gnorm=0.973, loss_scale=16, train_wall=295, gb_free=7.2, wall=35091
2022-03-04 19:00:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:04:43 | INFO | train_inner | epoch 056:    196 / 196 loss=3.112, ppl=8.65, wps=20434.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10900, lr=0.000302891, gnorm=1.012, loss_scale=16, train_wall=298, gb_free=7.2, wall=35411
2022-03-04 19:04:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:04:48 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.441 | ppl 694.95 | wps 41497.2 | wpb 510.9 | bsz 1 | num_updates 10900 | best_loss 6.944
2022-03-04 19:04:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 10900 updates
2022-03-04 19:04:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 19:04:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 19:04:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 56 @ 10900 updates, score 9.441) (writing took 3.2934821136295795 seconds)
2022-03-04 19:04:52 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 19:04:52 | INFO | train | epoch 056 | loss 3.063 | ppl 8.36 | wps 20246.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10900 | lr 0.000302891 | gnorm 0.993 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 35420
2022-03-04 19:04:52 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 19:04:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:07:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:10:13 | INFO | train_inner | epoch 057:    101 / 196 loss=2.985, ppl=7.92, wps=19887.9, ups=0.3, wpb=65532.4, bsz=128, num_updates=11000, lr=0.000301511, gnorm=1.003, loss_scale=16, train_wall=298, gb_free=7.2, wall=35741
2022-03-04 19:14:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:15:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:15:21 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.463 | ppl 705.79 | wps 41897.1 | wpb 510.9 | bsz 1 | num_updates 11094 | best_loss 6.944
2022-03-04 19:15:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 11094 updates
2022-03-04 19:15:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 19:15:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 19:15:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 57 @ 11094 updates, score 9.463) (writing took 3.5987106328830123 seconds)
2022-03-04 19:15:24 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 19:15:24 | INFO | train | epoch 057 | loss 3.032 | ppl 8.18 | wps 20068.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 11094 | lr 0.000300231 | gnorm 1.007 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 36053
2022-03-04 19:15:24 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 19:15:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:15:44 | INFO | train_inner | epoch 058:      6 / 196 loss=3.071, ppl=8.4, wps=19754.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=11100, lr=0.00030015, gnorm=1.014, loss_scale=16, train_wall=298, gb_free=7.2, wall=36072
2022-03-04 19:21:04 | INFO | train_inner | epoch 058:    106 / 196 loss=2.964, ppl=7.8, wps=20449.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.98, loss_scale=16, train_wall=295, gb_free=7.2, wall=36392
2022-03-04 19:22:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:25:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:25:54 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.502 | ppl 724.83 | wps 41631.5 | wpb 510.9 | bsz 1 | num_updates 11289 | best_loss 6.944
2022-03-04 19:25:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 11289 updates
2022-03-04 19:25:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 19:25:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 19:25:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 58 @ 11289 updates, score 9.502) (writing took 3.275330463424325 seconds)
2022-03-04 19:25:58 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 19:25:58 | INFO | train | epoch 058 | loss 3.002 | ppl 8.01 | wps 20155.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11289 | lr 0.000297627 | gnorm 1 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 36686
2022-03-04 19:25:58 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 19:25:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:26:33 | INFO | train_inner | epoch 059:     11 / 196 loss=3.036, ppl=8.2, wps=19888.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=11300, lr=0.000297482, gnorm=1.013, loss_scale=16, train_wall=298, gb_free=7.2, wall=36721
2022-03-04 19:29:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:31:53 | INFO | train_inner | epoch 059:    112 / 196 loss=2.941, ppl=7.68, wps=20425.2, ups=0.31, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.998, loss_scale=16, train_wall=298, gb_free=7.2, wall=37042
2022-03-04 19:36:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:36:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:36:25 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.539 | ppl 743.86 | wps 41613.2 | wpb 510.9 | bsz 1 | num_updates 11483 | best_loss 6.944
2022-03-04 19:36:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 11483 updates
2022-03-04 19:36:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 19:36:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 19:36:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 59 @ 11483 updates, score 9.539) (writing took 3.3063559448346496 seconds)
2022-03-04 19:36:28 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 19:36:28 | INFO | train | epoch 059 | loss 2.973 | ppl 7.85 | wps 20140.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 11483 | lr 0.000295102 | gnorm 1.002 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 37316
2022-03-04 19:36:28 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 19:36:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:37:22 | INFO | train_inner | epoch 060:     17 / 196 loss=2.994, ppl=7.97, wps=19897.7, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=11500, lr=0.000294884, gnorm=1.009, loss_scale=16, train_wall=297, gb_free=7.2, wall=37370
2022-03-04 19:42:39 | INFO | train_inner | epoch 060:    117 / 196 loss=2.919, ppl=7.56, wps=20647.2, ups=0.32, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=1.017, loss_scale=16, train_wall=295, gb_free=7.2, wall=37688
2022-03-04 19:43:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:46:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:46:55 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.605 | ppl 778.67 | wps 41486 | wpb 510.9 | bsz 1 | num_updates 11678 | best_loss 6.944
2022-03-04 19:46:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 11678 updates
2022-03-04 19:46:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 19:46:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 19:46:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 60 @ 11678 updates, score 9.605) (writing took 3.2860055221244693 seconds)
2022-03-04 19:46:58 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 19:46:58 | INFO | train | epoch 060 | loss 2.946 | ppl 7.7 | wps 20259.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11678 | lr 0.000292628 | gnorm 1.018 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 37946
2022-03-04 19:46:58 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 19:46:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:48:08 | INFO | train_inner | epoch 061:     22 / 196 loss=2.962, ppl=7.79, wps=19900.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=11700, lr=0.000292353, gnorm=1.014, loss_scale=16, train_wall=297, gb_free=7.2, wall=38016
2022-03-04 19:50:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:53:29 | INFO | train_inner | epoch 061:    123 / 196 loss=2.899, ppl=7.46, wps=20428.3, ups=0.31, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=1.029, loss_scale=16, train_wall=298, gb_free=7.2, wall=38337
2022-03-04 19:57:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:57:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:57:25 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.734 | ppl 851.68 | wps 41616.6 | wpb 510.9 | bsz 1 | num_updates 11872 | best_loss 6.944
2022-03-04 19:57:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 11872 updates
2022-03-04 19:57:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 19:57:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 19:57:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 61 @ 11872 updates, score 9.734) (writing took 3.3581665037199855 seconds)
2022-03-04 19:57:28 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 19:57:28 | INFO | train | epoch 061 | loss 2.917 | ppl 7.56 | wps 20141.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 11872 | lr 0.000290227 | gnorm 1.018 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 38576
2022-03-04 19:57:28 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 19:57:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:58:57 | INFO | train_inner | epoch 062:     28 / 196 loss=2.925, ppl=7.6, wps=19891.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=11900, lr=0.000289886, gnorm=1.014, loss_scale=16, train_wall=298, gb_free=7.2, wall=38665
2022-03-04 20:04:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:04:18 | INFO | train_inner | epoch 062:    129 / 196 loss=2.872, ppl=7.32, wps=20438.8, ups=0.31, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=1.016, loss_scale=16, train_wall=298, gb_free=7.2, wall=38986
2022-03-04 20:07:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:07:55 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.705 | ppl 834.61 | wps 41505.9 | wpb 510.9 | bsz 1 | num_updates 12067 | best_loss 6.944
2022-03-04 20:07:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 12067 updates
2022-03-04 20:07:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 20:07:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 20:07:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 62 @ 12067 updates, score 9.705) (writing took 3.324828553944826 seconds)
2022-03-04 20:07:59 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 20:07:59 | INFO | train | epoch 062 | loss 2.891 | ppl 7.42 | wps 20246.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12067 | lr 0.000287873 | gnorm 1.021 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 39207
2022-03-04 20:07:59 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 20:07:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:09:44 | INFO | train_inner | epoch 063:     33 / 196 loss=2.896, ppl=7.44, wps=20071.5, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=12100, lr=0.00028748, gnorm=1.034, loss_scale=16, train_wall=295, gb_free=7.2, wall=39312
2022-03-04 20:11:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:15:05 | INFO | train_inner | epoch 063:    134 / 196 loss=2.856, ppl=7.24, wps=20414.6, ups=0.31, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=1.029, loss_scale=16, train_wall=299, gb_free=7.2, wall=39633
2022-03-04 20:17:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:18:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:18:26 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.74 | ppl 855.12 | wps 41808.8 | wpb 510.9 | bsz 1 | num_updates 12261 | best_loss 6.944
2022-03-04 20:18:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 12261 updates
2022-03-04 20:18:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 20:18:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 20:18:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 63 @ 12261 updates, score 9.74) (writing took 3.5853890776634216 seconds)
2022-03-04 20:18:29 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 20:18:29 | INFO | train | epoch 063 | loss 2.864 | ppl 7.28 | wps 20126.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 12261 | lr 0.000285586 | gnorm 1.027 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 39838
2022-03-04 20:18:29 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 20:18:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:20:34 | INFO | train_inner | epoch 064:     39 / 196 loss=2.86, ppl=7.26, wps=19872.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=12300, lr=0.000285133, gnorm=1.012, loss_scale=16, train_wall=298, gb_free=7.2, wall=39962
2022-03-04 20:24:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:25:54 | INFO | train_inner | epoch 064:    140 / 196 loss=2.836, ppl=7.14, wps=20457.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=12400, lr=0.000283981, gnorm=1.021, loss_scale=16, train_wall=298, gb_free=7.2, wall=40282
2022-03-04 20:28:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:28:56 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.842 | ppl 917.82 | wps 41609.2 | wpb 510.9 | bsz 1 | num_updates 12456 | best_loss 6.944
2022-03-04 20:28:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 12456 updates
2022-03-04 20:28:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 20:28:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 20:29:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 64 @ 12456 updates, score 9.842) (writing took 3.563651946373284 seconds)
2022-03-04 20:29:00 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 20:29:00 | INFO | train | epoch 064 | loss 2.841 | ppl 7.17 | wps 20254.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12456 | lr 0.000283342 | gnorm 1.016 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 40468
2022-03-04 20:29:00 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 20:29:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:31:19 | INFO | train_inner | epoch 065:     44 / 196 loss=2.837, ppl=7.14, wps=20083.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=12500, lr=0.000282843, gnorm=1.026, loss_scale=16, train_wall=294, gb_free=7.2, wall=40608
2022-03-04 20:31:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:36:40 | INFO | train_inner | epoch 065:    145 / 196 loss=2.814, ppl=7.03, wps=20443.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=12600, lr=0.000281718, gnorm=1.024, loss_scale=16, train_wall=298, gb_free=7.2, wall=40928
2022-03-04 20:38:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:39:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:39:26 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.87 | ppl 935.92 | wps 41531 | wpb 510.9 | bsz 1 | num_updates 12650 | best_loss 6.944
2022-03-04 20:39:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 12650 updates
2022-03-04 20:39:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 20:39:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 20:39:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 65 @ 12650 updates, score 9.87) (writing took 3.510575090534985 seconds)
2022-03-04 20:39:30 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 20:39:30 | INFO | train | epoch 065 | loss 2.815 | ppl 7.04 | wps 20153.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 12650 | lr 0.000281161 | gnorm 1.035 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 41098
2022-03-04 20:39:30 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 20:39:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:42:09 | INFO | train_inner | epoch 066:     50 / 196 loss=2.801, ppl=6.97, wps=19888.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=12700, lr=0.000280607, gnorm=1.028, loss_scale=16, train_wall=297, gb_free=7.2, wall=41257
2022-03-04 20:45:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:47:29 | INFO | train_inner | epoch 066:    151 / 196 loss=2.794, ppl=6.94, wps=20437.9, ups=0.31, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=1.033, loss_scale=16, train_wall=298, gb_free=7.2, wall=41577
2022-03-04 20:49:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:49:57 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.901 | ppl 955.81 | wps 41641.6 | wpb 510.9 | bsz 1 | num_updates 12845 | best_loss 6.944
2022-03-04 20:49:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 12845 updates
2022-03-04 20:49:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 20:50:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 20:50:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 66 @ 12845 updates, score 9.901) (writing took 3.8338109869509935 seconds)
2022-03-04 20:50:00 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 20:50:00 | INFO | train | epoch 066 | loss 2.792 | ppl 6.92 | wps 20228.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12845 | lr 0.000279018 | gnorm 1.022 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 41729
2022-03-04 20:50:01 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 20:50:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:52:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:52:58 | INFO | train_inner | epoch 067:     56 / 196 loss=2.773, ppl=6.84, wps=19852.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=12900, lr=0.000278423, gnorm=1.025, loss_scale=16, train_wall=298, gb_free=7.2, wall=41907
2022-03-04 20:58:16 | INFO | train_inner | epoch 067:    156 / 196 loss=2.781, ppl=6.87, wps=20640.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=13000, lr=0.00027735, gnorm=1.025, loss_scale=16, train_wall=295, gb_free=7.2, wall=42224
2022-03-04 20:59:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:00:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:00:27 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 10.012 | ppl 1032.85 | wps 41198 | wpb 510.9 | bsz 1 | num_updates 13039 | best_loss 6.944
2022-03-04 21:00:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 13039 updates
2022-03-04 21:00:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 21:00:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 21:00:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 67 @ 13039 updates, score 10.012) (writing took 3.2538551595062017 seconds)
2022-03-04 21:00:31 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 21:00:31 | INFO | train | epoch 067 | loss 2.769 | ppl 6.81 | wps 20145.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 13039 | lr 0.000276935 | gnorm 1.032 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 42359
2022-03-04 21:00:31 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 21:00:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:03:45 | INFO | train_inner | epoch 068:     61 / 196 loss=2.746, ppl=6.71, wps=19882.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=13100, lr=0.000276289, gnorm=1.038, loss_scale=16, train_wall=298, gb_free=7.2, wall=42553
2022-03-04 21:06:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:09:05 | INFO | train_inner | epoch 068:    162 / 196 loss=2.758, ppl=6.77, wps=20439, ups=0.31, wpb=65532.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=1.042, loss_scale=16, train_wall=298, gb_free=7.2, wall=42874
2022-03-04 21:10:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:10:58 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 10.012 | ppl 1032.44 | wps 41419.4 | wpb 510.9 | bsz 1 | num_updates 13234 | best_loss 6.944
2022-03-04 21:10:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 13234 updates
2022-03-04 21:10:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 21:11:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 21:11:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 68 @ 13234 updates, score 10.012) (writing took 3.17667331174016 seconds)
2022-03-04 21:11:01 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 21:11:01 | INFO | train | epoch 068 | loss 2.746 | ppl 6.71 | wps 20254.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13234 | lr 0.000274887 | gnorm 1.04 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 42989
2022-03-04 21:11:01 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 21:11:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:13:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:14:34 | INFO | train_inner | epoch 069:     67 / 196 loss=2.714, ppl=6.56, wps=19910.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=13300, lr=0.000274204, gnorm=1.031, loss_scale=16, train_wall=297, gb_free=7.2, wall=43202
2022-03-04 21:19:51 | INFO | train_inner | epoch 069:    167 / 196 loss=2.741, ppl=6.68, wps=20646.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=13400, lr=0.000273179, gnorm=1.059, loss_scale=16, train_wall=295, gb_free=7.2, wall=43519
2022-03-04 21:19:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:21:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:21:28 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 10.041 | ppl 1053.79 | wps 41469.8 | wpb 510.9 | bsz 1 | num_updates 13428 | best_loss 6.944
2022-03-04 21:21:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 13428 updates
2022-03-04 21:21:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 21:21:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 21:21:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 69 @ 13428 updates, score 10.041) (writing took 3.190027811564505 seconds)
2022-03-04 21:21:31 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 21:21:31 | INFO | train | epoch 069 | loss 2.723 | ppl 6.6 | wps 20157.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 13428 | lr 0.000272894 | gnorm 1.048 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 43619
2022-03-04 21:21:31 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 21:21:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:25:19 | INFO | train_inner | epoch 070:     72 / 196 loss=2.687, ppl=6.44, wps=19902.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=1.053, loss_scale=8, train_wall=298, gb_free=7.2, wall=43848
2022-03-04 21:30:37 | INFO | train_inner | epoch 070:    172 / 196 loss=2.727, ppl=6.62, wps=20643.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=13600, lr=0.000271163, gnorm=1.058, loss_scale=16, train_wall=295, gb_free=7.2, wall=44165
2022-03-04 21:31:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:31:58 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 10.094 | ppl 1092.77 | wps 41643.3 | wpb 510.9 | bsz 1 | num_updates 13624 | best_loss 6.944
2022-03-04 21:31:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 13624 updates
2022-03-04 21:31:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 21:32:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 21:32:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 70 @ 13624 updates, score 10.094) (writing took 3.2635048255324364 seconds)
2022-03-04 21:32:01 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 21:32:01 | INFO | train | epoch 070 | loss 2.7 | ppl 6.5 | wps 20359.8 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 13624 | lr 0.000270924 | gnorm 1.054 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 44249
2022-03-04 21:32:01 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 21:32:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:33:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:36:05 | INFO | train_inner | epoch 071:     77 / 196 loss=2.658, ppl=6.31, wps=19907.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=13700, lr=0.000270172, gnorm=1.053, loss_scale=16, train_wall=297, gb_free=7.2, wall=44493
2022-03-04 21:40:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:41:26 | INFO | train_inner | epoch 071:    178 / 196 loss=2.705, ppl=6.52, wps=20426.5, ups=0.31, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=1.049, loss_scale=16, train_wall=298, gb_free=7.2, wall=44814
2022-03-04 21:42:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:42:37 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 10.144 | ppl 1131.56 | wps 41470.8 | wpb 510.9 | bsz 1 | num_updates 13818 | best_loss 6.944
2022-03-04 21:42:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 13818 updates
2022-03-04 21:42:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 21:42:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 21:42:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 71 @ 13818 updates, score 10.144) (writing took 3.994338230229914 seconds)
2022-03-04 21:42:41 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 21:42:41 | INFO | train | epoch 071 | loss 2.679 | ppl 6.41 | wps 19822.7 | ups 0.3 | wpb 65447.1 | bsz 127.8 | num_updates 13818 | lr 0.000269016 | gnorm 1.055 | loss_scale 16 | train_wall 580 | gb_free 7.2 | wall 44889
2022-03-04 21:42:41 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 21:42:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:47:04 | INFO | train_inner | epoch 072:     82 / 196 loss=2.636, ppl=6.22, wps=19352.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=1.045, loss_scale=16, train_wall=298, gb_free=7.2, wall=45152
2022-03-04 21:47:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:52:26 | INFO | train_inner | epoch 072:    183 / 196 loss=2.688, ppl=6.45, wps=20365.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=14000, lr=0.000267261, gnorm=1.08, loss_scale=16, train_wall=299, gb_free=7.2, wall=45474
2022-03-04 21:53:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:53:12 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 10.188 | ppl 1166.7 | wps 41617.5 | wpb 510.9 | bsz 1 | num_updates 14013 | best_loss 6.944
2022-03-04 21:53:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 14013 updates
2022-03-04 21:53:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 21:53:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 21:53:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 72 @ 14013 updates, score 10.188) (writing took 3.655280776321888 seconds)
2022-03-04 21:53:15 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 21:53:15 | INFO | train | epoch 072 | loss 2.659 | ppl 6.32 | wps 20133.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14013 | lr 0.000267137 | gnorm 1.059 | loss_scale 16 | train_wall 581 | gb_free 7.2 | wall 45523
2022-03-04 21:53:15 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 21:53:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:54:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:57:56 | INFO | train_inner | epoch 073:     88 / 196 loss=2.611, ppl=6.11, wps=19767, ups=0.3, wpb=65367, bsz=127.7, num_updates=14100, lr=0.000266312, gnorm=1.063, loss_scale=16, train_wall=299, gb_free=7.2, wall=45805
2022-03-04 22:01:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:03:18 | INFO | train_inner | epoch 073:    189 / 196 loss=2.67, ppl=6.36, wps=20397.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=14200, lr=0.000265372, gnorm=1.056, loss_scale=16, train_wall=299, gb_free=7.2, wall=46126
2022-03-04 22:03:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:03:44 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 10.253 | ppl 1220.37 | wps 41627.2 | wpb 510.9 | bsz 1 | num_updates 14207 | best_loss 6.944
2022-03-04 22:03:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 14207 updates
2022-03-04 22:03:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 22:03:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 22:03:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 73 @ 14207 updates, score 10.253) (writing took 3.778481979854405 seconds)
2022-03-04 22:03:48 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 22:03:48 | INFO | train | epoch 073 | loss 2.637 | ppl 6.22 | wps 20059 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14207 | lr 0.000265307 | gnorm 1.058 | loss_scale 16 | train_wall 580 | gb_free 7.2 | wall 46156
2022-03-04 22:03:48 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 22:03:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:08:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:08:47 | INFO | train_inner | epoch 074:     94 / 196 loss=2.585, ppl=6, wps=19850.3, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=1.048, loss_scale=16, train_wall=298, gb_free=7.2, wall=46455
2022-03-04 22:14:05 | INFO | train_inner | epoch 074:    194 / 196 loss=2.658, ppl=6.31, wps=20607.9, ups=0.31, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=1.08, loss_scale=16, train_wall=296, gb_free=7.2, wall=46773
2022-03-04 22:14:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:14:16 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 10.204 | ppl 1179.53 | wps 41570.5 | wpb 510.9 | bsz 1 | num_updates 14402 | best_loss 6.944
2022-03-04 22:14:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 14402 updates
2022-03-04 22:14:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 22:14:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 22:14:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 74 @ 14402 updates, score 10.204) (writing took 3.8177839815616608 seconds)
2022-03-04 22:14:20 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 22:14:20 | INFO | train | epoch 074 | loss 2.618 | ppl 6.14 | wps 20211.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14402 | lr 0.000263505 | gnorm 1.063 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 46788
2022-03-04 22:14:20 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 22:14:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:15:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:19:34 | INFO | train_inner | epoch 075:     99 / 196 loss=2.549, ppl=5.85, wps=19835.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=14500, lr=0.000262613, gnorm=1.064, loss_scale=16, train_wall=298, gb_free=7.2, wall=47103
2022-03-04 22:22:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:24:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:24:47 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 10.331 | ppl 1288 | wps 41528 | wpb 510.9 | bsz 1 | num_updates 14596 | best_loss 6.944
2022-03-04 22:24:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 14596 updates
2022-03-04 22:24:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 22:24:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 22:24:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 75 @ 14596 updates, score 10.331) (writing took 3.7441688338294625 seconds)
2022-03-04 22:24:51 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-04 22:24:51 | INFO | train | epoch 075 | loss 2.6 | ppl 6.06 | wps 20107.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14596 | lr 0.000261748 | gnorm 1.07 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 47419
2022-03-04 22:24:51 | INFO | fairseq.trainer | begin training epoch 76
2022-03-04 22:24:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:25:04 | INFO | train_inner | epoch 076:      4 / 196 loss=2.648, ppl=6.27, wps=19847.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=14600, lr=0.000261712, gnorm=1.077, loss_scale=16, train_wall=298, gb_free=7.2, wall=47432
2022-03-04 22:29:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:30:25 | INFO | train_inner | epoch 076:    105 / 196 loss=2.547, ppl=5.84, wps=20419, ups=0.31, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=1.053, loss_scale=16, train_wall=298, gb_free=7.2, wall=47753
2022-03-04 22:35:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:35:19 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 10.402 | ppl 1353.16 | wps 41625.6 | wpb 510.9 | bsz 1 | num_updates 14791 | best_loss 6.944
2022-03-04 22:35:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 14791 updates
2022-03-04 22:35:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 22:35:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 22:35:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 76 @ 14791 updates, score 10.402) (writing took 4.116717542521656 seconds)
2022-03-04 22:35:23 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-04 22:35:23 | INFO | train | epoch 076 | loss 2.58 | ppl 5.98 | wps 20199.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14791 | lr 0.000260017 | gnorm 1.058 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 48051
2022-03-04 22:35:23 | INFO | fairseq.trainer | begin training epoch 77
2022-03-04 22:35:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:35:52 | INFO | train_inner | epoch 077:      9 / 196 loss=2.608, ppl=6.1, wps=20002.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14800, lr=0.000259938, gnorm=1.063, loss_scale=16, train_wall=295, gb_free=7.2, wall=48080
2022-03-04 22:36:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:41:13 | INFO | train_inner | epoch 077:    110 / 196 loss=2.529, ppl=5.77, wps=20393.4, ups=0.31, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=1.067, loss_scale=16, train_wall=299, gb_free=7.2, wall=48401
2022-03-04 22:43:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:45:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:45:51 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 10.37 | ppl 1323.49 | wps 41588.3 | wpb 510.9 | bsz 1 | num_updates 14985 | best_loss 6.944
2022-03-04 22:45:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 14985 updates
2022-03-04 22:45:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 22:45:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 22:45:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 77 @ 14985 updates, score 10.37) (writing took 4.036400522105396 seconds)
2022-03-04 22:45:55 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-04 22:45:55 | INFO | train | epoch 077 | loss 2.561 | ppl 5.9 | wps 20080.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14985 | lr 0.000258328 | gnorm 1.071 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 48683
2022-03-04 22:45:55 | INFO | fairseq.trainer | begin training epoch 78
2022-03-04 22:45:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:46:43 | INFO | train_inner | epoch 078:     15 / 196 loss=2.585, ppl=6, wps=19812.9, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=1.073, loss_scale=16, train_wall=298, gb_free=7.2, wall=48731
2022-03-04 22:50:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:52:04 | INFO | train_inner | epoch 078:    116 / 196 loss=2.512, ppl=5.7, wps=20416.1, ups=0.31, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=1.064, loss_scale=16, train_wall=299, gb_free=7.2, wall=49052
2022-03-04 22:56:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:56:22 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 10.455 | ppl 1403.66 | wps 41676.6 | wpb 510.9 | bsz 1 | num_updates 15180 | best_loss 6.944
2022-03-04 22:56:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 15180 updates
2022-03-04 22:56:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 22:56:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 22:56:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 78 @ 15180 updates, score 10.455) (writing took 4.069196830503643 seconds)
2022-03-04 22:56:26 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-04 22:56:26 | INFO | train | epoch 078 | loss 2.544 | ppl 5.83 | wps 20219.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15180 | lr 0.000256664 | gnorm 1.07 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 49314
2022-03-04 22:56:26 | INFO | fairseq.trainer | begin training epoch 79
2022-03-04 22:56:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:57:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:57:33 | INFO | train_inner | epoch 079:     21 / 196 loss=2.57, ppl=5.94, wps=19852.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=15200, lr=0.000256495, gnorm=1.076, loss_scale=16, train_wall=297, gb_free=7.2, wall=49381
2022-03-04 23:02:51 | INFO | train_inner | epoch 079:    121 / 196 loss=2.499, ppl=5.65, wps=20629.2, ups=0.31, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=1.07, loss_scale=16, train_wall=295, gb_free=7.2, wall=49699
2022-03-04 23:04:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:06:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:06:54 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 10.479 | ppl 1427.54 | wps 41458.5 | wpb 510.9 | bsz 1 | num_updates 15374 | best_loss 6.944
2022-03-04 23:06:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 15374 updates
2022-03-04 23:06:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 23:06:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 23:06:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 79 @ 15374 updates, score 10.479) (writing took 3.8793594650924206 seconds)
2022-03-04 23:06:57 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-04 23:06:57 | INFO | train | epoch 079 | loss 2.524 | ppl 5.75 | wps 20118.4 | ups 0.31 | wpb 65448.9 | bsz 127.8 | num_updates 15374 | lr 0.000255039 | gnorm 1.072 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 49946
2022-03-04 23:06:57 | INFO | fairseq.trainer | begin training epoch 80
2022-03-04 23:06:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:08:20 | INFO | train_inner | epoch 080:     26 / 196 loss=2.542, ppl=5.82, wps=19851.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=1.073, loss_scale=16, train_wall=298, gb_free=7.2, wall=50028
2022-03-04 23:11:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:13:41 | INFO | train_inner | epoch 080:    127 / 196 loss=2.496, ppl=5.64, wps=20422.9, ups=0.31, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=1.071, loss_scale=16, train_wall=298, gb_free=7.2, wall=50349
2022-03-04 23:17:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:17:25 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 10.544 | ppl 1493.38 | wps 41500.8 | wpb 510.9 | bsz 1 | num_updates 15569 | best_loss 6.944
2022-03-04 23:17:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 15569 updates
2022-03-04 23:17:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 23:17:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 23:17:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 80 @ 15569 updates, score 10.544) (writing took 3.93825863301754 seconds)
2022-03-04 23:17:29 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-04 23:17:29 | INFO | train | epoch 080 | loss 2.509 | ppl 5.69 | wps 20220 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15569 | lr 0.000253437 | gnorm 1.083 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 50577
2022-03-04 23:17:29 | INFO | fairseq.trainer | begin training epoch 81
2022-03-04 23:17:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:18:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:19:10 | INFO | train_inner | epoch 081:     32 / 196 loss=2.514, ppl=5.71, wps=19840.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=15600, lr=0.000253185, gnorm=1.097, loss_scale=16, train_wall=298, gb_free=7.2, wall=50679
2022-03-04 23:24:28 | INFO | train_inner | epoch 081:    132 / 196 loss=2.476, ppl=5.56, wps=20611.9, ups=0.31, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=1.076, loss_scale=16, train_wall=296, gb_free=7.2, wall=50997
2022-03-04 23:25:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:27:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:27:56 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 10.6 | ppl 1551.64 | wps 41597.9 | wpb 510.9 | bsz 1 | num_updates 15763 | best_loss 6.944
2022-03-04 23:27:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 15763 updates
2022-03-04 23:27:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 23:28:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 23:28:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 81 @ 15763 updates, score 10.6) (writing took 4.0038739182055 seconds)
2022-03-04 23:28:00 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-04 23:28:00 | INFO | train | epoch 081 | loss 2.49 | ppl 5.62 | wps 20101.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 15763 | lr 0.000251872 | gnorm 1.082 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 51208
2022-03-04 23:28:00 | INFO | fairseq.trainer | begin training epoch 82
2022-03-04 23:28:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:29:58 | INFO | train_inner | epoch 082:     37 / 196 loss=2.492, ppl=5.62, wps=19825.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=1.086, loss_scale=16, train_wall=298, gb_free=7.2, wall=51326
2022-03-04 23:31:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:35:19 | INFO | train_inner | epoch 082:    138 / 196 loss=2.473, ppl=5.55, wps=20405.6, ups=0.31, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=1.071, loss_scale=16, train_wall=299, gb_free=7.2, wall=51647
2022-03-04 23:38:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:38:28 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 10.672 | ppl 1631.86 | wps 41758.5 | wpb 510.9 | bsz 1 | num_updates 15958 | best_loss 6.944
2022-03-04 23:38:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 15958 updates
2022-03-04 23:38:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 23:38:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 23:38:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 82 @ 15958 updates, score 10.672) (writing took 3.8990322053432465 seconds)
2022-03-04 23:38:32 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-04 23:38:32 | INFO | train | epoch 082 | loss 2.474 | ppl 5.56 | wps 20203.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15958 | lr 0.000250329 | gnorm 1.08 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 51840
2022-03-04 23:38:32 | INFO | fairseq.trainer | begin training epoch 83
2022-03-04 23:38:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:38:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:40:49 | INFO | train_inner | epoch 083:     43 / 196 loss=2.47, ppl=5.54, wps=19843.6, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=16000, lr=0.00025, gnorm=1.094, loss_scale=16, train_wall=298, gb_free=7.2, wall=51977
2022-03-04 23:45:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:46:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:46:13 | INFO | train_inner | epoch 083:    145 / 196 loss=2.456, ppl=5.49, wps=20228.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=16100, lr=0.000249222, gnorm=1.094, loss_scale=8, train_wall=301, gb_free=7.2, wall=52301
2022-03-04 23:48:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:48:59 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 10.65 | ppl 1607.37 | wps 41598.8 | wpb 510.9 | bsz 1 | num_updates 16151 | best_loss 6.944
2022-03-04 23:48:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 16151 updates
2022-03-04 23:48:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 23:49:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 23:49:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 83 @ 16151 updates, score 10.65) (writing took 3.7772033847868443 seconds)
2022-03-04 23:49:03 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-04 23:49:03 | INFO | train | epoch 083 | loss 2.458 | ppl 5.5 | wps 20021.9 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 16151 | lr 0.000248829 | gnorm 1.094 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 52471
2022-03-04 23:49:03 | INFO | fairseq.trainer | begin training epoch 84
2022-03-04 23:49:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:51:39 | INFO | train_inner | epoch 084:     49 / 196 loss=2.447, ppl=5.45, wps=20049.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=1.091, loss_scale=8, train_wall=295, gb_free=7.2, wall=52627
2022-03-04 23:56:57 | INFO | train_inner | epoch 084:    149 / 196 loss=2.45, ppl=5.47, wps=20601.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=16300, lr=0.000247689, gnorm=1.094, loss_scale=16, train_wall=296, gb_free=7.2, wall=52945
2022-03-04 23:59:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:59:31 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 10.692 | ppl 1654.28 | wps 41723 | wpb 510.9 | bsz 1 | num_updates 16347 | best_loss 6.944
2022-03-04 23:59:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 16347 updates
2022-03-04 23:59:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 23:59:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-04 23:59:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 84 @ 16347 updates, score 10.692) (writing took 4.2063135812059045 seconds)
2022-03-04 23:59:35 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-04 23:59:35 | INFO | train | epoch 084 | loss 2.442 | ppl 5.44 | wps 20296.3 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 16347 | lr 0.000247332 | gnorm 1.096 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 53103
2022-03-04 23:59:35 | INFO | fairseq.trainer | begin training epoch 85
2022-03-04 23:59:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:00:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:02:27 | INFO | train_inner | epoch 085:     54 / 196 loss=2.429, ppl=5.39, wps=19809.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=16400, lr=0.000246932, gnorm=1.099, loss_scale=16, train_wall=298, gb_free=7.2, wall=53275
2022-03-05 00:06:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:07:48 | INFO | train_inner | epoch 085:    155 / 196 loss=2.432, ppl=5.4, wps=20389.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=16500, lr=0.000246183, gnorm=1.106, loss_scale=16, train_wall=299, gb_free=7.2, wall=53596
2022-03-05 00:09:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:10:03 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 10.739 | ppl 1708.99 | wps 41368.4 | wpb 510.9 | bsz 1 | num_updates 16541 | best_loss 6.944
2022-03-05 00:10:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 16541 updates
2022-03-05 00:10:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 00:10:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 00:10:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 85 @ 16541 updates, score 10.739) (writing took 4.285921983420849 seconds)
2022-03-05 00:10:07 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-05 00:10:07 | INFO | train | epoch 085 | loss 2.424 | ppl 5.37 | wps 20073.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 16541 | lr 0.000245878 | gnorm 1.099 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 53736
2022-03-05 00:10:07 | INFO | fairseq.trainer | begin training epoch 86
2022-03-05 00:10:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:13:15 | INFO | train_inner | epoch 086:     59 / 196 loss=2.401, ppl=5.28, wps=19984.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=1.083, loss_scale=16, train_wall=295, gb_free=7.2, wall=53923
2022-03-05 00:13:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:18:37 | INFO | train_inner | epoch 086:    160 / 196 loss=2.426, ppl=5.37, wps=20389.6, ups=0.31, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=1.096, loss_scale=16, train_wall=299, gb_free=7.2, wall=54245
2022-03-05 00:20:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:20:35 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 10.835 | ppl 1827.23 | wps 41382.1 | wpb 510.9 | bsz 1 | num_updates 16736 | best_loss 6.944
2022-03-05 00:20:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 16736 updates
2022-03-05 00:20:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 00:20:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 00:20:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 86 @ 16736 updates, score 10.835) (writing took 3.9962482573464513 seconds)
2022-03-05 00:20:39 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-05 00:20:39 | INFO | train | epoch 086 | loss 2.411 | ppl 5.32 | wps 20189.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16736 | lr 0.000244441 | gnorm 1.086 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 54368
2022-03-05 00:20:39 | INFO | fairseq.trainer | begin training epoch 87
2022-03-05 00:20:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:20:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:24:06 | INFO | train_inner | epoch 087:     65 / 196 loss=2.392, ppl=5.25, wps=19815.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=16800, lr=0.000243975, gnorm=1.089, loss_scale=16, train_wall=298, gb_free=7.2, wall=54575
2022-03-05 00:27:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:29:28 | INFO | train_inner | epoch 087:    166 / 196 loss=2.407, ppl=5.31, wps=20393.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=16900, lr=0.000243252, gnorm=1.083, loss_scale=16, train_wall=299, gb_free=7.2, wall=54896
2022-03-05 00:31:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:31:08 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 10.781 | ppl 1759.37 | wps 41478.4 | wpb 510.9 | bsz 1 | num_updates 16930 | best_loss 6.944
2022-03-05 00:31:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 16930 updates
2022-03-05 00:31:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 00:31:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 00:31:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 87 @ 16930 updates, score 10.781) (writing took 4.102360358461738 seconds)
2022-03-05 00:31:12 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-05 00:31:12 | INFO | train | epoch 087 | loss 2.395 | ppl 5.26 | wps 20076.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 16930 | lr 0.000243037 | gnorm 1.084 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 55000
2022-03-05 00:31:12 | INFO | fairseq.trainer | begin training epoch 88
2022-03-05 00:31:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:34:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:34:58 | INFO | train_inner | epoch 088:     71 / 196 loss=2.364, ppl=5.15, wps=19809.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=1.084, loss_scale=16, train_wall=298, gb_free=7.2, wall=55226
2022-03-05 00:40:16 | INFO | train_inner | epoch 088:    171 / 196 loss=2.406, ppl=5.3, wps=20611.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=1.121, loss_scale=16, train_wall=296, gb_free=7.2, wall=55544
2022-03-05 00:41:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:41:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:41:40 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 10.809 | ppl 1793.77 | wps 41596 | wpb 510.9 | bsz 1 | num_updates 17124 | best_loss 6.944
2022-03-05 00:41:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 17124 updates
2022-03-05 00:41:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 00:41:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 00:41:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 88 @ 17124 updates, score 10.809) (writing took 3.9691405380144715 seconds)
2022-03-05 00:41:43 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-05 00:41:43 | INFO | train | epoch 088 | loss 2.381 | ppl 5.21 | wps 20101.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 17124 | lr 0.000241656 | gnorm 1.106 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 55632
2022-03-05 00:41:44 | INFO | fairseq.trainer | begin training epoch 89
2022-03-05 00:41:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:45:45 | INFO | train_inner | epoch 089:     76 / 196 loss=2.346, ppl=5.08, wps=19840.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=17200, lr=0.000241121, gnorm=1.096, loss_scale=16, train_wall=298, gb_free=7.2, wall=55873
2022-03-05 00:48:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:51:06 | INFO | train_inner | epoch 089:    177 / 196 loss=2.394, ppl=5.26, wps=20416.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=17300, lr=0.000240424, gnorm=1.098, loss_scale=16, train_wall=298, gb_free=7.2, wall=56194
2022-03-05 00:52:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:52:11 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 10.827 | ppl 1816.19 | wps 41485.5 | wpb 510.9 | bsz 1 | num_updates 17319 | best_loss 6.944
2022-03-05 00:52:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 17319 updates
2022-03-05 00:52:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 00:52:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 00:52:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 89 @ 17319 updates, score 10.827) (writing took 3.7143025463446975 seconds)
2022-03-05 00:52:15 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-05 00:52:15 | INFO | train | epoch 089 | loss 2.367 | ppl 5.16 | wps 20216.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 17319 | lr 0.000240292 | gnorm 1.095 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 56263
2022-03-05 00:52:15 | INFO | fairseq.trainer | begin training epoch 90
2022-03-05 00:52:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:55:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:56:36 | INFO | train_inner | epoch 090:     82 / 196 loss=2.326, ppl=5.01, wps=19821.4, ups=0.3, wpb=65367, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=1.089, loss_scale=16, train_wall=298, gb_free=7.2, wall=56524
2022-03-05 01:01:53 | INFO | train_inner | epoch 090:    182 / 196 loss=2.383, ppl=5.22, wps=20637.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=1.107, loss_scale=16, train_wall=295, gb_free=7.2, wall=56842
2022-03-05 01:02:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:02:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:02:42 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 10.843 | ppl 1836.49 | wps 41571.7 | wpb 510.9 | bsz 1 | num_updates 17513 | best_loss 6.944
2022-03-05 01:02:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 17513 updates
2022-03-05 01:02:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 01:02:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 01:02:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 90 @ 17513 updates, score 10.843) (writing took 3.0737265637144446 seconds)
2022-03-05 01:02:45 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-05 01:02:45 | INFO | train | epoch 090 | loss 2.352 | ppl 5.1 | wps 20134 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 17513 | lr 0.000238957 | gnorm 1.102 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 56894
2022-03-05 01:02:45 | INFO | fairseq.trainer | begin training epoch 91
2022-03-05 01:02:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:07:22 | INFO | train_inner | epoch 091:     87 / 196 loss=2.317, ppl=4.98, wps=19898.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=17600, lr=0.000238366, gnorm=1.103, loss_scale=16, train_wall=298, gb_free=7.2, wall=57170
2022-03-05 01:09:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:12:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:12:46 | INFO | train_inner | epoch 091:    189 / 196 loss=2.365, ppl=5.15, wps=20230, ups=0.31, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=1.096, loss_scale=8, train_wall=301, gb_free=7.2, wall=57494
2022-03-05 01:13:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:13:13 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 10.907 | ppl 1920.13 | wps 41497.3 | wpb 510.9 | bsz 1 | num_updates 17707 | best_loss 6.944
2022-03-05 01:13:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 17707 updates
2022-03-05 01:13:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 01:13:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 01:13:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 91 @ 17707 updates, score 10.907) (writing took 2.919243304990232 seconds)
2022-03-05 01:13:16 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-05 01:13:16 | INFO | train | epoch 091 | loss 2.338 | ppl 5.06 | wps 20148.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 17707 | lr 0.000237644 | gnorm 1.099 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 57524
2022-03-05 01:13:16 | INFO | fairseq.trainer | begin training epoch 92
2022-03-05 01:13:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:18:11 | INFO | train_inner | epoch 092:     93 / 196 loss=2.296, ppl=4.91, wps=20100.9, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=1.122, loss_scale=8, train_wall=295, gb_free=7.2, wall=57819
2022-03-05 01:23:29 | INFO | train_inner | epoch 092:    193 / 196 loss=2.361, ppl=5.14, wps=20627.5, ups=0.31, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=1.111, loss_scale=16, train_wall=296, gb_free=7.2, wall=58137
2022-03-05 01:23:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:23:43 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 10.91 | ppl 1923.79 | wps 41724.5 | wpb 510.9 | bsz 1 | num_updates 17903 | best_loss 6.944
2022-03-05 01:23:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 17903 updates
2022-03-05 01:23:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 01:23:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 01:23:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 92 @ 17903 updates, score 10.91) (writing took 3.7438602447509766 seconds)
2022-03-05 01:23:46 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-05 01:23:46 | INFO | train | epoch 092 | loss 2.326 | ppl 5.01 | wps 20330.9 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 17903 | lr 0.00023634 | gnorm 1.115 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 58155
2022-03-05 01:23:47 | INFO | fairseq.trainer | begin training epoch 93
2022-03-05 01:23:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:26:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:28:58 | INFO | train_inner | epoch 093:     98 / 196 loss=2.274, ppl=4.84, wps=19855.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18000, lr=0.000235702, gnorm=1.094, loss_scale=16, train_wall=298, gb_free=7.2, wall=58466
2022-03-05 01:32:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:34:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:34:14 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 11.025 | ppl 2084.39 | wps 41686.9 | wpb 510.9 | bsz 1 | num_updates 18097 | best_loss 6.944
2022-03-05 01:34:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 18097 updates
2022-03-05 01:34:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 01:34:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 01:34:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 93 @ 18097 updates, score 11.025) (writing took 2.990369213744998 seconds)
2022-03-05 01:34:17 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-05 01:34:17 | INFO | train | epoch 093 | loss 2.309 | ppl 4.96 | wps 20146.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 18097 | lr 0.00023507 | gnorm 1.1 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 58785
2022-03-05 01:34:17 | INFO | fairseq.trainer | begin training epoch 94
2022-03-05 01:34:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:34:26 | INFO | train_inner | epoch 094:      3 / 196 loss=2.345, ppl=5.08, wps=19905, ups=0.3, wpb=65367, bsz=127.7, num_updates=18100, lr=0.00023505, gnorm=1.107, loss_scale=16, train_wall=298, gb_free=7.2, wall=58795
2022-03-05 01:39:44 | INFO | train_inner | epoch 094:    103 / 196 loss=2.263, ppl=4.8, wps=20607.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=18200, lr=0.000234404, gnorm=1.097, loss_scale=16, train_wall=296, gb_free=7.2, wall=59113
2022-03-05 01:39:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:44:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:44:45 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 11.011 | ppl 2064.1 | wps 41337.4 | wpb 510.9 | bsz 1 | num_updates 18292 | best_loss 6.944
2022-03-05 01:44:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 18292 updates
2022-03-05 01:44:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 01:44:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 01:44:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 94 @ 18292 updates, score 11.011) (writing took 2.9962040903046727 seconds)
2022-03-05 01:44:48 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-05 01:44:48 | INFO | train | epoch 094 | loss 2.298 | ppl 4.92 | wps 20217.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18292 | lr 0.000233813 | gnorm 1.112 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 59416
2022-03-05 01:44:48 | INFO | fairseq.trainer | begin training epoch 95
2022-03-05 01:44:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:45:14 | INFO | train_inner | epoch 095:      8 / 196 loss=2.327, ppl=5.02, wps=19859.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=18300, lr=0.000233762, gnorm=1.129, loss_scale=16, train_wall=298, gb_free=7.2, wall=59442
2022-03-05 01:46:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:50:36 | INFO | train_inner | epoch 095:    109 / 196 loss=2.246, ppl=4.74, wps=20351.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=18400, lr=0.000233126, gnorm=1.102, loss_scale=16, train_wall=299, gb_free=7.2, wall=59764
2022-03-05 01:53:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:55:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:55:17 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 11.098 | ppl 2191.68 | wps 41381 | wpb 510.9 | bsz 1 | num_updates 18486 | best_loss 6.944
2022-03-05 01:55:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 18486 updates
2022-03-05 01:55:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 01:55:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 01:55:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 95 @ 18486 updates, score 11.098) (writing took 2.9420881690457463 seconds)
2022-03-05 01:55:20 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-05 01:55:20 | INFO | train | epoch 095 | loss 2.283 | ppl 4.87 | wps 20099.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 18486 | lr 0.000232583 | gnorm 1.117 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 60048
2022-03-05 01:55:20 | INFO | fairseq.trainer | begin training epoch 96
2022-03-05 01:55:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:56:04 | INFO | train_inner | epoch 096:     14 / 196 loss=2.313, ppl=4.97, wps=19883.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=18500, lr=0.000232495, gnorm=1.129, loss_scale=16, train_wall=298, gb_free=7.2, wall=60092
2022-03-05 02:00:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:01:26 | INFO | train_inner | epoch 096:    115 / 196 loss=2.245, ppl=4.74, wps=20357.7, ups=0.31, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=1.109, loss_scale=16, train_wall=299, gb_free=7.2, wall=60414
2022-03-05 02:05:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:05:48 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 11.118 | ppl 2222.86 | wps 41278.6 | wpb 510.9 | bsz 1 | num_updates 18681 | best_loss 6.944
2022-03-05 02:05:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 18681 updates
2022-03-05 02:05:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 02:05:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 02:05:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 96 @ 18681 updates, score 11.118) (writing took 3.008938517421484 seconds)
2022-03-05 02:05:51 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-05 02:05:51 | INFO | train | epoch 096 | loss 2.273 | ppl 4.83 | wps 20210 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18681 | lr 0.000231366 | gnorm 1.12 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 60679
2022-03-05 02:05:51 | INFO | fairseq.trainer | begin training epoch 97
2022-03-05 02:05:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:06:52 | INFO | train_inner | epoch 097:     19 / 196 loss=2.294, ppl=4.9, wps=20084, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=1.131, loss_scale=16, train_wall=295, gb_free=7.2, wall=60740
2022-03-05 02:07:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:12:12 | INFO | train_inner | epoch 097:    120 / 196 loss=2.238, ppl=4.72, wps=20437.5, ups=0.31, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=1.119, loss_scale=16, train_wall=298, gb_free=7.2, wall=61060
2022-03-05 02:12:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:16:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:16:18 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 11.224 | ppl 2392.06 | wps 41551.1 | wpb 510.9 | bsz 1 | num_updates 18875 | best_loss 6.944
2022-03-05 02:16:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 18875 updates
2022-03-05 02:16:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 02:16:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 02:16:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 97 @ 18875 updates, score 11.224) (writing took 3.0232917312532663 seconds)
2022-03-05 02:16:21 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-05 02:16:21 | INFO | train | epoch 097 | loss 2.259 | ppl 4.79 | wps 20154.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 18875 | lr 0.000230174 | gnorm 1.119 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 61309
2022-03-05 02:16:21 | INFO | fairseq.trainer | begin training epoch 98
2022-03-05 02:16:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:17:41 | INFO | train_inner | epoch 098:     25 / 196 loss=2.274, ppl=4.84, wps=19910.4, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=18900, lr=0.000230022, gnorm=1.12, loss_scale=8, train_wall=298, gb_free=7.2, wall=61389
2022-03-05 02:21:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:23:01 | INFO | train_inner | epoch 098:    126 / 196 loss=2.231, ppl=4.69, wps=20425.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=19000, lr=0.000229416, gnorm=1.105, loss_scale=8, train_wall=298, gb_free=7.2, wall=61710
2022-03-05 02:26:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:26:48 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 11.146 | ppl 2266.7 | wps 41541.9 | wpb 510.9 | bsz 1 | num_updates 19070 | best_loss 6.944
2022-03-05 02:26:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 19070 updates
2022-03-05 02:26:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 02:26:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 02:26:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 98 @ 19070 updates, score 11.146) (writing took 2.978222357109189 seconds)
2022-03-05 02:26:51 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-05 02:26:51 | INFO | train | epoch 098 | loss 2.247 | ppl 4.75 | wps 20246.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19070 | lr 0.000228994 | gnorm 1.115 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 61940
2022-03-05 02:26:51 | INFO | fairseq.trainer | begin training epoch 99
2022-03-05 02:26:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:28:27 | INFO | train_inner | epoch 099:     30 / 196 loss=2.257, ppl=4.78, wps=20089.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=1.119, loss_scale=8, train_wall=295, gb_free=7.2, wall=62035
2022-03-05 02:33:45 | INFO | train_inner | epoch 099:    130 / 196 loss=2.227, ppl=4.68, wps=20617.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=19200, lr=0.000228218, gnorm=1.132, loss_scale=16, train_wall=296, gb_free=7.2, wall=62353
2022-03-05 02:35:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:37:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:37:19 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 11.219 | ppl 2383.07 | wps 40079.4 | wpb 510.9 | bsz 1 | num_updates 19265 | best_loss 6.944
2022-03-05 02:37:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 19265 updates
2022-03-05 02:37:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 02:37:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 02:37:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 99 @ 19265 updates, score 11.219) (writing took 4.1469468008726835 seconds)
2022-03-05 02:37:23 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-05 02:37:23 | INFO | train | epoch 099 | loss 2.234 | ppl 4.7 | wps 20198.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19265 | lr 0.000227832 | gnorm 1.117 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 62571
2022-03-05 02:37:23 | INFO | fairseq.trainer | begin training epoch 100
2022-03-05 02:37:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:39:15 | INFO | train_inner | epoch 100:     35 / 196 loss=2.233, ppl=4.7, wps=19804.3, ups=0.3, wpb=65367, bsz=127.7, num_updates=19300, lr=0.000227626, gnorm=1.097, loss_scale=16, train_wall=298, gb_free=7.2, wall=62683
2022-03-05 02:42:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:44:36 | INFO | train_inner | epoch 100:    136 / 196 loss=2.222, ppl=4.66, wps=20426.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=19400, lr=0.000227038, gnorm=1.121, loss_scale=16, train_wall=298, gb_free=7.2, wall=63004
2022-03-05 02:47:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:47:51 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 11.196 | ppl 2346.32 | wps 41649.3 | wpb 510.9 | bsz 1 | num_updates 19460 | best_loss 6.944
2022-03-05 02:47:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 19460 updates
2022-03-05 02:47:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 02:47:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 02:47:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 100 @ 19460 updates, score 11.196) (writing took 4.0516765313223 seconds)
2022-03-05 02:47:55 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-05 02:47:55 | INFO | train | epoch 100 | loss 2.223 | ppl 4.67 | wps 20211.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19460 | lr 0.000226688 | gnorm 1.113 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 63203
2022-03-05 02:47:55 | INFO | fairseq.trainer | begin training epoch 101
2022-03-05 02:47:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:49:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:50:05 | INFO | train_inner | epoch 101:     41 / 196 loss=2.215, ppl=4.64, wps=19834.5, ups=0.3, wpb=65367, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=1.11, loss_scale=16, train_wall=298, gb_free=7.2, wall=63333
2022-03-05 02:55:23 | INFO | train_inner | epoch 101:    141 / 196 loss=2.212, ppl=4.63, wps=20591.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=19600, lr=0.000225877, gnorm=1.117, loss_scale=16, train_wall=296, gb_free=7.2, wall=63652
2022-03-05 02:56:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:58:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:58:23 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 11.283 | ppl 2491.78 | wps 41589.8 | wpb 510.9 | bsz 1 | num_updates 19654 | best_loss 6.944
2022-03-05 02:58:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 19654 updates
2022-03-05 02:58:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 02:58:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 02:58:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 101 @ 19654 updates, score 11.283) (writing took 4.161106918007135 seconds)
2022-03-05 02:58:27 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-05 02:58:27 | INFO | train | epoch 101 | loss 2.211 | ppl 4.63 | wps 20087.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19654 | lr 0.000225566 | gnorm 1.122 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 63835
2022-03-05 02:58:27 | INFO | fairseq.trainer | begin training epoch 102
2022-03-05 02:58:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:00:53 | INFO | train_inner | epoch 102:     46 / 196 loss=2.204, ppl=4.61, wps=19828.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=19700, lr=0.000225303, gnorm=1.136, loss_scale=16, train_wall=298, gb_free=7.2, wall=63981
2022-03-05 03:03:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:06:14 | INFO | train_inner | epoch 102:    147 / 196 loss=2.2, ppl=4.59, wps=20412.6, ups=0.31, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=1.126, loss_scale=16, train_wall=299, gb_free=7.2, wall=64302
2022-03-05 03:06:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:08:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:08:54 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 11.34 | ppl 2591.77 | wps 41607.8 | wpb 510.9 | bsz 1 | num_updates 19848 | best_loss 6.944
2022-03-05 03:08:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 19848 updates
2022-03-05 03:08:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 03:08:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 03:08:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 102 @ 19848 updates, score 11.34) (writing took 4.090908813290298 seconds)
2022-03-05 03:08:58 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-05 03:08:58 | INFO | train | epoch 102 | loss 2.198 | ppl 4.59 | wps 20102.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19848 | lr 0.000224461 | gnorm 1.132 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 64467
2022-03-05 03:08:58 | INFO | fairseq.trainer | begin training epoch 103
2022-03-05 03:08:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:11:44 | INFO | train_inner | epoch 103:     52 / 196 loss=2.193, ppl=4.57, wps=19817.8, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=1.132, loss_scale=8, train_wall=298, gb_free=7.2, wall=64632
2022-03-05 03:17:02 | INFO | train_inner | epoch 103:    152 / 196 loss=2.195, ppl=4.58, wps=20596.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=20000, lr=0.000223607, gnorm=1.122, loss_scale=16, train_wall=296, gb_free=7.2, wall=64950
2022-03-05 03:19:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:19:26 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 11.318 | ppl 2552.55 | wps 41546.3 | wpb 510.9 | bsz 1 | num_updates 20044 | best_loss 6.944
2022-03-05 03:19:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 20044 updates
2022-03-05 03:19:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 03:19:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 03:19:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 103 @ 20044 updates, score 11.318) (writing took 4.040734722279012 seconds)
2022-03-05 03:19:30 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-05 03:19:30 | INFO | train | epoch 103 | loss 2.188 | ppl 4.56 | wps 20298.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 20044 | lr 0.000223361 | gnorm 1.125 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 65099
2022-03-05 03:19:30 | INFO | fairseq.trainer | begin training epoch 104
2022-03-05 03:19:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:20:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:22:32 | INFO | train_inner | epoch 104:     57 / 196 loss=2.168, ppl=4.5, wps=19838.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=20100, lr=0.00022305, gnorm=1.129, loss_scale=16, train_wall=298, gb_free=7.2, wall=65280
2022-03-05 03:25:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:27:52 | INFO | train_inner | epoch 104:    158 / 196 loss=2.19, ppl=4.56, wps=20426.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=20200, lr=0.000222497, gnorm=1.135, loss_scale=8, train_wall=298, gb_free=7.2, wall=65601
2022-03-05 03:29:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:29:58 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 11.36 | ppl 2628.77 | wps 41351 | wpb 510.9 | bsz 1 | num_updates 20238 | best_loss 6.944
2022-03-05 03:29:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 20238 updates
2022-03-05 03:29:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 03:30:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 03:30:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 104 @ 20238 updates, score 11.36) (writing took 4.141276758164167 seconds)
2022-03-05 03:30:02 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-05 03:30:02 | INFO | train | epoch 104 | loss 2.176 | ppl 4.52 | wps 20106.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 20238 | lr 0.000222288 | gnorm 1.137 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 65730
2022-03-05 03:30:02 | INFO | fairseq.trainer | begin training epoch 105
2022-03-05 03:30:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:33:19 | INFO | train_inner | epoch 105:     62 / 196 loss=2.155, ppl=4.45, wps=20008.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=1.141, loss_scale=16, train_wall=295, gb_free=7.2, wall=65927
2022-03-05 03:38:37 | INFO | train_inner | epoch 105:    162 / 196 loss=2.179, ppl=4.53, wps=20617.2, ups=0.31, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=1.131, loss_scale=16, train_wall=296, gb_free=7.2, wall=66245
2022-03-05 03:39:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:40:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:40:29 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 11.449 | ppl 2795.56 | wps 41479.9 | wpb 510.9 | bsz 1 | num_updates 20433 | best_loss 6.944
2022-03-05 03:40:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 20433 updates
2022-03-05 03:40:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 03:40:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 03:40:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 105 @ 20433 updates, score 11.449) (writing took 4.120266475714743 seconds)
2022-03-05 03:40:34 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-05 03:40:34 | INFO | train | epoch 105 | loss 2.165 | ppl 4.48 | wps 20201.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 20433 | lr 0.000221225 | gnorm 1.125 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 66362
2022-03-05 03:40:34 | INFO | fairseq.trainer | begin training epoch 106
2022-03-05 03:40:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:44:06 | INFO | train_inner | epoch 106:     67 / 196 loss=2.145, ppl=4.42, wps=19835.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=20500, lr=0.000220863, gnorm=1.114, loss_scale=16, train_wall=298, gb_free=7.2, wall=66575
2022-03-05 03:46:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:49:27 | INFO | train_inner | epoch 106:    168 / 196 loss=2.169, ppl=4.5, wps=20423.6, ups=0.31, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=1.119, loss_scale=16, train_wall=298, gb_free=7.2, wall=66896
2022-03-05 03:50:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:51:01 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 11.404 | ppl 2709.55 | wps 41556.5 | wpb 510.9 | bsz 1 | num_updates 20628 | best_loss 6.944
2022-03-05 03:51:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 20628 updates
2022-03-05 03:51:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 03:51:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 03:51:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 106 @ 20628 updates, score 11.404) (writing took 4.260938104242086 seconds)
2022-03-05 03:51:05 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-05 03:51:05 | INFO | train | epoch 106 | loss 2.155 | ppl 4.45 | wps 20208.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 20628 | lr 0.000220177 | gnorm 1.125 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 66993
2022-03-05 03:51:05 | INFO | fairseq.trainer | begin training epoch 107
2022-03-05 03:51:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:52:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:54:57 | INFO | train_inner | epoch 107:     73 / 196 loss=2.128, ppl=4.37, wps=19811.9, ups=0.3, wpb=65359.9, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=1.136, loss_scale=8, train_wall=298, gb_free=7.2, wall=67225
2022-03-05 03:59:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:00:18 | INFO | train_inner | epoch 107:    174 / 196 loss=2.169, ppl=4.5, wps=20413.7, ups=0.31, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=1.135, loss_scale=8, train_wall=299, gb_free=7.2, wall=67546
2022-03-05 04:01:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:01:33 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 11.432 | ppl 2762.01 | wps 41565 | wpb 510.9 | bsz 1 | num_updates 20822 | best_loss 6.944
2022-03-05 04:01:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 20822 updates
2022-03-05 04:01:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 04:01:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 04:01:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 107 @ 20822 updates, score 11.432) (writing took 3.9988813400268555 seconds)
2022-03-05 04:01:37 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-05 04:01:37 | INFO | train | epoch 107 | loss 2.144 | ppl 4.42 | wps 20101.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 20822 | lr 0.000219149 | gnorm 1.134 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 67625
2022-03-05 04:01:37 | INFO | fairseq.trainer | begin training epoch 108
2022-03-05 04:01:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:05:45 | INFO | train_inner | epoch 108:     78 / 196 loss=2.111, ppl=4.32, wps=20028.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=20900, lr=0.000218739, gnorm=1.159, loss_scale=8, train_wall=295, gb_free=7.2, wall=67873
2022-03-05 04:10:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:11:06 | INFO | train_inner | epoch 108:    179 / 196 loss=2.162, ppl=4.47, wps=20422, ups=0.31, wpb=65532.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=1.121, loss_scale=8, train_wall=298, gb_free=7.2, wall=68194
2022-03-05 04:11:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:12:04 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 11.46 | ppl 2817.49 | wps 41470.7 | wpb 510.9 | bsz 1 | num_updates 21017 | best_loss 6.944
2022-03-05 04:12:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 21017 updates
2022-03-05 04:12:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 04:12:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 04:12:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 108 @ 21017 updates, score 11.46) (writing took 4.18006378877908 seconds)
2022-03-05 04:12:08 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-05 04:12:08 | INFO | train | epoch 108 | loss 2.133 | ppl 4.39 | wps 20208.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21017 | lr 0.00021813 | gnorm 1.138 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 68256
2022-03-05 04:12:08 | INFO | fairseq.trainer | begin training epoch 109
2022-03-05 04:12:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:16:32 | INFO | train_inner | epoch 109:     83 / 196 loss=2.094, ppl=4.27, wps=20016.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=1.119, loss_scale=8, train_wall=295, gb_free=7.2, wall=68520
2022-03-05 04:19:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:21:53 | INFO | train_inner | epoch 109:    184 / 196 loss=2.158, ppl=4.46, wps=20414.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=21200, lr=0.000217186, gnorm=1.146, loss_scale=8, train_wall=299, gb_free=7.2, wall=68841
2022-03-05 04:22:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:22:36 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 11.523 | ppl 2942.24 | wps 41385.5 | wpb 510.9 | bsz 1 | num_updates 21212 | best_loss 6.944
2022-03-05 04:22:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 21212 updates
2022-03-05 04:22:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 04:22:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 04:22:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 109 @ 21212 updates, score 11.523) (writing took 4.3365973541513085 seconds)
2022-03-05 04:22:40 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-05 04:22:40 | INFO | train | epoch 109 | loss 2.123 | ppl 4.36 | wps 20199.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21212 | lr 0.000217125 | gnorm 1.133 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 68888
2022-03-05 04:22:40 | INFO | fairseq.trainer | begin training epoch 110
2022-03-05 04:22:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:27:20 | INFO | train_inner | epoch 110:     88 / 196 loss=2.081, ppl=4.23, wps=20005.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=21300, lr=0.000216676, gnorm=1.138, loss_scale=16, train_wall=295, gb_free=7.2, wall=69168
2022-03-05 04:32:38 | INFO | train_inner | epoch 110:    188 / 196 loss=2.146, ppl=4.43, wps=20614.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=21400, lr=0.000216169, gnorm=1.122, loss_scale=16, train_wall=296, gb_free=7.2, wall=69486
2022-03-05 04:32:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:33:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:33:08 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 11.502 | ppl 2899.95 | wps 41503.9 | wpb 510.9 | bsz 1 | num_updates 21407 | best_loss 6.944
2022-03-05 04:33:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 21407 updates
2022-03-05 04:33:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 04:33:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 04:33:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 110 @ 21407 updates, score 11.502) (writing took 4.092251593247056 seconds)
2022-03-05 04:33:12 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-05 04:33:12 | INFO | train | epoch 110 | loss 2.113 | ppl 4.32 | wps 20204.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21407 | lr 0.000216134 | gnorm 1.129 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 69520
2022-03-05 04:33:12 | INFO | fairseq.trainer | begin training epoch 111
2022-03-05 04:33:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:38:07 | INFO | train_inner | epoch 111:     93 / 196 loss=2.076, ppl=4.22, wps=19824.9, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=1.132, loss_scale=16, train_wall=298, gb_free=7.2, wall=69816
2022-03-05 04:39:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:42:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:43:32 | INFO | train_inner | epoch 111:    195 / 196 loss=2.133, ppl=4.39, wps=20219.3, ups=0.31, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=1.141, loss_scale=8, train_wall=301, gb_free=7.2, wall=70140
2022-03-05 04:43:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:43:39 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 11.581 | ppl 3063.55 | wps 41443.9 | wpb 510.9 | bsz 1 | num_updates 21601 | best_loss 6.944
2022-03-05 04:43:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 21601 updates
2022-03-05 04:43:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 04:43:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 04:43:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 111 @ 21601 updates, score 11.581) (writing took 4.180945836007595 seconds)
2022-03-05 04:43:43 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-05 04:43:43 | INFO | train | epoch 111 | loss 2.103 | ppl 4.29 | wps 20099.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 21601 | lr 0.000215161 | gnorm 1.137 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 70152
2022-03-05 04:43:43 | INFO | fairseq.trainer | begin training epoch 112
2022-03-05 04:43:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:48:58 | INFO | train_inner | epoch 112:     99 / 196 loss=2.059, ppl=4.17, wps=20016.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21700, lr=0.000214669, gnorm=1.128, loss_scale=16, train_wall=295, gb_free=7.2, wall=70466
2022-03-05 04:54:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:54:11 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 11.6 | ppl 3104.54 | wps 41447.3 | wpb 510.9 | bsz 1 | num_updates 21797 | best_loss 6.944
2022-03-05 04:54:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 21797 updates
2022-03-05 04:54:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 04:54:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 04:54:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 112 @ 21797 updates, score 11.6) (writing took 3.950625150464475 seconds)
2022-03-05 04:54:15 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-05 04:54:15 | INFO | train | epoch 112 | loss 2.093 | ppl 4.27 | wps 20314.2 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 21797 | lr 0.000214191 | gnorm 1.136 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 70783
2022-03-05 04:54:15 | INFO | fairseq.trainer | begin training epoch 113
2022-03-05 04:54:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:54:25 | INFO | train_inner | epoch 113:      3 / 196 loss=2.125, ppl=4.36, wps=20027.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=21800, lr=0.000214176, gnorm=1.146, loss_scale=16, train_wall=295, gb_free=7.2, wall=70793
2022-03-05 04:55:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:59:46 | INFO | train_inner | epoch 113:    104 / 196 loss=2.052, ppl=4.15, wps=20407.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=21900, lr=0.000213687, gnorm=1.125, loss_scale=16, train_wall=299, gb_free=7.2, wall=71114
2022-03-05 05:02:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:04:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:04:43 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 11.573 | ppl 3045.84 | wps 41385.3 | wpb 510.9 | bsz 1 | num_updates 21991 | best_loss 6.944
2022-03-05 05:04:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 21991 updates
2022-03-05 05:04:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 05:04:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 05:04:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 113 @ 21991 updates, score 11.573) (writing took 3.8135808054357767 seconds)
2022-03-05 05:04:46 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-05 05:04:46 | INFO | train | epoch 113 | loss 2.082 | ppl 4.23 | wps 20106.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 21991 | lr 0.000213244 | gnorm 1.136 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 71415
2022-03-05 05:04:46 | INFO | fairseq.trainer | begin training epoch 114
2022-03-05 05:04:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:05:15 | INFO | train_inner | epoch 114:      9 / 196 loss=2.107, ppl=4.31, wps=19841.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=22000, lr=0.000213201, gnorm=1.147, loss_scale=16, train_wall=298, gb_free=7.2, wall=71443
2022-03-05 05:09:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:10:36 | INFO | train_inner | epoch 114:    110 / 196 loss=2.043, ppl=4.12, wps=20411.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=22100, lr=0.000212718, gnorm=1.136, loss_scale=16, train_wall=299, gb_free=7.2, wall=71764
2022-03-05 05:15:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:15:14 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 11.664 | ppl 3245.34 | wps 41429.7 | wpb 510.9 | bsz 1 | num_updates 22186 | best_loss 6.944
2022-03-05 05:15:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 22186 updates
2022-03-05 05:15:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 05:15:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 05:15:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 114 @ 22186 updates, score 11.664) (writing took 3.8227002657949924 seconds)
2022-03-05 05:15:18 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-05 05:15:18 | INFO | train | epoch 114 | loss 2.074 | ppl 4.21 | wps 20210.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22186 | lr 0.000212305 | gnorm 1.144 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 72046
2022-03-05 05:15:18 | INFO | fairseq.trainer | begin training epoch 115
2022-03-05 05:15:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:16:02 | INFO | train_inner | epoch 115:     14 / 196 loss=2.099, ppl=4.28, wps=20031.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=22200, lr=0.000212238, gnorm=1.151, loss_scale=16, train_wall=295, gb_free=7.2, wall=72091
2022-03-05 05:16:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:21:23 | INFO | train_inner | epoch 115:    115 / 196 loss=2.039, ppl=4.11, wps=20412.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=22300, lr=0.000211762, gnorm=1.132, loss_scale=8, train_wall=299, gb_free=7.2, wall=72412
2022-03-05 05:25:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:25:45 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 11.652 | ppl 3218.44 | wps 41564.1 | wpb 510.9 | bsz 1 | num_updates 22381 | best_loss 6.944
2022-03-05 05:25:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 22381 updates
2022-03-05 05:25:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 05:25:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 05:25:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 115 @ 22381 updates, score 11.652) (writing took 4.1634701192379 seconds)
2022-03-05 05:25:50 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-05 05:25:50 | INFO | train | epoch 115 | loss 2.064 | ppl 4.18 | wps 20203.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22381 | lr 0.000211378 | gnorm 1.139 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 72678
2022-03-05 05:25:50 | INFO | fairseq.trainer | begin training epoch 116
2022-03-05 05:25:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:26:50 | INFO | train_inner | epoch 116:     19 / 196 loss=2.09, ppl=4.26, wps=20017.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=22400, lr=0.000211289, gnorm=1.143, loss_scale=16, train_wall=295, gb_free=7.2, wall=72738
2022-03-05 05:28:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:32:11 | INFO | train_inner | epoch 116:    120 / 196 loss=2.037, ppl=4.1, wps=20409.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=22500, lr=0.000210819, gnorm=1.141, loss_scale=8, train_wall=299, gb_free=7.2, wall=73059
2022-03-05 05:36:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:36:17 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 11.736 | ppl 3410.72 | wps 41444 | wpb 510.9 | bsz 1 | num_updates 22576 | best_loss 6.944
2022-03-05 05:36:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 22576 updates
2022-03-05 05:36:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 05:36:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 05:36:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 116 @ 22576 updates, score 11.736) (writing took 4.13938407972455 seconds)
2022-03-05 05:36:21 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-05 05:36:21 | INFO | train | epoch 116 | loss 2.055 | ppl 4.15 | wps 20198.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22576 | lr 0.000210463 | gnorm 1.145 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 73310
2022-03-05 05:36:21 | INFO | fairseq.trainer | begin training epoch 117
2022-03-05 05:36:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:37:38 | INFO | train_inner | epoch 117:     24 / 196 loss=2.061, ppl=4.17, wps=20013.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=22600, lr=0.000210352, gnorm=1.146, loss_scale=16, train_wall=295, gb_free=7.2, wall=73386
2022-03-05 05:42:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:42:59 | INFO | train_inner | epoch 117:    125 / 196 loss=2.033, ppl=4.09, wps=20419.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=22700, lr=0.000209888, gnorm=1.157, loss_scale=16, train_wall=298, gb_free=7.2, wall=73707
2022-03-05 05:44:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:46:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:46:49 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 11.77 | ppl 3491.27 | wps 41529 | wpb 510.9 | bsz 1 | num_updates 22770 | best_loss 6.944
2022-03-05 05:46:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 22770 updates
2022-03-05 05:46:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 05:46:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 05:46:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 117 @ 22770 updates, score 11.77) (writing took 4.100211491808295 seconds)
2022-03-05 05:46:53 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-05 05:46:53 | INFO | train | epoch 117 | loss 2.044 | ppl 4.13 | wps 20108.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22770 | lr 0.000209565 | gnorm 1.15 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 73941
2022-03-05 05:46:53 | INFO | fairseq.trainer | begin training epoch 118
2022-03-05 05:46:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:48:28 | INFO | train_inner | epoch 118:     30 / 196 loss=2.058, ppl=4.16, wps=19837.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=22800, lr=0.000209427, gnorm=1.142, loss_scale=8, train_wall=298, gb_free=7.2, wall=74036
2022-03-05 05:51:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:53:49 | INFO | train_inner | epoch 118:    131 / 196 loss=2.023, ppl=4.06, wps=20422.5, ups=0.31, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=1.138, loss_scale=8, train_wall=298, gb_free=7.2, wall=74357
2022-03-05 05:57:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:57:20 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 11.74 | ppl 3419.36 | wps 41531.3 | wpb 510.9 | bsz 1 | num_updates 22965 | best_loss 6.944
2022-03-05 05:57:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 22965 updates
2022-03-05 05:57:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 05:57:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 05:57:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 118 @ 22965 updates, score 11.74) (writing took 3.205532849766314 seconds)
2022-03-05 05:57:23 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-05 05:57:23 | INFO | train | epoch 118 | loss 2.037 | ppl 4.1 | wps 20240.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22965 | lr 0.000208673 | gnorm 1.142 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 74572
2022-03-05 05:57:23 | INFO | fairseq.trainer | begin training epoch 119
2022-03-05 05:57:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:59:15 | INFO | train_inner | epoch 119:     35 / 196 loss=2.041, ppl=4.12, wps=20082.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=23000, lr=0.000208514, gnorm=1.15, loss_scale=16, train_wall=295, gb_free=7.2, wall=74683
2022-03-05 06:04:32 | INFO | train_inner | epoch 119:    135 / 196 loss=2.019, ppl=4.05, wps=20619.5, ups=0.31, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=1.129, loss_scale=16, train_wall=296, gb_free=7.2, wall=75001
2022-03-05 06:04:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:07:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:07:51 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 11.803 | ppl 3573.11 | wps 41580.2 | wpb 510.9 | bsz 1 | num_updates 23160 | best_loss 6.944
2022-03-05 06:07:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 23160 updates
2022-03-05 06:07:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 06:07:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 06:07:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 119 @ 23160 updates, score 11.803) (writing took 3.157567913644016 seconds)
2022-03-05 06:07:54 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-05 06:07:54 | INFO | train | epoch 119 | loss 2.026 | ppl 4.07 | wps 20247.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 23160 | lr 0.000207793 | gnorm 1.143 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 75202
2022-03-05 06:07:54 | INFO | fairseq.trainer | begin training epoch 120
2022-03-05 06:07:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:10:01 | INFO | train_inner | epoch 120:     40 / 196 loss=2.028, ppl=4.08, wps=19892.7, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=23200, lr=0.000207614, gnorm=1.156, loss_scale=8, train_wall=298, gb_free=7.2, wall=75329
2022-03-05 06:15:19 | INFO | train_inner | epoch 120:    140 / 196 loss=2.02, ppl=4.06, wps=20628.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=23300, lr=0.000207168, gnorm=1.135, loss_scale=16, train_wall=295, gb_free=7.2, wall=75647
2022-03-05 06:18:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:18:21 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 11.827 | ppl 3634.14 | wps 41607.9 | wpb 510.9 | bsz 1 | num_updates 23356 | best_loss 6.944
2022-03-05 06:18:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 23356 updates
2022-03-05 06:18:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 06:18:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 06:18:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 120 @ 23356 updates, score 11.827) (writing took 3.2871268717572093 seconds)
2022-03-05 06:18:24 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-05 06:18:24 | INFO | train | epoch 120 | loss 2.018 | ppl 4.05 | wps 20337.7 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 23356 | lr 0.000206919 | gnorm 1.144 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 75833
2022-03-05 06:18:24 | INFO | fairseq.trainer | begin training epoch 121
2022-03-05 06:18:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:18:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:20:47 | INFO | train_inner | epoch 121:     45 / 196 loss=2.009, ppl=4.03, wps=19880.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=23400, lr=0.000206725, gnorm=1.148, loss_scale=16, train_wall=298, gb_free=7.2, wall=75976
2022-03-05 06:25:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:25:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:26:11 | INFO | train_inner | epoch 121:    147 / 196 loss=2.007, ppl=4.02, wps=20224, ups=0.31, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=1.15, loss_scale=8, train_wall=301, gb_free=7.2, wall=76300
2022-03-05 06:28:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:28:51 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 11.957 | ppl 3975.58 | wps 41641.5 | wpb 510.9 | bsz 1 | num_updates 23549 | best_loss 6.944
2022-03-05 06:28:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 23549 updates
2022-03-05 06:28:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 06:28:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 06:28:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 121 @ 23549 updates, score 11.957) (writing took 3.3683999283239245 seconds)
2022-03-05 06:28:55 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-05 06:28:55 | INFO | train | epoch 121 | loss 2.009 | ppl 4.02 | wps 20035.2 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 23549 | lr 0.00020607 | gnorm 1.14 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 76463
2022-03-05 06:28:55 | INFO | fairseq.trainer | begin training epoch 122
2022-03-05 06:28:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:31:37 | INFO | train_inner | epoch 122:     51 / 196 loss=2.002, ppl=4, wps=20074.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=23600, lr=0.000205847, gnorm=1.151, loss_scale=8, train_wall=295, gb_free=7.2, wall=76625
2022-03-05 06:36:55 | INFO | train_inner | epoch 122:    151 / 196 loss=2.008, ppl=4.02, wps=20630.3, ups=0.31, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=1.163, loss_scale=16, train_wall=295, gb_free=7.2, wall=76943
2022-03-05 06:39:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:39:22 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 11.86 | ppl 3717.68 | wps 41492.3 | wpb 510.9 | bsz 1 | num_updates 23745 | best_loss 6.944
2022-03-05 06:39:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 23745 updates
2022-03-05 06:39:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 06:39:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 06:39:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 122 @ 23745 updates, score 11.86) (writing took 2.9184052096679807 seconds)
2022-03-05 06:39:25 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-05 06:39:25 | INFO | train | epoch 122 | loss 2.001 | ppl 4 | wps 20352.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 23745 | lr 0.000205217 | gnorm 1.161 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 77093
2022-03-05 06:39:25 | INFO | fairseq.trainer | begin training epoch 123
2022-03-05 06:39:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:39:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:42:23 | INFO | train_inner | epoch 123:     56 / 196 loss=1.992, ppl=3.98, wps=19918.1, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=23800, lr=0.00020498, gnorm=1.149, loss_scale=16, train_wall=298, gb_free=7.2, wall=77271
2022-03-05 06:42:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:47:44 | INFO | train_inner | epoch 123:    157 / 196 loss=2.001, ppl=4, wps=20424.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=23900, lr=0.000204551, gnorm=1.16, loss_scale=8, train_wall=298, gb_free=7.2, wall=77592
2022-03-05 06:49:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:49:52 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 11.954 | ppl 3967.06 | wps 41523.3 | wpb 510.9 | bsz 1 | num_updates 23939 | best_loss 6.944
2022-03-05 06:49:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 23939 updates
2022-03-05 06:49:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 06:49:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 06:49:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 123 @ 23939 updates, score 11.954) (writing took 3.1834703497588634 seconds)
2022-03-05 06:49:55 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-05 06:49:55 | INFO | train | epoch 123 | loss 1.992 | ppl 3.98 | wps 20144.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23939 | lr 0.000204384 | gnorm 1.161 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 77724
2022-03-05 06:49:55 | INFO | fairseq.trainer | begin training epoch 124
2022-03-05 06:49:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:53:09 | INFO | train_inner | epoch 124:     61 / 196 loss=1.972, ppl=3.92, wps=20088.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=24000, lr=0.000204124, gnorm=1.156, loss_scale=16, train_wall=295, gb_free=7.2, wall=77917
2022-03-05 06:54:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:58:30 | INFO | train_inner | epoch 124:    162 / 196 loss=2, ppl=4, wps=20444.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=24100, lr=0.0002037, gnorm=1.158, loss_scale=8, train_wall=298, gb_free=7.2, wall=78238
2022-03-05 07:00:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:00:22 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 11.919 | ppl 3871.64 | wps 41714.7 | wpb 510.9 | bsz 1 | num_updates 24134 | best_loss 6.944
2022-03-05 07:00:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 24134 updates
2022-03-05 07:00:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 07:00:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 07:00:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 124 @ 24134 updates, score 11.919) (writing took 2.9659640081226826 seconds)
2022-03-05 07:00:25 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-05 07:00:25 | INFO | train | epoch 124 | loss 1.984 | ppl 3.96 | wps 20270.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24134 | lr 0.000203557 | gnorm 1.152 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 78353
2022-03-05 07:00:25 | INFO | fairseq.trainer | begin training epoch 125
2022-03-05 07:00:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:03:55 | INFO | train_inner | epoch 125:     66 / 196 loss=1.963, ppl=3.9, wps=20108.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=24200, lr=0.000203279, gnorm=1.15, loss_scale=16, train_wall=295, gb_free=7.2, wall=78563
2022-03-05 07:07:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:09:16 | INFO | train_inner | epoch 125:    167 / 196 loss=1.995, ppl=3.99, wps=20428.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=24300, lr=0.00020286, gnorm=1.173, loss_scale=16, train_wall=298, gb_free=7.2, wall=78884
2022-03-05 07:10:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:10:52 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 11.883 | ppl 3777.59 | wps 41532.6 | wpb 510.9 | bsz 1 | num_updates 24329 | best_loss 6.944
2022-03-05 07:10:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 24329 updates
2022-03-05 07:10:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 07:10:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 07:10:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 125 @ 24329 updates, score 11.883) (writing took 2.9553579818457365 seconds)
2022-03-05 07:10:55 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-05 07:10:55 | INFO | train | epoch 125 | loss 1.977 | ppl 3.94 | wps 20249.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24329 | lr 0.000202739 | gnorm 1.159 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 78983
2022-03-05 07:10:55 | INFO | fairseq.trainer | begin training epoch 126
2022-03-05 07:10:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:14:41 | INFO | train_inner | epoch 126:     71 / 196 loss=1.951, ppl=3.87, wps=20071, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=24400, lr=0.000202444, gnorm=1.14, loss_scale=16, train_wall=295, gb_free=7.2, wall=79209
2022-03-05 07:14:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:18:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:20:05 | INFO | train_inner | epoch 126:    173 / 196 loss=1.987, ppl=3.97, wps=20218.3, ups=0.31, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=1.161, loss_scale=8, train_wall=301, gb_free=7.2, wall=79534
2022-03-05 07:21:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:21:23 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 11.959 | ppl 3980.98 | wps 41512 | wpb 510.9 | bsz 1 | num_updates 24523 | best_loss 6.944
2022-03-05 07:21:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 24523 updates
2022-03-05 07:21:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 07:21:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 07:21:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 126 @ 24523 updates, score 11.959) (writing took 2.883828667923808 seconds)
2022-03-05 07:21:26 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-05 07:21:26 | INFO | train | epoch 126 | loss 1.967 | ppl 3.91 | wps 20134.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 24523 | lr 0.000201936 | gnorm 1.153 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 79614
2022-03-05 07:21:26 | INFO | fairseq.trainer | begin training epoch 127
2022-03-05 07:21:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:25:31 | INFO | train_inner | epoch 127:     77 / 196 loss=1.945, ppl=3.85, wps=20092.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=24600, lr=0.000201619, gnorm=1.145, loss_scale=8, train_wall=295, gb_free=7.2, wall=79859
2022-03-05 07:30:49 | INFO | train_inner | epoch 127:    177 / 196 loss=1.982, ppl=3.95, wps=20611.7, ups=0.31, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=1.156, loss_scale=16, train_wall=296, gb_free=7.2, wall=80177
2022-03-05 07:31:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:31:54 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 11.947 | ppl 3948.55 | wps 41500.4 | wpb 510.9 | bsz 1 | num_updates 24719 | best_loss 6.944
2022-03-05 07:31:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 24719 updates
2022-03-05 07:31:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 07:31:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 07:31:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 127 @ 24719 updates, score 11.947) (writing took 3.034137500450015 seconds)
2022-03-05 07:31:57 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-05 07:31:57 | INFO | train | epoch 127 | loss 1.961 | ppl 3.89 | wps 20334.3 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 24719 | lr 0.000201134 | gnorm 1.151 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 80245
2022-03-05 07:31:57 | INFO | fairseq.trainer | begin training epoch 128
2022-03-05 07:31:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:32:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:36:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:36:21 | INFO | train_inner | epoch 128:     83 / 196 loss=1.928, ppl=3.81, wps=19695.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=24800, lr=0.000200805, gnorm=1.155, loss_scale=8, train_wall=301, gb_free=7.2, wall=80509
2022-03-05 07:41:38 | INFO | train_inner | epoch 128:    183 / 196 loss=1.977, ppl=3.94, wps=20627.4, ups=0.31, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=1.15, loss_scale=8, train_wall=296, gb_free=7.2, wall=80826
2022-03-05 07:42:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:42:24 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 11.914 | ppl 3859.45 | wps 41708.1 | wpb 510.9 | bsz 1 | num_updates 24913 | best_loss 6.944
2022-03-05 07:42:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 24913 updates
2022-03-05 07:42:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 07:42:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 07:42:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 128 @ 24913 updates, score 11.914) (writing took 2.9691256852820516 seconds)
2022-03-05 07:42:27 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-05 07:42:27 | INFO | train | epoch 128 | loss 1.951 | ppl 3.87 | wps 20144 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 24913 | lr 0.000200349 | gnorm 1.152 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 80875
2022-03-05 07:42:27 | INFO | fairseq.trainer | begin training epoch 129
2022-03-05 07:42:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:46:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:47:07 | INFO | train_inner | epoch 129:     88 / 196 loss=1.917, ppl=3.78, wps=19901.7, ups=0.3, wpb=65367, bsz=127.7, num_updates=25000, lr=0.0002, gnorm=1.159, loss_scale=8, train_wall=298, gb_free=7.2, wall=81155
2022-03-05 07:52:25 | INFO | train_inner | epoch 129:    188 / 196 loss=1.976, ppl=3.93, wps=20599.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=25100, lr=0.000199601, gnorm=1.166, loss_scale=8, train_wall=296, gb_free=7.2, wall=81473
2022-03-05 07:52:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:52:56 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 11.986 | ppl 4055.17 | wps 39866.9 | wpb 510.9 | bsz 1 | num_updates 25108 | best_loss 6.944
2022-03-05 07:52:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 25108 updates
2022-03-05 07:52:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 07:53:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 07:53:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 129 @ 25108 updates, score 11.986) (writing took 6.550256667658687 seconds)
2022-03-05 07:53:03 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-05 07:53:03 | INFO | train | epoch 129 | loss 1.945 | ppl 3.85 | wps 20078.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 25108 | lr 0.000199569 | gnorm 1.163 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 81511
2022-03-05 07:53:03 | INFO | fairseq.trainer | begin training epoch 130
2022-03-05 07:53:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:57:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:58:02 | INFO | train_inner | epoch 130:     93 / 196 loss=1.908, ppl=3.75, wps=19410.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=25200, lr=0.000199205, gnorm=1.151, loss_scale=8, train_wall=299, gb_free=7.2, wall=81810
2022-03-05 08:03:20 | INFO | train_inner | epoch 130:    193 / 196 loss=1.971, ppl=3.92, wps=20593.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=25300, lr=0.000198811, gnorm=1.169, loss_scale=8, train_wall=296, gb_free=7.2, wall=82128
2022-03-05 08:03:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:03:34 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 12.009 | ppl 4121.98 | wps 41379.5 | wpb 510.9 | bsz 1 | num_updates 25303 | best_loss 6.944
2022-03-05 08:03:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 25303 updates
2022-03-05 08:03:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 08:03:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 08:03:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 130 @ 25303 updates, score 12.009) (writing took 3.9030116721987724 seconds)
2022-03-05 08:03:38 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-05 08:03:38 | INFO | train | epoch 130 | loss 1.937 | ppl 3.83 | wps 20093.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 25303 | lr 0.000198799 | gnorm 1.162 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 82146
2022-03-05 08:03:38 | INFO | fairseq.trainer | begin training epoch 131
2022-03-05 08:03:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:08:46 | INFO | train_inner | epoch 131:     97 / 196 loss=1.894, ppl=3.72, wps=20029.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25400, lr=0.000198419, gnorm=1.16, loss_scale=16, train_wall=295, gb_free=7.2, wall=82454
2022-03-05 08:11:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:11:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:14:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:14:05 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 12.099 | ppl 4387.44 | wps 41555.5 | wpb 510.9 | bsz 1 | num_updates 25497 | best_loss 6.944
2022-03-05 08:14:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 25497 updates
2022-03-05 08:14:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 08:14:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 08:14:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 131 @ 25497 updates, score 12.099) (writing took 3.8883758513256907 seconds)
2022-03-05 08:14:09 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-05 08:14:09 | INFO | train | epoch 131 | loss 1.928 | ppl 3.81 | wps 20105.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25497 | lr 0.000198041 | gnorm 1.161 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 82777
2022-03-05 08:14:09 | INFO | fairseq.trainer | begin training epoch 132
2022-03-05 08:14:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:14:19 | INFO | train_inner | epoch 132:      3 / 196 loss=1.96, ppl=3.89, wps=19645.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=25500, lr=0.00019803, gnorm=1.165, loss_scale=8, train_wall=301, gb_free=7.2, wall=82787
2022-03-05 08:19:37 | INFO | train_inner | epoch 132:    103 / 196 loss=1.891, ppl=3.71, wps=20613.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=25600, lr=0.000197642, gnorm=1.166, loss_scale=16, train_wall=296, gb_free=7.2, wall=83105
2022-03-05 08:23:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:24:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:24:37 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 12.009 | ppl 4120.6 | wps 41591.9 | wpb 510.9 | bsz 1 | num_updates 25692 | best_loss 6.944
2022-03-05 08:24:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 25692 updates
2022-03-05 08:24:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 08:24:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 08:24:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 132 @ 25692 updates, score 12.009) (writing took 4.017078263685107 seconds)
2022-03-05 08:24:41 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-05 08:24:41 | INFO | train | epoch 132 | loss 1.922 | ppl 3.79 | wps 20205.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 25692 | lr 0.000197288 | gnorm 1.171 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 83409
2022-03-05 08:24:41 | INFO | fairseq.trainer | begin training epoch 133
2022-03-05 08:24:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:25:06 | INFO | train_inner | epoch 133:      8 / 196 loss=1.951, ppl=3.87, wps=19835.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=25700, lr=0.000197257, gnorm=1.178, loss_scale=8, train_wall=298, gb_free=7.2, wall=83435
2022-03-05 08:30:24 | INFO | train_inner | epoch 133:    108 / 196 loss=1.885, ppl=3.69, wps=20618.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=25800, lr=0.000196875, gnorm=1.156, loss_scale=8, train_wall=296, gb_free=7.2, wall=83752
2022-03-05 08:35:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:35:08 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 12.104 | ppl 4402.6 | wps 41575 | wpb 510.9 | bsz 1 | num_updates 25888 | best_loss 6.944
2022-03-05 08:35:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 25888 updates
2022-03-05 08:35:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 08:35:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 08:35:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 133 @ 25888 updates, score 12.104) (writing took 4.023588531650603 seconds)
2022-03-05 08:35:12 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-05 08:35:12 | INFO | train | epoch 133 | loss 1.913 | ppl 3.77 | wps 20311.1 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 25888 | lr 0.00019654 | gnorm 1.163 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 84041
2022-03-05 08:35:12 | INFO | fairseq.trainer | begin training epoch 134
2022-03-05 08:35:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:35:51 | INFO | train_inner | epoch 134:     12 / 196 loss=1.934, ppl=3.82, wps=20023.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=25900, lr=0.000196494, gnorm=1.169, loss_scale=16, train_wall=295, gb_free=7.2, wall=84079
2022-03-05 08:37:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:40:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:41:15 | INFO | train_inner | epoch 134:    114 / 196 loss=1.885, ppl=3.69, wps=20217.7, ups=0.31, wpb=65532.4, bsz=128, num_updates=26000, lr=0.000196116, gnorm=1.152, loss_scale=8, train_wall=301, gb_free=7.2, wall=84403
2022-03-05 08:45:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:45:40 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 12.096 | ppl 4379.28 | wps 41625.2 | wpb 510.9 | bsz 1 | num_updates 26082 | best_loss 6.944
2022-03-05 08:45:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 26082 updates
2022-03-05 08:45:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 08:45:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 08:45:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 134 @ 26082 updates, score 12.096) (writing took 4.013861685991287 seconds)
2022-03-05 08:45:44 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-05 08:45:44 | INFO | train | epoch 134 | loss 1.906 | ppl 3.75 | wps 20106.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26082 | lr 0.000195808 | gnorm 1.171 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 84672
2022-03-05 08:45:44 | INFO | fairseq.trainer | begin training epoch 135
2022-03-05 08:45:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:46:41 | INFO | train_inner | epoch 135:     18 / 196 loss=1.923, ppl=3.79, wps=20021.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=26100, lr=0.00019574, gnorm=1.192, loss_scale=8, train_wall=295, gb_free=7.2, wall=84729
2022-03-05 08:50:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:52:02 | INFO | train_inner | epoch 135:    119 / 196 loss=1.88, ppl=3.68, wps=20419.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=26200, lr=0.000195366, gnorm=1.168, loss_scale=8, train_wall=298, gb_free=7.2, wall=85050
2022-03-05 08:56:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:56:11 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 12.151 | ppl 4547.97 | wps 41482.9 | wpb 510.9 | bsz 1 | num_updates 26277 | best_loss 6.944
2022-03-05 08:56:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 26277 updates
2022-03-05 08:56:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 08:56:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 08:56:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 135 @ 26277 updates, score 12.151) (writing took 4.202868881635368 seconds)
2022-03-05 08:56:16 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-05 08:56:16 | INFO | train | epoch 135 | loss 1.898 | ppl 3.73 | wps 20202.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 26277 | lr 0.00019508 | gnorm 1.165 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 85304
2022-03-05 08:56:16 | INFO | fairseq.trainer | begin training epoch 136
2022-03-05 08:56:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:57:29 | INFO | train_inner | epoch 136:     23 / 196 loss=1.913, ppl=3.77, wps=20009.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=26300, lr=0.000194994, gnorm=1.155, loss_scale=16, train_wall=295, gb_free=7.2, wall=85377
2022-03-05 09:02:47 | INFO | train_inner | epoch 136:    123 / 196 loss=1.877, ppl=3.67, wps=20619.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=26400, lr=0.000194625, gnorm=1.16, loss_scale=16, train_wall=296, gb_free=7.2, wall=85695
2022-03-05 09:04:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:06:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:06:43 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 12.24 | ppl 4837.58 | wps 41411.2 | wpb 510.9 | bsz 1 | num_updates 26472 | best_loss 6.944
2022-03-05 09:06:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 26472 updates
2022-03-05 09:06:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 09:06:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 09:06:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 136 @ 26472 updates, score 12.24) (writing took 4.2124692387878895 seconds)
2022-03-05 09:06:47 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-05 09:06:47 | INFO | train | epoch 136 | loss 1.892 | ppl 3.71 | wps 20202.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 26472 | lr 0.00019436 | gnorm 1.16 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 85936
2022-03-05 09:06:47 | INFO | fairseq.trainer | begin training epoch 137
2022-03-05 09:06:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:08:16 | INFO | train_inner | epoch 137:     28 / 196 loss=1.902, ppl=3.74, wps=19824.8, ups=0.3, wpb=65367, bsz=127.7, num_updates=26500, lr=0.000194257, gnorm=1.164, loss_scale=16, train_wall=298, gb_free=7.2, wall=86025
2022-03-05 09:11:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:13:37 | INFO | train_inner | epoch 137:    129 / 196 loss=1.876, ppl=3.67, wps=20413.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=26600, lr=0.000193892, gnorm=1.151, loss_scale=16, train_wall=299, gb_free=7.2, wall=86346
2022-03-05 09:16:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:17:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:17:15 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 12.131 | ppl 4485.61 | wps 41342.6 | wpb 510.9 | bsz 1 | num_updates 26666 | best_loss 6.944
2022-03-05 09:17:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 26666 updates
2022-03-05 09:17:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 09:17:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 09:17:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 137 @ 26666 updates, score 12.131) (writing took 4.401507897302508 seconds)
2022-03-05 09:17:19 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-05 09:17:19 | INFO | train | epoch 137 | loss 1.884 | ppl 3.69 | wps 20092.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26666 | lr 0.000193652 | gnorm 1.156 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 86567
2022-03-05 09:17:19 | INFO | fairseq.trainer | begin training epoch 138
2022-03-05 09:17:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:19:07 | INFO | train_inner | epoch 138:     34 / 196 loss=1.885, ppl=3.69, wps=19810.9, ups=0.3, wpb=65367, bsz=127.7, num_updates=26700, lr=0.000193528, gnorm=1.164, loss_scale=8, train_wall=298, gb_free=7.2, wall=86676
2022-03-05 09:24:25 | INFO | train_inner | epoch 138:    134 / 196 loss=1.866, ppl=3.64, wps=20629.1, ups=0.31, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=1.163, loss_scale=16, train_wall=295, gb_free=7.2, wall=86993
2022-03-05 09:27:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:27:47 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 12.251 | ppl 4875.25 | wps 41405.2 | wpb 510.9 | bsz 1 | num_updates 26862 | best_loss 6.944
2022-03-05 09:27:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 26862 updates
2022-03-05 09:27:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 09:27:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 09:27:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 138 @ 26862 updates, score 12.251) (writing took 4.210334789939225 seconds)
2022-03-05 09:27:51 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-05 09:27:51 | INFO | train | epoch 138 | loss 1.879 | ppl 3.68 | wps 20312.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 26862 | lr 0.000192944 | gnorm 1.162 | loss_scale 16 | train_wall 578 | gb_free 7.2 | wall 87199
2022-03-05 09:27:51 | INFO | fairseq.trainer | begin training epoch 139
2022-03-05 09:27:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:29:52 | INFO | train_inner | epoch 139:     38 / 196 loss=1.889, ppl=3.7, wps=20012.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=26900, lr=0.000192807, gnorm=1.152, loss_scale=16, train_wall=295, gb_free=7.2, wall=87320
2022-03-05 09:30:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:35:13 | INFO | train_inner | epoch 139:    139 / 196 loss=1.869, ppl=3.65, wps=20413.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=27000, lr=0.00019245, gnorm=1.17, loss_scale=16, train_wall=299, gb_free=7.2, wall=87641
2022-03-05 09:37:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:38:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:38:18 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 12.235 | ppl 4821.33 | wps 41472.8 | wpb 510.9 | bsz 1 | num_updates 27056 | best_loss 6.944
2022-03-05 09:38:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 27056 updates
2022-03-05 09:38:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 09:38:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 09:38:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 139 @ 27056 updates, score 12.235) (writing took 4.391237448900938 seconds)
2022-03-05 09:38:23 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-05 09:38:23 | INFO | train | epoch 139 | loss 1.87 | ppl 3.66 | wps 20096.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27056 | lr 0.000192251 | gnorm 1.163 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 87831
2022-03-05 09:38:23 | INFO | fairseq.trainer | begin training epoch 140
2022-03-05 09:38:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:39:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:40:46 | INFO | train_inner | epoch 140:     45 / 196 loss=1.866, ppl=3.65, wps=19629.1, ups=0.3, wpb=65367, bsz=127.7, num_updates=27100, lr=0.000192095, gnorm=1.152, loss_scale=8, train_wall=301, gb_free=7.2, wall=87974
2022-03-05 09:46:04 | INFO | train_inner | epoch 140:    145 / 196 loss=1.869, ppl=3.65, wps=20614.3, ups=0.31, wpb=65532.4, bsz=128, num_updates=27200, lr=0.000191741, gnorm=1.18, loss_scale=8, train_wall=296, gb_free=7.2, wall=88292
2022-03-05 09:47:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:48:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:48:50 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 12.281 | ppl 4978.38 | wps 41457.9 | wpb 510.9 | bsz 1 | num_updates 27250 | best_loss 6.944
2022-03-05 09:48:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 27250 updates
2022-03-05 09:48:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 09:48:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 09:48:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 140 @ 27250 updates, score 12.281) (writing took 3.1734550707042217 seconds)
2022-03-05 09:48:53 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-05 09:48:53 | INFO | train | epoch 140 | loss 1.864 | ppl 3.64 | wps 20127.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27250 | lr 0.000191565 | gnorm 1.169 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 88462
2022-03-05 09:48:53 | INFO | fairseq.trainer | begin training epoch 141
2022-03-05 09:48:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:51:32 | INFO | train_inner | epoch 141:     50 / 196 loss=1.857, ppl=3.62, wps=19883, ups=0.3, wpb=65367, bsz=127.7, num_updates=27300, lr=0.00019139, gnorm=1.168, loss_scale=8, train_wall=298, gb_free=7.2, wall=88621
2022-03-05 09:55:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:56:53 | INFO | train_inner | epoch 141:    151 / 196 loss=1.859, ppl=3.63, wps=20422.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=27400, lr=0.00019104, gnorm=1.155, loss_scale=8, train_wall=298, gb_free=7.2, wall=88941
2022-03-05 09:59:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:59:21 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 12.238 | ppl 4831.96 | wps 41502 | wpb 510.9 | bsz 1 | num_updates 27445 | best_loss 6.944
2022-03-05 09:59:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 27445 updates
2022-03-05 09:59:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 09:59:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 09:59:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 141 @ 27445 updates, score 12.238) (writing took 3.2151458943262696 seconds)
2022-03-05 09:59:24 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-05 09:59:24 | INFO | train | epoch 141 | loss 1.858 | ppl 3.62 | wps 20244.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27445 | lr 0.000190883 | gnorm 1.164 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 89092
2022-03-05 09:59:24 | INFO | fairseq.trainer | begin training epoch 142
2022-03-05 09:59:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:02:19 | INFO | train_inner | epoch 142:     55 / 196 loss=1.841, ppl=3.58, wps=20083.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=27500, lr=0.000190693, gnorm=1.17, loss_scale=8, train_wall=295, gb_free=7.2, wall=89267
2022-03-05 10:07:36 | INFO | train_inner | epoch 142:    155 / 196 loss=1.867, ppl=3.65, wps=20637.5, ups=0.31, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=1.168, loss_scale=16, train_wall=295, gb_free=7.2, wall=89584
2022-03-05 10:08:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:09:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:09:51 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 12.276 | ppl 4958.48 | wps 41542 | wpb 510.9 | bsz 1 | num_updates 27640 | best_loss 6.944
2022-03-05 10:09:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 27640 updates
2022-03-05 10:09:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 10:09:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 10:09:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 142 @ 27640 updates, score 12.276) (writing took 2.9276191676035523 seconds)
2022-03-05 10:09:54 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-05 10:09:54 | INFO | train | epoch 142 | loss 1.851 | ppl 3.61 | wps 20254.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27640 | lr 0.000190209 | gnorm 1.173 | loss_scale 8 | train_wall 578 | gb_free 7.2 | wall 89722
2022-03-05 10:09:54 | INFO | fairseq.trainer | begin training epoch 143
2022-03-05 10:09:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:13:05 | INFO | train_inner | epoch 143:     60 / 196 loss=1.839, ppl=3.58, wps=19903, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=27700, lr=0.000190003, gnorm=1.181, loss_scale=8, train_wall=298, gb_free=7.2, wall=89913
2022-03-05 10:17:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:18:26 | INFO | train_inner | epoch 143:    161 / 196 loss=1.851, ppl=3.61, wps=20413.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=27800, lr=0.000189661, gnorm=1.18, loss_scale=8, train_wall=299, gb_free=7.2, wall=90234
2022-03-05 10:20:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:20:21 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 12.27 | ppl 4939.97 | wps 41550.4 | wpb 510.9 | bsz 1 | num_updates 27835 | best_loss 6.944
2022-03-05 10:20:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 27835 updates
2022-03-05 10:20:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 10:20:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 10:20:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 143 @ 27835 updates, score 12.27) (writing took 2.8389561269432306 seconds)
2022-03-05 10:20:24 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-05 10:20:24 | INFO | train | epoch 143 | loss 1.844 | ppl 3.59 | wps 20245.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27835 | lr 0.000189542 | gnorm 1.169 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 90352
2022-03-05 10:20:24 | INFO | fairseq.trainer | begin training epoch 144
2022-03-05 10:20:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:23:51 | INFO | train_inner | epoch 144:     65 / 196 loss=1.83, ppl=3.55, wps=20091.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=27900, lr=0.000189321, gnorm=1.162, loss_scale=8, train_wall=295, gb_free=7.2, wall=90559
2022-03-05 10:27:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:29:12 | INFO | train_inner | epoch 144:    166 / 196 loss=1.854, ppl=3.62, wps=20413.2, ups=0.31, wpb=65532.4, bsz=128, num_updates=28000, lr=0.000188982, gnorm=1.167, loss_scale=8, train_wall=299, gb_free=7.2, wall=90880
2022-03-05 10:30:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:30:52 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 12.321 | ppl 5118.47 | wps 41421.5 | wpb 510.9 | bsz 1 | num_updates 28030 | best_loss 6.944
2022-03-05 10:30:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 28030 updates
2022-03-05 10:30:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 10:30:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 10:30:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 144 @ 28030 updates, score 12.321) (writing took 3.0179949682205915 seconds)
2022-03-05 10:30:55 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-05 10:30:55 | INFO | train | epoch 144 | loss 1.838 | ppl 3.57 | wps 20238.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28030 | lr 0.000188881 | gnorm 1.172 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 90983
2022-03-05 10:30:55 | INFO | fairseq.trainer | begin training epoch 145
2022-03-05 10:30:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:34:38 | INFO | train_inner | epoch 145:     70 / 196 loss=1.819, ppl=3.53, wps=20085.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=28100, lr=0.000188646, gnorm=1.168, loss_scale=16, train_wall=295, gb_free=7.2, wall=91206
2022-03-05 10:39:56 | INFO | train_inner | epoch 145:    170 / 196 loss=1.846, ppl=3.6, wps=20601.1, ups=0.31, wpb=65532.4, bsz=128, num_updates=28200, lr=0.000188311, gnorm=1.166, loss_scale=16, train_wall=296, gb_free=7.2, wall=91524
2022-03-05 10:40:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:41:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:41:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:41:23 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 12.379 | ppl 5327.47 | wps 41495.3 | wpb 510.9 | bsz 1 | num_updates 28224 | best_loss 6.944
2022-03-05 10:41:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 28224 updates
2022-03-05 10:41:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 10:41:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 10:41:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 145 @ 28224 updates, score 12.379) (writing took 3.702836077660322 seconds)
2022-03-05 10:41:27 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-05 10:41:27 | INFO | train | epoch 145 | loss 1.831 | ppl 3.56 | wps 20099.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28224 | lr 0.000188231 | gnorm 1.163 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 91615
2022-03-05 10:41:27 | INFO | fairseq.trainer | begin training epoch 146
2022-03-05 10:41:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:45:28 | INFO | train_inner | epoch 146:     76 / 196 loss=1.804, ppl=3.49, wps=19644.2, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=28300, lr=0.000187978, gnorm=1.167, loss_scale=8, train_wall=301, gb_free=7.2, wall=91857
2022-03-05 10:48:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:50:49 | INFO | train_inner | epoch 146:    177 / 196 loss=1.849, ppl=3.6, wps=20413.2, ups=0.31, wpb=65536, bsz=128, num_updates=28400, lr=0.000187647, gnorm=1.177, loss_scale=8, train_wall=299, gb_free=7.2, wall=92178
2022-03-05 10:51:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:51:54 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 12.337 | ppl 5175.26 | wps 41407.3 | wpb 510.9 | bsz 1 | num_updates 28419 | best_loss 6.944
2022-03-05 10:51:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 28419 updates
2022-03-05 10:51:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 10:51:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 10:51:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 146 @ 28419 updates, score 12.337) (writing took 3.311635627411306 seconds)
2022-03-05 10:51:58 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-05 10:51:58 | INFO | train | epoch 146 | loss 1.825 | ppl 3.54 | wps 20224.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28419 | lr 0.000187584 | gnorm 1.174 | loss_scale 8 | train_wall 579 | gb_free 7.2 | wall 92246
2022-03-05 10:51:58 | INFO | fairseq.trainer | begin training epoch 147
2022-03-05 10:51:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:56:15 | INFO | train_inner | epoch 147:     81 / 196 loss=1.801, ppl=3.48, wps=20069.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=28500, lr=0.000187317, gnorm=1.162, loss_scale=16, train_wall=295, gb_free=7.2, wall=92503
2022-03-05 11:01:33 | INFO | train_inner | epoch 147:    181 / 196 loss=1.84, ppl=3.58, wps=20618.5, ups=0.31, wpb=65532.4, bsz=128, num_updates=28600, lr=0.000186989, gnorm=1.183, loss_scale=16, train_wall=296, gb_free=7.2, wall=92821
2022-03-05 11:02:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:02:26 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 12.37 | ppl 5292.95 | wps 41518.2 | wpb 510.9 | bsz 1 | num_updates 28615 | best_loss 6.944
2022-03-05 11:02:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 28615 updates
2022-03-05 11:02:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 11:02:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 11:02:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 147 @ 28615 updates, score 12.37) (writing took 3.7341296523809433 seconds)
2022-03-05 11:02:29 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-05 11:02:29 | INFO | train | epoch 147 | loss 1.819 | ppl 3.53 | wps 20303.4 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 28615 | lr 0.00018694 | gnorm 1.169 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 92878
2022-03-05 11:02:29 | INFO | fairseq.trainer | begin training epoch 148
2022-03-05 11:02:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:02:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:03:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:07:07 | INFO | train_inner | epoch 148:     87 / 196 loss=1.797, ppl=3.48, wps=19580.2, ups=0.3, wpb=65367, bsz=127.7, num_updates=28700, lr=0.000186663, gnorm=1.177, loss_scale=8, train_wall=302, gb_free=7.2, wall=93155
2022-03-05 11:12:25 | INFO | train_inner | epoch 148:    187 / 196 loss=1.835, ppl=3.57, wps=20614.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=28800, lr=0.000186339, gnorm=1.171, loss_scale=16, train_wall=296, gb_free=7.2, wall=93473
2022-03-05 11:12:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:12:58 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 12.416 | ppl 5463.48 | wps 41512.9 | wpb 510.9 | bsz 1 | num_updates 28809 | best_loss 6.944
2022-03-05 11:12:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 28809 updates
2022-03-05 11:12:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 11:13:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 11:13:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 148 @ 28809 updates, score 12.416) (writing took 3.6180462865158916 seconds)
2022-03-05 11:13:01 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-05 11:13:01 | INFO | train | epoch 148 | loss 1.813 | ppl 3.51 | wps 20088.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28809 | lr 0.00018631 | gnorm 1.176 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 93510
2022-03-05 11:13:01 | INFO | fairseq.trainer | begin training epoch 149
2022-03-05 11:13:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:16:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:17:54 | INFO | train_inner | epoch 149:     92 / 196 loss=1.779, ppl=3.43, wps=19851, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=28900, lr=0.000186016, gnorm=1.168, loss_scale=8, train_wall=298, gb_free=7.2, wall=93802
2022-03-05 11:23:12 | INFO | train_inner | epoch 149:    192 / 196 loss=1.839, ppl=3.58, wps=20614.8, ups=0.31, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=1.212, loss_scale=8, train_wall=296, gb_free=7.2, wall=94120
2022-03-05 11:23:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:23:29 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 12.426 | ppl 5501.57 | wps 41632.8 | wpb 510.9 | bsz 1 | num_updates 29004 | best_loss 6.944
2022-03-05 11:23:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 29004 updates
2022-03-05 11:23:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 11:23:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt
2022-03-05 11:23:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-cross_entropy_#1/checkpoint_last.pt (epoch 149 @ 29004 updates, score 12.426) (writing took 3.778722917661071 seconds)
2022-03-05 11:23:33 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-05 11:23:33 | INFO | train | epoch 149 | loss 1.808 | ppl 3.5 | wps 20211.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 29004 | lr 0.000185683 | gnorm 1.191 | loss_scale 16 | train_wall 579 | gb_free 7.2 | wall 94141
2022-03-05 11:23:33 | INFO | fairseq.trainer | begin training epoch 150
2022-03-05 11:23:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:24:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/cross_entropy.py", line 35, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 368, in forward
    x, attn = self.self_attn(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/multihead_attention.py", line 170, in forward
    return F.multi_head_attention_forward(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 4222, in multi_head_attention_forward
    q = q * scaling
KeyboardInterrupt
