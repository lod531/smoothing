Sender: LSF System <lsfadmin@eu-g3-055>
Subject: Job 210657916: <wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2> in cluster <euler> Exited

Job <wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2> was submitted from host <eu-login-27> by user <andriusb> in cluster <euler> at Wed Mar 23 19:48:02 2022
Job was executed on host(s) <eu-g3-055>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Wed Mar 23 19:48:27 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Mar 23 19:48:27 2022
Terminated at Wed Mar 23 19:50:10 2022
Results reported at Wed Mar 23 19:50:10 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-cleaned-bpe-size0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.35 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas \(0.01,0.09,0.9\) --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --seed 1321672 --no-epoch-checkpoints --no-last-checkpoints --patience 3 --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   96.48 sec.
    Max Memory :                                 2713 MB
    Average Memory :                             1370.33 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               17287.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   124 sec.
    Turnaround time :                            128 sec.

The output (if any) follows:

2022-03-23 19:48:33 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321672, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.35, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-cleaned-bpe-size0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321672, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.01,0.09,0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-23 19:48:33 | INFO | fairseq.tasks.language_modeling | dictionary: 39136 types
2022-03-23 19:48:34 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/train
Calculating frequency stats:
  0%|          | 0/112584 [00:00<?, ?it/s]  1%|          | 659/112584 [00:00<00:16, 6587.10it/s]  1%|          | 1318/112584 [00:00<00:19, 5575.41it/s]  2%|▏         | 1886/112584 [00:00<00:20, 5452.61it/s]  2%|▏         | 2436/112584 [00:00<00:20, 5299.17it/s]  3%|▎         | 3154/112584 [00:00<00:18, 5926.53it/s]  3%|▎         | 3753/112584 [00:00<00:18, 5737.98it/s]  4%|▍         | 4374/112584 [00:00<00:18, 5880.69it/s]  4%|▍         | 5065/112584 [00:00<00:17, 6192.94it/s]  5%|▌         | 5689/112584 [00:00<00:17, 6166.08it/s]  6%|▌         | 6309/112584 [00:01<00:17, 6055.81it/s]  6%|▌         | 6917/112584 [00:01<00:18, 5589.66it/s]  7%|▋         | 7523/112584 [00:01<00:18, 5716.12it/s]  7%|▋         | 8101/112584 [00:01<00:18, 5593.94it/s]  8%|▊         | 8665/112584 [00:01<00:18, 5476.09it/s]  8%|▊         | 9285/112584 [00:01<00:18, 5680.90it/s]  9%|▉         | 9873/112584 [00:01<00:17, 5734.04it/s]  9%|▉         | 10449/112584 [00:01<00:17, 5716.37it/s] 10%|▉         | 11023/112584 [00:01<00:17, 5669.94it/s] 10%|█         | 11592/112584 [00:02<00:18, 5558.97it/s] 11%|█         | 12205/112584 [00:02<00:17, 5711.37it/s] 11%|█▏        | 12778/112584 [00:02<00:17, 5596.86it/s] 12%|█▏        | 13368/112584 [00:02<00:17, 5677.57it/s] 12%|█▏        | 14001/112584 [00:02<00:16, 5866.31it/s] 13%|█▎        | 14589/112584 [00:02<00:16, 5766.08it/s] 14%|█▎        | 15255/112584 [00:02<00:16, 6022.79it/s] 14%|█▍        | 15859/112584 [00:02<00:16, 5742.87it/s] 15%|█▍        | 16437/112584 [00:02<00:17, 5541.22it/s] 15%|█▌        | 16997/112584 [00:02<00:17, 5557.15it/s] 16%|█▌        | 17562/112584 [00:03<00:17, 5582.86it/s] 16%|█▌        | 18122/112584 [00:03<00:16, 5577.82it/s] 17%|█▋        | 18681/112584 [00:03<00:16, 5566.21it/s] 17%|█▋        | 19461/112584 [00:03<00:14, 6217.39it/s] 18%|█▊        | 20085/112584 [00:03<00:15, 5910.37it/s] 18%|█▊        | 20681/112584 [00:03<00:15, 5744.10it/s] 19%|█▉        | 21259/112584 [00:03<00:16, 5617.48it/s] 19%|█▉        | 21824/112584 [00:03<00:16, 5571.58it/s] 20%|█▉        | 22383/112584 [00:03<00:16, 5563.46it/s] 20%|██        | 22992/112584 [00:04<00:15, 5706.91it/s] 21%|██        | 23627/112584 [00:04<00:15, 5891.53it/s] 22%|██▏       | 24307/112584 [00:04<00:14, 6155.19it/s] 22%|██▏       | 24957/112584 [00:04<00:14, 6255.80it/s] 23%|██▎       | 25584/112584 [00:04<00:13, 6245.42it/s] 23%|██▎       | 26210/112584 [00:04<00:14, 5947.04it/s] 24%|██▍       | 26809/112584 [00:04<00:15, 5539.67it/s] 24%|██▍       | 27370/112584 [00:04<00:15, 5455.09it/s] 25%|██▍       | 27921/112584 [00:04<00:15, 5394.40it/s] 25%|██▌       | 28630/112584 [00:04<00:14, 5864.05it/s] 26%|██▌       | 29222/112584 [00:05<00:14, 5727.54it/s] 26%|██▋       | 29799/112584 [00:05<00:14, 5708.00it/s] 27%|██▋       | 30373/112584 [00:05<00:14, 5628.63it/s] 27%|██▋       | 30938/112584 [00:05<00:15, 5365.00it/s] 28%|██▊       | 31483/112584 [00:05<00:15, 5388.71it/s] 28%|██▊       | 32025/112584 [00:05<00:15, 5281.27it/s] 29%|██▉       | 32565/112584 [00:05<00:15, 5305.14it/s] 29%|██▉       | 33097/112584 [00:05<00:15, 5150.16it/s] 30%|██▉       | 33657/112584 [00:05<00:14, 5279.02it/s] 30%|███       | 34207/112584 [00:06<00:14, 5335.41it/s] 31%|███       | 34900/112584 [00:06<00:13, 5795.77it/s] 32%|███▏      | 35482/112584 [00:06<00:13, 5571.61it/s] 32%|███▏      | 36043/112584 [00:06<00:13, 5559.40it/s] 33%|███▎      | 36607/112584 [00:06<00:13, 5570.68it/s] 33%|███▎      | 37166/112584 [00:06<00:13, 5419.16it/s] 33%|███▎      | 37710/112584 [00:06<00:14, 5246.00it/s] 34%|███▍      | 38265/112584 [00:06<00:13, 5332.41it/s] 34%|███▍      | 38841/112584 [00:06<00:13, 5454.73it/s] 35%|███▍      | 39389/112584 [00:06<00:13, 5296.17it/s] 36%|███▌      | 39986/112584 [00:07<00:13, 5487.70it/s] 36%|███▌      | 40631/112584 [00:07<00:12, 5759.85it/s] 37%|███▋      | 41210/112584 [00:07<00:12, 5719.85it/s] 37%|███▋      | 41784/112584 [00:07<00:13, 5368.25it/s] 38%|███▊      | 42326/112584 [00:07<00:13, 5197.54it/s] 38%|███▊      | 42863/112584 [00:07<00:13, 5242.53it/s] 39%|███▊      | 43391/112584 [00:07<00:13, 5245.57it/s] 39%|███▉      | 44022/112584 [00:07<00:12, 5551.16it/s] 40%|███▉      | 44634/112584 [00:07<00:11, 5713.72it/s] 40%|████      | 45208/112584 [00:08<00:12, 5488.50it/s] 41%|████      | 45803/112584 [00:08<00:11, 5620.00it/s] 41%|████▏     | 46486/112584 [00:08<00:11, 5967.21it/s] 42%|████▏     | 47086/112584 [00:08<00:11, 5949.56it/s] 43%|████▎     | 48003/112584 [00:08<00:09, 6897.62it/s] 43%|████▎     | 48696/112584 [00:08<00:09, 6717.74it/s] 44%|████▍     | 49371/112584 [00:08<00:09, 6614.32it/s] 44%|████▍     | 50035/112584 [00:08<00:09, 6328.09it/s] 45%|████▌     | 50672/112584 [00:08<00:10, 5905.81it/s] 46%|████▌     | 51270/112584 [00:08<00:10, 5820.64it/s] 46%|████▌     | 51863/112584 [00:09<00:10, 5850.14it/s] 47%|████▋     | 52600/112584 [00:09<00:09, 6281.86it/s] 47%|████▋     | 53233/112584 [00:09<00:09, 6020.84it/s] 48%|████▊     | 53840/112584 [00:09<00:09, 5911.81it/s] 48%|████▊     | 54435/112584 [00:09<00:10, 5589.23it/s] 49%|████▉     | 54999/112584 [00:09<00:10, 5511.30it/s] 49%|████▉     | 55601/112584 [00:09<00:10, 5651.92it/s] 50%|████▉     | 56210/112584 [00:09<00:09, 5775.61it/s] 50%|█████     | 56791/112584 [00:09<00:10, 5542.62it/s] 51%|█████     | 57349/112584 [00:10<00:10, 5265.28it/s] 51%|█████▏    | 57949/112584 [00:10<00:09, 5464.23it/s] 52%|█████▏    | 58692/112584 [00:10<00:08, 6014.78it/s] 53%|█████▎    | 59300/112584 [00:10<00:09, 5601.49it/s] 53%|█████▎    | 59870/112584 [00:10<00:09, 5562.60it/s] 54%|█████▎    | 60433/112584 [00:10<00:09, 5578.62it/s] 54%|█████▍    | 61142/112584 [00:10<00:08, 6008.87it/s] 55%|█████▍    | 61748/112584 [00:10<00:08, 5934.72it/s] 55%|█████▌    | 62346/112584 [00:10<00:08, 5751.04it/s] 56%|█████▌    | 62933/112584 [00:11<00:08, 5782.47it/s] 56%|█████▋    | 63514/112584 [00:11<00:08, 5661.08it/s] 57%|█████▋    | 64083/112584 [00:11<00:08, 5641.60it/s] 57%|█████▋    | 64663/112584 [00:11<00:08, 5676.70it/s] 58%|█████▊    | 65232/112584 [00:11<00:08, 5486.76it/s] 58%|█████▊    | 65841/112584 [00:11<00:08, 5658.42it/s] 59%|█████▉    | 66409/112584 [00:11<00:08, 5509.77it/s] 60%|█████▉    | 67031/112584 [00:11<00:07, 5703.56it/s] 60%|██████    | 67604/112584 [00:11<00:08, 5545.96it/s] 61%|██████    | 68161/112584 [00:11<00:08, 5345.54it/s] 61%|██████    | 68900/112584 [00:12<00:07, 5920.95it/s] 62%|██████▏   | 69497/112584 [00:12<00:07, 5763.26it/s] 62%|██████▏   | 70098/112584 [00:12<00:07, 5833.60it/s] 63%|██████▎   | 70685/112584 [00:12<00:07, 5698.59it/s] 63%|██████▎   | 71258/112584 [00:12<00:07, 5684.79it/s] 64%|██████▍   | 71829/112584 [00:12<00:07, 5671.82it/s] 64%|██████▍   | 72398/112584 [00:12<00:07, 5670.51it/s] 65%|██████▍   | 73127/112584 [00:12<00:06, 6142.38it/s] 66%|██████▌   | 73770/112584 [00:12<00:06, 6223.86it/s] 66%|██████▌   | 74531/112584 [00:13<00:05, 6630.00it/s] 67%|██████▋   | 75196/112584 [00:13<00:05, 6303.66it/s] 67%|██████▋   | 75831/112584 [00:13<00:06, 6109.50it/s] 68%|██████▊   | 76446/112584 [00:13<00:05, 6061.29it/s] 68%|██████▊   | 77055/112584 [00:13<00:06, 5894.01it/s] 69%|██████▉   | 77694/112584 [00:13<00:05, 6030.11it/s] 70%|██████▉   | 78300/112584 [00:13<00:05, 5717.36it/s] 70%|███████   | 78876/112584 [00:13<00:06, 5401.97it/s] 71%|███████   | 79422/112584 [00:13<00:06, 5254.77it/s] 71%|███████   | 79983/112584 [00:14<00:06, 5352.40it/s] 72%|███████▏  | 80522/112584 [00:14<00:06, 5279.22it/s] 72%|███████▏  | 81171/112584 [00:14<00:05, 5618.47it/s] 73%|███████▎  | 81737/112584 [00:14<00:05, 5570.73it/s] 73%|███████▎  | 82314/112584 [00:14<00:05, 5623.58it/s] 74%|███████▎  | 82904/112584 [00:14<00:05, 5699.94it/s] 74%|███████▍  | 83527/112584 [00:14<00:04, 5850.31it/s] 75%|███████▍  | 84114/112584 [00:14<00:04, 5720.95it/s] 75%|███████▌  | 84721/112584 [00:14<00:04, 5819.49it/s] 76%|███████▌  | 85305/112584 [00:14<00:04, 5488.76it/s] 76%|███████▋  | 85936/112584 [00:15<00:04, 5719.14it/s] 77%|███████▋  | 86513/112584 [00:15<00:04, 5669.62it/s] 77%|███████▋  | 87147/112584 [00:15<00:04, 5857.97it/s] 78%|███████▊  | 87736/112584 [00:15<00:04, 5808.46it/s] 78%|███████▊  | 88331/112584 [00:15<00:04, 5843.60it/s] 79%|███████▉  | 88917/112584 [00:15<00:04, 5495.25it/s] 79%|███████▉  | 89472/112584 [00:15<00:04, 5409.41it/s] 80%|███████▉  | 90017/112584 [00:15<00:04, 5314.80it/s] 80%|████████  | 90555/112584 [00:15<00:04, 5332.36it/s] 81%|████████  | 91117/112584 [00:15<00:03, 5412.59it/s] 81%|████████▏ | 91741/112584 [00:16<00:03, 5653.75it/s] 82%|████████▏ | 92325/112584 [00:16<00:03, 5708.06it/s] 83%|████████▎ | 92898/112584 [00:16<00:03, 5695.43it/s] 83%|████████▎ | 93469/112584 [00:16<00:03, 5650.91it/s] 84%|████████▎ | 94035/112584 [00:16<00:03, 5473.53it/s] 84%|████████▍ | 94709/112584 [00:16<00:03, 5840.52it/s] 85%|████████▍ | 95296/112584 [00:16<00:03, 5688.43it/s] 85%|████████▌ | 95868/112584 [00:16<00:02, 5618.46it/s] 86%|████████▌ | 96570/112584 [00:16<00:02, 6021.95it/s] 86%|████████▋ | 97224/112584 [00:17<00:02, 6172.82it/s] 87%|████████▋ | 97844/112584 [00:17<00:02, 5785.04it/s] 87%|████████▋ | 98429/112584 [00:17<00:02, 5782.80it/s] 88%|████████▊ | 99012/112584 [00:17<00:02, 5700.98it/s] 88%|████████▊ | 99586/112584 [00:17<00:02, 5475.51it/s] 89%|████████▉ | 100137/112584 [00:17<00:02, 5343.23it/s] 89%|████████▉ | 100674/112584 [00:17<00:02, 5269.89it/s] 90%|████████▉ | 101275/112584 [00:17<00:02, 5479.63it/s] 90%|█████████ | 101838/112584 [00:17<00:01, 5521.16it/s] 91%|█████████ | 102413/112584 [00:17<00:01, 5582.24it/s] 91%|█████████▏| 102998/112584 [00:18<00:01, 5660.06it/s] 92%|█████████▏| 103566/112584 [00:18<00:01, 5524.16it/s] 92%|█████████▏| 104120/112584 [00:18<00:01, 5313.54it/s] 93%|█████████▎| 104750/112584 [00:18<00:01, 5595.23it/s] 94%|█████████▎| 105313/112584 [00:18<00:01, 5497.86it/s] 94%|█████████▍| 105947/112584 [00:18<00:01, 5731.06it/s] 95%|█████████▍| 106523/112584 [00:18<00:01, 5529.12it/s] 95%|█████████▌| 107079/112584 [00:18<00:01, 5460.97it/s] 96%|█████████▌| 107627/112584 [00:18<00:00, 5417.32it/s] 96%|█████████▌| 108170/112584 [00:19<00:00, 5404.99it/s] 97%|█████████▋| 108712/112584 [00:19<00:00, 5201.86it/s] 97%|█████████▋| 109264/112584 [00:19<00:00, 5290.91it/s] 98%|█████████▊| 109837/112584 [00:19<00:00, 5416.76it/s] 98%|█████████▊| 110430/112584 [00:19<00:00, 5564.89it/s] 99%|█████████▊| 111118/112584 [00:19<00:00, 5943.87it/s] 99%|█████████▉| 111714/112584 [00:19<00:00, 5682.76it/s]100%|█████████▉| 112286/112584 [00:19<00:00, 5687.56it/s]100%|██████████| 112584/112584 [00:19<00:00, 5683.28it/s]

gathering stats for n=1
  0%|          | 0/112584 [00:00<?, ?it/s]  2%|▏         | 1804/112584 [00:00<00:06, 18023.00it/s]  3%|▎         | 3688/112584 [00:00<00:05, 18495.12it/s]  5%|▌         | 5722/112584 [00:00<00:05, 19332.46it/s]  7%|▋         | 7656/112584 [00:00<00:05, 18568.77it/s]  8%|▊         | 9518/112584 [00:00<00:05, 18287.26it/s] 10%|█         | 11350/112584 [00:00<00:05, 18223.69it/s] 12%|█▏        | 13245/112584 [00:00<00:05, 18449.50it/s] 13%|█▎        | 15184/112584 [00:00<00:05, 18742.72it/s] 15%|█▌        | 17060/112584 [00:00<00:05, 18320.01it/s] 17%|█▋        | 18931/112584 [00:01<00:05, 18428.95it/s] 18%|█▊        | 20809/112584 [00:01<00:04, 18526.18it/s] 20%|██        | 22664/112584 [00:01<00:04, 18372.63it/s] 22%|██▏       | 24783/112584 [00:01<00:04, 19212.51it/s] 24%|██▎       | 26707/112584 [00:01<00:04, 18817.79it/s] 25%|██▌       | 28592/112584 [00:01<00:04, 18677.85it/s] 27%|██▋       | 30462/112584 [00:01<00:04, 18496.94it/s] 29%|██▊       | 32314/112584 [00:01<00:04, 17874.89it/s] 30%|███       | 34114/112584 [00:01<00:04, 17910.20it/s] 32%|███▏      | 35945/112584 [00:01<00:04, 18021.89it/s] 34%|███▎      | 37750/112584 [00:02<00:04, 17924.85it/s] 35%|███▌      | 39545/112584 [00:02<00:04, 17777.39it/s] 37%|███▋      | 41468/112584 [00:02<00:03, 18200.06it/s] 38%|███▊      | 43290/112584 [00:02<00:03, 17692.06it/s] 40%|████      | 45196/112584 [00:02<00:03, 18086.73it/s] 42%|████▏     | 47350/112584 [00:02<00:03, 19100.48it/s] 44%|████▍     | 49574/112584 [00:02<00:03, 20020.04it/s] 46%|████▌     | 51581/112584 [00:02<00:03, 19171.84it/s] 48%|████▊     | 53620/112584 [00:02<00:03, 19522.62it/s] 49%|████▉     | 55581/112584 [00:02<00:02, 19079.11it/s] 51%|█████     | 57497/112584 [00:03<00:02, 18698.32it/s] 53%|█████▎    | 59419/112584 [00:03<00:02, 18845.02it/s] 55%|█████▍    | 61424/112584 [00:03<00:02, 19190.60it/s] 56%|█████▋    | 63348/112584 [00:03<00:02, 18727.64it/s] 58%|█████▊    | 65226/112584 [00:03<00:02, 18649.87it/s] 60%|█████▉    | 67121/112584 [00:03<00:02, 18736.83it/s] 61%|██████▏   | 69013/112584 [00:03<00:02, 18784.10it/s] 63%|██████▎   | 70894/112584 [00:03<00:02, 18655.97it/s] 65%|██████▍   | 72990/112584 [00:03<00:02, 19335.30it/s] 67%|██████▋   | 75087/112584 [00:04<00:01, 19814.70it/s] 68%|██████▊   | 77071/112584 [00:04<00:01, 19393.80it/s] 70%|███████   | 79014/112584 [00:04<00:01, 18863.05it/s] 72%|███████▏  | 80905/112584 [00:04<00:01, 18583.78it/s] 74%|███████▎  | 82846/112584 [00:04<00:01, 18814.53it/s] 75%|███████▌  | 84791/112584 [00:04<00:01, 18995.78it/s] 77%|███████▋  | 86694/112584 [00:04<00:01, 18780.23it/s] 79%|███████▊  | 88592/112584 [00:04<00:01, 18833.24it/s] 80%|████████  | 90477/112584 [00:04<00:01, 18472.62it/s] 82%|████████▏ | 92427/112584 [00:04<00:01, 18768.63it/s] 84%|████████▍ | 94307/112584 [00:05<00:00, 18510.73it/s] 86%|████████▌ | 96261/112584 [00:05<00:00, 18808.01it/s] 87%|████████▋ | 98283/112584 [00:05<00:00, 19221.43it/s] 89%|████████▉ | 100208/112584 [00:05<00:00, 18574.55it/s] 91%|█████████ | 102127/112584 [00:05<00:00, 18749.72it/s] 92%|█████████▏| 104007/112584 [00:05<00:00, 18395.73it/s] 94%|█████████▍| 105937/112584 [00:05<00:00, 18655.62it/s] 96%|█████████▌| 107807/112584 [00:05<00:00, 18255.75it/s] 97%|█████████▋| 109637/112584 [00:05<00:00, 18075.02it/s] 99%|█████████▉| 111463/112584 [00:05<00:00, 18123.36it/s]100%|██████████| 112584/112584 [00:06<00:00, 18636.44it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 427.08it/s]2022-03-23 19:49:03 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=39136, bias=False)
  )
)
2022-03-23 19:49:03 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-23 19:49:03 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-23 19:49:03 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-23 19:49:03 | INFO | fairseq_cli.train | num. shared model params: 38,951,936 (num. trained: 38,951,936)
2022-03-23 19:49:03 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-23 19:49:03 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/valid
2022-03-23 19:49:03 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-23 19:49:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 19:49:03 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-23 19:49:03 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 19:49:03 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-23 19:49:03 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-23 19:49:03 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_last.pt
2022-03-23 19:49:03 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_last.pt
2022-03-23 19:49:03 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-23 19:49:03 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/train
2022-03-23 19:49:03 | INFO | fairseq.trainer | begin training epoch 1
2022-03-23 19:49:03 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-23 19:49:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-23 19:49:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 19:49:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 19:49:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/jelinek_mercer.py", line 102, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 368, in forward
    x, attn = self.self_attn(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/multihead_attention.py", line 170, in forward
    return F.multi_head_attention_forward(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 4319, in multi_head_attention_forward
    attn_output = linear(attn_output, out_proj_weight, out_proj_bias)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 1692, in linear
    output = input.matmul(weight.t())
KeyboardInterrupt
Sender: LSF System <lsfadmin@eu-g3-055>
Subject: Job 210658143: <wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2> in cluster <euler> Done

Job <wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2> was submitted from host <eu-login-27> by user <andriusb> in cluster <euler> at Wed Mar 23 19:50:56 2022
Job was executed on host(s) <eu-g3-055>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Wed Mar 23 19:51:28 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Mar 23 19:51:28 2022
Terminated at Wed Mar 23 22:48:54 2022
Results reported at Wed Mar 23 22:48:54 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-cleaned-bpe-size0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.35 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas \(0.01,0.09,0.9\) --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --seed 66575612 --no-epoch-checkpoints --no-last-checkpoints --patience 3 --fp16 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   10627.53 sec.
    Max Memory :                                 4568 MB
    Average Memory :                             3324.77 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               15432.00 MB
    Max Swap :                                   65 MB
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   10645 sec.
    Turnaround time :                            10678 sec.

The output (if any) follows:

2022-03-23 19:51:44 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575612, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.35, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-cleaned-bpe-size0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575612, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.01,0.09,0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-23 19:51:44 | INFO | fairseq.tasks.language_modeling | dictionary: 39136 types
2022-03-23 19:51:45 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/train
Calculating frequency stats:
  0%|          | 0/112584 [00:00<?, ?it/s]  1%|          | 640/112584 [00:00<00:17, 6379.14it/s]  1%|          | 1278/112584 [00:00<00:20, 5433.69it/s]  2%|▏         | 1831/112584 [00:00<00:20, 5305.10it/s]  2%|▏         | 2366/112584 [00:00<00:21, 5040.02it/s]  3%|▎         | 3050/112584 [00:00<00:19, 5634.87it/s]  3%|▎         | 3621/112584 [00:00<00:19, 5477.44it/s]  4%|▍         | 4242/112584 [00:00<00:19, 5700.22it/s]  4%|▍         | 4898/112584 [00:00<00:18, 5957.83it/s]  5%|▍         | 5515/112584 [00:00<00:17, 6021.00it/s]  5%|▌         | 6139/112584 [00:01<00:17, 6086.15it/s]  6%|▌         | 6750/112584 [00:01<00:19, 5508.79it/s]  7%|▋         | 7318/112584 [00:01<00:18, 5553.46it/s]  7%|▋         | 7882/112584 [00:01<00:18, 5517.62it/s]  7%|▋         | 8440/112584 [00:01<00:19, 5361.97it/s]  8%|▊         | 8981/112584 [00:01<00:19, 5371.88it/s]  8%|▊         | 9522/112584 [00:01<00:19, 5374.48it/s]  9%|▉         | 10128/112584 [00:01<00:18, 5569.48it/s]  9%|▉         | 10688/112584 [00:01<00:18, 5483.99it/s] 10%|▉         | 11238/112584 [00:02<00:18, 5439.18it/s] 10%|█         | 11784/112584 [00:02<00:18, 5334.60it/s] 11%|█         | 12383/112584 [00:02<00:18, 5524.58it/s] 11%|█▏        | 12937/112584 [00:02<00:18, 5508.29it/s] 12%|█▏        | 13570/112584 [00:02<00:17, 5750.15it/s] 13%|█▎        | 14147/112584 [00:02<00:17, 5527.27it/s] 13%|█▎        | 14703/112584 [00:02<00:17, 5528.21it/s] 14%|█▎        | 15304/112584 [00:02<00:17, 5665.00it/s] 14%|█▍        | 15873/112584 [00:02<00:17, 5483.27it/s] 15%|█▍        | 16424/112584 [00:02<00:18, 5330.74it/s] 15%|█▌        | 16960/112584 [00:03<00:18, 5242.54it/s] 16%|█▌        | 17529/112584 [00:03<00:17, 5369.16it/s] 16%|█▌        | 18068/112584 [00:03<00:17, 5369.03it/s] 17%|█▋        | 18606/112584 [00:03<00:17, 5328.23it/s] 17%|█▋        | 19371/112584 [00:03<00:15, 6006.46it/s] 18%|█▊        | 19974/112584 [00:03<00:16, 5702.32it/s] 18%|█▊        | 20549/112584 [00:03<00:16, 5437.71it/s] 19%|█▊        | 21099/112584 [00:03<00:16, 5451.12it/s] 19%|█▉        | 21648/112584 [00:03<00:17, 5248.67it/s] 20%|█▉        | 22211/112584 [00:04<00:16, 5353.74it/s] 20%|██        | 22824/112584 [00:04<00:16, 5567.17it/s] 21%|██        | 23394/112584 [00:04<00:15, 5596.11it/s] 21%|██▏       | 24082/112584 [00:04<00:14, 5970.74it/s] 22%|██▏       | 24775/112584 [00:04<00:14, 6247.23it/s] 23%|██▎       | 25402/112584 [00:04<00:14, 6140.82it/s] 23%|██▎       | 26018/112584 [00:04<00:14, 5792.50it/s] 24%|██▎       | 26603/112584 [00:04<00:15, 5553.52it/s] 24%|██▍       | 27163/112584 [00:04<00:16, 5276.19it/s] 25%|██▍       | 27709/112584 [00:04<00:15, 5322.76it/s] 25%|██▌       | 28304/112584 [00:05<00:15, 5486.99it/s] 26%|██▌       | 28897/112584 [00:05<00:14, 5607.39it/s] 26%|██▌       | 29461/112584 [00:05<00:15, 5328.92it/s] 27%|██▋       | 30093/112584 [00:05<00:14, 5602.51it/s] 27%|██▋       | 30658/112584 [00:05<00:15, 5300.74it/s] 28%|██▊       | 31194/112584 [00:05<00:15, 5139.07it/s] 28%|██▊       | 31796/112584 [00:05<00:15, 5383.76it/s] 29%|██▊       | 32340/112584 [00:05<00:15, 5163.60it/s] 29%|██▉       | 32882/112584 [00:05<00:15, 5232.25it/s] 30%|██▉       | 33409/112584 [00:06<00:15, 5124.59it/s] 30%|███       | 33984/112584 [00:06<00:14, 5298.34it/s] 31%|███       | 34603/112584 [00:06<00:14, 5554.84it/s] 31%|███       | 35162/112584 [00:06<00:14, 5362.58it/s] 32%|███▏      | 35702/112584 [00:06<00:14, 5300.75it/s] 32%|███▏      | 36271/112584 [00:06<00:14, 5409.69it/s] 33%|███▎      | 36814/112584 [00:06<00:14, 5400.17it/s] 33%|███▎      | 37356/112584 [00:06<00:14, 5146.92it/s] 34%|███▎      | 37874/112584 [00:06<00:14, 5139.92it/s] 34%|███▍      | 38393/112584 [00:07<00:14, 5148.73it/s] 35%|███▍      | 38955/112584 [00:07<00:13, 5282.59it/s] 35%|███▌      | 39485/112584 [00:07<00:14, 5192.86it/s] 36%|███▌      | 40067/112584 [00:07<00:13, 5375.72it/s] 36%|███▌      | 40725/112584 [00:07<00:12, 5727.25it/s] 37%|███▋      | 41300/112584 [00:07<00:12, 5681.84it/s] 37%|███▋      | 41870/112584 [00:07<00:13, 5269.17it/s] 38%|███▊      | 42404/112584 [00:07<00:13, 5139.65it/s] 38%|███▊      | 42928/112584 [00:07<00:13, 5164.48it/s] 39%|███▊      | 43448/112584 [00:07<00:13, 5153.30it/s] 39%|███▉      | 44062/112584 [00:08<00:12, 5433.71it/s] 40%|███▉      | 44677/112584 [00:08<00:12, 5637.64it/s] 40%|████      | 45244/112584 [00:08<00:12, 5453.12it/s] 41%|████      | 45817/112584 [00:08<00:12, 5529.58it/s] 41%|████▏     | 46464/112584 [00:08<00:11, 5799.05it/s] 42%|████▏     | 47047/112584 [00:08<00:11, 5724.13it/s] 43%|████▎     | 47945/112584 [00:08<00:09, 6667.03it/s] 43%|████▎     | 48615/112584 [00:08<00:09, 6515.98it/s] 44%|████▍     | 49271/112584 [00:08<00:09, 6527.19it/s] 44%|████▍     | 49926/112584 [00:09<00:09, 6280.50it/s] 45%|████▍     | 50558/112584 [00:09<00:10, 5805.09it/s] 45%|████▌     | 51147/112584 [00:09<00:10, 5703.55it/s] 46%|████▌     | 51723/112584 [00:09<00:10, 5697.56it/s] 47%|████▋     | 52477/112584 [00:09<00:09, 6211.70it/s] 47%|████▋     | 53104/112584 [00:09<00:10, 5927.62it/s] 48%|████▊     | 53703/112584 [00:09<00:10, 5843.10it/s] 48%|████▊     | 54292/112584 [00:09<00:10, 5497.73it/s] 49%|████▊     | 54848/112584 [00:09<00:10, 5481.46it/s] 49%|████▉     | 55400/112584 [00:09<00:10, 5476.75it/s] 50%|████▉     | 56042/112584 [00:10<00:09, 5740.92it/s] 50%|█████     | 56620/112584 [00:10<00:10, 5577.36it/s] 51%|█████     | 57181/112584 [00:10<00:10, 5218.58it/s] 51%|█████▏    | 57722/112584 [00:10<00:10, 5270.46it/s] 52%|█████▏    | 58320/112584 [00:10<00:09, 5469.61it/s] 52%|█████▏    | 58971/112584 [00:10<00:09, 5762.75it/s] 53%|█████▎    | 59552/112584 [00:10<00:09, 5496.55it/s] 53%|█████▎    | 60107/112584 [00:10<00:09, 5429.16it/s] 54%|█████▍    | 60693/112584 [00:10<00:09, 5550.68it/s] 55%|█████▍    | 61417/112584 [00:11<00:08, 6039.07it/s] 55%|█████▌    | 62025/112584 [00:11<00:08, 5677.94it/s] 56%|█████▌    | 62603/112584 [00:11<00:08, 5700.60it/s] 56%|█████▌    | 63178/112584 [00:11<00:09, 5447.75it/s] 57%|█████▋    | 63763/112584 [00:11<00:08, 5559.65it/s] 57%|█████▋    | 64324/112584 [00:11<00:08, 5438.40it/s] 58%|█████▊    | 64921/112584 [00:11<00:08, 5582.12it/s] 58%|█████▊    | 65483/112584 [00:11<00:08, 5386.78it/s] 59%|█████▊    | 66053/112584 [00:11<00:08, 5473.63it/s] 59%|█████▉    | 66743/112584 [00:12<00:07, 5883.30it/s] 60%|█████▉    | 67335/112584 [00:12<00:08, 5509.06it/s] 60%|██████    | 67893/112584 [00:12<00:08, 5230.90it/s] 61%|██████    | 68440/112584 [00:12<00:08, 5290.40it/s] 61%|██████▏   | 69115/112584 [00:12<00:07, 5690.02it/s] 62%|██████▏   | 69726/112584 [00:12<00:07, 5808.27it/s] 62%|██████▏   | 70312/112584 [00:12<00:07, 5763.03it/s] 63%|██████▎   | 70892/112584 [00:12<00:07, 5577.79it/s] 63%|██████▎   | 71471/112584 [00:12<00:07, 5631.85it/s] 64%|██████▍   | 72061/112584 [00:12<00:07, 5703.06it/s] 65%|██████▍   | 72690/112584 [00:13<00:06, 5869.63it/s] 65%|██████▌   | 73355/112584 [00:13<00:06, 6097.13it/s] 66%|██████▌   | 74045/112584 [00:13<00:06, 6331.58it/s] 66%|██████▋   | 74690/112584 [00:13<00:05, 6366.32it/s] 67%|██████▋   | 75328/112584 [00:13<00:06, 5993.39it/s] 67%|██████▋   | 75933/112584 [00:13<00:06, 5966.83it/s] 68%|██████▊   | 76534/112584 [00:13<00:06, 5963.43it/s] 69%|██████▊   | 77133/112584 [00:13<00:06, 5691.02it/s] 69%|██████▉   | 77772/112584 [00:13<00:05, 5880.50it/s] 70%|██████▉   | 78364/112584 [00:14<00:06, 5627.86it/s] 70%|███████   | 78931/112584 [00:14<00:06, 5255.33it/s] 71%|███████   | 79463/112584 [00:14<00:06, 5215.15it/s] 71%|███████   | 79989/112584 [00:14<00:06, 5217.13it/s] 72%|███████▏  | 80514/112584 [00:14<00:06, 5219.56it/s] 72%|███████▏  | 81132/112584 [00:14<00:05, 5495.71it/s] 73%|███████▎  | 81704/112584 [00:14<00:05, 5555.83it/s] 73%|███████▎  | 82283/112584 [00:14<00:05, 5620.31it/s] 74%|███████▎  | 82867/112584 [00:14<00:05, 5682.83it/s] 74%|███████▍  | 83477/112584 [00:14<00:05, 5804.23it/s] 75%|███████▍  | 84059/112584 [00:15<00:04, 5778.64it/s] 75%|███████▌  | 84638/112584 [00:15<00:04, 5657.95it/s] 76%|███████▌  | 85205/112584 [00:15<00:05, 5445.88it/s] 76%|███████▌  | 85792/112584 [00:15<00:04, 5565.57it/s] 77%|███████▋  | 86354/112584 [00:15<00:04, 5576.38it/s] 77%|███████▋  | 87010/112584 [00:15<00:04, 5862.08it/s] 78%|███████▊  | 87598/112584 [00:15<00:04, 5856.71it/s] 78%|███████▊  | 88188/112584 [00:15<00:04, 5866.33it/s] 79%|███████▉  | 88776/112584 [00:15<00:04, 5319.76it/s] 79%|███████▉  | 89319/112584 [00:16<00:04, 5317.15it/s] 80%|███████▉  | 89891/112584 [00:16<00:04, 5427.80it/s] 80%|████████  | 90440/112584 [00:16<00:04, 5323.26it/s] 81%|████████  | 90977/112584 [00:16<00:04, 5169.20it/s] 81%|████████▏ | 91621/112584 [00:16<00:03, 5524.38it/s] 82%|████████▏ | 92178/112584 [00:16<00:03, 5521.31it/s] 82%|████████▏ | 92794/112584 [00:16<00:03, 5706.60it/s] 83%|████████▎ | 93368/112584 [00:16<00:03, 5479.49it/s] 83%|████████▎ | 93920/112584 [00:16<00:03, 5431.33it/s] 84%|████████▍ | 94506/112584 [00:16<00:03, 5554.29it/s] 85%|████████▍ | 95136/112584 [00:17<00:03, 5771.31it/s] 85%|████████▌ | 95716/112584 [00:17<00:03, 5521.03it/s] 86%|████████▌ | 96349/112584 [00:17<00:02, 5749.64it/s] 86%|████████▌ | 97100/112584 [00:17<00:02, 6256.11it/s] 87%|████████▋ | 97730/112584 [00:17<00:02, 5806.11it/s] 87%|████████▋ | 98331/112584 [00:17<00:02, 5862.82it/s] 88%|████████▊ | 98924/112584 [00:17<00:02, 5671.31it/s] 88%|████████▊ | 99497/112584 [00:17<00:02, 5405.45it/s] 89%|████████▉ | 100045/112584 [00:17<00:02, 5417.85it/s] 89%|████████▉ | 100591/112584 [00:18<00:02, 5223.31it/s] 90%|████████▉ | 101195/112584 [00:18<00:02, 5446.73it/s] 90%|█████████ | 101759/112584 [00:18<00:01, 5500.10it/s] 91%|█████████ | 102355/112584 [00:18<00:01, 5632.00it/s] 91%|█████████▏| 102924/112584 [00:18<00:01, 5645.90it/s] 92%|█████████▏| 103491/112584 [00:18<00:01, 5472.41it/s] 92%|█████████▏| 104041/112584 [00:18<00:01, 5322.55it/s] 93%|█████████▎| 104653/112584 [00:18<00:01, 5544.83it/s] 93%|█████████▎| 105210/112584 [00:18<00:01, 5395.85it/s] 94%|█████████▍| 105863/112584 [00:18<00:01, 5721.48it/s] 95%|█████████▍| 106439/112584 [00:19<00:01, 5377.53it/s] 95%|█████████▌| 106983/112584 [00:19<00:01, 5364.96it/s] 96%|█████████▌| 107524/112584 [00:19<00:00, 5356.31it/s] 96%|█████████▌| 108115/112584 [00:19<00:00, 5504.27it/s] 97%|█████████▋| 108668/112584 [00:19<00:00, 5261.78it/s] 97%|█████████▋| 109198/112584 [00:19<00:00, 5240.62it/s] 97%|█████████▋| 109749/112584 [00:19<00:00, 5316.74it/s] 98%|█████████▊| 110351/112584 [00:19<00:00, 5516.35it/s] 99%|█████████▊| 110944/112584 [00:19<00:00, 5630.61it/s] 99%|█████████▉| 111530/112584 [00:20<00:00, 5690.32it/s]100%|█████████▉| 112101/112584 [00:20<00:00, 5538.77it/s]100%|██████████| 112584/112584 [00:20<00:00, 5567.64it/s]

gathering stats for n=1
  0%|          | 0/112584 [00:00<?, ?it/s]  2%|▏         | 1788/112584 [00:00<00:06, 17870.26it/s]  3%|▎         | 3662/112584 [00:00<00:05, 18375.74it/s]  5%|▌         | 5721/112584 [00:00<00:05, 19384.41it/s]  7%|▋         | 7660/112584 [00:00<00:05, 18483.94it/s]  8%|▊         | 9515/112584 [00:00<00:05, 18250.05it/s] 10%|█         | 11344/112584 [00:00<00:05, 18201.15it/s] 12%|█▏        | 13194/112584 [00:00<00:05, 18293.89it/s] 13%|█▎        | 15110/112584 [00:00<00:05, 18565.08it/s] 15%|█▌        | 16969/112584 [00:00<00:05, 18155.55it/s] 17%|█▋        | 18857/112584 [00:01<00:05, 18368.25it/s] 18%|█▊        | 20732/112584 [00:01<00:04, 18481.72it/s] 20%|██        | 22582/112584 [00:01<00:04, 18290.64it/s] 22%|██▏       | 24709/112584 [00:01<00:04, 19178.50it/s] 24%|██▎       | 26630/112584 [00:01<00:04, 18690.19it/s] 25%|██▌       | 28504/112584 [00:01<00:04, 18515.77it/s] 27%|██▋       | 30359/112584 [00:01<00:04, 18417.08it/s] 29%|██▊       | 32203/112584 [00:01<00:04, 17785.18it/s] 30%|███       | 33987/112584 [00:01<00:04, 17798.75it/s] 32%|███▏      | 35820/112584 [00:01<00:04, 17952.33it/s] 33%|███▎      | 37619/112584 [00:02<00:04, 17579.72it/s] 35%|███▍      | 39392/112584 [00:02<00:04, 17618.09it/s] 37%|███▋      | 41314/112584 [00:02<00:03, 18080.73it/s] 38%|███▊      | 43125/112584 [00:02<00:03, 17484.75it/s] 40%|███▉      | 44974/112584 [00:02<00:03, 17773.65it/s] 42%|████▏     | 46888/112584 [00:02<00:03, 18173.03it/s] 44%|████▍     | 49256/112584 [00:02<00:03, 19796.41it/s] 46%|████▌     | 51242/112584 [00:02<00:03, 19057.76it/s] 47%|████▋     | 53246/112584 [00:02<00:03, 19338.96it/s] 49%|████▉     | 55188/112584 [00:03<00:03, 18702.40it/s] 51%|█████     | 57067/112584 [00:03<00:03, 18423.33it/s] 52%|█████▏    | 58986/112584 [00:03<00:02, 18643.27it/s] 54%|█████▍    | 60856/112584 [00:03<00:02, 18546.02it/s] 56%|█████▌    | 62734/112584 [00:03<00:02, 18610.59it/s] 57%|█████▋    | 64598/112584 [00:03<00:02, 18341.79it/s] 59%|█████▉    | 66435/112584 [00:03<00:02, 18311.67it/s] 61%|██████    | 68268/112584 [00:03<00:02, 18070.96it/s] 62%|██████▏   | 70256/112584 [00:03<00:02, 18596.79it/s] 64%|██████▍   | 72118/112584 [00:03<00:02, 18482.65it/s] 66%|██████▌   | 74321/112584 [00:04<00:01, 19522.38it/s] 68%|██████▊   | 76276/112584 [00:04<00:01, 19225.06it/s] 69%|██████▉   | 78202/112584 [00:04<00:01, 18951.09it/s] 71%|███████   | 80100/112584 [00:04<00:01, 18049.19it/s] 73%|███████▎  | 81975/112584 [00:04<00:01, 18244.66it/s] 75%|███████▍  | 83966/112584 [00:04<00:01, 18721.88it/s] 76%|███████▋  | 85846/112584 [00:04<00:01, 18456.68it/s] 78%|███████▊  | 87796/112584 [00:04<00:01, 18751.73it/s] 80%|███████▉  | 89676/112584 [00:04<00:01, 18400.87it/s] 81%|████████▏ | 91521/112584 [00:04<00:01, 18267.73it/s] 83%|████████▎ | 93353/112584 [00:05<00:01, 18279.26it/s] 85%|████████▍ | 95239/112584 [00:05<00:00, 18443.11it/s] 86%|████████▋ | 97330/112584 [00:05<00:00, 19164.52it/s] 88%|████████▊ | 99249/112584 [00:05<00:00, 18721.65it/s] 90%|████████▉ | 101125/112584 [00:05<00:00, 18206.63it/s] 91%|█████████▏| 102980/112584 [00:05<00:00, 18305.36it/s] 93%|█████████▎| 104815/112584 [00:05<00:00, 18161.40it/s] 95%|█████████▍| 106634/112584 [00:05<00:00, 18083.10it/s] 96%|█████████▋| 108444/112584 [00:05<00:00, 18034.59it/s] 98%|█████████▊| 110249/112584 [00:05<00:00, 17788.65it/s]100%|█████████▉| 112139/112584 [00:06<00:00, 18111.09it/s]100%|██████████| 112584/112584 [00:06<00:00, 18412.56it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 536.36it/s]2022-03-23 19:52:15 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=39136, bias=False)
  )
)
2022-03-23 19:52:15 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-23 19:52:15 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-23 19:52:15 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-23 19:52:15 | INFO | fairseq_cli.train | num. shared model params: 38,951,936 (num. trained: 38,951,936)
2022-03-23 19:52:15 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-23 19:52:15 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/valid
2022-03-23 19:52:15 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-23 19:52:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 19:52:15 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-23 19:52:15 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 19:52:15 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-23 19:52:15 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-23 19:52:15 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_last.pt
2022-03-23 19:52:15 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_last.pt
2022-03-23 19:52:15 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-23 19:52:15 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/train
2022-03-23 19:52:15 | INFO | fairseq.trainer | begin training epoch 1
2022-03-23 19:52:15 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-23 19:52:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-23 19:52:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 19:52:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 19:52:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-23 19:55:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 19:55:12 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.897 | ppl 7625.62 | wps 64043 | wpb 2040.3 | bsz 4 | num_updates 99
2022-03-23 19:55:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 99 updates
2022-03-23 19:55:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 19:55:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 19:55:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 1 @ 99 updates, score 12.897) (writing took 1.1160382790258154 seconds)
2022-03-23 19:55:13 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-23 19:55:13 | INFO | train | epoch 001 | loss 14.477 | ppl 22811.2 | wps 38812.3 | ups 0.59 | wpb 65303.3 | bsz 127.6 | num_updates 99 | lr 1.24725e-05 | gnorm 2.907 | loss_scale 8 | train_wall 168 | gb_free 20.8 | wall 178
KL Stats: Epoch 1 Divergences: Uniform: 0.5352639918381696 Unigram: 2.607850689272444
2022-03-23 19:55:13 | INFO | fairseq.trainer | begin training epoch 2
2022-03-23 19:55:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 19:55:15 | INFO | train_inner | epoch 002:      1 / 103 loss=14.464, ppl=22598.3, wps=38805.5, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=100, lr=1.25975e-05, gnorm=2.892, loss_scale=8, train_wall=170, gb_free=20.8, wall=180
2022-03-23 19:58:00 | INFO | train_inner | epoch 002:    101 / 103 loss=12.472, ppl=5679.82, wps=39843.7, ups=0.61, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.128, loss_scale=8, train_wall=159, gb_free=20.8, wall=345
2022-03-23 19:58:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 19:58:06 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.62 | ppl 3147.64 | wps 64120.4 | wpb 2040.3 | bsz 4 | num_updates 202 | best_loss 11.62
2022-03-23 19:58:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 202 updates
2022-03-23 19:58:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 19:58:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 19:58:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 2 @ 202 updates, score 11.62) (writing took 1.0901067709783092 seconds)
2022-03-23 19:58:07 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-23 19:58:07 | INFO | train | epoch 002 | loss 12.467 | ppl 5662.21 | wps 38725.7 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 202 | lr 2.5345e-05 | gnorm 1.125 | loss_scale 8 | train_wall 164 | gb_free 20.8 | wall 352
KL Stats: Epoch 2 Divergences: Uniform: 0.5524275608911546 Unigram: 1.1886507017466021
2022-03-23 19:58:07 | INFO | fairseq.trainer | begin training epoch 3
2022-03-23 19:58:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:00:48 | INFO | train_inner | epoch 003:     98 / 103 loss=11.241, ppl=2420.97, wps=38678.1, ups=0.59, wpb=65300.5, bsz=127.6, num_updates=300, lr=3.75925e-05, gnorm=0.633, loss_scale=8, train_wall=159, gb_free=20.8, wall=514
2022-03-23 20:00:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:01:00 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.723 | ppl 1690.03 | wps 64083.7 | wpb 2040.3 | bsz 4 | num_updates 305 | best_loss 10.723
2022-03-23 20:01:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 305 updates
2022-03-23 20:01:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:01:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:01:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 3 @ 305 updates, score 10.723) (writing took 1.083216795930639 seconds)
2022-03-23 20:01:01 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-23 20:01:01 | INFO | train | epoch 003 | loss 11.213 | ppl 2373.76 | wps 38717.1 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 305 | lr 3.82174e-05 | gnorm 0.62 | loss_scale 8 | train_wall 164 | gb_free 20.8 | wall 526
KL Stats: Epoch 3 Divergences: Uniform: 0.8069991834113684 Unigram: 0.501973229578726
2022-03-23 20:01:01 | INFO | fairseq.trainer | begin training epoch 4
2022-03-23 20:01:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:03:37 | INFO | train_inner | epoch 004:     95 / 103 loss=10.633, ppl=1587.5, wps=38716.2, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=400, lr=5.009e-05, gnorm=0.426, loss_scale=8, train_wall=159, gb_free=20.8, wall=682
2022-03-23 20:03:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:03:53 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.412 | ppl 1362.87 | wps 63881.1 | wpb 2040.3 | bsz 4 | num_updates 408 | best_loss 10.412
2022-03-23 20:03:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 408 updates
2022-03-23 20:03:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:03:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:03:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 4 @ 408 updates, score 10.412) (writing took 1.119167087948881 seconds)
2022-03-23 20:03:55 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-23 20:03:55 | INFO | train | epoch 004 | loss 10.614 | ppl 1567.49 | wps 38734.9 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 408 | lr 5.10898e-05 | gnorm 0.423 | loss_scale 8 | train_wall 164 | gb_free 20.8 | wall 700
KL Stats: Epoch 4 Divergences: Uniform: 1.2735840379687609 Unigram: 0.30656040401395607
2022-03-23 20:03:55 | INFO | fairseq.trainer | begin training epoch 5
2022-03-23 20:03:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:06:26 | INFO | train_inner | epoch 005:     92 / 103 loss=10.396, ppl=1347.5, wps=38688.6, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=500, lr=6.25875e-05, gnorm=0.416, loss_scale=8, train_wall=159, gb_free=20.8, wall=851
2022-03-23 20:06:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:06:47 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.189 | ppl 1167.16 | wps 64135.8 | wpb 2040.3 | bsz 4 | num_updates 511 | best_loss 10.189
2022-03-23 20:06:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 511 updates
2022-03-23 20:06:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:06:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:06:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 5 @ 511 updates, score 10.189) (writing took 1.0607228289591148 seconds)
2022-03-23 20:06:48 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-23 20:06:48 | INFO | train | epoch 005 | loss 10.377 | ppl 1329.44 | wps 38742 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 511 | lr 6.39622e-05 | gnorm 0.419 | loss_scale 8 | train_wall 164 | gb_free 20.8 | wall 873
KL Stats: Epoch 5 Divergences: Uniform: 1.5444344254003022 Unigram: 0.40316886195105284
2022-03-23 20:06:48 | INFO | fairseq.trainer | begin training epoch 6
2022-03-23 20:06:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:09:14 | INFO | train_inner | epoch 006:     89 / 103 loss=10.186, ppl=1164.69, wps=38753.1, ups=0.59, wpb=65310.7, bsz=127.6, num_updates=600, lr=7.5085e-05, gnorm=0.44, loss_scale=16, train_wall=159, gb_free=20.8, wall=1020
2022-03-23 20:09:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:09:40 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.963 | ppl 998.07 | wps 65165.6 | wpb 2040.3 | bsz 4 | num_updates 614 | best_loss 9.963
2022-03-23 20:09:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 614 updates
2022-03-23 20:09:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:09:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:09:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 6 @ 614 updates, score 9.963) (writing took 1.035405536997132 seconds)
2022-03-23 20:09:41 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-23 20:09:41 | INFO | train | epoch 006 | loss 10.163 | ppl 1146.31 | wps 38843.7 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 614 | lr 7.68347e-05 | gnorm 0.448 | loss_scale 16 | train_wall 163 | gb_free 20.8 | wall 1046
KL Stats: Epoch 6 Divergences: Uniform: 1.661944538444752 Unigram: 0.5377399676071454
2022-03-23 20:09:41 | INFO | fairseq.trainer | begin training epoch 7
2022-03-23 20:09:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:12:02 | INFO | train_inner | epoch 007:     86 / 103 loss=9.986, ppl=1013.9, wps=38940.9, ups=0.6, wpb=65300.5, bsz=127.6, num_updates=700, lr=8.75825e-05, gnorm=0.446, loss_scale=16, train_wall=158, gb_free=20.8, wall=1187
2022-03-23 20:12:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:12:33 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.757 | ppl 865.56 | wps 64167.3 | wpb 2040.3 | bsz 4 | num_updates 717 | best_loss 9.757
2022-03-23 20:12:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 717 updates
2022-03-23 20:12:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:12:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:12:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 7 @ 717 updates, score 9.757) (writing took 1.1069330760510638 seconds)
2022-03-23 20:12:34 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-23 20:12:34 | INFO | train | epoch 007 | loss 9.955 | ppl 992.82 | wps 38909.7 | ups 0.6 | wpb 65312.3 | bsz 127.6 | num_updates 717 | lr 8.97071e-05 | gnorm 0.438 | loss_scale 16 | train_wall 163 | gb_free 20.8 | wall 1219
KL Stats: Epoch 7 Divergences: Uniform: 1.7595076817017612 Unigram: 0.6613056695299054
2022-03-23 20:12:34 | INFO | fairseq.trainer | begin training epoch 8
2022-03-23 20:12:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:14:51 | INFO | train_inner | epoch 008:     83 / 103 loss=9.79, ppl=885.56, wps=38711.9, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=800, lr=0.00010008, gnorm=0.499, loss_scale=16, train_wall=159, gb_free=20.8, wall=1356
2022-03-23 20:15:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:15:27 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.543 | ppl 746.13 | wps 63874.3 | wpb 2040.3 | bsz 4 | num_updates 820 | best_loss 9.543
2022-03-23 20:15:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 820 updates
2022-03-23 20:15:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:15:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:15:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 8 @ 820 updates, score 9.543) (writing took 1.1725858999416232 seconds)
2022-03-23 20:15:28 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-23 20:15:28 | INFO | train | epoch 008 | loss 9.753 | ppl 862.94 | wps 38714.1 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 820 | lr 0.00010258 | gnorm 0.502 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 1393
KL Stats: Epoch 8 Divergences: Uniform: 1.8769326228260688 Unigram: 0.7773820599460918
2022-03-23 20:15:28 | INFO | fairseq.trainer | begin training epoch 9
2022-03-23 20:15:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:17:40 | INFO | train_inner | epoch 009:     80 / 103 loss=9.591, ppl=770.96, wps=38665.7, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=900, lr=0.000112578, gnorm=0.566, loss_scale=16, train_wall=159, gb_free=20.8, wall=1525
2022-03-23 20:18:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:18:21 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.351 | ppl 652.81 | wps 64293.2 | wpb 2040.3 | bsz 4 | num_updates 923 | best_loss 9.351
2022-03-23 20:18:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 923 updates
2022-03-23 20:18:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:18:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:18:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 9 @ 923 updates, score 9.351) (writing took 1.1028362130746245 seconds)
2022-03-23 20:18:22 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-23 20:18:22 | INFO | train | epoch 009 | loss 9.551 | ppl 750.12 | wps 38704.4 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 923 | lr 0.000115452 | gnorm 0.574 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 1567
KL Stats: Epoch 9 Divergences: Uniform: 2.0096299104104807 Unigram: 0.8926927061280857
2022-03-23 20:18:22 | INFO | fairseq.trainer | begin training epoch 10
2022-03-23 20:18:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:20:29 | INFO | train_inner | epoch 010:     77 / 103 loss=9.406, ppl=678.22, wps=38675.2, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=1000, lr=0.000125075, gnorm=0.554, loss_scale=16, train_wall=159, gb_free=20.8, wall=1694
2022-03-23 20:21:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:21:15 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 9.185 | ppl 582.07 | wps 63592.7 | wpb 2040.3 | bsz 4 | num_updates 1026 | best_loss 9.185
2022-03-23 20:21:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1026 updates
2022-03-23 20:21:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:21:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:21:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 10 @ 1026 updates, score 9.185) (writing took 1.119172397069633 seconds)
2022-03-23 20:21:16 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-23 20:21:16 | INFO | train | epoch 010 | loss 9.363 | ppl 658.59 | wps 38703.2 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 1026 | lr 0.000128324 | gnorm 0.563 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 1741
KL Stats: Epoch 10 Divergences: Uniform: 2.129611819787427 Unigram: 1.0005327948492737
2022-03-23 20:21:16 | INFO | fairseq.trainer | begin training epoch 11
2022-03-23 20:21:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:23:17 | INFO | train_inner | epoch 011:     74 / 103 loss=9.239, ppl=604.16, wps=38687.7, ups=0.59, wpb=65310.7, bsz=127.6, num_updates=1100, lr=0.000137573, gnorm=0.546, loss_scale=32, train_wall=159, gb_free=20.8, wall=1862
2022-03-23 20:24:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:24:08 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 9.045 | ppl 528.29 | wps 64009.1 | wpb 2040.3 | bsz 4 | num_updates 1129 | best_loss 9.045
2022-03-23 20:24:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1129 updates
2022-03-23 20:24:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:24:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:24:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 11 @ 1129 updates, score 9.045) (writing took 1.104412539047189 seconds)
2022-03-23 20:24:09 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-23 20:24:09 | INFO | train | epoch 011 | loss 9.198 | ppl 587.32 | wps 38729.4 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 1129 | lr 0.000141197 | gnorm 0.556 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 1914
KL Stats: Epoch 11 Divergences: Uniform: 2.2325137505946944 Unigram: 1.09508556081513
2022-03-23 20:24:09 | INFO | fairseq.trainer | begin training epoch 12
2022-03-23 20:24:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:26:07 | INFO | train_inner | epoch 012:     71 / 103 loss=9.095, ppl=546.94, wps=38562.4, ups=0.59, wpb=65300.5, bsz=127.6, num_updates=1200, lr=0.00015007, gnorm=0.575, loss_scale=32, train_wall=159, gb_free=20.8, wall=2032
2022-03-23 20:26:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:27:03 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.92 | ppl 484.55 | wps 63556.9 | wpb 2040.3 | bsz 4 | num_updates 1232 | best_loss 8.92
2022-03-23 20:27:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1232 updates
2022-03-23 20:27:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:27:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:27:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 12 @ 1232 updates, score 8.92) (writing took 1.1130725049879402 seconds)
2022-03-23 20:27:04 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-23 20:27:04 | INFO | train | epoch 012 | loss 9.053 | ppl 531.13 | wps 38556.6 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 1232 | lr 0.000154069 | gnorm 0.564 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 2089
KL Stats: Epoch 12 Divergences: Uniform: 2.3288531620829542 Unigram: 1.1746854893290501
2022-03-23 20:27:04 | INFO | fairseq.trainer | begin training epoch 13
2022-03-23 20:27:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:28:56 | INFO | train_inner | epoch 013:     68 / 103 loss=8.959, ppl=497.58, wps=38610.5, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=1300, lr=0.000162568, gnorm=0.546, loss_scale=32, train_wall=159, gb_free=20.8, wall=2201
2022-03-23 20:29:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:29:57 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.803 | ppl 446.8 | wps 64144 | wpb 2040.3 | bsz 4 | num_updates 1335 | best_loss 8.803
2022-03-23 20:29:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1335 updates
2022-03-23 20:29:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:29:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:29:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 13 @ 1335 updates, score 8.803) (writing took 1.0560497499536723 seconds)
2022-03-23 20:29:58 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-23 20:29:58 | INFO | train | epoch 013 | loss 8.922 | ppl 484.91 | wps 38676.8 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 1335 | lr 0.000166942 | gnorm 0.566 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 2263
KL Stats: Epoch 13 Divergences: Uniform: 2.4237257527796476 Unigram: 1.2453690470646042
2022-03-23 20:29:58 | INFO | fairseq.trainer | begin training epoch 14
2022-03-23 20:29:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:31:45 | INFO | train_inner | epoch 014:     65 / 103 loss=8.841, ppl=458.72, wps=38634.2, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=1400, lr=0.000175065, gnorm=0.585, loss_scale=32, train_wall=159, gb_free=20.8, wall=2370
2022-03-23 20:32:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:32:51 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.705 | ppl 417.29 | wps 63830 | wpb 2040.3 | bsz 4 | num_updates 1438 | best_loss 8.705
2022-03-23 20:32:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1438 updates
2022-03-23 20:32:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:32:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:32:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 14 @ 1438 updates, score 8.705) (writing took 1.0872471160255373 seconds)
2022-03-23 20:32:52 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-23 20:32:52 | INFO | train | epoch 014 | loss 8.797 | ppl 444.85 | wps 38651.9 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 1438 | lr 0.000179814 | gnorm 0.552 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 2437
KL Stats: Epoch 14 Divergences: Uniform: 2.5168436906231637 Unigram: 1.3084415922273647
2022-03-23 20:32:52 | INFO | fairseq.trainer | begin training epoch 15
2022-03-23 20:32:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:34:34 | INFO | train_inner | epoch 015:     62 / 103 loss=8.725, ppl=423.09, wps=38651, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=1500, lr=0.000187563, gnorm=0.571, loss_scale=32, train_wall=159, gb_free=20.8, wall=2539
2022-03-23 20:35:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:35:45 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.604 | ppl 389.02 | wps 64204.8 | wpb 2040.3 | bsz 4 | num_updates 1541 | best_loss 8.604
2022-03-23 20:35:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1541 updates
2022-03-23 20:35:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:35:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:35:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 15 @ 1541 updates, score 8.604) (writing took 1.0774883419508114 seconds)
2022-03-23 20:35:46 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-23 20:35:46 | INFO | train | epoch 015 | loss 8.683 | ppl 410.95 | wps 38688.8 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 1541 | lr 0.000192686 | gnorm 0.581 | loss_scale 64 | train_wall 164 | gb_free 20.8 | wall 2611
KL Stats: Epoch 15 Divergences: Uniform: 2.608187545981202 Unigram: 1.3670588928621101
2022-03-23 20:35:46 | INFO | fairseq.trainer | begin training epoch 16
2022-03-23 20:35:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:36:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 20:37:24 | INFO | train_inner | epoch 016:     60 / 103 loss=8.618, ppl=392.86, wps=38326.5, ups=0.59, wpb=65310.7, bsz=127.6, num_updates=1600, lr=0.00020006, gnorm=0.567, loss_scale=32, train_wall=160, gb_free=20.8, wall=2709
2022-03-23 20:38:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:38:38 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.517 | ppl 366.24 | wps 64232.9 | wpb 2040.3 | bsz 4 | num_updates 1643 | best_loss 8.517
2022-03-23 20:38:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1643 updates
2022-03-23 20:38:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:38:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:38:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 16 @ 1643 updates, score 8.517) (writing took 1.0841016460908577 seconds)
2022-03-23 20:38:39 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-23 20:38:39 | INFO | train | epoch 016 | loss 8.572 | ppl 380.51 | wps 38381.1 | ups 0.59 | wpb 65310.1 | bsz 127.6 | num_updates 1643 | lr 0.000205434 | gnorm 0.585 | loss_scale 32 | train_wall 163 | gb_free 20.8 | wall 2784
KL Stats: Epoch 16 Divergences: Uniform: 2.692187748607707 Unigram: 1.4192112725900787
2022-03-23 20:38:39 | INFO | fairseq.trainer | begin training epoch 17
2022-03-23 20:38:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:40:13 | INFO | train_inner | epoch 017:     57 / 103 loss=8.507, ppl=363.78, wps=38657, ups=0.59, wpb=65300.5, bsz=127.6, num_updates=1700, lr=0.000212558, gnorm=0.592, loss_scale=32, train_wall=159, gb_free=20.8, wall=2878
2022-03-23 20:41:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:41:32 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.42 | ppl 342.55 | wps 64185.2 | wpb 2040.3 | bsz 4 | num_updates 1746 | best_loss 8.42
2022-03-23 20:41:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1746 updates
2022-03-23 20:41:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:41:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:41:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 17 @ 1746 updates, score 8.42) (writing took 1.057388273999095 seconds)
2022-03-23 20:41:33 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-23 20:41:33 | INFO | train | epoch 017 | loss 8.463 | ppl 352.85 | wps 38661.1 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 1746 | lr 0.000218306 | gnorm 0.583 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 2958
KL Stats: Epoch 17 Divergences: Uniform: 2.77386583557634 Unigram: 1.4694468923332324
2022-03-23 20:41:33 | INFO | fairseq.trainer | begin training epoch 18
2022-03-23 20:41:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:43:02 | INFO | train_inner | epoch 018:     54 / 103 loss=8.4, ppl=337.91, wps=38652.5, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=1800, lr=0.000225055, gnorm=0.563, loss_scale=32, train_wall=159, gb_free=20.8, wall=3047
2022-03-23 20:44:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:44:26 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.336 | ppl 323.1 | wps 63671.5 | wpb 2040.3 | bsz 4 | num_updates 1849 | best_loss 8.336
2022-03-23 20:44:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1849 updates
2022-03-23 20:44:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:44:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:44:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 18 @ 1849 updates, score 8.336) (writing took 1.0598114249296486 seconds)
2022-03-23 20:44:27 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-23 20:44:27 | INFO | train | epoch 018 | loss 8.353 | ppl 327.06 | wps 38697 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 1849 | lr 0.000231179 | gnorm 0.574 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 3132
KL Stats: Epoch 18 Divergences: Uniform: 2.8563515593348816 Unigram: 1.5182049588887032
2022-03-23 20:44:27 | INFO | fairseq.trainer | begin training epoch 19
2022-03-23 20:44:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:45:51 | INFO | train_inner | epoch 019:     51 / 103 loss=8.302, ppl=315.51, wps=38633.6, ups=0.59, wpb=65310.7, bsz=127.6, num_updates=1900, lr=0.000237553, gnorm=0.604, loss_scale=32, train_wall=159, gb_free=20.8, wall=3216
2022-03-23 20:47:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:47:20 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.253 | ppl 305.09 | wps 64190.2 | wpb 2040.3 | bsz 4 | num_updates 1952 | best_loss 8.253
2022-03-23 20:47:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1952 updates
2022-03-23 20:47:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:47:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:47:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 19 @ 1952 updates, score 8.253) (writing took 1.0879733039764687 seconds)
2022-03-23 20:47:21 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-23 20:47:21 | INFO | train | epoch 019 | loss 8.248 | ppl 303.98 | wps 38644.5 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 1952 | lr 0.000244051 | gnorm 0.596 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 3306
KL Stats: Epoch 19 Divergences: Uniform: 2.936737805949103 Unigram: 1.5632610682311765
2022-03-23 20:47:21 | INFO | fairseq.trainer | begin training epoch 20
2022-03-23 20:47:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:48:40 | INFO | train_inner | epoch 020:     48 / 103 loss=8.195, ppl=293.09, wps=38652.2, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=2000, lr=0.00025005, gnorm=0.585, loss_scale=32, train_wall=159, gb_free=20.8, wall=3385
2022-03-23 20:50:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:50:14 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.177 | ppl 289.38 | wps 64465.9 | wpb 2040.3 | bsz 4 | num_updates 2055 | best_loss 8.177
2022-03-23 20:50:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 2055 updates
2022-03-23 20:50:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:50:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:50:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 20 @ 2055 updates, score 8.177) (writing took 1.0870720080565661 seconds)
2022-03-23 20:50:15 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-23 20:50:15 | INFO | train | epoch 020 | loss 8.142 | ppl 282.54 | wps 38702.4 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 2055 | lr 0.000256924 | gnorm 0.592 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 3480
KL Stats: Epoch 20 Divergences: Uniform: 3.0182487398002373 Unigram: 1.6090554889049018
2022-03-23 20:50:15 | INFO | fairseq.trainer | begin training epoch 21
2022-03-23 20:50:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:51:29 | INFO | train_inner | epoch 021:     45 / 103 loss=8.096, ppl=273.59, wps=38660.4, ups=0.59, wpb=65300.5, bsz=127.6, num_updates=2100, lr=0.000262548, gnorm=0.576, loss_scale=64, train_wall=159, gb_free=20.8, wall=3554
2022-03-23 20:53:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:53:08 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.103 | ppl 274.87 | wps 63924.2 | wpb 2040.3 | bsz 4 | num_updates 2158 | best_loss 8.103
2022-03-23 20:53:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2158 updates
2022-03-23 20:53:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:53:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:53:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 21 @ 2158 updates, score 8.103) (writing took 1.0821791590424255 seconds)
2022-03-23 20:53:09 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-23 20:53:09 | INFO | train | epoch 021 | loss 8.038 | ppl 262.91 | wps 38674.2 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 2158 | lr 0.000269796 | gnorm 0.566 | loss_scale 64 | train_wall 164 | gb_free 20.8 | wall 3654
KL Stats: Epoch 21 Divergences: Uniform: 3.100877270377616 Unigram: 1.653439786285984
2022-03-23 20:53:09 | INFO | fairseq.trainer | begin training epoch 22
2022-03-23 20:53:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:53:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 20:54:20 | INFO | train_inner | epoch 022:     43 / 103 loss=7.995, ppl=255.04, wps=38261.8, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=2200, lr=0.000275045, gnorm=0.577, loss_scale=32, train_wall=161, gb_free=20.8, wall=3725
2022-03-23 20:55:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:56:02 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.05 | ppl 265.1 | wps 63867.8 | wpb 2040.3 | bsz 4 | num_updates 2260 | best_loss 8.05
2022-03-23 20:56:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2260 updates
2022-03-23 20:56:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:56:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:56:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 22 @ 2260 updates, score 8.05) (writing took 1.0770465509267524 seconds)
2022-03-23 20:56:03 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-23 20:56:03 | INFO | train | epoch 022 | loss 7.939 | ppl 245.45 | wps 38310 | ups 0.59 | wpb 65310.1 | bsz 127.6 | num_updates 2260 | lr 0.000282544 | gnorm 0.58 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 3828
KL Stats: Epoch 22 Divergences: Uniform: 3.183561087198317 Unigram: 1.6950717726451765
2022-03-23 20:56:03 | INFO | fairseq.trainer | begin training epoch 23
2022-03-23 20:56:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:57:09 | INFO | train_inner | epoch 023:     40 / 103 loss=7.897, ppl=238.44, wps=38653.5, ups=0.59, wpb=65310.7, bsz=127.6, num_updates=2300, lr=0.000287543, gnorm=0.577, loss_scale=32, train_wall=159, gb_free=20.8, wall=3894
2022-03-23 20:58:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 20:58:56 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.972 | ppl 251.01 | wps 64071.9 | wpb 2040.3 | bsz 4 | num_updates 2363 | best_loss 7.972
2022-03-23 20:58:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2363 updates
2022-03-23 20:58:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:58:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 20:58:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 23 @ 2363 updates, score 7.972) (writing took 1.0739354849793017 seconds)
2022-03-23 20:58:57 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-23 20:58:57 | INFO | train | epoch 023 | loss 7.842 | ppl 229.49 | wps 38696.6 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 2363 | lr 0.000295416 | gnorm 0.575 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 4002
KL Stats: Epoch 23 Divergences: Uniform: 3.2580647307575163 Unigram: 1.734713084700354
2022-03-23 20:58:57 | INFO | fairseq.trainer | begin training epoch 24
2022-03-23 20:58:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 20:59:58 | INFO | train_inner | epoch 024:     37 / 103 loss=7.809, ppl=224.25, wps=38688.7, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=2400, lr=0.00030004, gnorm=0.565, loss_scale=32, train_wall=159, gb_free=20.8, wall=4063
2022-03-23 21:01:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:01:49 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.92 | ppl 242.23 | wps 64182.7 | wpb 2040.3 | bsz 4 | num_updates 2466 | best_loss 7.92
2022-03-23 21:01:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2466 updates
2022-03-23 21:01:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:01:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:01:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 24 @ 2466 updates, score 7.92) (writing took 1.0530094400746748 seconds)
2022-03-23 21:01:50 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-23 21:01:50 | INFO | train | epoch 024 | loss 7.748 | ppl 214.92 | wps 38731.2 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 2466 | lr 0.000308288 | gnorm 0.562 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 4175
KL Stats: Epoch 24 Divergences: Uniform: 3.3381086259696713 Unigram: 1.7729430688308767
2022-03-23 21:01:50 | INFO | fairseq.trainer | begin training epoch 25
2022-03-23 21:01:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:02:46 | INFO | train_inner | epoch 025:     34 / 103 loss=7.713, ppl=209.84, wps=38716.5, ups=0.59, wpb=65300.5, bsz=127.6, num_updates=2500, lr=0.000312538, gnorm=0.568, loss_scale=32, train_wall=159, gb_free=20.8, wall=4231
2022-03-23 21:04:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:04:43 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.856 | ppl 231.72 | wps 64011.4 | wpb 2040.3 | bsz 4 | num_updates 2569 | best_loss 7.856
2022-03-23 21:04:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2569 updates
2022-03-23 21:04:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:04:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:04:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 25 @ 2569 updates, score 7.856) (writing took 1.086940839071758 seconds)
2022-03-23 21:04:44 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-23 21:04:44 | INFO | train | epoch 025 | loss 7.657 | ppl 201.84 | wps 38758.9 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 2569 | lr 0.000321161 | gnorm 0.562 | loss_scale 32 | train_wall 163 | gb_free 20.8 | wall 4349
KL Stats: Epoch 25 Divergences: Uniform: 3.415601276974329 Unigram: 1.807999494979819
2022-03-23 21:04:44 | INFO | fairseq.trainer | begin training epoch 26
2022-03-23 21:04:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:05:35 | INFO | train_inner | epoch 026:     31 / 103 loss=7.631, ppl=198.2, wps=38707.9, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=2600, lr=0.000325035, gnorm=0.562, loss_scale=32, train_wall=159, gb_free=20.8, wall=4400
2022-03-23 21:07:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:07:36 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.805 | ppl 223.58 | wps 63997.9 | wpb 2040.3 | bsz 4 | num_updates 2672 | best_loss 7.805
2022-03-23 21:07:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2672 updates
2022-03-23 21:07:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:07:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:07:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 26 @ 2672 updates, score 7.805) (writing took 1.0540838680462912 seconds)
2022-03-23 21:07:37 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-23 21:07:37 | INFO | train | epoch 026 | loss 7.571 | ppl 190.18 | wps 38767.2 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 2672 | lr 0.000334033 | gnorm 0.554 | loss_scale 32 | train_wall 163 | gb_free 20.8 | wall 4523
KL Stats: Epoch 26 Divergences: Uniform: 3.493002778530647 Unigram: 1.8405372235914415
2022-03-23 21:07:38 | INFO | fairseq.trainer | begin training epoch 27
2022-03-23 21:07:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:08:24 | INFO | train_inner | epoch 027:     28 / 103 loss=7.552, ppl=187.7, wps=38748.2, ups=0.59, wpb=65310.7, bsz=127.6, num_updates=2700, lr=0.000337533, gnorm=0.543, loss_scale=64, train_wall=159, gb_free=20.8, wall=4569
2022-03-23 21:09:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 21:10:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:10:30 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.76 | ppl 216.74 | wps 64361.3 | wpb 2040.3 | bsz 4 | num_updates 2774 | best_loss 7.76
2022-03-23 21:10:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2774 updates
2022-03-23 21:10:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:10:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:10:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 27 @ 2774 updates, score 7.76) (writing took 1.0669338039588183 seconds)
2022-03-23 21:10:31 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-23 21:10:31 | INFO | train | epoch 027 | loss 7.49 | ppl 179.82 | wps 38410.5 | ups 0.59 | wpb 65310.1 | bsz 127.6 | num_updates 2774 | lr 0.000346781 | gnorm 0.548 | loss_scale 32 | train_wall 163 | gb_free 20.8 | wall 4696
KL Stats: Epoch 27 Divergences: Uniform: 3.568375975947996 Unigram: 1.8732295114882687
2022-03-23 21:10:31 | INFO | fairseq.trainer | begin training epoch 28
2022-03-23 21:10:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:11:14 | INFO | train_inner | epoch 028:     26 / 103 loss=7.468, ppl=177.1, wps=38355.3, ups=0.59, wpb=65300.5, bsz=127.6, num_updates=2800, lr=0.00035003, gnorm=0.561, loss_scale=32, train_wall=160, gb_free=20.8, wall=4739
2022-03-23 21:13:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:13:24 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.715 | ppl 210.16 | wps 63750.5 | wpb 2040.3 | bsz 4 | num_updates 2877 | best_loss 7.715
2022-03-23 21:13:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2877 updates
2022-03-23 21:13:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:13:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:13:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 28 @ 2877 updates, score 7.715) (writing took 1.0991302609909326 seconds)
2022-03-23 21:13:25 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-23 21:13:25 | INFO | train | epoch 028 | loss 7.413 | ppl 170.47 | wps 38686.2 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 2877 | lr 0.000359653 | gnorm 0.549 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 4870
KL Stats: Epoch 28 Divergences: Uniform: 3.6351631932681254 Unigram: 1.9007286413402493
2022-03-23 21:13:25 | INFO | fairseq.trainer | begin training epoch 29
2022-03-23 21:13:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:14:03 | INFO | train_inner | epoch 029:     23 / 103 loss=7.393, ppl=168.13, wps=38678.9, ups=0.59, wpb=65310.7, bsz=127.6, num_updates=2900, lr=0.000362528, gnorm=0.544, loss_scale=32, train_wall=159, gb_free=20.8, wall=4908
2022-03-23 21:16:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:16:17 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.671 | ppl 203.86 | wps 64656.4 | wpb 2040.3 | bsz 4 | num_updates 2980 | best_loss 7.671
2022-03-23 21:16:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2980 updates
2022-03-23 21:16:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:16:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:16:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 29 @ 2980 updates, score 7.671) (writing took 1.0790357469813898 seconds)
2022-03-23 21:16:18 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-23 21:16:18 | INFO | train | epoch 029 | loss 7.339 | ppl 161.96 | wps 38804.4 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 2980 | lr 0.000372526 | gnorm 0.532 | loss_scale 32 | train_wall 163 | gb_free 20.8 | wall 5043
KL Stats: Epoch 29 Divergences: Uniform: 3.6986276454020617 Unigram: 1.9278115593453198
2022-03-23 21:16:18 | INFO | fairseq.trainer | begin training epoch 30
2022-03-23 21:16:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:16:51 | INFO | train_inner | epoch 030:     20 / 103 loss=7.326, ppl=160.41, wps=38748.4, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=3000, lr=0.000375025, gnorm=0.532, loss_scale=32, train_wall=159, gb_free=20.8, wall=5076
2022-03-23 21:19:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:19:11 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.649 | ppl 200.71 | wps 63884.9 | wpb 2040.3 | bsz 4 | num_updates 3083 | best_loss 7.649
2022-03-23 21:19:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 3083 updates
2022-03-23 21:19:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:19:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:19:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 30 @ 3083 updates, score 7.649) (writing took 1.1078396290540695 seconds)
2022-03-23 21:19:12 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-23 21:19:12 | INFO | train | epoch 030 | loss 7.272 | ppl 154.53 | wps 38661.9 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 3083 | lr 0.000385398 | gnorm 0.541 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 5217
KL Stats: Epoch 30 Divergences: Uniform: 3.7624112631828304 Unigram: 1.9524137105517372
2022-03-23 21:19:12 | INFO | fairseq.trainer | begin training epoch 31
2022-03-23 21:19:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:19:40 | INFO | train_inner | epoch 031:     17 / 103 loss=7.259, ppl=153.17, wps=38632.3, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=3100, lr=0.000387523, gnorm=0.538, loss_scale=32, train_wall=159, gb_free=20.8, wall=5245
2022-03-23 21:22:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:22:05 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.619 | ppl 196.54 | wps 64170.5 | wpb 2040.3 | bsz 4 | num_updates 3186 | best_loss 7.619
2022-03-23 21:22:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 3186 updates
2022-03-23 21:22:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:22:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:22:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 31 @ 3186 updates, score 7.619) (writing took 1.042807999998331 seconds)
2022-03-23 21:22:06 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-23 21:22:06 | INFO | train | epoch 031 | loss 7.205 | ppl 147.54 | wps 38741.5 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 3186 | lr 0.00039827 | gnorm 0.53 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 5391
KL Stats: Epoch 31 Divergences: Uniform: 3.8189843997485324 Unigram: 1.977818822106617
2022-03-23 21:22:06 | INFO | fairseq.trainer | begin training epoch 32
2022-03-23 21:22:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:22:29 | INFO | train_inner | epoch 032:     14 / 103 loss=7.198, ppl=146.88, wps=38709.2, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=3200, lr=0.00040002, gnorm=0.529, loss_scale=32, train_wall=159, gb_free=20.8, wall=5414
2022-03-23 21:24:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 21:24:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:24:59 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.584 | ppl 191.86 | wps 63937.3 | wpb 2040.3 | bsz 4 | num_updates 3288 | best_loss 7.584
2022-03-23 21:24:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3288 updates
2022-03-23 21:24:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:25:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:25:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 32 @ 3288 updates, score 7.584) (writing took 1.0787074590334669 seconds)
2022-03-23 21:25:00 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-23 21:25:00 | INFO | train | epoch 032 | loss 7.141 | ppl 141.1 | wps 38336.5 | ups 0.59 | wpb 65310.1 | bsz 127.6 | num_updates 3288 | lr 0.000411018 | gnorm 0.527 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 5565
KL Stats: Epoch 32 Divergences: Uniform: 3.876739768378653 Unigram: 1.9993377367070861
2022-03-23 21:25:00 | INFO | fairseq.trainer | begin training epoch 33
2022-03-23 21:25:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:25:19 | INFO | train_inner | epoch 033:     12 / 103 loss=7.134, ppl=140.51, wps=38315.5, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=3300, lr=0.000412518, gnorm=0.524, loss_scale=32, train_wall=160, gb_free=20.8, wall=5584
2022-03-23 21:27:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:27:52 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.553 | ppl 187.75 | wps 64058.1 | wpb 2040.3 | bsz 4 | num_updates 3391 | best_loss 7.553
2022-03-23 21:27:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3391 updates
2022-03-23 21:27:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:27:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:27:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 33 @ 3391 updates, score 7.553) (writing took 1.0508987880311906 seconds)
2022-03-23 21:27:53 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-23 21:27:53 | INFO | train | epoch 033 | loss 7.082 | ppl 135.47 | wps 38761.9 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 3391 | lr 0.00042389 | gnorm 0.522 | loss_scale 32 | train_wall 163 | gb_free 20.8 | wall 5738
KL Stats: Epoch 33 Divergences: Uniform: 3.929541208815075 Unigram: 2.019986841476064
2022-03-23 21:27:53 | INFO | fairseq.trainer | begin training epoch 34
2022-03-23 21:27:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:28:08 | INFO | train_inner | epoch 034:      9 / 103 loss=7.072, ppl=134.54, wps=38714.6, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=3400, lr=0.000425015, gnorm=0.521, loss_scale=32, train_wall=159, gb_free=20.8, wall=5753
2022-03-23 21:30:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:30:46 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.53 | ppl 184.81 | wps 63895 | wpb 2040.3 | bsz 4 | num_updates 3494 | best_loss 7.53
2022-03-23 21:30:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3494 updates
2022-03-23 21:30:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:30:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:30:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 34 @ 3494 updates, score 7.53) (writing took 1.1051479069283232 seconds)
2022-03-23 21:30:47 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-23 21:30:47 | INFO | train | epoch 034 | loss 7.026 | ppl 130.34 | wps 38687.5 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 3494 | lr 0.000436763 | gnorm 0.518 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 5912
KL Stats: Epoch 34 Divergences: Uniform: 3.9817923238234467 Unigram: 2.04160332411429
2022-03-23 21:30:47 | INFO | fairseq.trainer | begin training epoch 35
2022-03-23 21:30:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:30:57 | INFO | train_inner | epoch 035:      6 / 103 loss=7.026, ppl=130.32, wps=38664.8, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=3500, lr=0.000437513, gnorm=0.52, loss_scale=32, train_wall=159, gb_free=20.8, wall=5922
2022-03-23 21:33:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:33:39 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.504 | ppl 181.47 | wps 63928.3 | wpb 2040.3 | bsz 4 | num_updates 3597 | best_loss 7.504
2022-03-23 21:33:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3597 updates
2022-03-23 21:33:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:33:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:33:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 35 @ 3597 updates, score 7.504) (writing took 1.103399851010181 seconds)
2022-03-23 21:33:41 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-23 21:33:41 | INFO | train | epoch 035 | loss 6.969 | ppl 125.32 | wps 38776.1 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 3597 | lr 0.000449635 | gnorm 0.503 | loss_scale 32 | train_wall 163 | gb_free 20.8 | wall 6086
KL Stats: Epoch 35 Divergences: Uniform: 4.031847815603051 Unigram: 2.0605101942796837
2022-03-23 21:33:41 | INFO | fairseq.trainer | begin training epoch 36
2022-03-23 21:33:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:33:46 | INFO | train_inner | epoch 036:      3 / 103 loss=6.966, ppl=125.06, wps=38738.6, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=3600, lr=0.00045001, gnorm=0.503, loss_scale=32, train_wall=159, gb_free=20.8, wall=6091
2022-03-23 21:33:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 21:36:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:36:33 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.472 | ppl 177.57 | wps 64201.9 | wpb 2040.3 | bsz 4 | num_updates 3699 | best_loss 7.472
2022-03-23 21:36:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 3699 updates
2022-03-23 21:36:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:36:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:36:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 36 @ 3699 updates, score 7.472) (writing took 1.1405544680310413 seconds)
2022-03-23 21:36:34 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-23 21:36:34 | INFO | train | epoch 036 | loss 6.918 | ppl 120.92 | wps 38290.3 | ups 0.59 | wpb 65310.1 | bsz 127.6 | num_updates 3699 | lr 0.000462383 | gnorm 0.507 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 6260
KL Stats: Epoch 36 Divergences: Uniform: 4.077636209960017 Unigram: 2.0779631293052345
2022-03-23 21:36:35 | INFO | fairseq.trainer | begin training epoch 37
2022-03-23 21:36:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:36:36 | INFO | train_inner | epoch 037:      1 / 103 loss=6.919, ppl=121.05, wps=38261.5, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=3700, lr=0.000462508, gnorm=0.505, loss_scale=16, train_wall=161, gb_free=20.8, wall=6261
2022-03-23 21:39:21 | INFO | train_inner | epoch 037:    101 / 103 loss=6.869, ppl=116.88, wps=39778.4, ups=0.61, wpb=65530.9, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.515, loss_scale=16, train_wall=160, gb_free=20.8, wall=6426
2022-03-23 21:39:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:39:27 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.456 | ppl 175.6 | wps 63539.3 | wpb 2040.3 | bsz 4 | num_updates 3802 | best_loss 7.456
2022-03-23 21:39:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 3802 updates
2022-03-23 21:39:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:39:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:39:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 37 @ 3802 updates, score 7.456) (writing took 1.085016708006151 seconds)
2022-03-23 21:39:28 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-23 21:39:28 | INFO | train | epoch 037 | loss 6.868 | ppl 116.85 | wps 38665.4 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 3802 | lr 0.000475255 | gnorm 0.514 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 6434
KL Stats: Epoch 37 Divergences: Uniform: 4.124000918580224 Unigram: 2.0972265932041743
2022-03-23 21:39:28 | INFO | fairseq.trainer | begin training epoch 38
2022-03-23 21:39:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:42:10 | INFO | train_inner | epoch 038:     98 / 103 loss=6.82, ppl=113, wps=38650.2, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=3900, lr=0.000487503, gnorm=0.505, loss_scale=16, train_wall=159, gb_free=20.8, wall=6595
2022-03-23 21:42:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:42:21 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.445 | ppl 174.3 | wps 64344.2 | wpb 2040.3 | bsz 4 | num_updates 3905 | best_loss 7.445
2022-03-23 21:42:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3905 updates
2022-03-23 21:42:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:42:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:42:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 38 @ 3905 updates, score 7.445) (writing took 1.0821309440070763 seconds)
2022-03-23 21:42:22 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-23 21:42:22 | INFO | train | epoch 038 | loss 6.82 | ppl 113 | wps 38687 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 3905 | lr 0.000488127 | gnorm 0.506 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 6607
KL Stats: Epoch 38 Divergences: Uniform: 4.166337809785406 Unigram: 2.1142493335321224
2022-03-23 21:42:22 | INFO | fairseq.trainer | begin training epoch 39
2022-03-23 21:42:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:44:59 | INFO | train_inner | epoch 039:     95 / 103 loss=6.774, ppl=109.46, wps=38669.8, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=4000, lr=0.0005, gnorm=0.498, loss_scale=16, train_wall=159, gb_free=20.8, wall=6764
2022-03-23 21:45:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:45:15 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.424 | ppl 171.77 | wps 64282.1 | wpb 2040.3 | bsz 4 | num_updates 4008 | best_loss 7.424
2022-03-23 21:45:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 4008 updates
2022-03-23 21:45:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:45:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:45:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 39 @ 4008 updates, score 7.424) (writing took 1.08078521292191 seconds)
2022-03-23 21:45:16 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-23 21:45:16 | INFO | train | epoch 039 | loss 6.774 | ppl 109.41 | wps 38704.6 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 4008 | lr 0.000499501 | gnorm 0.498 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 6781
KL Stats: Epoch 39 Divergences: Uniform: 4.209320160254436 Unigram: 2.131436053476337
2022-03-23 21:45:16 | INFO | fairseq.trainer | begin training epoch 40
2022-03-23 21:45:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:47:48 | INFO | train_inner | epoch 040:     92 / 103 loss=6.728, ppl=106.03, wps=38646.8, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=4100, lr=0.000493865, gnorm=0.488, loss_scale=16, train_wall=159, gb_free=20.8, wall=6933
2022-03-23 21:48:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:48:09 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.404 | ppl 169.4 | wps 64355 | wpb 2040.3 | bsz 4 | num_updates 4111 | best_loss 7.404
2022-03-23 21:48:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 4111 updates
2022-03-23 21:48:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:48:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:48:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 40 @ 4111 updates, score 7.404) (writing took 1.0869727469980717 seconds)
2022-03-23 21:48:10 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-23 21:48:10 | INFO | train | epoch 040 | loss 6.726 | ppl 105.85 | wps 38676.2 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 4111 | lr 0.000493204 | gnorm 0.487 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 6955
KL Stats: Epoch 40 Divergences: Uniform: 4.2477549814923625 Unigram: 2.147527443674304
2022-03-23 21:48:10 | INFO | fairseq.trainer | begin training epoch 41
2022-03-23 21:48:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:50:36 | INFO | train_inner | epoch 041:     89 / 103 loss=6.687, ppl=103.04, wps=38772.7, ups=0.59, wpb=65310.7, bsz=127.6, num_updates=4200, lr=0.00048795, gnorm=0.489, loss_scale=32, train_wall=159, gb_free=20.8, wall=7101
2022-03-23 21:50:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:51:02 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.382 | ppl 166.8 | wps 63893.5 | wpb 2040.3 | bsz 4 | num_updates 4214 | best_loss 7.382
2022-03-23 21:51:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 4214 updates
2022-03-23 21:51:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:51:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:51:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 41 @ 4214 updates, score 7.382) (writing took 1.0920319739961997 seconds)
2022-03-23 21:51:03 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-23 21:51:03 | INFO | train | epoch 041 | loss 6.679 | ppl 102.5 | wps 38809.7 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 4214 | lr 0.000487139 | gnorm 0.489 | loss_scale 32 | train_wall 163 | gb_free 20.8 | wall 7128
KL Stats: Epoch 41 Divergences: Uniform: 4.291300671273358 Unigram: 2.163603606194247
2022-03-23 21:51:03 | INFO | fairseq.trainer | begin training epoch 42
2022-03-23 21:51:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:53:25 | INFO | train_inner | epoch 042:     86 / 103 loss=6.635, ppl=99.36, wps=38641.2, ups=0.59, wpb=65300.5, bsz=127.6, num_updates=4300, lr=0.000482243, gnorm=0.471, loss_scale=32, train_wall=159, gb_free=20.8, wall=7270
2022-03-23 21:53:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:53:56 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.373 | ppl 165.75 | wps 64201.7 | wpb 2040.3 | bsz 4 | num_updates 4317 | best_loss 7.373
2022-03-23 21:53:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 4317 updates
2022-03-23 21:53:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:53:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:53:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 42 @ 4317 updates, score 7.373) (writing took 1.089470588020049 seconds)
2022-03-23 21:53:57 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-23 21:53:57 | INFO | train | epoch 042 | loss 6.634 | ppl 99.31 | wps 38658.7 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 4317 | lr 0.000481292 | gnorm 0.467 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 7302
KL Stats: Epoch 42 Divergences: Uniform: 4.3314058270161535 Unigram: 2.1820197641345325
2022-03-23 21:53:57 | INFO | fairseq.trainer | begin training epoch 43
2022-03-23 21:53:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:54:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 21:56:16 | INFO | train_inner | epoch 043:     84 / 103 loss=6.598, ppl=96.87, wps=38323.6, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=4400, lr=0.000476731, gnorm=0.468, loss_scale=16, train_wall=160, gb_free=20.8, wall=7441
2022-03-23 21:56:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:56:50 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.355 | ppl 163.65 | wps 64371.7 | wpb 2040.3 | bsz 4 | num_updates 4419 | best_loss 7.355
2022-03-23 21:56:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 4419 updates
2022-03-23 21:56:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:56:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:56:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 43 @ 4419 updates, score 7.355) (writing took 1.0483218029839918 seconds)
2022-03-23 21:56:51 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-23 21:56:51 | INFO | train | epoch 043 | loss 6.59 | ppl 96.33 | wps 38378.4 | ups 0.59 | wpb 65310.1 | bsz 127.6 | num_updates 4419 | lr 0.000475705 | gnorm 0.469 | loss_scale 16 | train_wall 163 | gb_free 20.8 | wall 7476
KL Stats: Epoch 43 Divergences: Uniform: 4.371402918025385 Unigram: 2.1974496000345334
2022-03-23 21:56:51 | INFO | fairseq.trainer | begin training epoch 44
2022-03-23 21:56:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 21:59:04 | INFO | train_inner | epoch 044:     81 / 103 loss=6.558, ppl=94.24, wps=38693.2, ups=0.59, wpb=65310.7, bsz=127.6, num_updates=4500, lr=0.000471405, gnorm=0.463, loss_scale=16, train_wall=159, gb_free=20.8, wall=7609
2022-03-23 21:59:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 21:59:44 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.352 | ppl 163.34 | wps 64037.3 | wpb 2040.3 | bsz 4 | num_updates 4522 | best_loss 7.352
2022-03-23 21:59:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 4522 updates
2022-03-23 21:59:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:59:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 21:59:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 44 @ 4522 updates, score 7.352) (writing took 1.0477939139818773 seconds)
2022-03-23 21:59:45 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-23 21:59:45 | INFO | train | epoch 044 | loss 6.55 | ppl 93.72 | wps 38708.4 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 4522 | lr 0.000470256 | gnorm 0.465 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 7650
KL Stats: Epoch 44 Divergences: Uniform: 4.40939379815189 Unigram: 2.213619379209832
2022-03-23 21:59:45 | INFO | fairseq.trainer | begin training epoch 45
2022-03-23 21:59:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:01:53 | INFO | train_inner | epoch 045:     78 / 103 loss=6.519, ppl=91.68, wps=38673, ups=0.59, wpb=65300.5, bsz=127.6, num_updates=4600, lr=0.000466252, gnorm=0.466, loss_scale=16, train_wall=159, gb_free=20.8, wall=7778
2022-03-23 22:02:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:02:38 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.346 | ppl 162.73 | wps 63982.6 | wpb 2040.3 | bsz 4 | num_updates 4625 | best_loss 7.346
2022-03-23 22:02:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 4625 updates
2022-03-23 22:02:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:02:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:02:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 45 @ 4625 updates, score 7.346) (writing took 1.1230663009919226 seconds)
2022-03-23 22:02:39 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-23 22:02:39 | INFO | train | epoch 045 | loss 6.513 | ppl 91.3 | wps 38688.8 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 4625 | lr 0.000464991 | gnorm 0.463 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 7824
KL Stats: Epoch 45 Divergences: Uniform: 4.447789276167942 Unigram: 2.228583010946753
2022-03-23 22:02:39 | INFO | fairseq.trainer | begin training epoch 46
2022-03-23 22:02:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:04:42 | INFO | train_inner | epoch 046:     75 / 103 loss=6.487, ppl=89.71, wps=38652.9, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=4700, lr=0.000461266, gnorm=0.469, loss_scale=16, train_wall=159, gb_free=20.8, wall=7947
2022-03-23 22:05:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:05:31 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.33 | ppl 160.95 | wps 64621.7 | wpb 2040.3 | bsz 4 | num_updates 4728 | best_loss 7.33
2022-03-23 22:05:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 4728 updates
2022-03-23 22:05:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:05:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:05:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 46 @ 4728 updates, score 7.33) (writing took 1.112345318077132 seconds)
2022-03-23 22:05:33 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-23 22:05:33 | INFO | train | epoch 046 | loss 6.477 | ppl 89.06 | wps 38698.9 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 4728 | lr 0.000459898 | gnorm 0.47 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 7998
KL Stats: Epoch 46 Divergences: Uniform: 4.481840145592305 Unigram: 2.2428211290023485
2022-03-23 22:05:33 | INFO | fairseq.trainer | begin training epoch 47
2022-03-23 22:05:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:07:31 | INFO | train_inner | epoch 047:     72 / 103 loss=6.449, ppl=87.34, wps=38730.2, ups=0.59, wpb=65310.7, bsz=127.6, num_updates=4800, lr=0.000456435, gnorm=0.462, loss_scale=16, train_wall=159, gb_free=20.8, wall=8116
2022-03-23 22:08:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:08:25 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.317 | ppl 159.47 | wps 64346.6 | wpb 2040.3 | bsz 4 | num_updates 4831 | best_loss 7.317
2022-03-23 22:08:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4831 updates
2022-03-23 22:08:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:08:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:08:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 47 @ 4831 updates, score 7.317) (writing took 1.1051411750959232 seconds)
2022-03-23 22:08:26 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-23 22:08:26 | INFO | train | epoch 047 | loss 6.441 | ppl 86.88 | wps 38763.4 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 4831 | lr 0.000454969 | gnorm 0.459 | loss_scale 16 | train_wall 163 | gb_free 20.8 | wall 8171
KL Stats: Epoch 47 Divergences: Uniform: 4.51261837402658 Unigram: 2.2559118163896947
2022-03-23 22:08:26 | INFO | fairseq.trainer | begin training epoch 48
2022-03-23 22:08:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:10:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 22:10:21 | INFO | train_inner | epoch 048:     70 / 103 loss=6.417, ppl=85.47, wps=38351.2, ups=0.59, wpb=65300.5, bsz=127.6, num_updates=4900, lr=0.000451754, gnorm=0.461, loss_scale=16, train_wall=160, gb_free=20.8, wall=8286
2022-03-23 22:11:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:11:19 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.311 | ppl 158.83 | wps 64115.8 | wpb 2040.3 | bsz 4 | num_updates 4933 | best_loss 7.311
2022-03-23 22:11:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 4933 updates
2022-03-23 22:11:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:11:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:11:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 48 @ 4933 updates, score 7.311) (writing took 1.106652538990602 seconds)
2022-03-23 22:11:20 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-23 22:11:20 | INFO | train | epoch 048 | loss 6.408 | ppl 84.92 | wps 38387.1 | ups 0.59 | wpb 65310.1 | bsz 127.6 | num_updates 4933 | lr 0.00045024 | gnorm 0.464 | loss_scale 16 | train_wall 163 | gb_free 20.8 | wall 8345
KL Stats: Epoch 48 Divergences: Uniform: 4.54960158461374 Unigram: 2.270139114292898
2022-03-23 22:11:20 | INFO | fairseq.trainer | begin training epoch 49
2022-03-23 22:11:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:13:10 | INFO | train_inner | epoch 049:     67 / 103 loss=6.386, ppl=83.63, wps=38692, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=5000, lr=0.000447214, gnorm=0.459, loss_scale=16, train_wall=159, gb_free=20.8, wall=8455
2022-03-23 22:14:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:14:12 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.312 | ppl 158.95 | wps 64356.9 | wpb 2040.3 | bsz 4 | num_updates 5036 | best_loss 7.311
2022-03-23 22:14:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 5036 updates
2022-03-23 22:14:12 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-23 22:14:12 | INFO | train | epoch 049 | loss 6.377 | ppl 83.1 | wps 38965.2 | ups 0.6 | wpb 65312.3 | bsz 127.6 | num_updates 5036 | lr 0.000445612 | gnorm 0.458 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 8517
KL Stats: Epoch 49 Divergences: Uniform: 4.58232196830309 Unigram: 2.284868698007879
2022-03-23 22:14:12 | INFO | fairseq.trainer | begin training epoch 50
2022-03-23 22:14:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:15:57 | INFO | train_inner | epoch 050:     64 / 103 loss=6.352, ppl=81.7, wps=38983.7, ups=0.6, wpb=65310.7, bsz=127.6, num_updates=5100, lr=0.000442807, gnorm=0.454, loss_scale=16, train_wall=159, gb_free=20.8, wall=8622
2022-03-23 22:17:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:17:05 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.309 | ppl 158.6 | wps 63878.4 | wpb 2040.3 | bsz 4 | num_updates 5139 | best_loss 7.309
2022-03-23 22:17:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 5139 updates
2022-03-23 22:17:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:17:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:17:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 50 @ 5139 updates, score 7.309) (writing took 1.1490017770556733 seconds)
2022-03-23 22:17:06 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-23 22:17:06 | INFO | train | epoch 050 | loss 6.346 | ppl 81.33 | wps 38762 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 5139 | lr 0.000441124 | gnorm 0.457 | loss_scale 16 | train_wall 163 | gb_free 20.8 | wall 8691
KL Stats: Epoch 50 Divergences: Uniform: 4.61123968771864 Unigram: 2.296774554489377
2022-03-23 22:17:06 | INFO | fairseq.trainer | begin training epoch 51
2022-03-23 22:17:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:18:46 | INFO | train_inner | epoch 051:     61 / 103 loss=6.331, ppl=80.52, wps=38666, ups=0.59, wpb=65300.5, bsz=127.6, num_updates=5200, lr=0.000438529, gnorm=0.464, loss_scale=16, train_wall=159, gb_free=20.8, wall=8791
2022-03-23 22:19:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:19:59 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.302 | ppl 157.78 | wps 63999.3 | wpb 2040.3 | bsz 4 | num_updates 5242 | best_loss 7.302
2022-03-23 22:19:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 5242 updates
2022-03-23 22:19:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:20:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:20:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 51 @ 5242 updates, score 7.302) (writing took 1.1085962729994208 seconds)
2022-03-23 22:20:00 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-23 22:20:00 | INFO | train | epoch 051 | loss 6.317 | ppl 79.7 | wps 38673.5 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 5242 | lr 0.000436769 | gnorm 0.46 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 8865
KL Stats: Epoch 51 Divergences: Uniform: 4.643956042195383 Unigram: 2.31076688384168
2022-03-23 22:20:00 | INFO | fairseq.trainer | begin training epoch 52
2022-03-23 22:20:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:21:35 | INFO | train_inner | epoch 052:     58 / 103 loss=6.295, ppl=78.52, wps=38628.6, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=5300, lr=0.000434372, gnorm=0.459, loss_scale=16, train_wall=159, gb_free=20.8, wall=8960
2022-03-23 22:22:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:22:53 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.303 | ppl 157.91 | wps 64259.7 | wpb 2040.3 | bsz 4 | num_updates 5345 | best_loss 7.302
2022-03-23 22:22:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 5345 updates
2022-03-23 22:22:53 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-23 22:22:53 | INFO | train | epoch 052 | loss 6.288 | ppl 78.13 | wps 38909.8 | ups 0.6 | wpb 65312.3 | bsz 127.6 | num_updates 5345 | lr 0.00043254 | gnorm 0.461 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 9038
KL Stats: Epoch 52 Divergences: Uniform: 4.671852078635975 Unigram: 2.3233645247992194
2022-03-23 22:22:53 | INFO | fairseq.trainer | begin training epoch 53
2022-03-23 22:22:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:24:23 | INFO | train_inner | epoch 053:     55 / 103 loss=6.275, ppl=77.42, wps=38963.1, ups=0.6, wpb=65310.7, bsz=127.6, num_updates=5400, lr=0.000430331, gnorm=0.462, loss_scale=16, train_wall=159, gb_free=20.8, wall=9128
2022-03-23 22:25:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:25:45 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.297 | ppl 157.22 | wps 64314 | wpb 2040.3 | bsz 4 | num_updates 5448 | best_loss 7.297
2022-03-23 22:25:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 5448 updates
2022-03-23 22:25:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:25:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:25:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 53 @ 5448 updates, score 7.297) (writing took 1.1043444889364764 seconds)
2022-03-23 22:25:46 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-23 22:25:46 | INFO | train | epoch 053 | loss 6.262 | ppl 76.74 | wps 38792.4 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 5448 | lr 0.000428432 | gnorm 0.462 | loss_scale 32 | train_wall 163 | gb_free 20.8 | wall 9211
KL Stats: Epoch 53 Divergences: Uniform: 4.702899572085528 Unigram: 2.334964659881311
2022-03-23 22:25:46 | INFO | fairseq.trainer | begin training epoch 54
2022-03-23 22:25:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:26:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 22:27:13 | INFO | train_inner | epoch 054:     53 / 103 loss=6.249, ppl=76.07, wps=38341.6, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=5500, lr=0.000426401, gnorm=0.462, loss_scale=16, train_wall=160, gb_free=20.8, wall=9298
2022-03-23 22:28:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:28:39 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.289 | ppl 156.37 | wps 63945.3 | wpb 2040.3 | bsz 4 | num_updates 5550 | best_loss 7.289
2022-03-23 22:28:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 5550 updates
2022-03-23 22:28:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:28:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:28:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 54 @ 5550 updates, score 7.289) (writing took 1.129595853970386 seconds)
2022-03-23 22:28:40 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-23 22:28:40 | INFO | train | epoch 054 | loss 6.236 | ppl 75.38 | wps 38312.4 | ups 0.59 | wpb 65310.1 | bsz 127.6 | num_updates 5550 | lr 0.000424476 | gnorm 0.461 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 9385
KL Stats: Epoch 54 Divergences: Uniform: 4.73064008462337 Unigram: 2.3459376275868222
2022-03-23 22:28:40 | INFO | fairseq.trainer | begin training epoch 55
2022-03-23 22:28:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:30:02 | INFO | train_inner | epoch 055:     50 / 103 loss=6.223, ppl=74.71, wps=38636.9, ups=0.59, wpb=65300.5, bsz=127.6, num_updates=5600, lr=0.000422577, gnorm=0.46, loss_scale=16, train_wall=159, gb_free=20.8, wall=9467
2022-03-23 22:31:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:31:33 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.292 | ppl 156.67 | wps 64148.8 | wpb 2040.3 | bsz 4 | num_updates 5653 | best_loss 7.289
2022-03-23 22:31:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 5653 updates
2022-03-23 22:31:33 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-23 22:31:33 | INFO | train | epoch 055 | loss 6.211 | ppl 74.08 | wps 38919.2 | ups 0.6 | wpb 65312.3 | bsz 127.6 | num_updates 5653 | lr 0.000420592 | gnorm 0.464 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 9558
KL Stats: Epoch 55 Divergences: Uniform: 4.759037626714731 Unigram: 2.358201619674722
2022-03-23 22:31:33 | INFO | fairseq.trainer | begin training epoch 56
2022-03-23 22:31:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:32:50 | INFO | train_inner | epoch 056:     47 / 103 loss=6.199, ppl=73.46, wps=38919, ups=0.6, wpb=65305.6, bsz=127.6, num_updates=5700, lr=0.000418854, gnorm=0.468, loss_scale=16, train_wall=159, gb_free=20.8, wall=9635
2022-03-23 22:34:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:34:25 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.284 | ppl 155.88 | wps 63695 | wpb 2040.3 | bsz 4 | num_updates 5756 | best_loss 7.284
2022-03-23 22:34:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 5756 updates
2022-03-23 22:34:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:34:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:34:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 56 @ 5756 updates, score 7.284) (writing took 1.1128679249668494 seconds)
2022-03-23 22:34:26 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-23 22:34:26 | INFO | train | epoch 056 | loss 6.187 | ppl 72.86 | wps 38732.7 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 5756 | lr 0.000416811 | gnorm 0.461 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 9732
KL Stats: Epoch 56 Divergences: Uniform: 4.787433106849281 Unigram: 2.368600170785077
2022-03-23 22:34:27 | INFO | fairseq.trainer | begin training epoch 57
2022-03-23 22:34:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:35:39 | INFO | train_inner | epoch 057:     44 / 103 loss=6.18, ppl=72.48, wps=38697.7, ups=0.59, wpb=65310.7, bsz=127.6, num_updates=5800, lr=0.000415227, gnorm=0.457, loss_scale=16, train_wall=159, gb_free=20.8, wall=9804
2022-03-23 22:37:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:37:19 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.288 | ppl 156.26 | wps 63618.1 | wpb 2040.3 | bsz 4 | num_updates 5859 | best_loss 7.284
2022-03-23 22:37:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 5859 updates
2022-03-23 22:37:19 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-23 22:37:19 | INFO | train | epoch 057 | loss 6.164 | ppl 71.72 | wps 38989.4 | ups 0.6 | wpb 65312.3 | bsz 127.6 | num_updates 5859 | lr 0.000413131 | gnorm 0.456 | loss_scale 16 | train_wall 163 | gb_free 20.8 | wall 9904
KL Stats: Epoch 57 Divergences: Uniform: 4.813715139755917 Unigram: 2.38017253581107
2022-03-23 22:37:19 | INFO | fairseq.trainer | begin training epoch 58
2022-03-23 22:37:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:38:26 | INFO | train_inner | epoch 058:     41 / 103 loss=6.155, ppl=71.24, wps=38975.9, ups=0.6, wpb=65305.6, bsz=127.6, num_updates=5900, lr=0.000411693, gnorm=0.46, loss_scale=16, train_wall=159, gb_free=20.8, wall=9972
2022-03-23 22:40:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:40:12 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.281 | ppl 155.5 | wps 64663.1 | wpb 2040.3 | bsz 4 | num_updates 5962 | best_loss 7.281
2022-03-23 22:40:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 5962 updates
2022-03-23 22:40:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:40:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:40:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 58 @ 5962 updates, score 7.281) (writing took 1.135154118994251 seconds)
2022-03-23 22:40:13 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-23 22:40:13 | INFO | train | epoch 058 | loss 6.142 | ppl 70.63 | wps 38725.4 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 5962 | lr 0.000409547 | gnorm 0.466 | loss_scale 16 | train_wall 164 | gb_free 20.8 | wall 10078
KL Stats: Epoch 58 Divergences: Uniform: 4.841217289763629 Unigram: 2.390864241636163
2022-03-23 22:40:13 | INFO | fairseq.trainer | begin training epoch 59
2022-03-23 22:40:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:41:15 | INFO | train_inner | epoch 059:     38 / 103 loss=6.132, ppl=70.13, wps=38658.4, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=6000, lr=0.000408248, gnorm=0.463, loss_scale=32, train_wall=159, gb_free=20.8, wall=10140
2022-03-23 22:43:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:43:06 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.281 | ppl 155.56 | wps 64715.3 | wpb 2040.3 | bsz 4 | num_updates 6065 | best_loss 7.281
2022-03-23 22:43:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 6065 updates
2022-03-23 22:43:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:43:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt
2022-03-23 22:43:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_dropout_0.35_jelinek_0.01_0.09_0.9_#2/checkpoint_best.pt (epoch 59 @ 6065 updates, score 7.281) (writing took 1.1197249860269949 seconds)
2022-03-23 22:43:07 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-23 22:43:07 | INFO | train | epoch 059 | loss 6.12 | ppl 69.55 | wps 38648.1 | ups 0.59 | wpb 65312.3 | bsz 127.6 | num_updates 6065 | lr 0.000406055 | gnorm 0.462 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 10252
KL Stats: Epoch 59 Divergences: Uniform: 4.866382757764215 Unigram: 2.4006885862473224
2022-03-23 22:43:07 | INFO | fairseq.trainer | begin training epoch 60
2022-03-23 22:43:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:44:05 | INFO | train_inner | epoch 060:     35 / 103 loss=6.113, ppl=69.22, wps=38598.7, ups=0.59, wpb=65305.6, bsz=127.6, num_updates=6100, lr=0.000404888, gnorm=0.464, loss_scale=32, train_wall=159, gb_free=20.8, wall=10310
2022-03-23 22:45:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:46:00 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.289 | ppl 156.41 | wps 64352.4 | wpb 2040.3 | bsz 4 | num_updates 6168 | best_loss 7.281
2022-03-23 22:46:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 6168 updates
2022-03-23 22:46:00 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-23 22:46:00 | INFO | train | epoch 060 | loss 6.099 | ppl 68.56 | wps 38865 | ups 0.6 | wpb 65312.3 | bsz 127.6 | num_updates 6168 | lr 0.00040265 | gnorm 0.462 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 10425
KL Stats: Epoch 60 Divergences: Uniform: 4.88920642737739 Unigram: 2.411606853312541
2022-03-23 22:46:00 | INFO | fairseq.trainer | begin training epoch 61
2022-03-23 22:46:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 22:46:53 | INFO | train_inner | epoch 061:     32 / 103 loss=6.095, ppl=68.33, wps=38883.2, ups=0.6, wpb=65305.6, bsz=127.6, num_updates=6200, lr=0.00040161, gnorm=0.467, loss_scale=32, train_wall=159, gb_free=20.8, wall=10478
2022-03-23 22:48:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 22:48:53 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.282 | ppl 155.68 | wps 63615.8 | wpb 2040.3 | bsz 4 | num_updates 6271 | best_loss 7.281
2022-03-23 22:48:53 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-23 22:48:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 6271 updates
2022-03-23 22:48:53 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-23 22:48:53 | INFO | train | epoch 061 | loss 6.079 | ppl 67.59 | wps 38952.5 | ups 0.6 | wpb 65312.3 | bsz 127.6 | num_updates 6271 | lr 0.00039933 | gnorm 0.467 | loss_scale 32 | train_wall 164 | gb_free 20.8 | wall 10598
2022-03-23 22:48:53 | INFO | fairseq_cli.train | done training in 10597.6 seconds
