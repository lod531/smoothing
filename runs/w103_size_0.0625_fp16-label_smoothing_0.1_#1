Sender: LSF System <lsfadmin@eu-g3-074>
Subject: Job 207133294: <w103_size_0.0625_fp16_label_smoothing_0.1_#1> in cluster <euler> Exited

Job <w103_size_0.0625_fp16_label_smoothing_0.1_#1> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Fri Mar  4 09:38:47 2022
Job was executed on host(s) <eu-g3-074>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Fri Mar  4 13:22:57 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Mar  4 13:22:57 2022
Terminated at Sat Mar  5 11:25:22 2022
Results reported at Sat Mar  5 11:25:22 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   79264.28 sec.
    Max Memory :                                 8317 MB
    Average Memory :                             4298.87 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11683.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   79344 sec.
    Turnaround time :                            92795 sec.

The output (if any) follows:

2022-03-04 13:23:05 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-04 13:23:05 | INFO | fairseq.tasks.language_modeling | dictionary: 138136 types
2022-03-04 13:23:07 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(138136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=138136, bias=False)
  )
)
2022-03-04 13:23:07 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-04 13:23:07 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-04 13:23:07 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-04 13:23:07 | INFO | fairseq_cli.train | num. shared model params: 89,639,936 (num. trained: 89,639,936)
2022-03-04 13:23:07 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-04 13:23:07 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.0625/valid
2022-03-04 13:23:13 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-04 13:23:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 13:23:13 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-04 13:23:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-04 13:23:13 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-04 13:23:13 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-04 13:23:13 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 13:23:13 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 13:23:13 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-04 13:23:13 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-raw-size-0.0625/train
2022-03-04 13:23:13 | INFO | fairseq.trainer | begin training epoch 1
2022-03-04 13:23:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:23:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-04 13:23:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:23:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:23:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 13:27:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:27:42 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.799 | nll_loss 14.462 | ppl 22565.8 | wps 45695.2 | wpb 510.9 | bsz 1 | num_updates 93
2022-03-04 13:27:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 93 updates
2022-03-04 13:27:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:27:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:27:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 1 @ 93 updates, score 14.799) (writing took 5.591705154627562 seconds)
2022-03-04 13:27:48 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-04 13:27:48 | INFO | train | epoch 001 | loss 16.115 | nll_loss 15.922 | ppl 62087.6 | wps 24569.1 | ups 0.38 | wpb 65489.7 | bsz 127.9 | num_updates 93 | lr 1.17227e-05 | gnorm 3.164 | loss_scale 8 | train_wall 242 | gb_free 21 | wall 275
2022-03-04 13:27:48 | INFO | fairseq.trainer | begin training epoch 2
2022-03-04 13:27:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:28:06 | INFO | train_inner | epoch 002:      7 / 97 loss=16.024, nll_loss=15.822, ppl=57912.4, wps=24628.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=100, lr=1.25975e-05, gnorm=3.052, loss_scale=8, train_wall=259, gb_free=21, wall=293
2022-03-04 13:29:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 13:31:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:32:00 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.326 | nll_loss 12.825 | ppl 7258.27 | wps 45314.5 | wpb 510.9 | bsz 1 | num_updates 189 | best_loss 13.326
2022-03-04 13:32:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 189 updates
2022-03-04 13:32:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:32:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:32:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 2 @ 189 updates, score 13.326) (writing took 5.7775909677147865 seconds)
2022-03-04 13:32:06 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-04 13:32:06 | INFO | train | epoch 002 | loss 14.187 | nll_loss 13.787 | ppl 14139.4 | wps 24324.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 189 | lr 2.37203e-05 | gnorm 1.43 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 533
2022-03-04 13:32:06 | INFO | fairseq.trainer | begin training epoch 3
2022-03-04 13:32:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:32:34 | INFO | train_inner | epoch 003:     11 / 97 loss=14.044, nll_loss=13.628, ppl=12663.5, wps=24382, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=200, lr=2.5095e-05, gnorm=1.395, loss_scale=8, train_wall=235, gb_free=21, wall=561
2022-03-04 13:36:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:36:18 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.841 | nll_loss 11.127 | ppl 2236.3 | wps 45946.8 | wpb 510.9 | bsz 1 | num_updates 286 | best_loss 11.841
2022-03-04 13:36:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 286 updates
2022-03-04 13:36:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:36:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:36:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 3 @ 286 updates, score 11.841) (writing took 5.806166793219745 seconds)
2022-03-04 13:36:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-04 13:36:24 | INFO | train | epoch 003 | loss 12.568 | nll_loss 11.969 | ppl 4008.61 | wps 24615.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 286 | lr 3.58429e-05 | gnorm 0.997 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 791
2022-03-04 13:36:24 | INFO | fairseq.trainer | begin training epoch 4
2022-03-04 13:36:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:37:00 | INFO | train_inner | epoch 004:     14 / 97 loss=12.376, nll_loss=11.749, ppl=3441.94, wps=24650.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=300, lr=3.75925e-05, gnorm=0.93, loss_scale=16, train_wall=233, gb_free=21, wall=827
2022-03-04 13:40:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:40:37 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.167 | nll_loss 10.292 | ppl 1253.97 | wps 45146.5 | wpb 510.9 | bsz 1 | num_updates 383 | best_loss 11.167
2022-03-04 13:40:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 383 updates
2022-03-04 13:40:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:40:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:40:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 4 @ 383 updates, score 11.167) (writing took 5.87783432379365 seconds)
2022-03-04 13:40:43 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-04 13:40:43 | INFO | train | epoch 004 | loss 11.437 | nll_loss 10.64 | ppl 1595.83 | wps 24589 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 383 | lr 4.79654e-05 | gnorm 0.522 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 1050
2022-03-04 13:40:43 | INFO | fairseq.trainer | begin training epoch 5
2022-03-04 13:40:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:41:26 | INFO | train_inner | epoch 005:     17 / 97 loss=11.337, nll_loss=10.516, ppl=1464.07, wps=24617.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=400, lr=5.009e-05, gnorm=0.487, loss_scale=32, train_wall=233, gb_free=21, wall=1093
2022-03-04 13:44:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:44:55 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.85 | nll_loss 9.898 | ppl 954.36 | wps 45328.1 | wpb 510.9 | bsz 1 | num_updates 480 | best_loss 10.85
2022-03-04 13:44:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 480 updates
2022-03-04 13:44:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:44:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:45:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 5 @ 480 updates, score 10.85) (writing took 5.9785857470706105 seconds)
2022-03-04 13:45:01 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-04 13:45:01 | INFO | train | epoch 005 | loss 10.993 | nll_loss 10.08 | ppl 1082.03 | wps 24577.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 480 | lr 6.0088e-05 | gnorm 0.447 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 1308
2022-03-04 13:45:01 | INFO | fairseq.trainer | begin training epoch 6
2022-03-04 13:45:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:45:52 | INFO | train_inner | epoch 006:     20 / 97 loss=10.935, nll_loss=10.008, ppl=1029.96, wps=24608, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=500, lr=6.25875e-05, gnorm=0.457, loss_scale=32, train_wall=233, gb_free=21, wall=1359
2022-03-04 13:47:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:49:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:49:14 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.578 | nll_loss 9.583 | ppl 767.11 | wps 45573.5 | wpb 510.9 | bsz 1 | num_updates 576 | best_loss 10.578
2022-03-04 13:49:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 576 updates
2022-03-04 13:49:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:49:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:49:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 6 @ 576 updates, score 10.578) (writing took 5.782854715362191 seconds)
2022-03-04 13:49:20 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-04 13:49:20 | INFO | train | epoch 006 | loss 10.705 | nll_loss 9.735 | ppl 852.45 | wps 24325.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 576 | lr 7.20856e-05 | gnorm 0.507 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 1567
2022-03-04 13:49:20 | INFO | fairseq.trainer | begin training epoch 7
2022-03-04 13:49:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:50:21 | INFO | train_inner | epoch 007:     24 / 97 loss=10.641, nll_loss=9.661, ppl=809.55, wps=24368.1, ups=0.37, wpb=65495, bsz=127.9, num_updates=600, lr=7.5085e-05, gnorm=0.517, loss_scale=32, train_wall=236, gb_free=21, wall=1628
2022-03-04 13:53:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 13:53:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:53:32 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.354 | nll_loss 9.317 | ppl 637.87 | wps 45512.7 | wpb 510.9 | bsz 1 | num_updates 672 | best_loss 10.354
2022-03-04 13:53:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 672 updates
2022-03-04 13:53:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:53:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:53:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 7 @ 672 updates, score 10.354) (writing took 5.693472404032946 seconds)
2022-03-04 13:53:38 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 13:53:38 | INFO | train | epoch 007 | loss 10.45 | nll_loss 9.442 | ppl 695.6 | wps 24322.3 | ups 0.37 | wpb 65493.3 | bsz 127.9 | num_updates 672 | lr 8.40832e-05 | gnorm 0.578 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 1825
2022-03-04 13:53:38 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 13:53:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:54:50 | INFO | train_inner | epoch 008:     28 / 97 loss=10.388, nll_loss=9.371, ppl=662.06, wps=24369.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=700, lr=8.75825e-05, gnorm=0.611, loss_scale=32, train_wall=236, gb_free=21, wall=1897
2022-03-04 13:57:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:57:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:57:51 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.167 | nll_loss 9.103 | ppl 549.93 | wps 45366.3 | wpb 510.9 | bsz 1 | num_updates 768 | best_loss 10.167
2022-03-04 13:57:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 768 updates
2022-03-04 13:57:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:57:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 13:57:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 8 @ 768 updates, score 10.167) (writing took 5.591832623817027 seconds)
2022-03-04 13:57:56 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 13:57:56 | INFO | train | epoch 008 | loss 10.226 | nll_loss 9.184 | ppl 581.8 | wps 24333.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 768 | lr 9.60808e-05 | gnorm 0.714 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 2083
2022-03-04 13:57:56 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 13:57:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:59:18 | INFO | train_inner | epoch 009:     32 / 97 loss=10.16, nll_loss=9.109, ppl=552.17, wps=24383.6, ups=0.37, wpb=65495, bsz=127.9, num_updates=800, lr=0.00010008, gnorm=0.72, loss_scale=16, train_wall=236, gb_free=21, wall=2165
2022-03-04 14:02:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:02:09 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.971 | nll_loss 8.882 | ppl 471.93 | wps 45373.2 | wpb 510.9 | bsz 1 | num_updates 865 | best_loss 9.971
2022-03-04 14:02:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 865 updates
2022-03-04 14:02:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:02:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:02:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 9 @ 865 updates, score 9.971) (writing took 5.492248108610511 seconds)
2022-03-04 14:02:15 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 14:02:15 | INFO | train | epoch 009 | loss 10.016 | nll_loss 8.943 | ppl 492.09 | wps 24580.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 865 | lr 0.000108203 | gnorm 0.72 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 2342
2022-03-04 14:02:15 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 14:02:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:03:44 | INFO | train_inner | epoch 010:     35 / 97 loss=9.943, nll_loss=8.859, ppl=464.35, wps=24618.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=900, lr=0.000112578, gnorm=0.739, loss_scale=32, train_wall=233, gb_free=21, wall=2431
2022-03-04 14:06:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:06:28 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 9.81 | nll_loss 8.696 | ppl 414.64 | wps 45187.6 | wpb 510.9 | bsz 1 | num_updates 962 | best_loss 9.81
2022-03-04 14:06:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 962 updates
2022-03-04 14:06:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:06:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:06:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 10 @ 962 updates, score 9.81) (writing took 5.2956386329606175 seconds)
2022-03-04 14:06:33 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 14:06:33 | INFO | train | epoch 010 | loss 9.817 | nll_loss 8.714 | ppl 419.8 | wps 24624.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 962 | lr 0.000120326 | gnorm 0.778 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 2600
2022-03-04 14:06:33 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 14:06:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:08:10 | INFO | train_inner | epoch 011:     38 / 97 loss=9.744, nll_loss=8.629, ppl=395.8, wps=24645.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=1000, lr=0.000125075, gnorm=0.81, loss_scale=32, train_wall=233, gb_free=21, wall=2697
2022-03-04 14:09:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:10:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:10:46 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 9.659 | nll_loss 8.527 | ppl 368.97 | wps 44992.9 | wpb 510.9 | bsz 1 | num_updates 1058 | best_loss 9.659
2022-03-04 14:10:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1058 updates
2022-03-04 14:10:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:10:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:10:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 11 @ 1058 updates, score 9.659) (writing took 5.40741453319788 seconds)
2022-03-04 14:10:51 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 14:10:51 | INFO | train | epoch 011 | loss 9.638 | nll_loss 8.506 | ppl 363.58 | wps 24324.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1058 | lr 0.000132324 | gnorm 0.812 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 2858
2022-03-04 14:10:51 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 14:10:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:12:39 | INFO | train_inner | epoch 012:     42 / 97 loss=9.569, nll_loss=8.426, ppl=343.95, wps=24373, ups=0.37, wpb=65495, bsz=127.9, num_updates=1100, lr=0.000137573, gnorm=0.794, loss_scale=32, train_wall=236, gb_free=21, wall=2966
2022-03-04 14:14:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:15:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:15:04 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 9.531 | nll_loss 8.37 | ppl 330.78 | wps 45165.2 | wpb 510.9 | bsz 1 | num_updates 1154 | best_loss 9.531
2022-03-04 14:15:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1154 updates
2022-03-04 14:15:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:15:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:15:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 12 @ 1154 updates, score 9.531) (writing took 5.205922682769597 seconds)
2022-03-04 14:15:10 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 14:15:10 | INFO | train | epoch 012 | loss 9.476 | nll_loss 8.319 | ppl 319.29 | wps 24346.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1154 | lr 0.000144321 | gnorm 0.82 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 3117
2022-03-04 14:15:10 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 14:15:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:17:07 | INFO | train_inner | epoch 013:     46 / 97 loss=9.405, nll_loss=8.236, ppl=301.52, wps=24384.1, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=1200, lr=0.00015007, gnorm=0.847, loss_scale=32, train_wall=236, gb_free=21, wall=3234
2022-03-04 14:17:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:19:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:19:23 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 9.417 | nll_loss 8.235 | ppl 301.35 | wps 45026.2 | wpb 510.9 | bsz 1 | num_updates 1250 | best_loss 9.417
2022-03-04 14:19:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1250 updates
2022-03-04 14:19:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:19:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:19:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 13 @ 1250 updates, score 9.417) (writing took 5.236822906881571 seconds)
2022-03-04 14:19:28 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 14:19:28 | INFO | train | epoch 013 | loss 9.326 | nll_loss 8.144 | ppl 282.97 | wps 24340.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1250 | lr 0.000156319 | gnorm 0.849 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 3375
2022-03-04 14:19:28 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 14:19:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:21:36 | INFO | train_inner | epoch 014:     50 / 97 loss=9.251, nll_loss=8.058, ppl=266.57, wps=24384.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1300, lr=0.000162568, gnorm=0.857, loss_scale=16, train_wall=236, gb_free=21, wall=3503
2022-03-04 14:23:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:23:41 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 9.324 | nll_loss 8.12 | ppl 278.11 | wps 44805.3 | wpb 510.9 | bsz 1 | num_updates 1347 | best_loss 9.324
2022-03-04 14:23:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1347 updates
2022-03-04 14:23:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:23:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:23:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 14 @ 1347 updates, score 9.324) (writing took 5.30492121912539 seconds)
2022-03-04 14:23:46 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 14:23:46 | INFO | train | epoch 014 | loss 9.183 | nll_loss 7.979 | ppl 252.37 | wps 24570.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 1347 | lr 0.000168441 | gnorm 0.852 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 3633
2022-03-04 14:23:46 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 14:23:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:26:02 | INFO | train_inner | epoch 015:     53 / 97 loss=9.113, nll_loss=7.898, ppl=238.47, wps=24616.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=1400, lr=0.000175065, gnorm=0.873, loss_scale=32, train_wall=233, gb_free=21, wall=3769
2022-03-04 14:27:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:27:59 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 9.229 | nll_loss 8.019 | ppl 259.4 | wps 45299.5 | wpb 510.9 | bsz 1 | num_updates 1444 | best_loss 9.229
2022-03-04 14:27:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1444 updates
2022-03-04 14:27:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:28:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:28:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 15 @ 1444 updates, score 9.229) (writing took 5.453296173363924 seconds)
2022-03-04 14:28:05 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 14:28:05 | INFO | train | epoch 015 | loss 9.043 | nll_loss 7.817 | ppl 225.48 | wps 24583.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 1444 | lr 0.000180564 | gnorm 0.88 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 3892
2022-03-04 14:28:05 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 14:28:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:29:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:30:31 | INFO | train_inner | epoch 016:     57 / 97 loss=8.958, nll_loss=7.719, ppl=210.7, wps=24391.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1500, lr=0.000187563, gnorm=0.872, loss_scale=32, train_wall=236, gb_free=21, wall=4038
2022-03-04 14:32:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:32:18 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.136 | nll_loss 7.889 | ppl 237.04 | wps 45452.9 | wpb 510.9 | bsz 1 | num_updates 1540 | best_loss 9.136
2022-03-04 14:32:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1540 updates
2022-03-04 14:32:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:32:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:32:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 16 @ 1540 updates, score 9.136) (writing took 5.363729055970907 seconds)
2022-03-04 14:32:23 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 14:32:23 | INFO | train | epoch 016 | loss 8.907 | nll_loss 7.659 | ppl 202.15 | wps 24365.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1540 | lr 0.000192562 | gnorm 0.89 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 4150
2022-03-04 14:32:23 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 14:32:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:34:56 | INFO | train_inner | epoch 017:     60 / 97 loss=8.834, nll_loss=7.575, ppl=190.71, wps=24650.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=1600, lr=0.00020006, gnorm=0.883, loss_scale=32, train_wall=233, gb_free=21, wall=4303
2022-03-04 14:35:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:36:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:36:36 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 9.052 | nll_loss 7.792 | ppl 221.63 | wps 45466.8 | wpb 510.9 | bsz 1 | num_updates 1636 | best_loss 9.052
2022-03-04 14:36:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1636 updates
2022-03-04 14:36:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:36:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:36:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 17 @ 1636 updates, score 9.052) (writing took 5.389914450235665 seconds)
2022-03-04 14:36:41 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 14:36:41 | INFO | train | epoch 017 | loss 8.772 | nll_loss 7.503 | ppl 181.44 | wps 24363.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1636 | lr 0.000204559 | gnorm 0.882 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 4408
2022-03-04 14:36:41 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 14:36:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:39:25 | INFO | train_inner | epoch 018:     64 / 97 loss=8.682, nll_loss=7.399, ppl=168.76, wps=24399.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1700, lr=0.000212558, gnorm=0.891, loss_scale=32, train_wall=236, gb_free=21, wall=4572
2022-03-04 14:40:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:40:54 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.993 | nll_loss 7.71 | ppl 209.41 | wps 45145.9 | wpb 510.9 | bsz 1 | num_updates 1733 | best_loss 8.993
2022-03-04 14:40:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1733 updates
2022-03-04 14:40:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:40:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:40:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 18 @ 1733 updates, score 8.993) (writing took 5.3247636603191495 seconds)
2022-03-04 14:40:59 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 14:40:59 | INFO | train | epoch 018 | loss 8.639 | nll_loss 7.349 | ppl 163.01 | wps 24602 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 1733 | lr 0.000216682 | gnorm 0.88 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 4666
2022-03-04 14:40:59 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 14:40:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:41:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:43:53 | INFO | train_inner | epoch 019:     68 / 97 loss=8.552, nll_loss=7.247, ppl=151.95, wps=24373, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1800, lr=0.000225055, gnorm=0.889, loss_scale=32, train_wall=236, gb_free=21, wall=4840
2022-03-04 14:45:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:45:12 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.923 | nll_loss 7.631 | ppl 198.29 | wps 45721.4 | wpb 510.9 | bsz 1 | num_updates 1829 | best_loss 8.923
2022-03-04 14:45:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1829 updates
2022-03-04 14:45:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:45:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:45:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 19 @ 1829 updates, score 8.923) (writing took 5.34131401963532 seconds)
2022-03-04 14:45:18 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 14:45:18 | INFO | train | epoch 019 | loss 8.509 | nll_loss 7.198 | ppl 146.81 | wps 24316.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1829 | lr 0.000228679 | gnorm 0.9 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 4925
2022-03-04 14:45:18 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 14:45:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:47:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:48:22 | INFO | train_inner | epoch 020:     72 / 97 loss=8.417, nll_loss=7.091, ppl=136.32, wps=24373.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=1900, lr=0.000237553, gnorm=0.895, loss_scale=32, train_wall=236, gb_free=21, wall=5109
2022-03-04 14:49:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:49:31 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.87 | nll_loss 7.56 | ppl 188.77 | wps 45225.8 | wpb 510.9 | bsz 1 | num_updates 1925 | best_loss 8.87
2022-03-04 14:49:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1925 updates
2022-03-04 14:49:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:49:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:49:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 20 @ 1925 updates, score 8.87) (writing took 5.284714016132057 seconds)
2022-03-04 14:49:36 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 14:49:36 | INFO | train | epoch 020 | loss 8.382 | nll_loss 7.05 | ppl 132.55 | wps 24333.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 1925 | lr 0.000240677 | gnorm 0.882 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 5183
2022-03-04 14:49:36 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 14:49:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:52:48 | INFO | train_inner | epoch 021:     75 / 97 loss=8.288, nll_loss=6.941, ppl=122.84, wps=24614.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=2000, lr=0.00025005, gnorm=0.875, loss_scale=32, train_wall=233, gb_free=21, wall=5375
2022-03-04 14:53:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 14:53:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:53:49 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.819 | nll_loss 7.505 | ppl 181.7 | wps 45618.1 | wpb 510.9 | bsz 1 | num_updates 2021 | best_loss 8.819
2022-03-04 14:53:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2021 updates
2022-03-04 14:53:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:53:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:53:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 21 @ 2021 updates, score 8.819) (writing took 5.284041745588183 seconds)
2022-03-04 14:53:55 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 14:53:55 | INFO | train | epoch 021 | loss 8.261 | nll_loss 6.91 | ppl 120.25 | wps 24331.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2021 | lr 0.000252674 | gnorm 0.895 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 5441
2022-03-04 14:53:55 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 14:53:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:57:17 | INFO | train_inner | epoch 022:     79 / 97 loss=8.168, nll_loss=6.8, ppl=111.47, wps=24379, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=2100, lr=0.000262548, gnorm=0.903, loss_scale=32, train_wall=236, gb_free=21, wall=5644
2022-03-04 14:58:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:58:08 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.792 | nll_loss 7.486 | ppl 179.29 | wps 45330.1 | wpb 510.9 | bsz 1 | num_updates 2118 | best_loss 8.792
2022-03-04 14:58:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2118 updates
2022-03-04 14:58:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:58:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 14:58:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 22 @ 2118 updates, score 8.792) (writing took 5.2873883079737425 seconds)
2022-03-04 14:58:13 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 14:58:13 | INFO | train | epoch 022 | loss 8.143 | nll_loss 6.772 | ppl 109.28 | wps 24577.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 2118 | lr 0.000264797 | gnorm 0.897 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 5700
2022-03-04 14:58:13 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 14:58:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:00:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:01:46 | INFO | train_inner | epoch 023:     83 / 97 loss=8.05, nll_loss=6.663, ppl=101.35, wps=24360.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2200, lr=0.000275045, gnorm=0.874, loss_scale=32, train_wall=236, gb_free=21, wall=5913
2022-03-04 15:02:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:02:26 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.774 | nll_loss 7.437 | ppl 173.3 | wps 44902.5 | wpb 510.9 | bsz 1 | num_updates 2214 | best_loss 8.774
2022-03-04 15:02:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2214 updates
2022-03-04 15:02:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 15:02:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 15:02:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 23 @ 2214 updates, score 8.774) (writing took 5.328300595283508 seconds)
2022-03-04 15:02:32 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 15:02:32 | INFO | train | epoch 023 | loss 8.027 | nll_loss 6.637 | ppl 99.55 | wps 24307.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 2214 | lr 0.000276795 | gnorm 0.885 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 5959
2022-03-04 15:02:32 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 15:02:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:06:12 | INFO | train_inner | epoch 024:     86 / 97 loss=7.932, nll_loss=6.526, ppl=92.15, wps=24596.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2300, lr=0.000287543, gnorm=0.907, loss_scale=64, train_wall=234, gb_free=21, wall=6179
2022-03-04 15:06:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:06:45 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.737 | nll_loss 7.394 | ppl 168.22 | wps 45469.6 | wpb 510.9 | bsz 1 | num_updates 2311 | best_loss 8.737
2022-03-04 15:06:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2311 updates
2022-03-04 15:06:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 15:06:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-03-04 15:06:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 24 @ 2311 updates, score 8.737) (writing took 5.242763128131628 seconds)
2022-03-04 15:06:50 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 15:06:50 | INFO | train | epoch 024 | loss 7.918 | nll_loss 6.51 | ppl 91.13 | wps 24574.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 2311 | lr 0.000288917 | gnorm 0.891 | loss_scale 64 | train_wall 227 | gb_free 21 | wall 6217
2022-03-04 15:06:50 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 15:06:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:06:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:10:40 | INFO | train_inner | epoch 025:     90 / 97 loss=7.819, nll_loss=6.394, ppl=84.1, wps=24396.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=2400, lr=0.00030004, gnorm=0.901, loss_scale=32, train_wall=236, gb_free=21, wall=6447
2022-03-04 15:10:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:11:03 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.749 | nll_loss 7.413 | ppl 170.41 | wps 45267.9 | wpb 510.9 | bsz 1 | num_updates 2407 | best_loss 8.737
2022-03-04 15:11:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2407 updates
2022-03-04 15:11:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:11:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:11:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 25 @ 2407 updates, score 8.749) (writing took 2.3057594364508986 seconds)
2022-03-04 15:11:05 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 15:11:05 | INFO | train | epoch 025 | loss 7.809 | nll_loss 6.382 | ppl 83.41 | wps 24634.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 2407 | lr 0.000300915 | gnorm 0.92 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 6472
2022-03-04 15:11:05 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 15:11:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:13:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:15:06 | INFO | train_inner | epoch 026:     94 / 97 loss=7.712, nll_loss=6.269, ppl=77.14, wps=24671, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2500, lr=0.000312538, gnorm=0.875, loss_scale=32, train_wall=236, gb_free=21, wall=6713
2022-03-04 15:15:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:15:18 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.769 | nll_loss 7.436 | ppl 173.12 | wps 45395.7 | wpb 510.9 | bsz 1 | num_updates 2503 | best_loss 8.737
2022-03-04 15:15:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2503 updates
2022-03-04 15:15:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:15:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:15:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 26 @ 2503 updates, score 8.769) (writing took 2.581101605668664 seconds)
2022-03-04 15:15:21 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 15:15:21 | INFO | train | epoch 026 | loss 7.703 | nll_loss 6.259 | ppl 76.59 | wps 24607.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 2503 | lr 0.000312912 | gnorm 0.863 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 6728
2022-03-04 15:15:21 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 15:15:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:19:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:19:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:19:33 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.792 | nll_loss 7.467 | ppl 176.96 | wps 45562.7 | wpb 510.9 | bsz 1 | num_updates 2599 | best_loss 8.737
2022-03-04 15:19:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2599 updates
2022-03-04 15:19:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:19:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:19:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 27 @ 2599 updates, score 8.792) (writing took 2.5751548623666167 seconds)
2022-03-04 15:19:36 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 15:19:36 | INFO | train | epoch 027 | loss 7.604 | nll_loss 6.143 | ppl 70.68 | wps 24644.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 2599 | lr 0.00032491 | gnorm 0.914 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 6983
2022-03-04 15:19:36 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 15:19:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:19:39 | INFO | train_inner | epoch 028:      1 / 97 loss=7.607, nll_loss=6.146, ppl=70.84, wps=23998.7, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=2600, lr=0.000325035, gnorm=0.915, loss_scale=32, train_wall=235, gb_free=21, wall=6986
2022-03-04 15:23:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:23:49 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.781 | nll_loss 7.445 | ppl 174.31 | wps 45436.9 | wpb 510.9 | bsz 1 | num_updates 2696 | best_loss 8.737
2022-03-04 15:23:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2696 updates
2022-03-04 15:23:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:23:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:23:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 28 @ 2696 updates, score 8.781) (writing took 2.5415061665698886 seconds)
2022-03-04 15:23:51 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 15:23:51 | INFO | train | epoch 028 | loss 7.502 | nll_loss 6.023 | ppl 65.03 | wps 24894.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 2696 | lr 0.000337033 | gnorm 0.898 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 7238
2022-03-04 15:23:51 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 15:23:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:24:01 | INFO | train_inner | epoch 029:      4 / 97 loss=7.497, nll_loss=6.017, ppl=64.77, wps=24917.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2700, lr=0.000337533, gnorm=0.897, loss_scale=32, train_wall=233, gb_free=21, wall=7248
2022-03-04 15:25:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:27:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:28:04 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.804 | nll_loss 7.456 | ppl 175.64 | wps 45603.1 | wpb 510.9 | bsz 1 | num_updates 2792 | best_loss 8.737
2022-03-04 15:28:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2792 updates
2022-03-04 15:28:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:28:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:28:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 29 @ 2792 updates, score 8.804) (writing took 2.633416519500315 seconds)
2022-03-04 15:28:06 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 15:28:06 | INFO | train | epoch 029 | loss 7.4 | nll_loss 5.905 | ppl 59.92 | wps 24633.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 2792 | lr 0.00034903 | gnorm 0.897 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 7493
2022-03-04 15:28:06 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 15:28:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:28:27 | INFO | train_inner | epoch 030:      8 / 97 loss=7.391, nll_loss=5.894, ppl=59.47, wps=24673.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2800, lr=0.00035003, gnorm=0.897, loss_scale=32, train_wall=235, gb_free=21, wall=7514
2022-03-04 15:31:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:32:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:32:19 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.838 | nll_loss 7.484 | ppl 179.08 | wps 45492.1 | wpb 510.9 | bsz 1 | num_updates 2888 | best_loss 8.737
2022-03-04 15:32:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2888 updates
2022-03-04 15:32:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:32:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:32:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 30 @ 2888 updates, score 8.838) (writing took 2.5639938917011023 seconds)
2022-03-04 15:32:21 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 15:32:21 | INFO | train | epoch 030 | loss 7.307 | nll_loss 5.796 | ppl 55.54 | wps 24646.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 2888 | lr 0.000361028 | gnorm 0.914 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 7748
2022-03-04 15:32:22 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 15:32:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:32:52 | INFO | train_inner | epoch 031:     12 / 97 loss=7.294, nll_loss=5.78, ppl=54.94, wps=24685.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=2900, lr=0.000362528, gnorm=0.925, loss_scale=32, train_wall=235, gb_free=21, wall=7779
2022-03-04 15:36:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:36:34 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.856 | nll_loss 7.522 | ppl 183.76 | wps 45667.7 | wpb 510.9 | bsz 1 | num_updates 2985 | best_loss 8.737
2022-03-04 15:36:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2985 updates
2022-03-04 15:36:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:36:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:36:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 31 @ 2985 updates, score 8.856) (writing took 4.070840020664036 seconds)
2022-03-04 15:36:38 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 15:36:38 | INFO | train | epoch 031 | loss 7.212 | nll_loss 5.684 | ppl 51.4 | wps 24741.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 2985 | lr 0.00037315 | gnorm 0.926 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 8005
2022-03-04 15:36:38 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 15:36:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:37:17 | INFO | train_inner | epoch 032:     15 / 97 loss=7.198, nll_loss=5.667, ppl=50.81, wps=24767.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3000, lr=0.000375025, gnorm=0.937, loss_scale=32, train_wall=233, gb_free=21, wall=8044
2022-03-04 15:37:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-04 15:39:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:40:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:40:51 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.904 | nll_loss 7.544 | ppl 186.69 | wps 45566.8 | wpb 510.9 | bsz 1 | num_updates 3080 | best_loss 8.737
2022-03-04 15:40:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3080 updates
2022-03-04 15:40:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:40:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:40:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 32 @ 3080 updates, score 8.904) (writing took 2.6073976568877697 seconds)
2022-03-04 15:40:53 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 15:40:53 | INFO | train | epoch 032 | loss 7.123 | nll_loss 5.579 | ppl 47.81 | wps 24389 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 3080 | lr 0.000385023 | gnorm 0.954 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 8260
2022-03-04 15:40:53 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 15:40:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:41:45 | INFO | train_inner | epoch 033:     20 / 97 loss=7.104, nll_loss=5.557, ppl=47.07, wps=24448.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=3100, lr=0.000387523, gnorm=0.946, loss_scale=16, train_wall=238, gb_free=21, wall=8311
2022-03-04 15:45:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:45:06 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.914 | nll_loss 7.563 | ppl 189.11 | wps 45395 | wpb 510.9 | bsz 1 | num_updates 3177 | best_loss 8.737
2022-03-04 15:45:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3177 updates
2022-03-04 15:45:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:45:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:45:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 33 @ 3177 updates, score 8.914) (writing took 2.5716664949432015 seconds)
2022-03-04 15:45:08 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 15:45:08 | INFO | train | epoch 033 | loss 7.032 | nll_loss 5.472 | ppl 44.38 | wps 24899.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3177 | lr 0.000397146 | gnorm 0.933 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 8515
2022-03-04 15:45:08 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 15:45:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:45:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:46:10 | INFO | train_inner | epoch 034:     24 / 97 loss=7.011, nll_loss=5.448, ppl=43.64, wps=24682.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3200, lr=0.00040002, gnorm=0.979, loss_scale=16, train_wall=235, gb_free=21, wall=8577
2022-03-04 15:49:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:49:21 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 9.017 | nll_loss 7.678 | ppl 204.81 | wps 45442.9 | wpb 510.9 | bsz 1 | num_updates 3273 | best_loss 8.737
2022-03-04 15:49:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3273 updates
2022-03-04 15:49:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:49:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:49:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 34 @ 3273 updates, score 9.017) (writing took 2.6201069224625826 seconds)
2022-03-04 15:49:24 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 15:49:24 | INFO | train | epoch 034 | loss 6.944 | nll_loss 5.368 | ppl 41.3 | wps 24637.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3273 | lr 0.000409143 | gnorm 0.959 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 8771
2022-03-04 15:49:24 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 15:49:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:50:33 | INFO | train_inner | epoch 035:     27 / 97 loss=6.92, nll_loss=5.341, ppl=40.54, wps=24917.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3300, lr=0.000412518, gnorm=0.921, loss_scale=16, train_wall=233, gb_free=21, wall=8840
2022-03-04 15:53:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 15:53:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:53:36 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.994 | nll_loss 7.639 | ppl 199.32 | wps 45482.7 | wpb 510.9 | bsz 1 | num_updates 3369 | best_loss 8.737
2022-03-04 15:53:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3369 updates
2022-03-04 15:53:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:53:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:53:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 35 @ 3369 updates, score 8.994) (writing took 2.4769018376246095 seconds)
2022-03-04 15:53:39 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 15:53:39 | INFO | train | epoch 035 | loss 6.855 | nll_loss 5.264 | ppl 38.43 | wps 24654.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3369 | lr 0.000421141 | gnorm 0.978 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 9026
2022-03-04 15:53:39 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 15:53:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:54:58 | INFO | train_inner | epoch 036:     31 / 97 loss=6.819, nll_loss=5.222, ppl=37.31, wps=24694.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3400, lr=0.000425015, gnorm=0.95, loss_scale=16, train_wall=235, gb_free=21, wall=9105
2022-03-04 15:57:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:57:51 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 9.042 | nll_loss 7.686 | ppl 205.92 | wps 45426 | wpb 510.9 | bsz 1 | num_updates 3466 | best_loss 8.737
2022-03-04 15:57:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 3466 updates
2022-03-04 15:57:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:57:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 15:57:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 36 @ 3466 updates, score 9.042) (writing took 2.659607839770615 seconds)
2022-03-04 15:57:54 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 15:57:54 | INFO | train | epoch 036 | loss 6.769 | nll_loss 5.162 | ppl 35.81 | wps 24881.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3466 | lr 0.000433263 | gnorm 0.971 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 9281
2022-03-04 15:57:54 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 15:57:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:59:21 | INFO | train_inner | epoch 037:     34 / 97 loss=6.741, nll_loss=5.129, ppl=35, wps=24898, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3500, lr=0.000437513, gnorm=0.98, loss_scale=32, train_wall=233, gb_free=21, wall=9368
2022-03-04 16:00:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:02:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:02:07 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.127 | nll_loss 7.794 | ppl 221.9 | wps 45683.2 | wpb 510.9 | bsz 1 | num_updates 3562 | best_loss 8.737
2022-03-04 16:02:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 3562 updates
2022-03-04 16:02:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:02:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:02:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 37 @ 3562 updates, score 9.127) (writing took 2.4693236090242863 seconds)
2022-03-04 16:02:09 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 16:02:09 | INFO | train | epoch 037 | loss 6.679 | nll_loss 5.057 | ppl 33.28 | wps 24649.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3562 | lr 0.000445261 | gnorm 0.975 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 9536
2022-03-04 16:02:09 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 16:02:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:03:46 | INFO | train_inner | epoch 038:     38 / 97 loss=6.646, nll_loss=5.017, ppl=32.39, wps=24689, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3600, lr=0.00045001, gnorm=0.987, loss_scale=16, train_wall=235, gb_free=21, wall=9633
2022-03-04 16:05:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:06:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:06:22 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 9.197 | nll_loss 7.865 | ppl 233.09 | wps 45618.9 | wpb 510.9 | bsz 1 | num_updates 3658 | best_loss 8.737
2022-03-04 16:06:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3658 updates
2022-03-04 16:06:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:06:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:06:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 38 @ 3658 updates, score 9.197) (writing took 2.53962423838675 seconds)
2022-03-04 16:06:24 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 16:06:24 | INFO | train | epoch 038 | loss 6.595 | nll_loss 4.958 | ppl 31.07 | wps 24637.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3658 | lr 0.000457259 | gnorm 0.993 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 9791
2022-03-04 16:06:24 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 16:06:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:08:12 | INFO | train_inner | epoch 039:     42 / 97 loss=6.562, nll_loss=4.918, ppl=30.23, wps=24684.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=3700, lr=0.000462508, gnorm=1.011, loss_scale=16, train_wall=235, gb_free=21, wall=9899
2022-03-04 16:10:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:10:37 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.176 | nll_loss 7.812 | ppl 224.66 | wps 45335.2 | wpb 510.9 | bsz 1 | num_updates 3755 | best_loss 8.737
2022-03-04 16:10:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 3755 updates
2022-03-04 16:10:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:10:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:10:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 39 @ 3755 updates, score 9.176) (writing took 2.645113348029554 seconds)
2022-03-04 16:10:40 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 16:10:40 | INFO | train | epoch 039 | loss 6.517 | nll_loss 4.865 | ppl 29.14 | wps 24884.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3755 | lr 0.000469381 | gnorm 1.037 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 10046
2022-03-04 16:10:40 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 16:10:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:12:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:12:37 | INFO | train_inner | epoch 040:     46 / 97 loss=6.474, nll_loss=4.814, ppl=28.12, wps=24655.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3800, lr=0.000475005, gnorm=1.017, loss_scale=16, train_wall=236, gb_free=21, wall=10164
2022-03-04 16:14:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:14:52 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.24 | nll_loss 7.881 | ppl 235.8 | wps 45103.7 | wpb 510.9 | bsz 1 | num_updates 3851 | best_loss 8.737
2022-03-04 16:14:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 3851 updates
2022-03-04 16:14:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:14:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:14:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 40 @ 3851 updates, score 9.24) (writing took 2.4720467012375593 seconds)
2022-03-04 16:14:55 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 16:14:55 | INFO | train | epoch 040 | loss 6.431 | nll_loss 4.763 | ppl 27.16 | wps 24635.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 3851 | lr 0.000481379 | gnorm 1.009 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 10302
2022-03-04 16:14:55 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 16:14:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:17:00 | INFO | train_inner | epoch 041:     49 / 97 loss=6.391, nll_loss=4.716, ppl=26.28, wps=24911.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=3900, lr=0.000487503, gnorm=1.031, loss_scale=16, train_wall=233, gb_free=21, wall=10427
2022-03-04 16:19:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:19:08 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.258 | nll_loss 7.888 | ppl 236.93 | wps 45368.2 | wpb 510.9 | bsz 1 | num_updates 3948 | best_loss 8.737
2022-03-04 16:19:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 3948 updates
2022-03-04 16:19:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:19:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:19:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 41 @ 3948 updates, score 9.258) (writing took 2.485801475122571 seconds)
2022-03-04 16:19:10 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 16:19:10 | INFO | train | epoch 041 | loss 6.35 | nll_loss 4.667 | ppl 25.41 | wps 24878.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 3948 | lr 0.000493501 | gnorm 1.021 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 10557
2022-03-04 16:19:10 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 16:19:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:19:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:21:26 | INFO | train_inner | epoch 042:     53 / 97 loss=6.305, nll_loss=4.615, ppl=24.5, wps=24638.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4000, lr=0.0005, gnorm=1.024, loss_scale=16, train_wall=236, gb_free=21, wall=10693
2022-03-04 16:23:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:23:23 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.36 | nll_loss 8.014 | ppl 258.54 | wps 44774.2 | wpb 510.9 | bsz 1 | num_updates 4044 | best_loss 8.737
2022-03-04 16:23:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 4044 updates
2022-03-04 16:23:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:23:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:23:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 42 @ 4044 updates, score 9.36) (writing took 2.516597825102508 seconds)
2022-03-04 16:23:26 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 16:23:26 | INFO | train | epoch 042 | loss 6.267 | nll_loss 4.569 | ppl 23.73 | wps 24585.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4044 | lr 0.000497272 | gnorm 1.032 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 10813
2022-03-04 16:23:26 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 16:23:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:25:49 | INFO | train_inner | epoch 043:     56 / 97 loss=6.222, nll_loss=4.516, ppl=22.88, wps=24868.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=4100, lr=0.000493865, gnorm=1.028, loss_scale=32, train_wall=233, gb_free=21, wall=10956
2022-03-04 16:26:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:27:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:27:39 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.4 | nll_loss 8.048 | ppl 264.61 | wps 45270 | wpb 510.9 | bsz 1 | num_updates 4140 | best_loss 8.737
2022-03-04 16:27:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 4140 updates
2022-03-04 16:27:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:27:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:27:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 43 @ 4140 updates, score 9.4) (writing took 2.3430151445791125 seconds)
2022-03-04 16:27:41 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 16:27:41 | INFO | train | epoch 043 | loss 6.184 | nll_loss 4.471 | ppl 22.18 | wps 24604.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4140 | lr 0.000491473 | gnorm 1.019 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 11068
2022-03-04 16:27:41 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 16:27:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:30:15 | INFO | train_inner | epoch 044:     60 / 97 loss=6.131, nll_loss=4.408, ppl=21.23, wps=24646.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4200, lr=0.00048795, gnorm=1.006, loss_scale=16, train_wall=236, gb_free=21, wall=11222
2022-03-04 16:31:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:31:54 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.464 | nll_loss 8.115 | ppl 277.29 | wps 45282.6 | wpb 510.9 | bsz 1 | num_updates 4237 | best_loss 8.737
2022-03-04 16:31:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 4237 updates
2022-03-04 16:31:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:31:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:31:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 44 @ 4237 updates, score 9.464) (writing took 2.305051458068192 seconds)
2022-03-04 16:31:57 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 16:31:57 | INFO | train | epoch 044 | loss 6.098 | nll_loss 4.369 | ppl 20.67 | wps 24869.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4237 | lr 0.000485815 | gnorm 1.007 | loss_scale 32 | train_wall 227 | gb_free 21 | wall 11324
2022-03-04 16:31:57 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 16:31:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:32:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:34:41 | INFO | train_inner | epoch 045:     64 / 97 loss=6.047, nll_loss=4.309, ppl=19.82, wps=24651.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=4300, lr=0.000482243, gnorm=0.999, loss_scale=16, train_wall=236, gb_free=21, wall=11488
2022-03-04 16:36:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:36:10 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.551 | nll_loss 8.19 | ppl 291.97 | wps 44916.8 | wpb 510.9 | bsz 1 | num_updates 4333 | best_loss 8.737
2022-03-04 16:36:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 4333 updates
2022-03-04 16:36:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:36:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:36:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 45 @ 4333 updates, score 9.551) (writing took 2.2793110199272633 seconds)
2022-03-04 16:36:12 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 16:36:12 | INFO | train | epoch 045 | loss 6.012 | nll_loss 4.268 | ppl 19.27 | wps 24615.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4333 | lr 0.000480403 | gnorm 0.996 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 11579
2022-03-04 16:36:12 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 16:36:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:38:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:39:06 | INFO | train_inner | epoch 046:     68 / 97 loss=5.962, nll_loss=4.208, ppl=18.49, wps=24650.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4400, lr=0.000476731, gnorm=1.022, loss_scale=16, train_wall=236, gb_free=21, wall=11753
2022-03-04 16:40:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:40:25 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.633 | nll_loss 8.298 | ppl 314.84 | wps 45435.4 | wpb 510.9 | bsz 1 | num_updates 4429 | best_loss 8.737
2022-03-04 16:40:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 4429 updates
2022-03-04 16:40:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:40:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:40:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 46 @ 4429 updates, score 9.633) (writing took 2.2776224482804537 seconds)
2022-03-04 16:40:28 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 16:40:28 | INFO | train | epoch 046 | loss 5.935 | nll_loss 4.176 | ppl 18.07 | wps 24623.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4429 | lr 0.000475168 | gnorm 1.009 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 11834
2022-03-04 16:40:28 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 16:40:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:43:29 | INFO | train_inner | epoch 047:     71 / 97 loss=5.879, nll_loss=4.11, ppl=17.27, wps=24909.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4500, lr=0.000471405, gnorm=0.996, loss_scale=16, train_wall=233, gb_free=21, wall=12016
2022-03-04 16:44:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:44:41 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.723 | nll_loss 8.398 | ppl 337.39 | wps 45404.6 | wpb 510.9 | bsz 1 | num_updates 4526 | best_loss 8.737
2022-03-04 16:44:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4526 updates
2022-03-04 16:44:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:44:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:44:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 47 @ 4526 updates, score 9.723) (writing took 2.29478423204273 seconds)
2022-03-04 16:44:43 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 16:44:43 | INFO | train | epoch 047 | loss 5.86 | nll_loss 4.087 | ppl 16.99 | wps 24881.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4526 | lr 0.000470049 | gnorm 1.021 | loss_scale 32 | train_wall 226 | gb_free 21 | wall 12090
2022-03-04 16:44:43 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 16:44:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:44:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:46:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:47:57 | INFO | train_inner | epoch 048:     76 / 97 loss=5.803, nll_loss=4.019, ppl=16.22, wps=24430.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=4600, lr=0.000466252, gnorm=1.016, loss_scale=8, train_wall=238, gb_free=21, wall=12284
2022-03-04 16:48:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:48:56 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.766 | nll_loss 8.435 | ppl 346.15 | wps 44831.4 | wpb 510.9 | bsz 1 | num_updates 4621 | best_loss 8.737
2022-03-04 16:48:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 4621 updates
2022-03-04 16:48:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:48:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:48:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 48 @ 4621 updates, score 9.766) (writing took 2.273242556490004 seconds)
2022-03-04 16:48:58 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 16:48:58 | INFO | train | epoch 048 | loss 5.781 | nll_loss 3.993 | ppl 15.93 | wps 24374.1 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 4621 | lr 0.000465192 | gnorm 0.994 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 12345
2022-03-04 16:48:58 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 16:48:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:52:20 | INFO | train_inner | epoch 049:     79 / 97 loss=5.731, nll_loss=3.934, ppl=15.28, wps=24912.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4700, lr=0.000461266, gnorm=0.982, loss_scale=16, train_wall=233, gb_free=21, wall=12547
2022-03-04 16:53:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:53:11 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.816 | nll_loss 8.474 | ppl 355.59 | wps 45058 | wpb 510.9 | bsz 1 | num_updates 4718 | best_loss 8.737
2022-03-04 16:53:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 4718 updates
2022-03-04 16:53:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:53:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:53:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 49 @ 4718 updates, score 9.816) (writing took 2.288797928020358 seconds)
2022-03-04 16:53:13 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 16:53:13 | INFO | train | epoch 049 | loss 5.712 | nll_loss 3.911 | ppl 15.05 | wps 24890.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4718 | lr 0.000460385 | gnorm 0.994 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 12600
2022-03-04 16:53:13 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 16:53:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:54:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:56:46 | INFO | train_inner | epoch 050:     83 / 97 loss=5.655, nll_loss=3.844, ppl=14.36, wps=24663.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=4800, lr=0.000456435, gnorm=0.979, loss_scale=8, train_wall=236, gb_free=21, wall=12813
2022-03-04 16:57:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:57:26 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.885 | nll_loss 8.537 | ppl 371.47 | wps 45276.4 | wpb 510.9 | bsz 1 | num_updates 4814 | best_loss 8.737
2022-03-04 16:57:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 4814 updates
2022-03-04 16:57:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:57:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 16:57:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 50 @ 4814 updates, score 9.885) (writing took 2.2799806464463472 seconds)
2022-03-04 16:57:29 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 16:57:29 | INFO | train | epoch 050 | loss 5.641 | nll_loss 3.827 | ppl 14.2 | wps 24621.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 4814 | lr 0.000455771 | gnorm 0.983 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 12856
2022-03-04 16:57:29 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 16:57:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:01:09 | INFO | train_inner | epoch 051:     86 / 97 loss=5.588, nll_loss=3.763, ppl=13.58, wps=24888.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=4900, lr=0.000451754, gnorm=1.022, loss_scale=16, train_wall=234, gb_free=21, wall=13076
2022-03-04 17:01:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:01:42 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.953 | nll_loss 8.626 | ppl 394.96 | wps 45137 | wpb 510.9 | bsz 1 | num_updates 4911 | best_loss 8.737
2022-03-04 17:01:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 4911 updates
2022-03-04 17:01:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:01:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:01:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 51 @ 4911 updates, score 9.953) (writing took 2.6128046894446015 seconds)
2022-03-04 17:01:45 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 17:01:45 | INFO | train | epoch 051 | loss 5.576 | nll_loss 3.75 | ppl 13.45 | wps 24828.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 4911 | lr 0.000451248 | gnorm 1.001 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 13111
2022-03-04 17:01:45 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 17:01:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:04:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:05:35 | INFO | train_inner | epoch 052:     90 / 97 loss=5.519, nll_loss=3.681, ppl=12.83, wps=24636, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=5000, lr=0.000447214, gnorm=1, loss_scale=8, train_wall=236, gb_free=21, wall=13342
2022-03-04 17:05:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:05:57 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 10.069 | nll_loss 8.743 | ppl 428.31 | wps 45363.4 | wpb 510.9 | bsz 1 | num_updates 5007 | best_loss 8.737
2022-03-04 17:05:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 5007 updates
2022-03-04 17:05:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:06:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:06:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 52 @ 5007 updates, score 10.069) (writing took 2.2865761565044522 seconds)
2022-03-04 17:06:00 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 17:06:00 | INFO | train | epoch 052 | loss 5.51 | nll_loss 3.67 | ppl 12.73 | wps 24637.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5007 | lr 0.000446901 | gnorm 1.009 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 13367
2022-03-04 17:06:00 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 17:06:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:09:58 | INFO | train_inner | epoch 053:     93 / 97 loss=5.453, nll_loss=3.603, ppl=12.15, wps=24905.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5100, lr=0.000442807, gnorm=0.994, loss_scale=8, train_wall=233, gb_free=21, wall=13605
2022-03-04 17:10:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:10:13 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 10.122 | nll_loss 8.798 | ppl 445.16 | wps 44919.6 | wpb 510.9 | bsz 1 | num_updates 5104 | best_loss 8.737
2022-03-04 17:10:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 5104 updates
2022-03-04 17:10:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:10:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:10:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 53 @ 5104 updates, score 10.122) (writing took 2.3445250280201435 seconds)
2022-03-04 17:10:15 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 17:10:15 | INFO | train | epoch 053 | loss 5.447 | nll_loss 3.596 | ppl 12.09 | wps 24875.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5104 | lr 0.000442634 | gnorm 0.995 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 13622
2022-03-04 17:10:15 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 17:10:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:14:21 | INFO | train_inner | epoch 054:     96 / 97 loss=5.393, nll_loss=3.531, ppl=11.56, wps=24880.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5200, lr=0.000438529, gnorm=0.996, loss_scale=16, train_wall=234, gb_free=21, wall=13868
2022-03-04 17:14:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:14:28 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 10.224 | nll_loss 8.905 | ppl 479.34 | wps 45009.9 | wpb 510.9 | bsz 1 | num_updates 5201 | best_loss 8.737
2022-03-04 17:14:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 5201 updates
2022-03-04 17:14:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:14:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:14:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 54 @ 5201 updates, score 10.224) (writing took 2.6286848317831755 seconds)
2022-03-04 17:14:31 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 17:14:31 | INFO | train | epoch 054 | loss 5.387 | nll_loss 3.525 | ppl 11.51 | wps 24830.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5201 | lr 0.000438487 | gnorm 0.997 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 13878
2022-03-04 17:14:31 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 17:14:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:15:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:18:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:18:44 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 10.246 | nll_loss 8.933 | ppl 488.93 | wps 45265 | wpb 510.9 | bsz 1 | num_updates 5297 | best_loss 8.737
2022-03-04 17:18:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 5297 updates
2022-03-04 17:18:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:18:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:18:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 55 @ 5297 updates, score 10.246) (writing took 2.323684226721525 seconds)
2022-03-04 17:18:46 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 17:18:46 | INFO | train | epoch 055 | loss 5.327 | nll_loss 3.453 | ppl 10.95 | wps 24617.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5297 | lr 0.000434495 | gnorm 0.997 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 14133
2022-03-04 17:18:46 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 17:18:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:18:54 | INFO | train_inner | epoch 056:      3 / 97 loss=5.324, nll_loss=3.449, ppl=10.92, wps=23964, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=5300, lr=0.000434372, gnorm=0.998, loss_scale=16, train_wall=236, gb_free=21, wall=14141
2022-03-04 17:21:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:22:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:22:59 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 10.32 | nll_loss 9.01 | ppl 515.42 | wps 45170.9 | wpb 510.9 | bsz 1 | num_updates 5393 | best_loss 8.737
2022-03-04 17:22:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 5393 updates
2022-03-04 17:22:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:23:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:23:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 56 @ 5393 updates, score 10.32) (writing took 2.5972650488838553 seconds)
2022-03-04 17:23:02 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 17:23:02 | INFO | train | epoch 056 | loss 5.268 | nll_loss 3.383 | ppl 10.43 | wps 24590.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5393 | lr 0.000430611 | gnorm 0.993 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 14389
2022-03-04 17:23:02 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 17:23:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:23:20 | INFO | train_inner | epoch 057:      7 / 97 loss=5.262, nll_loss=3.375, ppl=10.37, wps=24631.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=5400, lr=0.000430331, gnorm=0.99, loss_scale=16, train_wall=236, gb_free=21, wall=14407
2022-03-04 17:27:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:27:15 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 10.446 | nll_loss 9.161 | ppl 572.45 | wps 45211.8 | wpb 510.9 | bsz 1 | num_updates 5490 | best_loss 8.737
2022-03-04 17:27:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 5490 updates
2022-03-04 17:27:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:27:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:27:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 57 @ 5490 updates, score 10.446) (writing took 2.3248933162540197 seconds)
2022-03-04 17:27:17 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 17:27:17 | INFO | train | epoch 057 | loss 5.218 | nll_loss 3.322 | ppl 10 | wps 24881.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5490 | lr 0.00042679 | gnorm 1.011 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 14644
2022-03-04 17:27:17 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 17:27:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:27:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:27:46 | INFO | train_inner | epoch 058:     11 / 97 loss=5.209, nll_loss=3.312, ppl=9.93, wps=24662.3, ups=0.38, wpb=65495, bsz=127.9, num_updates=5500, lr=0.000426401, gnorm=1.016, loss_scale=16, train_wall=236, gb_free=21, wall=14673
2022-03-04 17:28:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:31:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:31:30 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 10.422 | nll_loss 9.134 | ppl 561.78 | wps 45339.3 | wpb 510.9 | bsz 1 | num_updates 5585 | best_loss 8.737
2022-03-04 17:31:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 5585 updates
2022-03-04 17:31:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:31:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:31:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 58 @ 5585 updates, score 10.422) (writing took 2.509580290876329 seconds)
2022-03-04 17:31:33 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 17:31:33 | INFO | train | epoch 058 | loss 5.163 | nll_loss 3.256 | ppl 9.55 | wps 24344.7 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 5585 | lr 0.000423144 | gnorm 1.026 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 14900
2022-03-04 17:31:33 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 17:31:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:32:11 | INFO | train_inner | epoch 059:     15 / 97 loss=5.154, nll_loss=3.245, ppl=9.48, wps=24637.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5600, lr=0.000422577, gnorm=1.015, loss_scale=8, train_wall=236, gb_free=21, wall=14938
2022-03-04 17:35:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:35:46 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 10.506 | nll_loss 9.204 | ppl 589.68 | wps 45094.3 | wpb 510.9 | bsz 1 | num_updates 5682 | best_loss 8.737
2022-03-04 17:35:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 5682 updates
2022-03-04 17:35:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:35:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:35:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 59 @ 5682 updates, score 10.506) (writing took 2.33527869079262 seconds)
2022-03-04 17:35:48 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 17:35:48 | INFO | train | epoch 059 | loss 5.114 | nll_loss 3.198 | ppl 9.18 | wps 24888.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5682 | lr 0.000419517 | gnorm 1.001 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 15155
2022-03-04 17:35:48 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 17:35:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:36:34 | INFO | train_inner | epoch 060:     18 / 97 loss=5.105, nll_loss=3.187, ppl=9.11, wps=24911.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=5700, lr=0.000418854, gnorm=1.014, loss_scale=16, train_wall=233, gb_free=21, wall=15201
2022-03-04 17:39:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:40:01 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 10.62 | nll_loss 9.343 | ppl 649.43 | wps 45008 | wpb 510.9 | bsz 1 | num_updates 5779 | best_loss 8.737
2022-03-04 17:40:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 5779 updates
2022-03-04 17:40:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:40:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:40:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 60 @ 5779 updates, score 10.62) (writing took 2.471556936390698 seconds)
2022-03-04 17:40:04 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 17:40:04 | INFO | train | epoch 060 | loss 5.065 | nll_loss 3.139 | ppl 8.81 | wps 24847.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 5779 | lr 0.000415981 | gnorm 1.011 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 15411
2022-03-04 17:40:04 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 17:40:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:40:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:41:00 | INFO | train_inner | epoch 061:     22 / 97 loss=5.053, nll_loss=3.126, ppl=8.73, wps=24636.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=5800, lr=0.000415227, gnorm=1.006, loss_scale=16, train_wall=236, gb_free=21, wall=15467
2022-03-04 17:44:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:44:16 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 10.627 | nll_loss 9.325 | ppl 641.44 | wps 45494.2 | wpb 510.9 | bsz 1 | num_updates 5875 | best_loss 8.737
2022-03-04 17:44:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 5875 updates
2022-03-04 17:44:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:44:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:44:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 61 @ 5875 updates, score 10.627) (writing took 2.620848939754069 seconds)
2022-03-04 17:44:19 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 17:44:19 | INFO | train | epoch 061 | loss 5.016 | nll_loss 3.081 | ppl 8.46 | wps 24630.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 5875 | lr 0.000412568 | gnorm 1.03 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 15666
2022-03-04 17:44:19 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 17:44:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:45:23 | INFO | train_inner | epoch 062:     25 / 97 loss=5.001, nll_loss=3.063, ppl=8.36, wps=24914.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=5900, lr=0.000411693, gnorm=1.012, loss_scale=16, train_wall=233, gb_free=21, wall=15730
2022-03-04 17:46:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:46:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:48:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:48:32 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 10.724 | nll_loss 9.441 | ppl 695.2 | wps 45445 | wpb 510.9 | bsz 1 | num_updates 5970 | best_loss 8.737
2022-03-04 17:48:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 5970 updates
2022-03-04 17:48:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:48:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:48:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 62 @ 5970 updates, score 10.724) (writing took 2.6093625221401453 seconds)
2022-03-04 17:48:34 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 17:48:34 | INFO | train | epoch 062 | loss 4.969 | nll_loss 3.024 | ppl 8.14 | wps 24391.6 | ups 0.37 | wpb 65492.9 | bsz 127.9 | num_updates 5970 | lr 0.000409273 | gnorm 1.008 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 15921
2022-03-04 17:48:34 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 17:48:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:49:51 | INFO | train_inner | epoch 063:     30 / 97 loss=4.954, nll_loss=3.007, ppl=8.04, wps=24452.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=6000, lr=0.000408248, gnorm=1.025, loss_scale=8, train_wall=238, gb_free=21, wall=15998
2022-03-04 17:52:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:52:47 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 10.787 | nll_loss 9.514 | ppl 731.19 | wps 45436.3 | wpb 510.9 | bsz 1 | num_updates 6067 | best_loss 8.737
2022-03-04 17:52:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 6067 updates
2022-03-04 17:52:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:52:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:52:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 63 @ 6067 updates, score 10.787) (writing took 2.569471546448767 seconds)
2022-03-04 17:52:49 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 17:52:49 | INFO | train | epoch 063 | loss 4.929 | nll_loss 2.977 | ppl 7.88 | wps 24904.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6067 | lr 0.000405988 | gnorm 1.024 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 16176
2022-03-04 17:52:49 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 17:52:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:54:14 | INFO | train_inner | epoch 064:     33 / 97 loss=4.915, nll_loss=2.96, ppl=7.78, wps=24930.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=6100, lr=0.000404888, gnorm=1.028, loss_scale=16, train_wall=233, gb_free=21, wall=16261
2022-03-04 17:56:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:57:02 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 10.82 | nll_loss 9.555 | ppl 752.03 | wps 45529.3 | wpb 510.9 | bsz 1 | num_updates 6164 | best_loss 8.737
2022-03-04 17:57:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 6164 updates
2022-03-04 17:57:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:57:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 17:57:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 64 @ 6164 updates, score 10.82) (writing took 2.5806900057941675 seconds)
2022-03-04 17:57:04 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 17:57:04 | INFO | train | epoch 064 | loss 4.886 | nll_loss 2.925 | ppl 7.59 | wps 24912.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6164 | lr 0.000402781 | gnorm 1.022 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 16431
2022-03-04 17:57:04 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 17:57:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:57:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:58:39 | INFO | train_inner | epoch 065:     37 / 97 loss=4.868, nll_loss=2.904, ppl=7.48, wps=24689.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6200, lr=0.00040161, gnorm=1.005, loss_scale=16, train_wall=235, gb_free=21, wall=16526
2022-03-04 18:01:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:01:17 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 10.87 | nll_loss 9.608 | ppl 780.11 | wps 45658.6 | wpb 510.9 | bsz 1 | num_updates 6260 | best_loss 8.737
2022-03-04 18:01:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 6260 updates
2022-03-04 18:01:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:01:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:01:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 65 @ 6260 updates, score 10.87) (writing took 2.6135214380919933 seconds)
2022-03-04 18:01:19 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 18:01:19 | INFO | train | epoch 065 | loss 4.844 | nll_loss 2.875 | ppl 7.34 | wps 24642.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 6260 | lr 0.00039968 | gnorm 0.998 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 16686
2022-03-04 18:01:19 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 18:01:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:03:02 | INFO | train_inner | epoch 066:     40 / 97 loss=4.827, nll_loss=2.855, ppl=7.24, wps=24919.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6300, lr=0.00039841, gnorm=1.004, loss_scale=16, train_wall=233, gb_free=21, wall=16789
2022-03-04 18:03:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:05:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:05:32 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 10.943 | nll_loss 9.661 | ppl 809.37 | wps 45512.7 | wpb 510.9 | bsz 1 | num_updates 6356 | best_loss 8.737
2022-03-04 18:05:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 6356 updates
2022-03-04 18:05:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:05:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:05:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 66 @ 6356 updates, score 10.943) (writing took 2.6125514172017574 seconds)
2022-03-04 18:05:35 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 18:05:35 | INFO | train | epoch 066 | loss 4.803 | nll_loss 2.827 | ppl 7.1 | wps 24639.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 6356 | lr 0.000396651 | gnorm 1.019 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 16941
2022-03-04 18:05:35 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 18:05:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:07:27 | INFO | train_inner | epoch 067:     44 / 97 loss=4.786, nll_loss=2.806, ppl=6.99, wps=24673.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=6400, lr=0.000395285, gnorm=1.028, loss_scale=16, train_wall=235, gb_free=21, wall=17054
2022-03-04 18:09:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:09:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:09:47 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 10.943 | nll_loss 9.651 | ppl 804.05 | wps 45726.9 | wpb 510.9 | bsz 1 | num_updates 6452 | best_loss 8.737
2022-03-04 18:09:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 6452 updates
2022-03-04 18:09:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:09:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:09:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 67 @ 6452 updates, score 10.943) (writing took 2.5788508225232363 seconds)
2022-03-04 18:09:50 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 18:09:50 | INFO | train | epoch 067 | loss 4.765 | nll_loss 2.78 | ppl 6.87 | wps 24643.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 6452 | lr 0.000393689 | gnorm 1.012 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 17197
2022-03-04 18:09:50 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 18:09:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:11:52 | INFO | train_inner | epoch 068:     48 / 97 loss=4.749, nll_loss=2.762, ppl=6.78, wps=24692, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=6500, lr=0.000392232, gnorm=1, loss_scale=16, train_wall=235, gb_free=21, wall=17319
2022-03-04 18:13:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:14:02 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 10.994 | nll_loss 9.725 | ppl 846.54 | wps 45546.5 | wpb 510.9 | bsz 1 | num_updates 6549 | best_loss 8.737
2022-03-04 18:14:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 6549 updates
2022-03-04 18:14:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:14:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:14:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 68 @ 6549 updates, score 10.994) (writing took 2.7073198640719056 seconds)
2022-03-04 18:14:05 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 18:14:05 | INFO | train | epoch 068 | loss 4.729 | nll_loss 2.737 | ppl 6.67 | wps 24881.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6549 | lr 0.000390762 | gnorm 1.008 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 17452
2022-03-04 18:14:05 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 18:14:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:15:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:16:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:16:20 | INFO | train_inner | epoch 069:     53 / 97 loss=4.713, nll_loss=2.719, ppl=6.58, wps=24438.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=6600, lr=0.000389249, gnorm=1.021, loss_scale=8, train_wall=238, gb_free=21, wall=17587
2022-03-04 18:18:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:18:18 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 11.061 | nll_loss 9.792 | ppl 886.48 | wps 43044.6 | wpb 510.9 | bsz 1 | num_updates 6644 | best_loss 8.737
2022-03-04 18:18:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 6644 updates
2022-03-04 18:18:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:18:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:18:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 69 @ 6644 updates, score 11.061) (writing took 2.6344112511724234 seconds)
2022-03-04 18:18:21 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 18:18:21 | INFO | train | epoch 069 | loss 4.691 | nll_loss 2.693 | ppl 6.47 | wps 24340.9 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 6644 | lr 0.000387958 | gnorm 1.012 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 17708
2022-03-04 18:18:21 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 18:18:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:20:45 | INFO | train_inner | epoch 070:     56 / 97 loss=4.669, nll_loss=2.666, ppl=6.35, wps=24727.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=6700, lr=0.000386334, gnorm=1.015, loss_scale=8, train_wall=234, gb_free=21, wall=17852
2022-03-04 18:22:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:22:36 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 11.151 | nll_loss 9.901 | ppl 955.95 | wps 43083.9 | wpb 510.9 | bsz 1 | num_updates 6741 | best_loss 8.737
2022-03-04 18:22:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 6741 updates
2022-03-04 18:22:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:22:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:22:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 70 @ 6741 updates, score 11.151) (writing took 2.631501573137939 seconds)
2022-03-04 18:22:39 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 18:22:39 | INFO | train | epoch 070 | loss 4.659 | nll_loss 2.654 | ppl 6.29 | wps 24619.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6741 | lr 0.000385157 | gnorm 1.028 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 17966
2022-03-04 18:22:39 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 18:22:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:25:11 | INFO | train_inner | epoch 071:     59 / 97 loss=4.642, nll_loss=2.634, ppl=6.21, wps=24617.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=6800, lr=0.000383482, gnorm=1.024, loss_scale=16, train_wall=235, gb_free=21, wall=18118
2022-03-04 18:26:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:26:54 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 11.165 | nll_loss 9.92 | ppl 969.02 | wps 44813.3 | wpb 510.9 | bsz 1 | num_updates 6838 | best_loss 8.737
2022-03-04 18:26:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 6838 updates
2022-03-04 18:26:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:26:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:26:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 71 @ 6838 updates, score 11.165) (writing took 2.4905199063941836 seconds)
2022-03-04 18:26:57 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 18:26:57 | INFO | train | epoch 071 | loss 4.624 | nll_loss 2.612 | ppl 6.11 | wps 24607.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 6838 | lr 0.000382415 | gnorm 1.02 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 18224
2022-03-04 18:26:57 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 18:26:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:27:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:29:40 | INFO | train_inner | epoch 072:     63 / 97 loss=4.6, nll_loss=2.584, ppl=5.99, wps=24406.1, ups=0.37, wpb=65495, bsz=127.9, num_updates=6900, lr=0.000380693, gnorm=1.022, loss_scale=16, train_wall=238, gb_free=21, wall=18387
2022-03-04 18:31:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:31:13 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 11.192 | nll_loss 9.926 | ppl 973.11 | wps 43738.8 | wpb 510.9 | bsz 1 | num_updates 6934 | best_loss 8.737
2022-03-04 18:31:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 6934 updates
2022-03-04 18:31:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:31:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:31:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 72 @ 6934 updates, score 11.192) (writing took 2.642541171051562 seconds)
2022-03-04 18:31:15 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 18:31:15 | INFO | train | epoch 072 | loss 4.59 | nll_loss 2.571 | ppl 5.94 | wps 24331.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 6934 | lr 0.000379759 | gnorm 1.029 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 18482
2022-03-04 18:31:15 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 18:31:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:33:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:34:08 | INFO | train_inner | epoch 073:     67 / 97 loss=4.568, nll_loss=2.546, ppl=5.84, wps=24368.1, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=7000, lr=0.000377964, gnorm=1.023, loss_scale=16, train_wall=238, gb_free=21, wall=18655
2022-03-04 18:35:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:35:31 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 11.25 | nll_loss 10.002 | ppl 1025.33 | wps 43740.4 | wpb 510.9 | bsz 1 | num_updates 7030 | best_loss 8.737
2022-03-04 18:35:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 7030 updates
2022-03-04 18:35:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:35:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:35:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 73 @ 7030 updates, score 11.25) (writing took 2.5677216090261936 seconds)
2022-03-04 18:35:33 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 18:35:33 | INFO | train | epoch 073 | loss 4.557 | nll_loss 2.532 | ppl 5.79 | wps 24352.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7030 | lr 0.000377157 | gnorm 1.017 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 18740
2022-03-04 18:35:33 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 18:35:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:38:35 | INFO | train_inner | epoch 074:     70 / 97 loss=4.543, nll_loss=2.516, ppl=5.72, wps=24588, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=7100, lr=0.000375293, gnorm=1.015, loss_scale=16, train_wall=236, gb_free=21, wall=18922
2022-03-04 18:39:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:39:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:39:50 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 11.295 | nll_loss 10.056 | ppl 1064.46 | wps 44334.2 | wpb 510.9 | bsz 1 | num_updates 7126 | best_loss 8.737
2022-03-04 18:39:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 7126 updates
2022-03-04 18:39:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:39:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:39:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 74 @ 7126 updates, score 11.295) (writing took 2.6459386963397264 seconds)
2022-03-04 18:39:52 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 18:39:52 | INFO | train | epoch 074 | loss 4.528 | nll_loss 2.498 | ppl 5.65 | wps 24283 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7126 | lr 0.000374608 | gnorm 1.023 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 18999
2022-03-04 18:39:52 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 18:39:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:43:04 | INFO | train_inner | epoch 075:     74 / 97 loss=4.506, nll_loss=2.471, ppl=5.55, wps=24298.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7200, lr=0.000372678, gnorm=1.028, loss_scale=8, train_wall=239, gb_free=21, wall=19191
2022-03-04 18:44:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:44:09 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 11.399 | nll_loss 10.172 | ppl 1154.02 | wps 45380.7 | wpb 510.9 | bsz 1 | num_updates 7223 | best_loss 8.737
2022-03-04 18:44:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 7223 updates
2022-03-04 18:44:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:44:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:44:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 75 @ 7223 updates, score 11.399) (writing took 2.601023660041392 seconds)
2022-03-04 18:44:11 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-04 18:44:11 | INFO | train | epoch 075 | loss 4.499 | nll_loss 2.464 | ppl 5.52 | wps 24536.3 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 7223 | lr 0.000372084 | gnorm 1.027 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 19258
2022-03-04 18:44:11 | INFO | fairseq.trainer | begin training epoch 76
2022-03-04 18:44:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:47:31 | INFO | train_inner | epoch 076:     77 / 97 loss=4.478, nll_loss=2.439, ppl=5.42, wps=24559.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=7300, lr=0.000370117, gnorm=1.026, loss_scale=16, train_wall=236, gb_free=21, wall=19458
2022-03-04 18:48:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:48:28 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 11.377 | nll_loss 10.144 | ppl 1131.84 | wps 43411.3 | wpb 510.9 | bsz 1 | num_updates 7320 | best_loss 8.737
2022-03-04 18:48:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 7320 updates
2022-03-04 18:48:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:48:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:48:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 76 @ 7320 updates, score 11.377) (writing took 2.573009996674955 seconds)
2022-03-04 18:48:30 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-04 18:48:30 | INFO | train | epoch 076 | loss 4.47 | nll_loss 2.429 | ppl 5.38 | wps 24514.6 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 7320 | lr 0.000369611 | gnorm 1.02 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 19517
2022-03-04 18:48:30 | INFO | fairseq.trainer | begin training epoch 77
2022-03-04 18:48:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:49:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:52:00 | INFO | train_inner | epoch 077:     81 / 97 loss=4.446, nll_loss=2.4, ppl=5.28, wps=24332.6, ups=0.37, wpb=65495, bsz=127.9, num_updates=7400, lr=0.000367607, gnorm=1.03, loss_scale=8, train_wall=238, gb_free=21, wall=19727
2022-03-04 18:52:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:52:46 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 11.444 | nll_loss 10.211 | ppl 1185.51 | wps 43256 | wpb 510.9 | bsz 1 | num_updates 7416 | best_loss 8.737
2022-03-04 18:52:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 7416 updates
2022-03-04 18:52:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:52:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:52:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 77 @ 7416 updates, score 11.444) (writing took 2.653802001848817 seconds)
2022-03-04 18:52:49 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-04 18:52:49 | INFO | train | epoch 077 | loss 4.44 | nll_loss 2.394 | ppl 5.26 | wps 24293 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7416 | lr 0.000367211 | gnorm 1.025 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 19776
2022-03-04 18:52:49 | INFO | fairseq.trainer | begin training epoch 78
2022-03-04 18:52:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:56:26 | INFO | train_inner | epoch 078:     84 / 97 loss=4.422, nll_loss=2.371, ppl=5.17, wps=24634.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=7500, lr=0.000365148, gnorm=1.026, loss_scale=16, train_wall=235, gb_free=21, wall=19993
2022-03-04 18:56:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:57:04 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 11.49 | nll_loss 10.264 | ppl 1229.37 | wps 44730.9 | wpb 510.9 | bsz 1 | num_updates 7513 | best_loss 8.737
2022-03-04 18:57:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 7513 updates
2022-03-04 18:57:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:57:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 18:57:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 78 @ 7513 updates, score 11.49) (writing took 2.6269017476588488 seconds)
2022-03-04 18:57:07 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-04 18:57:07 | INFO | train | epoch 078 | loss 4.414 | nll_loss 2.363 | ppl 5.14 | wps 24645.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7513 | lr 0.000364832 | gnorm 1.029 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 20034
2022-03-04 18:57:07 | INFO | fairseq.trainer | begin training epoch 79
2022-03-04 18:57:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:00:52 | INFO | train_inner | epoch 079:     87 / 97 loss=4.392, nll_loss=2.336, ppl=5.05, wps=24612.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=7600, lr=0.000362738, gnorm=1.032, loss_scale=32, train_wall=236, gb_free=21, wall=20259
2022-03-04 19:01:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:01:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:01:23 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 11.569 | nll_loss 10.349 | ppl 1304.12 | wps 43005.3 | wpb 510.9 | bsz 1 | num_updates 7609 | best_loss 8.737
2022-03-04 19:01:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 7609 updates
2022-03-04 19:01:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:01:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:01:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 79 @ 7609 updates, score 11.569) (writing took 2.634797696955502 seconds)
2022-03-04 19:01:25 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-04 19:01:25 | INFO | train | epoch 079 | loss 4.386 | nll_loss 2.329 | ppl 5.02 | wps 24309.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7609 | lr 0.000362524 | gnorm 1.034 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 20292
2022-03-04 19:01:25 | INFO | fairseq.trainer | begin training epoch 80
2022-03-04 19:01:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:05:21 | INFO | train_inner | epoch 080:     91 / 97 loss=4.367, nll_loss=2.307, ppl=4.95, wps=24373.5, ups=0.37, wpb=65495, bsz=127.9, num_updates=7700, lr=0.000360375, gnorm=1.018, loss_scale=16, train_wall=238, gb_free=21, wall=20528
2022-03-04 19:05:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:05:41 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 11.594 | nll_loss 10.362 | ppl 1315.69 | wps 43816.8 | wpb 510.9 | bsz 1 | num_updates 7706 | best_loss 8.737
2022-03-04 19:05:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 7706 updates
2022-03-04 19:05:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:05:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:05:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 80 @ 7706 updates, score 11.594) (writing took 2.5780683290213346 seconds)
2022-03-04 19:05:44 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-04 19:05:44 | INFO | train | epoch 080 | loss 4.362 | nll_loss 2.301 | ppl 4.93 | wps 24593 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7706 | lr 0.000360235 | gnorm 1.018 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 20551
2022-03-04 19:05:44 | INFO | fairseq.trainer | begin training epoch 81
2022-03-04 19:05:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:06:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:09:48 | INFO | train_inner | epoch 081:     95 / 97 loss=4.341, nll_loss=2.276, ppl=4.84, wps=24458.3, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=7800, lr=0.000358057, gnorm=1.026, loss_scale=16, train_wall=237, gb_free=21, wall=20795
2022-03-04 19:09:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:09:58 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 11.566 | nll_loss 10.35 | ppl 1305.35 | wps 45363.8 | wpb 510.9 | bsz 1 | num_updates 7802 | best_loss 8.737
2022-03-04 19:09:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 7802 updates
2022-03-04 19:09:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:10:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:10:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 81 @ 7802 updates, score 11.566) (writing took 2.4759289119392633 seconds)
2022-03-04 19:10:01 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-04 19:10:01 | INFO | train | epoch 081 | loss 4.337 | nll_loss 2.27 | ppl 4.82 | wps 24461.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7802 | lr 0.000358012 | gnorm 1.024 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 20808
2022-03-04 19:10:01 | INFO | fairseq.trainer | begin training epoch 82
2022-03-04 19:10:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:12:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:14:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:14:15 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 11.65 | nll_loss 10.44 | ppl 1389.43 | wps 44852.7 | wpb 510.9 | bsz 1 | num_updates 7898 | best_loss 8.737
2022-03-04 19:14:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 7898 updates
2022-03-04 19:14:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:14:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:14:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 82 @ 7898 updates, score 11.65) (writing took 2.536436247639358 seconds)
2022-03-04 19:14:18 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-04 19:14:18 | INFO | train | epoch 082 | loss 4.314 | nll_loss 2.243 | ppl 4.73 | wps 24466.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 7898 | lr 0.000355829 | gnorm 1.017 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 21065
2022-03-04 19:14:18 | INFO | fairseq.trainer | begin training epoch 83
2022-03-04 19:14:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:14:23 | INFO | train_inner | epoch 083:      2 / 97 loss=4.314, nll_loss=2.244, ppl=4.74, wps=23837.4, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=7900, lr=0.000355784, gnorm=1.016, loss_scale=16, train_wall=237, gb_free=21, wall=21070
2022-03-04 19:18:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:18:32 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 11.688 | nll_loss 10.477 | ppl 1425.3 | wps 45471.2 | wpb 510.9 | bsz 1 | num_updates 7995 | best_loss 8.737
2022-03-04 19:18:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 7995 updates
2022-03-04 19:18:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:18:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:18:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 83 @ 7995 updates, score 11.688) (writing took 2.4863046621903777 seconds)
2022-03-04 19:18:35 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-04 19:18:35 | INFO | train | epoch 083 | loss 4.291 | nll_loss 2.217 | ppl 4.65 | wps 24712.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 7995 | lr 0.000353664 | gnorm 1.016 | loss_scale 32 | train_wall 228 | gb_free 21 | wall 21322
2022-03-04 19:18:35 | INFO | fairseq.trainer | begin training epoch 84
2022-03-04 19:18:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:18:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:18:50 | INFO | train_inner | epoch 084:      6 / 97 loss=4.286, nll_loss=2.211, ppl=4.63, wps=24498, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8000, lr=0.000353553, gnorm=1.016, loss_scale=16, train_wall=237, gb_free=21, wall=21337
2022-03-04 19:22:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:22:49 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 11.725 | nll_loss 10.526 | ppl 1474.63 | wps 44679.8 | wpb 510.9 | bsz 1 | num_updates 8091 | best_loss 8.737
2022-03-04 19:22:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 8091 updates
2022-03-04 19:22:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:22:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:22:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 84 @ 8091 updates, score 11.725) (writing took 2.479370907880366 seconds)
2022-03-04 19:22:52 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-04 19:22:52 | INFO | train | epoch 084 | loss 4.267 | nll_loss 2.188 | ppl 4.56 | wps 24472.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8091 | lr 0.00035156 | gnorm 1.022 | loss_scale 16 | train_wall 227 | gb_free 21 | wall 21579
2022-03-04 19:22:52 | INFO | fairseq.trainer | begin training epoch 85
2022-03-04 19:22:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:23:15 | INFO | train_inner | epoch 085:      9 / 97 loss=4.264, nll_loss=2.184, ppl=4.54, wps=24751.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8100, lr=0.000351364, gnorm=1.02, loss_scale=16, train_wall=235, gb_free=21, wall=21602
2022-03-04 19:24:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:27:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:27:07 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 11.756 | nll_loss 10.548 | ppl 1497.17 | wps 43921.9 | wpb 510.9 | bsz 1 | num_updates 8187 | best_loss 8.737
2022-03-04 19:27:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 8187 updates
2022-03-04 19:27:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:27:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:27:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 85 @ 8187 updates, score 11.756) (writing took 2.494495989754796 seconds)
2022-03-04 19:27:09 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-04 19:27:09 | INFO | train | epoch 085 | loss 4.245 | nll_loss 2.162 | ppl 4.48 | wps 24417.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8187 | lr 0.000349492 | gnorm 1.024 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 21836
2022-03-04 19:27:09 | INFO | fairseq.trainer | begin training epoch 86
2022-03-04 19:27:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:27:43 | INFO | train_inner | epoch 086:     13 / 97 loss=4.239, nll_loss=2.156, ppl=4.46, wps=24437.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8200, lr=0.000349215, gnorm=1.03, loss_scale=16, train_wall=237, gb_free=21, wall=21870
2022-03-04 19:30:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:31:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:31:24 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 11.848 | nll_loss 10.663 | ppl 1621.22 | wps 45422.8 | wpb 510.9 | bsz 1 | num_updates 8283 | best_loss 8.737
2022-03-04 19:31:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 8283 updates
2022-03-04 19:31:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:31:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:31:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 86 @ 8283 updates, score 11.848) (writing took 2.452445729635656 seconds)
2022-03-04 19:31:27 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-04 19:31:27 | INFO | train | epoch 086 | loss 4.224 | nll_loss 2.137 | ppl 4.4 | wps 24434.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8283 | lr 0.000347461 | gnorm 1.032 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 22093
2022-03-04 19:31:27 | INFO | fairseq.trainer | begin training epoch 87
2022-03-04 19:31:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:32:10 | INFO | train_inner | epoch 087:     17 / 97 loss=4.219, nll_loss=2.131, ppl=4.38, wps=24502.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8300, lr=0.000347105, gnorm=1.024, loss_scale=16, train_wall=237, gb_free=21, wall=22137
2022-03-04 19:35:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:35:42 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 11.774 | nll_loss 10.566 | ppl 1516.4 | wps 43464.7 | wpb 510.9 | bsz 1 | num_updates 8380 | best_loss 8.737
2022-03-04 19:35:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 8380 updates
2022-03-04 19:35:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:35:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:35:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 87 @ 8380 updates, score 11.774) (writing took 2.4805169720202684 seconds)
2022-03-04 19:35:44 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-04 19:35:44 | INFO | train | epoch 087 | loss 4.204 | nll_loss 2.114 | ppl 4.33 | wps 24656.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8380 | lr 0.000345444 | gnorm 1.02 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 22351
2022-03-04 19:35:44 | INFO | fairseq.trainer | begin training epoch 88
2022-03-04 19:35:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:36:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:36:38 | INFO | train_inner | epoch 088:     21 / 97 loss=4.199, nll_loss=2.108, ppl=4.31, wps=24429.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8400, lr=0.000345033, gnorm=1.026, loss_scale=16, train_wall=237, gb_free=21, wall=22405
2022-03-04 19:39:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:39:59 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 11.829 | nll_loss 10.631 | ppl 1585.58 | wps 43347.9 | wpb 510.9 | bsz 1 | num_updates 8476 | best_loss 8.737
2022-03-04 19:39:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 8476 updates
2022-03-04 19:39:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:40:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:40:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 88 @ 8476 updates, score 11.829) (writing took 2.513022412545979 seconds)
2022-03-04 19:40:02 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-04 19:40:02 | INFO | train | epoch 088 | loss 4.183 | nll_loss 2.089 | ppl 4.25 | wps 24397.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8476 | lr 0.000343482 | gnorm 1.017 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 22609
2022-03-04 19:40:02 | INFO | fairseq.trainer | begin training epoch 89
2022-03-04 19:40:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:41:04 | INFO | train_inner | epoch 089:     24 / 97 loss=4.175, nll_loss=2.08, ppl=4.23, wps=24658.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=8500, lr=0.000342997, gnorm=1.009, loss_scale=16, train_wall=235, gb_free=21, wall=22671
2022-03-04 19:41:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:44:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:44:18 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 11.839 | nll_loss 10.645 | ppl 1601.16 | wps 42686.2 | wpb 510.9 | bsz 1 | num_updates 8572 | best_loss 8.737
2022-03-04 19:44:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 8572 updates
2022-03-04 19:44:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:44:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:44:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 89 @ 8572 updates, score 11.839) (writing took 2.4882372114807367 seconds)
2022-03-04 19:44:20 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-04 19:44:20 | INFO | train | epoch 089 | loss 4.162 | nll_loss 2.064 | ppl 4.18 | wps 24355.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8572 | lr 0.000341554 | gnorm 1.016 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 22867
2022-03-04 19:44:20 | INFO | fairseq.trainer | begin training epoch 90
2022-03-04 19:44:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:45:33 | INFO | train_inner | epoch 090:     28 / 97 loss=4.155, nll_loss=2.056, ppl=4.16, wps=24382.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8600, lr=0.000340997, gnorm=1.016, loss_scale=16, train_wall=238, gb_free=21, wall=22940
2022-03-04 19:47:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:48:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:48:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:48:36 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 11.931 | nll_loss 10.75 | ppl 1722 | wps 44194.5 | wpb 510.9 | bsz 1 | num_updates 8667 | best_loss 8.737
2022-03-04 19:48:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 8667 updates
2022-03-04 19:48:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:48:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:48:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 90 @ 8667 updates, score 11.931) (writing took 2.485164829529822 seconds)
2022-03-04 19:48:38 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-04 19:48:38 | INFO | train | epoch 090 | loss 4.142 | nll_loss 2.041 | ppl 4.12 | wps 24077.2 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 8667 | lr 0.000339677 | gnorm 1.014 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 23125
2022-03-04 19:48:38 | INFO | fairseq.trainer | begin training epoch 91
2022-03-04 19:48:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:50:04 | INFO | train_inner | epoch 091:     33 / 97 loss=4.136, nll_loss=2.033, ppl=4.09, wps=24141.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=8700, lr=0.000339032, gnorm=1.021, loss_scale=8, train_wall=240, gb_free=21, wall=23211
2022-03-04 19:52:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:52:54 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 11.922 | nll_loss 10.738 | ppl 1707.53 | wps 45140.5 | wpb 510.9 | bsz 1 | num_updates 8764 | best_loss 8.737
2022-03-04 19:52:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 8764 updates
2022-03-04 19:52:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:52:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:52:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 91 @ 8764 updates, score 11.922) (writing took 2.566746272146702 seconds)
2022-03-04 19:52:57 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-04 19:52:57 | INFO | train | epoch 091 | loss 4.126 | nll_loss 2.022 | ppl 4.06 | wps 24608.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8764 | lr 0.000337792 | gnorm 1.024 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 23384
2022-03-04 19:52:57 | INFO | fairseq.trainer | begin training epoch 92
2022-03-04 19:52:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:54:30 | INFO | train_inner | epoch 092:     36 / 97 loss=4.119, nll_loss=2.014, ppl=4.04, wps=24652.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=8800, lr=0.0003371, gnorm=1.015, loss_scale=16, train_wall=235, gb_free=21, wall=23477
2022-03-04 19:57:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:57:11 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 11.985 | nll_loss 10.818 | ppl 1804.79 | wps 45590.7 | wpb 510.9 | bsz 1 | num_updates 8861 | best_loss 8.737
2022-03-04 19:57:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 8861 updates
2022-03-04 19:57:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:57:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 19:57:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 92 @ 8861 updates, score 11.985) (writing took 2.5307318354025483 seconds)
2022-03-04 19:57:14 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-04 19:57:14 | INFO | train | epoch 092 | loss 4.109 | nll_loss 2.002 | ppl 4 | wps 24691.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 8861 | lr 0.000335938 | gnorm 1.019 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 23641
2022-03-04 19:57:14 | INFO | fairseq.trainer | begin training epoch 93
2022-03-04 19:57:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:58:55 | INFO | train_inner | epoch 093:     39 / 97 loss=4.101, nll_loss=1.993, ppl=3.98, wps=24710.2, ups=0.38, wpb=65495, bsz=127.9, num_updates=8900, lr=0.000335201, gnorm=1.018, loss_scale=16, train_wall=235, gb_free=21, wall=23742
2022-03-04 19:59:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:01:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:01:29 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 11.996 | nll_loss 10.82 | ppl 1807.19 | wps 44216.6 | wpb 510.9 | bsz 1 | num_updates 8957 | best_loss 8.737
2022-03-04 20:01:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 8957 updates
2022-03-04 20:01:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:01:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:01:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 93 @ 8957 updates, score 11.996) (writing took 2.438584220595658 seconds)
2022-03-04 20:01:31 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-04 20:01:31 | INFO | train | epoch 093 | loss 4.089 | nll_loss 1.978 | ppl 3.94 | wps 24410.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 8957 | lr 0.000334132 | gnorm 1.011 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 23898
2022-03-04 20:01:31 | INFO | fairseq.trainer | begin training epoch 94
2022-03-04 20:01:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:03:23 | INFO | train_inner | epoch 094:     43 / 97 loss=4.08, nll_loss=1.968, ppl=3.91, wps=24441.6, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=9000, lr=0.000333333, gnorm=1.008, loss_scale=16, train_wall=238, gb_free=21, wall=24010
2022-03-04 20:05:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:05:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:05:47 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 12.038 | nll_loss 10.886 | ppl 1892.93 | wps 43867.9 | wpb 510.9 | bsz 1 | num_updates 9053 | best_loss 8.737
2022-03-04 20:05:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 9053 updates
2022-03-04 20:05:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:05:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:05:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 94 @ 9053 updates, score 12.038) (writing took 2.5121752386912704 seconds)
2022-03-04 20:05:49 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-04 20:05:49 | INFO | train | epoch 094 | loss 4.072 | nll_loss 1.959 | ppl 3.89 | wps 24380.2 | ups 0.37 | wpb 65533.8 | bsz 128 | num_updates 9053 | lr 0.000332356 | gnorm 1.017 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 24156
2022-03-04 20:05:49 | INFO | fairseq.trainer | begin training epoch 95
2022-03-04 20:05:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:07:51 | INFO | train_inner | epoch 095:     47 / 97 loss=4.066, nll_loss=1.952, ppl=3.87, wps=24427.1, ups=0.37, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=1.016, loss_scale=16, train_wall=238, gb_free=21, wall=24278
2022-03-04 20:10:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:10:05 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 12.007 | nll_loss 10.838 | ppl 1830.63 | wps 42841.4 | wpb 510.9 | bsz 1 | num_updates 9150 | best_loss 8.737
2022-03-04 20:10:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 9150 updates
2022-03-04 20:10:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:10:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:10:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 95 @ 9150 updates, score 12.007) (writing took 2.4719654135406017 seconds)
2022-03-04 20:10:07 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-04 20:10:07 | INFO | train | epoch 095 | loss 4.056 | nll_loss 1.94 | ppl 3.84 | wps 24628.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9150 | lr 0.00033059 | gnorm 1.019 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 24414
2022-03-04 20:10:07 | INFO | fairseq.trainer | begin training epoch 96
2022-03-04 20:10:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:11:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:12:19 | INFO | train_inner | epoch 096:     51 / 97 loss=4.047, nll_loss=1.93, ppl=3.81, wps=24439.6, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=9200, lr=0.00032969, gnorm=1.023, loss_scale=16, train_wall=237, gb_free=21, wall=24546
2022-03-04 20:14:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:14:22 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 12.054 | nll_loss 10.895 | ppl 1903.82 | wps 45425.7 | wpb 510.9 | bsz 1 | num_updates 9246 | best_loss 8.737
2022-03-04 20:14:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 9246 updates
2022-03-04 20:14:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:14:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:14:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 96 @ 9246 updates, score 12.054) (writing took 2.520336711779237 seconds)
2022-03-04 20:14:25 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-04 20:14:25 | INFO | train | epoch 096 | loss 4.038 | nll_loss 1.919 | ppl 3.78 | wps 24414.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9246 | lr 0.000328869 | gnorm 1.012 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 24672
2022-03-04 20:14:25 | INFO | fairseq.trainer | begin training epoch 97
2022-03-04 20:14:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:16:44 | INFO | train_inner | epoch 097:     54 / 97 loss=4.031, nll_loss=1.911, ppl=3.76, wps=24706, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=9300, lr=0.000327913, gnorm=1.018, loss_scale=16, train_wall=235, gb_free=21, wall=24811
2022-03-04 20:17:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:18:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:18:40 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 12.07 | nll_loss 10.91 | ppl 1923.64 | wps 45370.9 | wpb 510.9 | bsz 1 | num_updates 9342 | best_loss 8.737
2022-03-04 20:18:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 9342 updates
2022-03-04 20:18:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:18:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:18:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 97 @ 9342 updates, score 12.07) (writing took 2.507379649206996 seconds)
2022-03-04 20:18:42 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-04 20:18:42 | INFO | train | epoch 097 | loss 4.021 | nll_loss 1.899 | ppl 3.73 | wps 24450.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9342 | lr 0.000327175 | gnorm 1.014 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 24929
2022-03-04 20:18:42 | INFO | fairseq.trainer | begin training epoch 98
2022-03-04 20:18:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:21:11 | INFO | train_inner | epoch 098:     58 / 97 loss=4.011, nll_loss=1.888, ppl=3.7, wps=24496.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=9400, lr=0.000326164, gnorm=1.025, loss_scale=16, train_wall=237, gb_free=21, wall=25078
2022-03-04 20:22:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:22:57 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 12.106 | nll_loss 10.948 | ppl 1975.63 | wps 44190 | wpb 510.9 | bsz 1 | num_updates 9439 | best_loss 8.737
2022-03-04 20:22:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 9439 updates
2022-03-04 20:22:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:22:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:22:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 98 @ 9439 updates, score 12.106) (writing took 2.560496944002807 seconds)
2022-03-04 20:22:59 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-04 20:22:59 | INFO | train | epoch 098 | loss 4.008 | nll_loss 1.884 | ppl 3.69 | wps 24690.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9439 | lr 0.00032549 | gnorm 1.033 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 25186
2022-03-04 20:22:59 | INFO | fairseq.trainer | begin training epoch 99
2022-03-04 20:22:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:23:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:25:39 | INFO | train_inner | epoch 099:     62 / 97 loss=4, nll_loss=1.874, ppl=3.67, wps=24421, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=9500, lr=0.000324443, gnorm=1.029, loss_scale=16, train_wall=238, gb_free=21, wall=25346
2022-03-04 20:27:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:27:15 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 12.081 | nll_loss 10.907 | ppl 1920.57 | wps 43737.7 | wpb 510.9 | bsz 1 | num_updates 9535 | best_loss 8.737
2022-03-04 20:27:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 9535 updates
2022-03-04 20:27:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:27:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:27:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 99 @ 9535 updates, score 12.081) (writing took 2.5190915539860725 seconds)
2022-03-04 20:27:18 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-04 20:27:18 | INFO | train | epoch 099 | loss 3.991 | nll_loss 1.864 | ppl 3.64 | wps 24347.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9535 | lr 0.000323847 | gnorm 1.028 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 25445
2022-03-04 20:27:18 | INFO | fairseq.trainer | begin training epoch 100
2022-03-04 20:27:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:29:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:30:08 | INFO | train_inner | epoch 100:     66 / 97 loss=3.983, nll_loss=1.854, ppl=3.62, wps=24398.4, ups=0.37, wpb=65495, bsz=127.9, num_updates=9600, lr=0.000322749, gnorm=1.023, loss_scale=16, train_wall=238, gb_free=21, wall=25615
2022-03-04 20:31:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:31:33 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 12.136 | nll_loss 10.993 | ppl 2038.19 | wps 43251.7 | wpb 510.9 | bsz 1 | num_updates 9631 | best_loss 8.737
2022-03-04 20:31:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 9631 updates
2022-03-04 20:31:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:31:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:31:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 100 @ 9631 updates, score 12.136) (writing took 2.4876359468325973 seconds)
2022-03-04 20:31:35 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-04 20:31:35 | INFO | train | epoch 100 | loss 3.978 | nll_loss 1.849 | ppl 3.6 | wps 24406 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9631 | lr 0.000322229 | gnorm 1.039 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 25702
2022-03-04 20:31:35 | INFO | fairseq.trainer | begin training epoch 101
2022-03-04 20:31:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:34:34 | INFO | train_inner | epoch 101:     69 / 97 loss=3.968, nll_loss=1.838, ppl=3.57, wps=24634.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=9700, lr=0.000321081, gnorm=1.03, loss_scale=16, train_wall=235, gb_free=21, wall=25881
2022-03-04 20:34:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:35:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:35:51 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 12.133 | nll_loss 10.985 | ppl 2026.61 | wps 45320.1 | wpb 510.9 | bsz 1 | num_updates 9727 | best_loss 8.737
2022-03-04 20:35:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 9727 updates
2022-03-04 20:35:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:35:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:35:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 101 @ 9727 updates, score 12.133) (writing took 2.454987426288426 seconds)
2022-03-04 20:35:53 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-04 20:35:53 | INFO | train | epoch 101 | loss 3.962 | nll_loss 1.831 | ppl 3.56 | wps 24367.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9727 | lr 0.000320635 | gnorm 1.014 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 25960
2022-03-04 20:35:53 | INFO | fairseq.trainer | begin training epoch 102
2022-03-04 20:35:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:39:02 | INFO | train_inner | epoch 102:     73 / 97 loss=3.954, nll_loss=1.821, ppl=3.53, wps=24404.5, ups=0.37, wpb=65495, bsz=127.9, num_updates=9800, lr=0.000319438, gnorm=1.017, loss_scale=16, train_wall=238, gb_free=21, wall=26149
2022-03-04 20:40:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:40:09 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 12.151 | nll_loss 10.995 | ppl 2041.14 | wps 44946.7 | wpb 510.9 | bsz 1 | num_updates 9824 | best_loss 8.737
2022-03-04 20:40:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 9824 updates
2022-03-04 20:40:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:40:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:40:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 102 @ 9824 updates, score 12.151) (writing took 2.4419163884595037 seconds)
2022-03-04 20:40:11 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-04 20:40:11 | INFO | train | epoch 102 | loss 3.948 | nll_loss 1.815 | ppl 3.52 | wps 24612.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 9824 | lr 0.000319048 | gnorm 1.013 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 26218
2022-03-04 20:40:11 | INFO | fairseq.trainer | begin training epoch 103
2022-03-04 20:40:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:40:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:43:30 | INFO | train_inner | epoch 103:     77 / 97 loss=3.936, nll_loss=1.8, ppl=3.48, wps=24434.9, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=9900, lr=0.000317821, gnorm=1.009, loss_scale=16, train_wall=238, gb_free=21, wall=26417
2022-03-04 20:44:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:44:26 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 12.231 | nll_loss 11.093 | ppl 2183.65 | wps 45379.7 | wpb 510.9 | bsz 1 | num_updates 9920 | best_loss 8.737
2022-03-04 20:44:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 9920 updates
2022-03-04 20:44:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:44:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:44:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 103 @ 9920 updates, score 12.231) (writing took 2.6318756341934204 seconds)
2022-03-04 20:44:29 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-04 20:44:29 | INFO | train | epoch 103 | loss 3.934 | nll_loss 1.798 | ppl 3.48 | wps 24388.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 9920 | lr 0.0003175 | gnorm 1.011 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 26476
2022-03-04 20:44:29 | INFO | fairseq.trainer | begin training epoch 104
2022-03-04 20:44:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:46:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:47:58 | INFO | train_inner | epoch 104:     81 / 97 loss=3.928, nll_loss=1.791, ppl=3.46, wps=24428.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10000, lr=0.000316228, gnorm=1.012, loss_scale=16, train_wall=238, gb_free=21, wall=26685
2022-03-04 20:48:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:48:44 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 12.208 | nll_loss 11.066 | ppl 2143.58 | wps 44484.6 | wpb 510.9 | bsz 1 | num_updates 10016 | best_loss 8.737
2022-03-04 20:48:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 10016 updates
2022-03-04 20:48:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:48:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:48:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 104 @ 10016 updates, score 12.208) (writing took 2.403136035427451 seconds)
2022-03-04 20:48:47 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-04 20:48:47 | INFO | train | epoch 104 | loss 3.921 | nll_loss 1.783 | ppl 3.44 | wps 24392.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10016 | lr 0.000315975 | gnorm 1.012 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 26734
2022-03-04 20:48:47 | INFO | fairseq.trainer | begin training epoch 105
2022-03-04 20:48:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:52:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:52:26 | INFO | train_inner | epoch 105:     85 / 97 loss=3.911, nll_loss=1.771, ppl=3.41, wps=24418.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10100, lr=0.000314658, gnorm=1.008, loss_scale=16, train_wall=238, gb_free=21, wall=26953
2022-03-04 20:52:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:53:02 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 12.215 | nll_loss 11.067 | ppl 2145.59 | wps 43295.6 | wpb 510.9 | bsz 1 | num_updates 10112 | best_loss 8.737
2022-03-04 20:53:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 10112 updates
2022-03-04 20:53:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:53:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:53:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 105 @ 10112 updates, score 12.215) (writing took 2.429795816540718 seconds)
2022-03-04 20:53:05 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-04 20:53:05 | INFO | train | epoch 105 | loss 3.907 | nll_loss 1.767 | ppl 3.4 | wps 24369.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10112 | lr 0.000314472 | gnorm 1.012 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 26992
2022-03-04 20:53:05 | INFO | fairseq.trainer | begin training epoch 106
2022-03-04 20:53:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:56:52 | INFO | train_inner | epoch 106:     88 / 97 loss=3.896, nll_loss=1.754, ppl=3.37, wps=24618.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10200, lr=0.000313112, gnorm=1.016, loss_scale=16, train_wall=236, gb_free=21, wall=27219
2022-03-04 20:57:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:57:21 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 12.247 | nll_loss 11.1 | ppl 2195.53 | wps 43392 | wpb 510.9 | bsz 1 | num_updates 10209 | best_loss 8.737
2022-03-04 20:57:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 10209 updates
2022-03-04 20:57:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:57:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 20:57:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 106 @ 10209 updates, score 12.247) (writing took 2.474824981763959 seconds)
2022-03-04 20:57:23 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-04 20:57:23 | INFO | train | epoch 106 | loss 3.894 | nll_loss 1.752 | ppl 3.37 | wps 24597.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10209 | lr 0.000312974 | gnorm 1.011 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 27250
2022-03-04 20:57:23 | INFO | fairseq.trainer | begin training epoch 107
2022-03-04 20:57:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:58:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:01:21 | INFO | train_inner | epoch 107:     92 / 97 loss=3.886, nll_loss=1.743, ppl=3.35, wps=24384, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10300, lr=0.000311588, gnorm=1.009, loss_scale=16, train_wall=238, gb_free=21, wall=27488
2022-03-04 21:01:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:01:39 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 12.282 | nll_loss 11.141 | ppl 2257.71 | wps 43394.8 | wpb 510.9 | bsz 1 | num_updates 10305 | best_loss 8.737
2022-03-04 21:01:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 10305 updates
2022-03-04 21:01:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:01:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:01:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 107 @ 10305 updates, score 12.282) (writing took 2.5047814287245274 seconds)
2022-03-04 21:01:41 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-04 21:01:41 | INFO | train | epoch 107 | loss 3.882 | nll_loss 1.737 | ppl 3.33 | wps 24331.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10305 | lr 0.000311513 | gnorm 1.007 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 27508
2022-03-04 21:01:41 | INFO | fairseq.trainer | begin training epoch 108
2022-03-04 21:01:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:04:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:05:50 | INFO | train_inner | epoch 108:     96 / 97 loss=3.871, nll_loss=1.726, ppl=3.31, wps=24393.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10400, lr=0.000310087, gnorm=1.012, loss_scale=16, train_wall=238, gb_free=21, wall=27757
2022-03-04 21:05:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:05:57 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 12.261 | nll_loss 11.118 | ppl 2222.21 | wps 44264.1 | wpb 510.9 | bsz 1 | num_updates 10401 | best_loss 8.737
2022-03-04 21:05:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 10401 updates
2022-03-04 21:05:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:05:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:05:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 108 @ 10401 updates, score 12.261) (writing took 2.48126567248255 seconds)
2022-03-04 21:05:59 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-04 21:05:59 | INFO | train | epoch 108 | loss 3.868 | nll_loss 1.722 | ppl 3.3 | wps 24371.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10401 | lr 0.000310072 | gnorm 1.01 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 27766
2022-03-04 21:05:59 | INFO | fairseq.trainer | begin training epoch 109
2022-03-04 21:05:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:09:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:10:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:10:15 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 12.311 | nll_loss 11.19 | ppl 2335.92 | wps 45133.6 | wpb 510.9 | bsz 1 | num_updates 10497 | best_loss 8.737
2022-03-04 21:10:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 10497 updates
2022-03-04 21:10:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:10:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:10:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 109 @ 10497 updates, score 12.311) (writing took 2.4492384903132915 seconds)
2022-03-04 21:10:17 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-04 21:10:17 | INFO | train | epoch 109 | loss 3.857 | nll_loss 1.708 | ppl 3.27 | wps 24378.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10497 | lr 0.000308651 | gnorm 1.01 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 28024
2022-03-04 21:10:17 | INFO | fairseq.trainer | begin training epoch 110
2022-03-04 21:10:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:10:25 | INFO | train_inner | epoch 110:      3 / 97 loss=3.855, nll_loss=1.707, ppl=3.26, wps=23739, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=10500, lr=0.000308607, gnorm=1.009, loss_scale=16, train_wall=238, gb_free=21, wall=28032
2022-03-04 21:14:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:14:33 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 12.329 | nll_loss 11.202 | ppl 2355.16 | wps 45044.9 | wpb 510.9 | bsz 1 | num_updates 10594 | best_loss 8.737
2022-03-04 21:14:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 10594 updates
2022-03-04 21:14:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:14:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:14:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 110 @ 10594 updates, score 12.329) (writing took 2.5282408809289336 seconds)
2022-03-04 21:14:35 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-04 21:14:35 | INFO | train | epoch 110 | loss 3.846 | nll_loss 1.697 | ppl 3.24 | wps 24611.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10594 | lr 0.000307235 | gnorm 0.997 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 28282
2022-03-04 21:14:35 | INFO | fairseq.trainer | begin training epoch 111
2022-03-04 21:14:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:14:51 | INFO | train_inner | epoch 111:      6 / 97 loss=3.845, nll_loss=1.695, ppl=3.24, wps=24631.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10600, lr=0.000307148, gnorm=0.996, loss_scale=16, train_wall=236, gb_free=21, wall=28298
2022-03-04 21:15:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:16:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:18:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:18:51 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 12.35 | nll_loss 11.23 | ppl 2402.05 | wps 44517.5 | wpb 510.9 | bsz 1 | num_updates 10689 | best_loss 8.737
2022-03-04 21:18:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 10689 updates
2022-03-04 21:18:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:18:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:18:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 111 @ 10689 updates, score 12.35) (writing took 2.528536517173052 seconds)
2022-03-04 21:18:54 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-04 21:18:54 | INFO | train | epoch 111 | loss 3.832 | nll_loss 1.68 | ppl 3.2 | wps 24095.1 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 10689 | lr 0.000305866 | gnorm 1.023 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 28541
2022-03-04 21:18:54 | INFO | fairseq.trainer | begin training epoch 112
2022-03-04 21:18:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:19:22 | INFO | train_inner | epoch 112:     11 / 97 loss=3.827, nll_loss=1.675, ppl=3.19, wps=24157.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10700, lr=0.000305709, gnorm=1.02, loss_scale=8, train_wall=240, gb_free=21, wall=28569
2022-03-04 21:23:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:23:09 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 12.386 | nll_loss 11.267 | ppl 2464.88 | wps 43967.6 | wpb 510.9 | bsz 1 | num_updates 10786 | best_loss 8.737
2022-03-04 21:23:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 10786 updates
2022-03-04 21:23:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:23:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:23:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 112 @ 10786 updates, score 12.386) (writing took 2.526283510029316 seconds)
2022-03-04 21:23:12 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-04 21:23:12 | INFO | train | epoch 112 | loss 3.823 | nll_loss 1.67 | ppl 3.18 | wps 24608.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10786 | lr 0.000304488 | gnorm 1.008 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 28799
2022-03-04 21:23:12 | INFO | fairseq.trainer | begin training epoch 113
2022-03-04 21:23:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:23:48 | INFO | train_inner | epoch 113:     14 / 97 loss=3.819, nll_loss=1.666, ppl=3.17, wps=24639.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=10800, lr=0.00030429, gnorm=1.006, loss_scale=16, train_wall=235, gb_free=21, wall=28835
2022-03-04 21:27:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:27:27 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 12.381 | nll_loss 11.268 | ppl 2465.6 | wps 45367.1 | wpb 510.9 | bsz 1 | num_updates 10883 | best_loss 8.737
2022-03-04 21:27:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 10883 updates
2022-03-04 21:27:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:27:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:27:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 113 @ 10883 updates, score 12.381) (writing took 2.5374530935660005 seconds)
2022-03-04 21:27:29 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-04 21:27:29 | INFO | train | epoch 113 | loss 3.811 | nll_loss 1.657 | ppl 3.15 | wps 24669 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 10883 | lr 0.000303128 | gnorm 1.01 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 29056
2022-03-04 21:27:29 | INFO | fairseq.trainer | begin training epoch 114
2022-03-04 21:27:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:27:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:28:16 | INFO | train_inner | epoch 114:     18 / 97 loss=3.809, nll_loss=1.654, ppl=3.15, wps=24453.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=10900, lr=0.000302891, gnorm=1.008, loss_scale=16, train_wall=237, gb_free=21, wall=29103
2022-03-04 21:31:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:31:45 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 12.353 | nll_loss 11.219 | ppl 2383.99 | wps 44135.6 | wpb 510.9 | bsz 1 | num_updates 10979 | best_loss 8.737
2022-03-04 21:31:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 10979 updates
2022-03-04 21:31:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:31:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:31:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 114 @ 10979 updates, score 12.353) (writing took 2.5131544154137373 seconds)
2022-03-04 21:31:47 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-04 21:31:47 | INFO | train | epoch 114 | loss 3.799 | nll_loss 1.642 | ppl 3.12 | wps 24387.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 10979 | lr 0.0003018 | gnorm 1.02 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 29314
2022-03-04 21:31:47 | INFO | fairseq.trainer | begin training epoch 115
2022-03-04 21:31:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:32:41 | INFO | train_inner | epoch 115:     21 / 97 loss=3.797, nll_loss=1.64, ppl=3.12, wps=24661, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=11000, lr=0.000301511, gnorm=1.021, loss_scale=16, train_wall=235, gb_free=21, wall=29368
2022-03-04 21:33:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:35:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:36:03 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 12.345 | nll_loss 11.218 | ppl 2381.76 | wps 42904.7 | wpb 510.9 | bsz 1 | num_updates 11075 | best_loss 8.737
2022-03-04 21:36:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 11075 updates
2022-03-04 21:36:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:36:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:36:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 115 @ 11075 updates, score 12.345) (writing took 2.457911499775946 seconds)
2022-03-04 21:36:05 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-04 21:36:05 | INFO | train | epoch 115 | loss 3.789 | nll_loss 1.631 | ppl 3.1 | wps 24370 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11075 | lr 0.000300489 | gnorm 1.015 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 29572
2022-03-04 21:36:05 | INFO | fairseq.trainer | begin training epoch 116
2022-03-04 21:36:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:37:10 | INFO | train_inner | epoch 116:     25 / 97 loss=3.784, nll_loss=1.625, ppl=3.08, wps=24403.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11100, lr=0.00030015, gnorm=1.019, loss_scale=16, train_wall=238, gb_free=21, wall=29637
2022-03-04 21:39:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:40:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:40:21 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 12.384 | nll_loss 11.266 | ppl 2462.93 | wps 43271.2 | wpb 510.9 | bsz 1 | num_updates 11171 | best_loss 8.737
2022-03-04 21:40:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 11171 updates
2022-03-04 21:40:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:40:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:40:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 116 @ 11171 updates, score 12.384) (writing took 2.4263532115146518 seconds)
2022-03-04 21:40:23 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-04 21:40:23 | INFO | train | epoch 116 | loss 3.777 | nll_loss 1.617 | ppl 3.07 | wps 24381.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11171 | lr 0.000299195 | gnorm 1.006 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 29830
2022-03-04 21:40:23 | INFO | fairseq.trainer | begin training epoch 117
2022-03-04 21:40:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:41:38 | INFO | train_inner | epoch 117:     29 / 97 loss=3.774, nll_loss=1.614, ppl=3.06, wps=24412.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=11200, lr=0.000298807, gnorm=1.009, loss_scale=16, train_wall=238, gb_free=21, wall=29905
2022-03-04 21:43:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:44:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:44:39 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 12.35 | nll_loss 11.222 | ppl 2388.49 | wps 42959.3 | wpb 510.9 | bsz 1 | num_updates 11267 | best_loss 8.737
2022-03-04 21:44:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 11267 updates
2022-03-04 21:44:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:44:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:44:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 117 @ 11267 updates, score 12.35) (writing took 2.413778685964644 seconds)
2022-03-04 21:44:41 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-04 21:44:41 | INFO | train | epoch 117 | loss 3.767 | nll_loss 1.607 | ppl 3.05 | wps 24327.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11267 | lr 0.000297917 | gnorm 1.015 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 30088
2022-03-04 21:44:41 | INFO | fairseq.trainer | begin training epoch 118
2022-03-04 21:44:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:46:07 | INFO | train_inner | epoch 118:     33 / 97 loss=3.761, nll_loss=1.599, ppl=3.03, wps=24369.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11300, lr=0.000297482, gnorm=1.009, loss_scale=8, train_wall=238, gb_free=21, wall=30174
2022-03-04 21:48:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:48:57 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 12.449 | nll_loss 11.342 | ppl 2595.32 | wps 43097.9 | wpb 510.9 | bsz 1 | num_updates 11364 | best_loss 8.737
2022-03-04 21:48:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 11364 updates
2022-03-04 21:48:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:49:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:49:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 118 @ 11364 updates, score 12.449) (writing took 2.4431729540228844 seconds)
2022-03-04 21:49:00 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-04 21:49:00 | INFO | train | epoch 118 | loss 3.759 | nll_loss 1.597 | ppl 3.02 | wps 24588.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11364 | lr 0.000296643 | gnorm 1.019 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 30347
2022-03-04 21:49:00 | INFO | fairseq.trainer | begin training epoch 119
2022-03-04 21:49:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:50:33 | INFO | train_inner | epoch 119:     36 / 97 loss=3.756, nll_loss=1.594, ppl=3.02, wps=24617.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=11400, lr=0.000296174, gnorm=1.014, loss_scale=16, train_wall=236, gb_free=21, wall=30440
2022-03-04 21:52:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:53:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:53:16 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 12.381 | nll_loss 11.256 | ppl 2445.72 | wps 43309.7 | wpb 510.9 | bsz 1 | num_updates 11460 | best_loss 8.737
2022-03-04 21:53:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 11460 updates
2022-03-04 21:53:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:53:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:53:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 119 @ 11460 updates, score 12.381) (writing took 2.582035738043487 seconds)
2022-03-04 21:53:18 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-04 21:53:18 | INFO | train | epoch 119 | loss 3.748 | nll_loss 1.585 | ppl 3 | wps 24329.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11460 | lr 0.000295398 | gnorm 1.005 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 30605
2022-03-04 21:53:18 | INFO | fairseq.trainer | begin training epoch 120
2022-03-04 21:53:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:55:02 | INFO | train_inner | epoch 120:     40 / 97 loss=3.744, nll_loss=1.58, ppl=2.99, wps=24364, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=11500, lr=0.000294884, gnorm=1.007, loss_scale=8, train_wall=238, gb_free=21, wall=30709
2022-03-04 21:57:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:57:34 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 12.464 | nll_loss 11.36 | ppl 2627.9 | wps 43268.9 | wpb 510.9 | bsz 1 | num_updates 11557 | best_loss 8.737
2022-03-04 21:57:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 11557 updates
2022-03-04 21:57:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:57:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 21:57:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 120 @ 11557 updates, score 12.464) (writing took 2.3924215426668525 seconds)
2022-03-04 21:57:37 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-04 21:57:37 | INFO | train | epoch 120 | loss 3.739 | nll_loss 1.574 | ppl 2.98 | wps 24584.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11557 | lr 0.000294156 | gnorm 1.026 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 30864
2022-03-04 21:57:37 | INFO | fairseq.trainer | begin training epoch 121
2022-03-04 21:57:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:59:28 | INFO | train_inner | epoch 121:     43 / 97 loss=3.737, nll_loss=1.571, ppl=2.97, wps=24613.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=11600, lr=0.00029361, gnorm=1.037, loss_scale=16, train_wall=236, gb_free=21, wall=30975
2022-03-04 22:01:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:01:52 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 12.441 | nll_loss 11.342 | ppl 2596.26 | wps 43054.2 | wpb 510.9 | bsz 1 | num_updates 11654 | best_loss 8.737
2022-03-04 22:01:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 11654 updates
2022-03-04 22:01:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:01:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:01:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 121 @ 11654 updates, score 12.441) (writing took 2.4468565927818418 seconds)
2022-03-04 22:01:55 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-04 22:01:55 | INFO | train | epoch 121 | loss 3.726 | nll_loss 1.559 | ppl 2.95 | wps 24607.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11654 | lr 0.000292929 | gnorm 1.002 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 31122
2022-03-04 22:01:55 | INFO | fairseq.trainer | begin training epoch 122
2022-03-04 22:01:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:02:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:03:56 | INFO | train_inner | epoch 122:     47 / 97 loss=3.72, nll_loss=1.552, ppl=2.93, wps=24388.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=11700, lr=0.000292353, gnorm=0.985, loss_scale=8, train_wall=238, gb_free=21, wall=31243
2022-03-04 22:06:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:06:10 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 12.452 | nll_loss 11.342 | ppl 2596.13 | wps 43659.8 | wpb 510.9 | bsz 1 | num_updates 11750 | best_loss 8.737
2022-03-04 22:06:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 11750 updates
2022-03-04 22:06:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:06:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:06:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 122 @ 11750 updates, score 12.452) (writing took 2.510283396579325 seconds)
2022-03-04 22:06:13 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-04 22:06:13 | INFO | train | epoch 122 | loss 3.718 | nll_loss 1.55 | ppl 2.93 | wps 24356.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11750 | lr 0.00029173 | gnorm 0.999 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 31380
2022-03-04 22:06:13 | INFO | fairseq.trainer | begin training epoch 123
2022-03-04 22:06:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:08:22 | INFO | train_inner | epoch 123:     50 / 97 loss=3.716, nll_loss=1.548, ppl=2.92, wps=24619.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=11800, lr=0.000291111, gnorm=1.005, loss_scale=16, train_wall=236, gb_free=21, wall=31509
2022-03-04 22:08:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:10:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:10:29 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 12.428 | nll_loss 11.309 | ppl 2537.96 | wps 44592.2 | wpb 510.9 | bsz 1 | num_updates 11846 | best_loss 8.737
2022-03-04 22:10:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 11846 updates
2022-03-04 22:10:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:10:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:10:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 123 @ 11846 updates, score 12.428) (writing took 2.5350353475660086 seconds)
2022-03-04 22:10:32 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-04 22:10:32 | INFO | train | epoch 123 | loss 3.711 | nll_loss 1.542 | ppl 2.91 | wps 24310.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 11846 | lr 0.000290545 | gnorm 1.013 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 31638
2022-03-04 22:10:32 | INFO | fairseq.trainer | begin training epoch 124
2022-03-04 22:10:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:12:51 | INFO | train_inner | epoch 124:     54 / 97 loss=3.703, nll_loss=1.533, ppl=2.89, wps=24340.1, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=11900, lr=0.000289886, gnorm=1.014, loss_scale=8, train_wall=238, gb_free=21, wall=31778
2022-03-04 22:14:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:14:48 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 12.469 | nll_loss 11.372 | ppl 2649.89 | wps 44955.4 | wpb 510.9 | bsz 1 | num_updates 11943 | best_loss 8.737
2022-03-04 22:14:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 11943 updates
2022-03-04 22:14:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:14:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:14:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 124 @ 11943 updates, score 12.469) (writing took 2.4616755610331893 seconds)
2022-03-04 22:14:50 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-04 22:14:50 | INFO | train | epoch 124 | loss 3.701 | nll_loss 1.53 | ppl 2.89 | wps 24578.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 11943 | lr 0.000289363 | gnorm 1 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 31897
2022-03-04 22:14:50 | INFO | fairseq.trainer | begin training epoch 125
2022-03-04 22:14:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:17:18 | INFO | train_inner | epoch 125:     57 / 97 loss=3.697, nll_loss=1.527, ppl=2.88, wps=24606.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=12000, lr=0.000288675, gnorm=1.001, loss_scale=16, train_wall=236, gb_free=21, wall=32045
2022-03-04 22:19:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:19:06 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 12.508 | nll_loss 11.421 | ppl 2742.21 | wps 45025.5 | wpb 510.9 | bsz 1 | num_updates 12040 | best_loss 8.737
2022-03-04 22:19:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 12040 updates
2022-03-04 22:19:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:19:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:19:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 125 @ 12040 updates, score 12.508) (writing took 2.4768718648701906 seconds)
2022-03-04 22:19:08 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-04 22:19:08 | INFO | train | epoch 125 | loss 3.692 | nll_loss 1.521 | ppl 2.87 | wps 24588.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12040 | lr 0.000288195 | gnorm 1.005 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 32155
2022-03-04 22:19:08 | INFO | fairseq.trainer | begin training epoch 126
2022-03-04 22:19:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:20:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:21:47 | INFO | train_inner | epoch 126:     61 / 97 loss=3.687, nll_loss=1.515, ppl=2.86, wps=24350.7, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=12100, lr=0.00028748, gnorm=0.992, loss_scale=16, train_wall=238, gb_free=21, wall=32314
2022-03-04 22:22:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:23:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:23:25 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 12.481 | nll_loss 11.379 | ppl 2663.72 | wps 43648.1 | wpb 510.9 | bsz 1 | num_updates 12135 | best_loss 8.737
2022-03-04 22:23:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 12135 updates
2022-03-04 22:23:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:23:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:23:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 126 @ 12135 updates, score 12.481) (writing took 2.4948452562093735 seconds)
2022-03-04 22:23:27 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-04 22:23:27 | INFO | train | epoch 126 | loss 3.684 | nll_loss 1.512 | ppl 2.85 | wps 24048.2 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 12135 | lr 0.000287065 | gnorm 1.012 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 32414
2022-03-04 22:23:27 | INFO | fairseq.trainer | begin training epoch 127
2022-03-04 22:23:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:26:15 | INFO | train_inner | epoch 127:     65 / 97 loss=3.678, nll_loss=1.506, ppl=2.84, wps=24369.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12200, lr=0.000286299, gnorm=1.004, loss_scale=8, train_wall=238, gb_free=21, wall=32582
2022-03-04 22:27:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:27:43 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 12.473 | nll_loss 11.371 | ppl 2648.62 | wps 43622 | wpb 510.9 | bsz 1 | num_updates 12232 | best_loss 8.737
2022-03-04 22:27:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 12232 updates
2022-03-04 22:27:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:27:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:27:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 127 @ 12232 updates, score 12.473) (writing took 2.4858584692701697 seconds)
2022-03-04 22:27:45 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-04 22:27:45 | INFO | train | epoch 127 | loss 3.675 | nll_loss 1.501 | ppl 2.83 | wps 24584.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12232 | lr 0.000285924 | gnorm 0.98 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 32672
2022-03-04 22:27:45 | INFO | fairseq.trainer | begin training epoch 128
2022-03-04 22:27:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:30:41 | INFO | train_inner | epoch 128:     68 / 97 loss=3.671, nll_loss=1.497, ppl=2.82, wps=24603.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12300, lr=0.000285133, gnorm=0.99, loss_scale=16, train_wall=236, gb_free=21, wall=32848
2022-03-04 22:31:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:32:01 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 12.527 | nll_loss 11.425 | ppl 2749.6 | wps 42992.5 | wpb 510.9 | bsz 1 | num_updates 12329 | best_loss 8.737
2022-03-04 22:32:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 12329 updates
2022-03-04 22:32:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:32:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:32:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 128 @ 12329 updates, score 12.527) (writing took 2.5012667747214437 seconds)
2022-03-04 22:32:04 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-04 22:32:04 | INFO | train | epoch 128 | loss 3.667 | nll_loss 1.492 | ppl 2.81 | wps 24572.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12329 | lr 0.000284797 | gnorm 0.998 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 32931
2022-03-04 22:32:04 | INFO | fairseq.trainer | begin training epoch 129
2022-03-04 22:32:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:34:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:35:10 | INFO | train_inner | epoch 129:     72 / 97 loss=3.663, nll_loss=1.488, ppl=2.8, wps=24391.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12400, lr=0.000283981, gnorm=0.997, loss_scale=16, train_wall=238, gb_free=21, wall=33117
2022-03-04 22:36:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:36:19 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 12.471 | nll_loss 11.375 | ppl 2655.13 | wps 44913.6 | wpb 510.9 | bsz 1 | num_updates 12425 | best_loss 8.737
2022-03-04 22:36:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 12425 updates
2022-03-04 22:36:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:36:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:36:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 129 @ 12425 updates, score 12.471) (writing took 2.532893947325647 seconds)
2022-03-04 22:36:22 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-04 22:36:22 | INFO | train | epoch 129 | loss 3.658 | nll_loss 1.482 | ppl 2.79 | wps 24370.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12425 | lr 0.000283695 | gnorm 0.991 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 33189
2022-03-04 22:36:22 | INFO | fairseq.trainer | begin training epoch 130
2022-03-04 22:36:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:39:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:39:38 | INFO | train_inner | epoch 130:     76 / 97 loss=3.653, nll_loss=1.476, ppl=2.78, wps=24400.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12500, lr=0.000282843, gnorm=0.995, loss_scale=8, train_wall=238, gb_free=21, wall=33385
2022-03-04 22:40:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:40:38 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 12.5 | nll_loss 11.404 | ppl 2710.33 | wps 43488.4 | wpb 510.9 | bsz 1 | num_updates 12521 | best_loss 8.737
2022-03-04 22:40:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 12521 updates
2022-03-04 22:40:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:40:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:40:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 130 @ 12521 updates, score 12.5) (writing took 2.495110822841525 seconds)
2022-03-04 22:40:40 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-04 22:40:40 | INFO | train | epoch 130 | loss 3.65 | nll_loss 1.473 | ppl 2.78 | wps 24356.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12521 | lr 0.000282605 | gnorm 0.994 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 33447
2022-03-04 22:40:40 | INFO | fairseq.trainer | begin training epoch 131
2022-03-04 22:40:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:44:05 | INFO | train_inner | epoch 131:     79 / 97 loss=3.644, nll_loss=1.467, ppl=2.76, wps=24597.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12600, lr=0.000281718, gnorm=0.991, loss_scale=8, train_wall=236, gb_free=21, wall=33652
2022-03-04 22:44:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:44:56 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 12.585 | nll_loss 11.501 | ppl 2898.37 | wps 43130 | wpb 510.9 | bsz 1 | num_updates 12618 | best_loss 8.737
2022-03-04 22:44:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 12618 updates
2022-03-04 22:44:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:44:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:44:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 131 @ 12618 updates, score 12.585) (writing took 2.5374862626194954 seconds)
2022-03-04 22:44:59 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-04 22:44:59 | INFO | train | epoch 131 | loss 3.642 | nll_loss 1.464 | ppl 2.76 | wps 24570 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12618 | lr 0.000281517 | gnorm 0.991 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 33706
2022-03-04 22:44:59 | INFO | fairseq.trainer | begin training epoch 132
2022-03-04 22:44:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:45:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:48:34 | INFO | train_inner | epoch 132:     83 / 97 loss=3.636, nll_loss=1.458, ppl=2.75, wps=24360.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12700, lr=0.000280607, gnorm=0.989, loss_scale=8, train_wall=238, gb_free=21, wall=33920
2022-03-04 22:49:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:49:15 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 12.578 | nll_loss 11.502 | ppl 2900.9 | wps 43396.4 | wpb 510.9 | bsz 1 | num_updates 12714 | best_loss 8.737
2022-03-04 22:49:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 12714 updates
2022-03-04 22:49:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:49:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:49:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 132 @ 12714 updates, score 12.578) (writing took 2.5392208881676197 seconds)
2022-03-04 22:49:17 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-04 22:49:17 | INFO | train | epoch 132 | loss 3.634 | nll_loss 1.456 | ppl 2.74 | wps 24319.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12714 | lr 0.000280452 | gnorm 0.996 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 33964
2022-03-04 22:49:17 | INFO | fairseq.trainer | begin training epoch 133
2022-03-04 22:49:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:51:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:53:03 | INFO | train_inner | epoch 133:     87 / 97 loss=3.63, nll_loss=1.451, ppl=2.73, wps=24335.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=12800, lr=0.000279508, gnorm=1.009, loss_scale=8, train_wall=238, gb_free=21, wall=34190
2022-03-04 22:53:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:53:33 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 12.542 | nll_loss 11.446 | ppl 2790.58 | wps 44928.7 | wpb 510.9 | bsz 1 | num_updates 12810 | best_loss 8.737
2022-03-04 22:53:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 12810 updates
2022-03-04 22:53:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:53:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:53:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 133 @ 12810 updates, score 12.542) (writing took 2.478852830827236 seconds)
2022-03-04 22:53:36 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-04 22:53:36 | INFO | train | epoch 133 | loss 3.627 | nll_loss 1.447 | ppl 2.73 | wps 24307.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 12810 | lr 0.000279399 | gnorm 1 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 34223
2022-03-04 22:53:36 | INFO | fairseq.trainer | begin training epoch 134
2022-03-04 22:53:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:57:29 | INFO | train_inner | epoch 134:     90 / 97 loss=3.62, nll_loss=1.439, ppl=2.71, wps=24595.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=12900, lr=0.000278423, gnorm=0.99, loss_scale=16, train_wall=236, gb_free=21, wall=34456
2022-03-04 22:57:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:57:52 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 12.577 | nll_loss 11.499 | ppl 2893.44 | wps 43177.6 | wpb 510.9 | bsz 1 | num_updates 12907 | best_loss 8.737
2022-03-04 22:57:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 12907 updates
2022-03-04 22:57:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:57:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 22:57:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 134 @ 12907 updates, score 12.577) (writing took 2.4457606682553887 seconds)
2022-03-04 22:57:54 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-04 22:57:54 | INFO | train | epoch 134 | loss 3.619 | nll_loss 1.438 | ppl 2.71 | wps 24567.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 12907 | lr 0.000278348 | gnorm 0.989 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 34481
2022-03-04 22:57:54 | INFO | fairseq.trainer | begin training epoch 135
2022-03-04 22:57:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:01:55 | INFO | train_inner | epoch 135:     93 / 97 loss=3.616, nll_loss=1.436, ppl=2.71, wps=24656.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13000, lr=0.00027735, gnorm=0.985, loss_scale=16, train_wall=235, gb_free=21, wall=34722
2022-03-04 23:02:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:02:10 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 12.629 | nll_loss 11.556 | ppl 3011.92 | wps 44347.7 | wpb 510.9 | bsz 1 | num_updates 13004 | best_loss 8.737
2022-03-04 23:02:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 13004 updates
2022-03-04 23:02:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:02:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:02:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 135 @ 13004 updates, score 12.629) (writing took 2.6397343166172504 seconds)
2022-03-04 23:02:12 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-04 23:02:12 | INFO | train | epoch 135 | loss 3.613 | nll_loss 1.431 | ppl 2.7 | wps 24625.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13004 | lr 0.000277307 | gnorm 0.986 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 34739
2022-03-04 23:02:12 | INFO | fairseq.trainer | begin training epoch 136
2022-03-04 23:02:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:02:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:06:23 | INFO | train_inner | epoch 136:     97 / 97 loss=3.607, nll_loss=1.424, ppl=2.68, wps=24359.4, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=13100, lr=0.000276289, gnorm=0.992, loss_scale=16, train_wall=238, gb_free=21, wall=34990
2022-03-04 23:06:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:06:28 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 12.557 | nll_loss 11.481 | ppl 2857.51 | wps 43588.4 | wpb 510.9 | bsz 1 | num_updates 13100 | best_loss 8.737
2022-03-04 23:06:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 13100 updates
2022-03-04 23:06:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:06:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:06:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 136 @ 13100 updates, score 12.557) (writing took 2.5461973883211613 seconds)
2022-03-04 23:06:31 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-04 23:06:31 | INFO | train | epoch 136 | loss 3.605 | nll_loss 1.422 | ppl 2.68 | wps 24326.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13100 | lr 0.000276289 | gnorm 0.99 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 34998
2022-03-04 23:06:31 | INFO | fairseq.trainer | begin training epoch 137
2022-03-04 23:06:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:08:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:10:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:10:46 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 12.552 | nll_loss 11.474 | ppl 2844.11 | wps 43686.1 | wpb 510.9 | bsz 1 | num_updates 13196 | best_loss 8.737
2022-03-04 23:10:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 13196 updates
2022-03-04 23:10:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:10:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:10:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 137 @ 13196 updates, score 12.552) (writing took 2.4739263644441962 seconds)
2022-03-04 23:10:49 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-04 23:10:49 | INFO | train | epoch 137 | loss 3.598 | nll_loss 1.415 | ppl 2.67 | wps 24371.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13196 | lr 0.000275283 | gnorm 1 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 35256
2022-03-04 23:10:49 | INFO | fairseq.trainer | begin training epoch 138
2022-03-04 23:10:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:10:59 | INFO | train_inner | epoch 138:      4 / 97 loss=3.596, nll_loss=1.412, ppl=2.66, wps=23730.3, ups=0.36, wpb=65492.9, bsz=127.9, num_updates=13200, lr=0.000275241, gnorm=1, loss_scale=16, train_wall=238, gb_free=21, wall=35266
2022-03-04 23:14:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:14:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:15:04 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 12.547 | nll_loss 11.457 | ppl 2811.5 | wps 43341.7 | wpb 510.9 | bsz 1 | num_updates 13292 | best_loss 8.737
2022-03-04 23:15:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 13292 updates
2022-03-04 23:15:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:15:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:15:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 138 @ 13292 updates, score 12.547) (writing took 2.899548828601837 seconds)
2022-03-04 23:15:07 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-04 23:15:07 | INFO | train | epoch 138 | loss 3.59 | nll_loss 1.406 | ppl 2.65 | wps 24336.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13292 | lr 0.000274287 | gnorm 0.987 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 35514
2022-03-04 23:15:07 | INFO | fairseq.trainer | begin training epoch 139
2022-03-04 23:15:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:15:29 | INFO | train_inner | epoch 139:      8 / 97 loss=3.588, nll_loss=1.404, ppl=2.65, wps=24315.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13300, lr=0.000274204, gnorm=0.984, loss_scale=16, train_wall=238, gb_free=21, wall=35536
2022-03-04 23:19:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:19:24 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 12.637 | nll_loss 11.572 | ppl 3043.83 | wps 42888.7 | wpb 510.9 | bsz 1 | num_updates 13389 | best_loss 8.737
2022-03-04 23:19:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 13389 updates
2022-03-04 23:19:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:19:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:19:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 139 @ 13389 updates, score 12.637) (writing took 2.540432075969875 seconds)
2022-03-04 23:19:26 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-04 23:19:26 | INFO | train | epoch 139 | loss 3.582 | nll_loss 1.397 | ppl 2.63 | wps 24535.1 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 13389 | lr 0.000273291 | gnorm 0.984 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 35773
2022-03-04 23:19:26 | INFO | fairseq.trainer | begin training epoch 140
2022-03-04 23:19:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:19:55 | INFO | train_inner | epoch 140:     11 / 97 loss=3.581, nll_loss=1.396, ppl=2.63, wps=24618, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13400, lr=0.000273179, gnorm=0.987, loss_scale=16, train_wall=235, gb_free=21, wall=35802
2022-03-04 23:20:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:23:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:23:41 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 12.585 | nll_loss 11.507 | ppl 2909.65 | wps 43231.1 | wpb 510.9 | bsz 1 | num_updates 13485 | best_loss 8.737
2022-03-04 23:23:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 13485 updates
2022-03-04 23:23:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:23:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:23:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 140 @ 13485 updates, score 12.585) (writing took 2.5724523179233074 seconds)
2022-03-04 23:23:43 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-04 23:23:43 | INFO | train | epoch 140 | loss 3.576 | nll_loss 1.39 | ppl 2.62 | wps 24425 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13485 | lr 0.000272317 | gnorm 1.001 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 36030
2022-03-04 23:23:44 | INFO | fairseq.trainer | begin training epoch 141
2022-03-04 23:23:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:23:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:24:25 | INFO | train_inner | epoch 141:     16 / 97 loss=3.572, nll_loss=1.385, ppl=2.61, wps=24237.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13500, lr=0.000272166, gnorm=1.002, loss_scale=8, train_wall=239, gb_free=21, wall=36072
2022-03-04 23:27:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:27:58 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 12.639 | nll_loss 11.579 | ppl 3058.33 | wps 43458 | wpb 510.9 | bsz 1 | num_updates 13581 | best_loss 8.737
2022-03-04 23:27:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 13581 updates
2022-03-04 23:27:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:28:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:28:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 141 @ 13581 updates, score 12.639) (writing took 2.560721811838448 seconds)
2022-03-04 23:28:01 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-04 23:28:01 | INFO | train | epoch 141 | loss 3.57 | nll_loss 1.384 | ppl 2.61 | wps 24430.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13581 | lr 0.000271353 | gnorm 0.978 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 36288
2022-03-04 23:28:01 | INFO | fairseq.trainer | begin training epoch 142
2022-03-04 23:28:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:28:50 | INFO | train_inner | epoch 142:     19 / 97 loss=3.568, nll_loss=1.381, ppl=2.6, wps=24720, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13600, lr=0.000271163, gnorm=0.972, loss_scale=8, train_wall=235, gb_free=21, wall=36337
2022-03-04 23:30:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:32:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:32:16 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 12.611 | nll_loss 11.529 | ppl 2955.33 | wps 42902.5 | wpb 510.9 | bsz 1 | num_updates 13677 | best_loss 8.737
2022-03-04 23:32:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 13677 updates
2022-03-04 23:32:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:32:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:32:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 142 @ 13677 updates, score 12.611) (writing took 2.5145463794469833 seconds)
2022-03-04 23:32:19 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-04 23:32:19 | INFO | train | epoch 142 | loss 3.563 | nll_loss 1.376 | ppl 2.6 | wps 24376.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13677 | lr 0.000270399 | gnorm 1.005 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 36546
2022-03-04 23:32:19 | INFO | fairseq.trainer | begin training epoch 143
2022-03-04 23:32:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:33:18 | INFO | train_inner | epoch 143:     23 / 97 loss=3.56, nll_loss=1.372, ppl=2.59, wps=24380.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13700, lr=0.000270172, gnorm=1, loss_scale=8, train_wall=238, gb_free=21, wall=36605
2022-03-04 23:36:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:36:35 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 12.691 | nll_loss 11.626 | ppl 3159.86 | wps 43943.3 | wpb 510.9 | bsz 1 | num_updates 13774 | best_loss 8.737
2022-03-04 23:36:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 13774 updates
2022-03-04 23:36:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:36:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:36:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 143 @ 13774 updates, score 12.691) (writing took 2.563437309116125 seconds)
2022-03-04 23:36:37 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-04 23:36:37 | INFO | train | epoch 143 | loss 3.557 | nll_loss 1.369 | ppl 2.58 | wps 24568.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 13774 | lr 0.000269445 | gnorm 0.975 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 36804
2022-03-04 23:36:37 | INFO | fairseq.trainer | begin training epoch 144
2022-03-04 23:36:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:37:45 | INFO | train_inner | epoch 144:     26 / 97 loss=3.556, nll_loss=1.368, ppl=2.58, wps=24583.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=13800, lr=0.000269191, gnorm=0.979, loss_scale=16, train_wall=236, gb_free=21, wall=36872
2022-03-04 23:38:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:40:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:40:53 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 12.648 | nll_loss 11.583 | ppl 3067.5 | wps 44594.1 | wpb 510.9 | bsz 1 | num_updates 13870 | best_loss 8.737
2022-03-04 23:40:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 13870 updates
2022-03-04 23:40:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:40:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:40:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 144 @ 13870 updates, score 12.648) (writing took 2.5271823666989803 seconds)
2022-03-04 23:40:55 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-04 23:40:55 | INFO | train | epoch 144 | loss 3.549 | nll_loss 1.361 | ppl 2.57 | wps 24387.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13870 | lr 0.000268511 | gnorm 0.984 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 37062
2022-03-04 23:40:55 | INFO | fairseq.trainer | begin training epoch 145
2022-03-04 23:40:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:42:13 | INFO | train_inner | epoch 145:     30 / 97 loss=3.547, nll_loss=1.359, ppl=2.56, wps=24448.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=13900, lr=0.000268221, gnorm=0.987, loss_scale=8, train_wall=237, gb_free=21, wall=37140
2022-03-04 23:44:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:45:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:45:11 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 12.617 | nll_loss 11.552 | ppl 3003.01 | wps 43404.4 | wpb 510.9 | bsz 1 | num_updates 13966 | best_loss 8.737
2022-03-04 23:45:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 13966 updates
2022-03-04 23:45:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:45:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:45:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 145 @ 13966 updates, score 12.617) (writing took 2.4820602452382445 seconds)
2022-03-04 23:45:13 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-04 23:45:13 | INFO | train | epoch 145 | loss 3.543 | nll_loss 1.353 | ppl 2.56 | wps 24356.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 13966 | lr 0.000267586 | gnorm 0.985 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 37320
2022-03-04 23:45:13 | INFO | fairseq.trainer | begin training epoch 146
2022-03-04 23:45:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:46:41 | INFO | train_inner | epoch 146:     34 / 97 loss=3.54, nll_loss=1.35, ppl=2.55, wps=24378.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=14000, lr=0.000267261, gnorm=0.981, loss_scale=8, train_wall=238, gb_free=21, wall=37408
2022-03-04 23:49:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:49:29 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 12.691 | nll_loss 11.631 | ppl 3172.31 | wps 44083.4 | wpb 510.9 | bsz 1 | num_updates 14063 | best_loss 8.737
2022-03-04 23:49:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 14063 updates
2022-03-04 23:49:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:49:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:49:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 146 @ 14063 updates, score 12.691) (writing took 2.502089388668537 seconds)
2022-03-04 23:49:31 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-04 23:49:31 | INFO | train | epoch 146 | loss 3.537 | nll_loss 1.347 | ppl 2.54 | wps 24601.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14063 | lr 0.000266662 | gnorm 0.983 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 37578
2022-03-04 23:49:32 | INFO | fairseq.trainer | begin training epoch 147
2022-03-04 23:49:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:51:07 | INFO | train_inner | epoch 147:     37 / 97 loss=3.533, nll_loss=1.343, ppl=2.54, wps=24638.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=14100, lr=0.000266312, gnorm=0.986, loss_scale=16, train_wall=235, gb_free=21, wall=37674
2022-03-04 23:53:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:53:47 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 12.614 | nll_loss 11.547 | ppl 2993 | wps 42632.2 | wpb 510.9 | bsz 1 | num_updates 14160 | best_loss 8.737
2022-03-04 23:53:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 14160 updates
2022-03-04 23:53:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:53:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:53:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 147 @ 14160 updates, score 12.614) (writing took 2.52884264010936 seconds)
2022-03-04 23:53:50 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-04 23:53:50 | INFO | train | epoch 147 | loss 3.531 | nll_loss 1.34 | ppl 2.53 | wps 24595.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14160 | lr 0.000265747 | gnorm 0.987 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 37837
2022-03-04 23:53:50 | INFO | fairseq.trainer | begin training epoch 148
2022-03-04 23:53:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:55:33 | INFO | train_inner | epoch 148:     40 / 97 loss=3.527, nll_loss=1.336, ppl=2.52, wps=24616.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=14200, lr=0.000265372, gnorm=0.975, loss_scale=16, train_wall=236, gb_free=21, wall=37940
2022-03-04 23:56:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:56:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:58:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:58:05 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 12.669 | nll_loss 11.616 | ppl 3137.94 | wps 43446.8 | wpb 510.9 | bsz 1 | num_updates 14255 | best_loss 8.737
2022-03-04 23:58:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 14255 updates
2022-03-04 23:58:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:58:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-04 23:58:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 148 @ 14255 updates, score 12.669) (writing took 2.6488841120153666 seconds)
2022-03-04 23:58:07 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-04 23:58:07 | INFO | train | epoch 148 | loss 3.524 | nll_loss 1.332 | ppl 2.52 | wps 24144.1 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 14255 | lr 0.00026486 | gnorm 0.98 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 38094
2022-03-04 23:58:07 | INFO | fairseq.trainer | begin training epoch 149
2022-03-04 23:58:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:00:03 | INFO | train_inner | epoch 149:     45 / 97 loss=3.523, nll_loss=1.331, ppl=2.52, wps=24231.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=14300, lr=0.000264443, gnorm=0.994, loss_scale=8, train_wall=239, gb_free=21, wall=38210
2022-03-05 00:02:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:02:22 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 12.628 | nll_loss 11.559 | ppl 3016.87 | wps 44851.8 | wpb 510.9 | bsz 1 | num_updates 14352 | best_loss 8.737
2022-03-05 00:02:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 14352 updates
2022-03-05 00:02:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:02:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:02:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 149 @ 14352 updates, score 12.628) (writing took 2.5840425472706556 seconds)
2022-03-05 00:02:25 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-05 00:02:25 | INFO | train | epoch 149 | loss 3.52 | nll_loss 1.328 | ppl 2.51 | wps 24676.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14352 | lr 0.000263963 | gnorm 0.995 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 38352
2022-03-05 00:02:25 | INFO | fairseq.trainer | begin training epoch 150
2022-03-05 00:02:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:04:29 | INFO | train_inner | epoch 150:     48 / 97 loss=3.516, nll_loss=1.324, ppl=2.5, wps=24682.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14400, lr=0.000263523, gnorm=0.978, loss_scale=16, train_wall=235, gb_free=21, wall=38476
2022-03-05 00:04:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:06:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:06:40 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 12.658 | nll_loss 11.594 | ppl 3092.25 | wps 45129.1 | wpb 510.9 | bsz 1 | num_updates 14448 | best_loss 8.737
2022-03-05 00:06:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 14448 updates
2022-03-05 00:06:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:06:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:06:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 150 @ 14448 updates, score 12.658) (writing took 2.5696800500154495 seconds)
2022-03-05 00:06:43 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-05 00:06:43 | INFO | train | epoch 150 | loss 3.512 | nll_loss 1.319 | ppl 2.5 | wps 24377.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14448 | lr 0.000263085 | gnorm 0.97 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 38610
2022-03-05 00:06:43 | INFO | fairseq.trainer | begin training epoch 151
2022-03-05 00:06:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:08:57 | INFO | train_inner | epoch 151:     52 / 97 loss=3.509, nll_loss=1.316, ppl=2.49, wps=24396.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=14500, lr=0.000262613, gnorm=0.965, loss_scale=8, train_wall=238, gb_free=21, wall=38744
2022-03-05 00:10:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:10:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:10:58 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 12.666 | nll_loss 11.606 | ppl 3116.13 | wps 44671.1 | wpb 510.9 | bsz 1 | num_updates 14544 | best_loss 8.737
2022-03-05 00:10:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 14544 updates
2022-03-05 00:10:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:11:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:11:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 151 @ 14544 updates, score 12.666) (writing took 2.465973840095103 seconds)
2022-03-05 00:11:01 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-05 00:11:01 | INFO | train | epoch 151 | loss 3.508 | nll_loss 1.315 | ppl 2.49 | wps 24392 | ups 0.37 | wpb 65533.8 | bsz 128 | num_updates 14544 | lr 0.000262215 | gnorm 0.971 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 38868
2022-03-05 00:11:01 | INFO | fairseq.trainer | begin training epoch 152
2022-03-05 00:11:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:13:25 | INFO | train_inner | epoch 152:     56 / 97 loss=3.505, nll_loss=1.312, ppl=2.48, wps=24451.2, ups=0.37, wpb=65531.7, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.993, loss_scale=8, train_wall=238, gb_free=21, wall=39012
2022-03-05 00:15:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:15:16 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 12.722 | nll_loss 11.664 | ppl 3244.71 | wps 44029.8 | wpb 510.9 | bsz 1 | num_updates 14641 | best_loss 8.737
2022-03-05 00:15:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 14641 updates
2022-03-05 00:15:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:15:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:15:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 152 @ 14641 updates, score 12.722) (writing took 2.5022139297798276 seconds)
2022-03-05 00:15:18 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-03-05 00:15:18 | INFO | train | epoch 152 | loss 3.502 | nll_loss 1.308 | ppl 2.48 | wps 24655.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14641 | lr 0.000261345 | gnorm 0.987 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 39125
2022-03-05 00:15:18 | INFO | fairseq.trainer | begin training epoch 153
2022-03-05 00:15:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:17:51 | INFO | train_inner | epoch 153:     59 / 97 loss=3.5, nll_loss=1.306, ppl=2.47, wps=24652.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14700, lr=0.00026082, gnorm=0.993, loss_scale=16, train_wall=235, gb_free=21, wall=39278
2022-03-05 00:19:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:19:34 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 12.742 | nll_loss 11.696 | ppl 3318.5 | wps 42947.9 | wpb 510.9 | bsz 1 | num_updates 14738 | best_loss 8.737
2022-03-05 00:19:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 14738 updates
2022-03-05 00:19:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:19:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:19:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 153 @ 14738 updates, score 12.742) (writing took 2.5374727211892605 seconds)
2022-03-05 00:19:37 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-03-05 00:19:37 | INFO | train | epoch 153 | loss 3.496 | nll_loss 1.301 | ppl 2.46 | wps 24583.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14738 | lr 0.000260484 | gnorm 0.989 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 39384
2022-03-05 00:19:37 | INFO | fairseq.trainer | begin training epoch 154
2022-03-05 00:19:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:21:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:22:20 | INFO | train_inner | epoch 154:     63 / 97 loss=3.493, nll_loss=1.298, ppl=2.46, wps=24372.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=14800, lr=0.000259938, gnorm=0.974, loss_scale=8, train_wall=238, gb_free=21, wall=39547
2022-03-05 00:23:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:23:53 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 12.721 | nll_loss 11.662 | ppl 3241.21 | wps 43523.5 | wpb 510.9 | bsz 1 | num_updates 14834 | best_loss 8.737
2022-03-05 00:23:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 14834 updates
2022-03-05 00:23:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:23:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:23:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 154 @ 14834 updates, score 12.721) (writing took 2.489063729532063 seconds)
2022-03-05 00:23:55 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-03-05 00:23:55 | INFO | train | epoch 154 | loss 3.49 | nll_loss 1.295 | ppl 2.45 | wps 24335 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 14834 | lr 0.00025964 | gnorm 0.976 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 39642
2022-03-05 00:23:55 | INFO | fairseq.trainer | begin training epoch 155
2022-03-05 00:23:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:26:46 | INFO | train_inner | epoch 155:     66 / 97 loss=3.487, nll_loss=1.292, ppl=2.45, wps=24622.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=14900, lr=0.000259064, gnorm=0.968, loss_scale=8, train_wall=236, gb_free=21, wall=39813
2022-03-05 00:28:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:28:11 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 12.69 | nll_loss 11.638 | ppl 3186.75 | wps 43063.3 | wpb 510.9 | bsz 1 | num_updates 14931 | best_loss 8.737
2022-03-05 00:28:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 14931 updates
2022-03-05 00:28:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:28:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:28:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 155 @ 14931 updates, score 12.69) (writing took 2.5139106027781963 seconds)
2022-03-05 00:28:13 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-03-05 00:28:13 | INFO | train | epoch 155 | loss 3.485 | nll_loss 1.289 | ppl 2.44 | wps 24611.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 14931 | lr 0.000258795 | gnorm 0.967 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 39900
2022-03-05 00:28:13 | INFO | fairseq.trainer | begin training epoch 156
2022-03-05 00:28:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:31:12 | INFO | train_inner | epoch 156:     69 / 97 loss=3.483, nll_loss=1.288, ppl=2.44, wps=24629.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15000, lr=0.000258199, gnorm=0.973, loss_scale=16, train_wall=235, gb_free=21, wall=40079
2022-03-05 00:32:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:32:29 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 12.702 | nll_loss 11.657 | ppl 3229.71 | wps 43346.3 | wpb 510.9 | bsz 1 | num_updates 15028 | best_loss 8.737
2022-03-05 00:32:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 15028 updates
2022-03-05 00:32:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:32:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:32:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 156 @ 15028 updates, score 12.702) (writing took 2.5403431728482246 seconds)
2022-03-05 00:32:31 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-03-05 00:32:31 | INFO | train | epoch 156 | loss 3.48 | nll_loss 1.284 | ppl 2.43 | wps 24623.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15028 | lr 0.000257958 | gnorm 0.972 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 40158
2022-03-05 00:32:31 | INFO | fairseq.trainer | begin training epoch 157
2022-03-05 00:32:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:32:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 00:33:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:35:43 | INFO | train_inner | epoch 157:     74 / 97 loss=3.476, nll_loss=1.28, ppl=2.43, wps=24165.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=15100, lr=0.000257343, gnorm=0.968, loss_scale=8, train_wall=240, gb_free=21, wall=40350
2022-03-05 00:36:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:36:47 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 12.738 | nll_loss 11.693 | ppl 3310.38 | wps 43923.2 | wpb 510.9 | bsz 1 | num_updates 15123 | best_loss 8.737
2022-03-05 00:36:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 15123 updates
2022-03-05 00:36:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:36:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:36:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 157 @ 15123 updates, score 12.738) (writing took 2.585048043169081 seconds)
2022-03-05 00:36:49 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-03-05 00:36:49 | INFO | train | epoch 157 | loss 3.474 | nll_loss 1.277 | ppl 2.42 | wps 24096.1 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 15123 | lr 0.000257147 | gnorm 0.969 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 40416
2022-03-05 00:36:49 | INFO | fairseq.trainer | begin training epoch 158
2022-03-05 00:36:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:40:08 | INFO | train_inner | epoch 158:     77 / 97 loss=3.47, nll_loss=1.273, ppl=2.42, wps=24651.4, ups=0.38, wpb=65495, bsz=127.9, num_updates=15200, lr=0.000256495, gnorm=0.971, loss_scale=16, train_wall=235, gb_free=21, wall=40615
2022-03-05 00:41:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:41:05 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 12.715 | nll_loss 11.667 | ppl 3252.79 | wps 44590.3 | wpb 510.9 | bsz 1 | num_updates 15220 | best_loss 8.737
2022-03-05 00:41:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 15220 updates
2022-03-05 00:41:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:41:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:41:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 158 @ 15220 updates, score 12.715) (writing took 2.4897373970597982 seconds)
2022-03-05 00:41:07 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-03-05 00:41:07 | INFO | train | epoch 158 | loss 3.468 | nll_loss 1.271 | ppl 2.41 | wps 24655.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15220 | lr 0.000256326 | gnorm 0.972 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 40674
2022-03-05 00:41:07 | INFO | fairseq.trainer | begin training epoch 159
2022-03-05 00:41:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:43:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:44:36 | INFO | train_inner | epoch 159:     81 / 97 loss=3.468, nll_loss=1.271, ppl=2.41, wps=24448.9, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=15300, lr=0.000255655, gnorm=0.979, loss_scale=8, train_wall=237, gb_free=21, wall=40883
2022-03-05 00:45:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:45:22 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 12.684 | nll_loss 11.632 | ppl 3173.28 | wps 43402.7 | wpb 510.9 | bsz 1 | num_updates 15316 | best_loss 8.737
2022-03-05 00:45:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 15316 updates
2022-03-05 00:45:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:45:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:45:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 159 @ 15316 updates, score 12.684) (writing took 2.4764571264386177 seconds)
2022-03-05 00:45:25 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-03-05 00:45:25 | INFO | train | epoch 159 | loss 3.464 | nll_loss 1.266 | ppl 2.41 | wps 24399.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15316 | lr 0.000255521 | gnorm 0.977 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 40932
2022-03-05 00:45:25 | INFO | fairseq.trainer | begin training epoch 160
2022-03-05 00:45:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:49:02 | INFO | train_inner | epoch 160:     84 / 97 loss=3.46, nll_loss=1.262, ppl=2.4, wps=24660.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=15400, lr=0.000254824, gnorm=0.974, loss_scale=16, train_wall=235, gb_free=21, wall=41149
2022-03-05 00:49:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:49:40 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 12.788 | nll_loss 11.748 | ppl 3440.4 | wps 44523.2 | wpb 510.9 | bsz 1 | num_updates 15413 | best_loss 8.737
2022-03-05 00:49:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 15413 updates
2022-03-05 00:49:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:49:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:49:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 160 @ 15413 updates, score 12.788) (writing took 2.4529671454802155 seconds)
2022-03-05 00:49:43 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-03-05 00:49:43 | INFO | train | epoch 160 | loss 3.458 | nll_loss 1.26 | ppl 2.4 | wps 24639.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15413 | lr 0.000254716 | gnorm 0.968 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 41190
2022-03-05 00:49:43 | INFO | fairseq.trainer | begin training epoch 161
2022-03-05 00:49:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:50:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:53:29 | INFO | train_inner | epoch 161:     88 / 97 loss=3.453, nll_loss=1.254, ppl=2.39, wps=24467.6, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=15500, lr=0.000254, gnorm=0.973, loss_scale=8, train_wall=237, gb_free=21, wall=41416
2022-03-05 00:53:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:53:57 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 12.722 | nll_loss 11.682 | ppl 3285.88 | wps 44671.5 | wpb 510.9 | bsz 1 | num_updates 15509 | best_loss 8.737
2022-03-05 00:53:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 15509 updates
2022-03-05 00:53:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:54:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:54:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 161 @ 15509 updates, score 12.722) (writing took 2.5806211084127426 seconds)
2022-03-05 00:54:00 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-03-05 00:54:00 | INFO | train | epoch 161 | loss 3.452 | nll_loss 1.254 | ppl 2.38 | wps 24434.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15509 | lr 0.000253927 | gnorm 0.974 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 41447
2022-03-05 00:54:00 | INFO | fairseq.trainer | begin training epoch 162
2022-03-05 00:54:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:57:55 | INFO | train_inner | epoch 162:     91 / 97 loss=3.449, nll_loss=1.251, ppl=2.38, wps=24656.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15600, lr=0.000253185, gnorm=0.946, loss_scale=16, train_wall=235, gb_free=21, wall=41682
2022-03-05 00:58:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:58:15 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 12.707 | nll_loss 11.662 | ppl 3239.55 | wps 44266.8 | wpb 510.9 | bsz 1 | num_updates 15606 | best_loss 8.737
2022-03-05 00:58:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 15606 updates
2022-03-05 00:58:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:58:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 00:58:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 162 @ 15606 updates, score 12.707) (writing took 2.496105599217117 seconds)
2022-03-05 00:58:18 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-03-05 00:58:18 | INFO | train | epoch 162 | loss 3.448 | nll_loss 1.249 | ppl 2.38 | wps 24621.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15606 | lr 0.000253136 | gnorm 0.944 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 41705
2022-03-05 00:58:18 | INFO | fairseq.trainer | begin training epoch 163
2022-03-05 00:58:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:02:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:02:23 | INFO | train_inner | epoch 163:     95 / 97 loss=3.447, nll_loss=1.248, ppl=2.38, wps=24460.2, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=15700, lr=0.000252377, gnorm=0.98, loss_scale=16, train_wall=237, gb_free=21, wall=41950
2022-03-05 01:02:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:02:33 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 12.725 | nll_loss 11.682 | ppl 3285.25 | wps 45297.3 | wpb 510.9 | bsz 1 | num_updates 15702 | best_loss 8.737
2022-03-05 01:02:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 15702 updates
2022-03-05 01:02:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:02:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:02:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 163 @ 15702 updates, score 12.725) (writing took 2.507911358959973 seconds)
2022-03-05 01:02:35 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-03-05 01:02:35 | INFO | train | epoch 163 | loss 3.445 | nll_loss 1.245 | ppl 2.37 | wps 24442.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15702 | lr 0.000252361 | gnorm 0.982 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 41962
2022-03-05 01:02:35 | INFO | fairseq.trainer | begin training epoch 164
2022-03-05 01:02:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:03:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:06:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:06:50 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 12.727 | nll_loss 11.691 | ppl 3307.05 | wps 45292.6 | wpb 510.9 | bsz 1 | num_updates 15798 | best_loss 8.737
2022-03-05 01:06:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 15798 updates
2022-03-05 01:06:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:06:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:06:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 164 @ 15798 updates, score 12.727) (writing took 2.3991861697286367 seconds)
2022-03-05 01:06:53 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-03-05 01:06:53 | INFO | train | epoch 164 | loss 3.439 | nll_loss 1.24 | ppl 2.36 | wps 24407.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15798 | lr 0.000251593 | gnorm 0.968 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 42220
2022-03-05 01:06:53 | INFO | fairseq.trainer | begin training epoch 165
2022-03-05 01:06:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:06:58 | INFO | train_inner | epoch 165:      2 / 97 loss=3.439, nll_loss=1.24, ppl=2.36, wps=23777.8, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=15800, lr=0.000251577, gnorm=0.969, loss_scale=8, train_wall=237, gb_free=21, wall=42225
2022-03-05 01:11:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:11:08 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 12.77 | nll_loss 11.726 | ppl 3387.4 | wps 44007 | wpb 510.9 | bsz 1 | num_updates 15895 | best_loss 8.737
2022-03-05 01:11:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 15895 updates
2022-03-05 01:11:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:11:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:11:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 165 @ 15895 updates, score 12.77) (writing took 2.3581954343244433 seconds)
2022-03-05 01:11:10 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-03-05 01:11:10 | INFO | train | epoch 165 | loss 3.434 | nll_loss 1.234 | ppl 2.35 | wps 24676.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 15895 | lr 0.000250824 | gnorm 0.964 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 42477
2022-03-05 01:11:10 | INFO | fairseq.trainer | begin training epoch 166
2022-03-05 01:11:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:11:23 | INFO | train_inner | epoch 166:      5 / 97 loss=3.433, nll_loss=1.232, ppl=2.35, wps=24703.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=15900, lr=0.000250785, gnorm=0.963, loss_scale=16, train_wall=235, gb_free=21, wall=42490
2022-03-05 01:13:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:15:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:15:26 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 12.713 | nll_loss 11.671 | ppl 3261.43 | wps 43040.7 | wpb 510.9 | bsz 1 | num_updates 15991 | best_loss 8.737
2022-03-05 01:15:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 15991 updates
2022-03-05 01:15:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:15:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:15:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 166 @ 15991 updates, score 12.713) (writing took 2.380966783501208 seconds)
2022-03-05 01:15:28 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-03-05 01:15:28 | INFO | train | epoch 166 | loss 3.428 | nll_loss 1.227 | ppl 2.34 | wps 24356.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 15991 | lr 0.00025007 | gnorm 0.962 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 42735
2022-03-05 01:15:28 | INFO | fairseq.trainer | begin training epoch 167
2022-03-05 01:15:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:15:52 | INFO | train_inner | epoch 167:      9 / 97 loss=3.426, nll_loss=1.225, ppl=2.34, wps=24391.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16000, lr=0.00025, gnorm=0.961, loss_scale=8, train_wall=238, gb_free=21, wall=42759
2022-03-05 01:19:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:19:44 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 12.693 | nll_loss 11.648 | ppl 3208.16 | wps 43288.5 | wpb 510.9 | bsz 1 | num_updates 16088 | best_loss 8.737
2022-03-05 01:19:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 16088 updates
2022-03-05 01:19:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:19:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:19:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 167 @ 16088 updates, score 12.693) (writing took 2.3827359322458506 seconds)
2022-03-05 01:19:47 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-03-05 01:19:47 | INFO | train | epoch 167 | loss 3.424 | nll_loss 1.223 | ppl 2.33 | wps 24580.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16088 | lr 0.000249315 | gnorm 0.958 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 42994
2022-03-05 01:19:47 | INFO | fairseq.trainer | begin training epoch 168
2022-03-05 01:19:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:20:18 | INFO | train_inner | epoch 168:     12 / 97 loss=3.422, nll_loss=1.22, ppl=2.33, wps=24593.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16100, lr=0.000249222, gnorm=0.955, loss_scale=16, train_wall=236, gb_free=21, wall=43025
2022-03-05 01:23:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:24:02 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 12.764 | nll_loss 11.725 | ppl 3385 | wps 45353.5 | wpb 510.9 | bsz 1 | num_updates 16185 | best_loss 8.737
2022-03-05 01:24:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 16185 updates
2022-03-05 01:24:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:24:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:24:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 168 @ 16185 updates, score 12.764) (writing took 2.5442015808075666 seconds)
2022-03-05 01:24:05 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-03-05 01:24:05 | INFO | train | epoch 168 | loss 3.421 | nll_loss 1.22 | ppl 2.33 | wps 24629.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16185 | lr 0.000248567 | gnorm 0.962 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 43252
2022-03-05 01:24:05 | INFO | fairseq.trainer | begin training epoch 169
2022-03-05 01:24:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:24:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:24:46 | INFO | train_inner | epoch 169:     16 / 97 loss=3.421, nll_loss=1.22, ppl=2.33, wps=24429.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16200, lr=0.000248452, gnorm=0.969, loss_scale=8, train_wall=238, gb_free=21, wall=43293
2022-03-05 01:28:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:28:19 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 12.773 | nll_loss 11.74 | ppl 3421.19 | wps 44500.6 | wpb 510.9 | bsz 1 | num_updates 16281 | best_loss 8.737
2022-03-05 01:28:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 16281 updates
2022-03-05 01:28:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:28:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:28:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 169 @ 16281 updates, score 12.773) (writing took 2.388527094386518 seconds)
2022-03-05 01:28:22 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-03-05 01:28:22 | INFO | train | epoch 169 | loss 3.414 | nll_loss 1.212 | ppl 2.32 | wps 24447.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16281 | lr 0.000247833 | gnorm 0.966 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 43509
2022-03-05 01:28:22 | INFO | fairseq.trainer | begin training epoch 170
2022-03-05 01:28:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:29:11 | INFO | train_inner | epoch 170:     19 / 97 loss=3.411, nll_loss=1.209, ppl=2.31, wps=24724.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16300, lr=0.000247689, gnorm=0.954, loss_scale=8, train_wall=235, gb_free=21, wall=43558
2022-03-05 01:32:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:32:37 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 12.817 | nll_loss 11.791 | ppl 3543.5 | wps 43453.5 | wpb 510.9 | bsz 1 | num_updates 16378 | best_loss 8.737
2022-03-05 01:32:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 16378 updates
2022-03-05 01:32:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:32:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:32:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 170 @ 16378 updates, score 12.817) (writing took 2.3903849478811026 seconds)
2022-03-05 01:32:39 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-03-05 01:32:39 | INFO | train | epoch 170 | loss 3.412 | nll_loss 1.21 | ppl 2.31 | wps 24694.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16378 | lr 0.000247098 | gnorm 0.954 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 43766
2022-03-05 01:32:39 | INFO | fairseq.trainer | begin training epoch 171
2022-03-05 01:32:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:33:36 | INFO | train_inner | epoch 171:     22 / 97 loss=3.41, nll_loss=1.207, ppl=2.31, wps=24715.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=16400, lr=0.000246932, gnorm=0.962, loss_scale=16, train_wall=235, gb_free=21, wall=43823
2022-03-05 01:35:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:36:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:36:54 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 12.746 | nll_loss 11.717 | ppl 3367.25 | wps 44365.4 | wpb 510.9 | bsz 1 | num_updates 16474 | best_loss 8.737
2022-03-05 01:36:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 16474 updates
2022-03-05 01:36:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:36:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:36:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 171 @ 16474 updates, score 12.746) (writing took 2.4365514321252704 seconds)
2022-03-05 01:36:56 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-03-05 01:36:56 | INFO | train | epoch 171 | loss 3.406 | nll_loss 1.203 | ppl 2.3 | wps 24438.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16474 | lr 0.000246377 | gnorm 0.955 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 44023
2022-03-05 01:36:56 | INFO | fairseq.trainer | begin training epoch 172
2022-03-05 01:36:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:38:04 | INFO | train_inner | epoch 172:     26 / 97 loss=3.404, nll_loss=1.201, ppl=2.3, wps=24480, ups=0.37, wpb=65495, bsz=127.9, num_updates=16500, lr=0.000246183, gnorm=0.953, loss_scale=16, train_wall=237, gb_free=21, wall=44090
2022-03-05 01:41:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:41:11 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 12.747 | nll_loss 11.724 | ppl 3381.93 | wps 43400.8 | wpb 510.9 | bsz 1 | num_updates 16571 | best_loss 8.737
2022-03-05 01:41:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 16571 updates
2022-03-05 01:41:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:41:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:41:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 172 @ 16571 updates, score 12.747) (writing took 2.4036875013262033 seconds)
2022-03-05 01:41:14 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-03-05 01:41:14 | INFO | train | epoch 172 | loss 3.403 | nll_loss 1.2 | ppl 2.3 | wps 24684.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16571 | lr 0.000245655 | gnorm 0.962 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 44281
2022-03-05 01:41:14 | INFO | fairseq.trainer | begin training epoch 173
2022-03-05 01:41:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:41:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:42:31 | INFO | train_inner | epoch 173:     30 / 97 loss=3.403, nll_loss=1.2, ppl=2.3, wps=24458.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16600, lr=0.00024544, gnorm=0.957, loss_scale=16, train_wall=237, gb_free=21, wall=44358
2022-03-05 01:45:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:45:29 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 12.799 | nll_loss 11.777 | ppl 3509.02 | wps 43627.5 | wpb 510.9 | bsz 1 | num_updates 16667 | best_loss 8.737
2022-03-05 01:45:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 16667 updates
2022-03-05 01:45:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:45:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:45:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 173 @ 16667 updates, score 12.799) (writing took 2.4520666068419814 seconds)
2022-03-05 01:45:31 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-03-05 01:45:31 | INFO | train | epoch 173 | loss 3.397 | nll_loss 1.194 | ppl 2.29 | wps 24403 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16667 | lr 0.000244947 | gnorm 0.952 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 44538
2022-03-05 01:45:31 | INFO | fairseq.trainer | begin training epoch 174
2022-03-05 01:45:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:46:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:46:59 | INFO | train_inner | epoch 174:     34 / 97 loss=3.394, nll_loss=1.19, ppl=2.28, wps=24459, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=16700, lr=0.000244704, gnorm=0.955, loss_scale=8, train_wall=237, gb_free=21, wall=44626
2022-03-05 01:49:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:49:46 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 12.762 | nll_loss 11.733 | ppl 3402.82 | wps 44855.6 | wpb 510.9 | bsz 1 | num_updates 16763 | best_loss 8.737
2022-03-05 01:49:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 16763 updates
2022-03-05 01:49:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:49:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:49:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 174 @ 16763 updates, score 12.762) (writing took 2.504264824092388 seconds)
2022-03-05 01:49:49 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-03-05 01:49:49 | INFO | train | epoch 174 | loss 3.394 | nll_loss 1.19 | ppl 2.28 | wps 24437.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16763 | lr 0.000244244 | gnorm 0.961 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 44796
2022-03-05 01:49:49 | INFO | fairseq.trainer | begin training epoch 175
2022-03-05 01:49:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:51:24 | INFO | train_inner | epoch 175:     37 / 97 loss=3.392, nll_loss=1.188, ppl=2.28, wps=24711, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=16800, lr=0.000243975, gnorm=0.959, loss_scale=8, train_wall=235, gb_free=21, wall=44891
2022-03-05 01:53:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:54:04 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 12.817 | nll_loss 11.792 | ppl 3546.11 | wps 44521.8 | wpb 510.9 | bsz 1 | num_updates 16860 | best_loss 8.737
2022-03-05 01:54:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 16860 updates
2022-03-05 01:54:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:54:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:54:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 175 @ 16860 updates, score 12.817) (writing took 2.4903631545603275 seconds)
2022-03-05 01:54:07 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-03-05 01:54:07 | INFO | train | epoch 175 | loss 3.389 | nll_loss 1.184 | ppl 2.27 | wps 24636.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 16860 | lr 0.000243541 | gnorm 0.952 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 45053
2022-03-05 01:54:07 | INFO | fairseq.trainer | begin training epoch 176
2022-03-05 01:54:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:55:50 | INFO | train_inner | epoch 176:     40 / 97 loss=3.386, nll_loss=1.181, ppl=2.27, wps=24648.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=16900, lr=0.000243252, gnorm=0.962, loss_scale=16, train_wall=235, gb_free=21, wall=45157
2022-03-05 01:57:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 01:58:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:58:21 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 12.763 | nll_loss 11.733 | ppl 3403.01 | wps 45394.8 | wpb 510.9 | bsz 1 | num_updates 16956 | best_loss 8.737
2022-03-05 01:58:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 16956 updates
2022-03-05 01:58:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:58:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 01:58:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 176 @ 16956 updates, score 12.763) (writing took 2.4479871531948447 seconds)
2022-03-05 01:58:24 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-03-05 01:58:24 | INFO | train | epoch 176 | loss 3.384 | nll_loss 1.179 | ppl 2.26 | wps 24432.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 16956 | lr 0.00024285 | gnorm 0.963 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 45311
2022-03-05 01:58:24 | INFO | fairseq.trainer | begin training epoch 177
2022-03-05 01:58:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:00:17 | INFO | train_inner | epoch 177:     44 / 97 loss=3.382, nll_loss=1.178, ppl=2.26, wps=24474.5, ups=0.37, wpb=65495, bsz=127.9, num_updates=17000, lr=0.000242536, gnorm=0.946, loss_scale=16, train_wall=237, gb_free=21, wall=45424
2022-03-05 02:02:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:02:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:02:39 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 12.728 | nll_loss 11.693 | ppl 3310.58 | wps 42444.3 | wpb 510.9 | bsz 1 | num_updates 17052 | best_loss 8.737
2022-03-05 02:02:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 17052 updates
2022-03-05 02:02:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:02:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:02:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 177 @ 17052 updates, score 12.728) (writing took 2.398961139842868 seconds)
2022-03-05 02:02:41 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-03-05 02:02:41 | INFO | train | epoch 177 | loss 3.382 | nll_loss 1.177 | ppl 2.26 | wps 24421.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17052 | lr 0.000242166 | gnorm 0.948 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 45568
2022-03-05 02:02:41 | INFO | fairseq.trainer | begin training epoch 178
2022-03-05 02:02:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:04:45 | INFO | train_inner | epoch 178:     48 / 97 loss=3.377, nll_loss=1.172, ppl=2.25, wps=24471.7, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=17100, lr=0.000241825, gnorm=0.953, loss_scale=8, train_wall=237, gb_free=21, wall=45692
2022-03-05 02:06:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:06:56 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 12.775 | nll_loss 11.743 | ppl 3427.33 | wps 42439.2 | wpb 510.9 | bsz 1 | num_updates 17149 | best_loss 8.737
2022-03-05 02:06:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 17149 updates
2022-03-05 02:06:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:06:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:06:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 178 @ 17149 updates, score 12.775) (writing took 2.469047254882753 seconds)
2022-03-05 02:06:59 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-03-05 02:06:59 | INFO | train | epoch 178 | loss 3.377 | nll_loss 1.172 | ppl 2.25 | wps 24665.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17149 | lr 0.00024148 | gnorm 0.961 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 45826
2022-03-05 02:06:59 | INFO | fairseq.trainer | begin training epoch 179
2022-03-05 02:06:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:09:11 | INFO | train_inner | epoch 179:     51 / 97 loss=3.378, nll_loss=1.173, ppl=2.25, wps=24663.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17200, lr=0.000241121, gnorm=0.971, loss_scale=16, train_wall=235, gb_free=21, wall=45958
2022-03-05 02:11:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:11:14 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 12.809 | nll_loss 11.783 | ppl 3523.32 | wps 43102.8 | wpb 510.9 | bsz 1 | num_updates 17246 | best_loss 8.737
2022-03-05 02:11:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 17246 updates
2022-03-05 02:11:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:11:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:11:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 179 @ 17246 updates, score 12.809) (writing took 2.4357988042756915 seconds)
2022-03-05 02:11:17 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-03-05 02:11:17 | INFO | train | epoch 179 | loss 3.373 | nll_loss 1.168 | ppl 2.25 | wps 24642.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17246 | lr 0.0002408 | gnorm 0.959 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 46084
2022-03-05 02:11:17 | INFO | fairseq.trainer | begin training epoch 180
2022-03-05 02:11:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:13:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:13:39 | INFO | train_inner | epoch 180:     55 / 97 loss=3.37, nll_loss=1.164, ppl=2.24, wps=24411.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17300, lr=0.000240424, gnorm=0.946, loss_scale=16, train_wall=238, gb_free=21, wall=46226
2022-03-05 02:14:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:15:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:15:32 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 12.895 | nll_loss 11.878 | ppl 3762.89 | wps 45384.9 | wpb 510.9 | bsz 1 | num_updates 17341 | best_loss 8.737
2022-03-05 02:15:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 17341 updates
2022-03-05 02:15:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:15:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:15:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 180 @ 17341 updates, score 12.895) (writing took 2.4552135141566396 seconds)
2022-03-05 02:15:34 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-03-05 02:15:34 | INFO | train | epoch 180 | loss 3.368 | nll_loss 1.162 | ppl 2.24 | wps 24161.4 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 17341 | lr 0.000240139 | gnorm 0.943 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 46341
2022-03-05 02:15:34 | INFO | fairseq.trainer | begin training epoch 181
2022-03-05 02:15:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:18:06 | INFO | train_inner | epoch 181:     59 / 97 loss=3.365, nll_loss=1.158, ppl=2.23, wps=24480, ups=0.37, wpb=65495, bsz=127.9, num_updates=17400, lr=0.000239732, gnorm=0.945, loss_scale=8, train_wall=237, gb_free=21, wall=46493
2022-03-05 02:19:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:19:49 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 12.762 | nll_loss 11.737 | ppl 3414.27 | wps 43686.7 | wpb 510.9 | bsz 1 | num_updates 17438 | best_loss 8.737
2022-03-05 02:19:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 17438 updates
2022-03-05 02:19:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:19:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:19:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 181 @ 17438 updates, score 12.762) (writing took 2.4237816063687205 seconds)
2022-03-05 02:19:52 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-03-05 02:19:52 | INFO | train | epoch 181 | loss 3.366 | nll_loss 1.16 | ppl 2.23 | wps 24663.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17438 | lr 0.00023947 | gnorm 0.953 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 46599
2022-03-05 02:19:52 | INFO | fairseq.trainer | begin training epoch 182
2022-03-05 02:19:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:22:32 | INFO | train_inner | epoch 182:     62 / 97 loss=3.364, nll_loss=1.158, ppl=2.23, wps=24656.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=17500, lr=0.000239046, gnorm=0.944, loss_scale=16, train_wall=235, gb_free=21, wall=46759
2022-03-05 02:23:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:24:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:24:07 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 12.845 | nll_loss 11.822 | ppl 3620.96 | wps 44612.1 | wpb 510.9 | bsz 1 | num_updates 17534 | best_loss 8.737
2022-03-05 02:24:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 17534 updates
2022-03-05 02:24:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:24:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:24:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 182 @ 17534 updates, score 12.845) (writing took 2.4812865974381566 seconds)
2022-03-05 02:24:10 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-03-05 02:24:10 | INFO | train | epoch 182 | loss 3.36 | nll_loss 1.153 | ppl 2.22 | wps 24384.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17534 | lr 0.000238814 | gnorm 0.938 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 46857
2022-03-05 02:24:10 | INFO | fairseq.trainer | begin training epoch 183
2022-03-05 02:24:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:27:00 | INFO | train_inner | epoch 183:     66 / 97 loss=3.358, nll_loss=1.151, ppl=2.22, wps=24459.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17600, lr=0.000238366, gnorm=0.946, loss_scale=8, train_wall=237, gb_free=21, wall=47027
2022-03-05 02:28:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:28:25 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 12.815 | nll_loss 11.798 | ppl 3561.04 | wps 44524.9 | wpb 510.9 | bsz 1 | num_updates 17631 | best_loss 8.737
2022-03-05 02:28:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 17631 updates
2022-03-05 02:28:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:28:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:28:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 183 @ 17631 updates, score 12.815) (writing took 2.5196314491331577 seconds)
2022-03-05 02:28:27 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-03-05 02:28:27 | INFO | train | epoch 183 | loss 3.357 | nll_loss 1.15 | ppl 2.22 | wps 24653.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17631 | lr 0.000238156 | gnorm 0.95 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 47114
2022-03-05 02:28:27 | INFO | fairseq.trainer | begin training epoch 184
2022-03-05 02:28:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:31:26 | INFO | train_inner | epoch 184:     69 / 97 loss=3.355, nll_loss=1.148, ppl=2.22, wps=24627.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=17700, lr=0.000237691, gnorm=0.953, loss_scale=16, train_wall=236, gb_free=21, wall=47293
2022-03-05 02:32:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:32:43 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 12.818 | nll_loss 11.792 | ppl 3545.55 | wps 44360.5 | wpb 510.9 | bsz 1 | num_updates 17728 | best_loss 8.737
2022-03-05 02:32:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 17728 updates
2022-03-05 02:32:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:32:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:32:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 184 @ 17728 updates, score 12.818) (writing took 2.483285707421601 seconds)
2022-03-05 02:32:46 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-03-05 02:32:46 | INFO | train | epoch 184 | loss 3.353 | nll_loss 1.146 | ppl 2.21 | wps 24597.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 17728 | lr 0.000237504 | gnorm 0.952 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 47372
2022-03-05 02:32:46 | INFO | fairseq.trainer | begin training epoch 185
2022-03-05 02:32:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:35:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 02:35:54 | INFO | train_inner | epoch 185:     73 / 97 loss=3.354, nll_loss=1.147, ppl=2.21, wps=24395.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17800, lr=0.000237023, gnorm=0.959, loss_scale=16, train_wall=238, gb_free=21, wall=47561
2022-03-05 02:36:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:37:01 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 12.746 | nll_loss 11.718 | ppl 3367.93 | wps 44875.5 | wpb 510.9 | bsz 1 | num_updates 17824 | best_loss 8.737
2022-03-05 02:37:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 17824 updates
2022-03-05 02:37:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:37:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:37:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 185 @ 17824 updates, score 12.746) (writing took 2.519365193322301 seconds)
2022-03-05 02:37:04 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-03-05 02:37:04 | INFO | train | epoch 185 | loss 3.35 | nll_loss 1.143 | ppl 2.21 | wps 24360.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17824 | lr 0.000236863 | gnorm 0.954 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 47631
2022-03-05 02:37:04 | INFO | fairseq.trainer | begin training epoch 186
2022-03-05 02:37:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:39:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:40:23 | INFO | train_inner | epoch 186:     77 / 97 loss=3.345, nll_loss=1.137, ppl=2.2, wps=24387.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=17900, lr=0.00023636, gnorm=0.952, loss_scale=8, train_wall=238, gb_free=21, wall=47830
2022-03-05 02:41:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:41:19 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 12.811 | nll_loss 11.794 | ppl 3551.97 | wps 43858.9 | wpb 510.9 | bsz 1 | num_updates 17920 | best_loss 8.737
2022-03-05 02:41:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 17920 updates
2022-03-05 02:41:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:41:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:41:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 186 @ 17920 updates, score 12.811) (writing took 2.5695081278681755 seconds)
2022-03-05 02:41:22 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-03-05 02:41:22 | INFO | train | epoch 186 | loss 3.345 | nll_loss 1.137 | ppl 2.2 | wps 24351.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 17920 | lr 0.000236228 | gnorm 0.948 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 47889
2022-03-05 02:41:22 | INFO | fairseq.trainer | begin training epoch 187
2022-03-05 02:41:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:44:48 | INFO | train_inner | epoch 187:     80 / 97 loss=3.344, nll_loss=1.136, ppl=2.2, wps=24699.1, ups=0.38, wpb=65495, bsz=127.9, num_updates=18000, lr=0.000235702, gnorm=0.946, loss_scale=8, train_wall=235, gb_free=21, wall=48095
2022-03-05 02:45:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:45:37 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 12.714 | nll_loss 11.69 | ppl 3304.51 | wps 44355.2 | wpb 510.9 | bsz 1 | num_updates 18017 | best_loss 8.737
2022-03-05 02:45:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 18017 updates
2022-03-05 02:45:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:45:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:45:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 187 @ 18017 updates, score 12.714) (writing took 2.6316794380545616 seconds)
2022-03-05 02:45:39 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-03-05 02:45:39 | INFO | train | epoch 187 | loss 3.342 | nll_loss 1.134 | ppl 2.19 | wps 24666.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18017 | lr 0.000235591 | gnorm 0.952 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 48146
2022-03-05 02:45:39 | INFO | fairseq.trainer | begin training epoch 188
2022-03-05 02:45:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:47:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:49:16 | INFO | train_inner | epoch 188:     84 / 97 loss=3.339, nll_loss=1.131, ppl=2.19, wps=24425.7, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18100, lr=0.00023505, gnorm=0.946, loss_scale=8, train_wall=237, gb_free=21, wall=48363
2022-03-05 02:49:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:49:54 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 12.891 | nll_loss 11.886 | ppl 3784.03 | wps 44394.3 | wpb 510.9 | bsz 1 | num_updates 18113 | best_loss 8.737
2022-03-05 02:49:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 18113 updates
2022-03-05 02:49:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:49:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:49:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 188 @ 18113 updates, score 12.891) (writing took 2.600311638787389 seconds)
2022-03-05 02:49:57 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-03-05 02:49:57 | INFO | train | epoch 188 | loss 3.337 | nll_loss 1.128 | ppl 2.19 | wps 24418.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18113 | lr 0.000234966 | gnorm 0.94 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 48404
2022-03-05 02:49:57 | INFO | fairseq.trainer | begin training epoch 189
2022-03-05 02:49:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:53:41 | INFO | train_inner | epoch 189:     87 / 97 loss=3.336, nll_loss=1.127, ppl=2.18, wps=24700.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18200, lr=0.000234404, gnorm=0.934, loss_scale=16, train_wall=235, gb_free=21, wall=48628
2022-03-05 02:54:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:54:12 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 12.826 | nll_loss 11.811 | ppl 3594.07 | wps 43391.2 | wpb 510.9 | bsz 1 | num_updates 18210 | best_loss 8.737
2022-03-05 02:54:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 18210 updates
2022-03-05 02:54:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:54:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:54:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 189 @ 18210 updates, score 12.826) (writing took 2.5609414521604776 seconds)
2022-03-05 02:54:14 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-03-05 02:54:14 | INFO | train | epoch 189 | loss 3.335 | nll_loss 1.127 | ppl 2.18 | wps 24655.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18210 | lr 0.000234339 | gnorm 0.933 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 48661
2022-03-05 02:54:14 | INFO | fairseq.trainer | begin training epoch 190
2022-03-05 02:54:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:54:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:58:09 | INFO | train_inner | epoch 190:     91 / 97 loss=3.334, nll_loss=1.125, ppl=2.18, wps=24449, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=18300, lr=0.000233762, gnorm=0.951, loss_scale=8, train_wall=237, gb_free=21, wall=48896
2022-03-05 02:58:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:58:29 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 12.774 | nll_loss 11.755 | ppl 3455.52 | wps 43833.9 | wpb 510.9 | bsz 1 | num_updates 18306 | best_loss 8.737
2022-03-05 02:58:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 18306 updates
2022-03-05 02:58:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:58:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 02:58:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 190 @ 18306 updates, score 12.774) (writing took 2.554263476282358 seconds)
2022-03-05 02:58:32 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-03-05 02:58:32 | INFO | train | epoch 190 | loss 3.331 | nll_loss 1.123 | ppl 2.18 | wps 24417.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18306 | lr 0.000233724 | gnorm 0.953 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 48919
2022-03-05 02:58:32 | INFO | fairseq.trainer | begin training epoch 191
2022-03-05 02:58:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:02:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:02:38 | INFO | train_inner | epoch 191:     95 / 97 loss=3.329, nll_loss=1.12, ppl=2.17, wps=24387.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18400, lr=0.000233126, gnorm=0.947, loss_scale=8, train_wall=238, gb_free=21, wall=49165
2022-03-05 03:02:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:02:48 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 12.784 | nll_loss 11.771 | ppl 3495.71 | wps 42922.4 | wpb 510.9 | bsz 1 | num_updates 18402 | best_loss 8.737
2022-03-05 03:02:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 18402 updates
2022-03-05 03:02:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:02:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:02:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 191 @ 18402 updates, score 12.784) (writing took 2.4462726516649127 seconds)
2022-03-05 03:02:50 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-03-05 03:02:50 | INFO | train | epoch 191 | loss 3.327 | nll_loss 1.118 | ppl 2.17 | wps 24347.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18402 | lr 0.000233114 | gnorm 0.947 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 49177
2022-03-05 03:02:50 | INFO | fairseq.trainer | begin training epoch 192
2022-03-05 03:02:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:07:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:07:06 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 12.873 | nll_loss 11.86 | ppl 3717.06 | wps 42560.5 | wpb 510.9 | bsz 1 | num_updates 18499 | best_loss 8.737
2022-03-05 03:07:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 18499 updates
2022-03-05 03:07:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:07:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:07:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 192 @ 18499 updates, score 12.873) (writing took 2.539589018560946 seconds)
2022-03-05 03:07:08 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-03-05 03:07:08 | INFO | train | epoch 192 | loss 3.325 | nll_loss 1.116 | ppl 2.17 | wps 24594.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18499 | lr 0.000232502 | gnorm 0.94 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 49435
2022-03-05 03:07:08 | INFO | fairseq.trainer | begin training epoch 193
2022-03-05 03:07:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:07:11 | INFO | train_inner | epoch 193:      1 / 97 loss=3.326, nll_loss=1.117, ppl=2.17, wps=23923.7, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=18500, lr=0.000232495, gnorm=0.942, loss_scale=8, train_wall=235, gb_free=21, wall=49438
2022-03-05 03:09:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:11:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:11:24 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 12.819 | nll_loss 11.807 | ppl 3582.68 | wps 44709.4 | wpb 510.9 | bsz 1 | num_updates 18595 | best_loss 8.737
2022-03-05 03:11:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 18595 updates
2022-03-05 03:11:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:11:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:11:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 193 @ 18595 updates, score 12.819) (writing took 2.6247439673170447 seconds)
2022-03-05 03:11:27 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-03-05 03:11:27 | INFO | train | epoch 193 | loss 3.321 | nll_loss 1.112 | ppl 2.16 | wps 24315.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18595 | lr 0.000231901 | gnorm 0.955 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 49694
2022-03-05 03:11:27 | INFO | fairseq.trainer | begin training epoch 194
2022-03-05 03:11:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:11:40 | INFO | train_inner | epoch 194:      5 / 97 loss=3.32, nll_loss=1.11, ppl=2.16, wps=24352.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18600, lr=0.000231869, gnorm=0.955, loss_scale=8, train_wall=238, gb_free=21, wall=49707
2022-03-05 03:15:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:15:43 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 12.778 | nll_loss 11.757 | ppl 3461.21 | wps 45506.7 | wpb 510.9 | bsz 1 | num_updates 18692 | best_loss 8.737
2022-03-05 03:15:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 18692 updates
2022-03-05 03:15:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:15:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:15:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 194 @ 18692 updates, score 12.778) (writing took 2.6466684797778726 seconds)
2022-03-05 03:15:45 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-03-05 03:15:45 | INFO | train | epoch 194 | loss 3.317 | nll_loss 1.108 | ppl 2.16 | wps 24593.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18692 | lr 0.000231298 | gnorm 0.937 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 49952
2022-03-05 03:15:45 | INFO | fairseq.trainer | begin training epoch 195
2022-03-05 03:15:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:16:06 | INFO | train_inner | epoch 195:      8 / 97 loss=3.316, nll_loss=1.106, ppl=2.15, wps=24623.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18700, lr=0.000231249, gnorm=0.937, loss_scale=16, train_wall=236, gb_free=21, wall=49973
2022-03-05 03:19:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:19:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:20:01 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 12.777 | nll_loss 11.76 | ppl 3468.37 | wps 44021.3 | wpb 510.9 | bsz 1 | num_updates 18788 | best_loss 8.737
2022-03-05 03:20:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 18788 updates
2022-03-05 03:20:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:20:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:20:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 195 @ 18788 updates, score 12.777) (writing took 2.5672502880916 seconds)
2022-03-05 03:20:04 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-03-05 03:20:04 | INFO | train | epoch 195 | loss 3.314 | nll_loss 1.104 | ppl 2.15 | wps 24341.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18788 | lr 0.000230706 | gnorm 0.94 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 50211
2022-03-05 03:20:04 | INFO | fairseq.trainer | begin training epoch 196
2022-03-05 03:20:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:20:35 | INFO | train_inner | epoch 196:     12 / 97 loss=3.313, nll_loss=1.103, ppl=2.15, wps=24383.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=18800, lr=0.000230633, gnorm=0.938, loss_scale=8, train_wall=238, gb_free=21, wall=50242
2022-03-05 03:24:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:24:20 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 12.796 | nll_loss 11.779 | ppl 3515.19 | wps 43448 | wpb 510.9 | bsz 1 | num_updates 18885 | best_loss 8.737
2022-03-05 03:24:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 18885 updates
2022-03-05 03:24:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:24:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:24:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 196 @ 18885 updates, score 12.796) (writing took 2.452690877020359 seconds)
2022-03-05 03:24:22 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-03-05 03:24:22 | INFO | train | epoch 196 | loss 3.312 | nll_loss 1.102 | ppl 2.15 | wps 24579.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 18885 | lr 0.000230113 | gnorm 0.944 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 50469
2022-03-05 03:24:22 | INFO | fairseq.trainer | begin training epoch 197
2022-03-05 03:24:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:25:01 | INFO | train_inner | epoch 197:     15 / 97 loss=3.31, nll_loss=1.1, ppl=2.14, wps=24590.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=18900, lr=0.000230022, gnorm=0.947, loss_scale=8, train_wall=236, gb_free=21, wall=50508
2022-03-05 03:26:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:28:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:28:38 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 12.794 | nll_loss 11.782 | ppl 3520.38 | wps 44763.7 | wpb 510.9 | bsz 1 | num_updates 18981 | best_loss 8.737
2022-03-05 03:28:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 18981 updates
2022-03-05 03:28:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:28:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:28:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 197 @ 18981 updates, score 12.794) (writing took 2.565962804481387 seconds)
2022-03-05 03:28:40 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-03-05 03:28:40 | INFO | train | epoch 197 | loss 3.307 | nll_loss 1.097 | ppl 2.14 | wps 24349.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 18981 | lr 0.000229531 | gnorm 0.938 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 50727
2022-03-05 03:28:40 | INFO | fairseq.trainer | begin training epoch 198
2022-03-05 03:28:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:29:30 | INFO | train_inner | epoch 198:     19 / 97 loss=3.305, nll_loss=1.094, ppl=2.14, wps=24392.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=19000, lr=0.000229416, gnorm=0.933, loss_scale=8, train_wall=238, gb_free=21, wall=50776
2022-03-05 03:32:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:32:56 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 12.853 | nll_loss 11.841 | ppl 3669.57 | wps 45566.2 | wpb 510.9 | bsz 1 | num_updates 19078 | best_loss 8.737
2022-03-05 03:32:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 19078 updates
2022-03-05 03:32:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:32:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:32:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 198 @ 19078 updates, score 12.853) (writing took 2.471230920404196 seconds)
2022-03-05 03:32:58 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-03-05 03:32:58 | INFO | train | epoch 198 | loss 3.304 | nll_loss 1.093 | ppl 2.13 | wps 24611.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19078 | lr 0.000228946 | gnorm 0.941 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 50985
2022-03-05 03:32:58 | INFO | fairseq.trainer | begin training epoch 199
2022-03-05 03:32:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:33:55 | INFO | train_inner | epoch 199:     22 / 97 loss=3.302, nll_loss=1.091, ppl=2.13, wps=24632.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=19100, lr=0.000228814, gnorm=0.942, loss_scale=16, train_wall=236, gb_free=21, wall=51042
2022-03-05 03:37:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:37:14 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 12.784 | nll_loss 11.77 | ppl 3491.77 | wps 44051.4 | wpb 510.9 | bsz 1 | num_updates 19175 | best_loss 8.737
2022-03-05 03:37:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 19175 updates
2022-03-05 03:37:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:37:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:37:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 199 @ 19175 updates, score 12.784) (writing took 2.491426406428218 seconds)
2022-03-05 03:37:16 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-03-05 03:37:16 | INFO | train | epoch 199 | loss 3.301 | nll_loss 1.09 | ppl 2.13 | wps 24617.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19175 | lr 0.000228366 | gnorm 0.941 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 51243
2022-03-05 03:37:16 | INFO | fairseq.trainer | begin training epoch 200
2022-03-05 03:37:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:37:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:38:24 | INFO | train_inner | epoch 200:     26 / 97 loss=3.3, nll_loss=1.089, ppl=2.13, wps=24426, ups=0.37, wpb=65495, bsz=127.9, num_updates=19200, lr=0.000228218, gnorm=0.939, loss_scale=16, train_wall=238, gb_free=21, wall=51310
2022-03-05 03:41:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:41:32 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 12.85 | nll_loss 11.848 | ppl 3687.56 | wps 42827.1 | wpb 510.9 | bsz 1 | num_updates 19271 | best_loss 8.737
2022-03-05 03:41:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 19271 updates
2022-03-05 03:41:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:41:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:41:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 200 @ 19271 updates, score 12.85) (writing took 2.46260594855994 seconds)
2022-03-05 03:41:35 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-03-05 03:41:35 | INFO | train | epoch 200 | loss 3.297 | nll_loss 1.086 | ppl 2.12 | wps 24352.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19271 | lr 0.000227797 | gnorm 0.925 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 51502
2022-03-05 03:41:35 | INFO | fairseq.trainer | begin training epoch 201
2022-03-05 03:41:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:42:50 | INFO | train_inner | epoch 201:     29 / 97 loss=3.297, nll_loss=1.086, ppl=2.12, wps=24599, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=19300, lr=0.000227626, gnorm=0.924, loss_scale=16, train_wall=236, gb_free=21, wall=51577
2022-03-05 03:43:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:44:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:45:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:45:51 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 12.813 | nll_loss 11.809 | ppl 3586.97 | wps 42833.5 | wpb 510.9 | bsz 1 | num_updates 19366 | best_loss 8.737
2022-03-05 03:45:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 19366 updates
2022-03-05 03:45:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:45:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:45:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 201 @ 19366 updates, score 12.813) (writing took 2.473788815550506 seconds)
2022-03-05 03:45:53 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-03-05 03:45:53 | INFO | train | epoch 201 | loss 3.295 | nll_loss 1.083 | ppl 2.12 | wps 24076.9 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 19366 | lr 0.000227238 | gnorm 0.941 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 51760
2022-03-05 03:45:53 | INFO | fairseq.trainer | begin training epoch 202
2022-03-05 03:45:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:47:21 | INFO | train_inner | epoch 202:     34 / 97 loss=3.291, nll_loss=1.079, ppl=2.11, wps=24138.3, ups=0.37, wpb=65495, bsz=127.9, num_updates=19400, lr=0.000227038, gnorm=0.934, loss_scale=8, train_wall=240, gb_free=21, wall=51848
2022-03-05 03:50:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:50:09 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 12.827 | nll_loss 11.822 | ppl 3619.86 | wps 45530.2 | wpb 510.9 | bsz 1 | num_updates 19463 | best_loss 8.737
2022-03-05 03:50:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 19463 updates
2022-03-05 03:50:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:50:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:50:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 202 @ 19463 updates, score 12.827) (writing took 2.5648656534031034 seconds)
2022-03-05 03:50:11 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-03-05 03:50:11 | INFO | train | epoch 202 | loss 3.291 | nll_loss 1.08 | ppl 2.11 | wps 24589.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19463 | lr 0.000226671 | gnorm 0.933 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 52018
2022-03-05 03:50:11 | INFO | fairseq.trainer | begin training epoch 203
2022-03-05 03:50:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:51:47 | INFO | train_inner | epoch 203:     37 / 97 loss=3.292, nll_loss=1.081, ppl=2.12, wps=24614.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=19500, lr=0.000226455, gnorm=0.936, loss_scale=16, train_wall=236, gb_free=21, wall=52114
2022-03-05 03:54:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:54:27 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 12.857 | nll_loss 11.85 | ppl 3692.36 | wps 44923.8 | wpb 510.9 | bsz 1 | num_updates 19560 | best_loss 8.737
2022-03-05 03:54:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 19560 updates
2022-03-05 03:54:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:54:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:54:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 203 @ 19560 updates, score 12.857) (writing took 2.4615717492997646 seconds)
2022-03-05 03:54:30 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-03-05 03:54:30 | INFO | train | epoch 203 | loss 3.289 | nll_loss 1.077 | ppl 2.11 | wps 24590.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19560 | lr 0.000226108 | gnorm 0.939 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 52277
2022-03-05 03:54:30 | INFO | fairseq.trainer | begin training epoch 204
2022-03-05 03:54:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:55:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 03:56:16 | INFO | train_inner | epoch 204:     41 / 97 loss=3.286, nll_loss=1.075, ppl=2.11, wps=24381.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=19600, lr=0.000225877, gnorm=0.946, loss_scale=16, train_wall=238, gb_free=21, wall=52383
2022-03-05 03:58:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:58:45 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 12.804 | nll_loss 11.794 | ppl 3550.13 | wps 45184.3 | wpb 510.9 | bsz 1 | num_updates 19656 | best_loss 8.737
2022-03-05 03:58:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 19656 updates
2022-03-05 03:58:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:58:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 03:58:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 204 @ 19656 updates, score 12.804) (writing took 2.472818110138178 seconds)
2022-03-05 03:58:48 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-03-05 03:58:48 | INFO | train | epoch 204 | loss 3.285 | nll_loss 1.073 | ppl 2.1 | wps 24357.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19656 | lr 0.000225555 | gnorm 0.935 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 52535
2022-03-05 03:58:48 | INFO | fairseq.trainer | begin training epoch 205
2022-03-05 03:58:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:59:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:00:44 | INFO | train_inner | epoch 205:     45 / 97 loss=3.286, nll_loss=1.074, ppl=2.11, wps=24384.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=19700, lr=0.000225303, gnorm=0.938, loss_scale=8, train_wall=238, gb_free=21, wall=52651
2022-03-05 04:02:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:03:04 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 12.838 | nll_loss 11.836 | ppl 3656.07 | wps 44681.1 | wpb 510.9 | bsz 1 | num_updates 19752 | best_loss 8.737
2022-03-05 04:03:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 19752 updates
2022-03-05 04:03:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:03:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:03:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 205 @ 19752 updates, score 12.838) (writing took 2.551530571654439 seconds)
2022-03-05 04:03:06 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-03-05 04:03:06 | INFO | train | epoch 205 | loss 3.283 | nll_loss 1.071 | ppl 2.1 | wps 24321.7 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19752 | lr 0.000225006 | gnorm 0.935 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 52793
2022-03-05 04:03:06 | INFO | fairseq.trainer | begin training epoch 206
2022-03-05 04:03:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:05:11 | INFO | train_inner | epoch 206:     48 / 97 loss=3.281, nll_loss=1.069, ppl=2.1, wps=24597, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=19800, lr=0.000224733, gnorm=0.926, loss_scale=8, train_wall=236, gb_free=21, wall=52918
2022-03-05 04:07:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:07:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:07:22 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 12.879 | nll_loss 11.886 | ppl 3784.51 | wps 43019.3 | wpb 510.9 | bsz 1 | num_updates 19848 | best_loss 8.737
2022-03-05 04:07:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 19848 updates
2022-03-05 04:07:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:07:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:07:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 206 @ 19848 updates, score 12.879) (writing took 2.627965895459056 seconds)
2022-03-05 04:07:25 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-03-05 04:07:25 | INFO | train | epoch 206 | loss 3.28 | nll_loss 1.068 | ppl 2.1 | wps 24317.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 19848 | lr 0.000224461 | gnorm 0.928 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 53052
2022-03-05 04:07:25 | INFO | fairseq.trainer | begin training epoch 207
2022-03-05 04:07:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:09:39 | INFO | train_inner | epoch 207:     52 / 97 loss=3.278, nll_loss=1.066, ppl=2.09, wps=24381.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=19900, lr=0.000224168, gnorm=0.933, loss_scale=8, train_wall=238, gb_free=21, wall=53186
2022-03-05 04:11:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:11:41 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 12.76 | nll_loss 11.747 | ppl 3437.56 | wps 42784.4 | wpb 510.9 | bsz 1 | num_updates 19945 | best_loss 8.737
2022-03-05 04:11:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 19945 updates
2022-03-05 04:11:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:11:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:11:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 207 @ 19945 updates, score 12.76) (writing took 2.487416069023311 seconds)
2022-03-05 04:11:43 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-03-05 04:11:43 | INFO | train | epoch 207 | loss 3.277 | nll_loss 1.065 | ppl 2.09 | wps 24601.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 19945 | lr 0.000223915 | gnorm 0.933 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 53310
2022-03-05 04:11:43 | INFO | fairseq.trainer | begin training epoch 208
2022-03-05 04:11:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:13:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:14:08 | INFO | train_inner | epoch 208:     56 / 97 loss=3.276, nll_loss=1.064, ppl=2.09, wps=24364.6, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=20000, lr=0.000223607, gnorm=0.924, loss_scale=8, train_wall=238, gb_free=21, wall=53455
2022-03-05 04:15:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:15:59 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 12.775 | nll_loss 11.763 | ppl 3474.56 | wps 42731.4 | wpb 510.9 | bsz 1 | num_updates 20041 | best_loss 8.737
2022-03-05 04:15:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 20041 updates
2022-03-05 04:15:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:16:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:16:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 208 @ 20041 updates, score 12.775) (writing took 2.512879913672805 seconds)
2022-03-05 04:16:02 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-03-05 04:16:02 | INFO | train | epoch 208 | loss 3.274 | nll_loss 1.061 | ppl 2.09 | wps 24325.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20041 | lr 0.000223378 | gnorm 0.924 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 53569
2022-03-05 04:16:02 | INFO | fairseq.trainer | begin training epoch 209
2022-03-05 04:16:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:18:34 | INFO | train_inner | epoch 209:     59 / 97 loss=3.271, nll_loss=1.059, ppl=2.08, wps=24586.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=20100, lr=0.00022305, gnorm=0.916, loss_scale=8, train_wall=236, gb_free=21, wall=53721
2022-03-05 04:20:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:20:17 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 12.825 | nll_loss 11.823 | ppl 3622.47 | wps 44387.9 | wpb 510.9 | bsz 1 | num_updates 20138 | best_loss 8.737
2022-03-05 04:20:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 20138 updates
2022-03-05 04:20:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:20:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:20:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 209 @ 20138 updates, score 12.825) (writing took 2.49846316780895 seconds)
2022-03-05 04:20:20 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-03-05 04:20:20 | INFO | train | epoch 209 | loss 3.27 | nll_loss 1.058 | ppl 2.08 | wps 24614.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20138 | lr 0.000222839 | gnorm 0.913 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 53827
2022-03-05 04:20:20 | INFO | fairseq.trainer | begin training epoch 210
2022-03-05 04:20:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:23:00 | INFO | train_inner | epoch 210:     62 / 97 loss=3.27, nll_loss=1.057, ppl=2.08, wps=24669.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=20200, lr=0.000222497, gnorm=0.936, loss_scale=16, train_wall=235, gb_free=21, wall=53987
2022-03-05 04:24:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:24:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:24:35 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 12.826 | nll_loss 11.829 | ppl 3639.41 | wps 43733.6 | wpb 510.9 | bsz 1 | num_updates 20234 | best_loss 8.737
2022-03-05 04:24:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 20234 updates
2022-03-05 04:24:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:24:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:24:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 210 @ 20234 updates, score 12.826) (writing took 2.537349027581513 seconds)
2022-03-05 04:24:38 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-03-05 04:24:38 | INFO | train | epoch 210 | loss 3.268 | nll_loss 1.055 | ppl 2.08 | wps 24366.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20234 | lr 0.00022231 | gnorm 0.944 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 54085
2022-03-05 04:24:38 | INFO | fairseq.trainer | begin training epoch 211
2022-03-05 04:24:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:27:28 | INFO | train_inner | epoch 211:     66 / 97 loss=3.265, nll_loss=1.053, ppl=2.07, wps=24414.5, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=20300, lr=0.000221948, gnorm=0.937, loss_scale=8, train_wall=238, gb_free=21, wall=54255
2022-03-05 04:28:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:28:53 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 12.892 | nll_loss 11.902 | ppl 3825.92 | wps 43933.3 | wpb 510.9 | bsz 1 | num_updates 20331 | best_loss 8.737
2022-03-05 04:28:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 20331 updates
2022-03-05 04:28:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:28:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:28:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 211 @ 20331 updates, score 12.892) (writing took 2.5326309595257044 seconds)
2022-03-05 04:28:56 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-03-05 04:28:56 | INFO | train | epoch 211 | loss 3.265 | nll_loss 1.052 | ppl 2.07 | wps 24622.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20331 | lr 0.000221779 | gnorm 0.935 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 54343
2022-03-05 04:28:56 | INFO | fairseq.trainer | begin training epoch 212
2022-03-05 04:28:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:31:54 | INFO | train_inner | epoch 212:     69 / 97 loss=3.262, nll_loss=1.049, ppl=2.07, wps=24610.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=20400, lr=0.000221404, gnorm=0.93, loss_scale=16, train_wall=236, gb_free=21, wall=54521
2022-03-05 04:33:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:33:12 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 12.901 | nll_loss 11.91 | ppl 3847.31 | wps 43469.5 | wpb 510.9 | bsz 1 | num_updates 20428 | best_loss 8.737
2022-03-05 04:33:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 20428 updates
2022-03-05 04:33:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:33:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:33:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 212 @ 20428 updates, score 12.901) (writing took 2.948595656082034 seconds)
2022-03-05 04:33:15 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-03-05 04:33:15 | INFO | train | epoch 212 | loss 3.261 | nll_loss 1.048 | ppl 2.07 | wps 24545.3 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 20428 | lr 0.000221252 | gnorm 0.926 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 54601
2022-03-05 04:33:15 | INFO | fairseq.trainer | begin training epoch 213
2022-03-05 04:33:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:35:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:36:23 | INFO | train_inner | epoch 213:     73 / 97 loss=3.261, nll_loss=1.048, ppl=2.07, wps=24362.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20500, lr=0.000220863, gnorm=0.932, loss_scale=16, train_wall=238, gb_free=21, wall=54790
2022-03-05 04:37:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:37:30 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 12.835 | nll_loss 11.834 | ppl 3651.26 | wps 42799.7 | wpb 510.9 | bsz 1 | num_updates 20524 | best_loss 8.737
2022-03-05 04:37:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 20524 updates
2022-03-05 04:37:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:37:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:37:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 213 @ 20524 updates, score 12.835) (writing took 2.5254611875861883 seconds)
2022-03-05 04:37:33 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-03-05 04:37:33 | INFO | train | epoch 213 | loss 3.258 | nll_loss 1.045 | ppl 2.06 | wps 24345.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20524 | lr 0.000220734 | gnorm 0.931 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 54860
2022-03-05 04:37:33 | INFO | fairseq.trainer | begin training epoch 214
2022-03-05 04:37:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:40:49 | INFO | train_inner | epoch 214:     76 / 97 loss=3.256, nll_loss=1.042, ppl=2.06, wps=24596.6, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=20600, lr=0.000220326, gnorm=0.922, loss_scale=16, train_wall=236, gb_free=21, wall=55056
2022-03-05 04:41:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:41:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:41:48 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 12.834 | nll_loss 11.829 | ppl 3639.37 | wps 44154.2 | wpb 510.9 | bsz 1 | num_updates 20620 | best_loss 8.737
2022-03-05 04:41:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 20620 updates
2022-03-05 04:41:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:41:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:41:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 214 @ 20620 updates, score 12.834) (writing took 2.488385065458715 seconds)
2022-03-05 04:41:51 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-03-05 04:41:51 | INFO | train | epoch 214 | loss 3.255 | nll_loss 1.042 | ppl 2.06 | wps 24359.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20620 | lr 0.000220219 | gnorm 0.921 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 55118
2022-03-05 04:41:51 | INFO | fairseq.trainer | begin training epoch 215
2022-03-05 04:41:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:45:17 | INFO | train_inner | epoch 215:     80 / 97 loss=3.256, nll_loss=1.043, ppl=2.06, wps=24439.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20700, lr=0.000219793, gnorm=0.922, loss_scale=16, train_wall=237, gb_free=21, wall=55324
2022-03-05 04:46:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:46:06 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 12.857 | nll_loss 11.849 | ppl 3689.02 | wps 43574.1 | wpb 510.9 | bsz 1 | num_updates 20717 | best_loss 8.737
2022-03-05 04:46:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 20717 updates
2022-03-05 04:46:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:46:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:46:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 215 @ 20717 updates, score 12.857) (writing took 2.533254095353186 seconds)
2022-03-05 04:46:09 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-03-05 04:46:09 | INFO | train | epoch 215 | loss 3.254 | nll_loss 1.041 | ppl 2.06 | wps 24637.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 20717 | lr 0.000219703 | gnorm 0.921 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 55376
2022-03-05 04:46:09 | INFO | fairseq.trainer | begin training epoch 216
2022-03-05 04:46:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:46:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:49:45 | INFO | train_inner | epoch 216:     84 / 97 loss=3.252, nll_loss=1.039, ppl=2.05, wps=24437.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20800, lr=0.000219265, gnorm=0.926, loss_scale=16, train_wall=237, gb_free=21, wall=55592
2022-03-05 04:50:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:50:24 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 12.828 | nll_loss 11.835 | ppl 3652.84 | wps 42743.7 | wpb 510.9 | bsz 1 | num_updates 20813 | best_loss 8.737
2022-03-05 04:50:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 20813 updates
2022-03-05 04:50:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:50:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:50:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 216 @ 20813 updates, score 12.828) (writing took 2.463278274051845 seconds)
2022-03-05 04:50:26 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-03-05 04:50:26 | INFO | train | epoch 216 | loss 3.25 | nll_loss 1.037 | ppl 2.05 | wps 24399.5 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20813 | lr 0.000219196 | gnorm 0.925 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 55633
2022-03-05 04:50:26 | INFO | fairseq.trainer | begin training epoch 217
2022-03-05 04:50:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:52:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 04:54:14 | INFO | train_inner | epoch 217:     88 / 97 loss=3.25, nll_loss=1.037, ppl=2.05, wps=24354.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=20900, lr=0.000218739, gnorm=0.928, loss_scale=16, train_wall=238, gb_free=21, wall=55861
2022-03-05 04:54:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:54:43 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 12.846 | nll_loss 11.854 | ppl 3701.67 | wps 42874.9 | wpb 510.9 | bsz 1 | num_updates 20909 | best_loss 8.737
2022-03-05 04:54:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 20909 updates
2022-03-05 04:54:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:54:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:54:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 217 @ 20909 updates, score 12.846) (writing took 2.5018086098134518 seconds)
2022-03-05 04:54:45 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-03-05 04:54:45 | INFO | train | epoch 217 | loss 3.249 | nll_loss 1.035 | ppl 2.05 | wps 24310.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 20909 | lr 0.000218692 | gnorm 0.931 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 55892
2022-03-05 04:54:45 | INFO | fairseq.trainer | begin training epoch 218
2022-03-05 04:54:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:58:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:58:43 | INFO | train_inner | epoch 218:     92 / 97 loss=3.246, nll_loss=1.033, ppl=2.05, wps=24368.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21000, lr=0.000218218, gnorm=0.931, loss_scale=8, train_wall=238, gb_free=21, wall=56130
2022-03-05 04:58:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:59:01 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 12.871 | nll_loss 11.878 | ppl 3763.03 | wps 43515.4 | wpb 510.9 | bsz 1 | num_updates 21005 | best_loss 8.737
2022-03-05 04:59:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 21005 updates
2022-03-05 04:59:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:59:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 04:59:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 218 @ 21005 updates, score 12.871) (writing took 2.557182907126844 seconds)
2022-03-05 04:59:03 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-03-05 04:59:03 | INFO | train | epoch 218 | loss 3.246 | nll_loss 1.032 | ppl 2.04 | wps 24334.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21005 | lr 0.000218192 | gnorm 0.928 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 56150
2022-03-05 04:59:03 | INFO | fairseq.trainer | begin training epoch 219
2022-03-05 04:59:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:03:09 | INFO | train_inner | epoch 219:     95 / 97 loss=3.244, nll_loss=1.03, ppl=2.04, wps=24639.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21100, lr=0.0002177, gnorm=0.918, loss_scale=8, train_wall=235, gb_free=21, wall=56396
2022-03-05 05:03:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:03:19 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 12.861 | nll_loss 11.867 | ppl 3736.44 | wps 44212.3 | wpb 510.9 | bsz 1 | num_updates 21102 | best_loss 8.737
2022-03-05 05:03:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 21102 updates
2022-03-05 05:03:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:03:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:03:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 219 @ 21102 updates, score 12.861) (writing took 2.5600344073027372 seconds)
2022-03-05 05:03:21 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-03-05 05:03:21 | INFO | train | epoch 219 | loss 3.242 | nll_loss 1.028 | ppl 2.04 | wps 24618.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21102 | lr 0.00021769 | gnorm 0.918 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 56408
2022-03-05 05:03:21 | INFO | fairseq.trainer | begin training epoch 220
2022-03-05 05:03:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:04:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:07:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:07:37 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 12.893 | nll_loss 11.905 | ppl 3836.09 | wps 42897 | wpb 510.9 | bsz 1 | num_updates 21198 | best_loss 8.737
2022-03-05 05:07:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 21198 updates
2022-03-05 05:07:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:07:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:07:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 220 @ 21198 updates, score 12.893) (writing took 2.519501361064613 seconds)
2022-03-05 05:07:40 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-03-05 05:07:40 | INFO | train | epoch 220 | loss 3.24 | nll_loss 1.026 | ppl 2.04 | wps 24363 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 21198 | lr 0.000217196 | gnorm 0.916 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 56666
2022-03-05 05:07:40 | INFO | fairseq.trainer | begin training epoch 221
2022-03-05 05:07:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:07:45 | INFO | train_inner | epoch 221:      2 / 97 loss=3.24, nll_loss=1.026, ppl=2.04, wps=23716.7, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=21200, lr=0.000217186, gnorm=0.917, loss_scale=8, train_wall=238, gb_free=21, wall=56672
2022-03-05 05:11:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:11:55 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 12.913 | nll_loss 11.935 | ppl 3915.98 | wps 43027.5 | wpb 510.9 | bsz 1 | num_updates 21295 | best_loss 8.737
2022-03-05 05:11:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 21295 updates
2022-03-05 05:11:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:11:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:11:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 221 @ 21295 updates, score 12.913) (writing took 2.5393422190099955 seconds)
2022-03-05 05:11:58 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-03-05 05:11:58 | INFO | train | epoch 221 | loss 3.238 | nll_loss 1.024 | ppl 2.03 | wps 24578.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21295 | lr 0.000216701 | gnorm 0.92 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 56925
2022-03-05 05:11:58 | INFO | fairseq.trainer | begin training epoch 222
2022-03-05 05:11:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:12:11 | INFO | train_inner | epoch 222:      5 / 97 loss=3.237, nll_loss=1.023, ppl=2.03, wps=24602.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21300, lr=0.000216676, gnorm=0.919, loss_scale=16, train_wall=236, gb_free=21, wall=56938
2022-03-05 05:16:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:16:14 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 12.937 | nll_loss 11.951 | ppl 3958.39 | wps 43136.9 | wpb 510.9 | bsz 1 | num_updates 21392 | best_loss 8.737
2022-03-05 05:16:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 21392 updates
2022-03-05 05:16:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:16:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:16:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 222 @ 21392 updates, score 12.937) (writing took 2.552155436947942 seconds)
2022-03-05 05:16:17 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-03-05 05:16:17 | INFO | train | epoch 222 | loss 3.235 | nll_loss 1.021 | ppl 2.03 | wps 24557.9 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 21392 | lr 0.000216209 | gnorm 0.916 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 57184
2022-03-05 05:16:17 | INFO | fairseq.trainer | begin training epoch 223
2022-03-05 05:16:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:16:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:16:40 | INFO | train_inner | epoch 223:      9 / 97 loss=3.234, nll_loss=1.02, ppl=2.03, wps=24347.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21400, lr=0.000216169, gnorm=0.913, loss_scale=16, train_wall=238, gb_free=21, wall=57207
2022-03-05 05:18:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:20:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:20:32 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 12.946 | nll_loss 11.963 | ppl 3993.39 | wps 44621.4 | wpb 510.9 | bsz 1 | num_updates 21487 | best_loss 8.737
2022-03-05 05:20:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 21487 updates
2022-03-05 05:20:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:20:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:20:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 223 @ 21487 updates, score 12.946) (writing took 2.4925612527877092 seconds)
2022-03-05 05:20:35 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-03-05 05:20:35 | INFO | train | epoch 223 | loss 3.232 | nll_loss 1.018 | ppl 2.03 | wps 24088.3 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 21487 | lr 0.000215731 | gnorm 0.914 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 57442
2022-03-05 05:20:35 | INFO | fairseq.trainer | begin training epoch 224
2022-03-05 05:20:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:21:09 | INFO | train_inner | epoch 224:     13 / 97 loss=3.229, nll_loss=1.014, ppl=2.02, wps=24369.9, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21500, lr=0.000215666, gnorm=0.917, loss_scale=8, train_wall=238, gb_free=21, wall=57476
2022-03-05 05:24:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:24:51 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 12.839 | nll_loss 11.844 | ppl 3677.47 | wps 43529.9 | wpb 510.9 | bsz 1 | num_updates 21584 | best_loss 8.737
2022-03-05 05:24:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 21584 updates
2022-03-05 05:24:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:24:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:24:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 224 @ 21584 updates, score 12.839) (writing took 2.5228333780542016 seconds)
2022-03-05 05:24:53 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-03-05 05:24:53 | INFO | train | epoch 224 | loss 3.229 | nll_loss 1.015 | ppl 2.02 | wps 24580.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21584 | lr 0.000215245 | gnorm 0.922 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 57700
2022-03-05 05:24:53 | INFO | fairseq.trainer | begin training epoch 225
2022-03-05 05:24:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:25:35 | INFO | train_inner | epoch 225:     16 / 97 loss=3.229, nll_loss=1.014, ppl=2.02, wps=24610.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21600, lr=0.000215166, gnorm=0.921, loss_scale=16, train_wall=236, gb_free=21, wall=57742
2022-03-05 05:29:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:29:09 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 12.873 | nll_loss 11.88 | ppl 3769.29 | wps 44752.3 | wpb 510.9 | bsz 1 | num_updates 21681 | best_loss 8.737
2022-03-05 05:29:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 21681 updates
2022-03-05 05:29:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:29:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:29:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 225 @ 21681 updates, score 12.873) (writing took 2.55643395986408 seconds)
2022-03-05 05:29:12 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-03-05 05:29:12 | INFO | train | epoch 225 | loss 3.227 | nll_loss 1.013 | ppl 2.02 | wps 24568 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21681 | lr 0.000214763 | gnorm 0.915 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 57959
2022-03-05 05:29:12 | INFO | fairseq.trainer | begin training epoch 226
2022-03-05 05:29:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:29:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 05:30:04 | INFO | train_inner | epoch 226:     20 / 97 loss=3.226, nll_loss=1.011, ppl=2.02, wps=24357.6, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21700, lr=0.000214669, gnorm=0.911, loss_scale=16, train_wall=238, gb_free=21, wall=58011
2022-03-05 05:32:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:33:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:33:28 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 12.859 | nll_loss 11.876 | ppl 3757.44 | wps 43931.3 | wpb 510.9 | bsz 1 | num_updates 21776 | best_loss 8.737
2022-03-05 05:33:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 21776 updates
2022-03-05 05:33:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:33:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:33:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 226 @ 21776 updates, score 12.859) (writing took 2.434260882437229 seconds)
2022-03-05 05:33:30 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-03-05 05:33:30 | INFO | train | epoch 226 | loss 3.224 | nll_loss 1.009 | ppl 2.01 | wps 24099.6 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 21776 | lr 0.000214294 | gnorm 0.912 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 58217
2022-03-05 05:33:30 | INFO | fairseq.trainer | begin training epoch 227
2022-03-05 05:33:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:34:32 | INFO | train_inner | epoch 227:     24 / 97 loss=3.224, nll_loss=1.009, ppl=2.01, wps=24375.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=21800, lr=0.000214176, gnorm=0.919, loss_scale=8, train_wall=238, gb_free=21, wall=58279
2022-03-05 05:37:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:37:46 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 12.906 | nll_loss 11.918 | ppl 3868.86 | wps 45180.6 | wpb 510.9 | bsz 1 | num_updates 21873 | best_loss 8.737
2022-03-05 05:37:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 21873 updates
2022-03-05 05:37:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:37:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:37:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 227 @ 21873 updates, score 12.906) (writing took 2.6035953564569354 seconds)
2022-03-05 05:37:49 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-03-05 05:37:49 | INFO | train | epoch 227 | loss 3.224 | nll_loss 1.01 | ppl 2.01 | wps 24572.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21873 | lr 0.000213819 | gnorm 0.927 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 58476
2022-03-05 05:37:49 | INFO | fairseq.trainer | begin training epoch 228
2022-03-05 05:37:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:38:58 | INFO | train_inner | epoch 228:     27 / 97 loss=3.222, nll_loss=1.007, ppl=2.01, wps=24618.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=21900, lr=0.000213687, gnorm=0.917, loss_scale=16, train_wall=236, gb_free=21, wall=58545
2022-03-05 05:41:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:42:04 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 12.895 | nll_loss 11.906 | ppl 3838.04 | wps 45458.1 | wpb 510.9 | bsz 1 | num_updates 21970 | best_loss 8.737
2022-03-05 05:42:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 21970 updates
2022-03-05 05:42:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:42:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:42:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 228 @ 21970 updates, score 12.895) (writing took 2.504142683930695 seconds)
2022-03-05 05:42:07 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-03-05 05:42:07 | INFO | train | epoch 228 | loss 3.22 | nll_loss 1.004 | ppl 2.01 | wps 24616.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 21970 | lr 0.000213346 | gnorm 0.914 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 58734
2022-03-05 05:42:07 | INFO | fairseq.trainer | begin training epoch 229
2022-03-05 05:42:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:43:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:43:27 | INFO | train_inner | epoch 229:     31 / 97 loss=3.218, nll_loss=1.003, ppl=2, wps=24393.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=22000, lr=0.000213201, gnorm=0.921, loss_scale=8, train_wall=238, gb_free=21, wall=58814
2022-03-05 05:46:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:46:22 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 12.89 | nll_loss 11.904 | ppl 3833.22 | wps 44780.7 | wpb 510.9 | bsz 1 | num_updates 22066 | best_loss 8.737
2022-03-05 05:46:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 22066 updates
2022-03-05 05:46:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:46:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:46:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 229 @ 22066 updates, score 12.89) (writing took 2.5244172224774957 seconds)
2022-03-05 05:46:25 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-03-05 05:46:25 | INFO | train | epoch 229 | loss 3.216 | nll_loss 1.001 | ppl 2 | wps 24387.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22066 | lr 0.000212882 | gnorm 0.924 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 58991
2022-03-05 05:46:25 | INFO | fairseq.trainer | begin training epoch 230
2022-03-05 05:46:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:47:52 | INFO | train_inner | epoch 230:     34 / 97 loss=3.215, nll_loss=1, ppl=2, wps=24663.5, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=22100, lr=0.000212718, gnorm=0.919, loss_scale=8, train_wall=235, gb_free=21, wall=59079
2022-03-05 05:50:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:50:40 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 12.878 | nll_loss 11.889 | ppl 3792.56 | wps 45443.8 | wpb 510.9 | bsz 1 | num_updates 22163 | best_loss 8.737
2022-03-05 05:50:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 22163 updates
2022-03-05 05:50:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:50:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:50:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 230 @ 22163 updates, score 12.878) (writing took 2.4776456132531166 seconds)
2022-03-05 05:50:43 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-03-05 05:50:43 | INFO | train | epoch 230 | loss 3.215 | nll_loss 1 | ppl 2 | wps 24603.3 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22163 | lr 0.000212415 | gnorm 0.912 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 59250
2022-03-05 05:50:43 | INFO | fairseq.trainer | begin training epoch 231
2022-03-05 05:50:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:51:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:52:21 | INFO | train_inner | epoch 231:     38 / 97 loss=3.214, nll_loss=0.999, ppl=2, wps=24403.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=22200, lr=0.000212238, gnorm=0.908, loss_scale=8, train_wall=238, gb_free=21, wall=59348
2022-03-05 05:54:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:54:58 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 12.859 | nll_loss 11.879 | ppl 3767.11 | wps 45107.6 | wpb 510.9 | bsz 1 | num_updates 22259 | best_loss 8.737
2022-03-05 05:54:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 22259 updates
2022-03-05 05:54:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:55:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:55:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 231 @ 22259 updates, score 12.859) (writing took 2.530487911775708 seconds)
2022-03-05 05:55:01 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-03-05 05:55:01 | INFO | train | epoch 231 | loss 3.213 | nll_loss 0.998 | ppl 2 | wps 24369.2 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22259 | lr 0.000211957 | gnorm 0.912 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 59508
2022-03-05 05:55:01 | INFO | fairseq.trainer | begin training epoch 232
2022-03-05 05:55:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:56:47 | INFO | train_inner | epoch 232:     41 / 97 loss=3.212, nll_loss=0.996, ppl=1.99, wps=24639.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=22300, lr=0.000211762, gnorm=0.914, loss_scale=16, train_wall=235, gb_free=21, wall=59614
2022-03-05 05:57:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:59:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:59:16 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 12.885 | nll_loss 11.898 | ppl 3817.62 | wps 43789.2 | wpb 510.9 | bsz 1 | num_updates 22355 | best_loss 8.737
2022-03-05 05:59:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 22355 updates
2022-03-05 05:59:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:59:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 05:59:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 232 @ 22355 updates, score 12.885) (writing took 2.5250305011868477 seconds)
2022-03-05 05:59:18 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-03-05 05:59:18 | INFO | train | epoch 232 | loss 3.211 | nll_loss 0.995 | ppl 1.99 | wps 24396.9 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22355 | lr 0.000211501 | gnorm 0.931 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 59765
2022-03-05 05:59:18 | INFO | fairseq.trainer | begin training epoch 233
2022-03-05 05:59:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:01:15 | INFO | train_inner | epoch 233:     45 / 97 loss=3.209, nll_loss=0.994, ppl=1.99, wps=24447.8, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=22400, lr=0.000211289, gnorm=0.923, loss_scale=8, train_wall=237, gb_free=21, wall=59882
2022-03-05 06:03:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:03:34 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 12.925 | nll_loss 11.944 | ppl 3941.11 | wps 43191.8 | wpb 510.9 | bsz 1 | num_updates 22452 | best_loss 8.737
2022-03-05 06:03:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 22452 updates
2022-03-05 06:03:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:03:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:03:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 233 @ 22452 updates, score 12.925) (writing took 2.4648132091388106 seconds)
2022-03-05 06:03:36 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-03-05 06:03:36 | INFO | train | epoch 233 | loss 3.207 | nll_loss 0.991 | ppl 1.99 | wps 24639.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22452 | lr 0.000211044 | gnorm 0.9 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 60023
2022-03-05 06:03:36 | INFO | fairseq.trainer | begin training epoch 234
2022-03-05 06:03:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:05:40 | INFO | train_inner | epoch 234:     48 / 97 loss=3.206, nll_loss=0.99, ppl=1.99, wps=24633.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=22500, lr=0.000210819, gnorm=0.908, loss_scale=16, train_wall=235, gb_free=21, wall=60147
2022-03-05 06:07:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:07:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:07:52 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 12.838 | nll_loss 11.849 | ppl 3689.63 | wps 42591.6 | wpb 510.9 | bsz 1 | num_updates 22548 | best_loss 8.737
2022-03-05 06:07:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 22548 updates
2022-03-05 06:07:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:07:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:07:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 234 @ 22548 updates, score 12.838) (writing took 2.45561433583498 seconds)
2022-03-05 06:07:54 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-03-05 06:07:54 | INFO | train | epoch 234 | loss 3.207 | nll_loss 0.991 | ppl 1.99 | wps 24366.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22548 | lr 0.000210594 | gnorm 0.922 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 60281
2022-03-05 06:07:54 | INFO | fairseq.trainer | begin training epoch 235
2022-03-05 06:07:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:10:09 | INFO | train_inner | epoch 235:     52 / 97 loss=3.206, nll_loss=0.99, ppl=1.99, wps=24419.8, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=22600, lr=0.000210352, gnorm=0.92, loss_scale=8, train_wall=237, gb_free=21, wall=60416
2022-03-05 06:12:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:12:10 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 12.879 | nll_loss 11.894 | ppl 3805.2 | wps 42885.5 | wpb 510.9 | bsz 1 | num_updates 22645 | best_loss 8.737
2022-03-05 06:12:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 22645 updates
2022-03-05 06:12:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:12:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:12:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 235 @ 22645 updates, score 12.879) (writing took 2.4719754876568913 seconds)
2022-03-05 06:12:12 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-03-05 06:12:12 | INFO | train | epoch 235 | loss 3.204 | nll_loss 0.989 | ppl 1.98 | wps 24626.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22645 | lr 0.000210142 | gnorm 0.912 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 60539
2022-03-05 06:12:12 | INFO | fairseq.trainer | begin training epoch 236
2022-03-05 06:12:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:14:34 | INFO | train_inner | epoch 236:     55 / 97 loss=3.205, nll_loss=0.989, ppl=1.99, wps=24634.9, ups=0.38, wpb=65495, bsz=127.9, num_updates=22700, lr=0.000209888, gnorm=0.911, loss_scale=16, train_wall=235, gb_free=21, wall=60681
2022-03-05 06:15:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:16:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:16:28 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 12.891 | nll_loss 11.906 | ppl 3838.18 | wps 43091.6 | wpb 510.9 | bsz 1 | num_updates 22741 | best_loss 8.737
2022-03-05 06:16:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 22741 updates
2022-03-05 06:16:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:16:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:16:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 236 @ 22741 updates, score 12.891) (writing took 2.5232881968840957 seconds)
2022-03-05 06:16:31 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-03-05 06:16:31 | INFO | train | epoch 236 | loss 3.201 | nll_loss 0.985 | ppl 1.98 | wps 24329.1 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22741 | lr 0.000209698 | gnorm 0.906 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 60798
2022-03-05 06:16:31 | INFO | fairseq.trainer | begin training epoch 237
2022-03-05 06:16:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:19:03 | INFO | train_inner | epoch 237:     59 / 97 loss=3.199, nll_loss=0.983, ppl=1.98, wps=24356.8, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=22800, lr=0.000209427, gnorm=0.913, loss_scale=8, train_wall=238, gb_free=21, wall=60950
2022-03-05 06:20:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:20:47 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 12.873 | nll_loss 11.884 | ppl 3780.66 | wps 42802 | wpb 510.9 | bsz 1 | num_updates 22838 | best_loss 8.737
2022-03-05 06:20:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 22838 updates
2022-03-05 06:20:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:20:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:20:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 237 @ 22838 updates, score 12.873) (writing took 2.4740968300029635 seconds)
2022-03-05 06:20:49 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-03-05 06:20:49 | INFO | train | epoch 237 | loss 3.2 | nll_loss 0.984 | ppl 1.98 | wps 24587.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 22838 | lr 0.000209253 | gnorm 0.922 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 61056
2022-03-05 06:20:49 | INFO | fairseq.trainer | begin training epoch 238
2022-03-05 06:20:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:22:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:23:32 | INFO | train_inner | epoch 238:     63 / 97 loss=3.201, nll_loss=0.985, ppl=1.98, wps=24373.8, ups=0.37, wpb=65495, bsz=127.9, num_updates=22900, lr=0.000208969, gnorm=0.925, loss_scale=8, train_wall=238, gb_free=21, wall=61219
2022-03-05 06:25:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:25:05 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 12.945 | nll_loss 11.966 | ppl 4000.84 | wps 42836 | wpb 510.9 | bsz 1 | num_updates 22934 | best_loss 8.737
2022-03-05 06:25:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 22934 updates
2022-03-05 06:25:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:25:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:25:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 238 @ 22934 updates, score 12.945) (writing took 2.454337276518345 seconds)
2022-03-05 06:25:07 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-03-05 06:25:07 | INFO | train | epoch 238 | loss 3.196 | nll_loss 0.98 | ppl 1.97 | wps 24334 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 22934 | lr 0.000208814 | gnorm 0.914 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 61314
2022-03-05 06:25:07 | INFO | fairseq.trainer | begin training epoch 239
2022-03-05 06:25:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:27:58 | INFO | train_inner | epoch 239:     66 / 97 loss=3.193, nll_loss=0.977, ppl=1.97, wps=24637.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23000, lr=0.000208514, gnorm=0.897, loss_scale=8, train_wall=235, gb_free=21, wall=61485
2022-03-05 06:29:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:29:23 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 12.884 | nll_loss 11.908 | ppl 3841.85 | wps 44541.7 | wpb 510.9 | bsz 1 | num_updates 23031 | best_loss 8.737
2022-03-05 06:29:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 23031 updates
2022-03-05 06:29:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:29:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:29:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 239 @ 23031 updates, score 12.884) (writing took 2.5471576135605574 seconds)
2022-03-05 06:29:25 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-03-05 06:29:25 | INFO | train | epoch 239 | loss 3.196 | nll_loss 0.979 | ppl 1.97 | wps 24645.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23031 | lr 0.000208374 | gnorm 0.905 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 61572
2022-03-05 06:29:25 | INFO | fairseq.trainer | begin training epoch 240
2022-03-05 06:29:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:32:23 | INFO | train_inner | epoch 240:     69 / 97 loss=3.194, nll_loss=0.978, ppl=1.97, wps=24666.8, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=23100, lr=0.000208063, gnorm=0.9, loss_scale=16, train_wall=235, gb_free=21, wall=61750
2022-03-05 06:33:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:33:41 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 12.953 | nll_loss 11.974 | ppl 4023.12 | wps 43814.8 | wpb 510.9 | bsz 1 | num_updates 23128 | best_loss 8.737
2022-03-05 06:33:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 23128 updates
2022-03-05 06:33:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:33:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:33:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 240 @ 23128 updates, score 12.953) (writing took 2.479311720468104 seconds)
2022-03-05 06:33:43 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-03-05 06:33:43 | INFO | train | epoch 240 | loss 3.193 | nll_loss 0.977 | ppl 1.97 | wps 24626.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23128 | lr 0.000207937 | gnorm 0.903 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 61830
2022-03-05 06:33:43 | INFO | fairseq.trainer | begin training epoch 241
2022-03-05 06:33:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:34:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:36:52 | INFO | train_inner | epoch 241:     73 / 97 loss=3.192, nll_loss=0.976, ppl=1.97, wps=24366, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23200, lr=0.000207614, gnorm=0.905, loss_scale=16, train_wall=238, gb_free=21, wall=62019
2022-03-05 06:37:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:37:59 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 12.891 | nll_loss 11.914 | ppl 3859.63 | wps 43133.1 | wpb 510.9 | bsz 1 | num_updates 23224 | best_loss 8.737
2022-03-05 06:37:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 23224 updates
2022-03-05 06:37:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:38:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:38:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 241 @ 23224 updates, score 12.891) (writing took 2.5249712765216827 seconds)
2022-03-05 06:38:02 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-03-05 06:38:02 | INFO | train | epoch 241 | loss 3.19 | nll_loss 0.974 | ppl 1.96 | wps 24299.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23224 | lr 0.000207506 | gnorm 0.897 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 62089
2022-03-05 06:38:02 | INFO | fairseq.trainer | begin training epoch 242
2022-03-05 06:38:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:39:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 06:41:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:41:24 | INFO | train_inner | epoch 242:     78 / 97 loss=3.19, nll_loss=0.974, ppl=1.96, wps=24119.5, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23300, lr=0.000207168, gnorm=0.905, loss_scale=8, train_wall=241, gb_free=21, wall=62291
2022-03-05 06:42:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:42:18 | INFO | valid | epoch 242 | valid on 'valid' subset | loss 12.917 | nll_loss 11.943 | ppl 3936.55 | wps 43377.7 | wpb 510.9 | bsz 1 | num_updates 23319 | best_loss 8.737
2022-03-05 06:42:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 242 @ 23319 updates
2022-03-05 06:42:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:42:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:42:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 242 @ 23319 updates, score 12.917) (writing took 2.454365447163582 seconds)
2022-03-05 06:42:20 | INFO | fairseq_cli.train | end of epoch 242 (average epoch stats below)
2022-03-05 06:42:20 | INFO | train | epoch 242 | loss 3.189 | nll_loss 0.972 | ppl 1.96 | wps 24074.6 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 23319 | lr 0.000207083 | gnorm 0.902 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 62347
2022-03-05 06:42:20 | INFO | fairseq.trainer | begin training epoch 243
2022-03-05 06:42:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:45:50 | INFO | train_inner | epoch 243:     81 / 97 loss=3.189, nll_loss=0.973, ppl=1.96, wps=24619.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23400, lr=0.000206725, gnorm=0.915, loss_scale=8, train_wall=236, gb_free=21, wall=62557
2022-03-05 06:46:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:46:36 | INFO | valid | epoch 243 | valid on 'valid' subset | loss 12.887 | nll_loss 11.904 | ppl 3831.65 | wps 42951.3 | wpb 510.9 | bsz 1 | num_updates 23416 | best_loss 8.737
2022-03-05 06:46:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 243 @ 23416 updates
2022-03-05 06:46:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:46:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:46:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 243 @ 23416 updates, score 12.887) (writing took 2.448022248223424 seconds)
2022-03-05 06:46:39 | INFO | fairseq_cli.train | end of epoch 243 (average epoch stats below)
2022-03-05 06:46:39 | INFO | train | epoch 243 | loss 3.187 | nll_loss 0.971 | ppl 1.96 | wps 24591.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23416 | lr 0.000206654 | gnorm 0.918 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 62606
2022-03-05 06:46:39 | INFO | fairseq.trainer | begin training epoch 244
2022-03-05 06:46:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:50:16 | INFO | train_inner | epoch 244:     84 / 97 loss=3.186, nll_loss=0.969, ppl=1.96, wps=24633.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23500, lr=0.000206284, gnorm=0.9, loss_scale=16, train_wall=235, gb_free=21, wall=62823
2022-03-05 06:50:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:50:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:50:54 | INFO | valid | epoch 244 | valid on 'valid' subset | loss 12.993 | nll_loss 12.027 | ppl 4173.44 | wps 43488.7 | wpb 510.9 | bsz 1 | num_updates 23512 | best_loss 8.737
2022-03-05 06:50:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 244 @ 23512 updates
2022-03-05 06:50:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:50:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:50:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 244 @ 23512 updates, score 12.993) (writing took 2.552461488172412 seconds)
2022-03-05 06:50:57 | INFO | fairseq_cli.train | end of epoch 244 (average epoch stats below)
2022-03-05 06:50:57 | INFO | train | epoch 244 | loss 3.184 | nll_loss 0.967 | ppl 1.96 | wps 24368.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23512 | lr 0.000206232 | gnorm 0.898 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 62864
2022-03-05 06:50:57 | INFO | fairseq.trainer | begin training epoch 245
2022-03-05 06:50:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:54:44 | INFO | train_inner | epoch 245:     88 / 97 loss=3.182, nll_loss=0.965, ppl=1.95, wps=24383.8, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23600, lr=0.000205847, gnorm=0.909, loss_scale=8, train_wall=238, gb_free=21, wall=63091
2022-03-05 06:55:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:55:12 | INFO | valid | epoch 245 | valid on 'valid' subset | loss 12.81 | nll_loss 11.819 | ppl 3611.85 | wps 43772.1 | wpb 510.9 | bsz 1 | num_updates 23609 | best_loss 8.737
2022-03-05 06:55:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 245 @ 23609 updates
2022-03-05 06:55:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:55:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:55:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 245 @ 23609 updates, score 12.81) (writing took 2.506477888673544 seconds)
2022-03-05 06:55:15 | INFO | fairseq_cli.train | end of epoch 245 (average epoch stats below)
2022-03-05 06:55:15 | INFO | train | epoch 245 | loss 3.181 | nll_loss 0.964 | ppl 1.95 | wps 24602.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23609 | lr 0.000205808 | gnorm 0.902 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 63122
2022-03-05 06:55:15 | INFO | fairseq.trainer | begin training epoch 246
2022-03-05 06:55:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:57:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:59:13 | INFO | train_inner | epoch 246:     92 / 97 loss=3.18, nll_loss=0.963, ppl=1.95, wps=24392, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=23700, lr=0.000205412, gnorm=0.898, loss_scale=8, train_wall=238, gb_free=21, wall=63360
2022-03-05 06:59:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:59:31 | INFO | valid | epoch 246 | valid on 'valid' subset | loss 12.959 | nll_loss 11.988 | ppl 4063.01 | wps 43287.8 | wpb 510.9 | bsz 1 | num_updates 23705 | best_loss 8.737
2022-03-05 06:59:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 246 @ 23705 updates
2022-03-05 06:59:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:59:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 06:59:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 246 @ 23705 updates, score 12.959) (writing took 2.5031410260125995 seconds)
2022-03-05 06:59:33 | INFO | fairseq_cli.train | end of epoch 246 (average epoch stats below)
2022-03-05 06:59:33 | INFO | train | epoch 246 | loss 3.179 | nll_loss 0.962 | ppl 1.95 | wps 24336.3 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23705 | lr 0.00020539 | gnorm 0.9 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 63380
2022-03-05 06:59:33 | INFO | fairseq.trainer | begin training epoch 247
2022-03-05 06:59:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:03:39 | INFO | train_inner | epoch 247:     95 / 97 loss=3.179, nll_loss=0.963, ppl=1.95, wps=24622.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=23800, lr=0.00020498, gnorm=0.906, loss_scale=16, train_wall=235, gb_free=21, wall=63626
2022-03-05 07:03:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:03:49 | INFO | valid | epoch 247 | valid on 'valid' subset | loss 12.839 | nll_loss 11.853 | ppl 3700.46 | wps 42644.4 | wpb 510.9 | bsz 1 | num_updates 23802 | best_loss 8.737
2022-03-05 07:03:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 247 @ 23802 updates
2022-03-05 07:03:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:03:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:03:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 247 @ 23802 updates, score 12.839) (writing took 2.5541194546967745 seconds)
2022-03-05 07:03:52 | INFO | fairseq_cli.train | end of epoch 247 (average epoch stats below)
2022-03-05 07:03:52 | INFO | train | epoch 247 | loss 3.179 | nll_loss 0.962 | ppl 1.95 | wps 24591.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 23802 | lr 0.000204971 | gnorm 0.907 | loss_scale 16 | train_wall 228 | gb_free 21 | wall 63638
2022-03-05 07:03:52 | INFO | fairseq.trainer | begin training epoch 248
2022-03-05 07:03:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:05:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:08:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:08:08 | INFO | valid | epoch 248 | valid on 'valid' subset | loss 12.875 | nll_loss 11.895 | ppl 3807.2 | wps 43926.3 | wpb 510.9 | bsz 1 | num_updates 23898 | best_loss 8.737
2022-03-05 07:08:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 248 @ 23898 updates
2022-03-05 07:08:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:08:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:08:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 248 @ 23898 updates, score 12.875) (writing took 2.462460480630398 seconds)
2022-03-05 07:08:10 | INFO | fairseq_cli.train | end of epoch 248 (average epoch stats below)
2022-03-05 07:08:10 | INFO | train | epoch 248 | loss 3.175 | nll_loss 0.958 | ppl 1.94 | wps 24313.6 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 23898 | lr 0.000204559 | gnorm 0.901 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 63897
2022-03-05 07:08:10 | INFO | fairseq.trainer | begin training epoch 249
2022-03-05 07:08:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:08:15 | INFO | train_inner | epoch 249:      2 / 97 loss=3.175, nll_loss=0.958, ppl=1.94, wps=23657.1, ups=0.36, wpb=65451.9, bsz=127.8, num_updates=23900, lr=0.000204551, gnorm=0.9, loss_scale=8, train_wall=238, gb_free=21, wall=63902
2022-03-05 07:12:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:12:26 | INFO | valid | epoch 249 | valid on 'valid' subset | loss 12.927 | nll_loss 11.955 | ppl 3970.26 | wps 43065.2 | wpb 510.9 | bsz 1 | num_updates 23995 | best_loss 8.737
2022-03-05 07:12:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 249 @ 23995 updates
2022-03-05 07:12:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:12:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:12:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 249 @ 23995 updates, score 12.927) (writing took 2.565192853100598 seconds)
2022-03-05 07:12:29 | INFO | fairseq_cli.train | end of epoch 249 (average epoch stats below)
2022-03-05 07:12:29 | INFO | train | epoch 249 | loss 3.174 | nll_loss 0.957 | ppl 1.94 | wps 24545 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 23995 | lr 0.000204145 | gnorm 0.904 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 64156
2022-03-05 07:12:29 | INFO | fairseq.trainer | begin training epoch 250
2022-03-05 07:12:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:12:42 | INFO | train_inner | epoch 250:      5 / 97 loss=3.173, nll_loss=0.956, ppl=1.94, wps=24574.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=24000, lr=0.000204124, gnorm=0.901, loss_scale=16, train_wall=236, gb_free=21, wall=64169
2022-03-05 07:15:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:16:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:16:45 | INFO | valid | epoch 250 | valid on 'valid' subset | loss 12.946 | nll_loss 11.969 | ppl 4008.19 | wps 43187.2 | wpb 510.9 | bsz 1 | num_updates 24091 | best_loss 8.737
2022-03-05 07:16:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 250 @ 24091 updates
2022-03-05 07:16:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:16:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:16:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 250 @ 24091 updates, score 12.946) (writing took 2.4663000032305717 seconds)
2022-03-05 07:16:47 | INFO | fairseq_cli.train | end of epoch 250 (average epoch stats below)
2022-03-05 07:16:47 | INFO | train | epoch 250 | loss 3.171 | nll_loss 0.953 | ppl 1.94 | wps 24331.8 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24091 | lr 0.000203738 | gnorm 0.883 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 64414
2022-03-05 07:16:47 | INFO | fairseq.trainer | begin training epoch 251
2022-03-05 07:16:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:17:11 | INFO | train_inner | epoch 251:      9 / 97 loss=3.17, nll_loss=0.952, ppl=1.93, wps=24369.1, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=24100, lr=0.0002037, gnorm=0.886, loss_scale=8, train_wall=238, gb_free=21, wall=64438
2022-03-05 07:20:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:21:03 | INFO | valid | epoch 251 | valid on 'valid' subset | loss 12.967 | nll_loss 11.998 | ppl 4091.12 | wps 43834.1 | wpb 510.9 | bsz 1 | num_updates 24188 | best_loss 8.737
2022-03-05 07:21:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 251 @ 24188 updates
2022-03-05 07:21:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:21:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:21:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 251 @ 24188 updates, score 12.967) (writing took 2.50531052891165 seconds)
2022-03-05 07:21:06 | INFO | fairseq_cli.train | end of epoch 251 (average epoch stats below)
2022-03-05 07:21:06 | INFO | train | epoch 251 | loss 3.17 | nll_loss 0.953 | ppl 1.94 | wps 24561.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24188 | lr 0.000203329 | gnorm 0.893 | loss_scale 8 | train_wall 229 | gb_free 21 | wall 64673
2022-03-05 07:21:06 | INFO | fairseq.trainer | begin training epoch 252
2022-03-05 07:21:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:21:37 | INFO | train_inner | epoch 252:     12 / 97 loss=3.169, nll_loss=0.952, ppl=1.94, wps=24571.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=24200, lr=0.000203279, gnorm=0.894, loss_scale=8, train_wall=236, gb_free=21, wall=64704
2022-03-05 07:25:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:25:22 | INFO | valid | epoch 252 | valid on 'valid' subset | loss 12.92 | nll_loss 11.944 | ppl 3938.93 | wps 43294.4 | wpb 510.9 | bsz 1 | num_updates 24285 | best_loss 8.737
2022-03-05 07:25:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 252 @ 24285 updates
2022-03-05 07:25:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:25:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:25:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 252 @ 24285 updates, score 12.92) (writing took 2.496643795631826 seconds)
2022-03-05 07:25:25 | INFO | fairseq_cli.train | end of epoch 252 (average epoch stats below)
2022-03-05 07:25:25 | INFO | train | epoch 252 | loss 3.168 | nll_loss 0.951 | ppl 1.93 | wps 24529.1 | ups 0.37 | wpb 65491.6 | bsz 127.9 | num_updates 24285 | lr 0.000202923 | gnorm 0.896 | loss_scale 16 | train_wall 229 | gb_free 21 | wall 64932
2022-03-05 07:25:25 | INFO | fairseq.trainer | begin training epoch 253
2022-03-05 07:25:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:26:04 | INFO | train_inner | epoch 253:     15 / 97 loss=3.167, nll_loss=0.95, ppl=1.93, wps=24566.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=24300, lr=0.00020286, gnorm=0.899, loss_scale=16, train_wall=236, gb_free=21, wall=64971
2022-03-05 07:27:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:29:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:29:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:29:41 | INFO | valid | epoch 253 | valid on 'valid' subset | loss 12.869 | nll_loss 11.891 | ppl 3796.86 | wps 43444.4 | wpb 510.9 | bsz 1 | num_updates 24380 | best_loss 8.737
2022-03-05 07:29:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 253 @ 24380 updates
2022-03-05 07:29:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:29:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:29:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 253 @ 24380 updates, score 12.869) (writing took 2.505582426674664 seconds)
2022-03-05 07:29:43 | INFO | fairseq_cli.train | end of epoch 253 (average epoch stats below)
2022-03-05 07:29:43 | INFO | train | epoch 253 | loss 3.165 | nll_loss 0.948 | ppl 1.93 | wps 24095.5 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 24380 | lr 0.000202527 | gnorm 0.903 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 65190
2022-03-05 07:29:43 | INFO | fairseq.trainer | begin training epoch 254
2022-03-05 07:29:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:30:35 | INFO | train_inner | epoch 254:     20 / 97 loss=3.163, nll_loss=0.946, ppl=1.93, wps=24165.9, ups=0.37, wpb=65495, bsz=127.9, num_updates=24400, lr=0.000202444, gnorm=0.898, loss_scale=8, train_wall=240, gb_free=21, wall=65242
2022-03-05 07:33:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:33:58 | INFO | valid | epoch 254 | valid on 'valid' subset | loss 12.859 | nll_loss 11.88 | ppl 3768.14 | wps 43859.1 | wpb 510.9 | bsz 1 | num_updates 24477 | best_loss 8.737
2022-03-05 07:33:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 254 @ 24477 updates
2022-03-05 07:33:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:34:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:34:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 254 @ 24477 updates, score 12.859) (writing took 2.457569864578545 seconds)
2022-03-05 07:34:01 | INFO | fairseq_cli.train | end of epoch 254 (average epoch stats below)
2022-03-05 07:34:01 | INFO | train | epoch 254 | loss 3.164 | nll_loss 0.946 | ppl 1.93 | wps 24661.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24477 | lr 0.000202125 | gnorm 0.891 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 65448
2022-03-05 07:34:01 | INFO | fairseq.trainer | begin training epoch 255
2022-03-05 07:34:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:35:00 | INFO | train_inner | epoch 255:     23 / 97 loss=3.163, nll_loss=0.945, ppl=1.93, wps=24687.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=24500, lr=0.000202031, gnorm=0.895, loss_scale=16, train_wall=235, gb_free=21, wall=65507
2022-03-05 07:35:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:38:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:38:15 | INFO | valid | epoch 255 | valid on 'valid' subset | loss 12.909 | nll_loss 11.936 | ppl 3918.27 | wps 45426.6 | wpb 510.9 | bsz 1 | num_updates 24573 | best_loss 8.737
2022-03-05 07:38:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 255 @ 24573 updates
2022-03-05 07:38:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:38:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:38:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 255 @ 24573 updates, score 12.909) (writing took 2.5594719545915723 seconds)
2022-03-05 07:38:18 | INFO | fairseq_cli.train | end of epoch 255 (average epoch stats below)
2022-03-05 07:38:18 | INFO | train | epoch 255 | loss 3.161 | nll_loss 0.944 | ppl 1.92 | wps 24443.4 | ups 0.37 | wpb 65491.1 | bsz 127.9 | num_updates 24573 | lr 0.00020173 | gnorm 0.908 | loss_scale 8 | train_wall 228 | gb_free 21 | wall 65705
2022-03-05 07:38:18 | INFO | fairseq.trainer | begin training epoch 256
2022-03-05 07:38:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:39:27 | INFO | train_inner | epoch 256:     27 / 97 loss=3.16, nll_loss=0.943, ppl=1.92, wps=24533.2, ups=0.37, wpb=65490.8, bsz=127.9, num_updates=24600, lr=0.000201619, gnorm=0.904, loss_scale=8, train_wall=237, gb_free=21, wall=65774
2022-03-05 07:42:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:42:31 | INFO | valid | epoch 256 | valid on 'valid' subset | loss 12.899 | nll_loss 11.918 | ppl 3869.89 | wps 45753.5 | wpb 510.9 | bsz 1 | num_updates 24670 | best_loss 8.737
2022-03-05 07:42:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 256 @ 24670 updates
2022-03-05 07:42:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:42:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:42:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 256 @ 24670 updates, score 12.899) (writing took 2.5857803663238883 seconds)
2022-03-05 07:42:33 | INFO | fairseq_cli.train | end of epoch 256 (average epoch stats below)
2022-03-05 07:42:33 | INFO | train | epoch 256 | loss 3.16 | nll_loss 0.943 | ppl 1.92 | wps 24890.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24670 | lr 0.000201333 | gnorm 0.898 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 65960
2022-03-05 07:42:33 | INFO | fairseq.trainer | begin training epoch 257
2022-03-05 07:42:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:43:50 | INFO | train_inner | epoch 257:     30 / 97 loss=3.159, nll_loss=0.942, ppl=1.92, wps=24924.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=24700, lr=0.000201211, gnorm=0.901, loss_scale=16, train_wall=233, gb_free=21, wall=66037
2022-03-05 07:46:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:46:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:46:45 | INFO | valid | epoch 257 | valid on 'valid' subset | loss 12.892 | nll_loss 11.921 | ppl 3876.68 | wps 45583.2 | wpb 510.9 | bsz 1 | num_updates 24766 | best_loss 8.737
2022-03-05 07:46:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 257 @ 24766 updates
2022-03-05 07:46:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:46:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:46:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 257 @ 24766 updates, score 12.892) (writing took 2.5190065167844296 seconds)
2022-03-05 07:46:48 | INFO | fairseq_cli.train | end of epoch 257 (average epoch stats below)
2022-03-05 07:46:48 | INFO | train | epoch 257 | loss 3.158 | nll_loss 0.94 | ppl 1.92 | wps 24673.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 24766 | lr 0.000200943 | gnorm 0.898 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 66215
2022-03-05 07:46:48 | INFO | fairseq.trainer | begin training epoch 258
2022-03-05 07:46:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:48:15 | INFO | train_inner | epoch 258:     34 / 97 loss=3.155, nll_loss=0.937, ppl=1.91, wps=24703.3, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=24800, lr=0.000200805, gnorm=0.89, loss_scale=16, train_wall=235, gb_free=21, wall=66302
2022-03-05 07:50:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:51:00 | INFO | valid | epoch 258 | valid on 'valid' subset | loss 12.918 | nll_loss 11.945 | ppl 3943.28 | wps 45516.7 | wpb 510.9 | bsz 1 | num_updates 24863 | best_loss 8.737
2022-03-05 07:51:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 258 @ 24863 updates
2022-03-05 07:51:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:51:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:51:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 258 @ 24863 updates, score 12.918) (writing took 2.4639370311051607 seconds)
2022-03-05 07:51:03 | INFO | fairseq_cli.train | end of epoch 258 (average epoch stats below)
2022-03-05 07:51:03 | INFO | train | epoch 258 | loss 3.156 | nll_loss 0.939 | ppl 1.92 | wps 24923.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 24863 | lr 0.00020055 | gnorm 0.882 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 66470
2022-03-05 07:51:03 | INFO | fairseq.trainer | begin training epoch 259
2022-03-05 07:51:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:52:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 07:52:40 | INFO | train_inner | epoch 259:     38 / 97 loss=3.156, nll_loss=0.938, ppl=1.92, wps=24707.6, ups=0.38, wpb=65495, bsz=127.9, num_updates=24900, lr=0.000200401, gnorm=0.88, loss_scale=16, train_wall=235, gb_free=21, wall=66567
2022-03-05 07:53:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:55:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:55:15 | INFO | valid | epoch 259 | valid on 'valid' subset | loss 12.955 | nll_loss 11.987 | ppl 4060.08 | wps 45475 | wpb 510.9 | bsz 1 | num_updates 24958 | best_loss 8.737
2022-03-05 07:55:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 259 @ 24958 updates
2022-03-05 07:55:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:55:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:55:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 259 @ 24958 updates, score 12.955) (writing took 2.6370426239445806 seconds)
2022-03-05 07:55:18 | INFO | fairseq_cli.train | end of epoch 259 (average epoch stats below)
2022-03-05 07:55:18 | INFO | train | epoch 259 | loss 3.154 | nll_loss 0.937 | ppl 1.91 | wps 24379 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 24958 | lr 0.000200168 | gnorm 0.895 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 66725
2022-03-05 07:55:18 | INFO | fairseq.trainer | begin training epoch 260
2022-03-05 07:55:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:57:06 | INFO | train_inner | epoch 260:     42 / 97 loss=3.155, nll_loss=0.938, ppl=1.92, wps=24665.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=25000, lr=0.0002, gnorm=0.896, loss_scale=8, train_wall=235, gb_free=21, wall=66832
2022-03-05 07:59:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:59:31 | INFO | valid | epoch 260 | valid on 'valid' subset | loss 12.892 | nll_loss 11.914 | ppl 3860.24 | wps 44892.6 | wpb 510.9 | bsz 1 | num_updates 25055 | best_loss 8.737
2022-03-05 07:59:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 260 @ 25055 updates
2022-03-05 07:59:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:59:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 07:59:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 260 @ 25055 updates, score 12.892) (writing took 2.365657575428486 seconds)
2022-03-05 07:59:33 | INFO | fairseq_cli.train | end of epoch 260 (average epoch stats below)
2022-03-05 07:59:33 | INFO | train | epoch 260 | loss 3.153 | nll_loss 0.936 | ppl 1.91 | wps 24911.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25055 | lr 0.00019978 | gnorm 0.879 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 66980
2022-03-05 07:59:33 | INFO | fairseq.trainer | begin training epoch 261
2022-03-05 07:59:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:01:28 | INFO | train_inner | epoch 261:     45 / 97 loss=3.151, nll_loss=0.934, ppl=1.91, wps=24945.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25100, lr=0.000199601, gnorm=0.883, loss_scale=16, train_wall=233, gb_free=21, wall=67095
2022-03-05 08:03:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:03:46 | INFO | valid | epoch 261 | valid on 'valid' subset | loss 12.936 | nll_loss 11.966 | ppl 4001.6 | wps 45490.8 | wpb 510.9 | bsz 1 | num_updates 25152 | best_loss 8.737
2022-03-05 08:03:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 261 @ 25152 updates
2022-03-05 08:03:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:03:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:03:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 261 @ 25152 updates, score 12.936) (writing took 2.53817349858582 seconds)
2022-03-05 08:03:48 | INFO | fairseq_cli.train | end of epoch 261 (average epoch stats below)
2022-03-05 08:03:48 | INFO | train | epoch 261 | loss 3.151 | nll_loss 0.933 | ppl 1.91 | wps 24906.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25152 | lr 0.000199395 | gnorm 0.889 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 67235
2022-03-05 08:03:48 | INFO | fairseq.trainer | begin training epoch 262
2022-03-05 08:03:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:04:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:04:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:05:56 | INFO | train_inner | epoch 262:     50 / 97 loss=3.149, nll_loss=0.932, ppl=1.91, wps=24444.3, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=25200, lr=0.000199205, gnorm=0.89, loss_scale=8, train_wall=238, gb_free=21, wall=67363
2022-03-05 08:07:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:08:01 | INFO | valid | epoch 262 | valid on 'valid' subset | loss 12.947 | nll_loss 11.974 | ppl 4021.99 | wps 45403.7 | wpb 510.9 | bsz 1 | num_updates 25247 | best_loss 8.737
2022-03-05 08:08:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 262 @ 25247 updates
2022-03-05 08:08:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:08:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:08:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 262 @ 25247 updates, score 12.947) (writing took 2.3731562439352274 seconds)
2022-03-05 08:08:03 | INFO | fairseq_cli.train | end of epoch 262 (average epoch stats below)
2022-03-05 08:08:03 | INFO | train | epoch 262 | loss 3.147 | nll_loss 0.93 | ppl 1.9 | wps 24386.8 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 25247 | lr 0.000199019 | gnorm 0.896 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 67490
2022-03-05 08:08:03 | INFO | fairseq.trainer | begin training epoch 263
2022-03-05 08:08:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:10:19 | INFO | train_inner | epoch 263:     53 / 97 loss=3.148, nll_loss=0.93, ppl=1.91, wps=24914.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25300, lr=0.000198811, gnorm=0.903, loss_scale=8, train_wall=233, gb_free=21, wall=67626
2022-03-05 08:11:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:12:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:12:16 | INFO | valid | epoch 263 | valid on 'valid' subset | loss 12.924 | nll_loss 11.966 | ppl 3999.27 | wps 45272.4 | wpb 510.9 | bsz 1 | num_updates 25343 | best_loss 8.737
2022-03-05 08:12:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 263 @ 25343 updates
2022-03-05 08:12:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:12:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:12:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 263 @ 25343 updates, score 12.924) (writing took 2.3964852252975106 seconds)
2022-03-05 08:12:18 | INFO | fairseq_cli.train | end of epoch 263 (average epoch stats below)
2022-03-05 08:12:18 | INFO | train | epoch 263 | loss 3.147 | nll_loss 0.929 | ppl 1.9 | wps 24636.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 25343 | lr 0.000198642 | gnorm 0.898 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 67745
2022-03-05 08:12:18 | INFO | fairseq.trainer | begin training epoch 264
2022-03-05 08:12:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:14:44 | INFO | train_inner | epoch 264:     57 / 97 loss=3.144, nll_loss=0.927, ppl=1.9, wps=24674.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=25400, lr=0.000198419, gnorm=0.883, loss_scale=8, train_wall=236, gb_free=21, wall=67891
2022-03-05 08:16:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:16:31 | INFO | valid | epoch 264 | valid on 'valid' subset | loss 12.933 | nll_loss 11.973 | ppl 4019.32 | wps 45363.9 | wpb 510.9 | bsz 1 | num_updates 25440 | best_loss 8.737
2022-03-05 08:16:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 264 @ 25440 updates
2022-03-05 08:16:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:16:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:16:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 264 @ 25440 updates, score 12.933) (writing took 2.3752506263554096 seconds)
2022-03-05 08:16:34 | INFO | fairseq_cli.train | end of epoch 264 (average epoch stats below)
2022-03-05 08:16:34 | INFO | train | epoch 264 | loss 3.145 | nll_loss 0.928 | ppl 1.9 | wps 24891.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25440 | lr 0.000198263 | gnorm 0.885 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 68001
2022-03-05 08:16:34 | INFO | fairseq.trainer | begin training epoch 265
2022-03-05 08:16:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:17:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:19:10 | INFO | train_inner | epoch 265:     61 / 97 loss=3.145, nll_loss=0.927, ppl=1.9, wps=24674.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25500, lr=0.00019803, gnorm=0.899, loss_scale=8, train_wall=236, gb_free=21, wall=68157
2022-03-05 08:20:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:20:46 | INFO | valid | epoch 265 | valid on 'valid' subset | loss 12.937 | nll_loss 11.972 | ppl 4016.42 | wps 45513.7 | wpb 510.9 | bsz 1 | num_updates 25536 | best_loss 8.737
2022-03-05 08:20:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 265 @ 25536 updates
2022-03-05 08:20:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:20:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:20:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 265 @ 25536 updates, score 12.937) (writing took 2.4815513249486685 seconds)
2022-03-05 08:20:49 | INFO | fairseq_cli.train | end of epoch 265 (average epoch stats below)
2022-03-05 08:20:49 | INFO | train | epoch 265 | loss 3.143 | nll_loss 0.925 | ppl 1.9 | wps 24629.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 25536 | lr 0.00019789 | gnorm 0.902 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 68256
2022-03-05 08:20:49 | INFO | fairseq.trainer | begin training epoch 266
2022-03-05 08:20:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:23:33 | INFO | train_inner | epoch 266:     64 / 97 loss=3.143, nll_loss=0.926, ppl=1.9, wps=24911.1, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=25600, lr=0.000197642, gnorm=0.889, loss_scale=16, train_wall=233, gb_free=21, wall=68420
2022-03-05 08:24:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:25:02 | INFO | valid | epoch 266 | valid on 'valid' subset | loss 12.927 | nll_loss 11.961 | ppl 3985.62 | wps 45297 | wpb 510.9 | bsz 1 | num_updates 25633 | best_loss 8.737
2022-03-05 08:25:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 266 @ 25633 updates
2022-03-05 08:25:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:25:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:25:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 266 @ 25633 updates, score 12.927) (writing took 2.5127842742949724 seconds)
2022-03-05 08:25:04 | INFO | fairseq_cli.train | end of epoch 266 (average epoch stats below)
2022-03-05 08:25:04 | INFO | train | epoch 266 | loss 3.142 | nll_loss 0.924 | ppl 1.9 | wps 24877.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25633 | lr 0.000197515 | gnorm 0.886 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 68511
2022-03-05 08:25:04 | INFO | fairseq.trainer | begin training epoch 267
2022-03-05 08:25:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:27:56 | INFO | train_inner | epoch 267:     67 / 97 loss=3.139, nll_loss=0.921, ppl=1.89, wps=24897.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=25700, lr=0.000197257, gnorm=0.883, loss_scale=16, train_wall=233, gb_free=21, wall=68683
2022-03-05 08:27:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:29:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:29:17 | INFO | valid | epoch 267 | valid on 'valid' subset | loss 12.933 | nll_loss 11.964 | ppl 3996 | wps 45455.2 | wpb 510.9 | bsz 1 | num_updates 25729 | best_loss 8.737
2022-03-05 08:29:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 267 @ 25729 updates
2022-03-05 08:29:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:29:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:29:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 267 @ 25729 updates, score 12.933) (writing took 2.43444686755538 seconds)
2022-03-05 08:29:20 | INFO | fairseq_cli.train | end of epoch 267 (average epoch stats below)
2022-03-05 08:29:20 | INFO | train | epoch 267 | loss 3.138 | nll_loss 0.92 | ppl 1.89 | wps 24637.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 25729 | lr 0.000197146 | gnorm 0.892 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 68766
2022-03-05 08:29:20 | INFO | fairseq.trainer | begin training epoch 268
2022-03-05 08:29:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:32:21 | INFO | train_inner | epoch 268:     71 / 97 loss=3.139, nll_loss=0.921, ppl=1.89, wps=24676.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=25800, lr=0.000196875, gnorm=0.892, loss_scale=8, train_wall=236, gb_free=21, wall=68948
2022-03-05 08:33:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:33:32 | INFO | valid | epoch 268 | valid on 'valid' subset | loss 12.953 | nll_loss 11.986 | ppl 4055.34 | wps 45552.2 | wpb 510.9 | bsz 1 | num_updates 25826 | best_loss 8.737
2022-03-05 08:33:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 268 @ 25826 updates
2022-03-05 08:33:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:33:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:33:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 268 @ 25826 updates, score 12.953) (writing took 2.4255433520302176 seconds)
2022-03-05 08:33:35 | INFO | fairseq_cli.train | end of epoch 268 (average epoch stats below)
2022-03-05 08:33:35 | INFO | train | epoch 268 | loss 3.137 | nll_loss 0.919 | ppl 1.89 | wps 24898.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 25826 | lr 0.000196776 | gnorm 0.879 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 69022
2022-03-05 08:33:35 | INFO | fairseq.trainer | begin training epoch 269
2022-03-05 08:33:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:36:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:36:46 | INFO | train_inner | epoch 269:     75 / 97 loss=3.136, nll_loss=0.918, ppl=1.89, wps=24687.4, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=25900, lr=0.000196494, gnorm=0.879, loss_scale=8, train_wall=235, gb_free=21, wall=69213
2022-03-05 08:37:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:37:47 | INFO | valid | epoch 269 | valid on 'valid' subset | loss 12.934 | nll_loss 11.966 | ppl 4001.69 | wps 45300.2 | wpb 510.9 | bsz 1 | num_updates 25922 | best_loss 8.737
2022-03-05 08:37:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 269 @ 25922 updates
2022-03-05 08:37:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:37:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:37:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 269 @ 25922 updates, score 12.934) (writing took 2.470860557630658 seconds)
2022-03-05 08:37:50 | INFO | fairseq_cli.train | end of epoch 269 (average epoch stats below)
2022-03-05 08:37:50 | INFO | train | epoch 269 | loss 3.135 | nll_loss 0.917 | ppl 1.89 | wps 24638.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 25922 | lr 0.000196411 | gnorm 0.88 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 69277
2022-03-05 08:37:50 | INFO | fairseq.trainer | begin training epoch 270
2022-03-05 08:37:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:41:09 | INFO | train_inner | epoch 270:     78 / 97 loss=3.135, nll_loss=0.917, ppl=1.89, wps=24915.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26000, lr=0.000196116, gnorm=0.889, loss_scale=8, train_wall=233, gb_free=21, wall=69476
2022-03-05 08:41:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:42:02 | INFO | valid | epoch 270 | valid on 'valid' subset | loss 12.889 | nll_loss 11.918 | ppl 3868.76 | wps 45392.5 | wpb 510.9 | bsz 1 | num_updates 26019 | best_loss 8.737
2022-03-05 08:42:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 270 @ 26019 updates
2022-03-05 08:42:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:42:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:42:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 270 @ 26019 updates, score 12.889) (writing took 2.4383154567331076 seconds)
2022-03-05 08:42:05 | INFO | fairseq_cli.train | end of epoch 270 (average epoch stats below)
2022-03-05 08:42:05 | INFO | train | epoch 270 | loss 3.135 | nll_loss 0.917 | ppl 1.89 | wps 24900.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26019 | lr 0.000196045 | gnorm 0.885 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 69532
2022-03-05 08:42:05 | INFO | fairseq.trainer | begin training epoch 271
2022-03-05 08:42:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:43:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:45:35 | INFO | train_inner | epoch 271:     82 / 97 loss=3.134, nll_loss=0.915, ppl=1.89, wps=24678.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26100, lr=0.00019574, gnorm=0.895, loss_scale=8, train_wall=235, gb_free=21, wall=69742
2022-03-05 08:46:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:46:18 | INFO | valid | epoch 271 | valid on 'valid' subset | loss 12.945 | nll_loss 11.989 | ppl 4064.65 | wps 45510.1 | wpb 510.9 | bsz 1 | num_updates 26115 | best_loss 8.737
2022-03-05 08:46:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 271 @ 26115 updates
2022-03-05 08:46:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:46:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:46:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 271 @ 26115 updates, score 12.945) (writing took 2.4140499839559197 seconds)
2022-03-05 08:46:20 | INFO | fairseq_cli.train | end of epoch 271 (average epoch stats below)
2022-03-05 08:46:20 | INFO | train | epoch 271 | loss 3.133 | nll_loss 0.915 | ppl 1.89 | wps 24641.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26115 | lr 0.000195684 | gnorm 0.899 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 69787
2022-03-05 08:46:20 | INFO | fairseq.trainer | begin training epoch 272
2022-03-05 08:46:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:49:57 | INFO | train_inner | epoch 272:     85 / 97 loss=3.133, nll_loss=0.915, ppl=1.89, wps=24919.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26200, lr=0.000195366, gnorm=0.89, loss_scale=16, train_wall=233, gb_free=21, wall=70004
2022-03-05 08:50:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:50:33 | INFO | valid | epoch 272 | valid on 'valid' subset | loss 13.021 | nll_loss 12.067 | ppl 4291.95 | wps 46001 | wpb 510.9 | bsz 1 | num_updates 26212 | best_loss 8.737
2022-03-05 08:50:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 272 @ 26212 updates
2022-03-05 08:50:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:50:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:50:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 272 @ 26212 updates, score 13.021) (writing took 2.458771301433444 seconds)
2022-03-05 08:50:35 | INFO | fairseq_cli.train | end of epoch 272 (average epoch stats below)
2022-03-05 08:50:35 | INFO | train | epoch 272 | loss 3.132 | nll_loss 0.914 | ppl 1.88 | wps 24895.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26212 | lr 0.000195321 | gnorm 0.889 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 70042
2022-03-05 08:50:35 | INFO | fairseq.trainer | begin training epoch 273
2022-03-05 08:50:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:54:20 | INFO | train_inner | epoch 273:     88 / 97 loss=3.131, nll_loss=0.913, ppl=1.88, wps=24922.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26300, lr=0.000194994, gnorm=0.891, loss_scale=16, train_wall=233, gb_free=21, wall=70267
2022-03-05 08:54:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 08:54:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:54:48 | INFO | valid | epoch 273 | valid on 'valid' subset | loss 12.949 | nll_loss 11.983 | ppl 4047.77 | wps 45292.2 | wpb 510.9 | bsz 1 | num_updates 26308 | best_loss 8.737
2022-03-05 08:54:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 273 @ 26308 updates
2022-03-05 08:54:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:54:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:54:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 273 @ 26308 updates, score 12.949) (writing took 2.3909960547462106 seconds)
2022-03-05 08:54:50 | INFO | fairseq_cli.train | end of epoch 273 (average epoch stats below)
2022-03-05 08:54:50 | INFO | train | epoch 273 | loss 3.13 | nll_loss 0.912 | ppl 1.88 | wps 24645.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26308 | lr 0.000194965 | gnorm 0.889 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 70297
2022-03-05 08:54:50 | INFO | fairseq.trainer | begin training epoch 274
2022-03-05 08:54:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:58:46 | INFO | train_inner | epoch 274:     92 / 97 loss=3.129, nll_loss=0.911, ppl=1.88, wps=24687.8, ups=0.38, wpb=65495, bsz=127.9, num_updates=26400, lr=0.000194625, gnorm=0.883, loss_scale=16, train_wall=235, gb_free=21, wall=70532
2022-03-05 08:58:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:59:03 | INFO | valid | epoch 274 | valid on 'valid' subset | loss 12.883 | nll_loss 11.912 | ppl 3852.9 | wps 45483.4 | wpb 510.9 | bsz 1 | num_updates 26405 | best_loss 8.737
2022-03-05 08:59:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 274 @ 26405 updates
2022-03-05 08:59:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:59:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 08:59:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 274 @ 26405 updates, score 12.883) (writing took 2.4063669675961137 seconds)
2022-03-05 08:59:05 | INFO | fairseq_cli.train | end of epoch 274 (average epoch stats below)
2022-03-05 08:59:05 | INFO | train | epoch 274 | loss 3.128 | nll_loss 0.91 | ppl 1.88 | wps 24907.9 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26405 | lr 0.000194606 | gnorm 0.885 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 70552
2022-03-05 08:59:05 | INFO | fairseq.trainer | begin training epoch 275
2022-03-05 08:59:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:00:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:03:11 | INFO | train_inner | epoch 275:     96 / 97 loss=3.127, nll_loss=0.909, ppl=1.88, wps=24681.7, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=26500, lr=0.000194257, gnorm=0.882, loss_scale=16, train_wall=235, gb_free=21, wall=70798
2022-03-05 09:03:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:03:18 | INFO | valid | epoch 275 | valid on 'valid' subset | loss 12.932 | nll_loss 11.965 | ppl 3997.91 | wps 45474.5 | wpb 510.9 | bsz 1 | num_updates 26501 | best_loss 8.737
2022-03-05 09:03:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 275 @ 26501 updates
2022-03-05 09:03:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:03:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:03:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 275 @ 26501 updates, score 12.932) (writing took 2.3600106704980135 seconds)
2022-03-05 09:03:20 | INFO | fairseq_cli.train | end of epoch 275 (average epoch stats below)
2022-03-05 09:03:20 | INFO | train | epoch 275 | loss 3.125 | nll_loss 0.907 | ppl 1.88 | wps 24646.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26501 | lr 0.000194254 | gnorm 0.881 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 70807
2022-03-05 09:03:21 | INFO | fairseq.trainer | begin training epoch 276
2022-03-05 09:03:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:05:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:07:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:07:33 | INFO | valid | epoch 276 | valid on 'valid' subset | loss 12.995 | nll_loss 12.036 | ppl 4198.68 | wps 45726.4 | wpb 510.9 | bsz 1 | num_updates 26597 | best_loss 8.737
2022-03-05 09:07:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 276 @ 26597 updates
2022-03-05 09:07:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:07:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:07:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 276 @ 26597 updates, score 12.995) (writing took 2.403062072582543 seconds)
2022-03-05 09:07:35 | INFO | fairseq_cli.train | end of epoch 276 (average epoch stats below)
2022-03-05 09:07:35 | INFO | train | epoch 276 | loss 3.123 | nll_loss 0.905 | ppl 1.87 | wps 24665.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26597 | lr 0.000193903 | gnorm 0.878 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 71062
2022-03-05 09:07:35 | INFO | fairseq.trainer | begin training epoch 277
2022-03-05 09:07:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:07:43 | INFO | train_inner | epoch 277:      3 / 97 loss=3.123, nll_loss=0.904, ppl=1.87, wps=24037.3, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=26600, lr=0.000193892, gnorm=0.877, loss_scale=16, train_wall=235, gb_free=21, wall=71070
2022-03-05 09:08:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:11:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:11:48 | INFO | valid | epoch 277 | valid on 'valid' subset | loss 13.004 | nll_loss 12.045 | ppl 4225.27 | wps 45459.3 | wpb 510.9 | bsz 1 | num_updates 26693 | best_loss 8.737
2022-03-05 09:11:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 277 @ 26693 updates
2022-03-05 09:11:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:11:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:11:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 277 @ 26693 updates, score 13.004) (writing took 2.3922750828787684 seconds)
2022-03-05 09:11:50 | INFO | fairseq_cli.train | end of epoch 277 (average epoch stats below)
2022-03-05 09:11:50 | INFO | train | epoch 277 | loss 3.123 | nll_loss 0.905 | ppl 1.87 | wps 24672.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26693 | lr 0.000193554 | gnorm 0.887 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 71317
2022-03-05 09:11:50 | INFO | fairseq.trainer | begin training epoch 278
2022-03-05 09:11:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:12:08 | INFO | train_inner | epoch 278:      7 / 97 loss=3.122, nll_loss=0.904, ppl=1.87, wps=24711, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26700, lr=0.000193528, gnorm=0.887, loss_scale=8, train_wall=235, gb_free=21, wall=71335
2022-03-05 09:15:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:16:03 | INFO | valid | epoch 278 | valid on 'valid' subset | loss 12.952 | nll_loss 11.991 | ppl 4070.44 | wps 45421.2 | wpb 510.9 | bsz 1 | num_updates 26790 | best_loss 8.737
2022-03-05 09:16:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 278 @ 26790 updates
2022-03-05 09:16:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:16:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:16:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 278 @ 26790 updates, score 12.952) (writing took 2.3688330873847008 seconds)
2022-03-05 09:16:05 | INFO | fairseq_cli.train | end of epoch 278 (average epoch stats below)
2022-03-05 09:16:05 | INFO | train | epoch 278 | loss 3.121 | nll_loss 0.903 | ppl 1.87 | wps 24911.4 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26790 | lr 0.000193203 | gnorm 0.881 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 71572
2022-03-05 09:16:05 | INFO | fairseq.trainer | begin training epoch 279
2022-03-05 09:16:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:16:31 | INFO | train_inner | epoch 279:     10 / 97 loss=3.121, nll_loss=0.902, ppl=1.87, wps=24934.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26800, lr=0.000193167, gnorm=0.88, loss_scale=16, train_wall=233, gb_free=21, wall=71598
2022-03-05 09:18:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:20:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:20:18 | INFO | valid | epoch 279 | valid on 'valid' subset | loss 12.981 | nll_loss 12.022 | ppl 4158.12 | wps 45324.6 | wpb 510.9 | bsz 1 | num_updates 26886 | best_loss 8.737
2022-03-05 09:20:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 279 @ 26886 updates
2022-03-05 09:20:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:20:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:20:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 279 @ 26886 updates, score 12.981) (writing took 2.3895847285166383 seconds)
2022-03-05 09:20:20 | INFO | fairseq_cli.train | end of epoch 279 (average epoch stats below)
2022-03-05 09:20:20 | INFO | train | epoch 279 | loss 3.119 | nll_loss 0.901 | ppl 1.87 | wps 24655.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 26886 | lr 0.000192858 | gnorm 0.886 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 71827
2022-03-05 09:20:20 | INFO | fairseq.trainer | begin training epoch 280
2022-03-05 09:20:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:20:56 | INFO | train_inner | epoch 280:     14 / 97 loss=3.118, nll_loss=0.899, ppl=1.86, wps=24691.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=26900, lr=0.000192807, gnorm=0.884, loss_scale=8, train_wall=235, gb_free=21, wall=71863
2022-03-05 09:24:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:24:33 | INFO | valid | epoch 280 | valid on 'valid' subset | loss 12.948 | nll_loss 11.99 | ppl 4068.45 | wps 45337 | wpb 510.9 | bsz 1 | num_updates 26983 | best_loss 8.737
2022-03-05 09:24:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 280 @ 26983 updates
2022-03-05 09:24:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:24:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:24:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 280 @ 26983 updates, score 12.948) (writing took 2.3585982592776418 seconds)
2022-03-05 09:24:35 | INFO | fairseq_cli.train | end of epoch 280 (average epoch stats below)
2022-03-05 09:24:35 | INFO | train | epoch 280 | loss 3.118 | nll_loss 0.9 | ppl 1.87 | wps 24886.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 26983 | lr 0.000192511 | gnorm 0.885 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 72082
2022-03-05 09:24:36 | INFO | fairseq.trainer | begin training epoch 281
2022-03-05 09:24:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:25:19 | INFO | train_inner | epoch 281:     17 / 97 loss=3.118, nll_loss=0.9, ppl=1.87, wps=24913, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27000, lr=0.00019245, gnorm=0.883, loss_scale=16, train_wall=233, gb_free=21, wall=72126
2022-03-05 09:28:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:28:48 | INFO | valid | epoch 281 | valid on 'valid' subset | loss 12.986 | nll_loss 12.025 | ppl 4166.7 | wps 45308.5 | wpb 510.9 | bsz 1 | num_updates 27080 | best_loss 8.737
2022-03-05 09:28:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 281 @ 27080 updates
2022-03-05 09:28:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:28:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:28:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 281 @ 27080 updates, score 12.986) (writing took 2.389963284134865 seconds)
2022-03-05 09:28:50 | INFO | fairseq_cli.train | end of epoch 281 (average epoch stats below)
2022-03-05 09:28:50 | INFO | train | epoch 281 | loss 3.116 | nll_loss 0.898 | ppl 1.86 | wps 24933.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27080 | lr 0.000192166 | gnorm 0.877 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 72337
2022-03-05 09:28:50 | INFO | fairseq.trainer | begin training epoch 282
2022-03-05 09:28:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:29:41 | INFO | train_inner | epoch 282:     20 / 97 loss=3.116, nll_loss=0.897, ppl=1.86, wps=24957.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27100, lr=0.000192095, gnorm=0.875, loss_scale=16, train_wall=233, gb_free=21, wall=72388
2022-03-05 09:29:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:32:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:33:03 | INFO | valid | epoch 282 | valid on 'valid' subset | loss 12.9 | nll_loss 11.932 | ppl 3908.59 | wps 45462 | wpb 510.9 | bsz 1 | num_updates 27176 | best_loss 8.737
2022-03-05 09:33:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 282 @ 27176 updates
2022-03-05 09:33:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:33:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:33:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 282 @ 27176 updates, score 12.9) (writing took 2.4819277804344893 seconds)
2022-03-05 09:33:05 | INFO | fairseq_cli.train | end of epoch 282 (average epoch stats below)
2022-03-05 09:33:05 | INFO | train | epoch 282 | loss 3.115 | nll_loss 0.897 | ppl 1.86 | wps 24664.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27176 | lr 0.000191826 | gnorm 0.877 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 72592
2022-03-05 09:33:05 | INFO | fairseq.trainer | begin training epoch 283
2022-03-05 09:33:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:34:07 | INFO | train_inner | epoch 283:     24 / 97 loss=3.115, nll_loss=0.897, ppl=1.86, wps=24693.7, ups=0.38, wpb=65495, bsz=127.9, num_updates=27200, lr=0.000191741, gnorm=0.88, loss_scale=16, train_wall=235, gb_free=21, wall=72654
2022-03-05 09:34:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:37:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:37:18 | INFO | valid | epoch 283 | valid on 'valid' subset | loss 12.961 | nll_loss 12.007 | ppl 4115.24 | wps 45576.4 | wpb 510.9 | bsz 1 | num_updates 27272 | best_loss 8.737
2022-03-05 09:37:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 283 @ 27272 updates
2022-03-05 09:37:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:37:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:37:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 283 @ 27272 updates, score 12.961) (writing took 2.501089827157557 seconds)
2022-03-05 09:37:20 | INFO | fairseq_cli.train | end of epoch 283 (average epoch stats below)
2022-03-05 09:37:20 | INFO | train | epoch 283 | loss 3.113 | nll_loss 0.895 | ppl 1.86 | wps 24653.2 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27272 | lr 0.000191488 | gnorm 0.887 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 72847
2022-03-05 09:37:20 | INFO | fairseq.trainer | begin training epoch 284
2022-03-05 09:37:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:38:32 | INFO | train_inner | epoch 284:     28 / 97 loss=3.112, nll_loss=0.893, ppl=1.86, wps=24703.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27300, lr=0.00019139, gnorm=0.883, loss_scale=8, train_wall=235, gb_free=21, wall=72919
2022-03-05 09:41:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:41:33 | INFO | valid | epoch 284 | valid on 'valid' subset | loss 12.978 | nll_loss 12.02 | ppl 4153.51 | wps 45522.6 | wpb 510.9 | bsz 1 | num_updates 27369 | best_loss 8.737
2022-03-05 09:41:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 284 @ 27369 updates
2022-03-05 09:41:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:41:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:41:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 284 @ 27369 updates, score 12.978) (writing took 2.461406311020255 seconds)
2022-03-05 09:41:35 | INFO | fairseq_cli.train | end of epoch 284 (average epoch stats below)
2022-03-05 09:41:35 | INFO | train | epoch 284 | loss 3.112 | nll_loss 0.894 | ppl 1.86 | wps 24935.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27369 | lr 0.000191148 | gnorm 0.871 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 73102
2022-03-05 09:41:35 | INFO | fairseq.trainer | begin training epoch 285
2022-03-05 09:41:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:42:54 | INFO | train_inner | epoch 285:     31 / 97 loss=3.111, nll_loss=0.892, ppl=1.86, wps=24959.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27400, lr=0.00019104, gnorm=0.874, loss_scale=16, train_wall=233, gb_free=21, wall=73181
2022-03-05 09:45:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:45:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:45:47 | INFO | valid | epoch 285 | valid on 'valid' subset | loss 12.985 | nll_loss 12.029 | ppl 4179.92 | wps 45459.9 | wpb 510.9 | bsz 1 | num_updates 27465 | best_loss 8.737
2022-03-05 09:45:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 285 @ 27465 updates
2022-03-05 09:45:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:45:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:45:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 285 @ 27465 updates, score 12.985) (writing took 2.408370236866176 seconds)
2022-03-05 09:45:50 | INFO | fairseq_cli.train | end of epoch 285 (average epoch stats below)
2022-03-05 09:45:50 | INFO | train | epoch 285 | loss 3.11 | nll_loss 0.892 | ppl 1.86 | wps 24682.8 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27465 | lr 0.000190814 | gnorm 0.869 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 73357
2022-03-05 09:45:50 | INFO | fairseq.trainer | begin training epoch 286
2022-03-05 09:45:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:47:19 | INFO | train_inner | epoch 286:     35 / 97 loss=3.109, nll_loss=0.89, ppl=1.85, wps=24711.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27500, lr=0.000190693, gnorm=0.866, loss_scale=16, train_wall=235, gb_free=21, wall=73446
2022-03-05 09:49:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:50:02 | INFO | valid | epoch 286 | valid on 'valid' subset | loss 12.975 | nll_loss 12.018 | ppl 4147.62 | wps 45576.5 | wpb 510.9 | bsz 1 | num_updates 27562 | best_loss 8.737
2022-03-05 09:50:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 286 @ 27562 updates
2022-03-05 09:50:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:50:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:50:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 286 @ 27562 updates, score 12.975) (writing took 2.4988738913089037 seconds)
2022-03-05 09:50:05 | INFO | fairseq_cli.train | end of epoch 286 (average epoch stats below)
2022-03-05 09:50:05 | INFO | train | epoch 286 | loss 3.109 | nll_loss 0.891 | ppl 1.85 | wps 24926 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27562 | lr 0.000190478 | gnorm 0.874 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 73612
2022-03-05 09:50:05 | INFO | fairseq.trainer | begin training epoch 287
2022-03-05 09:50:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:51:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:51:44 | INFO | train_inner | epoch 287:     39 / 97 loss=3.108, nll_loss=0.89, ppl=1.85, wps=24705.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=27600, lr=0.000190347, gnorm=0.874, loss_scale=16, train_wall=235, gb_free=21, wall=73711
2022-03-05 09:54:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:54:17 | INFO | valid | epoch 287 | valid on 'valid' subset | loss 12.922 | nll_loss 11.96 | ppl 3984.46 | wps 45088.3 | wpb 510.9 | bsz 1 | num_updates 27658 | best_loss 8.737
2022-03-05 09:54:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 287 @ 27658 updates
2022-03-05 09:54:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:54:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:54:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 287 @ 27658 updates, score 12.922) (writing took 2.694908110424876 seconds)
2022-03-05 09:54:20 | INFO | fairseq_cli.train | end of epoch 287 (average epoch stats below)
2022-03-05 09:54:20 | INFO | train | epoch 287 | loss 3.106 | nll_loss 0.887 | ppl 1.85 | wps 24620.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 27658 | lr 0.000190147 | gnorm 0.871 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 73867
2022-03-05 09:54:20 | INFO | fairseq.trainer | begin training epoch 288
2022-03-05 09:54:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 09:56:07 | INFO | train_inner | epoch 288:     42 / 97 loss=3.106, nll_loss=0.888, ppl=1.85, wps=24909, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27700, lr=0.000190003, gnorm=0.88, loss_scale=16, train_wall=233, gb_free=21, wall=73974
2022-03-05 09:56:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 09:58:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 09:58:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 09:58:32 | INFO | valid | epoch 288 | valid on 'valid' subset | loss 13.016 | nll_loss 12.063 | ppl 4277.89 | wps 45228.1 | wpb 510.9 | bsz 1 | num_updates 27753 | best_loss 8.737
2022-03-05 09:58:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 288 @ 27753 updates
2022-03-05 09:58:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:58:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 09:58:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 288 @ 27753 updates, score 13.016) (writing took 2.4001640528440475 seconds)
2022-03-05 09:58:35 | INFO | fairseq_cli.train | end of epoch 288 (average epoch stats below)
2022-03-05 09:58:35 | INFO | train | epoch 288 | loss 3.105 | nll_loss 0.887 | ppl 1.85 | wps 24407.9 | ups 0.37 | wpb 65490.6 | bsz 127.9 | num_updates 27753 | lr 0.000189821 | gnorm 0.887 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 74122
2022-03-05 09:58:35 | INFO | fairseq.trainer | begin training epoch 289
2022-03-05 09:58:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:00:35 | INFO | train_inner | epoch 289:     47 / 97 loss=3.103, nll_loss=0.884, ppl=1.85, wps=24458.1, ups=0.37, wpb=65495, bsz=127.9, num_updates=27800, lr=0.000189661, gnorm=0.883, loss_scale=8, train_wall=238, gb_free=21, wall=74242
2022-03-05 10:02:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:02:47 | INFO | valid | epoch 289 | valid on 'valid' subset | loss 12.937 | nll_loss 11.971 | ppl 4014.99 | wps 45414.9 | wpb 510.9 | bsz 1 | num_updates 27850 | best_loss 8.737
2022-03-05 10:02:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 289 @ 27850 updates
2022-03-05 10:02:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:02:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:02:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 289 @ 27850 updates, score 12.937) (writing took 2.5811478504911065 seconds)
2022-03-05 10:02:50 | INFO | fairseq_cli.train | end of epoch 289 (average epoch stats below)
2022-03-05 10:02:50 | INFO | train | epoch 289 | loss 3.104 | nll_loss 0.886 | ppl 1.85 | wps 24899.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27850 | lr 0.00018949 | gnorm 0.881 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 74377
2022-03-05 10:02:50 | INFO | fairseq.trainer | begin training epoch 290
2022-03-05 10:02:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:04:58 | INFO | train_inner | epoch 290:     50 / 97 loss=3.103, nll_loss=0.885, ppl=1.85, wps=24927.4, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=27900, lr=0.000189321, gnorm=0.889, loss_scale=16, train_wall=233, gb_free=21, wall=74505
2022-03-05 10:06:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:07:02 | INFO | valid | epoch 290 | valid on 'valid' subset | loss 12.998 | nll_loss 12.037 | ppl 4203.05 | wps 45455.7 | wpb 510.9 | bsz 1 | num_updates 27947 | best_loss 8.737
2022-03-05 10:07:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 290 @ 27947 updates
2022-03-05 10:07:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:07:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:07:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 290 @ 27947 updates, score 12.998) (writing took 2.5435571782290936 seconds)
2022-03-05 10:07:05 | INFO | fairseq_cli.train | end of epoch 290 (average epoch stats below)
2022-03-05 10:07:05 | INFO | train | epoch 290 | loss 3.102 | nll_loss 0.883 | ppl 1.84 | wps 24911.2 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 27947 | lr 0.000189161 | gnorm 0.878 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 74632
2022-03-05 10:07:05 | INFO | fairseq.trainer | begin training epoch 291
2022-03-05 10:07:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:07:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:09:23 | INFO | train_inner | epoch 291:     54 / 97 loss=3.102, nll_loss=0.883, ppl=1.84, wps=24690, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28000, lr=0.000188982, gnorm=0.873, loss_scale=8, train_wall=235, gb_free=21, wall=74770
2022-03-05 10:11:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:11:18 | INFO | valid | epoch 291 | valid on 'valid' subset | loss 12.889 | nll_loss 11.925 | ppl 3889.44 | wps 45455.7 | wpb 510.9 | bsz 1 | num_updates 28043 | best_loss 8.737
2022-03-05 10:11:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 291 @ 28043 updates
2022-03-05 10:11:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:11:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:11:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 291 @ 28043 updates, score 12.889) (writing took 2.588391410186887 seconds)
2022-03-05 10:11:20 | INFO | fairseq_cli.train | end of epoch 291 (average epoch stats below)
2022-03-05 10:11:20 | INFO | train | epoch 291 | loss 3.101 | nll_loss 0.883 | ppl 1.84 | wps 24641.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28043 | lr 0.000188837 | gnorm 0.878 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 74887
2022-03-05 10:11:20 | INFO | fairseq.trainer | begin training epoch 292
2022-03-05 10:11:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:13:46 | INFO | train_inner | epoch 292:     57 / 97 loss=3.1, nll_loss=0.882, ppl=1.84, wps=24931.5, ups=0.38, wpb=65495, bsz=127.9, num_updates=28100, lr=0.000188646, gnorm=0.873, loss_scale=16, train_wall=233, gb_free=21, wall=75033
2022-03-05 10:14:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:15:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:15:33 | INFO | valid | epoch 292 | valid on 'valid' subset | loss 12.996 | nll_loss 12.046 | ppl 4229.45 | wps 45349.4 | wpb 510.9 | bsz 1 | num_updates 28139 | best_loss 8.737
2022-03-05 10:15:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 292 @ 28139 updates
2022-03-05 10:15:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:15:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:15:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 292 @ 28139 updates, score 12.996) (writing took 2.5156502230092883 seconds)
2022-03-05 10:15:35 | INFO | fairseq_cli.train | end of epoch 292 (average epoch stats below)
2022-03-05 10:15:35 | INFO | train | epoch 292 | loss 3.099 | nll_loss 0.88 | ppl 1.84 | wps 24661.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28139 | lr 0.000188515 | gnorm 0.881 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 75142
2022-03-05 10:15:35 | INFO | fairseq.trainer | begin training epoch 293
2022-03-05 10:15:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:18:11 | INFO | train_inner | epoch 293:     61 / 97 loss=3.097, nll_loss=0.878, ppl=1.84, wps=24683.7, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28200, lr=0.000188311, gnorm=0.874, loss_scale=8, train_wall=235, gb_free=21, wall=75298
2022-03-05 10:19:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:19:48 | INFO | valid | epoch 293 | valid on 'valid' subset | loss 12.956 | nll_loss 11.995 | ppl 4081.63 | wps 45435.1 | wpb 510.9 | bsz 1 | num_updates 28236 | best_loss 8.737
2022-03-05 10:19:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 293 @ 28236 updates
2022-03-05 10:19:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:19:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:19:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 293 @ 28236 updates, score 12.956) (writing took 2.7431734930723906 seconds)
2022-03-05 10:19:50 | INFO | fairseq_cli.train | end of epoch 293 (average epoch stats below)
2022-03-05 10:19:50 | INFO | train | epoch 293 | loss 3.097 | nll_loss 0.878 | ppl 1.84 | wps 24879.7 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28236 | lr 0.000188191 | gnorm 0.855 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 75397
2022-03-05 10:19:50 | INFO | fairseq.trainer | begin training epoch 294
2022-03-05 10:19:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:20:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:22:36 | INFO | train_inner | epoch 294:     65 / 97 loss=3.097, nll_loss=0.878, ppl=1.84, wps=24677.9, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=28300, lr=0.000187978, gnorm=0.864, loss_scale=8, train_wall=235, gb_free=21, wall=75563
2022-03-05 10:23:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:24:03 | INFO | valid | epoch 294 | valid on 'valid' subset | loss 13.007 | nll_loss 12.057 | ppl 4260.24 | wps 45496.7 | wpb 510.9 | bsz 1 | num_updates 28332 | best_loss 8.737
2022-03-05 10:24:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 294 @ 28332 updates
2022-03-05 10:24:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:24:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:24:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 294 @ 28332 updates, score 13.007) (writing took 2.536009724251926 seconds)
2022-03-05 10:24:05 | INFO | fairseq_cli.train | end of epoch 294 (average epoch stats below)
2022-03-05 10:24:05 | INFO | train | epoch 294 | loss 3.096 | nll_loss 0.877 | ppl 1.84 | wps 24661.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28332 | lr 0.000187872 | gnorm 0.872 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 75652
2022-03-05 10:24:05 | INFO | fairseq.trainer | begin training epoch 295
2022-03-05 10:24:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:26:59 | INFO | train_inner | epoch 295:     68 / 97 loss=3.097, nll_loss=0.879, ppl=1.84, wps=24925.1, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28400, lr=0.000187647, gnorm=0.876, loss_scale=16, train_wall=233, gb_free=21, wall=75826
2022-03-05 10:28:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:28:18 | INFO | valid | epoch 295 | valid on 'valid' subset | loss 12.947 | nll_loss 11.987 | ppl 4057.89 | wps 45467.2 | wpb 510.9 | bsz 1 | num_updates 28429 | best_loss 8.737
2022-03-05 10:28:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 295 @ 28429 updates
2022-03-05 10:28:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:28:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:28:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 295 @ 28429 updates, score 12.947) (writing took 2.5657477248460054 seconds)
2022-03-05 10:28:20 | INFO | fairseq_cli.train | end of epoch 295 (average epoch stats below)
2022-03-05 10:28:20 | INFO | train | epoch 295 | loss 3.097 | nll_loss 0.879 | ppl 1.84 | wps 24899.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28429 | lr 0.000187551 | gnorm 0.887 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 75907
2022-03-05 10:28:20 | INFO | fairseq.trainer | begin training epoch 296
2022-03-05 10:28:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:31:22 | INFO | train_inner | epoch 296:     71 / 97 loss=3.095, nll_loss=0.877, ppl=1.84, wps=24931.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28500, lr=0.000187317, gnorm=0.873, loss_scale=16, train_wall=233, gb_free=21, wall=76089
2022-03-05 10:32:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:32:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:32:33 | INFO | valid | epoch 296 | valid on 'valid' subset | loss 12.899 | nll_loss 11.933 | ppl 3910.74 | wps 45514.7 | wpb 510.9 | bsz 1 | num_updates 28525 | best_loss 8.737
2022-03-05 10:32:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 296 @ 28525 updates
2022-03-05 10:32:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:32:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:32:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 296 @ 28525 updates, score 12.899) (writing took 2.4786419244483113 seconds)
2022-03-05 10:32:35 | INFO | fairseq_cli.train | end of epoch 296 (average epoch stats below)
2022-03-05 10:32:35 | INFO | train | epoch 296 | loss 3.093 | nll_loss 0.875 | ppl 1.83 | wps 24660.9 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28525 | lr 0.000187235 | gnorm 0.865 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 76162
2022-03-05 10:32:35 | INFO | fairseq.trainer | begin training epoch 297
2022-03-05 10:32:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:35:47 | INFO | train_inner | epoch 297:     75 / 97 loss=3.092, nll_loss=0.873, ppl=1.83, wps=24695.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28600, lr=0.000186989, gnorm=0.868, loss_scale=16, train_wall=235, gb_free=21, wall=76354
2022-03-05 10:36:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:36:48 | INFO | valid | epoch 297 | valid on 'valid' subset | loss 13.033 | nll_loss 12.086 | ppl 4348.44 | wps 45492.7 | wpb 510.9 | bsz 1 | num_updates 28622 | best_loss 8.737
2022-03-05 10:36:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 297 @ 28622 updates
2022-03-05 10:36:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:36:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:36:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 297 @ 28622 updates, score 13.033) (writing took 2.7045659963041544 seconds)
2022-03-05 10:36:51 | INFO | fairseq_cli.train | end of epoch 297 (average epoch stats below)
2022-03-05 10:36:51 | INFO | train | epoch 297 | loss 3.091 | nll_loss 0.873 | ppl 1.83 | wps 24889.8 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 28622 | lr 0.000186918 | gnorm 0.866 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 76418
2022-03-05 10:36:51 | INFO | fairseq.trainer | begin training epoch 298
2022-03-05 10:36:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:37:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:40:13 | INFO | train_inner | epoch 298:     79 / 97 loss=3.091, nll_loss=0.872, ppl=1.83, wps=24655.6, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28700, lr=0.000186663, gnorm=0.863, loss_scale=16, train_wall=236, gb_free=21, wall=76620
2022-03-05 10:40:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:41:03 | INFO | valid | epoch 298 | valid on 'valid' subset | loss 12.942 | nll_loss 11.983 | ppl 4049.24 | wps 45596.7 | wpb 510.9 | bsz 1 | num_updates 28718 | best_loss 8.737
2022-03-05 10:41:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 298 @ 28718 updates
2022-03-05 10:41:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:41:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:41:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 298 @ 28718 updates, score 12.942) (writing took 2.462585248053074 seconds)
2022-03-05 10:41:06 | INFO | fairseq_cli.train | end of epoch 298 (average epoch stats below)
2022-03-05 10:41:06 | INFO | train | epoch 298 | loss 3.09 | nll_loss 0.871 | ppl 1.83 | wps 24639.5 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28718 | lr 0.000186605 | gnorm 0.863 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 76673
2022-03-05 10:41:06 | INFO | fairseq.trainer | begin training epoch 299
2022-03-05 10:41:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:43:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:44:38 | INFO | train_inner | epoch 299:     83 / 97 loss=3.09, nll_loss=0.872, ppl=1.83, wps=24685.8, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28800, lr=0.000186339, gnorm=0.874, loss_scale=16, train_wall=235, gb_free=21, wall=76885
2022-03-05 10:45:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:45:18 | INFO | valid | epoch 299 | valid on 'valid' subset | loss 13.053 | nll_loss 12.108 | ppl 4414.65 | wps 44996 | wpb 510.9 | bsz 1 | num_updates 28814 | best_loss 8.737
2022-03-05 10:45:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 299 @ 28814 updates
2022-03-05 10:45:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:45:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:45:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 299 @ 28814 updates, score 13.053) (writing took 2.5435717394575477 seconds)
2022-03-05 10:45:21 | INFO | fairseq_cli.train | end of epoch 299 (average epoch stats below)
2022-03-05 10:45:21 | INFO | train | epoch 299 | loss 3.09 | nll_loss 0.872 | ppl 1.83 | wps 24632.6 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28814 | lr 0.000186294 | gnorm 0.871 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 76928
2022-03-05 10:45:21 | INFO | fairseq.trainer | begin training epoch 300
2022-03-05 10:45:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:49:01 | INFO | train_inner | epoch 300:     86 / 97 loss=3.088, nll_loss=0.869, ppl=1.83, wps=24889.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=28900, lr=0.000186016, gnorm=0.865, loss_scale=16, train_wall=233, gb_free=21, wall=77148
2022-03-05 10:49:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 10:49:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:49:34 | INFO | valid | epoch 300 | valid on 'valid' subset | loss 12.949 | nll_loss 11.986 | ppl 4057.57 | wps 45590.7 | wpb 510.9 | bsz 1 | num_updates 28910 | best_loss 8.737
2022-03-05 10:49:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 300 @ 28910 updates
2022-03-05 10:49:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:49:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:49:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 300 @ 28910 updates, score 12.949) (writing took 2.3939145244657993 seconds)
2022-03-05 10:49:36 | INFO | fairseq_cli.train | end of epoch 300 (average epoch stats below)
2022-03-05 10:49:36 | INFO | train | epoch 300 | loss 3.087 | nll_loss 0.868 | ppl 1.83 | wps 24627.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 28910 | lr 0.000185984 | gnorm 0.868 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 77183
2022-03-05 10:49:36 | INFO | fairseq.trainer | begin training epoch 301
2022-03-05 10:49:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:51:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 10:53:29 | INFO | train_inner | epoch 301:     91 / 97 loss=3.088, nll_loss=0.87, ppl=1.83, wps=24432.4, ups=0.37, wpb=65492.9, bsz=127.9, num_updates=29000, lr=0.000185695, gnorm=0.873, loss_scale=8, train_wall=238, gb_free=21, wall=77416
2022-03-05 10:53:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:53:49 | INFO | valid | epoch 301 | valid on 'valid' subset | loss 13.028 | nll_loss 12.08 | ppl 4329.58 | wps 45554.7 | wpb 510.9 | bsz 1 | num_updates 29006 | best_loss 8.737
2022-03-05 10:53:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 301 @ 29006 updates
2022-03-05 10:53:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:53:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:53:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 301 @ 29006 updates, score 13.028) (writing took 2.4898161496967077 seconds)
2022-03-05 10:53:52 | INFO | fairseq_cli.train | end of epoch 301 (average epoch stats below)
2022-03-05 10:53:52 | INFO | train | epoch 301 | loss 3.087 | nll_loss 0.868 | ppl 1.83 | wps 24620.4 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29006 | lr 0.000185676 | gnorm 0.87 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 77439
2022-03-05 10:53:52 | INFO | fairseq.trainer | begin training epoch 302
2022-03-05 10:53:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:57:52 | INFO | train_inner | epoch 302:     94 / 97 loss=3.086, nll_loss=0.868, ppl=1.82, wps=24913.9, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29100, lr=0.000185376, gnorm=0.873, loss_scale=16, train_wall=233, gb_free=21, wall=77679
2022-03-05 10:58:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 10:58:04 | INFO | valid | epoch 302 | valid on 'valid' subset | loss 12.953 | nll_loss 11.998 | ppl 4091.02 | wps 45578 | wpb 510.9 | bsz 1 | num_updates 29103 | best_loss 8.737
2022-03-05 10:58:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 302 @ 29103 updates
2022-03-05 10:58:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:58:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 10:58:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 302 @ 29103 updates, score 12.953) (writing took 2.4030647734180093 seconds)
2022-03-05 10:58:07 | INFO | fairseq_cli.train | end of epoch 302 (average epoch stats below)
2022-03-05 10:58:07 | INFO | train | epoch 302 | loss 3.086 | nll_loss 0.867 | ppl 1.82 | wps 24901.5 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29103 | lr 0.000185366 | gnorm 0.874 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 77694
2022-03-05 10:58:07 | INFO | fairseq.trainer | begin training epoch 303
2022-03-05 10:58:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 10:59:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:02:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:02:19 | INFO | valid | epoch 303 | valid on 'valid' subset | loss 13.029 | nll_loss 12.083 | ppl 4339.18 | wps 45376.4 | wpb 510.9 | bsz 1 | num_updates 29199 | best_loss 8.737
2022-03-05 11:02:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 303 @ 29199 updates
2022-03-05 11:02:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 11:02:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 11:02:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 303 @ 29199 updates, score 13.029) (writing took 2.344540849328041 seconds)
2022-03-05 11:02:22 | INFO | fairseq_cli.train | end of epoch 303 (average epoch stats below)
2022-03-05 11:02:22 | INFO | train | epoch 303 | loss 3.083 | nll_loss 0.864 | ppl 1.82 | wps 24663.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29199 | lr 0.000185061 | gnorm 0.874 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 77949
2022-03-05 11:02:22 | INFO | fairseq.trainer | begin training epoch 304
2022-03-05 11:02:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:02:24 | INFO | train_inner | epoch 304:      1 / 97 loss=3.084, nll_loss=0.865, ppl=1.82, wps=24032.3, ups=0.37, wpb=65451.9, bsz=127.8, num_updates=29200, lr=0.000185058, gnorm=0.873, loss_scale=8, train_wall=235, gb_free=21, wall=77951
2022-03-05 11:06:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:06:34 | INFO | valid | epoch 304 | valid on 'valid' subset | loss 12.941 | nll_loss 11.984 | ppl 4052.05 | wps 45651.6 | wpb 510.9 | bsz 1 | num_updates 29296 | best_loss 8.737
2022-03-05 11:06:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 304 @ 29296 updates
2022-03-05 11:06:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 11:06:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 11:06:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 304 @ 29296 updates, score 12.941) (writing took 2.408887440338731 seconds)
2022-03-05 11:06:37 | INFO | fairseq_cli.train | end of epoch 304 (average epoch stats below)
2022-03-05 11:06:37 | INFO | train | epoch 304 | loss 3.083 | nll_loss 0.864 | ppl 1.82 | wps 24897.1 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29296 | lr 0.000184755 | gnorm 0.874 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 78204
2022-03-05 11:06:37 | INFO | fairseq.trainer | begin training epoch 305
2022-03-05 11:06:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:06:47 | INFO | train_inner | epoch 305:      4 / 97 loss=3.082, nll_loss=0.863, ppl=1.82, wps=24919.5, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29300, lr=0.000184742, gnorm=0.874, loss_scale=16, train_wall=233, gb_free=21, wall=78214
2022-03-05 11:10:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:10:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:10:50 | INFO | valid | epoch 305 | valid on 'valid' subset | loss 12.974 | nll_loss 12.018 | ppl 4146.23 | wps 45265.8 | wpb 510.9 | bsz 1 | num_updates 29392 | best_loss 8.737
2022-03-05 11:10:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 305 @ 29392 updates
2022-03-05 11:10:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 11:10:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 11:10:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 305 @ 29392 updates, score 12.974) (writing took 2.393099620938301 seconds)
2022-03-05 11:10:52 | INFO | fairseq_cli.train | end of epoch 305 (average epoch stats below)
2022-03-05 11:10:52 | INFO | train | epoch 305 | loss 3.081 | nll_loss 0.863 | ppl 1.82 | wps 24637.3 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29392 | lr 0.000184453 | gnorm 0.871 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 78459
2022-03-05 11:10:52 | INFO | fairseq.trainer | begin training epoch 306
2022-03-05 11:10:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:11:13 | INFO | train_inner | epoch 306:      8 / 97 loss=3.08, nll_loss=0.861, ppl=1.82, wps=24677.3, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29400, lr=0.000184428, gnorm=0.871, loss_scale=16, train_wall=236, gb_free=21, wall=78480
2022-03-05 11:15:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:15:05 | INFO | valid | epoch 306 | valid on 'valid' subset | loss 12.985 | nll_loss 12.03 | ppl 4182.66 | wps 45283.7 | wpb 510.9 | bsz 1 | num_updates 29489 | best_loss 8.737
2022-03-05 11:15:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 306 @ 29489 updates
2022-03-05 11:15:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 11:15:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 11:15:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 306 @ 29489 updates, score 12.985) (writing took 2.4407723769545555 seconds)
2022-03-05 11:15:07 | INFO | fairseq_cli.train | end of epoch 306 (average epoch stats below)
2022-03-05 11:15:07 | INFO | train | epoch 306 | loss 3.079 | nll_loss 0.861 | ppl 1.82 | wps 24897.6 | ups 0.38 | wpb 65491.6 | bsz 127.9 | num_updates 29489 | lr 0.000184149 | gnorm 0.866 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 78714
2022-03-05 11:15:07 | INFO | fairseq.trainer | begin training epoch 307
2022-03-05 11:15:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:15:35 | INFO | train_inner | epoch 307:     11 / 97 loss=3.079, nll_loss=0.86, ppl=1.82, wps=24913.2, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29500, lr=0.000184115, gnorm=0.865, loss_scale=16, train_wall=233, gb_free=21, wall=78742
2022-03-05 11:16:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-05 11:19:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:19:20 | INFO | valid | epoch 307 | valid on 'valid' subset | loss 12.955 | nll_loss 12.002 | ppl 4101.44 | wps 45385.8 | wpb 510.9 | bsz 1 | num_updates 29585 | best_loss 8.737
2022-03-05 11:19:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 307 @ 29585 updates
2022-03-05 11:19:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 11:19:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 11:19:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 307 @ 29585 updates, score 12.955) (writing took 2.4045943710952997 seconds)
2022-03-05 11:19:22 | INFO | fairseq_cli.train | end of epoch 307 (average epoch stats below)
2022-03-05 11:19:22 | INFO | train | epoch 307 | loss 3.078 | nll_loss 0.859 | ppl 1.81 | wps 24644.7 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29585 | lr 0.00018385 | gnorm 0.854 | loss_scale 16 | train_wall 226 | gb_free 21 | wall 78969
2022-03-05 11:19:22 | INFO | fairseq.trainer | begin training epoch 308
2022-03-05 11:19:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:20:01 | INFO | train_inner | epoch 308:     15 / 97 loss=3.078, nll_loss=0.859, ppl=1.81, wps=24691, ups=0.38, wpb=65492.9, bsz=127.9, num_updates=29600, lr=0.000183804, gnorm=0.856, loss_scale=16, train_wall=235, gb_free=21, wall=79008
2022-03-05 11:21:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 11:23:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 11:23:35 | INFO | valid | epoch 308 | valid on 'valid' subset | loss 12.989 | nll_loss 12.038 | ppl 4204.42 | wps 45343.1 | wpb 510.9 | bsz 1 | num_updates 29681 | best_loss 8.737
2022-03-05 11:23:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 308 @ 29681 updates
2022-03-05 11:23:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 11:23:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-03-05 11:23:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.0625_fp16-label_smoothing_0.1_#1/checkpoint_last.pt (epoch 308 @ 29681 updates, score 12.989) (writing took 2.42246119864285 seconds)
2022-03-05 11:23:37 | INFO | fairseq_cli.train | end of epoch 308 (average epoch stats below)
2022-03-05 11:23:37 | INFO | train | epoch 308 | loss 3.077 | nll_loss 0.858 | ppl 1.81 | wps 24647.1 | ups 0.38 | wpb 65491.1 | bsz 127.9 | num_updates 29681 | lr 0.000183553 | gnorm 0.871 | loss_scale 8 | train_wall 226 | gb_free 21 | wall 79224
2022-03-05 11:23:37 | INFO | fairseq.trainer | begin training epoch 309
2022-03-05 11:23:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 11:24:26 | INFO | train_inner | epoch 309:     19 / 97 loss=3.075, nll_loss=0.856, ppl=1.81, wps=24684.2, ups=0.38, wpb=65490.8, bsz=127.9, num_updates=29700, lr=0.000183494, gnorm=0.87, loss_scale=8, train_wall=235, gb_free=21, wall=79273
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 79, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 368, in forward
    x, attn = self.self_attn(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/multihead_attention.py", line 170, in forward
    return F.multi_head_attention_forward(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 4216, in multi_head_attention_forward
    k = linear(key, k_proj_weight_non_opt, in_proj_bias[embed_dim:(embed_dim * 2)])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 1694, in linear
    output += bias
KeyboardInterrupt
