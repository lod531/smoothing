Sender: LSF System <lsfadmin@eu-g3-053>
Subject: Job 210595549: <iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1> in cluster <euler> Done

Job <iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1> was submitted from host <eu-login-06> by user <andriusb> in cluster <euler> at Wed Mar 23 11:35:55 2022
Job was executed on host(s) <eu-g3-053>, in queue <gpuhe.4h>, as user <andriusb> in cluster <euler> at Wed Mar 23 11:36:27 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Mar 23 11:36:27 2022
Terminated at Wed Mar 23 12:37:31 2022
Results reported at Wed Mar 23 12:37:31 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/iwslt14.tokenized.de-en --save-dir /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1 --arch transformer_iwslt_de_en --share-decoder-input-output-embed --optimizer adam --adam-betas "(0.9, 0.98)" --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.3 --weight-decay 0.0001 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas \(0.025,0.325,0.65\) --max-tokens 32768 --eval-bleu --eval-bleu-args '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --eval-bleu-print-samples --fp16 --no-epoch-checkpoints --patience 3 --seed 66575611 --best-checkpoint-metric bleu --maximize-best-checkpoint-metric
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3636.62 sec.
    Max Memory :                                 5502 MB
    Average Memory :                             4304.56 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               14498.00 MB
    Max Swap :                                   1 MB
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   3664 sec.
    Turnaround time :                            3696 sec.

The output (if any) follows:

2022-03-23 11:36:41 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 32768, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 32768, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer_iwslt_de_en', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, alphas='(0.025,0.325,0.65)', amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer_iwslt_de_en', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='jelinek_mercer_smoothing', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14.tokenized.de-en', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=True, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, jelinek_n=2, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_tokens=32768, max_tokens_valid=32768, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=3, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1', save_interval=1, save_interval_updates=0, scoring='bleu', seed=66575611, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt14.tokenized.de-en', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': True}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.025,0.325,0.65)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-23 11:36:41 | INFO | fairseq.tasks.translation | [de] dictionary: 8848 types
2022-03-23 11:36:41 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types
2022-03-23 11:36:42 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.de
2022-03-23 11:36:42 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.en
2022-03-23 11:36:42 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en train de-en 160239 examples
Calculating frequency stats:
  0%|          | 0/160239 [00:00<?, ?it/s]  1%|          | 1009/160239 [00:00<00:15, 10080.20it/s]  1%|▏         | 2298/160239 [00:00<00:13, 11729.46it/s]  2%|▏         | 3648/160239 [00:00<00:12, 12534.50it/s]  3%|▎         | 4983/160239 [00:00<00:12, 12852.59it/s]  4%|▍         | 6269/160239 [00:00<00:12, 12624.35it/s]  5%|▍         | 7548/160239 [00:00<00:12, 12673.28it/s]  6%|▌         | 8816/160239 [00:00<00:12, 12319.17it/s]  6%|▋         | 10125/160239 [00:00<00:11, 12557.82it/s]  7%|▋         | 11445/160239 [00:00<00:11, 12751.30it/s]  8%|▊         | 12722/160239 [00:01<00:11, 12577.74it/s]  9%|▊         | 13982/160239 [00:01<00:11, 12576.78it/s] 10%|▉         | 15241/160239 [00:01<00:11, 12443.37it/s] 10%|█         | 16487/160239 [00:01<00:11, 12227.66it/s] 11%|█         | 17721/160239 [00:01<00:11, 12255.72it/s] 12%|█▏        | 18958/160239 [00:01<00:11, 12288.19it/s] 13%|█▎        | 20375/160239 [00:01<00:10, 12846.12it/s] 14%|█▎        | 21661/160239 [00:01<00:11, 12463.25it/s] 14%|█▍        | 22911/160239 [00:01<00:11, 12460.58it/s] 15%|█▌        | 24160/160239 [00:01<00:10, 12456.41it/s] 16%|█▌        | 25408/160239 [00:02<00:10, 12459.96it/s] 17%|█▋        | 26656/160239 [00:02<00:10, 12266.30it/s] 17%|█▋        | 27960/160239 [00:02<00:10, 12492.73it/s] 18%|█▊        | 29213/160239 [00:02<00:10, 12503.12it/s] 19%|█▉        | 30465/160239 [00:02<00:10, 12213.84it/s] 20%|█▉        | 31832/160239 [00:02<00:10, 12639.47it/s] 21%|██        | 33099/160239 [00:02<00:10, 12471.88it/s] 21%|██▏       | 34349/160239 [00:02<00:10, 12245.05it/s] 22%|██▏       | 35576/160239 [00:02<00:10, 12124.23it/s] 23%|██▎       | 36790/160239 [00:02<00:10, 12093.88it/s] 24%|██▎       | 38021/160239 [00:03<00:10, 12157.09it/s] 25%|██▍       | 39289/160239 [00:03<00:09, 12309.53it/s] 25%|██▌       | 40576/160239 [00:03<00:09, 12474.99it/s] 26%|██▌       | 41825/160239 [00:03<00:09, 12321.08it/s] 27%|██▋       | 43058/160239 [00:03<00:09, 12097.92it/s] 28%|██▊       | 44269/160239 [00:03<00:09, 11941.67it/s] 28%|██▊       | 45533/160239 [00:03<00:09, 12145.79it/s] 29%|██▉       | 46835/160239 [00:03<00:09, 12402.86it/s] 30%|███       | 48112/160239 [00:03<00:08, 12509.31it/s] 31%|███       | 49372/160239 [00:03<00:08, 12528.04it/s] 32%|███▏      | 50626/160239 [00:04<00:08, 12405.60it/s] 32%|███▏      | 51879/160239 [00:04<00:08, 12441.70it/s] 33%|███▎      | 53182/160239 [00:04<00:08, 12614.74it/s] 34%|███▍      | 54444/160239 [00:04<00:08, 12536.99it/s] 35%|███▍      | 55699/160239 [00:04<00:08, 12496.52it/s] 36%|███▌      | 57035/160239 [00:04<00:08, 12748.97it/s] 36%|███▋      | 58362/160239 [00:04<00:07, 12902.76it/s] 37%|███▋      | 59653/160239 [00:04<00:07, 12894.79it/s] 38%|███▊      | 60943/160239 [00:04<00:07, 12708.00it/s] 39%|███▉      | 62215/160239 [00:04<00:07, 12686.10it/s] 40%|███▉      | 63515/160239 [00:05<00:07, 12778.21it/s] 41%|████      | 64969/160239 [00:05<00:07, 13302.48it/s] 41%|████▏     | 66300/160239 [00:05<00:07, 13289.75it/s] 42%|████▏     | 67630/160239 [00:05<00:07, 12857.40it/s] 43%|████▎     | 68919/160239 [00:05<00:07, 12593.30it/s] 44%|████▍     | 70204/160239 [00:05<00:07, 12666.94it/s] 45%|████▍     | 71473/160239 [00:05<00:07, 12658.98it/s] 45%|████▌     | 72741/160239 [00:05<00:06, 12539.03it/s] 46%|████▌     | 73997/160239 [00:05<00:06, 12503.00it/s] 47%|████▋     | 75249/160239 [00:06<00:06, 12352.98it/s] 48%|████▊     | 76486/160239 [00:06<00:06, 12137.67it/s] 49%|████▊     | 77860/160239 [00:06<00:06, 12604.09it/s] 49%|████▉     | 79156/160239 [00:06<00:06, 12706.88it/s] 50%|█████     | 80491/160239 [00:06<00:06, 12895.87it/s] 51%|█████     | 81813/160239 [00:06<00:06, 12991.14it/s] 52%|█████▏    | 83120/160239 [00:06<00:05, 13008.01it/s] 53%|█████▎    | 84422/160239 [00:06<00:05, 12901.66it/s] 54%|█████▎    | 85795/160239 [00:06<00:05, 13146.07it/s] 54%|█████▍    | 87116/160239 [00:06<00:05, 13164.73it/s] 55%|█████▌    | 88433/160239 [00:07<00:05, 12974.31it/s] 56%|█████▌    | 89780/160239 [00:07<00:05, 13120.62it/s] 57%|█████▋    | 91093/160239 [00:07<00:05, 12913.98it/s] 58%|█████▊    | 92386/160239 [00:07<00:05, 12870.85it/s] 58%|█████▊    | 93674/160239 [00:07<00:05, 12717.99it/s] 59%|█████▉    | 94947/160239 [00:07<00:05, 12489.63it/s] 60%|██████    | 96250/160239 [00:07<00:05, 12644.81it/s] 61%|██████    | 97529/160239 [00:07<00:04, 12686.43it/s] 62%|██████▏   | 98820/160239 [00:07<00:04, 12751.06it/s] 63%|██████▎   | 100174/160239 [00:07<00:04, 12984.77it/s] 63%|██████▎   | 101474/160239 [00:08<00:04, 12881.14it/s] 64%|██████▍   | 102763/160239 [00:08<00:04, 12736.64it/s] 65%|██████▍   | 104038/160239 [00:08<00:04, 12618.95it/s] 66%|██████▌   | 105396/160239 [00:08<00:04, 12901.64it/s] 67%|██████▋   | 106688/160239 [00:08<00:04, 12848.10it/s] 67%|██████▋   | 107974/160239 [00:08<00:04, 12458.30it/s] 68%|██████▊   | 109223/160239 [00:08<00:04, 12193.42it/s] 69%|██████▉   | 110475/160239 [00:08<00:04, 12287.38it/s] 70%|██████▉   | 111856/160239 [00:08<00:03, 12729.74it/s] 71%|███████   | 113132/160239 [00:08<00:03, 12547.45it/s] 71%|███████▏  | 114428/160239 [00:09<00:03, 12666.87it/s] 72%|███████▏  | 115709/160239 [00:09<00:03, 12707.72it/s] 73%|███████▎  | 116982/160239 [00:09<00:03, 12485.33it/s] 74%|███████▍  | 118291/160239 [00:09<00:03, 12661.50it/s] 75%|███████▍  | 119617/160239 [00:09<00:03, 12836.67it/s] 75%|███████▌  | 120902/160239 [00:09<00:03, 12558.00it/s] 76%|███████▋  | 122346/160239 [00:09<00:02, 13109.39it/s] 77%|███████▋  | 123660/160239 [00:09<00:02, 12888.92it/s] 78%|███████▊  | 124952/160239 [00:09<00:02, 12564.34it/s] 79%|███████▉  | 126219/160239 [00:10<00:02, 12593.04it/s] 80%|███████▉  | 127493/160239 [00:10<00:02, 12634.20it/s] 80%|████████  | 128803/160239 [00:10<00:02, 12771.26it/s] 81%|████████  | 130082/160239 [00:10<00:02, 12317.75it/s] 82%|████████▏ | 131364/160239 [00:10<00:02, 12459.49it/s] 83%|████████▎ | 132614/160239 [00:10<00:02, 12453.08it/s] 84%|████████▎ | 133862/160239 [00:10<00:02, 12200.85it/s] 84%|████████▍ | 135093/160239 [00:10<00:02, 12230.91it/s] 85%|████████▌ | 136364/160239 [00:10<00:01, 12370.78it/s] 86%|████████▌ | 137669/160239 [00:10<00:01, 12569.41it/s] 87%|████████▋ | 138952/160239 [00:11<00:01, 12646.37it/s] 88%|████████▊ | 140218/160239 [00:11<00:01, 12578.47it/s] 88%|████████▊ | 141525/160239 [00:11<00:01, 12722.94it/s] 89%|████████▉ | 142798/160239 [00:11<00:01, 12453.32it/s] 90%|████████▉ | 144056/160239 [00:11<00:01, 12489.97it/s] 91%|█████████ | 145318/160239 [00:11<00:01, 12524.44it/s] 91%|█████████▏| 146572/160239 [00:11<00:01, 12311.49it/s] 92%|█████████▏| 147814/160239 [00:11<00:01, 12337.88it/s] 93%|█████████▎| 149049/160239 [00:11<00:00, 12023.24it/s] 94%|█████████▍| 150328/160239 [00:11<00:00, 12245.47it/s] 95%|█████████▍| 151583/160239 [00:12<00:00, 12333.37it/s] 95%|█████████▌| 152850/160239 [00:12<00:00, 12429.71it/s] 96%|█████████▌| 154095/160239 [00:12<00:00, 12417.82it/s] 97%|█████████▋| 155452/160239 [00:12<00:00, 12759.94it/s] 98%|█████████▊| 156729/160239 [00:12<00:00, 12701.60it/s] 99%|█████████▊| 158000/160239 [00:12<00:00, 12671.20it/s] 99%|█████████▉| 159268/160239 [00:12<00:00, 12649.78it/s]100%|██████████| 160239/160239 [00:12<00:00, 12569.01it/s]

gathering stats for n=1
  0%|          | 0/160239 [00:00<?, ?it/s]  2%|▏         | 3830/160239 [00:00<00:04, 38296.93it/s]  5%|▍         | 7660/160239 [00:00<00:03, 38231.09it/s]  7%|▋         | 11528/160239 [00:00<00:03, 38434.72it/s] 10%|▉         | 15372/160239 [00:00<00:03, 38329.19it/s] 12%|█▏        | 19205/160239 [00:00<00:03, 38250.94it/s] 14%|█▍        | 23047/160239 [00:00<00:03, 38307.03it/s] 17%|█▋        | 26878/160239 [00:00<00:03, 38286.42it/s] 19%|█▉        | 30707/160239 [00:00<00:03, 38220.78it/s] 22%|██▏       | 34555/160239 [00:00<00:03, 38299.73it/s] 24%|██▍       | 38386/160239 [00:01<00:03, 38160.09it/s] 26%|██▋       | 42203/160239 [00:01<00:03, 38026.43it/s] 29%|██▊       | 46006/160239 [00:01<00:03, 37847.00it/s] 31%|███       | 49829/160239 [00:01<00:02, 37959.22it/s] 33%|███▎      | 53680/160239 [00:01<00:02, 38121.20it/s] 36%|███▌      | 57633/160239 [00:01<00:02, 38543.71it/s] 38%|███▊      | 61492/160239 [00:01<00:02, 38553.09it/s] 41%|████      | 65612/160239 [00:01<00:02, 39342.96it/s] 43%|████▎     | 69547/160239 [00:01<00:02, 38611.36it/s] 46%|████▌     | 73412/160239 [00:01<00:02, 38502.39it/s] 48%|████▊     | 77274/160239 [00:02<00:02, 38536.01it/s] 51%|█████     | 81341/160239 [00:02<00:02, 39170.45it/s] 53%|█████▎    | 85260/160239 [00:02<00:01, 38962.93it/s] 56%|█████▌    | 89297/160239 [00:02<00:01, 39376.41it/s] 58%|█████▊    | 93236/160239 [00:02<00:01, 39337.41it/s] 61%|██████    | 97171/160239 [00:02<00:01, 39133.34it/s] 63%|██████▎   | 101156/160239 [00:02<00:01, 39345.32it/s] 66%|██████▌   | 105092/160239 [00:02<00:01, 39223.33it/s] 68%|██████▊   | 109015/160239 [00:02<00:01, 38635.18it/s] 70%|███████   | 112889/160239 [00:02<00:01, 38663.48it/s] 73%|███████▎  | 116763/160239 [00:03<00:01, 38684.76it/s] 75%|███████▌  | 120658/160239 [00:03<00:01, 38761.01it/s] 78%|███████▊  | 124635/160239 [00:03<00:00, 39058.08it/s] 80%|████████  | 128551/160239 [00:03<00:00, 39084.35it/s] 83%|████████▎ | 132460/160239 [00:03<00:00, 38290.77it/s] 85%|████████▌ | 136293/160239 [00:03<00:00, 38164.11it/s] 88%|████████▊ | 140310/160239 [00:03<00:00, 38754.52it/s] 90%|████████▉ | 144189/160239 [00:03<00:00, 38631.15it/s] 92%|█████████▏| 148054/160239 [00:03<00:00, 38300.87it/s] 95%|█████████▍| 151886/160239 [00:03<00:00, 38144.19it/s] 97%|█████████▋| 155742/160239 [00:04<00:00, 38266.47it/s]100%|█████████▉| 159611/160239 [00:04<00:00, 38389.76it/s]100%|██████████| 160239/160239 [00:04<00:00, 38566.90it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 1972.86it/s]2022-03-23 11:37:05 | INFO | fairseq_cli.train | TransformerModel(
  (encoder): TransformerEncoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8848, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (decoder): TransformerDecoderBase(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6632, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=6632, bias=False)
  )
)
2022-03-23 11:37:05 | INFO | fairseq_cli.train | task: TranslationTask
2022-03-23 11:37:05 | INFO | fairseq_cli.train | model: TransformerModel
2022-03-23 11:37:05 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-23 11:37:05 | INFO | fairseq_cli.train | num. shared model params: 39,469,056 (num. trained: 39,469,056)
2022-03-23 11:37:05 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-23 11:37:05 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de
2022-03-23 11:37:05 | INFO | fairseq.data.data_utils | loaded 7,283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en
2022-03-23 11:37:05 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples
2022-03-23 11:37:05 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-23 11:37:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 11:37:05 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-23 11:37:05 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 11:37:05 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-23 11:37:05 | INFO | fairseq_cli.train | max tokens per device = 32768 and max sentences per device = None
2022-03-23 11:37:05 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 11:37:05 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 11:37:05 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-23 11:37:05 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.de
2022-03-23 11:37:05 | INFO | fairseq.data.data_utils | loaded 160,239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.en
2022-03-23 11:37:05 | INFO | fairseq.tasks.translation | data-bin/iwslt14.tokenized.de-en train de-en 160239 examples
2022-03-23 11:37:06 | INFO | fairseq.trainer | begin training epoch 1
2022-03-23 11:37:06 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-23 11:37:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-23 11:37:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 11:37:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 11:37:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-23 11:37:49 | INFO | train_inner | epoch 001:    104 / 157 loss=13.023, ppl=8321.05, wps=66017.5, ups=2.62, wpb=25146.2, bsz=969, num_updates=100, lr=1.25e-05, gnorm=3.685, loss_scale=8, train_wall=43, gb_free=12.1, wall=44
2022-03-23 11:38:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 11:38:12 | INFO | fairseq.tasks.translation | example hypothesis: ,,,......
2022-03-23 11:38:12 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 11:38:15 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,....
2022-03-23 11:38:15 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 11:38:18 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,
2022-03-23 11:38:18 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 11:38:21 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,
2022-03-23 11:38:21 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 11:38:25 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-23 11:38:25 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 11:38:29 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,
2022-03-23 11:38:29 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 11:38:33 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-23 11:38:33 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 11:38:38 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-23 11:38:38 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 11:38:44 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-23 11:38:44 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 11:38:46 | INFO | fairseq.tasks.translation | example hypothesis: ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-23 11:38:46 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 11:38:46 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.242 | ppl 4844.3 | bleu 0.01 | wps 4809 | wpb 17862.2 | bsz 728.3 | num_updates 153
2022-03-23 11:38:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 153 updates
2022-03-23 11:38:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:38:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:38:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 1 @ 153 updates, score 0.01) (writing took 1.6260894909501076 seconds)
2022-03-23 11:38:48 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-23 11:38:48 | INFO | train | epoch 001 | loss 12.507 | ppl 5818.83 | wps 39524.7 | ups 1.58 | wpb 25079.4 | bsz 998 | num_updates 153 | lr 1.9125e-05 | gnorm 2.866 | loss_scale 8 | train_wall 62 | gb_free 22.3 | wall 102
KL Stats: Epoch 1 Divergences: Uniform: 0.5820593497338287 Unigram: 1.4274797873706069
2022-03-23 11:38:48 | INFO | fairseq.trainer | begin training epoch 2
2022-03-23 11:38:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 11:39:06 | INFO | train_inner | epoch 002:     47 / 157 loss=11.185, ppl=2328.55, wps=32922.3, ups=1.3, wpb=25333.2, bsz=1104.8, num_updates=200, lr=2.5e-05, gnorm=1.181, loss_scale=8, train_wall=37, gb_free=12.9, wall=121
2022-03-23 11:39:44 | INFO | train_inner | epoch 002:    147 / 157 loss=10.596, ppl=1547.98, wps=66364, ups=2.64, wpb=25185, bsz=961.8, num_updates=300, lr=3.75e-05, gnorm=1.004, loss_scale=8, train_wall=38, gb_free=12.2, wall=159
2022-03-23 11:39:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 11:39:50 | INFO | fairseq.tasks.translation | example hypothesis: we we.
2022-03-23 11:39:50 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 11:39:53 | INFO | fairseq.tasks.translation | example hypothesis: the the the the.
2022-03-23 11:39:53 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 11:39:57 | INFO | fairseq.tasks.translation | example hypothesis: and the the the the the the.
2022-03-23 11:39:57 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 11:40:00 | INFO | fairseq.tasks.translation | example hypothesis: and and,,,,,,,,,,,.
2022-03-23 11:40:00 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 11:40:04 | INFO | fairseq.tasks.translation | example hypothesis: and and and,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-23 11:40:04 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 11:40:09 | INFO | fairseq.tasks.translation | example hypothesis: and and and and and,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.
2022-03-23 11:40:09 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 11:40:14 | INFO | fairseq.tasks.translation | example hypothesis: and and,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.
2022-03-23 11:40:14 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 11:40:19 | INFO | fairseq.tasks.translation | example hypothesis: and and and,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-23 11:40:19 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 11:40:25 | INFO | fairseq.tasks.translation | example hypothesis: and and,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,.
2022-03-23 11:40:25 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 11:40:27 | INFO | fairseq.tasks.translation | example hypothesis: and and and,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2022-03-23 11:40:27 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 11:40:27 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.318 | ppl 2553.01 | bleu 0.02 | wps 4481.8 | wpb 17862.2 | bsz 728.3 | num_updates 310 | best_bleu 0.02
2022-03-23 11:40:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 310 updates
2022-03-23 11:40:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:40:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:40:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 2 @ 310 updates, score 0.02) (writing took 1.7964263283647597 seconds)
2022-03-23 11:40:29 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-23 11:40:29 | INFO | train | epoch 002 | loss 10.665 | ppl 1623.92 | wps 39112.2 | ups 1.55 | wpb 25153.6 | bsz 1020.6 | num_updates 310 | lr 3.875e-05 | gnorm 0.99 | loss_scale 8 | train_wall 58 | gb_free 12.1 | wall 203
KL Stats: Epoch 2 Divergences: Uniform: 0.8770393974506486 Unigram: 0.2923931689730674
2022-03-23 11:40:29 | INFO | fairseq.trainer | begin training epoch 3
2022-03-23 11:40:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 11:41:03 | INFO | train_inner | epoch 003:     90 / 157 loss=10.298, ppl=1259.07, wps=31143.9, ups=1.27, wpb=24585.2, bsz=969, num_updates=400, lr=5e-05, gnorm=0.737, loss_scale=8, train_wall=36, gb_free=11.8, wall=237
2022-03-23 11:41:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 11:41:31 | INFO | fairseq.tasks.translation | example hypothesis: we.
2022-03-23 11:41:31 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 11:41:35 | INFO | fairseq.tasks.translation | example hypothesis: it's the the the.
2022-03-23 11:41:35 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 11:41:38 | INFO | fairseq.tasks.translation | example hypothesis: it's.
2022-03-23 11:41:38 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 11:41:42 | INFO | fairseq.tasks.translation | example hypothesis: and it's.
2022-03-23 11:41:42 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 11:41:46 | INFO | fairseq.tasks.translation | example hypothesis: and it's, it's's, and it's's's, and it's.
2022-03-23 11:41:46 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 11:41:51 | INFO | fairseq.tasks.translation | example hypothesis: and and the, and the the the, and the, and and the the the the the, and and and and and and the the the the the the the the the the.
2022-03-23 11:41:51 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 11:41:56 | INFO | fairseq.tasks.translation | example hypothesis: and it's, it's, it's, and the the the the, and the, and the, and the, and the, and the the the, and the the the the, and the, and the, and the, and the, and the the the the the the the, and the, and the, and the, and the,
2022-03-23 11:41:56 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 11:42:03 | INFO | fairseq.tasks.translation | example hypothesis: and we, we, and we, and we, and we, and we, and we, and we, and we, and we, and the the the the the the the the the the, and we, and we, and we, and the the the the the the the the, and we, and we, and we, and the the the the the the the the the the the the the the the the the the the the the the the the
2022-03-23 11:42:03 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 11:42:10 | INFO | fairseq.tasks.translation | example hypothesis: and the, "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "
2022-03-23 11:42:10 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 11:42:13 | INFO | fairseq.tasks.translation | example hypothesis: and the, we, we, the, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, we, and the, we, we, we, and the the the the the the the the the, the, and the, and the, and the the the the, and the, and the the the the, and the, and the the the, and the the the the the, and the the the the the the the the the the the, and the, and the, and the the, and the, and the, and the, and the the the the the the the, we, we, we, and the, and the, we, we, and the, and the, and the, we, and the, and the, and the, we, we, we, we, we, and the, we, we, we, we, we, and the, we, we, we, we, we, we,
2022-03-23 11:42:13 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 11:42:13 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.189 | ppl 2334.65 | bleu 0.23 | wps 3970 | wpb 17862.2 | bsz 728.3 | num_updates 467 | best_bleu 0.23
2022-03-23 11:42:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 467 updates
2022-03-23 11:42:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:42:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:42:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 3 @ 467 updates, score 0.23) (writing took 1.7166046383790672 seconds)
2022-03-23 11:42:14 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-23 11:42:14 | INFO | train | epoch 003 | loss 10.209 | ppl 1183.96 | wps 37396.6 | ups 1.49 | wpb 25153.6 | bsz 1020.6 | num_updates 467 | lr 5.8375e-05 | gnorm 0.906 | loss_scale 8 | train_wall 58 | gb_free 11.8 | wall 309
KL Stats: Epoch 3 Divergences: Uniform: 1.3145611424644295 Unigram: 0.15195442628216582
2022-03-23 11:42:15 | INFO | fairseq.trainer | begin training epoch 4
2022-03-23 11:42:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 11:42:28 | INFO | train_inner | epoch 004:     33 / 157 loss=10.122, ppl=1114.05, wps=30068.9, ups=1.18, wpb=25454.8, bsz=1088.2, num_updates=500, lr=6.25e-05, gnorm=1.046, loss_scale=8, train_wall=37, gb_free=12, wall=322
2022-03-23 11:43:05 | INFO | train_inner | epoch 004:    133 / 157 loss=10.043, ppl=1054.92, wps=66769.5, ups=2.64, wpb=25263.8, bsz=1024.8, num_updates=600, lr=7.5e-05, gnorm=1.035, loss_scale=8, train_wall=37, gb_free=10.8, wall=360
2022-03-23 11:43:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 11:43:17 | INFO | fairseq.tasks.translation | example hypothesis: we're the.
2022-03-23 11:43:17 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 11:43:21 | INFO | fairseq.tasks.translation | example hypothesis: this is the that's the.
2022-03-23 11:43:21 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 11:43:24 | INFO | fairseq.tasks.translation | example hypothesis: this is a of the.
2022-03-23 11:43:24 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 11:43:28 | INFO | fairseq.tasks.translation | example hypothesis: and it's a, and it's a, and it's a.
2022-03-23 11:43:28 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 11:43:32 | INFO | fairseq.tasks.translation | example hypothesis: and it's a that's a that's a that's a that's not not not not not not not.
2022-03-23 11:43:32 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 11:43:36 | INFO | fairseq.tasks.translation | example hypothesis: and this is this is the of the of the of the world of the of the world, and this is the world of the world.
2022-03-23 11:43:36 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 11:43:41 | INFO | fairseq.tasks.translation | example hypothesis: but they're a, but you're a, but you're're a, but you're a to be be be be be be be be be be be be be be be be be.
2022-03-23 11:43:41 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 11:43:47 | INFO | fairseq.tasks.translation | example hypothesis: and we can can can can can the of the world, and we can can can can can can can can see the of the of the world, and we're the of the of the world.
2022-03-23 11:43:47 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 11:43:53 | INFO | fairseq.tasks.translation | example hypothesis: and, "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" "" ""
2022-03-23 11:43:53 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 11:43:56 | INFO | fairseq.tasks.translation | example hypothesis: so, we're a a a a a a to be a a, and you can can can can can can can be a a to be a to be a to be a of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the of the world, and it's a of the of the of the world, and you can can can can can can can can can can can can can
2022-03-23 11:43:56 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 11:43:56 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.971 | ppl 2007.06 | bleu 0.99 | wps 4283.9 | wpb 17862.2 | bsz 728.3 | num_updates 624 | best_bleu 0.99
2022-03-23 11:43:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 624 updates
2022-03-23 11:43:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:43:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:43:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 4 @ 624 updates, score 0.99) (writing took 1.7852896940894425 seconds)
2022-03-23 11:43:57 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-23 11:43:57 | INFO | train | epoch 004 | loss 10.039 | ppl 1051.84 | wps 38334.5 | ups 1.52 | wpb 25153.6 | bsz 1020.6 | num_updates 624 | lr 7.8e-05 | gnorm 1.008 | loss_scale 8 | train_wall 58 | gb_free 12.1 | wall 412
KL Stats: Epoch 4 Divergences: Uniform: 1.4264055032809142 Unigram: 0.21732204524811496
2022-03-23 11:43:58 | INFO | fairseq.trainer | begin training epoch 5
2022-03-23 11:43:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 11:44:26 | INFO | train_inner | epoch 005:     76 / 157 loss=9.986, ppl=1014.01, wps=30295, ups=1.23, wpb=24556.2, bsz=953.2, num_updates=700, lr=8.75e-05, gnorm=1.09, loss_scale=8, train_wall=37, gb_free=11.5, wall=441
2022-03-23 11:44:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 11:45:01 | INFO | fairseq.tasks.translation | example hypothesis: we're in the world in the world, we're in the world.
2022-03-23 11:45:01 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 11:45:06 | INFO | fairseq.tasks.translation | example hypothesis: this is this is the
2022-03-23 11:45:06 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 11:45:10 | INFO | fairseq.tasks.translation | example hypothesis: now, we have to be a
2022-03-23 11:45:10 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 11:45:14 | INFO | fairseq.tasks.translation | example hypothesis: and there's a
2022-03-23 11:45:14 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 11:45:18 | INFO | fairseq.tasks.translation | example hypothesis: and it's what we're not not not not not not not a lot of the world, and it's going to do it's going to do it's going to do it's going to do it's going to be a lot of the
2022-03-23 11:45:18 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 11:45:23 | INFO | fairseq.tasks.translation | example hypothesis: and this is in the world, and in the world, and in the world, and in the world, and in the world, and in the world in the world, and in the world, and in the world, and in the world, and the world, and in the world is
2022-03-23 11:45:23 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 11:45:27 | INFO | fairseq.tasks.translation | example hypothesis: but it's a lot of the
2022-03-23 11:45:27 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 11:45:31 | INFO | fairseq.tasks.translation | example hypothesis: and so, we're a lot of the
2022-03-23 11:45:31 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 11:45:36 | INFO | fairseq.tasks.translation | example hypothesis: so, i'm a, "the," the, "the," it's a, "the," the, and it's a, "the," the, "the," the, and i was a, "the, and i was a," the, "the," it's a, "the," the, "the, and i was a, and i'm a," the, "the," the, and i was a, "the," the first first first, "the first first first first first first first first first first first first first first first first first first first first first first first first first first first first first first first first first first first," the, "the," the, "the," the, "the," the, "the," the, "the, and i was a," the
2022-03-23 11:45:36 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 11:45:38 | INFO | fairseq.tasks.translation | example hypothesis: so, i think that's a lot of the world of the, and i think, in the world, and the world, and the world, and the world, the world, the world, the world, the world, and the
2022-03-23 11:45:38 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 11:45:38 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.784 | ppl 1763.27 | bleu 1.74 | wps 4417.1 | wpb 17862.2 | bsz 728.3 | num_updates 781 | best_bleu 1.74
2022-03-23 11:45:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 781 updates
2022-03-23 11:45:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:45:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:45:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 5 @ 781 updates, score 1.74) (writing took 1.7370399618521333 seconds)
2022-03-23 11:45:40 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-23 11:45:40 | INFO | train | epoch 005 | loss 9.822 | ppl 904.9 | wps 38448 | ups 1.53 | wpb 25153.6 | bsz 1020.6 | num_updates 781 | lr 9.7625e-05 | gnorm 1 | loss_scale 8 | train_wall 58 | gb_free 12.3 | wall 515
KL Stats: Epoch 5 Divergences: Uniform: 1.4944010483894232 Unigram: 0.3124869823325241
2022-03-23 11:45:40 | INFO | fairseq.trainer | begin training epoch 6
2022-03-23 11:45:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 11:45:48 | INFO | train_inner | epoch 006:     19 / 157 loss=9.694, ppl=828.17, wps=31269.5, ups=1.23, wpb=25377, bsz=1038.3, num_updates=800, lr=0.0001, gnorm=1.004, loss_scale=8, train_wall=37, gb_free=12.7, wall=522
2022-03-23 11:46:25 | INFO | train_inner | epoch 006:    119 / 157 loss=9.631, ppl=793.01, wps=66824.7, ups=2.64, wpb=25320.5, bsz=1021.9, num_updates=900, lr=0.0001125, gnorm=0.973, loss_scale=8, train_wall=38, gb_free=11.9, wall=560
2022-03-23 11:46:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 11:46:43 | INFO | fairseq.tasks.translation | example hypothesis: we're going to see in the world.
2022-03-23 11:46:43 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 11:46:47 | INFO | fairseq.tasks.translation | example hypothesis: this is the.
2022-03-23 11:46:47 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 11:46:51 | INFO | fairseq.tasks.translation | example hypothesis: we're going to be going to be two.
2022-03-23 11:46:51 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 11:46:56 | INFO | fairseq.tasks.translation | example hypothesis: and there's a lot of, there's a, there's, there's a lot of, and there's a lot of the, and there's, and there's, and there's, there's
2022-03-23 11:46:56 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 11:47:01 | INFO | fairseq.tasks.translation | example hypothesis: and it's what we're going to do that we're going to do it, and it's going to do it's going to do that we're going to do that we're going to do it.
2022-03-23 11:47:01 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 11:47:06 | INFO | fairseq.tasks.translation | example hypothesis: and this is a lot of the world, and in the world, and in the world, in the world, and in the world, and in the world, and in the world, and in the world, and in the world, and the world, and in the world, and the
2022-03-23 11:47:06 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 11:47:12 | INFO | fairseq.tasks.translation | example hypothesis: but if you're going to see, you're going to see, but they're going to be a lot of the, but they're going to be not going to be, but they're going to be, but they're going to be a lot of the, but they're going to be, but they're going to be, but they're not
2022-03-23 11:47:12 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 11:47:18 | INFO | fairseq.tasks.translation | example hypothesis: so, we're going to see, and we're going to see that we're going to see the world, and we can see the world, and we can see that we can see that we can see that we can see, and we can see the world, and we can see the world, and we can see that we can see that we can see that we can see that we can see the world.
2022-03-23 11:47:18 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 11:47:25 | INFO | fairseq.tasks.translation | example hypothesis: and if you know, "you know," you know, it's going to say, "it's going to say," it's going to say, "you know," you know, "you know," it's going to say, "it's going to say," you know, "it's going to say," it's going to say, "it's going to say," it's going to say, "it's going to say," it's going to say, "it's going to say," it's going to say, "it's going to say," it's going to say, "it's going to say," it's going to say, "it's going to say," it's going to say, "it's going to say," it's going to say, "it's going to say," "
2022-03-23 11:47:25 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 11:47:28 | INFO | fairseq.tasks.translation | example hypothesis: if we're going to be a lot of the world, we're going to see that we're going to be a lot of the world, and we're going to be, and we're going to see that we're going to be a lot of the world, that we're going to be, and we're going to be going to see that we're going to be a lot of the world, and we're going to be, and we're going to be going to see that we're going to be a lot of the way that we're going to be a, and that we're going to be going to see that we're going to see that we're going to see that we're going to be a lot of the world, that we're going to be going to be a lot of the world, that we're going to be, and we're going to see that we're going to see that we're going to see that we're going to see that we're going to see that we're going to see that we're going to
2022-03-23 11:47:28 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 11:47:28 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.542 | ppl 1491.45 | bleu 1.81 | wps 3679.1 | wpb 17862.2 | bsz 728.3 | num_updates 938 | best_bleu 1.81
2022-03-23 11:47:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 938 updates
2022-03-23 11:47:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:47:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:47:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 6 @ 938 updates, score 1.81) (writing took 1.7794610690325499 seconds)
2022-03-23 11:47:30 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-23 11:47:30 | INFO | train | epoch 006 | loss 9.642 | ppl 798.8 | wps 36075.6 | ups 1.43 | wpb 25153.6 | bsz 1020.6 | num_updates 938 | lr 0.00011725 | gnorm 0.974 | loss_scale 8 | train_wall 58 | gb_free 12.9 | wall 624
KL Stats: Epoch 6 Divergences: Uniform: 1.5659214210324794 Unigram: 0.3902733579671263
2022-03-23 11:47:30 | INFO | fairseq.trainer | begin training epoch 7
2022-03-23 11:47:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 11:47:53 | INFO | train_inner | epoch 007:     62 / 157 loss=9.542, ppl=745.31, wps=28667.4, ups=1.14, wpb=25195.5, bsz=1022.5, num_updates=1000, lr=0.000125, gnorm=0.864, loss_scale=8, train_wall=37, gb_free=11.6, wall=648
2022-03-23 11:48:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 11:48:33 | INFO | fairseq.tasks.translation | example hypothesis: we're going to see in the world.
2022-03-23 11:48:33 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 11:48:37 | INFO | fairseq.tasks.translation | example hypothesis: this is the most most of the world.
2022-03-23 11:48:37 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 11:48:41 | INFO | fairseq.tasks.translation | example hypothesis: so you're going to be a new new new new new new.
2022-03-23 11:48:41 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 11:48:46 | INFO | fairseq.tasks.translation | example hypothesis: and there's a lot of, and it's a lot of, there's a lot of, and it's a lot of, and it's going to be a lot of, there.
2022-03-23 11:48:46 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 11:48:51 | INFO | fairseq.tasks.translation | example hypothesis: it's a lot of, and it's going to do that we're going to do it, and it's going to do it.
2022-03-23 11:48:51 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 11:48:56 | INFO | fairseq.tasks.translation | example hypothesis: and this is a lot of people in the people in the people in the people, and it's a lot of people in the people in the people in the people.
2022-03-23 11:48:56 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 11:49:01 | INFO | fairseq.tasks.translation | example hypothesis: but if you're a lot of the, you're going to see, you're going to see, you're going to see, you're going to see, you're going to see, you're going to be a lot of the
2022-03-23 11:49:01 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 11:49:06 | INFO | fairseq.tasks.translation | example hypothesis: so, if we're going to see, we're going to get a lot of the world, and we're going to see the world, and we can see the world, we can see that we're going to see the world.
2022-03-23 11:49:06 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 11:49:13 | INFO | fairseq.tasks.translation | example hypothesis: and if you know, you know, "you know, you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know,"
2022-03-23 11:49:13 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 11:49:15 | INFO | fairseq.tasks.translation | example hypothesis: in fact, it's a lot of the, we've got a lot of the world, which is a lot of the world, which is a lot of the world, which is that we're going to be a lot of the world, which is, and we're going to be a lot of the world, which is that we're going to be a lot of the world, which is a lot of the
2022-03-23 11:49:15 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 11:49:15 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.404 | ppl 1355.21 | bleu 2.44 | wps 3894.3 | wpb 17862.2 | bsz 728.3 | num_updates 1095 | best_bleu 2.44
2022-03-23 11:49:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1095 updates
2022-03-23 11:49:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:49:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:49:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 7 @ 1095 updates, score 2.44) (writing took 1.8505868837237358 seconds)
2022-03-23 11:49:17 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-23 11:49:17 | INFO | train | epoch 007 | loss 9.47 | ppl 709.13 | wps 36807.9 | ups 1.46 | wpb 25153.6 | bsz 1020.6 | num_updates 1095 | lr 0.000136875 | gnorm 0.914 | loss_scale 8 | train_wall 58 | gb_free 12.6 | wall 731
KL Stats: Epoch 7 Divergences: Uniform: 1.6119645515387722 Unigram: 0.4508016038917678
2022-03-23 11:49:17 | INFO | fairseq.trainer | begin training epoch 8
2022-03-23 11:49:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 11:49:19 | INFO | train_inner | epoch 008:      5 / 157 loss=9.462, ppl=705.4, wps=29064.5, ups=1.16, wpb=25002.6, bsz=1042.3, num_updates=1100, lr=0.0001375, gnorm=0.913, loss_scale=8, train_wall=37, gb_free=12, wall=734
2022-03-23 11:49:57 | INFO | train_inner | epoch 008:    105 / 157 loss=9.286, ppl=624.32, wps=67177.8, ups=2.67, wpb=25137.5, bsz=1075.3, num_updates=1200, lr=0.00015, gnorm=0.894, loss_scale=8, train_wall=37, gb_free=12.3, wall=771
2022-03-23 11:50:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 11:50:20 | INFO | fairseq.tasks.translation | example hypothesis: we, we've got this in the in the in the, in the world.
2022-03-23 11:50:20 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 11:50:26 | INFO | fairseq.tasks.translation | example hypothesis: this is the most most of the most most most of the most most most of the most most most of the most of the most most most of the
2022-03-23 11:50:26 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 11:50:31 | INFO | fairseq.tasks.translation | example hypothesis: these are new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new new
2022-03-23 11:50:31 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 11:50:36 | INFO | fairseq.tasks.translation | example hypothesis: for example example example, there's a
2022-03-23 11:50:36 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 11:50:42 | INFO | fairseq.tasks.translation | example hypothesis: it's not that we're going to do it, and we're going to do it, and we're going to do that we're going to do it, and what we're going to do.
2022-03-23 11:50:42 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 11:50:47 | INFO | fairseq.tasks.translation | example hypothesis: and in fact, in the people, in the people in the people in the people in the people in the people in the people in the people in the people in the people in the people in the people in the people in the
2022-03-23 11:50:47 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 11:50:53 | INFO | fairseq.tasks.translation | example hypothesis: some of some some of some of some people who are, but it's not a, but if you're going to go to get a little little bit of, but they're not not, but it, but they're not, but it, but it, but it's not not, but it's a little little, but it's not not the
2022-03-23 11:50:53 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 11:50:59 | INFO | fairseq.tasks.translation | example hypothesis: so if we're going to take the
2022-03-23 11:50:59 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 11:51:05 | INFO | fairseq.tasks.translation | example hypothesis: one: "oh," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know,", "" "" it's, "it's a," it's, "it's a," it's a, "" it's, "it's a," it's, "it's a," you know, "we know," you know, "you know," you know, "it's a lot of me," you know, "you know,
2022-03-23 11:51:05 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 11:51:08 | INFO | fairseq.tasks.translation | example hypothesis: and it's the way that we're going to be a, if we're going to make a, we're going to make a, and then we're going to get a little little bit of that we're going to make a, and then we're going to get a, and then we're going to be going to make a lot of the
2022-03-23 11:51:08 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 11:51:08 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.25 | ppl 1218.14 | bleu 2.92 | wps 3461.3 | wpb 17862.2 | bsz 728.3 | num_updates 1252 | best_bleu 2.92
2022-03-23 11:51:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1252 updates
2022-03-23 11:51:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:51:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:51:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 8 @ 1252 updates, score 2.92) (writing took 1.7612580228596926 seconds)
2022-03-23 11:51:10 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-23 11:51:10 | INFO | train | epoch 008 | loss 9.321 | ppl 639.46 | wps 35042.1 | ups 1.39 | wpb 25153.6 | bsz 1020.6 | num_updates 1252 | lr 0.0001565 | gnorm 0.867 | loss_scale 8 | train_wall 58 | gb_free 11.7 | wall 844
KL Stats: Epoch 8 Divergences: Uniform: 1.6534415713394786 Unigram: 0.49685783985713355
2022-03-23 11:51:10 | INFO | fairseq.trainer | begin training epoch 9
2022-03-23 11:51:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 11:51:28 | INFO | train_inner | epoch 009:     48 / 157 loss=9.207, ppl=590.79, wps=28065.9, ups=1.09, wpb=25702.9, bsz=1011, num_updates=1300, lr=0.0001625, gnorm=0.782, loss_scale=8, train_wall=37, gb_free=12.6, wall=863
2022-03-23 11:52:06 | INFO | train_inner | epoch 009:    148 / 157 loss=9.217, ppl=595.3, wps=66482.9, ups=2.68, wpb=24780.2, bsz=958.6, num_updates=1400, lr=0.000175, gnorm=0.865, loss_scale=8, train_wall=37, gb_free=11.9, wall=900
2022-03-23 11:52:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-23 11:52:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 11:52:13 | INFO | fairseq.tasks.translation | example hypothesis: we did this.
2022-03-23 11:52:13 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 11:52:16 | INFO | fairseq.tasks.translation | example hypothesis: this is the.
2022-03-23 11:52:16 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 11:52:19 | INFO | fairseq.tasks.translation | example hypothesis: so, these are new new new new new new new new new new new.
2022-03-23 11:52:19 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 11:52:23 | INFO | fairseq.tasks.translation | example hypothesis: for example, there's a.
2022-03-23 11:52:23 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 11:52:27 | INFO | fairseq.tasks.translation | example hypothesis: it's not that we don't know, and we're going to know what's going to do.
2022-03-23 11:52:27 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 11:52:31 | INFO | fairseq.tasks.translation | example hypothesis: and in the
2022-03-23 11:52:31 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 11:52:36 | INFO | fairseq.tasks.translation | example hypothesis: so, first of some of the.
2022-03-23 11:52:36 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 11:52:40 | INFO | fairseq.tasks.translation | example hypothesis: so if we're going to use the
2022-03-23 11:52:40 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 11:52:45 | INFO | fairseq.tasks.translation | example hypothesis: yeah, one of the world, and it's a lot of people who said, "if we're going to say," well, "well," you know, "well," you know, "well," you know, "you know," you know, "well," well, "well," you know, "you know," you know, "well," well, "well," well, "well," well, "you know," well, "you know," you know, "well," well, "well," you know, "you know," you know, "well," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know," you know, "you know,"
2022-03-23 11:52:45 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 11:52:48 | INFO | fairseq.tasks.translation | example hypothesis: and in fact, it's still still still, and if we're going to be a lot of the, if we're going to do that we're going to make a lot of, and we're going to do that we're going to make a lot of the world, and we're going to be able to make a lot of the world, and we're going to be able to be able to be able to be able to make a lot of the world, that we're going to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to make a lot of the world that we have to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to make a
2022-03-23 11:52:48 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 11:52:48 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.035 | ppl 1049.09 | bleu 4.8 | wps 4684.6 | wpb 17862.2 | bsz 728.3 | num_updates 1408 | best_bleu 4.8
2022-03-23 11:52:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1408 updates
2022-03-23 11:52:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:52:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:52:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 9 @ 1408 updates, score 4.8) (writing took 1.8370960233733058 seconds)
2022-03-23 11:52:49 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-23 11:52:49 | INFO | train | epoch 009 | loss 9.173 | ppl 577.27 | wps 39267 | ups 1.56 | wpb 25135.4 | bsz 1009 | num_updates 1408 | lr 0.000176 | gnorm 0.825 | loss_scale 4 | train_wall 58 | gb_free 12.1 | wall 944
KL Stats: Epoch 9 Divergences: Uniform: 1.7023067576015178 Unigram: 0.5401155995188908
2022-03-23 11:52:50 | INFO | fairseq.trainer | begin training epoch 10
2022-03-23 11:52:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 11:53:25 | INFO | train_inner | epoch 010:     92 / 157 loss=9.108, ppl=551.78, wps=31706.1, ups=1.26, wpb=25104.2, bsz=1005, num_updates=1500, lr=0.0001875, gnorm=0.78, loss_scale=4, train_wall=38, gb_free=12.4, wall=979
2022-03-23 11:53:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 11:53:52 | INFO | fairseq.tasks.translation | example hypothesis: we did this.
2022-03-23 11:53:52 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 11:53:56 | INFO | fairseq.tasks.translation | example hypothesis: this is the most of you know, most of most of the most of the most most of the most most of you know.
2022-03-23 11:53:56 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 11:54:00 | INFO | fairseq.tasks.translation | example hypothesis: they're going to new new new new new new new new new new new new.
2022-03-23 11:54:00 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 11:54:04 | INFO | fairseq.tasks.translation | example hypothesis: so for example, there's a
2022-03-23 11:54:04 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 11:54:08 | INFO | fairseq.tasks.translation | example hypothesis: it's not that we don't just just just just just a few years, and what's going to do.
2022-03-23 11:54:08 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 11:54:12 | INFO | fairseq.tasks.translation | example hypothesis: and in the ma.
2022-03-23 11:54:12 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 11:54:17 | INFO | fairseq.tasks.translation | example hypothesis: first of some of some of you're going to go in the same, but if you don't have to get the, if you don't have to have the same energy, you don't need to have the energy, if you don't have the energy, and you need to have the energy, you don't have the energy, you don't have
2022-03-23 11:54:17 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 11:54:22 | INFO | fairseq.tasks.translation | example hypothesis: so if we're going to use the information that we can see this.
2022-03-23 11:54:22 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 11:54:27 | INFO | fairseq.tasks.translation | example hypothesis: rb: one of the reasons, and it's interesting, and it's very interesting to me, and then it's a long time that we've got to go back to the first time, and then if you're going to go back to the first time.
2022-03-23 11:54:27 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 11:54:29 | INFO | fairseq.tasks.translation | example hypothesis: in fact, it's always always always always always always a lot of the work, and when we have a lot of work, if you're going to have a lot of the system, we're going to have to have a lot of the system, and we're going to have a new system that we're going to have to have to be able to have to be able to have a lot of the system that we're going to have to be able to have a lot of the system that we've got to be able to have a lot of the system, and the system that we've got to be able to be able to create a new system that we have a lot of the system that we have a little bit of the system that we have a lot of the system that we have a little bit of the system, and the system, and the system that we've got to be able to have to be able to be able to have to be able to have to be able to be able to be able to have to be able to be able to
2022-03-23 11:54:29 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 11:54:29 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 9.839 | ppl 915.72 | bleu 7.49 | wps 4476.5 | wpb 17862.2 | bsz 728.3 | num_updates 1565 | best_bleu 7.49
2022-03-23 11:54:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1565 updates
2022-03-23 11:54:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:54:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:54:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 10 @ 1565 updates, score 7.49) (writing took 1.800500361714512 seconds)
2022-03-23 11:54:31 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-23 11:54:31 | INFO | train | epoch 010 | loss 8.985 | ppl 506.78 | wps 38910.9 | ups 1.55 | wpb 25153.6 | bsz 1020.6 | num_updates 1565 | lr 0.000195625 | gnorm 0.792 | loss_scale 4 | train_wall 58 | gb_free 11.9 | wall 1046
KL Stats: Epoch 10 Divergences: Uniform: 1.7414260211062316 Unigram: 0.5808956236518047
2022-03-23 11:54:31 | INFO | fairseq.trainer | begin training epoch 11
2022-03-23 11:54:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 11:54:45 | INFO | train_inner | epoch 011:     35 / 157 loss=8.936, ppl=489.81, wps=31176.3, ups=1.25, wpb=24855.9, bsz=1006.2, num_updates=1600, lr=0.0002, gnorm=0.819, loss_scale=4, train_wall=36, gb_free=11.5, wall=1059
2022-03-23 11:55:22 | INFO | train_inner | epoch 011:    135 / 157 loss=8.663, ppl=405.34, wps=67831.2, ups=2.66, wpb=25548.4, bsz=1066.4, num_updates=1700, lr=0.0002125, gnorm=0.765, loss_scale=4, train_wall=37, gb_free=11.5, wall=1097
2022-03-23 11:55:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 11:55:34 | INFO | fairseq.tasks.translation | example hypothesis: we had this pppp, in the end of the, on.
2022-03-23 11:55:34 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 11:55:38 | INFO | fairseq.tasks.translation | example hypothesis: this is the, and most of you know, most of most of the most of here.
2022-03-23 11:55:38 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 11:55:42 | INFO | fairseq.tasks.translation | example hypothesis: these are new new new, two new new new new new new new new.
2022-03-23 11:55:42 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 11:55:46 | INFO | fairseq.tasks.translation | example hypothesis: for example, there's a chinese chinese, where he's going to come with the, and it's going to be.
2022-03-23 11:55:46 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 11:55:50 | INFO | fairseq.tasks.translation | example hypothesis: it's not sure that we're not just just a few of his head, and what's going to understand.
2022-03-23 11:55:50 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 11:55:54 | INFO | fairseq.tasks.translation | example hypothesis: and in the mamamale of people who had been working for the number of the number of animals, and that's a number of.
2022-03-23 11:55:54 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 11:55:58 | INFO | fairseq.tasks.translation | example hypothesis: first of some of the, some of the, but they don't need to use the, but if they don't need the energy, they don't need the energy.
2022-03-23 11:55:58 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 11:56:02 | INFO | fairseq.tasks.translation | example hypothesis: so if we use information the information that we can take a piece of information, we can use the structure of the information, and the structure of the information.
2022-03-23 11:56:02 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 11:56:06 | INFO | fairseq.tasks.translation | example hypothesis: rb: one of the reasons, and it's interesting for me, "well," well, "well," well, "you know," well, "we've got a lot of women."
2022-03-23 11:56:06 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 11:56:08 | INFO | fairseq.tasks.translation | example hypothesis: unfortunately, it's still still the mother, and we have a lot of work that we had to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able.
2022-03-23 11:56:08 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 11:56:08 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 9.669 | ppl 814.04 | bleu 9.78 | wps 4884.7 | wpb 17862.2 | bsz 728.3 | num_updates 1722 | best_bleu 9.78
2022-03-23 11:56:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1722 updates
2022-03-23 11:56:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:56:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:56:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 11 @ 1722 updates, score 9.78) (writing took 1.7520534410141408 seconds)
2022-03-23 11:56:10 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-23 11:56:10 | INFO | train | epoch 011 | loss 8.8 | ppl 445.81 | wps 39968.6 | ups 1.59 | wpb 25153.6 | bsz 1020.6 | num_updates 1722 | lr 0.00021525 | gnorm 0.778 | loss_scale 4 | train_wall 58 | gb_free 12.2 | wall 1144
KL Stats: Epoch 11 Divergences: Uniform: 1.7872060342477722 Unigram: 0.6179594712629566
2022-03-23 11:56:10 | INFO | fairseq.trainer | begin training epoch 12
2022-03-23 11:56:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 11:56:39 | INFO | train_inner | epoch 012:     78 / 157 loss=8.726, ppl=423.5, wps=32349.4, ups=1.29, wpb=24994.5, bsz=978.4, num_updates=1800, lr=0.000225, gnorm=0.748, loss_scale=4, train_wall=37, gb_free=12.1, wall=1174
2022-03-23 11:57:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 11:57:13 | INFO | fairseq.tasks.translation | example hypothesis: we did this.
2022-03-23 11:57:13 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 11:57:17 | INFO | fairseq.tasks.translation | example hypothesis: and this is the right line of doha, most of the most most of most of the most most of the most.
2022-03-23 11:57:17 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 11:57:21 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to be new.
2022-03-23 11:57:21 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 11:57:26 | INFO | fairseq.tasks.translation | example hypothesis: for example, there's a french chinese chinese chinese chinese chinese, where they're going to be, and they're going to be able to be.
2022-03-23 11:57:26 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 11:57:30 | INFO | fairseq.tasks.translation | example hypothesis: it's really that we don't just just just just just a few of the computer on the head of his head, and what's going to understand what's going on on.
2022-03-23 11:57:30 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 11:57:35 | INFO | fairseq.tasks.translation | example hypothesis: and in the mama
2022-03-23 11:57:35 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 11:57:39 | INFO | fairseq.tasks.translation | example hypothesis: first of these are some of
2022-03-23 11:57:39 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 11:57:44 | INFO | fairseq.tasks.translation | example hypothesis: so if we use information, we can use this information, we can start with a kind of, we can start going to start with a huge form of the structure, and the structure of the structure of information, and the information that's all the structure of the structure of the structure, and all the structure of the structure of the structure of the information, and all the information, and the structure of the structure of the structure of the structure of the information, and
2022-03-23 11:57:44 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 11:57:48 | INFO | fairseq.tasks.translation | example hypothesis: rb: one of the reasons that it's interesting, and it's interesting for me to be working for women, "well," well, if you've got to say, "well, you're going to say," if you're going to say, "well," we're going to say, "if you're going to say," you're going to say, "well," well, "if you're going to say," well, "well," well, "if you're going to say," well, "
2022-03-23 11:57:48 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 11:57:51 | INFO | fairseq.tasks.translation | example hypothesis: unfortunately, it's still still still the mother of the invention, and a lot of work that we've got to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to see that
2022-03-23 11:57:51 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 11:57:51 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 9.463 | ppl 705.91 | bleu 11.52 | wps 4340.6 | wpb 17862.2 | bsz 728.3 | num_updates 1879 | best_bleu 11.52
2022-03-23 11:57:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1879 updates
2022-03-23 11:57:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:57:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:57:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 12 @ 1879 updates, score 11.52) (writing took 1.761746831703931 seconds)
2022-03-23 11:57:52 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-23 11:57:52 | INFO | train | epoch 012 | loss 8.601 | ppl 388.4 | wps 38457.6 | ups 1.53 | wpb 25153.6 | bsz 1020.6 | num_updates 1879 | lr 0.000234875 | gnorm 0.752 | loss_scale 4 | train_wall 58 | gb_free 12.3 | wall 1247
KL Stats: Epoch 12 Divergences: Uniform: 1.8346528884827003 Unigram: 0.6479154430098566
2022-03-23 11:57:53 | INFO | fairseq.trainer | begin training epoch 13
2022-03-23 11:57:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 11:58:01 | INFO | train_inner | epoch 013:     21 / 157 loss=8.501, ppl=362.39, wps=30916.6, ups=1.23, wpb=25100.1, bsz=1056.5, num_updates=1900, lr=0.0002375, gnorm=0.807, loss_scale=4, train_wall=37, gb_free=12, wall=1255
2022-03-23 11:58:38 | INFO | train_inner | epoch 013:    121 / 157 loss=8.469, ppl=354.31, wps=66973, ups=2.65, wpb=25287.4, bsz=1028.2, num_updates=2000, lr=0.00025, gnorm=0.785, loss_scale=4, train_wall=37, gb_free=11.7, wall=1293
2022-03-23 11:58:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 11:58:55 | INFO | fairseq.tasks.translation | example hypothesis: we did these pppk in the clinics.
2022-03-23 11:58:55 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 11:59:00 | INFO | fairseq.tasks.translation | example hypothesis: this is the line of doha, most of the most most of you know here.
2022-03-23 11:59:00 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 11:59:03 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to make new.
2022-03-23 11:59:03 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 11:59:07 | INFO | fairseq.tasks.translation | example hypothesis: for example, there's french chinese chinese food, where the legs will be.
2022-03-23 11:59:07 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 11:59:11 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we're not just just a couple of electrops on his head, and what's going to understand.
2022-03-23 11:59:11 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 11:59:15 | INFO | fairseq.tasks.translation | example hypothesis: and in the mamace of people like the responsibility, for the number of animals, and this is a number of animals that has been used to be in namiiiibia.
2022-03-23 11:59:15 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 11:59:19 | INFO | fairseq.tasks.translation | example hypothesis: first of those are some of the magic lines in the lines, but if you don't need to move it, you don't need your energy energy, and if you need your energy.
2022-03-23 11:59:19 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 11:59:23 | INFO | fairseq.tasks.translation | example hypothesis: so if we use information information information that we can start from this structure, we can start able to start with a big form of the structure, and the structure of the structure of information, and the whole structure.
2022-03-23 11:59:23 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 11:59:27 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's interesting and interesting for me, "oh, you know, you know," yeah, you know, "if you're going to say," you're going to say, "if you're going to say," well, you're going to say, "if you're going to say," well, "if you're going to say," well, you're going to have a long time. "
2022-03-23 11:59:27 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 11:59:29 | INFO | fairseq.tasks.translation | example hypothesis: and unfortunately, it's still the mother of the invention, and part of our work that we had to solve the airplane, and if we had to use it, we had to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able.
2022-03-23 11:59:29 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 11:59:29 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 9.305 | ppl 632.74 | bleu 13.55 | wps 4950.5 | wpb 17862.2 | bsz 728.3 | num_updates 2036 | best_bleu 13.55
2022-03-23 11:59:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2036 updates
2022-03-23 11:59:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:59:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 11:59:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 13 @ 2036 updates, score 13.55) (writing took 1.8145600850693882 seconds)
2022-03-23 11:59:30 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-23 11:59:30 | INFO | train | epoch 013 | loss 8.432 | ppl 345.39 | wps 40280.9 | ups 1.6 | wpb 25153.6 | bsz 1020.6 | num_updates 2036 | lr 0.0002545 | gnorm 0.788 | loss_scale 4 | train_wall 58 | gb_free 11.6 | wall 1345
KL Stats: Epoch 13 Divergences: Uniform: 1.883025684471605 Unigram: 0.6747792039551925
2022-03-23 11:59:31 | INFO | fairseq.trainer | begin training epoch 14
2022-03-23 11:59:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 11:59:55 | INFO | train_inner | epoch 014:     64 / 157 loss=8.341, ppl=324.3, wps=32632.2, ups=1.31, wpb=24965.5, bsz=985.9, num_updates=2100, lr=0.0002625, gnorm=0.735, loss_scale=4, train_wall=37, gb_free=12.3, wall=1370
2022-03-23 12:00:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:00:34 | INFO | fairseq.tasks.translation | example hypothesis: we made these ppppure in the clinics.
2022-03-23 12:00:34 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:00:38 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha ha ha ha, most of the most of you know.
2022-03-23 12:00:38 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:00:42 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new dins.
2022-03-23 12:00:42 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:00:46 | INFO | fairseq.tasks.translation | example hypothesis: for example, there's french chinese chinese chinese food, where the legs will be happy, and it's going to be.
2022-03-23 12:00:46 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:00:50 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we don't just get a few electrodes on his head and understand what all of the thoughts are in the mind.
2022-03-23 12:00:50 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:00:54 | INFO | fairseq.tasks.translation | example hypothesis: and in the mama, like the responsibility, people grew up for the responsibility of the number of animals, and that has become become a, in the country.
2022-03-23 12:00:54 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:00:58 | INFO | fairseq.tasks.translation | example hypothesis: first first of these are some of magnetic magnetic lines, but in the lines, but if you don't need it, and if you need your energy.
2022-03-23 12:00:58 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:01:02 | INFO | fairseq.tasks.translation | example hypothesis: so if we use information, the information comes from this reflection, we can start able to start with a traditional traditional, and all of the information, and the whole structure of all the information.
2022-03-23 12:01:02 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:01:04 | INFO | fairseq.tasks.translation | example hypothesis: th reasons, one of the reasons that it's interesting, and it's interesting for me to do this, "well, if we've got a lot of women, and then we're going to tell you that the."
2022-03-23 12:01:04 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:01:05 | INFO | fairseq.tasks.translation | example hypothesis: unfortunately, of course, is still the mother, the invention of the invention of the design, and one of our work, if we had to use a unique system, we had to see everything.
2022-03-23 12:01:05 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:01:05 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 9.185 | ppl 582 | bleu 15.22 | wps 5167 | wpb 17862.2 | bsz 728.3 | num_updates 2193 | best_bleu 15.22
2022-03-23 12:01:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2193 updates
2022-03-23 12:01:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:01:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:01:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 14 @ 2193 updates, score 15.22) (writing took 1.816547648049891 seconds)
2022-03-23 12:01:07 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-23 12:01:07 | INFO | train | epoch 014 | loss 8.238 | ppl 302.01 | wps 40822.1 | ups 1.62 | wpb 25153.6 | bsz 1020.6 | num_updates 2193 | lr 0.000274125 | gnorm 0.728 | loss_scale 4 | train_wall 58 | gb_free 11.9 | wall 1442
KL Stats: Epoch 14 Divergences: Uniform: 1.9401731350210532 Unigram: 0.6987133741672475
2022-03-23 12:01:07 | INFO | fairseq.trainer | begin training epoch 15
2022-03-23 12:01:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:01:10 | INFO | train_inner | epoch 015:      7 / 157 loss=8.115, ppl=277.17, wps=33844.7, ups=1.33, wpb=25541.8, bsz=1065.6, num_updates=2200, lr=0.000275, gnorm=0.688, loss_scale=4, train_wall=37, gb_free=12, wall=1445
2022-03-23 12:01:48 | INFO | train_inner | epoch 015:    107 / 157 loss=8.073, ppl=269.25, wps=67063.2, ups=2.67, wpb=25146.5, bsz=1064.7, num_updates=2300, lr=0.0002875, gnorm=0.751, loss_scale=4, train_wall=37, gb_free=12, wall=1483
2022-03-23 12:02:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:02:10 | INFO | fairseq.tasks.translation | example hypothesis: we made these pppills in the clinics.
2022-03-23 12:02:10 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:02:14 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha, the most of the most of you know here.
2022-03-23 12:02:14 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:02:18 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new.
2022-03-23 12:02:18 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:02:23 | INFO | fairseq.tasks.translation | example hypothesis: for example, there are french chinese chinese chinese food, where legs will be happy with legs, and they're going.
2022-03-23 12:02:23 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:02:27 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we're not just a couple of electrodes on his head and understand what all of the thoughts are.
2022-03-23 12:02:27 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:02:31 | INFO | fairseq.tasks.translation | example hypothesis: and in the mamamamamamamamated people who grew up for the responsibility, and that's a number of animals, and that's a.
2022-03-23 12:02:31 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:02:36 | INFO | fairseq.tasks.translation | example hypothesis: first, some of these are some of the magnetic magnetic lines in the lines, but in the way, if you're not going to move it, you don't need the power of the energy, you need to move the energy.
2022-03-23 12:02:36 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:02:40 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflect reflect reflection, we can start with a traditional face of traditional face, we can start able to start able to start with the shape of the structure, and the whole structure of the structure.
2022-03-23 12:02:40 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:02:44 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's interesting, and it's interesting for me to be here for ted.
2022-03-23 12:02:44 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:02:46 | INFO | fairseq.tasks.translation | example hypothesis: fortunately, the mother is still the invention of the invention, and a big design part of our work on our airplane, we had to solve the airplane, which was a unique result that we had to solve a unique result of the ground, or if we had to solve it.
2022-03-23 12:02:46 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:02:46 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.982 | ppl 505.57 | bleu 16.71 | wps 4532.3 | wpb 17862.2 | bsz 728.3 | num_updates 2350 | best_bleu 16.71
2022-03-23 12:02:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2350 updates
2022-03-23 12:02:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:02:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:02:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 15 @ 2350 updates, score 16.71) (writing took 1.8595339027233422 seconds)
2022-03-23 12:02:48 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-23 12:02:48 | INFO | train | epoch 015 | loss 8.089 | ppl 272.3 | wps 39039.1 | ups 1.55 | wpb 25153.6 | bsz 1020.6 | num_updates 2350 | lr 0.00029375 | gnorm 0.724 | loss_scale 4 | train_wall 58 | gb_free 11.9 | wall 1543
KL Stats: Epoch 15 Divergences: Uniform: 1.9877885066843048 Unigram: 0.7160581677542422
2022-03-23 12:02:49 | INFO | fairseq.trainer | begin training epoch 16
2022-03-23 12:02:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:03:08 | INFO | train_inner | epoch 016:     50 / 157 loss=8.017, ppl=259, wps=31818.2, ups=1.25, wpb=25427.2, bsz=928.4, num_updates=2400, lr=0.0003, gnorm=0.686, loss_scale=4, train_wall=37, gb_free=12.4, wall=1562
2022-03-23 12:03:45 | INFO | train_inner | epoch 016:    150 / 157 loss=7.988, ppl=253.89, wps=66253.1, ups=2.69, wpb=24656.8, bsz=1032.6, num_updates=2500, lr=0.0003125, gnorm=0.666, loss_scale=4, train_wall=37, gb_free=12.6, wall=1600
2022-03-23 12:03:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:03:52 | INFO | fairseq.tasks.translation | example hypothesis: we made these ppace in clinic clinic.
2022-03-23 12:03:52 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:03:55 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha.
2022-03-23 12:03:55 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:03:59 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new logic.
2022-03-23 12:03:59 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:04:03 | INFO | fairseq.tasks.translation | example hypothesis: for example, there's french chinese food, where happy legs will be.
2022-03-23 12:04:03 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:04:06 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we're not just going to understand a few electrodes on his head and understand what all of the thoughts are on his mind.
2022-03-23 12:04:06 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:04:10 | INFO | fairseq.tasks.translation | example hypothesis: and in the mamammals of responsibility, the number of animals grew up again, and this is the number of animals.
2022-03-23 12:04:10 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:04:13 | INFO | fairseq.tasks.translation | example hypothesis: first, some of the magnetic lines in the field, but the sucks don't move, if they don't need to move their energy, they don't need their energy, and they need their energy.
2022-03-23 12:04:13 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:04:17 | INFO | fairseq.tasks.translation | example hypothesis: so if we use information from this reflection, we can begin to start with a traditional face that can begin to begin to start with traditional face of the face of the face of the face of the face of the face, and that's the shape of the shape of the face of the shape of the shape of the shape of the information, and the information, and the information.
2022-03-23 12:04:17 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:04:21 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons it's interesting for me and measure for ted. "
2022-03-23 12:04:21 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:04:22 | INFO | fairseq.tasks.translation | example hypothesis: fortunately, the mother of the invention, and a big part of the work that we have to solve on our airplane.
2022-03-23 12:04:22 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:04:22 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.931 | ppl 487.93 | bleu 12.57 | wps 5454.3 | wpb 17862.2 | bsz 728.3 | num_updates 2507 | best_bleu 16.71
2022-03-23 12:04:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2507 updates
2022-03-23 12:04:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 12:04:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 12:04:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt (epoch 16 @ 2507 updates, score 12.57) (writing took 0.8064809925854206 seconds)
2022-03-23 12:04:23 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-23 12:04:23 | INFO | train | epoch 016 | loss 7.93 | ppl 243.8 | wps 41910.3 | ups 1.67 | wpb 25153.6 | bsz 1020.6 | num_updates 2507 | lr 0.000313375 | gnorm 0.684 | loss_scale 4 | train_wall 58 | gb_free 12.1 | wall 1637
KL Stats: Epoch 16 Divergences: Uniform: 2.0343519538070702 Unigram: 0.7367142337163968
2022-03-23 12:04:23 | INFO | fairseq.trainer | begin training epoch 17
2022-03-23 12:04:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:04:58 | INFO | train_inner | epoch 017:     93 / 157 loss=7.819, ppl=225.86, wps=34529.5, ups=1.36, wpb=25300.9, bsz=1053.6, num_updates=2600, lr=0.000325, gnorm=0.691, loss_scale=4, train_wall=37, gb_free=13.1, wall=1673
2022-03-23 12:05:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:05:26 | INFO | fairseq.tasks.translation | example hypothesis: we made these ppills in the clinic clinic in the clinic.
2022-03-23 12:05:26 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:05:30 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha doha that most of you know.
2022-03-23 12:05:30 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:05:35 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new gollolocks that create the two new fuels.
2022-03-23 12:05:35 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:05:39 | INFO | fairseq.tasks.translation | example hypothesis: for example, there are french chinese food food food food, where frog legs are going to be salt with sales and fat.
2022-03-23 12:05:39 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:05:43 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we don't just get a few electroelectrodes on his head, and understand what all of the thoughts are on the top of the way.
2022-03-23 12:05:43 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:05:48 | INFO | fairseq.tasks.translation | example hypothesis: and in the mammals like the responsibility for the wild life, the number of animals grew up again, and this is a basis of natural protection in namibia.
2022-03-23 12:05:48 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:05:52 | INFO | fairseq.tasks.translation | example hypothesis: first, some of the bb, magnetic magnetic field in the inside the inside, but the sulungs don't have energy, if you need your energy, and you need some of the sulungs, you need to do it.
2022-03-23 12:05:52 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:05:57 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection reflection, we can start with a traditional facial facial face of the face of the, and there's a whole form of information, and through the whole structure of information, and all the structure of the structure, and all the structure of the structure, and all the structure of the structure, and all the structure of the structure is going to fold.
2022-03-23 12:05:57 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:06:03 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's interesting, and measure it's interesting for me to be in tedwomen, "and then," well, in the best
2022-03-23 12:06:03 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:06:06 | INFO | fairseq.tasks.translation | example hypothesis: ludly, the mother is still the invention of invention, and a big part of the design work that we had to solve a unique result that we had to solve the problems that we had to solve all the problems that were connected to the ground of the ground -- and if you have to solve it was connected to a
2022-03-23 12:06:06 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:06:06 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.787 | ppl 441.71 | bleu 18.33 | wps 4134.6 | wpb 17862.2 | bsz 728.3 | num_updates 2664 | best_bleu 18.33
2022-03-23 12:06:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 2664 updates
2022-03-23 12:06:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:06:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:06:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 17 @ 2664 updates, score 18.33) (writing took 1.7942354143597186 seconds)
2022-03-23 12:06:07 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-23 12:06:07 | INFO | train | epoch 017 | loss 7.814 | ppl 225.05 | wps 37684.6 | ups 1.5 | wpb 25153.6 | bsz 1020.6 | num_updates 2664 | lr 0.000333 | gnorm 0.687 | loss_scale 4 | train_wall 58 | gb_free 11.8 | wall 1742
KL Stats: Epoch 17 Divergences: Uniform: 2.076945085100259 Unigram: 0.7513808991852449
2022-03-23 12:06:08 | INFO | fairseq.trainer | begin training epoch 18
2022-03-23 12:06:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:06:21 | INFO | train_inner | epoch 018:     36 / 157 loss=7.721, ppl=211.02, wps=30372.2, ups=1.2, wpb=25229.8, bsz=1000.4, num_updates=2700, lr=0.0003375, gnorm=0.676, loss_scale=4, train_wall=37, gb_free=12.5, wall=1756
2022-03-23 12:06:59 | INFO | train_inner | epoch 018:    136 / 157 loss=7.729, ppl=212.18, wps=66365.1, ups=2.67, wpb=24823.4, bsz=1023, num_updates=2800, lr=0.00035, gnorm=0.591, loss_scale=4, train_wall=37, gb_free=12.3, wall=1793
2022-03-23 12:07:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:07:10 | INFO | fairseq.tasks.translation | example hypothesis: we made these pills in the clinic clinic.
2022-03-23 12:07:10 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:07:14 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha, which is probably the most familiar here.
2022-03-23 12:07:14 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:07:18 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new golf locks.
2022-03-23 12:07:18 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:07:22 | INFO | fairseq.tasks.translation | example hypothesis: for example, there's french food food food, where frog legs will be filled with salce and fat.
2022-03-23 12:07:22 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:07:26 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we don't just get a couple of electroelectrodes on his head, and understand exactly what all of the thoughts are.
2022-03-23 12:07:26 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:07:30 | INFO | fairseq.tasks.translation | example hypothesis: and in the mammers like the responsibility for the wild, the number of wild animals grew back, and this is a foundation for the natural protection of conservation.
2022-03-23 12:07:30 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:07:34 | INFO | fairseq.tasks.translation | example hypothesis: first, some of them are blooding by magnetic field, but the sulouter, if you don't like it, you need your energy movements, you don't need your energy movements, and so the superconductor.
2022-03-23 12:07:34 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:07:38 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection of reflection, we can start with a traditional facial facial face, the big constructions of the face, and the shape of the information, and there are the information, which is the whole structure.
2022-03-23 12:07:38 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:07:43 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's interesting and measure it for me to be in tedwomen, "well, when we've been in the best time."
2022-03-23 12:07:43 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:07:44 | INFO | fairseq.tasks.translation | example hypothesis: fortunately, the mother is still the invention of invention, and a big part of the design work that we're in our airplane, was a result that we had to solve.
2022-03-23 12:07:44 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:07:44 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.608 | ppl 390.29 | bleu 20.78 | wps 4898 | wpb 17862.2 | bsz 728.3 | num_updates 2821 | best_bleu 20.78
2022-03-23 12:07:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 2821 updates
2022-03-23 12:07:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:07:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:07:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 18 @ 2821 updates, score 20.78) (writing took 1.8261058437637985 seconds)
2022-03-23 12:07:46 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-23 12:07:46 | INFO | train | epoch 018 | loss 7.668 | ppl 203.44 | wps 40200.5 | ups 1.6 | wpb 25153.6 | bsz 1020.6 | num_updates 2821 | lr 0.000352625 | gnorm 0.588 | loss_scale 4 | train_wall 58 | gb_free 11.8 | wall 1840
KL Stats: Epoch 18 Divergences: Uniform: 2.10864735020792 Unigram: 0.7654564756597678
2022-03-23 12:07:46 | INFO | fairseq.trainer | begin training epoch 19
2022-03-23 12:07:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:08:16 | INFO | train_inner | epoch 019:     79 / 157 loss=7.567, ppl=189.64, wps=33158.2, ups=1.29, wpb=25639, bsz=997.8, num_updates=2900, lr=0.0003625, gnorm=0.578, loss_scale=4, train_wall=37, gb_free=12.2, wall=1871
2022-03-23 12:08:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:08:49 | INFO | fairseq.tasks.translation | example hypothesis: we did these pills in the clinic clinic.
2022-03-23 12:08:49 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:08:53 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha that most know here.
2022-03-23 12:08:53 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:08:57 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new golf locks.
2022-03-23 12:08:57 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:09:01 | INFO | fairseq.tasks.translation | example hypothesis: for example, there are french chinese food food, where frog legs with salz and fat.
2022-03-23 12:09:01 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:09:05 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we don't just get a couple of electrodes on his head and understand exactly what all his thoughts are.
2022-03-23 12:09:05 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:09:08 | INFO | fairseq.tasks.translation | example hypothesis: and in the mammals like people's responsibility for the wild animals, the number of wild animals grew back again. and this is a foundation of conservation in nambia.
2022-03-23 12:09:08 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:09:12 | INFO | fairseq.tasks.translation | example hypothesis: first, some of the magnetic fields in the inner field, but the superconductor don't like that, if they need the energy, and so the superconductor of the altitude of magnetic field.
2022-03-23 12:09:12 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:09:17 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can begin to start with a traditional facial of the face and the shape of the information through the whole structure, which is the whole structure of this structure, and the whole structure of the structure that all the structure of these reflection and fold the structure.
2022-03-23 12:09:17 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:09:21 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons the high-interesting and measure for me here at tedwomen, is that... "
2022-03-23 12:09:21 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:09:23 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of the invention, and a great part of the design work on our airplane was a result that we had to solve the unique problems on the ground.
2022-03-23 12:09:23 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:09:23 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.597 | ppl 387.19 | bleu 21.68 | wps 4696.1 | wpb 17862.2 | bsz 728.3 | num_updates 2978 | best_bleu 21.68
2022-03-23 12:09:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 2978 updates
2022-03-23 12:09:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:09:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:09:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 19 @ 2978 updates, score 21.68) (writing took 1.7799358982592821 seconds)
2022-03-23 12:09:25 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-23 12:09:25 | INFO | train | epoch 019 | loss 7.546 | ppl 186.87 | wps 39606.9 | ups 1.57 | wpb 25153.6 | bsz 1020.6 | num_updates 2978 | lr 0.00037225 | gnorm 0.584 | loss_scale 4 | train_wall 58 | gb_free 12 | wall 1940
KL Stats: Epoch 19 Divergences: Uniform: 2.1316140071102927 Unigram: 0.7793668957524053
2022-03-23 12:09:26 | INFO | fairseq.trainer | begin training epoch 20
2022-03-23 12:09:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:09:34 | INFO | train_inner | epoch 020:     22 / 157 loss=7.516, ppl=183, wps=31794.7, ups=1.28, wpb=24793.5, bsz=1030.8, num_updates=3000, lr=0.000375, gnorm=0.56, loss_scale=4, train_wall=36, gb_free=12.8, wall=1949
2022-03-23 12:10:12 | INFO | train_inner | epoch 020:    122 / 157 loss=7.371, ppl=165.54, wps=67927.9, ups=2.63, wpb=25866.7, bsz=1014.2, num_updates=3100, lr=0.0003875, gnorm=0.51, loss_scale=4, train_wall=38, gb_free=11.8, wall=1987
2022-03-23 12:10:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:10:28 | INFO | fairseq.tasks.translation | example hypothesis: we did these sheep in the clinic.
2022-03-23 12:10:28 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:10:32 | INFO | fairseq.tasks.translation | example hypothesis: that's the skyline of doha that probably knows most here.
2022-03-23 12:10:32 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:10:36 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new goldicks.
2022-03-23 12:10:36 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:10:40 | INFO | fairseq.tasks.translation | example hypothesis: for example, there are french chinese food, where frog legs with salz and ppet.
2022-03-23 12:10:40 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:10:45 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we're not just going to bring some electrodes on his head and understand exactly what all his thoughts are on the track.
2022-03-23 12:10:45 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:10:49 | INFO | fairseq.tasks.translation | example hypothesis: and in the mammals, people like responsibility for the wild, the number of wild animals grew up again, and that's a foundation of conservation in namibia.
2022-03-23 12:10:49 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:10:53 | INFO | fairseq.tasks.translation | example hypothesis: first, some bloop of magnetic fields in the inside the inner, but the sulant doesn't like it, if you need your movements, and so the sulalarm disorders.
2022-03-23 12:10:53 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:10:57 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial can begin to start with a traditional facial, the big constructions of the face, and the basic shape of the information, and through the gulf, which is the entire portion and fold the whole structure and fold the whole structure.
2022-03-23 12:10:57 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:11:03 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's interesting and measured for me to be here at tedwomen, is that...
2022-03-23 12:11:03 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:11:05 | INFO | fairseq.tasks.translation | example hypothesis: luckness is still the mother of the invention of invention, and a big part of the design work that we're in our airplane, a result of it is that we had to solve.
2022-03-23 12:11:05 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:11:05 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.478 | ppl 356.49 | bleu 23.52 | wps 4478 | wpb 17862.2 | bsz 728.3 | num_updates 3135 | best_bleu 23.52
2022-03-23 12:11:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3135 updates
2022-03-23 12:11:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:11:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:11:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 20 @ 3135 updates, score 23.52) (writing took 1.7938950867392123 seconds)
2022-03-23 12:11:07 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-23 12:11:07 | INFO | train | epoch 020 | loss 7.431 | ppl 172.61 | wps 38973.7 | ups 1.55 | wpb 25153.6 | bsz 1020.6 | num_updates 3135 | lr 0.000391875 | gnorm 0.542 | loss_scale 4 | train_wall 58 | gb_free 12.3 | wall 2041
KL Stats: Epoch 20 Divergences: Uniform: 2.1528349163646334 Unigram: 0.787487776667926
2022-03-23 12:11:07 | INFO | fairseq.trainer | begin training epoch 21
2022-03-23 12:11:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:11:32 | INFO | train_inner | epoch 021:     65 / 157 loss=7.343, ppl=162.36, wps=31337.4, ups=1.26, wpb=24883, bsz=1097.7, num_updates=3200, lr=0.0004, gnorm=0.591, loss_scale=4, train_wall=36, gb_free=12, wall=2066
2022-03-23 12:12:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:12:10 | INFO | fairseq.tasks.translation | example hypothesis: we made these sheep in the clinic.
2022-03-23 12:12:10 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:12:14 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha who probably know most here.
2022-03-23 12:12:14 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:12:18 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new goldicks that create the two new pigments.
2022-03-23 12:12:18 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:12:22 | INFO | fairseq.tasks.translation | example hypothesis: for example, there are french chinese food, where frog legs are served with salz and pace.
2022-03-23 12:12:22 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:12:26 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we're not just going to bring some electrodes on his head and understand exactly what all his thoughts are on the track.
2022-03-23 12:12:26 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:12:30 | INFO | fairseq.tasks.translation | example hypothesis: and in the masteribia, people have been devoted for the wild animals, the number of wildlife animals grew again, and this is a foundation for the conservation protection in namibia.
2022-03-23 12:12:30 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:12:35 | INFO | fairseq.tasks.translation | example hypothesis: first of all, some of the magnetic field are caught in the inner lines, but the superconductor doesn't like, if you move, you don't like to move, you need to move, because your movements need to move, and so the superconductive disorders.
2022-03-23 12:12:35 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:12:39 | INFO | fairseq.tasks.translation | example hypothesis: so, if we use the information that comes from this reflection, we can start with a traditional facial facial, which is the big constructions of the face and the basic form, and the basic form of information, the whole structure and fold all a fold.
2022-03-23 12:12:39 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:12:43 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes you care about the men and tell you, "well, is that..."
2022-03-23 12:12:43 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:12:45 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention, and is a big part of the design work that we're going to see in our plane, was a result that we had to solve the unique problems that were interconnected to the ground -- all of a variable system that allows us to refrigered with a.
2022-03-23 12:12:45 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:12:45 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.39 | ppl 335.38 | bleu 24.57 | wps 4656.7 | wpb 17862.2 | bsz 728.3 | num_updates 3292 | best_bleu 24.57
2022-03-23 12:12:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 3292 updates
2022-03-23 12:12:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:12:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:12:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 21 @ 3292 updates, score 24.57) (writing took 1.867835951037705 seconds)
2022-03-23 12:12:47 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-23 12:12:47 | INFO | train | epoch 021 | loss 7.354 | ppl 163.57 | wps 39318.6 | ups 1.56 | wpb 25153.6 | bsz 1020.6 | num_updates 3292 | lr 0.0004115 | gnorm 0.539 | loss_scale 4 | train_wall 58 | gb_free 12.9 | wall 2142
KL Stats: Epoch 21 Divergences: Uniform: 2.165867532324606 Unigram: 0.7942691029083602
2022-03-23 12:12:47 | INFO | fairseq.trainer | begin training epoch 22
2022-03-23 12:12:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:12:51 | INFO | train_inner | epoch 022:      8 / 157 loss=7.439, ppl=173.47, wps=31346.9, ups=1.27, wpb=24765.2, bsz=946.6, num_updates=3300, lr=0.0004125, gnorm=0.52, loss_scale=4, train_wall=37, gb_free=12, wall=2145
2022-03-23 12:13:28 | INFO | train_inner | epoch 022:    108 / 157 loss=7.371, ppl=165.56, wps=66087.7, ups=2.68, wpb=24641.4, bsz=1004.1, num_updates=3400, lr=0.000425, gnorm=0.544, loss_scale=4, train_wall=37, gb_free=12, wall=2183
2022-03-23 12:13:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:13:50 | INFO | fairseq.tasks.translation | example hypothesis: we put these sheep in the clinic.
2022-03-23 12:13:50 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:13:54 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha who probably know most here.
2022-03-23 12:13:54 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:13:58 | INFO | fairseq.tasks.translation | example hypothesis: stars become new goldicks.
2022-03-23 12:13:58 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:14:01 | INFO | fairseq.tasks.translation | example hypothesis: for example, there's french chinese food, where frog legs are served with salz and ppepper.
2022-03-23 12:14:01 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:14:05 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we don't just get some electrodes on his head and understand what all his thoughts are on the track.
2022-03-23 12:14:05 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:14:09 | INFO | fairseq.tasks.translation | example hypothesis: and in the mathe people like the responsibility for the wild, the number of wild animals grew back. and that's a foundation of conservation.
2022-03-23 12:14:09 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:14:13 | INFO | fairseq.tasks.translation | example hypothesis: first of all, some of the magnet lines are caught in the inner, but the suconductor doesn't like if they're moving.
2022-03-23 12:14:13 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:14:16 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial.
2022-03-23 12:14:16 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:14:20 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it was very interesting and measured for me to be here at tedwomen, is that... "well."
2022-03-23 12:14:20 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:14:20 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention, and a big part of the design work that we're going to see in our airplane, was a result of the unique problems that were connected to the ground.
2022-03-23 12:14:20 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:14:20 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.409 | ppl 339.8 | bleu 21.8 | wps 5486.5 | wpb 17862.2 | bsz 728.3 | num_updates 3449 | best_bleu 24.57
2022-03-23 12:14:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 3449 updates
2022-03-23 12:14:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 12:14:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 12:14:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt (epoch 22 @ 3449 updates, score 21.8) (writing took 0.8164617023430765 seconds)
2022-03-23 12:14:21 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-23 12:14:21 | INFO | train | epoch 022 | loss 7.287 | ppl 156.19 | wps 42004.5 | ups 1.67 | wpb 25153.6 | bsz 1020.6 | num_updates 3449 | lr 0.000431125 | gnorm 0.512 | loss_scale 4 | train_wall 58 | gb_free 12.5 | wall 2236
KL Stats: Epoch 22 Divergences: Uniform: 2.1798166242115156 Unigram: 0.8008767611055289
2022-03-23 12:14:21 | INFO | fairseq.trainer | begin training epoch 23
2022-03-23 12:14:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:14:41 | INFO | train_inner | epoch 023:     51 / 157 loss=7.239, ppl=151.05, wps=34954.4, ups=1.37, wpb=25503.2, bsz=954.5, num_updates=3500, lr=0.0004375, gnorm=0.446, loss_scale=4, train_wall=37, gb_free=11.9, wall=2255
2022-03-23 12:15:18 | INFO | train_inner | epoch 023:    151 / 157 loss=7.091, ppl=136.32, wps=68085.5, ups=2.68, wpb=25389.8, bsz=1103.3, num_updates=3600, lr=0.00045, gnorm=0.488, loss_scale=4, train_wall=37, gb_free=11.9, wall=2293
2022-03-23 12:15:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:15:24 | INFO | fairseq.tasks.translation | example hypothesis: we put these sheet in the clinic.
2022-03-23 12:15:24 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:15:28 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha who probably know most here.
2022-03-23 12:15:28 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:15:32 | INFO | fairseq.tasks.translation | example hypothesis: stars become new goldicks that create the two new pigs.
2022-03-23 12:15:32 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:15:36 | INFO | fairseq.tasks.translation | example hypothesis: for example, there are french chinese food, where frog legs are served with salz and ppet.
2022-03-23 12:15:36 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:15:40 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we don't just get some electrodes on his head and understand exactly what all of their thoughts are on the track.
2022-03-23 12:15:40 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:15:44 | INFO | fairseq.tasks.translation | example hypothesis: and in the mastery, the number of wild animals grew back to the wild animals, and this is a foundation for conservation conservation in namibia.
2022-03-23 12:15:44 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:15:48 | INFO | fairseq.tasks.translation | example hypothesis: first, some of the magnetic field are caught in the inside, but the superconductor doesn't like that if they're moving, because their movements need movements, and so the superconducting disorder.
2022-03-23 12:15:48 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:15:53 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial, which is the big configurations of the face and the basic basic reform of the face and the basic basic basic reform of the face, and through the fundamental constructions of the fundamental constructions of the face.
2022-03-23 12:15:53 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:15:58 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that are very interesting and measured to me here at tedwomen, is that... well, at the dinner dinner dinner dinner dinner, it was the best dinner dinner dinner when somebody said, "
2022-03-23 12:15:58 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:16:00 | INFO | fairseq.tasks.translation | example hypothesis: fortunately, the mother of invention, is still the mother of the invention, and a big part of the design work that we're going to use in our airplane, or a result that we had to solve the unique problems that we had to solve the unique problems that were connected to the unique problems that we had to solve.
2022-03-23 12:16:00 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:16:00 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.356 | ppl 327.68 | bleu 25.4 | wps 4616.1 | wpb 17862.2 | bsz 728.3 | num_updates 3606 | best_bleu 25.4
2022-03-23 12:16:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 3606 updates
2022-03-23 12:16:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:16:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:16:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 23 @ 3606 updates, score 25.4) (writing took 1.8107166131958365 seconds)
2022-03-23 12:16:01 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-23 12:16:01 | INFO | train | epoch 023 | loss 7.193 | ppl 146.32 | wps 39356 | ups 1.56 | wpb 25153.6 | bsz 1020.6 | num_updates 3606 | lr 0.00045075 | gnorm 0.475 | loss_scale 4 | train_wall 58 | gb_free 12.8 | wall 2336
KL Stats: Epoch 23 Divergences: Uniform: 2.1845607465797765 Unigram: 0.8043448319436785
2022-03-23 12:16:02 | INFO | fairseq.trainer | begin training epoch 24
2022-03-23 12:16:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:16:37 | INFO | train_inner | epoch 024:     94 / 157 loss=7.198, ppl=146.86, wps=31529.9, ups=1.26, wpb=24931.7, bsz=1035.4, num_updates=3700, lr=0.0004625, gnorm=0.454, loss_scale=4, train_wall=37, gb_free=11.9, wall=2372
2022-03-23 12:17:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:17:04 | INFO | fairseq.tasks.translation | example hypothesis: we put these sheep into the clinic.
2022-03-23 12:17:04 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:17:08 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha who probably know most here.
2022-03-23 12:17:08 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:17:12 | INFO | fairseq.tasks.translation | example hypothesis: stars will create new gold locks.
2022-03-23 12:17:12 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:17:16 | INFO | fairseq.tasks.translation | example hypothesis: for example, there's french chinese food, where frog legs are served with salz and pitcase.
2022-03-23 12:17:16 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:17:20 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we're not just bringing some electrodes on his head and understand exactly what all his thoughts are on the track.
2022-03-23 12:17:20 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:17:24 | INFO | fairseq.tasks.translation | example hypothesis: and in the mature, as people took responsibility to the wild, the number of wildlife animals grew back, and that's a foundation for conservation in namibia.
2022-03-23 12:17:24 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:17:28 | INFO | fairseq.tasks.translation | example hypothesis: first, some of the magnet lines are caught in the inside, but the superconductor doesn't like if they move, because their movements need energy, and so the superconductor disorders.
2022-03-23 12:17:28 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:17:32 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflect reflection, we can start with a traditional face, which gives the big configurations of the face and the basic shape, and through the information that contains all the ports and all the ports.
2022-03-23 12:17:32 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:17:35 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and measured to me here at tedwomen, is that... tyes, it was the best dinner when someone said, "turn men on a table and say," if the revolution starts to support you. "
2022-03-23 12:17:35 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:17:37 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of the invention, and a big part of the design work that we're on our airplane was a result that we had to solve the unique problems that were connected to operate on the ground -- everything from a continually variable system and refrigerated and refrightening system that we can see in our airplane.
2022-03-23 12:17:37 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:17:37 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.231 | ppl 300.53 | bleu 27.53 | wps 5064.7 | wpb 17862.2 | bsz 728.3 | num_updates 3763 | best_bleu 27.53
2022-03-23 12:17:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 3763 updates
2022-03-23 12:17:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:17:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:17:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 24 @ 3763 updates, score 27.53) (writing took 1.864135772921145 seconds)
2022-03-23 12:17:39 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-23 12:17:39 | INFO | train | epoch 024 | loss 7.13 | ppl 140.06 | wps 40615.5 | ups 1.61 | wpb 25153.6 | bsz 1020.6 | num_updates 3763 | lr 0.000470375 | gnorm 0.438 | loss_scale 4 | train_wall 58 | gb_free 12.5 | wall 2433
KL Stats: Epoch 24 Divergences: Uniform: 2.1941086673645858 Unigram: 0.8111409764929578
2022-03-23 12:17:39 | INFO | fairseq.trainer | begin training epoch 25
2022-03-23 12:17:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:17:53 | INFO | train_inner | epoch 025:     37 / 157 loss=7.017, ppl=129.47, wps=33588.8, ups=1.32, wpb=25486.7, bsz=1056.2, num_updates=3800, lr=0.000475, gnorm=0.421, loss_scale=4, train_wall=37, gb_free=12.1, wall=2448
2022-03-23 12:18:31 | INFO | train_inner | epoch 025:    137 / 157 loss=7.114, ppl=138.52, wps=66671.8, ups=2.66, wpb=25037.1, bsz=988.5, num_updates=3900, lr=0.0004875, gnorm=0.461, loss_scale=4, train_wall=37, gb_free=12, wall=2485
2022-03-23 12:18:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:18:42 | INFO | fairseq.tasks.translation | example hypothesis: we put these piesters in the clinic.
2022-03-23 12:18:42 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:18:46 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha who probably know most here.
2022-03-23 12:18:46 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:18:50 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new golf locks.
2022-03-23 12:18:50 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:18:53 | INFO | fairseq.tasks.translation | example hypothesis: for example, there are french chinese food where frog legs are served with salz and p.
2022-03-23 12:18:53 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:18:57 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we don't just get some electrodes on his head and understand what all his thoughts are on the track.
2022-03-23 12:18:57 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:19:01 | INFO | fairseq.tasks.translation | example hypothesis: and in the mammals like people, the number of wildlife animals grew back, and that's a basis for conservation in namibia.
2022-03-23 12:19:01 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:19:05 | INFO | fairseq.tasks.translation | example hypothesis: first, some of the magnetic field lines are caught in the inner, but the superconductor doesn't like moving, because their movements need energy, and so the superconducting disorders.
2022-03-23 12:19:05 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:19:09 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial that gives the big configuration of the face and the basic shape of the face and the basic shape of the real face and the basic shape of the basic information that pulls the whole portion structure.
2022-03-23 12:19:09 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:19:13 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it was very interesting and measured to me here at tedwomen, is that... tyes, the dinner was already touched to you. "
2022-03-23 12:19:13 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:19:14 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention, and a great part of the design work that we're on our plane on the stest, was a result that we had to solve the unique problems that were connected to operate in the ground -- all of a continuous variable system that allows us to use the power of a.
2022-03-23 12:19:14 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:19:14 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.228 | ppl 299.81 | bleu 26.17 | wps 5130.3 | wpb 17862.2 | bsz 728.3 | num_updates 3920 | best_bleu 27.53
2022-03-23 12:19:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 3920 updates
2022-03-23 12:19:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 12:19:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 12:19:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt (epoch 25 @ 3920 updates, score 26.17) (writing took 0.819511063862592 seconds)
2022-03-23 12:19:15 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-23 12:19:15 | INFO | train | epoch 025 | loss 7.083 | ppl 135.62 | wps 41066.4 | ups 1.63 | wpb 25153.6 | bsz 1020.6 | num_updates 3920 | lr 0.00049 | gnorm 0.449 | loss_scale 4 | train_wall 58 | gb_free 12.8 | wall 2529
KL Stats: Epoch 25 Divergences: Uniform: 2.19772950326497 Unigram: 0.8120879080152927
2022-03-23 12:19:15 | INFO | fairseq.trainer | begin training epoch 26
2022-03-23 12:19:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:19:45 | INFO | train_inner | epoch 026:     80 / 157 loss=6.974, ppl=125.71, wps=34036.2, ups=1.34, wpb=25441.6, bsz=1009.2, num_updates=4000, lr=0.0005, gnorm=0.414, loss_scale=4, train_wall=37, gb_free=12.2, wall=2560
2022-03-23 12:20:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:20:18 | INFO | fairseq.tasks.translation | example hypothesis: we put these pie, in the clinic.
2022-03-23 12:20:18 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:20:22 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha, probably most of you know.
2022-03-23 12:20:22 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:20:26 | INFO | fairseq.tasks.translation | example hypothesis: stars will create new goldilocks that are two new pigments.
2022-03-23 12:20:26 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:20:30 | INFO | fairseq.tasks.translation | example hypothesis: for example, there are french chinese food, where frog legs are served with salce and ppepper.
2022-03-23 12:20:30 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:20:34 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we don't just get some electrodes on his head and understand exactly what all of its thoughts are on the track.
2022-03-23 12:20:34 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:20:38 | INFO | fairseq.tasks.translation | example hypothesis: and in the, like the people of responsibility for the wildlife, grew up the number of wildlife animals, and this is a foundation for conservation in namibia.
2022-03-23 12:20:38 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:20:42 | INFO | fairseq.tasks.translation | example hypothesis: first, some bundle of magnetic field lines are caught in the inside, but the superconductor doesn't like when they move, because their movements, and so the superconducting disorder.
2022-03-23 12:20:42 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:20:46 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial that gives the big constraints of the face and the basic form of information that refuses the whole porter structure and folds.
2022-03-23 12:20:46 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:20:51 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's very interesting and appropriate for me to be here at tedwomen, is that -- well, in the dinner, it was the best thing when someone said, "turn to the men on your table and tell them," if the revolution starts to support you, "the truth is that we've already been supported to you is that we've already started with you,", "well," well, "well, we've already been in this time, we've already started with you know," coal, "by the future,"
2022-03-23 12:20:51 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:20:53 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still the mother of invention, and a big part of the design work that we're on our airplane, was a result that we had to solve the unique problems that were connected to operate on the ground -- everything from a continuous variable system and a refrigerator system that allows us to a refrigerator machine to a refrigerator machine that we're in the most, to a particular way that we're using to the propellable to the propelled to the propellism of the propelled to the propelled to the propellism, or a mechanism, when we're either, when we're, it's, it is to be able to the.
2022-03-23 12:20:53 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:20:53 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.116 | ppl 277.39 | bleu 29.37 | wps 4675.2 | wpb 17862.2 | bsz 728.3 | num_updates 4077 | best_bleu 29.37
2022-03-23 12:20:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 4077 updates
2022-03-23 12:20:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:20:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:20:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 26 @ 4077 updates, score 29.37) (writing took 1.810313256457448 seconds)
2022-03-23 12:20:55 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-23 12:20:55 | INFO | train | epoch 026 | loss 7.022 | ppl 129.98 | wps 39495.1 | ups 1.57 | wpb 25153.6 | bsz 1020.6 | num_updates 4077 | lr 0.000495256 | gnorm 0.429 | loss_scale 4 | train_wall 58 | gb_free 12.4 | wall 2629
KL Stats: Epoch 26 Divergences: Uniform: 2.200342477471811 Unigram: 0.8144003738207635
2022-03-23 12:20:55 | INFO | fairseq.trainer | begin training epoch 27
2022-03-23 12:20:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:21:04 | INFO | train_inner | epoch 027:     23 / 157 loss=7.049, ppl=132.41, wps=31669.6, ups=1.27, wpb=24953.1, bsz=1099.1, num_updates=4100, lr=0.000493865, gnorm=0.431, loss_scale=4, train_wall=37, gb_free=12.9, wall=2639
2022-03-23 12:21:42 | INFO | train_inner | epoch 027:    123 / 157 loss=6.985, ppl=126.63, wps=66936.3, ups=2.67, wpb=25041.4, bsz=943.9, num_updates=4200, lr=0.00048795, gnorm=0.414, loss_scale=4, train_wall=37, gb_free=11.7, wall=2676
2022-03-23 12:21:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:21:58 | INFO | fairseq.tasks.translation | example hypothesis: we put these pieppers in the clinic.
2022-03-23 12:21:58 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:22:02 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha, which most of you know here.
2022-03-23 12:22:02 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:22:06 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new goldilocks that generate two new pigments.
2022-03-23 12:22:06 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:22:10 | INFO | fairseq.tasks.translation | example hypothesis: for example, there are french chinese food, where frog legs are served with salz and pitcase.
2022-03-23 12:22:10 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:22:14 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we don't just bring some electrodes on your head and understand exactly what all his thoughts are on the track.
2022-03-23 12:22:14 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:22:18 | INFO | fairseq.tasks.translation | example hypothesis: and in the mashes like people's responsibility for wildwildlife, the number of wildlife grew back. and that's a foundation for conservation in namibia.
2022-03-23 12:22:18 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:22:22 | INFO | fairseq.tasks.translation | example hypothesis: first, some of the magnetic fields are caught inside, but the superconductor doesn't like when they move, because their movements are using magnetic disorder, and so the superconductor disorders.
2022-03-23 12:22:22 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:22:26 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial that restores the great constructions of the face and the basic shape, and refolds it through the entire portion structure and all the folds.
2022-03-23 12:22:26 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:22:31 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's very interesting and measured to me here at tedwomen is that... tyes, when dinner was the best summary of the men in your table, and they say, "well, if the revolution starts to support you," the truth is that we've already been supporting you in the harbor, "
2022-03-23 12:22:31 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:22:33 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention, and a large part of the design work that we're on our airplane was a result that we had to solve the unique problems that were connected to the ground -- everything from a continuous variation for a refrigeration system that allows us to see, or a refrigerator system that drives us to the refrigerators of an aircraft.
2022-03-23 12:22:33 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:22:33 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.133 | ppl 280.73 | bleu 29.1 | wps 4713.8 | wpb 17862.2 | bsz 728.3 | num_updates 4234 | best_bleu 29.37
2022-03-23 12:22:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 4234 updates
2022-03-23 12:22:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 12:22:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 12:22:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt (epoch 27 @ 4234 updates, score 29.1) (writing took 0.8568348661065102 seconds)
2022-03-23 12:22:34 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-23 12:22:34 | INFO | train | epoch 027 | loss 6.969 | ppl 125.28 | wps 39957 | ups 1.59 | wpb 25153.6 | bsz 1020.6 | num_updates 4234 | lr 0.000485987 | gnorm 0.409 | loss_scale 4 | train_wall 58 | gb_free 12.1 | wall 2728
KL Stats: Epoch 27 Divergences: Uniform: 2.2053900153809503 Unigram: 0.8197667743791354
2022-03-23 12:22:34 | INFO | fairseq.trainer | begin training epoch 28
2022-03-23 12:22:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:22:59 | INFO | train_inner | epoch 028:     66 / 157 loss=6.989, ppl=127.04, wps=32279.3, ups=1.3, wpb=24892.1, bsz=1013.7, num_updates=4300, lr=0.000482243, gnorm=0.399, loss_scale=4, train_wall=37, gb_free=12.8, wall=2753
2022-03-23 12:23:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:23:36 | INFO | fairseq.tasks.translation | example hypothesis: we put these bleep in the clinic.
2022-03-23 12:23:36 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:23:40 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha, which probably most of you know here.
2022-03-23 12:23:40 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:23:44 | INFO | fairseq.tasks.translation | example hypothesis: stars will generate new goldilocks that are going to cross two new pigs.
2022-03-23 12:23:44 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:23:48 | INFO | fairseq.tasks.translation | example hypothesis: for example, there's french chinese food, where frog legs are served with salce and pills.
2022-03-23 12:23:48 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:23:52 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we don't just bring some electrodes on his head and understand exactly what all his thoughts are on the track.
2022-03-23 12:23:52 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:23:57 | INFO | fairseq.tasks.translation | example hypothesis: and in the, like people's responsibility for wildlife, the number of wildlife animals grew back. and that's a basis for conservation in namibia.
2022-03-23 12:23:57 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:24:01 | INFO | fairseq.tasks.translation | example hypothesis: first of all, some of the magnetic field lines are caught inside, but the superconductor doesn't like it, if you move, because your movements are using energy, and that's how the superconductor disorder disorders.
2022-03-23 12:24:01 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:24:05 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial, which is the great contexts of the face and restores the basic form, and through this information that pulls the whole porto structure and all the folds.
2022-03-23 12:24:05 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:24:09 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that make it highly interesting and measured to me here at tedwomen, is that -- tyes, when dinner has been the best summarized when someone said, "turn to the men on your table and say," if the revolution starts to you. "
2022-03-23 12:24:09 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:24:11 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention, and a big part of the design work that we're at our plane at the stumble, was a result that we had to solve the unique problems that were connected to operate on the ground -- everything, from a continual variables and a cooling system that allows us to use a refrigerator for the most, or the refrigeration of a car car car, or the way we had to be able to be able to use.
2022-03-23 12:24:11 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:24:11 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.063 | ppl 267.43 | bleu 30.34 | wps 4772.8 | wpb 17862.2 | bsz 728.3 | num_updates 4391 | best_bleu 30.34
2022-03-23 12:24:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 4391 updates
2022-03-23 12:24:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:24:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:24:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 28 @ 4391 updates, score 30.34) (writing took 1.8709187018685043 seconds)
2022-03-23 12:24:13 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-23 12:24:13 | INFO | train | epoch 028 | loss 6.93 | ppl 121.97 | wps 39854.9 | ups 1.58 | wpb 25153.6 | bsz 1020.6 | num_updates 4391 | lr 0.00047722 | gnorm 0.411 | loss_scale 4 | train_wall 58 | gb_free 11.9 | wall 2827
KL Stats: Epoch 28 Divergences: Uniform: 2.209191072993661 Unigram: 0.8202666161385352
2022-03-23 12:24:13 | INFO | fairseq.trainer | begin training epoch 29
2022-03-23 12:24:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:24:16 | INFO | train_inner | epoch 029:      9 / 157 loss=6.9, ppl=119.45, wps=32425.8, ups=1.29, wpb=25193, bsz=990.5, num_updates=4400, lr=0.000476731, gnorm=0.44, loss_scale=4, train_wall=37, gb_free=11.8, wall=2831
2022-03-23 12:24:54 | INFO | train_inner | epoch 029:    109 / 157 loss=6.903, ppl=119.7, wps=67020.8, ups=2.67, wpb=25138.3, bsz=1028.2, num_updates=4500, lr=0.000471405, gnorm=0.38, loss_scale=4, train_wall=37, gb_free=11.7, wall=2869
2022-03-23 12:25:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:25:16 | INFO | fairseq.tasks.translation | example hypothesis: we put these pieppers in the clinic.
2022-03-23 12:25:16 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:25:20 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha that probably most of you know here.
2022-03-23 12:25:20 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:25:24 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new goldilocks that are going to transcend two new pigs.
2022-03-23 12:25:24 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:25:28 | INFO | fairseq.tasks.translation | example hypothesis: for example, there are french chinese food, where frog legs are served with salt and ppeacekeepers.
2022-03-23 12:25:28 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:25:32 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we're not just going to put some electrodes on his head and understand exactly what all his thoughts are on the track.
2022-03-23 12:25:32 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:25:36 | INFO | fairseq.tasks.translation | example hypothesis: and in the corners like the people's responsibility for wildlife, the number of wildlife animals grew again, and that's become a basis for conservation in namibia.
2022-03-23 12:25:36 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:25:40 | INFO | fairseq.tasks.translation | example hypothesis: first, a couple of bundles of magnetic field are caught in the inside, but the superconductor doesn't like when they move, because their movements use energy, and so the superconducting disorders.
2022-03-23 12:25:40 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:25:44 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial that gives the big configurations of the face and the basic form, and through the information that pulls the whole portion structure and all the folds a fold.
2022-03-23 12:25:44 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:25:48 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it was very interesting and measured to me here at tedwomen is that... well, when dinner was tested best when somebody said, "turn to the men on your table and tell them," if the revolution starts to be here at tedwomen, we support you. "the truth is that we've already supported you for a long time,"
2022-03-23 12:25:48 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:25:51 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention, and a big part of the design work that we are on our airplane was a result that we had to solve the unique problems that were connected to operate on the ground -- everything, from a continuous variation and a refrigeration system that allows us to stop aircraft in the stumbling traffic, or if you can see the mechanism, or if you can see the air facility, or if you can see the mechanism, it's either the mechanism of a mechanism, it's all the way, it's all the way to the propelled by a mechanism, or the way that allows us to use the way, the way, it's all the way that we can see the propelled by a mechanism, until you can see the way that allows us to use the propellers to use the way, until you can see the way, or the propelled by a mechanism, until you can see the propelled by a mechanism,
2022-03-23 12:25:51 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:25:51 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.026 | ppl 260.63 | bleu 30.76 | wps 4706.3 | wpb 17862.2 | bsz 728.3 | num_updates 4548 | best_bleu 30.76
2022-03-23 12:25:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 4548 updates
2022-03-23 12:25:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:25:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:25:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 29 @ 4548 updates, score 30.76) (writing took 1.8581197741441429 seconds)
2022-03-23 12:25:52 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-23 12:25:52 | INFO | train | epoch 029 | loss 6.882 | ppl 117.94 | wps 39615.2 | ups 1.57 | wpb 25153.6 | bsz 1020.6 | num_updates 4548 | lr 0.00046891 | gnorm 0.389 | loss_scale 4 | train_wall 58 | gb_free 11.5 | wall 2927
KL Stats: Epoch 29 Divergences: Uniform: 2.20843013342443 Unigram: 0.8220560635885372
2022-03-23 12:25:53 | INFO | fairseq.trainer | begin training epoch 30
2022-03-23 12:25:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:26:12 | INFO | train_inner | epoch 030:     52 / 157 loss=6.887, ppl=118.36, wps=31936.3, ups=1.27, wpb=25075.3, bsz=967.8, num_updates=4600, lr=0.000466252, gnorm=0.376, loss_scale=4, train_wall=37, gb_free=12.1, wall=2947
2022-03-23 12:26:50 | INFO | train_inner | epoch 030:    152 / 157 loss=6.812, ppl=112.36, wps=67686.5, ups=2.67, wpb=25320.2, bsz=1072.2, num_updates=4700, lr=0.000461266, gnorm=0.348, loss_scale=4, train_wall=37, gb_free=12.9, wall=2985
2022-03-23 12:26:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:26:56 | INFO | fairseq.tasks.translation | example hypothesis: we put these pietters in the clinic.
2022-03-23 12:26:56 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:27:00 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha, probably most of you know here.
2022-03-23 12:27:00 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:27:04 | INFO | fairseq.tasks.translation | example hypothesis: stars will create new goldilocks that will be transcend two new pigs.
2022-03-23 12:27:04 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:27:08 | INFO | fairseq.tasks.translation | example hypothesis: for example, there's french chinese food, where frog legs are served with salt and pills.
2022-03-23 12:27:08 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:27:12 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we're not just putting some electrodes on his head and understanding exactly what all its thoughts are on the track.
2022-03-23 12:27:12 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:27:16 | INFO | fairseq.tasks.translation | example hypothesis: and in the, like people's responsibility for wildlife, the number of wildlife animals grew back again, and that's a basis for conservation in namibia.
2022-03-23 12:27:16 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:27:20 | INFO | fairseq.tasks.translation | example hypothesis: first, some of the magnetic field lines are captured in the inside, but the superconductor doesn't like when they move, because their movements are using energy, and so the superconducting disorder.
2022-03-23 12:27:20 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:27:24 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial that gives the big configurations of the face and the basic shape, and through that information that puts the whole portion structure and all folds.
2022-03-23 12:27:24 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:27:27 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and measured for me here at tedwomen is that... well, dinner was best summarized when someone said, "turn to the men of your table and tell you," if the revolution starts to support you, "the truth is that we've already been supporting you for a long time."
2022-03-23 12:27:27 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:27:30 | INFO | fairseq.tasks.translation | example hypothesis: fortunately, the mother of invention is still the mother of invention, and a big part of the design work that we're on our airplane on the most stumbling, was a result that we had to solve the unique problems that were connected to surgery -- everything from a continuous variation and a refrigeration system that allows us to use an aircraft, until you can use it to a specific vehicle, until you get the propherself to the propeller.
2022-03-23 12:27:30 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:27:30 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.992 | ppl 254.63 | bleu 30.9 | wps 4827 | wpb 17862.2 | bsz 728.3 | num_updates 4705 | best_bleu 30.9
2022-03-23 12:27:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 4705 updates
2022-03-23 12:27:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:27:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:27:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 30 @ 4705 updates, score 30.9) (writing took 1.8457323969341815 seconds)
2022-03-23 12:27:32 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-23 12:27:32 | INFO | train | epoch 030 | loss 6.833 | ppl 114.02 | wps 39724.6 | ups 1.58 | wpb 25153.6 | bsz 1020.6 | num_updates 4705 | lr 0.00046102 | gnorm 0.355 | loss_scale 4 | train_wall 58 | gb_free 11.5 | wall 3026
KL Stats: Epoch 30 Divergences: Uniform: 2.209368154823356 Unigram: 0.8240143665806219
2022-03-23 12:27:32 | INFO | fairseq.trainer | begin training epoch 31
2022-03-23 12:27:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:28:08 | INFO | train_inner | epoch 031:     95 / 157 loss=6.768, ppl=108.96, wps=32547.6, ups=1.27, wpb=25536.1, bsz=1010.6, num_updates=4800, lr=0.000456435, gnorm=0.377, loss_scale=4, train_wall=37, gb_free=11.7, wall=3063
2022-03-23 12:28:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:28:35 | INFO | fairseq.tasks.translation | example hypothesis: we put these bleep in the clinic.
2022-03-23 12:28:35 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:28:39 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha that probably most of you know here.
2022-03-23 12:28:39 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:28:43 | INFO | fairseq.tasks.translation | example hypothesis: stars will create new goldilocks that will transcend two new pigs.
2022-03-23 12:28:43 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:28:46 | INFO | fairseq.tasks.translation | example hypothesis: for example, there are french chinese food, where frog legs are served with salt and pills.
2022-03-23 12:28:46 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:28:50 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we don't just put some electrodes on his head and understand exactly what all his thoughts are on the track.
2022-03-23 12:28:50 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:28:55 | INFO | fairseq.tasks.translation | example hypothesis: and in the, as people took responsibility for wildlife, the number of wildlife animals grew back, and that's become a foundation for conservation in namibia.
2022-03-23 12:28:55 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:28:59 | INFO | fairseq.tasks.translation | example hypothesis: first, some bundles of magnetic field are captured inside, but the superconductor doesn't like it if you move, because your movements are using energy, and the superconducting disorder.
2022-03-23 12:28:59 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:29:03 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this mirror reflection, we can start with a traditional facial that gives the big constructions of the face and restores the basic shape, and put it through this information that pulls the whole portion and all the fits a fold.
2022-03-23 12:29:03 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:29:07 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that it's very interesting and appropriate for me to be here at tedwomen, is that -- well, when dinner was stripped up, it's one of the reasons that somebody said, "turn to men on your table and say," if the revolution begins, then we support you. "the truth is that we have already supported you for a long time with a proud of the future carrival, and then you've already started to download."
2022-03-23 12:29:07 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:29:09 | INFO | fairseq.tasks.translation | example hypothesis: fortunately, the mother of invention is still the mother of invention, and a big part of the design work that we're on our plane at the stumbling toes, which is a result that we had to solve the unique problems that were connected to operate on the ground -- everything from a continuously variable and a refrigerator system that allows us to use a traffic on the most of a car car car car vehicle, or if you see the most progressive, the most progressive, the way that we have to be able to use of a progressive, the propelled to the propellable to be able to be able to be able to use of a progressive.
2022-03-23 12:29:09 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:29:09 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.98 | ppl 252.52 | bleu 31.71 | wps 4744.5 | wpb 17862.2 | bsz 728.3 | num_updates 4862 | best_bleu 31.71
2022-03-23 12:29:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 4862 updates
2022-03-23 12:29:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:29:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:29:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 31 @ 4862 updates, score 31.71) (writing took 1.8126734420657158 seconds)
2022-03-23 12:29:11 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-23 12:29:11 | INFO | train | epoch 031 | loss 6.812 | ppl 112.36 | wps 39715.7 | ups 1.58 | wpb 25153.6 | bsz 1020.6 | num_updates 4862 | lr 0.000453516 | gnorm 0.366 | loss_scale 4 | train_wall 58 | gb_free 11.5 | wall 3126
KL Stats: Epoch 31 Divergences: Uniform: 2.2084795548708787 Unigram: 0.8262922726881938
2022-03-23 12:29:12 | INFO | fairseq.trainer | begin training epoch 32
2022-03-23 12:29:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:29:26 | INFO | train_inner | epoch 032:     38 / 157 loss=6.82, ppl=113.01, wps=31992.2, ups=1.28, wpb=24912.5, bsz=1051, num_updates=4900, lr=0.000451754, gnorm=0.336, loss_scale=4, train_wall=37, gb_free=12.5, wall=3141
2022-03-23 12:30:04 | INFO | train_inner | epoch 032:    138 / 157 loss=6.737, ppl=106.67, wps=67306, ups=2.66, wpb=25273.6, bsz=1027.3, num_updates=5000, lr=0.000447214, gnorm=0.372, loss_scale=4, train_wall=37, gb_free=12.5, wall=3178
2022-03-23 12:30:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:30:14 | INFO | fairseq.tasks.translation | example hypothesis: we put these bleep in the clinic.
2022-03-23 12:30:14 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:30:18 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha, probably most of you know here.
2022-03-23 12:30:18 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:30:22 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new goldilocks that are going to cross two new pigs.
2022-03-23 12:30:22 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:30:26 | INFO | fairseq.tasks.translation | example hypothesis: for example, there are french chinese food, where frog legs are served with salce and pills.
2022-03-23 12:30:26 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:30:30 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we don't just put some electrodes on his head and understand exactly what all his thoughts are on the track.
2022-03-23 12:30:30 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:30:34 | INFO | fairseq.tasks.translation | example hypothesis: and in the maggots like people's responsibility for wildlife, the number of wildlife animals grew back. and that's become a basis for conservation in namibia.
2022-03-23 12:30:34 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:30:38 | INFO | fairseq.tasks.translation | example hypothesis: first, some magnetic field lines are trapped inside, but the superconductor doesn't like moving, because their movements are using energy, and that's how the superconductor disorder.
2022-03-23 12:30:38 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:30:42 | INFO | fairseq.tasks.translation | example hypothesis: so when we use the information that comes from this reflection, we can start with a traditional face that gives the big configurations of the face and restores the basic shape, and puts it through the information that pulls the whole portural structure and all the folds.
2022-03-23 12:30:42 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:30:46 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and measured to me here at tedwomen is that... well, when dinner was dinner, it was best summarized when someone said, "turn to men on your table and tell you," if the revolution begins, we support you. "the truth is that we have already supported you."
2022-03-23 12:30:46 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:30:48 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still the mother of invention, and a large part of the design work that we're on our airplane was a result that we had to solve the unique problems that were connected to doing it on the ground -- everything from a continuous variables and a refrigerator system that allows us to use an aircraft in the clothes and gossip to a particular vehicle, or to the propelled, or the propelled, to the propelled, to a state of a state of a state, or the propelled.
2022-03-23 12:30:48 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:30:48 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.007 | ppl 257.22 | bleu 31.05 | wps 4854.8 | wpb 17862.2 | bsz 728.3 | num_updates 5019 | best_bleu 31.71
2022-03-23 12:30:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 5019 updates
2022-03-23 12:30:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 12:30:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 12:30:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt (epoch 32 @ 5019 updates, score 31.05) (writing took 0.8468463341705501 seconds)
2022-03-23 12:30:49 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-23 12:30:49 | INFO | train | epoch 032 | loss 6.772 | ppl 109.26 | wps 40420.2 | ups 1.61 | wpb 25153.6 | bsz 1020.6 | num_updates 5019 | lr 0.000446366 | gnorm 0.354 | loss_scale 4 | train_wall 58 | gb_free 12.6 | wall 3224
KL Stats: Epoch 32 Divergences: Uniform: 2.2111255825493226 Unigram: 0.831628165901058
2022-03-23 12:30:49 | INFO | fairseq.trainer | begin training epoch 33
2022-03-23 12:30:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:31:20 | INFO | train_inner | epoch 033:     81 / 157 loss=6.753, ppl=107.84, wps=32856.5, ups=1.31, wpb=25080.4, bsz=1119.7, num_updates=5100, lr=0.000442807, gnorm=0.357, loss_scale=4, train_wall=37, gb_free=12.1, wall=3255
2022-03-23 12:31:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:31:52 | INFO | fairseq.tasks.translation | example hypothesis: we put these bleep in the clinic.
2022-03-23 12:31:52 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:31:56 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha, which probably most of you know here.
2022-03-23 12:31:56 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:32:01 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new goldilocks, the two new pigs.
2022-03-23 12:32:01 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:32:05 | INFO | fairseq.tasks.translation | example hypothesis: for example, there are french chinese food, where frog legs are served with salt and ppeffer.
2022-03-23 12:32:05 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:32:09 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we're not just putting some electrodes on his head and understanding exactly what all his thoughts are on the track.
2022-03-23 12:32:09 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:32:13 | INFO | fairseq.tasks.translation | example hypothesis: and in the, as the people responsibility for the wildlife, the number of wildlife grew again, and that's become a basis for conservation in namibia.
2022-03-23 12:32:13 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:32:17 | INFO | fairseq.tasks.translation | example hypothesis: first of all, some bundle of magnetic field lines are trapped inside, but the superconductor doesn't like to move, because their movements are using energy, and so the superconductor.
2022-03-23 12:32:17 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:32:21 | INFO | fairseq.tasks.translation | example hypothesis: so, when we use the information that comes from this reflection, we can start with a traditional facial can that restore the big constructions of the face and the basic shape, and the basic form of information that the whole porch structure and all the fine fold.
2022-03-23 12:32:21 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:32:26 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and measured, for me here at tedwomen, is that -- well, in striking dinner, it was best summarized when someone said, "turn you to the men on your table and tell you," if the revolution starts to support you, 'the truth is that we've already been supporting you for a long time, "cake,"] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["] ["]
2022-03-23 12:32:26 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:32:28 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of the invention, and a big part of the design work that we're on our airplane at the stumbling toes was a result that we had to solve the unique problems that were connected to operate on the ground -- everything from a continuous variables and a refrigerator system that allows us to use an aircraft in the
2022-03-23 12:32:28 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:32:28 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.919 | ppl 242.05 | bleu 32.58 | wps 4586.4 | wpb 17862.2 | bsz 728.3 | num_updates 5176 | best_bleu 32.58
2022-03-23 12:32:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 5176 updates
2022-03-23 12:32:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:32:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:32:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 33 @ 5176 updates, score 32.58) (writing took 1.7617139369249344 seconds)
2022-03-23 12:32:30 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-23 12:32:30 | INFO | train | epoch 033 | loss 6.746 | ppl 107.34 | wps 39175.3 | ups 1.56 | wpb 25153.6 | bsz 1020.6 | num_updates 5176 | lr 0.000439545 | gnorm 0.352 | loss_scale 4 | train_wall 58 | gb_free 12.1 | wall 3324
KL Stats: Epoch 33 Divergences: Uniform: 2.2126240826320744 Unigram: 0.8295183371041656
2022-03-23 12:32:30 | INFO | fairseq.trainer | begin training epoch 34
2022-03-23 12:32:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:32:40 | INFO | train_inner | epoch 034:     24 / 157 loss=6.768, ppl=108.99, wps=31621.2, ups=1.26, wpb=25148, bsz=918.9, num_updates=5200, lr=0.000438529, gnorm=0.357, loss_scale=4, train_wall=37, gb_free=12, wall=3334
2022-03-23 12:33:17 | INFO | train_inner | epoch 034:    124 / 157 loss=6.714, ppl=104.97, wps=66813, ups=2.66, wpb=25139.6, bsz=1054.5, num_updates=5300, lr=0.000434372, gnorm=0.336, loss_scale=4, train_wall=37, gb_free=11.8, wall=3372
2022-03-23 12:33:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:33:33 | INFO | fairseq.tasks.translation | example hypothesis: we put these bleep up in the clinic.
2022-03-23 12:33:33 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:33:37 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha, which probably most of you know here.
2022-03-23 12:33:37 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:33:41 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new goldilocks that are going to cross two new pigs.
2022-03-23 12:33:41 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:33:45 | INFO | fairseq.tasks.translation | example hypothesis: for example, there's french chinese food, where frog legs are served with salz and pills.
2022-03-23 12:33:45 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:33:49 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we don't just put some electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:33:49 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:33:53 | INFO | fairseq.tasks.translation | example hypothesis: and in the, as people took responsibility for wildlife, the number of wild animals grew back again, and that's become a basis for conservation in namibia.
2022-03-23 12:33:53 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:33:57 | INFO | fairseq.tasks.translation | example hypothesis: first, some bundle of magnetic field lines are trapped inside, but the superconductor doesn't like it when they move, because their movements are using the superconductor.
2022-03-23 12:33:57 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:34:01 | INFO | fairseq.tasks.translation | example hypothesis: so if we use the information that comes from this reflection, we can start with a traditional facial that gives the big contexts of the face and the basic shape, and then recover it through the one that pulls the whole portion structure and all the folds.
2022-03-23 12:34:01 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:34:06 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it interesting and appropriate to me here at tedwomen is that, you know, we've already been supported for you for a long time at raspring, when someone said, "turn to the men on your table and say," if the revolution begins, we support you. "the truth is that we've already supported you."
2022-03-23 12:34:06 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:34:08 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still the mother of invention, and a big part of the design work that we're on our plane at the most staggering, which is a result that we had to solve the unique problems that were connected to operate on the ground -- everything from a continuously variable and a cooling system that allows us to use an aircraft in the go-go-till the propelled, until the propelled the propelled to a particular vehicle, until the propelled the propelled, or the propelled, until the propelled, until we see the steady, the steady, until we see the steady, or the steady of a mechanism of a mechanism, until the steady, until the steady, and the most specific vehicle is, until the propelled in the propelled in the propelled, and the propelled in the most, until the propelled down the most
2022-03-23 12:34:08 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:34:08 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.941 | ppl 245.76 | bleu 32.29 | wps 4727.3 | wpb 17862.2 | bsz 728.3 | num_updates 5333 | best_bleu 32.58
2022-03-23 12:34:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 5333 updates
2022-03-23 12:34:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 12:34:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 12:34:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt (epoch 34 @ 5333 updates, score 32.29) (writing took 0.7732015317305923 seconds)
2022-03-23 12:34:09 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-23 12:34:09 | INFO | train | epoch 034 | loss 6.717 | ppl 105.22 | wps 39797.9 | ups 1.58 | wpb 25153.6 | bsz 1020.6 | num_updates 5333 | lr 0.000433026 | gnorm 0.363 | loss_scale 4 | train_wall 58 | gb_free 11.8 | wall 3424
KL Stats: Epoch 34 Divergences: Uniform: 2.2137681276618046 Unigram: 0.8301541936493241
2022-03-23 12:34:09 | INFO | fairseq.trainer | begin training epoch 35
2022-03-23 12:34:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:34:35 | INFO | train_inner | epoch 035:     67 / 157 loss=6.669, ppl=101.78, wps=32470, ups=1.29, wpb=25102.4, bsz=954, num_updates=5400, lr=0.000430331, gnorm=0.368, loss_scale=4, train_wall=37, gb_free=12.9, wall=3449
2022-03-23 12:35:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:35:12 | INFO | fairseq.tasks.translation | example hypothesis: we put these bleep in the clinic.
2022-03-23 12:35:12 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:35:16 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha, which probably most people know here.
2022-03-23 12:35:16 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:35:20 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new goldilocks
2022-03-23 12:35:20 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:35:24 | INFO | fairseq.tasks.translation | example hypothesis: for example, there's french chinese food, where frog legs are served with salt and pepper.
2022-03-23 12:35:24 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:35:28 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we're not just putting some electrodes on his head and understanding exactly what all his thoughts are on the track.
2022-03-23 12:35:28 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:35:32 | INFO | fairseq.tasks.translation | example hypothesis: and in the, as people were taking responsibility for wildlife, the number of wildlife animals grew up again, and this has become a foundation for conservation in namibia.
2022-03-23 12:35:32 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:35:36 | INFO | fairseq.tasks.translation | example hypothesis: first, some bundles of magnetic field lines are captured inside, but the superconductor doesn't like it when they move, their movements are using energy, and so the superconducting disorder.
2022-03-23 12:35:36 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:35:40 | INFO | fairseq.tasks.translation | example hypothesis: so when we use the information that comes from this reflection, we can start with a traditional facial that restores the big configurations of the face and the basic shape, and enhances it through that information that refers the whole portion structure and all the folds.
2022-03-23 12:35:40 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:35:44 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it
2022-03-23 12:35:44 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:35:46 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still the mother of the invention, and a big part of the design work that we're on our airplane on the stumbling toes was a result that we had to solve the unique problems that were connected to operate on the ground -- everything from a continuous variables and a refrigerator system that allows us to use an aircraft in a particular way, until we're either driving the propelled to the ground, or when we're going to see the propelled to the air conditioning ground, until the mechanism of a mechanism, until we're going to the propelled to the air conditioning, until we're going to the air conditioning mechanism, until we're going to the air conditioning.
2022-03-23 12:35:46 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:35:46 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.909 | ppl 240.38 | bleu 32.24 | wps 4841.8 | wpb 17862.2 | bsz 728.3 | num_updates 5490 | best_bleu 32.58
2022-03-23 12:35:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 5490 updates
2022-03-23 12:35:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 12:35:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt
2022-03-23 12:35:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_last.pt (epoch 35 @ 5490 updates, score 32.24) (writing took 0.8057567519135773 seconds)
2022-03-23 12:35:47 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-23 12:35:47 | INFO | train | epoch 035 | loss 6.684 | ppl 102.81 | wps 40245.8 | ups 1.6 | wpb 25153.6 | bsz 1020.6 | num_updates 5490 | lr 0.00042679 | gnorm 0.323 | loss_scale 4 | train_wall 58 | gb_free 11.5 | wall 3522
KL Stats: Epoch 35 Divergences: Uniform: 2.212460862603227 Unigram: 0.8340603259166744
2022-03-23 12:35:47 | INFO | fairseq.trainer | begin training epoch 36
2022-03-23 12:35:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 12:35:52 | INFO | train_inner | epoch 036:     10 / 157 loss=6.756, ppl=108.07, wps=32343.7, ups=1.3, wpb=24921.2, bsz=1053.9, num_updates=5500, lr=0.000426401, gnorm=0.312, loss_scale=4, train_wall=37, gb_free=12.8, wall=3526
2022-03-23 12:36:29 | INFO | train_inner | epoch 036:    110 / 157 loss=6.659, ppl=101.06, wps=66924.3, ups=2.65, wpb=25297.2, bsz=1042, num_updates=5600, lr=0.000422577, gnorm=0.345, loss_scale=4, train_wall=37, gb_free=12.9, wall=3564
2022-03-23 12:36:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 12:36:51 | INFO | fairseq.tasks.translation | example hypothesis: we put these piepses in the clinic.
2022-03-23 12:36:51 | INFO | fairseq.tasks.translation | example reference: we put the beeper in the clinic.
2022-03-23 12:36:55 | INFO | fairseq.tasks.translation | example hypothesis: this is the skyline of doha that probably most of you know here.
2022-03-23 12:36:55 | INFO | fairseq.tasks.translation | example reference: this is probably the skyline that most of you know about doha.
2022-03-23 12:36:59 | INFO | fairseq.tasks.translation | example hypothesis: stars are going to create new goldilocks that are going to transcend two new pigs.
2022-03-23 12:36:59 | INFO | fairseq.tasks.translation | example reference: stars will create the goldilocks conditions for crossing two new thresholds.
2022-03-23 12:37:03 | INFO | fairseq.tasks.translation | example hypothesis: for example, there's french chinese food, where frog legs are served with salt and pepper.
2022-03-23 12:37:03 | INFO | fairseq.tasks.translation | example reference: for example, there is french chinese food, where they serve salt and pepper frog legs.
2022-03-23 12:37:07 | INFO | fairseq.tasks.translation | example hypothesis: it's clear that we're not just putting some electrodes on his head and understanding exactly what all of his thoughts are on the track.
2022-03-23 12:37:07 | INFO | fairseq.tasks.translation | example reference: now, clearly we're not going to put a couple of electrodes on his head and understand exactly what all of his thoughts are on the track.
2022-03-23 12:37:11 | INFO | fairseq.tasks.translation | example hypothesis: and in the stomach as people took responsibility for wildlife, the number of wildlife grew again, and that's become a basis for conservation in namibia.
2022-03-23 12:37:11 | INFO | fairseq.tasks.translation | example reference: and thus, as people started feeling ownership over wildlife, wildlife numbers started coming back, and that's actually becoming a foundation for conservation in namibia.
2022-03-23 12:37:16 | INFO | fairseq.tasks.translation | example hypothesis: first of all, some bundles of magnetic field are captured inside, but the superconductor doesn't like it when they move, because their movements are consuming energy, and that's how the superconductor disrupts.
2022-03-23 12:37:16 | INFO | fairseq.tasks.translation | example reference: well, first there are strands of magnetic field left inside, but now the superconductor doesn't like them moving around, because their movements dissipate energy, which breaks the superconductivity state.
2022-03-23 12:37:20 | INFO | fairseq.tasks.translation | example hypothesis: so when we use the information that comes from this reflection, we can start with a traditional facial bar that restores the big constructions of the face and the basic shape, and it restores the entire por-structure and all the folds.
2022-03-23 12:37:20 | INFO | fairseq.tasks.translation | example reference: so, if we use information that comes off of this specular reflection, we can go from a traditional face scan that might have the gross contours of the face and the basic shape, and augment it with information that puts in all of that skin pore structure and fine wrinkles.
2022-03-23 12:37:24 | INFO | fairseq.tasks.translation | example hypothesis: th: one of the reasons that makes it very interesting and measured to me here at tedwomen is that -- well, when dinner was striking, it became best summarized when someone said, "turn to the men on your desk and tell them," if the revolution begins, then we support you. "" the truth is that we've already supported you for a long time. "
2022-03-23 12:37:24 | INFO | fairseq.tasks.translation | example reference: th: one of this things that's exciting and appropriate for me to be here at tedwomen is that, well, i think it was summed up best last night at dinner when someone said, "turn to the man at your table and tell them, 'when the revolution starts, we've got your back.'" the truth is, women, you've had our back on this issue for a very long time, starting with rachel carson's "silent spring" to theo colborn's "our stolen future" to sandra steingraber's books "living downstream" and "having faith."
2022-03-23 12:37:27 | INFO | fairseq.tasks.translation | example hypothesis: luckily, the mother of invention is still a mother of the invention, and a big part of the design work that we're on our plane is a result that we had to solve the unique problems that were connected to doing it on the ground -- everything, from a continuous variables, and a refrigerator system that allows us to use a stop-go-of-the-shelf machine in the aircraft, until you can see it, or if you're going to operate it, or if you can see it, it's a mechanism, it's going to the ground, or if you can see it's a mechanism, either, or if you can see it's going to be connected to a mechanism, or if you can see it's going to a mechanism, or if you can see it's going to the basement, if you can see it's going to the basement, or if you're going to be able to be able to be able to be able to be able to the basement,
2022-03-23 12:37:27 | INFO | fairseq.tasks.translation | example reference: fortunately, necessity remains the mother of invention, and a lot of the design work that we're the most proud of with the aircraft came out of solving the unique problems of operating it on the ground -- everything from a continuously-variable transmission and liquid-based cooling system that allows us to use an aircraft engine in stop-and-go traffic, to a custom-designed gearbox that powers either the propeller when you're flying or the wheels on the ground, to the automated wing-folding mechanism that we'll see in a moment, to crash safety features.
2022-03-23 12:37:27 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.923 | ppl 242.75 | bleu 32.58 | wps 4607.6 | wpb 17862.2 | bsz 728.3 | num_updates 5647 | best_bleu 32.58
2022-03-23 12:37:27 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-23 12:37:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 5647 updates
2022-03-23 12:37:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:37:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt
2022-03-23 12:37:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/iwslt14_de_en_dropout_0.3_jelinek_0.025_0.325_0.65_#1/checkpoint_best.pt (epoch 36 @ 5647 updates, score 32.58) (writing took 1.9058899339288473 seconds)
2022-03-23 12:37:29 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-23 12:37:29 | INFO | train | epoch 036 | loss 6.675 | ppl 102.2 | wps 38769.2 | ups 1.54 | wpb 25153.6 | bsz 1020.6 | num_updates 5647 | lr 0.000420815 | gnorm 0.357 | loss_scale 4 | train_wall 58 | gb_free 12 | wall 3624
2022-03-23 12:37:29 | INFO | fairseq_cli.train | done training in 3623.2 seconds
