Sender: LSF System <lsfadmin@eu-g3-069>
Subject: Job 207014395: <w103_size_0.25_fp16_label_smoothing_0.1_#4> in cluster <euler> Done

Job <w103_size_0.25_fp16_label_smoothing_0.1_#4> was submitted from host <eu-login-19> by user <andriusb> in cluster <euler> at Thu Mar  3 10:52:49 2022
Job was executed on host(s) <eu-g3-069>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Thu Mar  3 10:53:19 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Mar  3 10:53:19 2022
Terminated at Fri Mar  4 22:57:52 2022
Results reported at Fri Mar  4 22:57:52 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --seed 1321614 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   129815.12 sec.
    Max Memory :                                 8514 MB
    Average Memory :                             3808.85 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11486.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   129872 sec.
    Turnaround time :                            129903 sec.

The output (if any) follows:

2022-03-03 10:53:34 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321614, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321614, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-03 10:53:34 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-03 10:53:38 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-03 10:53:38 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-03 10:53:38 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-03 10:53:38 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-03 10:53:38 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-03 10:53:38 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-03 10:53:38 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-03 10:53:40 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-03 10:53:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 10:53:40 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-03 10:53:40 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 10:53:40 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-03 10:53:40 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-03 10:53:40 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 10:53:40 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 10:53:40 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-03 10:53:41 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-03 10:53:41 | INFO | fairseq.trainer | begin training epoch 1
2022-03-03 10:53:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 10:53:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-03 10:53:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 10:53:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 10:53:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 10:54:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 10:58:22 | INFO | train_inner | epoch 001:    105 / 393 loss=17.137, nll_loss=16.941, ppl=125784, wps=25485.9, ups=0.39, wpb=65530.2, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.295, loss_scale=4, train_wall=275, gb_free=12.3, wall=282
2022-03-03 11:02:39 | INFO | train_inner | epoch 001:    205 / 393 loss=14.908, nll_loss=14.469, ppl=22677.3, wps=25460.3, ups=0.39, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.493, loss_scale=4, train_wall=253, gb_free=12.3, wall=539
2022-03-03 11:06:56 | INFO | train_inner | epoch 001:    305 / 393 loss=13.023, nll_loss=12.346, ppl=5207.52, wps=25482.7, ups=0.39, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.983, loss_scale=4, train_wall=252, gb_free=12.3, wall=796
2022-03-03 11:10:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 11:10:45 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.406 | nll_loss 10.427 | ppl 1376.69 | wps 66853.2 | wpb 2034.1 | bsz 4 | num_updates 388
2022-03-03 11:10:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-03 11:10:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 11:10:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 11:10:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 1 @ 388 updates, score 11.406) (writing took 8.97114583896473 seconds)
2022-03-03 11:10:54 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-03 11:10:54 | INFO | train | epoch 001 | loss 14.288 | nll_loss 13.748 | ppl 13758.2 | wps 25167.9 | ups 0.38 | wpb 65460.6 | bsz 127.9 | num_updates 388 | lr 4.85903e-05 | gnorm 1.614 | loss_scale 4 | train_wall 1001 | gb_free 12.3 | wall 1034
2022-03-03 11:10:54 | INFO | fairseq.trainer | begin training epoch 2
2022-03-03 11:10:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 11:11:25 | INFO | train_inner | epoch 002:     12 / 393 loss=11.725, nll_loss=10.824, ppl=1813.07, wps=24313.4, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.544, loss_scale=4, train_wall=251, gb_free=12.3, wall=1065
2022-03-03 11:15:42 | INFO | train_inner | epoch 002:    112 / 393 loss=11.21, nll_loss=10.177, ppl=1157.52, wps=25477, ups=0.39, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.463, loss_scale=4, train_wall=252, gb_free=12.3, wall=1322
2022-03-03 11:19:59 | INFO | train_inner | epoch 002:    212 / 393 loss=10.911, nll_loss=9.818, ppl=902.61, wps=25479.9, ups=0.39, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.534, loss_scale=8, train_wall=252, gb_free=12.3, wall=1579
2022-03-03 11:24:16 | INFO | train_inner | epoch 002:    312 / 393 loss=10.666, nll_loss=9.537, ppl=742.86, wps=25486.8, ups=0.39, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.56, loss_scale=8, train_wall=252, gb_free=12.3, wall=1836
2022-03-03 11:27:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 11:27:47 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.329 | nll_loss 9.154 | ppl 569.59 | wps 66813.5 | wpb 2034.1 | bsz 4 | num_updates 781 | best_loss 10.329
2022-03-03 11:27:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 781 updates
2022-03-03 11:27:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 11:27:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 11:27:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 2 @ 781 updates, score 10.329) (writing took 9.012080715037882 seconds)
2022-03-03 11:27:56 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-03 11:27:56 | INFO | train | epoch 002 | loss 10.848 | nll_loss 9.751 | ppl 861.81 | wps 25170.6 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 0.544 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 2056
2022-03-03 11:27:56 | INFO | fairseq.trainer | begin training epoch 3
2022-03-03 11:27:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 11:28:45 | INFO | train_inner | epoch 003:     19 / 393 loss=10.445, nll_loss=9.286, ppl=624.11, wps=24298.1, ups=0.37, wpb=65243.5, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.688, loss_scale=8, train_wall=251, gb_free=12.3, wall=2105
2022-03-03 11:33:02 | INFO | train_inner | epoch 003:    119 / 393 loss=10.235, nll_loss=9.047, ppl=528.79, wps=25475.5, ups=0.39, wpb=65530.9, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.744, loss_scale=8, train_wall=252, gb_free=12.3, wall=2362
2022-03-03 11:37:19 | INFO | train_inner | epoch 003:    219 / 393 loss=10.076, nll_loss=8.865, ppl=466.3, wps=25478, ups=0.39, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.789, loss_scale=8, train_wall=252, gb_free=12.3, wall=2619
2022-03-03 11:41:37 | INFO | train_inner | epoch 003:    319 / 393 loss=9.913, nll_loss=8.679, ppl=409.92, wps=25473.2, ups=0.39, wpb=65535.4, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.805, loss_scale=16, train_wall=252, gb_free=12.3, wall=2877
2022-03-03 11:44:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 11:44:49 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.659 | nll_loss 8.373 | ppl 331.62 | wps 66420.6 | wpb 2034.1 | bsz 4 | num_updates 1174 | best_loss 9.659
2022-03-03 11:44:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1174 updates
2022-03-03 11:44:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 11:44:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 11:44:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 3 @ 1174 updates, score 9.659) (writing took 8.952585629653186 seconds)
2022-03-03 11:44:58 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-03 11:44:58 | INFO | train | epoch 003 | loss 10.034 | nll_loss 8.817 | ppl 451.12 | wps 25167.8 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 1174 | lr 0.000146821 | gnorm 0.796 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 3078
2022-03-03 11:44:58 | INFO | fairseq.trainer | begin training epoch 4
2022-03-03 11:44:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 11:46:05 | INFO | train_inner | epoch 004:     26 / 393 loss=9.753, nll_loss=8.497, ppl=361.39, wps=24297.8, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.824, loss_scale=16, train_wall=251, gb_free=12.3, wall=3145
2022-03-03 11:50:22 | INFO | train_inner | epoch 004:    126 / 393 loss=9.613, nll_loss=8.337, ppl=323.4, wps=25473.7, ups=0.39, wpb=65535.4, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.846, loss_scale=16, train_wall=252, gb_free=12.3, wall=3402
2022-03-03 11:54:40 | INFO | train_inner | epoch 004:    226 / 393 loss=9.498, nll_loss=8.205, ppl=295.15, wps=25469.4, ups=0.39, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.833, loss_scale=16, train_wall=252, gb_free=12.3, wall=3660
2022-03-03 11:58:57 | INFO | train_inner | epoch 004:    326 / 393 loss=9.385, nll_loss=8.077, ppl=269.98, wps=25467.9, ups=0.39, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.824, loss_scale=16, train_wall=252, gb_free=12.3, wall=3917
2022-03-03 12:01:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:01:52 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.186 | nll_loss 7.837 | ppl 228.61 | wps 66673.1 | wpb 2034.1 | bsz 4 | num_updates 1567 | best_loss 9.186
2022-03-03 12:01:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1567 updates
2022-03-03 12:01:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 12:01:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 12:02:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 4 @ 1567 updates, score 9.186) (writing took 8.987255467101932 seconds)
2022-03-03 12:02:01 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-03 12:02:01 | INFO | train | epoch 004 | loss 9.476 | nll_loss 8.18 | ppl 290.1 | wps 25159.6 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 1567 | lr 0.000195936 | gnorm 0.83 | loss_scale 32 | train_wall 991 | gb_free 12.3 | wall 4101
2022-03-03 12:02:01 | INFO | fairseq.trainer | begin training epoch 5
2022-03-03 12:02:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:03:26 | INFO | train_inner | epoch 005:     33 / 393 loss=9.264, nll_loss=7.939, ppl=245.46, wps=24292, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.837, loss_scale=32, train_wall=251, gb_free=12.3, wall=4186
2022-03-03 12:07:43 | INFO | train_inner | epoch 005:    133 / 393 loss=9.125, nll_loss=7.78, ppl=219.8, wps=25456.5, ups=0.39, wpb=65535.4, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.841, loss_scale=32, train_wall=253, gb_free=12.3, wall=4443
2022-03-03 12:12:01 | INFO | train_inner | epoch 005:    233 / 393 loss=9.05, nll_loss=7.695, ppl=207.16, wps=25454.2, ups=0.39, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.811, loss_scale=32, train_wall=253, gb_free=12.3, wall=4700
2022-03-03 12:16:18 | INFO | train_inner | epoch 005:    333 / 393 loss=8.956, nll_loss=7.587, ppl=192.28, wps=25452.8, ups=0.39, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.799, loss_scale=32, train_wall=253, gb_free=12.3, wall=4958
2022-03-03 12:18:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:18:55 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.818 | nll_loss 7.41 | ppl 170.03 | wps 66617.3 | wpb 2034.1 | bsz 4 | num_updates 1960 | best_loss 8.818
2022-03-03 12:18:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1960 updates
2022-03-03 12:18:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 12:18:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 12:19:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 5 @ 1960 updates, score 8.818) (writing took 8.902496022172272 seconds)
2022-03-03 12:19:04 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-03 12:19:04 | INFO | train | epoch 005 | loss 9.034 | nll_loss 7.676 | ppl 204.51 | wps 25151.1 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 1960 | lr 0.000245051 | gnorm 0.816 | loss_scale 32 | train_wall 991 | gb_free 12.3 | wall 5123
2022-03-03 12:19:04 | INFO | fairseq.trainer | begin training epoch 6
2022-03-03 12:19:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:20:47 | INFO | train_inner | epoch 006:     40 / 393 loss=8.85, nll_loss=7.466, ppl=176.8, wps=24293.3, ups=0.37, wpb=65243.5, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.794, loss_scale=32, train_wall=251, gb_free=12.3, wall=5227
2022-03-03 12:23:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 12:25:07 | INFO | train_inner | epoch 006:    141 / 393 loss=8.743, nll_loss=7.343, ppl=162.38, wps=25211.9, ups=0.38, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.797, loss_scale=32, train_wall=255, gb_free=12.3, wall=5486
2022-03-03 12:29:24 | INFO | train_inner | epoch 006:    241 / 393 loss=8.679, nll_loss=7.271, ppl=154.42, wps=25477.6, ups=0.39, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.785, loss_scale=32, train_wall=252, gb_free=12.3, wall=5744
2022-03-03 12:33:41 | INFO | train_inner | epoch 006:    341 / 393 loss=8.604, nll_loss=7.184, ppl=145.46, wps=25484.2, ups=0.39, wpb=65530.9, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.762, loss_scale=32, train_wall=252, gb_free=12.3, wall=6001
2022-03-03 12:35:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:35:57 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.553 | nll_loss 7.092 | ppl 136.39 | wps 66673.9 | wpb 2034.1 | bsz 4 | num_updates 2352 | best_loss 8.553
2022-03-03 12:35:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2352 updates
2022-03-03 12:35:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 12:36:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 12:36:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 6 @ 2352 updates, score 8.553) (writing took 8.948067996185273 seconds)
2022-03-03 12:36:06 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-03 12:36:06 | INFO | train | epoch 006 | loss 8.672 | nll_loss 7.262 | ppl 153.52 | wps 25101.7 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 2352 | lr 0.000294041 | gnorm 0.78 | loss_scale 32 | train_wall 991 | gb_free 12.3 | wall 6146
2022-03-03 12:36:06 | INFO | fairseq.trainer | begin training epoch 7
2022-03-03 12:36:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:38:09 | INFO | train_inner | epoch 007:     48 / 393 loss=8.509, nll_loss=7.077, ppl=135.02, wps=24300, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.762, loss_scale=32, train_wall=251, gb_free=12.3, wall=6269
2022-03-03 12:42:27 | INFO | train_inner | epoch 007:    148 / 393 loss=8.408, nll_loss=6.96, ppl=124.52, wps=25471.1, ups=0.39, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.732, loss_scale=32, train_wall=252, gb_free=12.3, wall=6527
2022-03-03 12:45:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 12:46:47 | INFO | train_inner | epoch 007:    249 / 393 loss=8.376, nll_loss=6.923, ppl=121.39, wps=25228.5, ups=0.38, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.735, loss_scale=32, train_wall=255, gb_free=12.3, wall=6786
2022-03-03 12:51:04 | INFO | train_inner | epoch 007:    349 / 393 loss=8.322, nll_loss=6.862, ppl=116.3, wps=25473.5, ups=0.39, wpb=65530.2, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.721, loss_scale=32, train_wall=252, gb_free=12.3, wall=7044
2022-03-03 12:51:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 12:52:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:52:59 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.317 | nll_loss 6.831 | ppl 113.88 | wps 66904.8 | wpb 2034.1 | bsz 4 | num_updates 2743 | best_loss 8.317
2022-03-03 12:52:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2743 updates
2022-03-03 12:52:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 12:53:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 12:53:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 7 @ 2743 updates, score 8.317) (writing took 8.989575071260333 seconds)
2022-03-03 12:53:08 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-03 12:53:08 | INFO | train | epoch 007 | loss 8.369 | nll_loss 6.916 | ppl 120.74 | wps 25037.1 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 2743 | lr 0.000342906 | gnorm 0.729 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 7168
2022-03-03 12:53:08 | INFO | fairseq.trainer | begin training epoch 8
2022-03-03 12:53:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:55:35 | INFO | train_inner | epoch 008:     57 / 393 loss=8.222, nll_loss=6.748, ppl=107.5, wps=24067.4, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.715, loss_scale=16, train_wall=254, gb_free=12.3, wall=7315
2022-03-03 12:59:52 | INFO | train_inner | epoch 008:    157 / 393 loss=8.138, nll_loss=6.652, ppl=100.56, wps=25485.8, ups=0.39, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.699, loss_scale=16, train_wall=252, gb_free=12.3, wall=7572
2022-03-03 13:04:09 | INFO | train_inner | epoch 008:    257 / 393 loss=8.119, nll_loss=6.63, ppl=99.05, wps=25476.9, ups=0.39, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.72, loss_scale=16, train_wall=252, gb_free=12.3, wall=7829
2022-03-03 13:08:26 | INFO | train_inner | epoch 008:    357 / 393 loss=8.075, nll_loss=6.579, ppl=95.6, wps=25480, ups=0.39, wpb=65535.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.692, loss_scale=16, train_wall=252, gb_free=12.3, wall=8086
2022-03-03 13:09:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 13:10:01 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.154 | nll_loss 6.635 | ppl 99.41 | wps 66615.1 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 8.154
2022-03-03 13:10:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3136 updates
2022-03-03 13:10:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 13:10:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 13:10:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 8 @ 3136 updates, score 8.154) (writing took 8.93154550390318 seconds)
2022-03-03 13:10:10 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-03 13:10:10 | INFO | train | epoch 008 | loss 8.116 | nll_loss 6.627 | ppl 98.86 | wps 25169.9 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 3136 | lr 0.000392022 | gnorm 0.702 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 8190
2022-03-03 13:10:10 | INFO | fairseq.trainer | begin training epoch 9
2022-03-03 13:10:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 13:12:55 | INFO | train_inner | epoch 009:     64 / 393 loss=7.968, nll_loss=6.459, ppl=87.97, wps=24298.4, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.668, loss_scale=16, train_wall=251, gb_free=12.3, wall=8355
2022-03-03 13:17:12 | INFO | train_inner | epoch 009:    164 / 393 loss=7.914, nll_loss=6.397, ppl=84.25, wps=25477.8, ups=0.39, wpb=65530.9, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.665, loss_scale=32, train_wall=252, gb_free=12.3, wall=8612
2022-03-03 13:21:30 | INFO | train_inner | epoch 009:    264 / 393 loss=7.905, nll_loss=6.386, ppl=83.63, wps=25442.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.662, loss_scale=32, train_wall=253, gb_free=12.3, wall=8870
2022-03-03 13:22:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 13:25:50 | INFO | train_inner | epoch 009:    365 / 393 loss=7.89, nll_loss=6.368, ppl=82.62, wps=25225.8, ups=0.38, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.656, loss_scale=16, train_wall=255, gb_free=12.3, wall=9129
2022-03-03 13:27:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 13:27:04 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.022 | nll_loss 6.481 | ppl 89.33 | wps 66472.4 | wpb 2034.1 | bsz 4 | num_updates 3528 | best_loss 8.022
2022-03-03 13:27:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3528 updates
2022-03-03 13:27:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 13:27:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 13:27:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 9 @ 3528 updates, score 8.022) (writing took 8.944793205708265 seconds)
2022-03-03 13:27:13 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-03 13:27:13 | INFO | train | epoch 009 | loss 7.904 | nll_loss 6.384 | ppl 83.54 | wps 25096.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 3528 | lr 0.000441012 | gnorm 0.66 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 9213
2022-03-03 13:27:13 | INFO | fairseq.trainer | begin training epoch 10
2022-03-03 13:27:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 13:30:18 | INFO | train_inner | epoch 010:     72 / 393 loss=7.767, nll_loss=6.229, ppl=74.99, wps=24305.6, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.644, loss_scale=16, train_wall=251, gb_free=12.3, wall=9398
2022-03-03 13:34:36 | INFO | train_inner | epoch 010:    172 / 393 loss=7.729, nll_loss=6.185, ppl=72.74, wps=25446.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.633, loss_scale=16, train_wall=253, gb_free=12.3, wall=9655
2022-03-03 13:38:53 | INFO | train_inner | epoch 010:    272 / 393 loss=7.726, nll_loss=6.182, ppl=72.59, wps=25480.2, ups=0.39, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.632, loss_scale=16, train_wall=252, gb_free=12.3, wall=9913
2022-03-03 13:43:10 | INFO | train_inner | epoch 010:    372 / 393 loss=7.72, nll_loss=6.175, ppl=72.24, wps=25477.8, ups=0.39, wpb=65530.9, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.63, loss_scale=16, train_wall=252, gb_free=12.3, wall=10170
2022-03-03 13:44:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 13:44:06 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.914 | nll_loss 6.349 | ppl 81.5 | wps 66317.9 | wpb 2034.1 | bsz 4 | num_updates 3921 | best_loss 7.914
2022-03-03 13:44:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3921 updates
2022-03-03 13:44:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 13:44:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 13:44:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 10 @ 3921 updates, score 7.914) (writing took 8.965521136764437 seconds)
2022-03-03 13:44:15 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-03 13:44:15 | INFO | train | epoch 010 | loss 7.724 | nll_loss 6.179 | ppl 72.45 | wps 25162.1 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 3921 | lr 0.000490127 | gnorm 0.635 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 10235
2022-03-03 13:44:15 | INFO | fairseq.trainer | begin training epoch 11
2022-03-03 13:44:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 13:47:39 | INFO | train_inner | epoch 011:     79 / 393 loss=7.578, nll_loss=6.014, ppl=64.62, wps=24297.5, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.622, loss_scale=32, train_wall=251, gb_free=12.3, wall=10438
2022-03-03 13:49:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 13:51:58 | INFO | train_inner | epoch 011:    180 / 393 loss=7.564, nll_loss=5.997, ppl=63.85, wps=25229.8, ups=0.38, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.602, loss_scale=16, train_wall=255, gb_free=12.3, wall=10698
2022-03-03 13:56:16 | INFO | train_inner | epoch 011:    280 / 393 loss=7.571, nll_loss=6.005, ppl=64.21, wps=25472.5, ups=0.39, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.575, loss_scale=16, train_wall=252, gb_free=12.3, wall=10955
2022-03-03 14:00:33 | INFO | train_inner | epoch 011:    380 / 393 loss=7.573, nll_loss=6.008, ppl=64.34, wps=25476, ups=0.39, wpb=65535.4, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.59, loss_scale=16, train_wall=252, gb_free=12.3, wall=11213
2022-03-03 14:01:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:01:08 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.821 | nll_loss 6.242 | ppl 75.68 | wps 66826.1 | wpb 2034.1 | bsz 4 | num_updates 4313 | best_loss 7.821
2022-03-03 14:01:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4313 updates
2022-03-03 14:01:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 14:01:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 14:01:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 11 @ 4313 updates, score 7.821) (writing took 8.845448498148471 seconds)
2022-03-03 14:01:17 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-03 14:01:17 | INFO | train | epoch 011 | loss 7.563 | nll_loss 5.996 | ppl 63.84 | wps 25105.9 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 4313 | lr 0.000481515 | gnorm 0.594 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 11257
2022-03-03 14:01:17 | INFO | fairseq.trainer | begin training epoch 12
2022-03-03 14:01:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:05:01 | INFO | train_inner | epoch 012:     87 / 393 loss=7.408, nll_loss=5.821, ppl=56.53, wps=24310.8, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.568, loss_scale=16, train_wall=251, gb_free=12.3, wall=11481
2022-03-03 14:09:19 | INFO | train_inner | epoch 012:    187 / 393 loss=7.411, nll_loss=5.823, ppl=56.62, wps=25463.5, ups=0.39, wpb=65530.2, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.568, loss_scale=16, train_wall=252, gb_free=12.3, wall=11738
2022-03-03 14:11:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 14:13:38 | INFO | train_inner | epoch 012:    288 / 393 loss=7.421, nll_loss=5.835, ppl=57.09, wps=25223.3, ups=0.38, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.55, loss_scale=16, train_wall=255, gb_free=12.3, wall=11998
2022-03-03 14:17:56 | INFO | train_inner | epoch 012:    388 / 393 loss=7.427, nll_loss=5.843, ppl=57.39, wps=25471.7, ups=0.39, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.54, loss_scale=16, train_wall=252, gb_free=12.3, wall=12256
2022-03-03 14:18:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:18:11 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.764 | nll_loss 6.208 | ppl 73.9 | wps 66774.3 | wpb 2034.1 | bsz 4 | num_updates 4705 | best_loss 7.764
2022-03-03 14:18:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4705 updates
2022-03-03 14:18:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 14:18:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 14:18:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 12 @ 4705 updates, score 7.764) (writing took 8.889178884215653 seconds)
2022-03-03 14:18:20 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-03 14:18:20 | INFO | train | epoch 012 | loss 7.413 | nll_loss 5.826 | ppl 56.74 | wps 25101.1 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 4705 | lr 0.00046102 | gnorm 0.556 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 12279
2022-03-03 14:18:20 | INFO | fairseq.trainer | begin training epoch 13
2022-03-03 14:18:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:22:24 | INFO | train_inner | epoch 013:     95 / 393 loss=7.263, nll_loss=5.656, ppl=50.42, wps=24303.9, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.55, loss_scale=16, train_wall=251, gb_free=12.3, wall=12524
2022-03-03 14:26:41 | INFO | train_inner | epoch 013:    195 / 393 loss=7.286, nll_loss=5.681, ppl=51.32, wps=25471.5, ups=0.39, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.554, loss_scale=16, train_wall=252, gb_free=12.3, wall=12781
2022-03-03 14:30:59 | INFO | train_inner | epoch 013:    295 / 393 loss=7.301, nll_loss=5.699, ppl=51.96, wps=25470.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.533, loss_scale=16, train_wall=252, gb_free=12.3, wall=13039
2022-03-03 14:33:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 14:35:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:35:13 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.707 | nll_loss 6.126 | ppl 69.82 | wps 66045.2 | wpb 2034.1 | bsz 4 | num_updates 5097 | best_loss 7.707
2022-03-03 14:35:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5097 updates
2022-03-03 14:35:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 14:35:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 14:35:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 13 @ 5097 updates, score 7.707) (writing took 8.910476344637573 seconds)
2022-03-03 14:35:22 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-03 14:35:22 | INFO | train | epoch 013 | loss 7.288 | nll_loss 5.684 | ppl 51.41 | wps 25100.9 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 5097 | lr 0.000442938 | gnorm 0.541 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 13302
2022-03-03 14:35:22 | INFO | fairseq.trainer | begin training epoch 14
2022-03-03 14:35:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:35:30 | INFO | train_inner | epoch 014:      3 / 393 loss=7.303, nll_loss=5.702, ppl=52.04, wps=24079.1, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.528, loss_scale=16, train_wall=254, gb_free=12.3, wall=13310
2022-03-03 14:39:47 | INFO | train_inner | epoch 014:    103 / 393 loss=7.141, nll_loss=5.518, ppl=45.81, wps=25475.8, ups=0.39, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.534, loss_scale=16, train_wall=252, gb_free=12.3, wall=13567
2022-03-03 14:44:04 | INFO | train_inner | epoch 014:    203 / 393 loss=7.179, nll_loss=5.56, ppl=47.19, wps=25460.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.529, loss_scale=16, train_wall=252, gb_free=12.3, wall=13824
2022-03-03 14:48:22 | INFO | train_inner | epoch 014:    303 / 393 loss=7.2, nll_loss=5.584, ppl=47.96, wps=25477, ups=0.39, wpb=65530.9, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.527, loss_scale=16, train_wall=252, gb_free=12.3, wall=14081
2022-03-03 14:52:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:52:15 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.67 | nll_loss 6.087 | ppl 67.99 | wps 66417.7 | wpb 2034.1 | bsz 4 | num_updates 5490 | best_loss 7.67
2022-03-03 14:52:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5490 updates
2022-03-03 14:52:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 14:52:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 14:52:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 14 @ 5490 updates, score 7.67) (writing took 8.855414350982755 seconds)
2022-03-03 14:52:24 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-03 14:52:24 | INFO | train | epoch 014 | loss 7.179 | nll_loss 5.561 | ppl 47.21 | wps 25168.5 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 5490 | lr 0.00042679 | gnorm 0.528 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 14324
2022-03-03 14:52:24 | INFO | fairseq.trainer | begin training epoch 15
2022-03-03 14:52:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:52:50 | INFO | train_inner | epoch 015:     10 / 393 loss=7.189, nll_loss=5.572, ppl=47.57, wps=24315, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.522, loss_scale=16, train_wall=251, gb_free=12.3, wall=14350
2022-03-03 14:56:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 14:57:10 | INFO | train_inner | epoch 015:    111 / 393 loss=7.046, nll_loss=5.409, ppl=42.5, wps=25224.8, ups=0.38, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.528, loss_scale=16, train_wall=255, gb_free=12.3, wall=14610
2022-03-03 15:01:27 | INFO | train_inner | epoch 015:    211 / 393 loss=7.078, nll_loss=5.445, ppl=43.56, wps=25474.5, ups=0.39, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.515, loss_scale=16, train_wall=252, gb_free=12.3, wall=14867
2022-03-03 15:05:44 | INFO | train_inner | epoch 015:    311 / 393 loss=7.104, nll_loss=5.475, ppl=44.47, wps=25469.4, ups=0.39, wpb=65530.2, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.529, loss_scale=16, train_wall=252, gb_free=12.3, wall=15124
2022-03-03 15:09:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 15:09:17 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.657 | nll_loss 6.075 | ppl 67.39 | wps 66474.2 | wpb 2034.1 | bsz 4 | num_updates 5882 | best_loss 7.657
2022-03-03 15:09:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5882 updates
2022-03-03 15:09:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 15:09:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 15:09:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 15 @ 5882 updates, score 7.657) (writing took 8.875178256072104 seconds)
2022-03-03 15:09:26 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-03 15:09:26 | INFO | train | epoch 015 | loss 7.086 | nll_loss 5.454 | ppl 43.85 | wps 25103.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 5882 | lr 0.000412323 | gnorm 0.525 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 15346
2022-03-03 15:09:26 | INFO | fairseq.trainer | begin training epoch 16
2022-03-03 15:09:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 15:10:13 | INFO | train_inner | epoch 016:     18 / 393 loss=7.091, nll_loss=5.461, ppl=44.04, wps=24310.9, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.526, loss_scale=16, train_wall=251, gb_free=12.3, wall=15393
2022-03-03 15:14:30 | INFO | train_inner | epoch 016:    118 / 393 loss=6.959, nll_loss=5.31, ppl=39.68, wps=25469.3, ups=0.39, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.527, loss_scale=16, train_wall=252, gb_free=12.3, wall=15650
2022-03-03 15:18:47 | INFO | train_inner | epoch 016:    218 / 393 loss=7, nll_loss=5.356, ppl=40.97, wps=25468.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.528, loss_scale=32, train_wall=252, gb_free=12.3, wall=15907
2022-03-03 15:19:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 15:23:07 | INFO | train_inner | epoch 016:    319 / 393 loss=7.029, nll_loss=5.39, ppl=41.93, wps=25227.1, ups=0.38, wpb=65530.9, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.52, loss_scale=16, train_wall=255, gb_free=12.3, wall=16167
2022-03-03 15:26:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 15:26:20 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.648 | nll_loss 6.054 | ppl 66.44 | wps 66750.5 | wpb 2034.1 | bsz 4 | num_updates 6274 | best_loss 7.648
2022-03-03 15:26:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6274 updates
2022-03-03 15:26:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 15:26:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 15:26:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 16 @ 6274 updates, score 7.648) (writing took 8.847128050867468 seconds)
2022-03-03 15:26:29 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-03 15:26:29 | INFO | train | epoch 016 | loss 7.004 | nll_loss 5.361 | ppl 41.11 | wps 25102.2 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 6274 | lr 0.000399234 | gnorm 0.524 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 16368
2022-03-03 15:26:29 | INFO | fairseq.trainer | begin training epoch 17
2022-03-03 15:26:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 15:27:35 | INFO | train_inner | epoch 017:     26 / 393 loss=7.002, nll_loss=5.36, ppl=41.06, wps=24312, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.516, loss_scale=16, train_wall=251, gb_free=12.3, wall=16435
2022-03-03 15:31:53 | INFO | train_inner | epoch 017:    126 / 393 loss=6.889, nll_loss=5.23, ppl=37.52, wps=25475.1, ups=0.39, wpb=65535.4, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.523, loss_scale=16, train_wall=252, gb_free=12.3, wall=16693
2022-03-03 15:36:10 | INFO | train_inner | epoch 017:    226 / 393 loss=6.93, nll_loss=5.276, ppl=38.74, wps=25479.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.536, loss_scale=16, train_wall=252, gb_free=12.3, wall=16950
2022-03-03 15:40:27 | INFO | train_inner | epoch 017:    326 / 393 loss=6.955, nll_loss=5.305, ppl=39.54, wps=25465.5, ups=0.39, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.523, loss_scale=16, train_wall=252, gb_free=12.3, wall=17207
2022-03-03 15:41:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 15:43:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 15:43:22 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.65 | nll_loss 6.042 | ppl 65.89 | wps 66789.2 | wpb 2034.1 | bsz 4 | num_updates 6666 | best_loss 7.648
2022-03-03 15:43:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6666 updates
2022-03-03 15:43:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 15:43:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 15:43:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 17 @ 6666 updates, score 7.65) (writing took 4.063183373305947 seconds)
2022-03-03 15:43:26 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-03 15:43:26 | INFO | train | epoch 017 | loss 6.93 | nll_loss 5.276 | ppl 38.76 | wps 25225.4 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 6666 | lr 0.000387318 | gnorm 0.524 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 17386
2022-03-03 15:43:26 | INFO | fairseq.trainer | begin training epoch 18
2022-03-03 15:43:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 15:44:53 | INFO | train_inner | epoch 018:     34 / 393 loss=6.92, nll_loss=5.266, ppl=38.47, wps=24520.4, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.524, loss_scale=16, train_wall=254, gb_free=12.3, wall=17473
2022-03-03 15:49:11 | INFO | train_inner | epoch 018:    134 / 393 loss=6.827, nll_loss=5.159, ppl=35.73, wps=25476.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.526, loss_scale=16, train_wall=252, gb_free=12.3, wall=17730
2022-03-03 15:53:28 | INFO | train_inner | epoch 018:    234 / 393 loss=6.85, nll_loss=5.185, ppl=36.37, wps=25472.7, ups=0.39, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.519, loss_scale=16, train_wall=252, gb_free=12.3, wall=17988
2022-03-03 15:57:45 | INFO | train_inner | epoch 018:    334 / 393 loss=6.897, nll_loss=5.238, ppl=37.75, wps=25478.4, ups=0.39, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.53, loss_scale=16, train_wall=252, gb_free=12.3, wall=18245
2022-03-03 16:00:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:00:19 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.644 | nll_loss 6.058 | ppl 66.64 | wps 66495.5 | wpb 2034.1 | bsz 4 | num_updates 7059 | best_loss 7.644
2022-03-03 16:00:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7059 updates
2022-03-03 16:00:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 16:00:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 16:00:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 18 @ 7059 updates, score 7.644) (writing took 8.925008304417133 seconds)
2022-03-03 16:00:28 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-03 16:00:28 | INFO | train | epoch 018 | loss 6.863 | nll_loss 5.2 | ppl 36.75 | wps 25166.6 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 7059 | lr 0.000376382 | gnorm 0.528 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 18408
2022-03-03 16:00:28 | INFO | fairseq.trainer | begin training epoch 19
2022-03-03 16:00:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:02:14 | INFO | train_inner | epoch 019:     41 / 393 loss=6.84, nll_loss=5.174, ppl=36.1, wps=24305.5, ups=0.37, wpb=65239, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.527, loss_scale=16, train_wall=251, gb_free=12.3, wall=18513
2022-03-03 16:03:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 16:06:33 | INFO | train_inner | epoch 019:    142 / 393 loss=6.759, nll_loss=5.081, ppl=33.84, wps=25226.2, ups=0.38, wpb=65535.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.538, loss_scale=16, train_wall=255, gb_free=12.3, wall=18773
2022-03-03 16:10:51 | INFO | train_inner | epoch 019:    242 / 393 loss=6.802, nll_loss=5.13, ppl=35.01, wps=25475.3, ups=0.39, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.531, loss_scale=16, train_wall=252, gb_free=12.3, wall=19030
2022-03-03 16:15:08 | INFO | train_inner | epoch 019:    342 / 393 loss=6.84, nll_loss=5.173, ppl=36.09, wps=25476.5, ups=0.39, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.539, loss_scale=16, train_wall=252, gb_free=12.3, wall=19288
2022-03-03 16:17:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:17:21 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.643 | nll_loss 6.048 | ppl 66.17 | wps 66633.8 | wpb 2034.1 | bsz 4 | num_updates 7451 | best_loss 7.643
2022-03-03 16:17:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7451 updates
2022-03-03 16:17:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 16:17:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 16:17:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 19 @ 7451 updates, score 7.643) (writing took 8.85675171809271 seconds)
2022-03-03 16:17:30 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-03 16:17:30 | INFO | train | epoch 019 | loss 6.801 | nll_loss 5.129 | ppl 35 | wps 25107.9 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 7451 | lr 0.000366347 | gnorm 0.533 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 19430
2022-03-03 16:17:30 | INFO | fairseq.trainer | begin training epoch 20
2022-03-03 16:17:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:19:36 | INFO | train_inner | epoch 020:     49 / 393 loss=6.774, nll_loss=5.099, ppl=34.26, wps=24315.6, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.537, loss_scale=16, train_wall=251, gb_free=12.3, wall=19556
2022-03-03 16:23:53 | INFO | train_inner | epoch 020:    149 / 393 loss=6.713, nll_loss=5.028, ppl=32.63, wps=25474.5, ups=0.39, wpb=65535.4, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.533, loss_scale=16, train_wall=252, gb_free=12.3, wall=19813
2022-03-03 16:26:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 16:28:13 | INFO | train_inner | epoch 020:    250 / 393 loss=6.744, nll_loss=5.063, ppl=33.43, wps=25234.8, ups=0.39, wpb=65530.9, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.527, loss_scale=16, train_wall=255, gb_free=12.3, wall=20073
2022-03-03 16:32:30 | INFO | train_inner | epoch 020:    350 / 393 loss=6.781, nll_loss=5.106, ppl=34.43, wps=25479.8, ups=0.39, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.54, loss_scale=16, train_wall=252, gb_free=12.3, wall=20330
2022-03-03 16:34:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:34:23 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.641 | nll_loss 6.063 | ppl 66.85 | wps 66517.1 | wpb 2034.1 | bsz 4 | num_updates 7843 | best_loss 7.641
2022-03-03 16:34:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7843 updates
2022-03-03 16:34:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 16:34:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt
2022-03-03 16:34:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_best.pt (epoch 20 @ 7843 updates, score 7.641) (writing took 8.777345961891115 seconds)
2022-03-03 16:34:32 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-03 16:34:32 | INFO | train | epoch 020 | loss 6.744 | nll_loss 5.064 | ppl 33.44 | wps 25112 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 7843 | lr 0.000357075 | gnorm 0.539 | loss_scale 16 | train_wall 990 | gb_free 12.3 | wall 20452
2022-03-03 16:34:32 | INFO | fairseq.trainer | begin training epoch 21
2022-03-03 16:34:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:35:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 16:37:01 | INFO | train_inner | epoch 021:     58 / 393 loss=6.701, nll_loss=5.015, ppl=32.33, wps=24085.2, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.542, loss_scale=8, train_wall=254, gb_free=12.3, wall=20601
2022-03-03 16:41:18 | INFO | train_inner | epoch 021:    158 / 393 loss=6.66, nll_loss=4.967, ppl=31.28, wps=25472.5, ups=0.39, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.54, loss_scale=8, train_wall=252, gb_free=12.3, wall=20858
2022-03-03 16:45:36 | INFO | train_inner | epoch 021:    258 / 393 loss=6.703, nll_loss=5.016, ppl=32.35, wps=25484.5, ups=0.39, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.549, loss_scale=8, train_wall=252, gb_free=12.3, wall=21116
2022-03-03 16:49:53 | INFO | train_inner | epoch 021:    358 / 393 loss=6.734, nll_loss=5.051, ppl=33.16, wps=25475.5, ups=0.39, wpb=65535.4, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.537, loss_scale=8, train_wall=252, gb_free=12.3, wall=21373
2022-03-03 16:51:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:51:25 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.658 | nll_loss 6.063 | ppl 66.87 | wps 66906.8 | wpb 2034.1 | bsz 4 | num_updates 8235 | best_loss 7.641
2022-03-03 16:51:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8235 updates
2022-03-03 16:51:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 16:51:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 16:51:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 21 @ 8235 updates, score 7.658) (writing took 4.078166853170842 seconds)
2022-03-03 16:51:29 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-03 16:51:29 | INFO | train | epoch 021 | loss 6.693 | nll_loss 5.005 | ppl 32.1 | wps 25225.4 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 8235 | lr 0.000348472 | gnorm 0.538 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 21469
2022-03-03 16:51:29 | INFO | fairseq.trainer | begin training epoch 22
2022-03-03 16:51:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:54:16 | INFO | train_inner | epoch 022:     65 / 393 loss=6.637, nll_loss=4.941, ppl=30.72, wps=24758.1, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.551, loss_scale=8, train_wall=251, gb_free=12.3, wall=21636
2022-03-03 16:58:34 | INFO | train_inner | epoch 022:    165 / 393 loss=6.608, nll_loss=4.907, ppl=30.01, wps=25486, ups=0.39, wpb=65530.9, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.551, loss_scale=16, train_wall=252, gb_free=12.3, wall=21893
2022-03-03 17:02:51 | INFO | train_inner | epoch 022:    265 / 393 loss=6.665, nll_loss=4.972, ppl=31.38, wps=25475, ups=0.39, wpb=65535.4, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.547, loss_scale=16, train_wall=252, gb_free=12.3, wall=22151
2022-03-03 17:04:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 17:07:10 | INFO | train_inner | epoch 022:    366 / 393 loss=6.688, nll_loss=4.999, ppl=31.97, wps=25241.2, ups=0.39, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.55, loss_scale=8, train_wall=255, gb_free=12.3, wall=22410
2022-03-03 17:08:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:08:22 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.67 | nll_loss 6.066 | ppl 67.01 | wps 66571.4 | wpb 2034.1 | bsz 4 | num_updates 8627 | best_loss 7.641
2022-03-03 17:08:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8627 updates
2022-03-03 17:08:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 17:08:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 17:08:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 22 @ 8627 updates, score 7.67) (writing took 4.008193101268262 seconds)
2022-03-03 17:08:26 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-03 17:08:26 | INFO | train | epoch 022 | loss 6.644 | nll_loss 4.948 | ppl 30.87 | wps 25234.8 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 8627 | lr 0.000340463 | gnorm 0.55 | loss_scale 8 | train_wall 990 | gb_free 12.3 | wall 22486
2022-03-03 17:08:26 | INFO | fairseq.trainer | begin training epoch 23
2022-03-03 17:08:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 17:11:34 | INFO | train_inner | epoch 023:     73 / 393 loss=6.58, nll_loss=4.876, ppl=29.37, wps=24759.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.549, loss_scale=8, train_wall=251, gb_free=12.3, wall=22674
2022-03-03 17:15:51 | INFO | train_inner | epoch 023:    173 / 393 loss=6.571, nll_loss=4.865, ppl=29.13, wps=25483.7, ups=0.39, wpb=65530.9, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.563, loss_scale=8, train_wall=252, gb_free=12.3, wall=22931
2022-03-03 17:20:08 | INFO | train_inner | epoch 023:    273 / 393 loss=6.611, nll_loss=4.91, ppl=30.07, wps=25490.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.547, loss_scale=8, train_wall=252, gb_free=12.3, wall=23188
2022-03-03 17:24:25 | INFO | train_inner | epoch 023:    373 / 393 loss=6.647, nll_loss=4.952, ppl=30.95, wps=25483.8, ups=0.39, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.563, loss_scale=8, train_wall=252, gb_free=12.3, wall=23445
2022-03-03 17:25:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:25:19 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.692 | nll_loss 6.085 | ppl 67.89 | wps 67199 | wpb 2034.1 | bsz 4 | num_updates 9020 | best_loss 7.641
2022-03-03 17:25:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9020 updates
2022-03-03 17:25:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 17:25:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 17:25:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 23 @ 9020 updates, score 7.692) (writing took 4.088452805299312 seconds)
2022-03-03 17:25:23 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-03 17:25:23 | INFO | train | epoch 023 | loss 6.598 | nll_loss 4.896 | ppl 29.77 | wps 25296.2 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 9020 | lr 0.000332964 | gnorm 0.556 | loss_scale 8 | train_wall 990 | gb_free 12.3 | wall 23503
2022-03-03 17:25:23 | INFO | fairseq.trainer | begin training epoch 24
2022-03-03 17:25:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 17:28:49 | INFO | train_inner | epoch 024:     80 / 393 loss=6.533, nll_loss=4.821, ppl=28.27, wps=24761.6, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.554, loss_scale=16, train_wall=251, gb_free=12.3, wall=23709
2022-03-03 17:33:06 | INFO | train_inner | epoch 024:    180 / 393 loss=6.525, nll_loss=4.811, ppl=28.07, wps=25476, ups=0.39, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.561, loss_scale=16, train_wall=252, gb_free=12.3, wall=23966
2022-03-03 17:37:23 | INFO | train_inner | epoch 024:    280 / 393 loss=6.571, nll_loss=4.864, ppl=29.13, wps=25474.1, ups=0.39, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.551, loss_scale=16, train_wall=252, gb_free=12.3, wall=24223
2022-03-03 17:41:41 | INFO | train_inner | epoch 024:    380 / 393 loss=6.607, nll_loss=4.906, ppl=29.97, wps=25468.2, ups=0.39, wpb=65535.4, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.562, loss_scale=16, train_wall=252, gb_free=12.3, wall=24481
2022-03-03 17:42:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:42:16 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.685 | nll_loss 6.081 | ppl 67.72 | wps 66489.6 | wpb 2034.1 | bsz 4 | num_updates 9413 | best_loss 7.641
2022-03-03 17:42:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9413 updates
2022-03-03 17:42:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 17:42:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 17:42:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 24 @ 9413 updates, score 7.685) (writing took 4.11240157764405 seconds)
2022-03-03 17:42:21 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-03 17:42:21 | INFO | train | epoch 024 | loss 6.556 | nll_loss 4.847 | ppl 28.78 | wps 25285.5 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 9413 | lr 0.000325939 | gnorm 0.557 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 24520
2022-03-03 17:42:21 | INFO | fairseq.trainer | begin training epoch 25
2022-03-03 17:42:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 17:42:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 17:46:07 | INFO | train_inner | epoch 025:     88 / 393 loss=6.474, nll_loss=4.753, ppl=26.96, wps=24520.6, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.565, loss_scale=8, train_wall=254, gb_free=12.3, wall=24747
2022-03-03 17:50:24 | INFO | train_inner | epoch 025:    188 / 393 loss=6.492, nll_loss=4.773, ppl=27.34, wps=25477.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.561, loss_scale=8, train_wall=252, gb_free=12.3, wall=25004
2022-03-03 17:54:41 | INFO | train_inner | epoch 025:    288 / 393 loss=6.539, nll_loss=4.827, ppl=28.38, wps=25480.8, ups=0.39, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.569, loss_scale=8, train_wall=252, gb_free=12.3, wall=25261
2022-03-03 17:58:58 | INFO | train_inner | epoch 025:    388 / 393 loss=6.568, nll_loss=4.86, ppl=29.04, wps=25483.6, ups=0.39, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.573, loss_scale=8, train_wall=252, gb_free=12.3, wall=25518
2022-03-03 17:59:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:59:13 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.701 | nll_loss 6.108 | ppl 68.97 | wps 66556.3 | wpb 2034.1 | bsz 4 | num_updates 9805 | best_loss 7.641
2022-03-03 17:59:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9805 updates
2022-03-03 17:59:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 17:59:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 17:59:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 25 @ 9805 updates, score 7.701) (writing took 4.120074580423534 seconds)
2022-03-03 17:59:18 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-03 17:59:18 | INFO | train | epoch 025 | loss 6.515 | nll_loss 4.8 | ppl 27.86 | wps 25230.2 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 9805 | lr 0.000319357 | gnorm 0.567 | loss_scale 8 | train_wall 990 | gb_free 12.3 | wall 25537
2022-03-03 17:59:18 | INFO | fairseq.trainer | begin training epoch 26
2022-03-03 17:59:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:03:22 | INFO | train_inner | epoch 026:     95 / 393 loss=6.419, nll_loss=4.69, ppl=25.81, wps=24755.3, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.569, loss_scale=8, train_wall=251, gb_free=12.3, wall=25782
2022-03-03 18:07:39 | INFO | train_inner | epoch 026:    195 / 393 loss=6.455, nll_loss=4.73, ppl=26.54, wps=25476.7, ups=0.39, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.576, loss_scale=16, train_wall=252, gb_free=12.3, wall=26039
2022-03-03 18:09:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 18:11:59 | INFO | train_inner | epoch 026:    296 / 393 loss=6.507, nll_loss=4.79, ppl=27.67, wps=25244.6, ups=0.39, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.579, loss_scale=8, train_wall=255, gb_free=12.3, wall=26299
2022-03-03 18:16:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:16:10 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.73 | nll_loss 6.146 | ppl 70.82 | wps 66688.8 | wpb 2034.1 | bsz 4 | num_updates 10197 | best_loss 7.641
2022-03-03 18:16:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10197 updates
2022-03-03 18:16:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 18:16:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 18:16:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 26 @ 10197 updates, score 7.73) (writing took 4.088479447644204 seconds)
2022-03-03 18:16:14 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-03 18:16:14 | INFO | train | epoch 026 | loss 6.478 | nll_loss 4.756 | ppl 27.03 | wps 25236.7 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 10197 | lr 0.000313158 | gnorm 0.577 | loss_scale 8 | train_wall 990 | gb_free 12.3 | wall 26554
2022-03-03 18:16:14 | INFO | fairseq.trainer | begin training epoch 27
2022-03-03 18:16:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:16:22 | INFO | train_inner | epoch 027:      3 / 393 loss=6.532, nll_loss=4.819, ppl=28.23, wps=24772.7, ups=0.38, wpb=65243.5, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.585, loss_scale=8, train_wall=251, gb_free=12.3, wall=26562
2022-03-03 18:20:39 | INFO | train_inner | epoch 027:    103 / 393 loss=6.376, nll_loss=4.639, ppl=24.92, wps=25488.8, ups=0.39, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.585, loss_scale=8, train_wall=252, gb_free=12.3, wall=26819
2022-03-03 18:24:56 | INFO | train_inner | epoch 027:    203 / 393 loss=6.424, nll_loss=4.694, ppl=25.88, wps=25498.8, ups=0.39, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.575, loss_scale=8, train_wall=252, gb_free=12.3, wall=27076
2022-03-03 18:29:14 | INFO | train_inner | epoch 027:    303 / 393 loss=6.471, nll_loss=4.749, ppl=26.88, wps=25475.9, ups=0.39, wpb=65530.2, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.581, loss_scale=8, train_wall=252, gb_free=12.3, wall=27333
2022-03-03 18:32:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 18:33:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:33:07 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.744 | nll_loss 6.138 | ppl 70.41 | wps 66818.7 | wpb 2034.1 | bsz 4 | num_updates 10589 | best_loss 7.641
2022-03-03 18:33:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10589 updates
2022-03-03 18:33:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 18:33:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 18:33:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 27 @ 10589 updates, score 7.744) (writing took 4.175727222114801 seconds)
2022-03-03 18:33:11 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-03 18:33:11 | INFO | train | epoch 027 | loss 6.442 | nll_loss 4.715 | ppl 26.27 | wps 25232.5 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 10589 | lr 0.000307307 | gnorm 0.584 | loss_scale 8 | train_wall 990 | gb_free 12.3 | wall 27571
2022-03-03 18:33:11 | INFO | fairseq.trainer | begin training epoch 28
2022-03-03 18:33:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:33:40 | INFO | train_inner | epoch 028:     11 / 393 loss=6.485, nll_loss=4.764, ppl=27.18, wps=24514.7, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.593, loss_scale=8, train_wall=254, gb_free=12.3, wall=27600
2022-03-03 18:37:57 | INFO | train_inner | epoch 028:    111 / 393 loss=6.348, nll_loss=4.607, ppl=24.37, wps=25487.1, ups=0.39, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.575, loss_scale=8, train_wall=252, gb_free=12.3, wall=27857
2022-03-03 18:42:14 | INFO | train_inner | epoch 028:    211 / 393 loss=6.4, nll_loss=4.666, ppl=25.39, wps=25494.3, ups=0.39, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.587, loss_scale=8, train_wall=252, gb_free=12.3, wall=28114
2022-03-03 18:46:31 | INFO | train_inner | epoch 028:    311 / 393 loss=6.438, nll_loss=4.71, ppl=26.17, wps=25475.6, ups=0.39, wpb=65530.9, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.591, loss_scale=8, train_wall=252, gb_free=12.3, wall=28371
2022-03-03 18:50:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:50:04 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.745 | nll_loss 6.149 | ppl 70.95 | wps 66607 | wpb 2034.1 | bsz 4 | num_updates 10982 | best_loss 7.641
2022-03-03 18:50:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10982 updates
2022-03-03 18:50:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 18:50:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 18:50:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 28 @ 10982 updates, score 7.745) (writing took 4.136506585869938 seconds)
2022-03-03 18:50:08 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-03 18:50:08 | INFO | train | epoch 028 | loss 6.409 | nll_loss 4.677 | ppl 25.59 | wps 25293.8 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 10982 | lr 0.000301758 | gnorm 0.583 | loss_scale 8 | train_wall 990 | gb_free 12.3 | wall 28588
2022-03-03 18:50:08 | INFO | fairseq.trainer | begin training epoch 29
2022-03-03 18:50:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:50:55 | INFO | train_inner | epoch 029:     18 / 393 loss=6.441, nll_loss=4.714, ppl=26.24, wps=24746.9, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.58, loss_scale=8, train_wall=251, gb_free=12.3, wall=28635
2022-03-03 18:55:12 | INFO | train_inner | epoch 029:    118 / 393 loss=6.319, nll_loss=4.574, ppl=23.82, wps=25470.5, ups=0.39, wpb=65530.2, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.582, loss_scale=16, train_wall=252, gb_free=12.3, wall=28892
2022-03-03 18:57:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 18:59:32 | INFO | train_inner | epoch 029:    219 / 393 loss=6.369, nll_loss=4.631, ppl=24.77, wps=25236.1, ups=0.39, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.591, loss_scale=8, train_wall=255, gb_free=12.3, wall=29152
2022-03-03 19:03:49 | INFO | train_inner | epoch 029:    319 / 393 loss=6.407, nll_loss=4.674, ppl=25.53, wps=25488.3, ups=0.39, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.595, loss_scale=8, train_wall=252, gb_free=12.3, wall=29409
2022-03-03 19:06:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:07:01 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.766 | nll_loss 6.168 | ppl 71.93 | wps 66778.6 | wpb 2034.1 | bsz 4 | num_updates 11374 | best_loss 7.641
2022-03-03 19:07:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11374 updates
2022-03-03 19:07:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 19:07:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 19:07:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 29 @ 11374 updates, score 7.766) (writing took 4.149237733799964 seconds)
2022-03-03 19:07:05 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-03 19:07:05 | INFO | train | epoch 029 | loss 6.377 | nll_loss 4.64 | ppl 24.94 | wps 25232.6 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 11374 | lr 0.000296513 | gnorm 0.592 | loss_scale 8 | train_wall 990 | gb_free 12.3 | wall 29605
2022-03-03 19:07:05 | INFO | fairseq.trainer | begin training epoch 30
2022-03-03 19:07:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 19:08:12 | INFO | train_inner | epoch 030:     26 / 393 loss=6.399, nll_loss=4.665, ppl=25.36, wps=24765.2, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.6, loss_scale=8, train_wall=251, gb_free=12.3, wall=29672
2022-03-03 19:12:30 | INFO | train_inner | epoch 030:    126 / 393 loss=6.297, nll_loss=4.548, ppl=23.39, wps=25481.7, ups=0.39, wpb=65535.4, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.592, loss_scale=8, train_wall=252, gb_free=12.3, wall=29929
2022-03-03 19:16:47 | INFO | train_inner | epoch 030:    226 / 393 loss=6.339, nll_loss=4.596, ppl=24.18, wps=25482.7, ups=0.39, wpb=65530.9, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.6, loss_scale=8, train_wall=252, gb_free=12.3, wall=30187
2022-03-03 19:19:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 19:21:06 | INFO | train_inner | epoch 030:    327 / 393 loss=6.381, nll_loss=4.645, ppl=25.01, wps=25233.9, ups=0.39, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.601, loss_scale=8, train_wall=255, gb_free=12.3, wall=30446
2022-03-03 19:23:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:23:58 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.791 | nll_loss 6.2 | ppl 73.54 | wps 66767.7 | wpb 2034.1 | bsz 4 | num_updates 11766 | best_loss 7.641
2022-03-03 19:23:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11766 updates
2022-03-03 19:23:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 19:24:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 19:24:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 30 @ 11766 updates, score 7.791) (writing took 4.13712446577847 seconds)
2022-03-03 19:24:02 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-03 19:24:02 | INFO | train | epoch 030 | loss 6.348 | nll_loss 4.606 | ppl 24.35 | wps 25232.4 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 11766 | lr 0.000291532 | gnorm 0.597 | loss_scale 8 | train_wall 990 | gb_free 12.3 | wall 30622
2022-03-03 19:24:02 | INFO | fairseq.trainer | begin training epoch 31
2022-03-03 19:24:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 19:25:30 | INFO | train_inner | epoch 031:     34 / 393 loss=6.357, nll_loss=4.616, ppl=24.52, wps=24763.2, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.601, loss_scale=8, train_wall=251, gb_free=12.3, wall=30710
2022-03-03 19:29:47 | INFO | train_inner | epoch 031:    134 / 393 loss=6.266, nll_loss=4.512, ppl=22.81, wps=25495.7, ups=0.39, wpb=65530.9, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.606, loss_scale=8, train_wall=252, gb_free=12.3, wall=30967
2022-03-03 19:34:04 | INFO | train_inner | epoch 031:    234 / 393 loss=6.321, nll_loss=4.575, ppl=23.84, wps=25493.1, ups=0.39, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.613, loss_scale=8, train_wall=252, gb_free=12.3, wall=31224
2022-03-03 19:38:21 | INFO | train_inner | epoch 031:    334 / 393 loss=6.36, nll_loss=4.619, ppl=24.58, wps=25484.7, ups=0.39, wpb=65535.4, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.612, loss_scale=8, train_wall=252, gb_free=12.3, wall=31481
2022-03-03 19:40:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:40:55 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.796 | nll_loss 6.203 | ppl 73.66 | wps 66563.2 | wpb 2034.1 | bsz 4 | num_updates 12159 | best_loss 7.641
2022-03-03 19:40:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12159 updates
2022-03-03 19:40:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 19:40:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 19:40:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 31 @ 12159 updates, score 7.796) (writing took 4.16775910416618 seconds)
2022-03-03 19:40:59 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-03 19:40:59 | INFO | train | epoch 031 | loss 6.319 | nll_loss 4.573 | ppl 23.8 | wps 25299.8 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 12159 | lr 0.000286781 | gnorm 0.607 | loss_scale 8 | train_wall 990 | gb_free 12.3 | wall 31639
2022-03-03 19:40:59 | INFO | fairseq.trainer | begin training epoch 32
2022-03-03 19:40:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 19:42:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 19:42:47 | INFO | train_inner | epoch 032:     42 / 393 loss=6.315, nll_loss=4.568, ppl=23.72, wps=24517.1, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.61, loss_scale=8, train_wall=254, gb_free=12.3, wall=31747
2022-03-03 19:47:05 | INFO | train_inner | epoch 032:    142 / 393 loss=6.24, nll_loss=4.481, ppl=22.33, wps=25468.9, ups=0.39, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.612, loss_scale=8, train_wall=252, gb_free=12.3, wall=32004
2022-03-03 19:51:22 | INFO | train_inner | epoch 032:    242 / 393 loss=6.29, nll_loss=4.538, ppl=23.24, wps=25473.6, ups=0.39, wpb=65530.9, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.61, loss_scale=8, train_wall=252, gb_free=12.3, wall=32262
2022-03-03 19:55:39 | INFO | train_inner | epoch 032:    342 / 393 loss=6.343, nll_loss=4.6, ppl=24.25, wps=25485.3, ups=0.39, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.617, loss_scale=8, train_wall=252, gb_free=12.3, wall=32519
2022-03-03 19:57:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:57:52 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.803 | nll_loss 6.219 | ppl 74.5 | wps 66614.7 | wpb 2034.1 | bsz 4 | num_updates 12551 | best_loss 7.641
2022-03-03 19:57:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12551 updates
2022-03-03 19:57:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 19:57:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 19:57:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 32 @ 12551 updates, score 7.803) (writing took 4.227665887679905 seconds)
2022-03-03 19:57:57 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-03 19:57:57 | INFO | train | epoch 032 | loss 6.292 | nll_loss 4.541 | ppl 23.27 | wps 25222.2 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 12551 | lr 0.000282267 | gnorm 0.616 | loss_scale 8 | train_wall 990 | gb_free 12.3 | wall 32657
2022-03-03 19:57:57 | INFO | fairseq.trainer | begin training epoch 33
2022-03-03 19:57:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:00:03 | INFO | train_inner | epoch 033:     49 / 393 loss=6.276, nll_loss=4.523, ppl=22.99, wps=24735, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.621, loss_scale=8, train_wall=251, gb_free=12.3, wall=32783
2022-03-03 20:04:20 | INFO | train_inner | epoch 033:    149 / 393 loss=6.225, nll_loss=4.463, ppl=22.06, wps=25480.6, ups=0.39, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.615, loss_scale=16, train_wall=252, gb_free=12.3, wall=33040
2022-03-03 20:08:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:08:40 | INFO | train_inner | epoch 033:    250 / 393 loss=6.27, nll_loss=4.515, ppl=22.87, wps=25205.9, ups=0.38, wpb=65535.4, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.619, loss_scale=8, train_wall=255, gb_free=12.3, wall=33300
2022-03-03 20:12:57 | INFO | train_inner | epoch 033:    350 / 393 loss=6.311, nll_loss=4.562, ppl=23.62, wps=25468.5, ups=0.39, wpb=65530.9, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.643, loss_scale=8, train_wall=252, gb_free=12.3, wall=33557
2022-03-03 20:14:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:14:50 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.824 | nll_loss 6.237 | ppl 75.45 | wps 66698.3 | wpb 2034.1 | bsz 4 | num_updates 12943 | best_loss 7.641
2022-03-03 20:14:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12943 updates
2022-03-03 20:14:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 20:14:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 20:14:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 33 @ 12943 updates, score 7.824) (writing took 4.210787035059184 seconds)
2022-03-03 20:14:54 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-03 20:14:54 | INFO | train | epoch 033 | loss 6.265 | nll_loss 4.51 | ppl 22.78 | wps 25216.2 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 12943 | lr 0.00027796 | gnorm 0.625 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 33674
2022-03-03 20:14:54 | INFO | fairseq.trainer | begin training epoch 34
2022-03-03 20:14:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:17:21 | INFO | train_inner | epoch 034:     57 / 393 loss=6.235, nll_loss=4.475, ppl=22.24, wps=24731.7, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.62, loss_scale=8, train_wall=251, gb_free=12.3, wall=33821
2022-03-03 20:21:38 | INFO | train_inner | epoch 034:    157 / 393 loss=6.199, nll_loss=4.434, ppl=21.61, wps=25470, ups=0.39, wpb=65530.9, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.624, loss_scale=8, train_wall=252, gb_free=12.3, wall=34078
2022-03-03 20:25:56 | INFO | train_inner | epoch 034:    257 / 393 loss=6.249, nll_loss=4.491, ppl=22.48, wps=25469, ups=0.39, wpb=65535.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.627, loss_scale=8, train_wall=252, gb_free=12.3, wall=34336
2022-03-03 20:30:13 | INFO | train_inner | epoch 034:    357 / 393 loss=6.296, nll_loss=4.545, ppl=23.35, wps=25465.6, ups=0.39, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.635, loss_scale=8, train_wall=252, gb_free=12.3, wall=34593
2022-03-03 20:31:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:31:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:31:48 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.859 | nll_loss 6.258 | ppl 76.55 | wps 66326 | wpb 2034.1 | bsz 4 | num_updates 13335 | best_loss 7.641
2022-03-03 20:31:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13335 updates
2022-03-03 20:31:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 20:31:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 20:31:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 34 @ 13335 updates, score 7.859) (writing took 4.196475575212389 seconds)
2022-03-03 20:31:52 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-03 20:31:52 | INFO | train | epoch 034 | loss 6.241 | nll_loss 4.482 | ppl 22.35 | wps 25211.2 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 13335 | lr 0.000273844 | gnorm 0.626 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 34692
2022-03-03 20:31:52 | INFO | fairseq.trainer | begin training epoch 35
2022-03-03 20:31:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:34:40 | INFO | train_inner | epoch 035:     65 / 393 loss=6.202, nll_loss=4.437, ppl=21.67, wps=24487.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.621, loss_scale=8, train_wall=254, gb_free=12.3, wall=34859
2022-03-03 20:38:57 | INFO | train_inner | epoch 035:    165 / 393 loss=6.177, nll_loss=4.408, ppl=21.23, wps=25471.5, ups=0.39, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.624, loss_scale=8, train_wall=252, gb_free=12.3, wall=35117
2022-03-03 20:43:14 | INFO | train_inner | epoch 035:    265 / 393 loss=6.228, nll_loss=4.466, ppl=22.11, wps=25463, ups=0.39, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.64, loss_scale=8, train_wall=252, gb_free=12.3, wall=35374
2022-03-03 20:47:32 | INFO | train_inner | epoch 035:    365 / 393 loss=6.277, nll_loss=4.523, ppl=22.99, wps=25470.2, ups=0.39, wpb=65530.2, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.632, loss_scale=8, train_wall=252, gb_free=12.3, wall=35631
2022-03-03 20:48:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:48:46 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.851 | nll_loss 6.252 | ppl 76.24 | wps 66352.1 | wpb 2034.1 | bsz 4 | num_updates 13728 | best_loss 7.641
2022-03-03 20:48:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13728 updates
2022-03-03 20:48:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 20:48:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 20:48:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 35 @ 13728 updates, score 7.851) (writing took 4.189158685039729 seconds)
2022-03-03 20:48:50 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-03 20:48:50 | INFO | train | epoch 035 | loss 6.218 | nll_loss 4.455 | ppl 21.94 | wps 25275.6 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 13728 | lr 0.000269896 | gnorm 0.631 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 35710
2022-03-03 20:48:50 | INFO | fairseq.trainer | begin training epoch 36
2022-03-03 20:48:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:51:55 | INFO | train_inner | epoch 036:     72 / 393 loss=6.17, nll_loss=4.4, ppl=21.11, wps=24739.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.64, loss_scale=8, train_wall=251, gb_free=12.3, wall=35895
2022-03-03 20:53:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:56:15 | INFO | train_inner | epoch 036:    173 / 393 loss=6.164, nll_loss=4.392, ppl=21, wps=25228, ups=0.38, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.639, loss_scale=8, train_wall=255, gb_free=12.3, wall=36155
2022-03-03 21:00:32 | INFO | train_inner | epoch 036:    273 / 393 loss=6.21, nll_loss=4.446, ppl=21.79, wps=25484.3, ups=0.39, wpb=65530.2, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.647, loss_scale=8, train_wall=252, gb_free=12.3, wall=36412
2022-03-03 21:04:50 | INFO | train_inner | epoch 036:    373 / 393 loss=6.247, nll_loss=4.489, ppl=22.45, wps=25468, ups=0.39, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.641, loss_scale=8, train_wall=252, gb_free=12.3, wall=36669
2022-03-03 21:05:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:05:43 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.855 | nll_loss 6.269 | ppl 77.14 | wps 66867.1 | wpb 2034.1 | bsz 4 | num_updates 14120 | best_loss 7.641
2022-03-03 21:05:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14120 updates
2022-03-03 21:05:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 21:05:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 21:05:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 36 @ 14120 updates, score 7.855) (writing took 4.155633748974651 seconds)
2022-03-03 21:05:47 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-03 21:05:47 | INFO | train | epoch 036 | loss 6.196 | nll_loss 4.429 | ppl 21.54 | wps 25223 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 14120 | lr 0.000266123 | gnorm 0.642 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 36727
2022-03-03 21:05:47 | INFO | fairseq.trainer | begin training epoch 37
2022-03-03 21:05:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:09:13 | INFO | train_inner | epoch 037:     80 / 393 loss=6.142, nll_loss=4.366, ppl=20.63, wps=24740.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.65, loss_scale=8, train_wall=251, gb_free=12.3, wall=36933
2022-03-03 21:13:31 | INFO | train_inner | epoch 037:    180 / 393 loss=6.147, nll_loss=4.372, ppl=20.71, wps=25472.3, ups=0.39, wpb=65535.4, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.653, loss_scale=8, train_wall=252, gb_free=12.3, wall=37190
2022-03-03 21:17:48 | INFO | train_inner | epoch 037:    280 / 393 loss=6.194, nll_loss=4.427, ppl=21.51, wps=25477.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.641, loss_scale=16, train_wall=252, gb_free=12.3, wall=37448
2022-03-03 21:18:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 21:22:08 | INFO | train_inner | epoch 037:    381 / 393 loss=6.23, nll_loss=4.468, ppl=22.13, wps=25221.7, ups=0.38, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.651, loss_scale=8, train_wall=255, gb_free=12.3, wall=37707
2022-03-03 21:22:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:22:41 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.886 | nll_loss 6.294 | ppl 78.49 | wps 66499.5 | wpb 2034.1 | bsz 4 | num_updates 14512 | best_loss 7.641
2022-03-03 21:22:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14512 updates
2022-03-03 21:22:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 21:22:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 21:22:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 37 @ 14512 updates, score 7.886) (writing took 4.199972507078201 seconds)
2022-03-03 21:22:45 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-03 21:22:45 | INFO | train | epoch 037 | loss 6.175 | nll_loss 4.405 | ppl 21.18 | wps 25218.9 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 14512 | lr 0.000262504 | gnorm 0.648 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 37745
2022-03-03 21:22:45 | INFO | fairseq.trainer | begin training epoch 38
2022-03-03 21:22:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:26:31 | INFO | train_inner | epoch 038:     88 / 393 loss=6.099, nll_loss=4.317, ppl=19.93, wps=24730, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.656, loss_scale=8, train_wall=251, gb_free=12.3, wall=37971
2022-03-03 21:30:49 | INFO | train_inner | epoch 038:    188 / 393 loss=6.132, nll_loss=4.355, ppl=20.47, wps=25472.8, ups=0.39, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.651, loss_scale=8, train_wall=252, gb_free=12.3, wall=38229
2022-03-03 21:35:06 | INFO | train_inner | epoch 038:    288 / 393 loss=6.18, nll_loss=4.41, ppl=21.26, wps=25474.2, ups=0.39, wpb=65530.2, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.657, loss_scale=8, train_wall=252, gb_free=12.3, wall=38486
2022-03-03 21:39:23 | INFO | train_inner | epoch 038:    388 / 393 loss=6.216, nll_loss=4.452, ppl=21.89, wps=25470.5, ups=0.39, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.663, loss_scale=8, train_wall=252, gb_free=12.3, wall=38743
2022-03-03 21:39:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:39:38 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.89 | nll_loss 6.304 | ppl 79.03 | wps 66384.3 | wpb 2034.1 | bsz 4 | num_updates 14905 | best_loss 7.641
2022-03-03 21:39:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14905 updates
2022-03-03 21:39:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 21:39:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 21:39:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 38 @ 14905 updates, score 7.89) (writing took 4.343065275810659 seconds)
2022-03-03 21:39:43 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-03 21:39:43 | INFO | train | epoch 038 | loss 6.155 | nll_loss 4.381 | ppl 20.84 | wps 25275.7 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 14905 | lr 0.00025902 | gnorm 0.656 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 38763
2022-03-03 21:39:43 | INFO | fairseq.trainer | begin training epoch 39
2022-03-03 21:39:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:41:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 21:43:50 | INFO | train_inner | epoch 039:     96 / 393 loss=6.08, nll_loss=4.295, ppl=19.63, wps=24471.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.651, loss_scale=8, train_wall=254, gb_free=12.3, wall=39010
2022-03-03 21:48:07 | INFO | train_inner | epoch 039:    196 / 393 loss=6.112, nll_loss=4.332, ppl=20.13, wps=25477.6, ups=0.39, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.661, loss_scale=8, train_wall=252, gb_free=12.3, wall=39267
2022-03-03 21:52:24 | INFO | train_inner | epoch 039:    296 / 393 loss=6.154, nll_loss=4.38, ppl=20.82, wps=25477.4, ups=0.39, wpb=65530.2, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.669, loss_scale=8, train_wall=252, gb_free=12.3, wall=39524
2022-03-03 21:56:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:56:36 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.909 | nll_loss 6.322 | ppl 79.99 | wps 66588.6 | wpb 2034.1 | bsz 4 | num_updates 15297 | best_loss 7.641
2022-03-03 21:56:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15297 updates
2022-03-03 21:56:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 21:56:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 21:56:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 39 @ 15297 updates, score 7.909) (writing took 4.255133656319231 seconds)
2022-03-03 21:56:40 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-03 21:56:40 | INFO | train | epoch 039 | loss 6.134 | nll_loss 4.357 | ppl 20.5 | wps 25216.3 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 15297 | lr 0.00025568 | gnorm 0.659 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 39780
2022-03-03 21:56:40 | INFO | fairseq.trainer | begin training epoch 40
2022-03-03 21:56:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:56:48 | INFO | train_inner | epoch 040:      3 / 393 loss=6.195, nll_loss=4.427, ppl=21.52, wps=24733.7, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.656, loss_scale=8, train_wall=251, gb_free=12.3, wall=39788
2022-03-03 22:01:05 | INFO | train_inner | epoch 040:    103 / 393 loss=6.048, nll_loss=4.257, ppl=19.12, wps=25479.2, ups=0.39, wpb=65535.4, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.655, loss_scale=8, train_wall=252, gb_free=12.3, wall=40045
2022-03-03 22:05:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 22:05:25 | INFO | train_inner | epoch 040:    204 / 393 loss=6.102, nll_loss=4.32, ppl=19.97, wps=25224.3, ups=0.38, wpb=65530.9, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.66, loss_scale=8, train_wall=255, gb_free=12.3, wall=40305
2022-03-03 22:09:43 | INFO | train_inner | epoch 040:    304 / 393 loss=6.142, nll_loss=4.366, ppl=20.63, wps=25452.2, ups=0.39, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.671, loss_scale=8, train_wall=253, gb_free=12.3, wall=40562
2022-03-03 22:13:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:13:34 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.899 | nll_loss 6.308 | ppl 79.25 | wps 66348 | wpb 2034.1 | bsz 4 | num_updates 15689 | best_loss 7.641
2022-03-03 22:13:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15689 updates
2022-03-03 22:13:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 22:13:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 22:13:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 40 @ 15689 updates, score 7.899) (writing took 4.2552952552214265 seconds)
2022-03-03 22:13:38 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-03 22:13:38 | INFO | train | epoch 040 | loss 6.117 | nll_loss 4.337 | ppl 20.21 | wps 25205.6 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 15689 | lr 0.000252466 | gnorm 0.664 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 40798
2022-03-03 22:13:38 | INFO | fairseq.trainer | begin training epoch 41
2022-03-03 22:13:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:14:07 | INFO | train_inner | epoch 041:     11 / 393 loss=6.167, nll_loss=4.394, ppl=21.03, wps=24701, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.671, loss_scale=8, train_wall=252, gb_free=12.3, wall=40827
2022-03-03 22:18:24 | INFO | train_inner | epoch 041:    111 / 393 loss=6.033, nll_loss=4.24, ppl=18.89, wps=25469.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.67, loss_scale=8, train_wall=252, gb_free=12.3, wall=41084
2022-03-03 22:22:41 | INFO | train_inner | epoch 041:    211 / 393 loss=6.079, nll_loss=4.293, ppl=19.61, wps=25481.7, ups=0.39, wpb=65530.9, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.673, loss_scale=8, train_wall=252, gb_free=12.3, wall=41341
2022-03-03 22:26:59 | INFO | train_inner | epoch 041:    311 / 393 loss=6.131, nll_loss=4.352, ppl=20.43, wps=25468.9, ups=0.39, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.664, loss_scale=8, train_wall=252, gb_free=12.3, wall=41598
2022-03-03 22:30:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:30:32 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.916 | nll_loss 6.336 | ppl 80.77 | wps 66584.1 | wpb 2034.1 | bsz 4 | num_updates 16082 | best_loss 7.641
2022-03-03 22:30:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16082 updates
2022-03-03 22:30:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 22:30:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 22:30:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 41 @ 16082 updates, score 7.916) (writing took 4.236661822069436 seconds)
2022-03-03 22:30:36 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-03 22:30:36 | INFO | train | epoch 041 | loss 6.099 | nll_loss 4.317 | ppl 19.93 | wps 25284.2 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 16082 | lr 0.000249362 | gnorm 0.67 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 41816
2022-03-03 22:30:36 | INFO | fairseq.trainer | begin training epoch 42
2022-03-03 22:30:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:31:22 | INFO | train_inner | epoch 042:     18 / 393 loss=6.146, nll_loss=4.371, ppl=20.69, wps=24742.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.675, loss_scale=16, train_wall=251, gb_free=12.3, wall=41862
2022-03-03 22:31:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 22:35:42 | INFO | train_inner | epoch 042:    119 / 393 loss=6.023, nll_loss=4.228, ppl=18.74, wps=25221.1, ups=0.38, wpb=65535.4, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.677, loss_scale=8, train_wall=255, gb_free=12.3, wall=42122
2022-03-03 22:39:59 | INFO | train_inner | epoch 042:    219 / 393 loss=6.07, nll_loss=4.283, ppl=19.47, wps=25470.4, ups=0.39, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.668, loss_scale=8, train_wall=252, gb_free=12.3, wall=42379
2022-03-03 22:44:17 | INFO | train_inner | epoch 042:    319 / 393 loss=6.112, nll_loss=4.331, ppl=20.13, wps=25481.1, ups=0.39, wpb=65530.9, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.685, loss_scale=8, train_wall=252, gb_free=12.3, wall=42636
2022-03-03 22:47:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:47:29 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.939 | nll_loss 6.349 | ppl 81.53 | wps 66518.1 | wpb 2034.1 | bsz 4 | num_updates 16474 | best_loss 7.641
2022-03-03 22:47:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16474 updates
2022-03-03 22:47:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 22:47:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 22:47:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 42 @ 16474 updates, score 7.939) (writing took 4.271773299202323 seconds)
2022-03-03 22:47:33 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-03 22:47:33 | INFO | train | epoch 042 | loss 6.082 | nll_loss 4.297 | ppl 19.65 | wps 25217.4 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 16474 | lr 0.000246377 | gnorm 0.679 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 42833
2022-03-03 22:47:33 | INFO | fairseq.trainer | begin training epoch 43
2022-03-03 22:47:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:48:40 | INFO | train_inner | epoch 043:     26 / 393 loss=6.112, nll_loss=4.331, ppl=20.13, wps=24734, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.685, loss_scale=8, train_wall=251, gb_free=12.3, wall=42900
2022-03-03 22:52:58 | INFO | train_inner | epoch 043:    126 / 393 loss=6.008, nll_loss=4.21, ppl=18.51, wps=25467.2, ups=0.39, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.683, loss_scale=8, train_wall=252, gb_free=12.3, wall=43158
2022-03-03 22:56:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 22:57:18 | INFO | train_inner | epoch 043:    227 / 393 loss=6.056, nll_loss=4.266, ppl=19.24, wps=25217.8, ups=0.38, wpb=65530.9, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.673, loss_scale=8, train_wall=255, gb_free=12.3, wall=43417
2022-03-03 23:01:35 | INFO | train_inner | epoch 043:    327 / 393 loss=6.107, nll_loss=4.325, ppl=20.05, wps=25473.9, ups=0.39, wpb=65535.4, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.681, loss_scale=8, train_wall=252, gb_free=12.3, wall=43675
2022-03-03 23:04:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:04:27 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.962 | nll_loss 6.38 | ppl 83.26 | wps 66492.2 | wpb 2034.1 | bsz 4 | num_updates 16866 | best_loss 7.641
2022-03-03 23:04:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16866 updates
2022-03-03 23:04:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 23:04:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 23:04:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 43 @ 16866 updates, score 7.962) (writing took 4.28048627730459 seconds)
2022-03-03 23:04:31 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-03 23:04:31 | INFO | train | epoch 043 | loss 6.066 | nll_loss 4.278 | ppl 19.4 | wps 25215.7 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 16866 | lr 0.000243497 | gnorm 0.679 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 43851
2022-03-03 23:04:31 | INFO | fairseq.trainer | begin training epoch 44
2022-03-03 23:04:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:05:59 | INFO | train_inner | epoch 044:     34 / 393 loss=6.082, nll_loss=4.297, ppl=19.65, wps=24727.7, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.697, loss_scale=8, train_wall=251, gb_free=12.3, wall=43939
2022-03-03 23:10:16 | INFO | train_inner | epoch 044:    134 / 393 loss=5.996, nll_loss=4.197, ppl=18.34, wps=25473.7, ups=0.39, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.675, loss_scale=8, train_wall=252, gb_free=12.3, wall=44196
2022-03-03 23:14:33 | INFO | train_inner | epoch 044:    234 / 393 loss=6.054, nll_loss=4.263, ppl=19.2, wps=25456.6, ups=0.39, wpb=65530.9, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.678, loss_scale=8, train_wall=253, gb_free=12.3, wall=44453
2022-03-03 23:18:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 23:18:53 | INFO | train_inner | epoch 044:    335 / 393 loss=6.09, nll_loss=4.305, ppl=19.77, wps=25220.1, ups=0.38, wpb=65535.4, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.683, loss_scale=8, train_wall=255, gb_free=12.3, wall=44713
2022-03-03 23:21:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:21:25 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.972 | nll_loss 6.383 | ppl 83.49 | wps 66677.9 | wpb 2034.1 | bsz 4 | num_updates 17258 | best_loss 7.641
2022-03-03 23:21:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17258 updates
2022-03-03 23:21:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 23:21:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 23:21:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 44 @ 17258 updates, score 7.972) (writing took 4.201405663974583 seconds)
2022-03-03 23:21:29 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-03 23:21:29 | INFO | train | epoch 044 | loss 6.051 | nll_loss 4.26 | ppl 19.16 | wps 25211.9 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 17258 | lr 0.000240716 | gnorm 0.684 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 44869
2022-03-03 23:21:29 | INFO | fairseq.trainer | begin training epoch 45
2022-03-03 23:21:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:23:17 | INFO | train_inner | epoch 045:     42 / 393 loss=6.049, nll_loss=4.258, ppl=19.14, wps=24735.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.691, loss_scale=8, train_wall=251, gb_free=12.3, wall=44977
2022-03-03 23:27:34 | INFO | train_inner | epoch 045:    142 / 393 loss=5.985, nll_loss=4.184, ppl=18.18, wps=25463.4, ups=0.39, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.678, loss_scale=8, train_wall=252, gb_free=12.3, wall=45234
2022-03-03 23:31:52 | INFO | train_inner | epoch 045:    242 / 393 loss=6.044, nll_loss=4.252, ppl=19.05, wps=25472.3, ups=0.39, wpb=65535.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.694, loss_scale=8, train_wall=252, gb_free=12.3, wall=45492
2022-03-03 23:34:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 23:36:12 | INFO | train_inner | epoch 045:    343 / 393 loss=6.082, nll_loss=4.295, ppl=19.63, wps=25219.3, ups=0.38, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.709, loss_scale=4, train_wall=255, gb_free=12.3, wall=45751
2022-03-03 23:38:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:38:22 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.976 | nll_loss 6.384 | ppl 83.53 | wps 66360.3 | wpb 2034.1 | bsz 4 | num_updates 17650 | best_loss 7.641
2022-03-03 23:38:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17650 updates
2022-03-03 23:38:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 23:38:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 23:38:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 45 @ 17650 updates, score 7.976) (writing took 4.256139336153865 seconds)
2022-03-03 23:38:27 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-03 23:38:27 | INFO | train | epoch 045 | loss 6.037 | nll_loss 4.243 | ppl 18.94 | wps 25213.8 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 17650 | lr 0.000238028 | gnorm 0.695 | loss_scale 4 | train_wall 991 | gb_free 12.3 | wall 45887
2022-03-03 23:38:27 | INFO | fairseq.trainer | begin training epoch 46
2022-03-03 23:38:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:40:35 | INFO | train_inner | epoch 046:     50 / 393 loss=6.02, nll_loss=4.224, ppl=18.69, wps=24733.6, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.697, loss_scale=4, train_wall=251, gb_free=12.3, wall=46015
2022-03-03 23:44:53 | INFO | train_inner | epoch 046:    150 / 393 loss=5.981, nll_loss=4.179, ppl=18.11, wps=25470.1, ups=0.39, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.697, loss_scale=4, train_wall=252, gb_free=12.3, wall=46273
2022-03-03 23:49:10 | INFO | train_inner | epoch 046:    250 / 393 loss=6.032, nll_loss=4.238, ppl=18.87, wps=25465.5, ups=0.39, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.704, loss_scale=4, train_wall=252, gb_free=12.3, wall=46530
2022-03-03 23:53:27 | INFO | train_inner | epoch 046:    350 / 393 loss=6.072, nll_loss=4.284, ppl=19.49, wps=25459.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.698, loss_scale=4, train_wall=253, gb_free=12.3, wall=46787
2022-03-03 23:55:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:55:20 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.993 | nll_loss 6.403 | ppl 84.6 | wps 66515 | wpb 2034.1 | bsz 4 | num_updates 18043 | best_loss 7.641
2022-03-03 23:55:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18043 updates
2022-03-03 23:55:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 23:55:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-03 23:55:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 46 @ 18043 updates, score 7.993) (writing took 4.291054689325392 seconds)
2022-03-03 23:55:25 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-03 23:55:25 | INFO | train | epoch 046 | loss 6.024 | nll_loss 4.228 | ppl 18.74 | wps 25273.8 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 18043 | lr 0.000235421 | gnorm 0.699 | loss_scale 4 | train_wall 991 | gb_free 12.3 | wall 46904
2022-03-03 23:55:25 | INFO | fairseq.trainer | begin training epoch 47
2022-03-03 23:55:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:57:51 | INFO | train_inner | epoch 047:     57 / 393 loss=6.003, nll_loss=4.204, ppl=18.43, wps=24734.6, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.7, loss_scale=8, train_wall=251, gb_free=12.3, wall=47051
2022-03-04 00:02:09 | INFO | train_inner | epoch 047:    157 / 393 loss=5.968, nll_loss=4.163, ppl=17.92, wps=25466.3, ups=0.39, wpb=65535.4, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.694, loss_scale=8, train_wall=252, gb_free=12.3, wall=47308
2022-03-04 00:06:26 | INFO | train_inner | epoch 047:    257 / 393 loss=6.017, nll_loss=4.22, ppl=18.63, wps=25466.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.699, loss_scale=8, train_wall=252, gb_free=12.3, wall=47566
2022-03-04 00:10:43 | INFO | train_inner | epoch 047:    357 / 393 loss=6.063, nll_loss=4.273, ppl=19.34, wps=25476.9, ups=0.39, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.699, loss_scale=8, train_wall=252, gb_free=12.3, wall=47823
2022-03-04 00:12:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:12:18 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.987 | nll_loss 6.407 | ppl 84.89 | wps 66404.8 | wpb 2034.1 | bsz 4 | num_updates 18436 | best_loss 7.641
2022-03-04 00:12:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18436 updates
2022-03-04 00:12:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 00:12:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 00:12:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 47 @ 18436 updates, score 7.987) (writing took 4.281582036986947 seconds)
2022-03-04 00:12:22 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 00:12:22 | INFO | train | epoch 047 | loss 6.011 | nll_loss 4.213 | ppl 18.55 | wps 25280.1 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 18436 | lr 0.000232898 | gnorm 0.699 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 47922
2022-03-04 00:12:22 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 00:12:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:15:07 | INFO | train_inner | epoch 048:     64 / 393 loss=5.981, nll_loss=4.179, ppl=18.11, wps=24740.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.708, loss_scale=8, train_wall=251, gb_free=12.3, wall=48087
2022-03-04 00:19:24 | INFO | train_inner | epoch 048:    164 / 393 loss=5.966, nll_loss=4.162, ppl=17.9, wps=25479.5, ups=0.39, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.716, loss_scale=16, train_wall=252, gb_free=12.3, wall=48344
2022-03-04 00:19:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 00:23:44 | INFO | train_inner | epoch 048:    265 / 393 loss=6.01, nll_loss=4.212, ppl=18.53, wps=25229.5, ups=0.39, wpb=65530.9, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.699, loss_scale=8, train_wall=255, gb_free=12.3, wall=48604
2022-03-04 00:28:01 | INFO | train_inner | epoch 048:    365 / 393 loss=6.047, nll_loss=4.255, ppl=19.09, wps=25453.9, ups=0.39, wpb=65535.4, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.707, loss_scale=8, train_wall=253, gb_free=12.3, wall=48861
2022-03-04 00:29:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:29:15 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.003 | nll_loss 6.418 | ppl 85.52 | wps 66702 | wpb 2034.1 | bsz 4 | num_updates 18828 | best_loss 7.641
2022-03-04 00:29:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18828 updates
2022-03-04 00:29:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 00:29:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 00:29:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 48 @ 18828 updates, score 8.003) (writing took 4.311069136019796 seconds)
2022-03-04 00:29:20 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 00:29:20 | INFO | train | epoch 048 | loss 5.998 | nll_loss 4.198 | ppl 18.35 | wps 25217.1 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 18828 | lr 0.000230461 | gnorm 0.706 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 48940
2022-03-04 00:29:20 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 00:29:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:32:25 | INFO | train_inner | epoch 049:     72 / 393 loss=5.952, nll_loss=4.145, ppl=17.69, wps=24738.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.72, loss_scale=8, train_wall=251, gb_free=12.3, wall=49125
2022-03-04 00:36:42 | INFO | train_inner | epoch 049:    172 / 393 loss=5.955, nll_loss=4.148, ppl=17.73, wps=25470.7, ups=0.39, wpb=65530.2, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.704, loss_scale=8, train_wall=252, gb_free=12.3, wall=49382
2022-03-04 00:41:00 | INFO | train_inner | epoch 049:    272 / 393 loss=5.992, nll_loss=4.192, ppl=18.27, wps=25480.9, ups=0.39, wpb=65536, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.707, loss_scale=8, train_wall=252, gb_free=12.3, wall=49639
2022-03-04 00:42:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 00:45:19 | INFO | train_inner | epoch 049:    373 / 393 loss=6.051, nll_loss=4.259, ppl=19.14, wps=25214, ups=0.38, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.714, loss_scale=8, train_wall=255, gb_free=12.3, wall=49899
2022-03-04 00:46:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:46:13 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.005 | nll_loss 6.423 | ppl 85.82 | wps 66632.2 | wpb 2034.1 | bsz 4 | num_updates 19220 | best_loss 7.641
2022-03-04 00:46:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19220 updates
2022-03-04 00:46:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 00:46:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 00:46:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 49 @ 19220 updates, score 8.005) (writing took 4.265902844257653 seconds)
2022-03-04 00:46:17 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 00:46:17 | INFO | train | epoch 049 | loss 5.986 | nll_loss 4.184 | ppl 18.17 | wps 25219.2 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 19220 | lr 0.000228099 | gnorm 0.711 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 49957
2022-03-04 00:46:17 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 00:46:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:49:43 | INFO | train_inner | epoch 050:     80 / 393 loss=5.937, nll_loss=4.127, ppl=17.48, wps=24735, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=19300, lr=0.000227626, gnorm=0.712, loss_scale=8, train_wall=251, gb_free=12.3, wall=50163
2022-03-04 00:54:01 | INFO | train_inner | epoch 050:    180 / 393 loss=5.941, nll_loss=4.132, ppl=17.53, wps=25467.5, ups=0.39, wpb=65535.4, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.709, loss_scale=8, train_wall=252, gb_free=12.3, wall=50420
2022-03-04 00:58:18 | INFO | train_inner | epoch 050:    280 / 393 loss=5.999, nll_loss=4.199, ppl=18.37, wps=25481.5, ups=0.39, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.723, loss_scale=8, train_wall=252, gb_free=12.3, wall=50678
2022-03-04 01:02:35 | INFO | train_inner | epoch 050:    380 / 393 loss=6.034, nll_loss=4.239, ppl=18.88, wps=25480.6, ups=0.39, wpb=65530.9, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.723, loss_scale=8, train_wall=252, gb_free=12.3, wall=50935
2022-03-04 01:03:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:03:11 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.02 | nll_loss 6.44 | ppl 86.83 | wps 66821.5 | wpb 2034.1 | bsz 4 | num_updates 19613 | best_loss 7.641
2022-03-04 01:03:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19613 updates
2022-03-04 01:03:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 01:03:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 01:03:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 50 @ 19613 updates, score 8.02) (writing took 4.3059788211248815 seconds)
2022-03-04 01:03:15 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 01:03:15 | INFO | train | epoch 050 | loss 5.975 | nll_loss 4.171 | ppl 18.02 | wps 25283.6 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 19613 | lr 0.000225802 | gnorm 0.718 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 50975
2022-03-04 01:03:15 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 01:03:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:06:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 01:07:01 | INFO | train_inner | epoch 051:     88 / 393 loss=5.907, nll_loss=4.093, ppl=17.07, wps=24493, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.716, loss_scale=8, train_wall=254, gb_free=12.3, wall=51201
2022-03-04 01:11:19 | INFO | train_inner | epoch 051:    188 / 393 loss=5.935, nll_loss=4.124, ppl=17.44, wps=25468.1, ups=0.39, wpb=65530.9, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.713, loss_scale=8, train_wall=252, gb_free=12.3, wall=51458
2022-03-04 01:15:36 | INFO | train_inner | epoch 051:    288 / 393 loss=5.985, nll_loss=4.182, ppl=18.15, wps=25474.3, ups=0.39, wpb=65535.4, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.734, loss_scale=8, train_wall=252, gb_free=12.3, wall=51716
2022-03-04 01:19:53 | INFO | train_inner | epoch 051:    388 / 393 loss=6.034, nll_loss=4.239, ppl=18.88, wps=25481.8, ups=0.39, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.723, loss_scale=8, train_wall=252, gb_free=12.3, wall=51973
2022-03-04 01:20:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:20:08 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 8.015 | nll_loss 6.428 | ppl 86.13 | wps 66375.2 | wpb 2034.1 | bsz 4 | num_updates 20005 | best_loss 7.641
2022-03-04 01:20:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20005 updates
2022-03-04 01:20:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 01:20:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 01:20:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 51 @ 20005 updates, score 8.015) (writing took 4.207232520915568 seconds)
2022-03-04 01:20:12 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 01:20:12 | INFO | train | epoch 051 | loss 5.963 | nll_loss 4.158 | ppl 17.85 | wps 25218.8 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 20005 | lr 0.000223579 | gnorm 0.722 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 51992
2022-03-04 01:20:12 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 01:20:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:24:17 | INFO | train_inner | epoch 052:     95 / 393 loss=5.889, nll_loss=4.071, ppl=16.81, wps=24728.6, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=20100, lr=0.00022305, gnorm=0.713, loss_scale=8, train_wall=251, gb_free=12.3, wall=52237
2022-03-04 01:28:34 | INFO | train_inner | epoch 052:    195 / 393 loss=5.929, nll_loss=4.117, ppl=17.36, wps=25471.6, ups=0.39, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.717, loss_scale=8, train_wall=252, gb_free=12.3, wall=52494
2022-03-04 01:29:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 01:32:54 | INFO | train_inner | epoch 052:    296 / 393 loss=5.973, nll_loss=4.169, ppl=17.99, wps=25225.6, ups=0.38, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.726, loss_scale=8, train_wall=255, gb_free=12.3, wall=52754
2022-03-04 01:37:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:37:06 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 8.027 | nll_loss 6.445 | ppl 87.11 | wps 67038.7 | wpb 2034.1 | bsz 4 | num_updates 20397 | best_loss 7.641
2022-03-04 01:37:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20397 updates
2022-03-04 01:37:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 01:37:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 01:37:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 52 @ 20397 updates, score 8.027) (writing took 4.251310113817453 seconds)
2022-03-04 01:37:10 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 01:37:10 | INFO | train | epoch 052 | loss 5.953 | nll_loss 4.145 | ppl 17.7 | wps 25219.2 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 20397 | lr 0.00022142 | gnorm 0.719 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 53010
2022-03-04 01:37:10 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 01:37:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:37:18 | INFO | train_inner | epoch 053:      3 / 393 loss=6.022, nll_loss=4.225, ppl=18.7, wps=24744.9, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.724, loss_scale=8, train_wall=251, gb_free=12.3, wall=53018
2022-03-04 01:41:35 | INFO | train_inner | epoch 053:    103 / 393 loss=5.88, nll_loss=4.061, ppl=16.69, wps=25439.4, ups=0.39, wpb=65530.2, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.733, loss_scale=8, train_wall=253, gb_free=12.3, wall=53275
2022-03-04 01:45:52 | INFO | train_inner | epoch 053:    203 / 393 loss=5.921, nll_loss=4.109, ppl=17.25, wps=25485.7, ups=0.39, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.731, loss_scale=8, train_wall=252, gb_free=12.3, wall=53532
2022-03-04 01:50:10 | INFO | train_inner | epoch 053:    303 / 393 loss=5.963, nll_loss=4.158, ppl=17.85, wps=25477.1, ups=0.39, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.732, loss_scale=8, train_wall=252, gb_free=12.3, wall=53790
2022-03-04 01:53:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 01:54:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:54:03 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 8.048 | nll_loss 6.463 | ppl 88.22 | wps 66536.1 | wpb 2034.1 | bsz 4 | num_updates 20789 | best_loss 7.641
2022-03-04 01:54:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20789 updates
2022-03-04 01:54:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 01:54:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 01:54:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 53 @ 20789 updates, score 8.048) (writing took 4.20829593623057 seconds)
2022-03-04 01:54:08 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 01:54:08 | INFO | train | epoch 053 | loss 5.942 | nll_loss 4.133 | ppl 17.55 | wps 25216.5 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 20789 | lr 0.000219323 | gnorm 0.733 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 54027
2022-03-04 01:54:08 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 01:54:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:54:36 | INFO | train_inner | epoch 054:     11 / 393 loss=5.998, nll_loss=4.198, ppl=18.35, wps=24508.7, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=20800, lr=0.000219265, gnorm=0.736, loss_scale=8, train_wall=254, gb_free=12.3, wall=54056
2022-03-04 01:58:53 | INFO | train_inner | epoch 054:    111 / 393 loss=5.867, nll_loss=4.046, ppl=16.52, wps=25483.1, ups=0.39, wpb=65530.9, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.716, loss_scale=8, train_wall=252, gb_free=12.3, wall=54313
2022-03-04 02:03:10 | INFO | train_inner | epoch 054:    211 / 393 loss=5.918, nll_loss=4.105, ppl=17.21, wps=25470.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.742, loss_scale=8, train_wall=252, gb_free=12.3, wall=54570
2022-03-04 02:07:28 | INFO | train_inner | epoch 054:    311 / 393 loss=5.963, nll_loss=4.156, ppl=17.83, wps=25482.7, ups=0.39, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.747, loss_scale=8, train_wall=252, gb_free=12.3, wall=54827
2022-03-04 02:10:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:11:01 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 8.057 | nll_loss 6.476 | ppl 88.99 | wps 66296.6 | wpb 2034.1 | bsz 4 | num_updates 21182 | best_loss 7.641
2022-03-04 02:11:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21182 updates
2022-03-04 02:11:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 02:11:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 02:11:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 54 @ 21182 updates, score 8.057) (writing took 4.3118842276744545 seconds)
2022-03-04 02:11:05 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 02:11:05 | INFO | train | epoch 054 | loss 5.933 | nll_loss 4.123 | ppl 17.42 | wps 25280.1 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 21182 | lr 0.000217278 | gnorm 0.737 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 55045
2022-03-04 02:11:05 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 02:11:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:11:52 | INFO | train_inner | epoch 055:     18 / 393 loss=5.974, nll_loss=4.17, ppl=18, wps=24713.8, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.739, loss_scale=8, train_wall=251, gb_free=12.3, wall=55091
2022-03-04 02:16:09 | INFO | train_inner | epoch 055:    118 / 393 loss=5.862, nll_loss=4.039, ppl=16.44, wps=25458.6, ups=0.39, wpb=65530.9, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.724, loss_scale=16, train_wall=253, gb_free=12.3, wall=55349
2022-03-04 02:16:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 02:20:29 | INFO | train_inner | epoch 055:    219 / 393 loss=5.911, nll_loss=4.096, ppl=17.1, wps=25211, ups=0.38, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.727, loss_scale=8, train_wall=255, gb_free=12.3, wall=55609
2022-03-04 02:24:46 | INFO | train_inner | epoch 055:    319 / 393 loss=5.955, nll_loss=4.148, ppl=17.72, wps=25467.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.739, loss_scale=8, train_wall=252, gb_free=12.3, wall=55866
2022-03-04 02:27:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:27:59 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 8.053 | nll_loss 6.473 | ppl 88.81 | wps 66497.2 | wpb 2034.1 | bsz 4 | num_updates 21574 | best_loss 7.641
2022-03-04 02:27:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 21574 updates
2022-03-04 02:27:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 02:28:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 02:28:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 55 @ 21574 updates, score 8.053) (writing took 4.213557067792863 seconds)
2022-03-04 02:28:03 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 02:28:03 | INFO | train | epoch 055 | loss 5.923 | nll_loss 4.11 | ppl 17.27 | wps 25210.1 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 21574 | lr 0.000215295 | gnorm 0.731 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 56063
2022-03-04 02:28:03 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 02:28:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:29:10 | INFO | train_inner | epoch 056:     26 / 393 loss=5.96, nll_loss=4.153, ppl=17.79, wps=24733.3, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=21600, lr=0.000215166, gnorm=0.741, loss_scale=8, train_wall=251, gb_free=12.3, wall=56130
2022-03-04 02:33:27 | INFO | train_inner | epoch 056:    126 / 393 loss=5.856, nll_loss=4.033, ppl=16.37, wps=25479.5, ups=0.39, wpb=65530.2, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.728, loss_scale=8, train_wall=252, gb_free=12.3, wall=56387
2022-03-04 02:37:44 | INFO | train_inner | epoch 056:    226 / 393 loss=5.907, nll_loss=4.091, ppl=17.04, wps=25479.5, ups=0.39, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.753, loss_scale=8, train_wall=252, gb_free=12.3, wall=56644
2022-03-04 02:39:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 02:42:04 | INFO | train_inner | epoch 056:    327 / 393 loss=5.957, nll_loss=4.15, ppl=17.75, wps=25228.6, ups=0.38, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.743, loss_scale=8, train_wall=255, gb_free=12.3, wall=56904
2022-03-04 02:44:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:44:56 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 8.066 | nll_loss 6.493 | ppl 90.05 | wps 66494.8 | wpb 2034.1 | bsz 4 | num_updates 21966 | best_loss 7.641
2022-03-04 02:44:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 21966 updates
2022-03-04 02:44:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 02:45:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 02:45:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 56 @ 21966 updates, score 8.066) (writing took 4.247798709664494 seconds)
2022-03-04 02:45:00 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 02:45:00 | INFO | train | epoch 056 | loss 5.915 | nll_loss 4.101 | ppl 17.16 | wps 25221.3 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 21966 | lr 0.000213366 | gnorm 0.743 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 57080
2022-03-04 02:45:00 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 02:45:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:46:28 | INFO | train_inner | epoch 057:     34 / 393 loss=5.932, nll_loss=4.121, ppl=17.4, wps=24721.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=22000, lr=0.000213201, gnorm=0.735, loss_scale=8, train_wall=251, gb_free=12.3, wall=57168
2022-03-04 02:50:46 | INFO | train_inner | epoch 057:    134 / 393 loss=5.843, nll_loss=4.018, ppl=16.2, wps=25449.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.744, loss_scale=8, train_wall=253, gb_free=12.3, wall=57426
2022-03-04 02:55:03 | INFO | train_inner | epoch 057:    234 / 393 loss=5.909, nll_loss=4.094, ppl=17.08, wps=25477, ups=0.39, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.761, loss_scale=8, train_wall=252, gb_free=12.3, wall=57683
2022-03-04 02:59:20 | INFO | train_inner | epoch 057:    334 / 393 loss=5.948, nll_loss=4.139, ppl=17.62, wps=25471.1, ups=0.39, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.744, loss_scale=8, train_wall=252, gb_free=12.3, wall=57940
2022-03-04 03:01:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:01:54 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 8.083 | nll_loss 6.51 | ppl 91.16 | wps 66767 | wpb 2034.1 | bsz 4 | num_updates 22359 | best_loss 7.641
2022-03-04 03:01:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 22359 updates
2022-03-04 03:01:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 03:01:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 03:01:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 57 @ 22359 updates, score 8.083) (writing took 4.221285528037697 seconds)
2022-03-04 03:01:58 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 03:01:58 | INFO | train | epoch 057 | loss 5.906 | nll_loss 4.091 | ppl 17.04 | wps 25272.7 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 22359 | lr 0.000211482 | gnorm 0.744 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 58098
2022-03-04 03:01:58 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 03:01:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:03:44 | INFO | train_inner | epoch 058:     41 / 393 loss=5.915, nll_loss=4.101, ppl=17.16, wps=24729.2, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=22400, lr=0.000211289, gnorm=0.732, loss_scale=16, train_wall=251, gb_free=12.3, wall=58204
2022-03-04 03:03:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 03:08:04 | INFO | train_inner | epoch 058:    142 / 393 loss=5.845, nll_loss=4.019, ppl=16.22, wps=25216.5, ups=0.38, wpb=65535.4, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.751, loss_scale=8, train_wall=255, gb_free=12.3, wall=58464
2022-03-04 03:12:21 | INFO | train_inner | epoch 058:    242 / 393 loss=5.897, nll_loss=4.08, ppl=16.91, wps=25467, ups=0.39, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.75, loss_scale=8, train_wall=252, gb_free=12.3, wall=58721
2022-03-04 03:16:39 | INFO | train_inner | epoch 058:    342 / 393 loss=5.948, nll_loss=4.138, ppl=17.61, wps=25472.3, ups=0.39, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.758, loss_scale=8, train_wall=252, gb_free=12.3, wall=58978
2022-03-04 03:18:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:18:52 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 8.076 | nll_loss 6.492 | ppl 90 | wps 65818.3 | wpb 2034.1 | bsz 4 | num_updates 22751 | best_loss 7.641
2022-03-04 03:18:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 22751 updates
2022-03-04 03:18:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 03:18:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 03:18:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 58 @ 22751 updates, score 8.076) (writing took 4.364361660089344 seconds)
2022-03-04 03:18:56 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 03:18:56 | INFO | train | epoch 058 | loss 5.897 | nll_loss 4.08 | ppl 16.91 | wps 25208.7 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 22751 | lr 0.000209652 | gnorm 0.75 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 59116
2022-03-04 03:18:56 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 03:18:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:21:03 | INFO | train_inner | epoch 059:     49 / 393 loss=5.891, nll_loss=4.073, ppl=16.83, wps=24717.6, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=22800, lr=0.000209427, gnorm=0.75, loss_scale=8, train_wall=251, gb_free=12.3, wall=59242
2022-03-04 03:25:20 | INFO | train_inner | epoch 059:    149 / 393 loss=5.841, nll_loss=4.015, ppl=16.17, wps=25470.7, ups=0.39, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.755, loss_scale=8, train_wall=252, gb_free=12.3, wall=59500
2022-03-04 03:27:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 03:29:40 | INFO | train_inner | epoch 059:    250 / 393 loss=5.887, nll_loss=4.068, ppl=16.77, wps=25225.6, ups=0.38, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.754, loss_scale=8, train_wall=255, gb_free=12.3, wall=59759
2022-03-04 03:33:57 | INFO | train_inner | epoch 059:    350 / 393 loss=5.939, nll_loss=4.128, ppl=17.49, wps=25479.4, ups=0.39, wpb=65530.2, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.762, loss_scale=8, train_wall=252, gb_free=12.3, wall=60017
2022-03-04 03:35:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:35:50 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 8.079 | nll_loss 6.503 | ppl 90.71 | wps 66366.1 | wpb 2034.1 | bsz 4 | num_updates 23143 | best_loss 7.641
2022-03-04 03:35:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 23143 updates
2022-03-04 03:35:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 03:35:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 03:35:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 59 @ 23143 updates, score 8.079) (writing took 4.253359316848218 seconds)
2022-03-04 03:35:54 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 03:35:54 | INFO | train | epoch 059 | loss 5.889 | nll_loss 4.071 | ppl 16.81 | wps 25219.1 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 23143 | lr 0.000207869 | gnorm 0.758 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 60134
2022-03-04 03:35:54 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 03:35:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:38:21 | INFO | train_inner | epoch 060:     57 / 393 loss=5.881, nll_loss=4.061, ppl=16.69, wps=24740.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=23200, lr=0.000207614, gnorm=0.758, loss_scale=8, train_wall=251, gb_free=12.3, wall=60280
2022-03-04 03:42:38 | INFO | train_inner | epoch 060:    157 / 393 loss=5.844, nll_loss=4.018, ppl=16.2, wps=25480.6, ups=0.39, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.751, loss_scale=8, train_wall=252, gb_free=12.3, wall=60538
2022-03-04 03:46:55 | INFO | train_inner | epoch 060:    257 / 393 loss=5.887, nll_loss=4.068, ppl=16.77, wps=25485.5, ups=0.39, wpb=65535.4, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.755, loss_scale=8, train_wall=252, gb_free=12.3, wall=60795
2022-03-04 03:50:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 03:51:15 | INFO | train_inner | epoch 060:    358 / 393 loss=5.935, nll_loss=4.124, ppl=17.43, wps=25208.1, ups=0.38, wpb=65530.9, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.761, loss_scale=8, train_wall=255, gb_free=12.3, wall=61055
2022-03-04 03:52:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:52:47 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 8.105 | nll_loss 6.527 | ppl 92.25 | wps 66642.5 | wpb 2034.1 | bsz 4 | num_updates 23535 | best_loss 7.641
2022-03-04 03:52:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 23535 updates
2022-03-04 03:52:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 03:52:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 03:52:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 60 @ 23535 updates, score 8.105) (writing took 4.241556752938777 seconds)
2022-03-04 03:52:51 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 03:52:51 | INFO | train | epoch 060 | loss 5.882 | nll_loss 4.062 | ppl 16.7 | wps 25222.1 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 23535 | lr 0.000206131 | gnorm 0.754 | loss_scale 8 | train_wall 990 | gb_free 12.3 | wall 61151
2022-03-04 03:52:51 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 03:52:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:55:39 | INFO | train_inner | epoch 061:     65 / 393 loss=5.854, nll_loss=4.03, ppl=16.33, wps=24744.3, ups=0.38, wpb=65243.5, bsz=127.4, num_updates=23600, lr=0.000205847, gnorm=0.755, loss_scale=8, train_wall=251, gb_free=12.3, wall=61318
2022-03-04 03:59:56 | INFO | train_inner | epoch 061:    165 / 393 loss=5.839, nll_loss=4.012, ppl=16.13, wps=25475.8, ups=0.39, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.748, loss_scale=8, train_wall=252, gb_free=12.3, wall=61576
2022-03-04 04:04:13 | INFO | train_inner | epoch 061:    265 / 393 loss=5.877, nll_loss=4.056, ppl=16.63, wps=25479.8, ups=0.39, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.754, loss_scale=8, train_wall=252, gb_free=12.3, wall=61833
2022-03-04 04:08:30 | INFO | train_inner | epoch 061:    365 / 393 loss=5.928, nll_loss=4.115, ppl=17.33, wps=25469.9, ups=0.39, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.767, loss_scale=8, train_wall=252, gb_free=12.3, wall=62090
2022-03-04 04:09:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:09:44 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 8.079 | nll_loss 6.5 | ppl 90.54 | wps 66215.5 | wpb 2034.1 | bsz 4 | num_updates 23928 | best_loss 7.641
2022-03-04 04:09:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 23928 updates
2022-03-04 04:09:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 04:09:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 04:09:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 61 @ 23928 updates, score 8.079) (writing took 4.208400597795844 seconds)
2022-03-04 04:09:49 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 04:09:49 | INFO | train | epoch 061 | loss 5.874 | nll_loss 4.053 | ppl 16.6 | wps 25285.5 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 23928 | lr 0.000204431 | gnorm 0.758 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 62169
2022-03-04 04:09:49 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 04:09:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:12:54 | INFO | train_inner | epoch 062:     72 / 393 loss=5.84, nll_loss=4.014, ppl=16.16, wps=24746.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=24000, lr=0.000204124, gnorm=0.753, loss_scale=8, train_wall=251, gb_free=12.3, wall=62354
2022-03-04 04:14:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 04:17:14 | INFO | train_inner | epoch 062:    173 / 393 loss=5.835, nll_loss=4.008, ppl=16.09, wps=25224, ups=0.38, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.755, loss_scale=8, train_wall=255, gb_free=12.3, wall=62614
2022-03-04 04:21:31 | INFO | train_inner | epoch 062:    273 / 393 loss=5.879, nll_loss=4.058, ppl=16.66, wps=25467.9, ups=0.39, wpb=65535.4, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.765, loss_scale=8, train_wall=252, gb_free=12.3, wall=62871
2022-03-04 04:25:48 | INFO | train_inner | epoch 062:    373 / 393 loss=5.926, nll_loss=4.113, ppl=17.3, wps=25467.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.765, loss_scale=8, train_wall=252, gb_free=12.3, wall=63128
2022-03-04 04:26:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:26:42 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 8.099 | nll_loss 6.517 | ppl 91.58 | wps 66246.6 | wpb 2034.1 | bsz 4 | num_updates 24320 | best_loss 7.641
2022-03-04 04:26:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 24320 updates
2022-03-04 04:26:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 04:26:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 04:26:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 62 @ 24320 updates, score 8.099) (writing took 4.230334405787289 seconds)
2022-03-04 04:26:46 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 04:26:46 | INFO | train | epoch 062 | loss 5.867 | nll_loss 4.044 | ppl 16.5 | wps 25216.1 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 24320 | lr 0.000202777 | gnorm 0.757 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 63186
2022-03-04 04:26:46 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 04:26:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:30:12 | INFO | train_inner | epoch 063:     80 / 393 loss=5.823, nll_loss=3.994, ppl=15.93, wps=24727.9, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=24400, lr=0.000202444, gnorm=0.746, loss_scale=8, train_wall=251, gb_free=12.3, wall=63392
2022-03-04 04:34:29 | INFO | train_inner | epoch 063:    180 / 393 loss=5.822, nll_loss=3.992, ppl=15.91, wps=25487.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.762, loss_scale=8, train_wall=252, gb_free=12.3, wall=63649
2022-03-04 04:36:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 04:38:49 | INFO | train_inner | epoch 063:    281 / 393 loss=5.876, nll_loss=4.055, ppl=16.63, wps=25218.4, ups=0.38, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.765, loss_scale=8, train_wall=255, gb_free=12.3, wall=63909
2022-03-04 04:43:06 | INFO | train_inner | epoch 063:    381 / 393 loss=5.921, nll_loss=4.108, ppl=17.24, wps=25484.1, ups=0.39, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.764, loss_scale=8, train_wall=252, gb_free=12.3, wall=64166
2022-03-04 04:43:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:43:40 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 8.122 | nll_loss 6.548 | ppl 93.58 | wps 66423.5 | wpb 2034.1 | bsz 4 | num_updates 24712 | best_loss 7.641
2022-03-04 04:43:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 24712 updates
2022-03-04 04:43:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 04:43:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 04:43:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 63 @ 24712 updates, score 8.122) (writing took 4.243811218068004 seconds)
2022-03-04 04:43:44 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 04:43:44 | INFO | train | epoch 063 | loss 5.859 | nll_loss 4.036 | ppl 16.4 | wps 25221.6 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 24712 | lr 0.000201162 | gnorm 0.76 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 64204
2022-03-04 04:43:44 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 04:43:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:47:30 | INFO | train_inner | epoch 064:     88 / 393 loss=5.799, nll_loss=3.966, ppl=15.62, wps=24717.3, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=24800, lr=0.000200805, gnorm=0.765, loss_scale=8, train_wall=251, gb_free=12.3, wall=64430
2022-03-04 04:51:48 | INFO | train_inner | epoch 064:    188 / 393 loss=5.828, nll_loss=3.999, ppl=15.99, wps=25474.8, ups=0.39, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.771, loss_scale=8, train_wall=252, gb_free=12.3, wall=64688
2022-03-04 04:56:05 | INFO | train_inner | epoch 064:    288 / 393 loss=5.872, nll_loss=4.05, ppl=16.57, wps=25465.5, ups=0.39, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.78, loss_scale=8, train_wall=252, gb_free=12.3, wall=64945
2022-03-04 04:59:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 05:00:25 | INFO | train_inner | epoch 064:    389 / 393 loss=5.921, nll_loss=4.107, ppl=17.23, wps=25227.7, ups=0.38, wpb=65530.2, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.78, loss_scale=8, train_wall=255, gb_free=12.3, wall=65205
2022-03-04 05:00:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:00:37 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 8.115 | nll_loss 6.536 | ppl 92.78 | wps 66578.3 | wpb 2034.1 | bsz 4 | num_updates 25104 | best_loss 7.641
2022-03-04 05:00:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 25104 updates
2022-03-04 05:00:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 05:00:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 05:00:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 64 @ 25104 updates, score 8.115) (writing took 4.210464268922806 seconds)
2022-03-04 05:00:41 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 05:00:41 | INFO | train | epoch 064 | loss 5.853 | nll_loss 4.028 | ppl 16.31 | wps 25214.1 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 25104 | lr 0.000199585 | gnorm 0.774 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 65221
2022-03-04 05:00:42 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 05:00:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:04:49 | INFO | train_inner | epoch 065:     96 / 393 loss=5.776, nll_loss=3.939, ppl=15.34, wps=24730.8, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=25200, lr=0.000199205, gnorm=0.752, loss_scale=8, train_wall=251, gb_free=12.3, wall=65468
2022-03-04 05:09:06 | INFO | train_inner | epoch 065:    196 / 393 loss=5.825, nll_loss=3.996, ppl=15.96, wps=25464.3, ups=0.39, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.765, loss_scale=8, train_wall=252, gb_free=12.3, wall=65726
2022-03-04 05:13:23 | INFO | train_inner | epoch 065:    296 / 393 loss=5.87, nll_loss=4.048, ppl=16.54, wps=25479.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.785, loss_scale=8, train_wall=252, gb_free=12.3, wall=65983
2022-03-04 05:17:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:17:35 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 8.111 | nll_loss 6.533 | ppl 92.59 | wps 66635.8 | wpb 2034.1 | bsz 4 | num_updates 25497 | best_loss 7.641
2022-03-04 05:17:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 25497 updates
2022-03-04 05:17:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 05:17:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 05:17:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 65 @ 25497 updates, score 8.111) (writing took 4.2158343540504575 seconds)
2022-03-04 05:17:39 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 05:17:39 | INFO | train | epoch 065 | loss 5.846 | nll_loss 4.02 | ppl 16.23 | wps 25281 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 25497 | lr 0.000198041 | gnorm 0.768 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 66239
2022-03-04 05:17:39 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 05:17:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:17:47 | INFO | train_inner | epoch 066:      3 / 393 loss=5.914, nll_loss=4.098, ppl=17.13, wps=24739.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=25500, lr=0.00019803, gnorm=0.772, loss_scale=8, train_wall=251, gb_free=12.3, wall=66247
2022-03-04 05:22:04 | INFO | train_inner | epoch 066:    103 / 393 loss=5.774, nll_loss=3.936, ppl=15.31, wps=25467.1, ups=0.39, wpb=65536, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.767, loss_scale=16, train_wall=252, gb_free=12.3, wall=66504
2022-03-04 05:23:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 05:26:24 | INFO | train_inner | epoch 066:    204 / 393 loss=5.82, nll_loss=3.99, ppl=15.89, wps=25219.8, ups=0.38, wpb=65530.2, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.765, loss_scale=8, train_wall=255, gb_free=12.3, wall=66764
2022-03-04 05:30:41 | INFO | train_inner | epoch 066:    304 / 393 loss=5.862, nll_loss=4.039, ppl=16.43, wps=25474.9, ups=0.39, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.774, loss_scale=8, train_wall=252, gb_free=12.3, wall=67021
2022-03-04 05:34:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:34:33 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 8.116 | nll_loss 6.534 | ppl 92.7 | wps 66321.8 | wpb 2034.1 | bsz 4 | num_updates 25889 | best_loss 7.641
2022-03-04 05:34:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 25889 updates
2022-03-04 05:34:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 05:34:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 05:34:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 66 @ 25889 updates, score 8.116) (writing took 4.241860546171665 seconds)
2022-03-04 05:34:37 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 05:34:37 | INFO | train | epoch 066 | loss 5.84 | nll_loss 4.012 | ppl 16.14 | wps 25214.5 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 25889 | lr 0.000196536 | gnorm 0.77 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 67257
2022-03-04 05:34:37 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 05:34:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:35:05 | INFO | train_inner | epoch 067:     11 / 393 loss=5.894, nll_loss=4.076, ppl=16.86, wps=24729.3, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=25900, lr=0.000196494, gnorm=0.779, loss_scale=8, train_wall=251, gb_free=12.3, wall=67285
2022-03-04 05:39:22 | INFO | train_inner | epoch 067:    111 / 393 loss=5.774, nll_loss=3.937, ppl=15.32, wps=25477.9, ups=0.39, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.762, loss_scale=8, train_wall=252, gb_free=12.3, wall=67542
2022-03-04 05:43:40 | INFO | train_inner | epoch 067:    211 / 393 loss=5.814, nll_loss=3.983, ppl=15.81, wps=25475.5, ups=0.39, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.785, loss_scale=8, train_wall=252, gb_free=12.3, wall=67800
2022-03-04 05:45:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 05:48:00 | INFO | train_inner | epoch 067:    312 / 393 loss=5.86, nll_loss=4.037, ppl=16.41, wps=25210.2, ups=0.38, wpb=65530.9, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.778, loss_scale=8, train_wall=255, gb_free=12.3, wall=68059
2022-03-04 05:51:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:51:30 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 8.118 | nll_loss 6.545 | ppl 93.36 | wps 66925.1 | wpb 2034.1 | bsz 4 | num_updates 26281 | best_loss 7.641
2022-03-04 05:51:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 26281 updates
2022-03-04 05:51:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 05:51:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 05:51:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 67 @ 26281 updates, score 8.118) (writing took 4.291456233710051 seconds)
2022-03-04 05:51:35 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 05:51:35 | INFO | train | epoch 067 | loss 5.833 | nll_loss 4.005 | ppl 16.05 | wps 25212.1 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 26281 | lr 0.000195065 | gnorm 0.777 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 68274
2022-03-04 05:51:35 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 05:51:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:52:24 | INFO | train_inner | epoch 068:     19 / 393 loss=5.874, nll_loss=4.052, ppl=16.59, wps=24720.1, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=26300, lr=0.000194994, gnorm=0.778, loss_scale=8, train_wall=251, gb_free=12.3, wall=68323
2022-03-04 05:56:41 | INFO | train_inner | epoch 068:    119 / 393 loss=5.773, nll_loss=3.935, ppl=15.29, wps=25450.7, ups=0.39, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.779, loss_scale=8, train_wall=253, gb_free=12.3, wall=68581
2022-03-04 06:00:58 | INFO | train_inner | epoch 068:    219 / 393 loss=5.814, nll_loss=3.982, ppl=15.8, wps=25473.8, ups=0.39, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.785, loss_scale=8, train_wall=252, gb_free=12.3, wall=68838
2022-03-04 06:05:16 | INFO | train_inner | epoch 068:    319 / 393 loss=5.86, nll_loss=4.037, ppl=16.41, wps=25482.6, ups=0.39, wpb=65530.9, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.777, loss_scale=8, train_wall=252, gb_free=12.3, wall=69095
2022-03-04 06:08:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:08:28 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 8.141 | nll_loss 6.576 | ppl 95.43 | wps 66888.6 | wpb 2034.1 | bsz 4 | num_updates 26674 | best_loss 7.641
2022-03-04 06:08:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 26674 updates
2022-03-04 06:08:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 06:08:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 06:08:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 68 @ 26674 updates, score 8.141) (writing took 4.277932855300605 seconds)
2022-03-04 06:08:32 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 06:08:32 | INFO | train | epoch 068 | loss 5.827 | nll_loss 3.998 | ppl 15.98 | wps 25275.8 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 26674 | lr 0.000193623 | gnorm 0.782 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 69292
2022-03-04 06:08:32 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 06:08:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:08:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 06:09:42 | INFO | train_inner | epoch 069:     27 / 393 loss=5.858, nll_loss=4.034, ppl=16.38, wps=24492.5, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=26700, lr=0.000193528, gnorm=0.791, loss_scale=8, train_wall=254, gb_free=12.3, wall=69362
2022-03-04 06:13:59 | INFO | train_inner | epoch 069:    127 / 393 loss=5.763, nll_loss=3.924, ppl=15.18, wps=25475.2, ups=0.39, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.775, loss_scale=8, train_wall=252, gb_free=12.3, wall=69619
2022-03-04 06:18:16 | INFO | train_inner | epoch 069:    227 / 393 loss=5.815, nll_loss=3.984, ppl=15.82, wps=25469.7, ups=0.39, wpb=65530.2, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.782, loss_scale=8, train_wall=252, gb_free=12.3, wall=69876
2022-03-04 06:22:34 | INFO | train_inner | epoch 069:    327 / 393 loss=5.854, nll_loss=4.029, ppl=16.32, wps=25473.6, ups=0.39, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.789, loss_scale=8, train_wall=252, gb_free=12.3, wall=70134
2022-03-04 06:25:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:25:26 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 8.144 | nll_loss 6.582 | ppl 95.79 | wps 66403.7 | wpb 2034.1 | bsz 4 | num_updates 27066 | best_loss 7.641
2022-03-04 06:25:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 27066 updates
2022-03-04 06:25:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 06:25:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 06:25:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 69 @ 27066 updates, score 8.144) (writing took 4.188549465499818 seconds)
2022-03-04 06:25:30 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 06:25:30 | INFO | train | epoch 069 | loss 5.82 | nll_loss 3.989 | ppl 15.88 | wps 25221.3 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 27066 | lr 0.000192215 | gnorm 0.783 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 70310
2022-03-04 06:25:30 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 06:25:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:26:57 | INFO | train_inner | epoch 070:     34 / 393 loss=5.835, nll_loss=4.007, ppl=16.08, wps=24740.6, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=27100, lr=0.000192095, gnorm=0.784, loss_scale=8, train_wall=251, gb_free=12.3, wall=70397
2022-03-04 06:30:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 06:31:17 | INFO | train_inner | epoch 070:    135 / 393 loss=5.757, nll_loss=3.916, ppl=15.1, wps=25225.6, ups=0.38, wpb=65530.9, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.763, loss_scale=8, train_wall=255, gb_free=12.3, wall=70657
2022-03-04 06:35:34 | INFO | train_inner | epoch 070:    235 / 393 loss=5.809, nll_loss=3.977, ppl=15.74, wps=25477.6, ups=0.39, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.787, loss_scale=8, train_wall=252, gb_free=12.3, wall=70914
2022-03-04 06:39:52 | INFO | train_inner | epoch 070:    335 / 393 loss=5.861, nll_loss=4.036, ppl=16.41, wps=25473.9, ups=0.39, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.788, loss_scale=8, train_wall=252, gb_free=12.3, wall=71172
2022-03-04 06:42:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:42:23 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 8.114 | nll_loss 6.534 | ppl 92.64 | wps 66623 | wpb 2034.1 | bsz 4 | num_updates 27458 | best_loss 7.641
2022-03-04 06:42:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 27458 updates
2022-03-04 06:42:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 06:42:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 06:42:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 70 @ 27458 updates, score 8.114) (writing took 4.182947491295636 seconds)
2022-03-04 06:42:27 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 06:42:27 | INFO | train | epoch 070 | loss 5.814 | nll_loss 3.983 | ppl 15.81 | wps 25220.6 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 27458 | lr 0.000190838 | gnorm 0.781 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 71327
2022-03-04 06:42:27 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 06:42:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:44:15 | INFO | train_inner | epoch 071:     42 / 393 loss=5.821, nll_loss=3.991, ppl=15.9, wps=24740, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=27500, lr=0.000190693, gnorm=0.787, loss_scale=8, train_wall=251, gb_free=12.3, wall=71435
2022-03-04 06:48:33 | INFO | train_inner | epoch 071:    142 / 393 loss=5.757, nll_loss=3.916, ppl=15.1, wps=25479.4, ups=0.39, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.777, loss_scale=8, train_wall=252, gb_free=12.3, wall=71693
2022-03-04 06:52:50 | INFO | train_inner | epoch 071:    242 / 393 loss=5.804, nll_loss=3.971, ppl=15.69, wps=25475.7, ups=0.39, wpb=65530.9, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.787, loss_scale=16, train_wall=252, gb_free=12.3, wall=71950
2022-03-04 06:56:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 06:57:10 | INFO | train_inner | epoch 071:    343 / 393 loss=5.856, nll_loss=4.031, ppl=16.35, wps=25224.5, ups=0.38, wpb=65535.4, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.793, loss_scale=8, train_wall=255, gb_free=12.3, wall=72210
2022-03-04 06:59:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:59:21 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 8.137 | nll_loss 6.57 | ppl 94.98 | wps 66301.6 | wpb 2034.1 | bsz 4 | num_updates 27850 | best_loss 7.641
2022-03-04 06:59:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 27850 updates
2022-03-04 06:59:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 06:59:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 06:59:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 71 @ 27850 updates, score 8.137) (writing took 4.229580439627171 seconds)
2022-03-04 06:59:25 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 06:59:25 | INFO | train | epoch 071 | loss 5.809 | nll_loss 3.976 | ppl 15.74 | wps 25217 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 27850 | lr 0.00018949 | gnorm 0.788 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 72345
2022-03-04 06:59:25 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 06:59:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:01:34 | INFO | train_inner | epoch 072:     50 / 393 loss=5.808, nll_loss=3.975, ppl=15.73, wps=24725, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=27900, lr=0.000189321, gnorm=0.792, loss_scale=8, train_wall=251, gb_free=12.3, wall=72473
2022-03-04 07:05:51 | INFO | train_inner | epoch 072:    150 / 393 loss=5.76, nll_loss=3.92, ppl=15.14, wps=25478.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.79, loss_scale=8, train_wall=252, gb_free=12.3, wall=72731
2022-03-04 07:10:08 | INFO | train_inner | epoch 072:    250 / 393 loss=5.81, nll_loss=3.978, ppl=15.75, wps=25478.7, ups=0.39, wpb=65530.9, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.793, loss_scale=8, train_wall=252, gb_free=12.3, wall=72988
2022-03-04 07:14:25 | INFO | train_inner | epoch 072:    350 / 393 loss=5.848, nll_loss=4.022, ppl=16.24, wps=25474.5, ups=0.39, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.797, loss_scale=8, train_wall=252, gb_free=12.3, wall=73245
2022-03-04 07:16:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:16:18 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 8.128 | nll_loss 6.559 | ppl 94.27 | wps 66302.7 | wpb 2034.1 | bsz 4 | num_updates 28243 | best_loss 7.641
2022-03-04 07:16:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 28243 updates
2022-03-04 07:16:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 07:16:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 07:16:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 72 @ 28243 updates, score 8.128) (writing took 4.264410420320928 seconds)
2022-03-04 07:16:22 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 07:16:22 | INFO | train | epoch 072 | loss 5.803 | nll_loss 3.97 | ppl 15.67 | wps 25284.5 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 28243 | lr 0.000188167 | gnorm 0.793 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 73362
2022-03-04 07:16:22 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 07:16:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:18:49 | INFO | train_inner | epoch 073:     57 / 393 loss=5.792, nll_loss=3.957, ppl=15.53, wps=24735.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=28300, lr=0.000187978, gnorm=0.799, loss_scale=16, train_wall=251, gb_free=12.3, wall=73509
2022-03-04 07:18:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 07:23:09 | INFO | train_inner | epoch 073:    158 / 393 loss=5.757, nll_loss=3.917, ppl=15.1, wps=25206.1, ups=0.38, wpb=65530.9, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.788, loss_scale=8, train_wall=255, gb_free=12.3, wall=73769
2022-03-04 07:27:26 | INFO | train_inner | epoch 073:    258 / 393 loss=5.801, nll_loss=3.967, ppl=15.64, wps=25464.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.788, loss_scale=8, train_wall=252, gb_free=12.3, wall=74026
2022-03-04 07:31:44 | INFO | train_inner | epoch 073:    358 / 393 loss=5.848, nll_loss=4.022, ppl=16.24, wps=25472.7, ups=0.39, wpb=65536, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.796, loss_scale=8, train_wall=252, gb_free=12.3, wall=74284
2022-03-04 07:33:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:33:16 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 8.141 | nll_loss 6.567 | ppl 94.84 | wps 66202.4 | wpb 2034.1 | bsz 4 | num_updates 28635 | best_loss 7.641
2022-03-04 07:33:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 28635 updates
2022-03-04 07:33:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 07:33:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 07:33:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 73 @ 28635 updates, score 8.141) (writing took 4.257393381558359 seconds)
2022-03-04 07:33:20 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 07:33:20 | INFO | train | epoch 073 | loss 5.797 | nll_loss 3.963 | ppl 15.59 | wps 25211.9 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 28635 | lr 0.000186875 | gnorm 0.791 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 74380
2022-03-04 07:33:20 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 07:33:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:36:07 | INFO | train_inner | epoch 074:     65 / 393 loss=5.771, nll_loss=3.933, ppl=15.27, wps=24737.3, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=28700, lr=0.000186663, gnorm=0.79, loss_scale=8, train_wall=251, gb_free=12.3, wall=74547
2022-03-04 07:40:25 | INFO | train_inner | epoch 074:    165 / 393 loss=5.761, nll_loss=3.92, ppl=15.14, wps=25475.1, ups=0.39, wpb=65530.2, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.785, loss_scale=8, train_wall=252, gb_free=12.3, wall=74805
2022-03-04 07:41:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 07:44:45 | INFO | train_inner | epoch 074:    266 / 393 loss=5.799, nll_loss=3.964, ppl=15.61, wps=25224.9, ups=0.38, wpb=65536, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.793, loss_scale=8, train_wall=255, gb_free=12.3, wall=75064
2022-03-04 07:49:02 | INFO | train_inner | epoch 074:    366 / 393 loss=5.843, nll_loss=4.015, ppl=16.17, wps=25488.5, ups=0.39, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=0.807, loss_scale=8, train_wall=252, gb_free=12.3, wall=75321
2022-03-04 07:50:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:50:13 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 8.163 | nll_loss 6.596 | ppl 96.74 | wps 66330.1 | wpb 2034.1 | bsz 4 | num_updates 29027 | best_loss 7.641
2022-03-04 07:50:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 29027 updates
2022-03-04 07:50:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 07:50:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 07:50:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 74 @ 29027 updates, score 8.163) (writing took 4.174778899177909 seconds)
2022-03-04 07:50:17 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 07:50:17 | INFO | train | epoch 074 | loss 5.792 | nll_loss 3.956 | ppl 15.52 | wps 25225.9 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 29027 | lr 0.000185609 | gnorm 0.793 | loss_scale 8 | train_wall 990 | gb_free 12.3 | wall 75397
2022-03-04 07:50:17 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 07:50:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:53:25 | INFO | train_inner | epoch 075:     73 / 393 loss=5.761, nll_loss=3.921, ppl=15.14, wps=24740.8, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=29100, lr=0.000185376, gnorm=0.788, loss_scale=8, train_wall=251, gb_free=12.3, wall=75585
2022-03-04 07:57:43 | INFO | train_inner | epoch 075:    173 / 393 loss=5.746, nll_loss=3.903, ppl=14.96, wps=25473.4, ups=0.39, wpb=65530.9, bsz=128, num_updates=29200, lr=0.000185058, gnorm=0.792, loss_scale=8, train_wall=252, gb_free=12.3, wall=75842
2022-03-04 08:02:00 | INFO | train_inner | epoch 075:    273 / 393 loss=5.8, nll_loss=3.966, ppl=15.62, wps=25465.9, ups=0.39, wpb=65536, bsz=128, num_updates=29300, lr=0.000184742, gnorm=0.805, loss_scale=8, train_wall=252, gb_free=12.3, wall=76100
2022-03-04 08:04:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 08:06:20 | INFO | train_inner | epoch 075:    374 / 393 loss=5.847, nll_loss=4.021, ppl=16.23, wps=25238.3, ups=0.39, wpb=65536, bsz=128, num_updates=29400, lr=0.000184428, gnorm=0.806, loss_scale=8, train_wall=255, gb_free=12.3, wall=76359
2022-03-04 08:07:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:07:11 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 8.169 | nll_loss 6.6 | ppl 97 | wps 66632 | wpb 2034.1 | bsz 4 | num_updates 29419 | best_loss 7.641
2022-03-04 08:07:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 29419 updates
2022-03-04 08:07:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 08:07:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 08:07:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 75 @ 29419 updates, score 8.169) (writing took 4.1880438243970275 seconds)
2022-03-04 08:07:15 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-04 08:07:15 | INFO | train | epoch 075 | loss 5.786 | nll_loss 3.95 | ppl 15.45 | wps 25220.8 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 29419 | lr 0.000184368 | gnorm 0.799 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 76415
2022-03-04 08:07:15 | INFO | fairseq.trainer | begin training epoch 76
2022-03-04 08:07:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:10:43 | INFO | train_inner | epoch 076:     81 / 393 loss=5.739, nll_loss=3.896, ppl=14.88, wps=24744.7, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=29500, lr=0.000184115, gnorm=0.808, loss_scale=8, train_wall=251, gb_free=12.3, wall=76623
2022-03-04 08:15:01 | INFO | train_inner | epoch 076:    181 / 393 loss=5.749, nll_loss=3.907, ppl=15, wps=25472.8, ups=0.39, wpb=65536, bsz=128, num_updates=29600, lr=0.000183804, gnorm=0.777, loss_scale=8, train_wall=252, gb_free=12.3, wall=76880
2022-03-04 08:19:18 | INFO | train_inner | epoch 076:    281 / 393 loss=5.805, nll_loss=3.972, ppl=15.69, wps=25459.7, ups=0.39, wpb=65530.9, bsz=128, num_updates=29700, lr=0.000183494, gnorm=0.814, loss_scale=8, train_wall=252, gb_free=12.3, wall=77138
2022-03-04 08:23:35 | INFO | train_inner | epoch 076:    381 / 393 loss=5.84, nll_loss=4.012, ppl=16.13, wps=25479.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=29800, lr=0.000183186, gnorm=0.803, loss_scale=8, train_wall=252, gb_free=12.3, wall=77395
2022-03-04 08:24:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:24:08 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 8.178 | nll_loss 6.608 | ppl 97.55 | wps 66253.8 | wpb 2034.1 | bsz 4 | num_updates 29812 | best_loss 7.641
2022-03-04 08:24:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 29812 updates
2022-03-04 08:24:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 08:24:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 08:24:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 76 @ 29812 updates, score 8.178) (writing took 4.197936799377203 seconds)
2022-03-04 08:24:12 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-04 08:24:12 | INFO | train | epoch 076 | loss 5.782 | nll_loss 3.945 | ppl 15.4 | wps 25282 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 29812 | lr 0.000183149 | gnorm 0.801 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 77432
2022-03-04 08:24:12 | INFO | fairseq.trainer | begin training epoch 77
2022-03-04 08:24:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:27:59 | INFO | train_inner | epoch 077:     88 / 393 loss=5.728, nll_loss=3.882, ppl=14.74, wps=24730.4, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=29900, lr=0.000182879, gnorm=0.792, loss_scale=16, train_wall=251, gb_free=12.3, wall=77659
2022-03-04 08:30:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 08:32:19 | INFO | train_inner | epoch 077:    189 / 393 loss=5.741, nll_loss=3.897, ppl=14.9, wps=25225.4, ups=0.38, wpb=65536, bsz=128, num_updates=30000, lr=0.000182574, gnorm=0.8, loss_scale=8, train_wall=255, gb_free=12.3, wall=77919
2022-03-04 08:36:36 | INFO | train_inner | epoch 077:    289 / 393 loss=5.802, nll_loss=3.967, ppl=15.64, wps=25471.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=30100, lr=0.000182271, gnorm=0.79, loss_scale=8, train_wall=252, gb_free=12.3, wall=78176
2022-03-04 08:40:53 | INFO | train_inner | epoch 077:    389 / 393 loss=5.843, nll_loss=4.016, ppl=16.18, wps=25475.9, ups=0.39, wpb=65536, bsz=128, num_updates=30200, lr=0.000181969, gnorm=0.817, loss_scale=8, train_wall=252, gb_free=12.3, wall=78433
2022-03-04 08:41:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:41:06 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 8.176 | nll_loss 6.605 | ppl 97.31 | wps 66595.8 | wpb 2034.1 | bsz 4 | num_updates 30204 | best_loss 7.641
2022-03-04 08:41:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 30204 updates
2022-03-04 08:41:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 08:41:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 08:41:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 77 @ 30204 updates, score 8.176) (writing took 4.19479123968631 seconds)
2022-03-04 08:41:10 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-04 08:41:10 | INFO | train | epoch 077 | loss 5.777 | nll_loss 3.939 | ppl 15.33 | wps 25217.3 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 30204 | lr 0.000181957 | gnorm 0.8 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 78450
2022-03-04 08:41:10 | INFO | fairseq.trainer | begin training epoch 78
2022-03-04 08:41:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:45:17 | INFO | train_inner | epoch 078:     96 / 393 loss=5.709, nll_loss=3.86, ppl=14.52, wps=24750.4, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=30300, lr=0.000181668, gnorm=0.801, loss_scale=8, train_wall=251, gb_free=12.3, wall=78697
2022-03-04 08:49:34 | INFO | train_inner | epoch 078:    196 / 393 loss=5.75, nll_loss=3.907, ppl=15, wps=25466.8, ups=0.39, wpb=65536, bsz=128, num_updates=30400, lr=0.000181369, gnorm=0.806, loss_scale=8, train_wall=252, gb_free=12.3, wall=78954
2022-03-04 08:53:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 08:53:54 | INFO | train_inner | epoch 078:    297 / 393 loss=5.79, nll_loss=3.955, ppl=15.5, wps=25209.1, ups=0.38, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=0.809, loss_scale=8, train_wall=255, gb_free=12.3, wall=79214
2022-03-04 08:58:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:58:04 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 8.181 | nll_loss 6.604 | ppl 97.25 | wps 66599.2 | wpb 2034.1 | bsz 4 | num_updates 30596 | best_loss 7.641
2022-03-04 08:58:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 30596 updates
2022-03-04 08:58:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 08:58:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 08:58:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 78 @ 30596 updates, score 8.181) (writing took 4.192831623367965 seconds)
2022-03-04 08:58:08 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-04 08:58:08 | INFO | train | epoch 078 | loss 5.771 | nll_loss 3.932 | ppl 15.26 | wps 25214.6 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 30596 | lr 0.000180787 | gnorm 0.805 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 79468
2022-03-04 08:58:08 | INFO | fairseq.trainer | begin training epoch 79
2022-03-04 08:58:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:58:18 | INFO | train_inner | epoch 079:      4 / 393 loss=5.836, nll_loss=4.007, ppl=16.08, wps=24729.1, ups=0.38, wpb=65243.5, bsz=127.4, num_updates=30600, lr=0.000180775, gnorm=0.807, loss_scale=8, train_wall=251, gb_free=12.3, wall=79478
2022-03-04 09:02:35 | INFO | train_inner | epoch 079:    104 / 393 loss=5.698, nll_loss=3.848, ppl=14.4, wps=25475.7, ups=0.39, wpb=65536, bsz=128, num_updates=30700, lr=0.000180481, gnorm=0.8, loss_scale=8, train_wall=252, gb_free=12.3, wall=79735
2022-03-04 09:06:53 | INFO | train_inner | epoch 079:    204 / 393 loss=5.753, nll_loss=3.911, ppl=15.04, wps=25458.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=30800, lr=0.000180187, gnorm=0.805, loss_scale=8, train_wall=252, gb_free=12.3, wall=79993
2022-03-04 09:11:10 | INFO | train_inner | epoch 079:    304 / 393 loss=5.794, nll_loss=3.958, ppl=15.55, wps=25468.7, ups=0.39, wpb=65536, bsz=128, num_updates=30900, lr=0.000179896, gnorm=0.817, loss_scale=8, train_wall=252, gb_free=12.3, wall=80250
2022-03-04 09:14:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:15:01 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 8.188 | nll_loss 6.619 | ppl 98.3 | wps 66388.3 | wpb 2034.1 | bsz 4 | num_updates 30989 | best_loss 7.641
2022-03-04 09:15:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 30989 updates
2022-03-04 09:15:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 09:15:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 09:15:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 79 @ 30989 updates, score 8.188) (writing took 4.201288054697216 seconds)
2022-03-04 09:15:05 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-04 09:15:05 | INFO | train | epoch 079 | loss 5.767 | nll_loss 3.927 | ppl 15.21 | wps 25280 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 30989 | lr 0.000179637 | gnorm 0.81 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 80485
2022-03-04 09:15:05 | INFO | fairseq.trainer | begin training epoch 80
2022-03-04 09:15:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:15:34 | INFO | train_inner | epoch 080:     11 / 393 loss=5.814, nll_loss=3.982, ppl=15.8, wps=24740.7, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=31000, lr=0.000179605, gnorm=0.816, loss_scale=8, train_wall=251, gb_free=12.3, wall=80514
2022-03-04 09:19:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:19:54 | INFO | train_inner | epoch 080:    112 / 393 loss=5.7, nll_loss=3.85, ppl=14.42, wps=25210.1, ups=0.38, wpb=65536, bsz=128, num_updates=31100, lr=0.000179316, gnorm=0.797, loss_scale=8, train_wall=255, gb_free=12.3, wall=80774
2022-03-04 09:24:11 | INFO | train_inner | epoch 080:    212 / 393 loss=5.744, nll_loss=3.901, ppl=14.94, wps=25473.1, ups=0.39, wpb=65536, bsz=128, num_updates=31200, lr=0.000179029, gnorm=0.808, loss_scale=8, train_wall=252, gb_free=12.3, wall=81031
2022-03-04 09:28:28 | INFO | train_inner | epoch 080:    312 / 393 loss=5.79, nll_loss=3.954, ppl=15.49, wps=25483.2, ups=0.39, wpb=65536, bsz=128, num_updates=31300, lr=0.000178743, gnorm=0.809, loss_scale=8, train_wall=252, gb_free=12.3, wall=81288
2022-03-04 09:31:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:31:59 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 8.18 | nll_loss 6.611 | ppl 97.73 | wps 66230.8 | wpb 2034.1 | bsz 4 | num_updates 31381 | best_loss 7.641
2022-03-04 09:31:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 31381 updates
2022-03-04 09:31:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 09:32:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 09:32:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 80 @ 31381 updates, score 8.18) (writing took 4.213458174839616 seconds)
2022-03-04 09:32:03 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-04 09:32:03 | INFO | train | epoch 080 | loss 5.762 | nll_loss 3.921 | ppl 15.15 | wps 25217.9 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 31381 | lr 0.000178512 | gnorm 0.811 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 81503
2022-03-04 09:32:03 | INFO | fairseq.trainer | begin training epoch 81
2022-03-04 09:32:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:32:52 | INFO | train_inner | epoch 081:     19 / 393 loss=5.808, nll_loss=3.975, ppl=15.72, wps=24741.8, ups=0.38, wpb=65243.5, bsz=127.4, num_updates=31400, lr=0.000178458, gnorm=0.826, loss_scale=8, train_wall=251, gb_free=12.3, wall=81552
2022-03-04 09:37:09 | INFO | train_inner | epoch 081:    119 / 393 loss=5.694, nll_loss=3.843, ppl=14.35, wps=25475.8, ups=0.39, wpb=65530.9, bsz=128, num_updates=31500, lr=0.000178174, gnorm=0.812, loss_scale=8, train_wall=252, gb_free=12.3, wall=81809
2022-03-04 09:41:26 | INFO | train_inner | epoch 081:    219 / 393 loss=5.75, nll_loss=3.907, ppl=15, wps=25475.2, ups=0.39, wpb=65535.4, bsz=128, num_updates=31600, lr=0.000177892, gnorm=0.809, loss_scale=16, train_wall=252, gb_free=12.3, wall=82066
2022-03-04 09:42:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:45:46 | INFO | train_inner | epoch 081:    320 / 393 loss=5.79, nll_loss=3.954, ppl=15.5, wps=25215.4, ups=0.38, wpb=65536, bsz=128, num_updates=31700, lr=0.000177611, gnorm=0.825, loss_scale=8, train_wall=255, gb_free=12.3, wall=82326
2022-03-04 09:48:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:48:56 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 8.192 | nll_loss 6.621 | ppl 98.44 | wps 66424.6 | wpb 2034.1 | bsz 4 | num_updates 31773 | best_loss 7.641
2022-03-04 09:48:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 31773 updates
2022-03-04 09:48:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 09:49:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 09:49:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 81 @ 31773 updates, score 8.192) (writing took 4.246227997355163 seconds)
2022-03-04 09:49:01 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-04 09:49:01 | INFO | train | epoch 081 | loss 5.757 | nll_loss 3.916 | ppl 15.09 | wps 25218.7 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 31773 | lr 0.000177407 | gnorm 0.814 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 82520
2022-03-04 09:49:01 | INFO | fairseq.trainer | begin training epoch 82
2022-03-04 09:49:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:50:10 | INFO | train_inner | epoch 082:     27 / 393 loss=5.788, nll_loss=3.952, ppl=15.48, wps=24732.2, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=31800, lr=0.000177332, gnorm=0.817, loss_scale=8, train_wall=251, gb_free=12.3, wall=82590
2022-03-04 09:54:27 | INFO | train_inner | epoch 082:    127 / 393 loss=5.694, nll_loss=3.843, ppl=14.35, wps=25475.8, ups=0.39, wpb=65536, bsz=128, num_updates=31900, lr=0.000177054, gnorm=0.797, loss_scale=8, train_wall=252, gb_free=12.3, wall=82847
2022-03-04 09:58:45 | INFO | train_inner | epoch 082:    227 / 393 loss=5.752, nll_loss=3.909, ppl=15.03, wps=25462, ups=0.39, wpb=65535.4, bsz=128, num_updates=32000, lr=0.000176777, gnorm=0.821, loss_scale=8, train_wall=252, gb_free=12.3, wall=83105
2022-03-04 10:03:02 | INFO | train_inner | epoch 082:    327 / 393 loss=5.791, nll_loss=3.955, ppl=15.51, wps=25485.1, ups=0.39, wpb=65536, bsz=128, num_updates=32100, lr=0.000176501, gnorm=0.821, loss_scale=8, train_wall=252, gb_free=12.3, wall=83362
2022-03-04 10:05:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:05:54 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 8.196 | nll_loss 6.629 | ppl 98.99 | wps 66630.9 | wpb 2034.1 | bsz 4 | num_updates 32166 | best_loss 7.641
2022-03-04 10:05:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 32166 updates
2022-03-04 10:05:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 10:05:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 10:05:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 82 @ 32166 updates, score 8.196) (writing took 4.166336386464536 seconds)
2022-03-04 10:05:58 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-04 10:05:58 | INFO | train | epoch 082 | loss 5.754 | nll_loss 3.911 | ppl 15.05 | wps 25281.1 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 32166 | lr 0.00017632 | gnorm 0.816 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 83538
2022-03-04 10:05:58 | INFO | fairseq.trainer | begin training epoch 83
2022-03-04 10:05:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:07:26 | INFO | train_inner | epoch 083:     34 / 393 loss=5.768, nll_loss=3.928, ppl=15.22, wps=24735.9, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=32200, lr=0.000176227, gnorm=0.814, loss_scale=16, train_wall=251, gb_free=12.3, wall=83626
2022-03-04 10:08:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 10:11:45 | INFO | train_inner | epoch 083:    135 / 393 loss=5.698, nll_loss=3.847, ppl=14.39, wps=25227.7, ups=0.38, wpb=65536, bsz=128, num_updates=32300, lr=0.000175954, gnorm=0.805, loss_scale=8, train_wall=255, gb_free=12.3, wall=83885
2022-03-04 10:16:03 | INFO | train_inner | epoch 083:    235 / 393 loss=5.745, nll_loss=3.901, ppl=14.94, wps=25467, ups=0.39, wpb=65536, bsz=128, num_updates=32400, lr=0.000175682, gnorm=0.815, loss_scale=8, train_wall=252, gb_free=12.3, wall=84143
2022-03-04 10:20:20 | INFO | train_inner | epoch 083:    335 / 393 loss=5.786, nll_loss=3.949, ppl=15.44, wps=25476, ups=0.39, wpb=65536, bsz=128, num_updates=32500, lr=0.000175412, gnorm=0.818, loss_scale=8, train_wall=252, gb_free=12.3, wall=84400
2022-03-04 10:22:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:22:51 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 8.196 | nll_loss 6.623 | ppl 98.6 | wps 66567.2 | wpb 2034.1 | bsz 4 | num_updates 32558 | best_loss 7.641
2022-03-04 10:22:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 32558 updates
2022-03-04 10:22:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 10:22:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 10:22:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 83 @ 32558 updates, score 8.196) (writing took 4.226283090189099 seconds)
2022-03-04 10:22:56 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-04 10:22:56 | INFO | train | epoch 083 | loss 5.749 | nll_loss 3.906 | ppl 14.99 | wps 25216.8 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 32558 | lr 0.000175255 | gnorm 0.813 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 84556
2022-03-04 10:22:56 | INFO | fairseq.trainer | begin training epoch 84
2022-03-04 10:22:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:24:44 | INFO | train_inner | epoch 084:     42 / 393 loss=5.76, nll_loss=3.918, ppl=15.12, wps=24736.6, ups=0.38, wpb=65238.4, bsz=127.4, num_updates=32600, lr=0.000175142, gnorm=0.819, loss_scale=8, train_wall=251, gb_free=12.3, wall=84664
2022-03-04 10:29:01 | INFO | train_inner | epoch 084:    142 / 393 loss=5.696, nll_loss=3.845, ppl=14.37, wps=25481.5, ups=0.39, wpb=65536, bsz=128, num_updates=32700, lr=0.000174874, gnorm=0.804, loss_scale=8, train_wall=252, gb_free=12.3, wall=84921
2022-03-04 10:32:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 10:33:21 | INFO | train_inner | epoch 084:    243 / 393 loss=5.74, nll_loss=3.895, ppl=14.88, wps=25213.4, ups=0.38, wpb=65536, bsz=128, num_updates=32800, lr=0.000174608, gnorm=0.828, loss_scale=8, train_wall=255, gb_free=12.3, wall=85181
2022-03-04 10:37:38 | INFO | train_inner | epoch 084:    343 / 393 loss=5.792, nll_loss=3.956, ppl=15.52, wps=25475.1, ups=0.39, wpb=65535.4, bsz=128, num_updates=32900, lr=0.000174342, gnorm=0.828, loss_scale=8, train_wall=252, gb_free=12.3, wall=85438
2022-03-04 10:39:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:39:49 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 8.207 | nll_loss 6.642 | ppl 99.88 | wps 66566.9 | wpb 2034.1 | bsz 4 | num_updates 32950 | best_loss 7.641
2022-03-04 10:39:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 32950 updates
2022-03-04 10:39:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 10:39:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 10:39:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 84 @ 32950 updates, score 8.207) (writing took 4.160848357714713 seconds)
2022-03-04 10:39:53 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-04 10:39:53 | INFO | train | epoch 084 | loss 5.745 | nll_loss 3.901 | ppl 14.94 | wps 25221.5 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 32950 | lr 0.00017421 | gnorm 0.821 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 85573
2022-03-04 10:39:53 | INFO | fairseq.trainer | begin training epoch 85
2022-03-04 10:39:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:42:02 | INFO | train_inner | epoch 085:     50 / 393 loss=5.744, nll_loss=3.9, ppl=14.93, wps=24747.7, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=33000, lr=0.000174078, gnorm=0.821, loss_scale=8, train_wall=251, gb_free=12.3, wall=85702
2022-03-04 10:46:19 | INFO | train_inner | epoch 085:    150 / 393 loss=5.695, nll_loss=3.843, ppl=14.35, wps=25465.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=33100, lr=0.000173814, gnorm=0.825, loss_scale=8, train_wall=252, gb_free=12.3, wall=85959
2022-03-04 10:50:37 | INFO | train_inner | epoch 085:    250 / 393 loss=5.743, nll_loss=3.899, ppl=14.92, wps=25406.5, ups=0.39, wpb=65530.9, bsz=128, num_updates=33200, lr=0.000173553, gnorm=0.823, loss_scale=8, train_wall=253, gb_free=12.3, wall=86217
2022-03-04 10:54:55 | INFO | train_inner | epoch 085:    350 / 393 loss=5.786, nll_loss=3.949, ppl=15.44, wps=25442.2, ups=0.39, wpb=65536, bsz=128, num_updates=33300, lr=0.000173292, gnorm=0.828, loss_scale=8, train_wall=253, gb_free=12.3, wall=86475
2022-03-04 10:56:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:56:48 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 8.185 | nll_loss 6.618 | ppl 98.2 | wps 66669.6 | wpb 2034.1 | bsz 4 | num_updates 33343 | best_loss 7.641
2022-03-04 10:56:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 33343 updates
2022-03-04 10:56:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 10:56:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 10:56:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 85 @ 33343 updates, score 8.185) (writing took 4.158655139617622 seconds)
2022-03-04 10:56:52 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-04 10:56:52 | INFO | train | epoch 085 | loss 5.741 | nll_loss 3.896 | ppl 14.89 | wps 25258.7 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 33343 | lr 0.00017318 | gnorm 0.825 | loss_scale 16 | train_wall 992 | gb_free 12.3 | wall 86592
2022-03-04 10:56:52 | INFO | fairseq.trainer | begin training epoch 86
2022-03-04 10:56:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:59:18 | INFO | train_inner | epoch 086:     57 / 393 loss=5.729, nll_loss=3.883, ppl=14.76, wps=24743.4, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=33400, lr=0.000173032, gnorm=0.83, loss_scale=16, train_wall=251, gb_free=12.3, wall=86738
2022-03-04 11:02:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 11:03:38 | INFO | train_inner | epoch 086:    158 / 393 loss=5.696, nll_loss=3.845, ppl=14.37, wps=25228, ups=0.38, wpb=65536, bsz=128, num_updates=33500, lr=0.000172774, gnorm=0.815, loss_scale=8, train_wall=255, gb_free=12.3, wall=86998
2022-03-04 11:07:55 | INFO | train_inner | epoch 086:    258 / 393 loss=5.743, nll_loss=3.898, ppl=14.91, wps=25477.2, ups=0.39, wpb=65530.2, bsz=128, num_updates=33600, lr=0.000172516, gnorm=0.828, loss_scale=8, train_wall=252, gb_free=12.3, wall=87255
2022-03-04 11:12:13 | INFO | train_inner | epoch 086:    358 / 393 loss=5.79, nll_loss=3.954, ppl=15.49, wps=25477.8, ups=0.39, wpb=65536, bsz=128, num_updates=33700, lr=0.00017226, gnorm=0.828, loss_scale=8, train_wall=252, gb_free=12.3, wall=87512
2022-03-04 11:13:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:13:45 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 8.198 | nll_loss 6.631 | ppl 99.14 | wps 66837.2 | wpb 2034.1 | bsz 4 | num_updates 33735 | best_loss 7.641
2022-03-04 11:13:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 33735 updates
2022-03-04 11:13:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 11:13:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 11:13:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 86 @ 33735 updates, score 8.198) (writing took 4.189683683216572 seconds)
2022-03-04 11:13:49 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-04 11:13:49 | INFO | train | epoch 086 | loss 5.736 | nll_loss 3.891 | ppl 14.84 | wps 25224 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 33735 | lr 0.000172171 | gnorm 0.825 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 87609
2022-03-04 11:13:49 | INFO | fairseq.trainer | begin training epoch 87
2022-03-04 11:13:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:16:36 | INFO | train_inner | epoch 087:     65 / 393 loss=5.713, nll_loss=3.865, ppl=14.57, wps=24746.7, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=33800, lr=0.000172005, gnorm=0.828, loss_scale=8, train_wall=251, gb_free=12.3, wall=87776
2022-03-04 11:20:54 | INFO | train_inner | epoch 087:    165 / 393 loss=5.701, nll_loss=3.85, ppl=14.42, wps=25469.5, ups=0.39, wpb=65530.9, bsz=128, num_updates=33900, lr=0.000171751, gnorm=0.829, loss_scale=8, train_wall=252, gb_free=12.3, wall=88033
2022-03-04 11:25:11 | INFO | train_inner | epoch 087:    265 / 393 loss=5.741, nll_loss=3.897, ppl=14.9, wps=25471.2, ups=0.39, wpb=65535.4, bsz=128, num_updates=34000, lr=0.000171499, gnorm=0.826, loss_scale=16, train_wall=252, gb_free=12.3, wall=88291
2022-03-04 11:26:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 11:29:31 | INFO | train_inner | epoch 087:    366 / 393 loss=5.785, nll_loss=3.948, ppl=15.43, wps=25225.7, ups=0.38, wpb=65536, bsz=128, num_updates=34100, lr=0.000171247, gnorm=0.824, loss_scale=8, train_wall=255, gb_free=12.3, wall=88551
2022-03-04 11:30:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:30:42 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 8.212 | nll_loss 6.642 | ppl 99.88 | wps 66702.6 | wpb 2034.1 | bsz 4 | num_updates 34127 | best_loss 7.641
2022-03-04 11:30:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 34127 updates
2022-03-04 11:30:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 11:30:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 11:30:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 87 @ 34127 updates, score 8.212) (writing took 4.170603239908814 seconds)
2022-03-04 11:30:46 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-04 11:30:46 | INFO | train | epoch 087 | loss 5.733 | nll_loss 3.887 | ppl 14.8 | wps 25220.7 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 34127 | lr 0.000171179 | gnorm 0.826 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 88626
2022-03-04 11:30:46 | INFO | fairseq.trainer | begin training epoch 88
2022-03-04 11:30:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:33:54 | INFO | train_inner | epoch 088:     73 / 393 loss=5.696, nll_loss=3.844, ppl=14.36, wps=24734.4, ups=0.38, wpb=65243.5, bsz=127.4, num_updates=34200, lr=0.000170996, gnorm=0.842, loss_scale=8, train_wall=251, gb_free=12.3, wall=88814
2022-03-04 11:38:12 | INFO | train_inner | epoch 088:    173 / 393 loss=5.697, nll_loss=3.845, ppl=14.37, wps=25465.6, ups=0.39, wpb=65536, bsz=128, num_updates=34300, lr=0.000170747, gnorm=0.826, loss_scale=8, train_wall=252, gb_free=12.3, wall=89072
2022-03-04 11:42:29 | INFO | train_inner | epoch 088:    273 / 393 loss=5.74, nll_loss=3.895, ppl=14.88, wps=25478.7, ups=0.39, wpb=65536, bsz=128, num_updates=34400, lr=0.000170499, gnorm=0.832, loss_scale=8, train_wall=252, gb_free=12.3, wall=89329
2022-03-04 11:46:46 | INFO | train_inner | epoch 088:    373 / 393 loss=5.791, nll_loss=3.954, ppl=15.5, wps=25468.8, ups=0.39, wpb=65536, bsz=128, num_updates=34500, lr=0.000170251, gnorm=0.833, loss_scale=8, train_wall=252, gb_free=12.3, wall=89586
2022-03-04 11:47:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:47:40 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 8.219 | nll_loss 6.654 | ppl 100.73 | wps 66616 | wpb 2034.1 | bsz 4 | num_updates 34520 | best_loss 7.641
2022-03-04 11:47:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 34520 updates
2022-03-04 11:47:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 11:47:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 11:47:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 88 @ 34520 updates, score 8.219) (writing took 4.22884694673121 seconds)
2022-03-04 11:47:44 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-04 11:47:44 | INFO | train | epoch 088 | loss 5.73 | nll_loss 3.884 | ppl 14.76 | wps 25278.1 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 34520 | lr 0.000170202 | gnorm 0.832 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 89644
2022-03-04 11:47:44 | INFO | fairseq.trainer | begin training epoch 89
2022-03-04 11:47:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:50:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 11:51:13 | INFO | train_inner | epoch 089:     81 / 393 loss=5.684, nll_loss=3.83, ppl=14.22, wps=24503.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=34600, lr=0.000170005, gnorm=0.82, loss_scale=8, train_wall=254, gb_free=12.3, wall=89852
2022-03-04 11:55:30 | INFO | train_inner | epoch 089:    181 / 393 loss=5.696, nll_loss=3.844, ppl=14.36, wps=25468.3, ups=0.39, wpb=65530.2, bsz=128, num_updates=34700, lr=0.00016976, gnorm=0.818, loss_scale=8, train_wall=252, gb_free=12.3, wall=90110
2022-03-04 11:59:47 | INFO | train_inner | epoch 089:    281 / 393 loss=5.739, nll_loss=3.894, ppl=14.87, wps=25473.9, ups=0.39, wpb=65536, bsz=128, num_updates=34800, lr=0.000169516, gnorm=0.835, loss_scale=8, train_wall=252, gb_free=12.3, wall=90367
2022-03-04 12:04:04 | INFO | train_inner | epoch 089:    381 / 393 loss=5.79, nll_loss=3.953, ppl=15.49, wps=25478.1, ups=0.39, wpb=65536, bsz=128, num_updates=34900, lr=0.000169273, gnorm=0.837, loss_scale=8, train_wall=252, gb_free=12.3, wall=90624
2022-03-04 12:04:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:04:37 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 8.231 | nll_loss 6.661 | ppl 101.18 | wps 66214.4 | wpb 2034.1 | bsz 4 | num_updates 34912 | best_loss 7.641
2022-03-04 12:04:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 34912 updates
2022-03-04 12:04:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 12:04:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 12:04:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 89 @ 34912 updates, score 8.231) (writing took 4.221688727848232 seconds)
2022-03-04 12:04:42 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-04 12:04:42 | INFO | train | epoch 089 | loss 5.726 | nll_loss 3.879 | ppl 14.71 | wps 25218.8 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 34912 | lr 0.000169244 | gnorm 0.828 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 90662
2022-03-04 12:04:42 | INFO | fairseq.trainer | begin training epoch 90
2022-03-04 12:04:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:08:28 | INFO | train_inner | epoch 090:     88 / 393 loss=5.668, nll_loss=3.812, ppl=14.04, wps=24730.2, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=35000, lr=0.000169031, gnorm=0.85, loss_scale=8, train_wall=251, gb_free=12.3, wall=90888
2022-03-04 12:12:46 | INFO | train_inner | epoch 090:    188 / 393 loss=5.7, nll_loss=3.849, ppl=14.41, wps=25465.7, ups=0.39, wpb=65536, bsz=128, num_updates=35100, lr=0.00016879, gnorm=0.821, loss_scale=16, train_wall=252, gb_free=12.3, wall=91145
2022-03-04 12:14:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 12:17:05 | INFO | train_inner | epoch 090:    289 / 393 loss=5.743, nll_loss=3.899, ppl=14.92, wps=25239.1, ups=0.39, wpb=65530.9, bsz=128, num_updates=35200, lr=0.00016855, gnorm=0.839, loss_scale=8, train_wall=255, gb_free=12.3, wall=91405
2022-03-04 12:21:22 | INFO | train_inner | epoch 090:    389 / 393 loss=5.785, nll_loss=3.947, ppl=15.42, wps=25477.7, ups=0.39, wpb=65536, bsz=128, num_updates=35300, lr=0.000168311, gnorm=0.843, loss_scale=8, train_wall=252, gb_free=12.3, wall=91662
2022-03-04 12:21:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:21:35 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 8.203 | nll_loss 6.641 | ppl 99.8 | wps 66518 | wpb 2034.1 | bsz 4 | num_updates 35304 | best_loss 7.641
2022-03-04 12:21:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 35304 updates
2022-03-04 12:21:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 12:21:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 12:21:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 90 @ 35304 updates, score 8.203) (writing took 4.202971528284252 seconds)
2022-03-04 12:21:39 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-04 12:21:39 | INFO | train | epoch 090 | loss 5.722 | nll_loss 3.875 | ppl 14.67 | wps 25220.8 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 35304 | lr 0.000168302 | gnorm 0.838 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 91679
2022-03-04 12:21:39 | INFO | fairseq.trainer | begin training epoch 91
2022-03-04 12:21:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:25:46 | INFO | train_inner | epoch 091:     96 / 393 loss=5.654, nll_loss=3.796, ppl=13.89, wps=24741.8, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=35400, lr=0.000168073, gnorm=0.836, loss_scale=8, train_wall=251, gb_free=12.3, wall=91926
2022-03-04 12:30:03 | INFO | train_inner | epoch 091:    196 / 393 loss=5.701, nll_loss=3.85, ppl=14.42, wps=25483.7, ups=0.39, wpb=65536, bsz=128, num_updates=35500, lr=0.000167836, gnorm=0.846, loss_scale=8, train_wall=252, gb_free=12.3, wall=92183
2022-03-04 12:34:21 | INFO | train_inner | epoch 091:    296 / 393 loss=5.74, nll_loss=3.895, ppl=14.88, wps=25482.2, ups=0.39, wpb=65535.4, bsz=128, num_updates=35600, lr=0.0001676, gnorm=0.837, loss_scale=8, train_wall=252, gb_free=12.3, wall=92440
2022-03-04 12:37:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 12:38:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:38:32 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 8.232 | nll_loss 6.666 | ppl 101.53 | wps 66612.6 | wpb 2034.1 | bsz 4 | num_updates 35696 | best_loss 7.641
2022-03-04 12:38:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 35696 updates
2022-03-04 12:38:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 12:38:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 12:38:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 91 @ 35696 updates, score 8.232) (writing took 4.312203718349338 seconds)
2022-03-04 12:38:37 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-04 12:38:37 | INFO | train | epoch 091 | loss 5.719 | nll_loss 3.871 | ppl 14.64 | wps 25220.2 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 35696 | lr 0.000167375 | gnorm 0.839 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 92696
2022-03-04 12:38:37 | INFO | fairseq.trainer | begin training epoch 92
2022-03-04 12:38:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:38:47 | INFO | train_inner | epoch 092:      4 / 393 loss=5.783, nll_loss=3.945, ppl=15.4, wps=24484.4, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=35700, lr=0.000167365, gnorm=0.84, loss_scale=8, train_wall=254, gb_free=12.3, wall=92707
2022-03-04 12:43:04 | INFO | train_inner | epoch 092:    104 / 393 loss=5.652, nll_loss=3.793, ppl=13.86, wps=25471.9, ups=0.39, wpb=65536, bsz=128, num_updates=35800, lr=0.000167132, gnorm=0.835, loss_scale=8, train_wall=252, gb_free=12.3, wall=92964
2022-03-04 12:47:22 | INFO | train_inner | epoch 092:    204 / 393 loss=5.701, nll_loss=3.85, ppl=14.42, wps=25406.9, ups=0.39, wpb=65535.4, bsz=128, num_updates=35900, lr=0.000166899, gnorm=0.837, loss_scale=8, train_wall=253, gb_free=12.3, wall=93222
2022-03-04 12:51:40 | INFO | train_inner | epoch 092:    304 / 393 loss=5.743, nll_loss=3.899, ppl=14.92, wps=25443.9, ups=0.39, wpb=65536, bsz=128, num_updates=36000, lr=0.000166667, gnorm=0.856, loss_scale=8, train_wall=253, gb_free=12.3, wall=93480
2022-03-04 12:55:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:55:31 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 8.209 | nll_loss 6.643 | ppl 99.92 | wps 66447.1 | wpb 2034.1 | bsz 4 | num_updates 36089 | best_loss 7.641
2022-03-04 12:55:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 36089 updates
2022-03-04 12:55:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 12:55:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 12:55:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 92 @ 36089 updates, score 8.209) (writing took 4.299084379337728 seconds)
2022-03-04 12:55:35 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-04 12:55:35 | INFO | train | epoch 092 | loss 5.717 | nll_loss 3.868 | ppl 14.6 | wps 25252.7 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 36089 | lr 0.000166461 | gnorm 0.842 | loss_scale 8 | train_wall 992 | gb_free 12.3 | wall 93715
2022-03-04 12:55:35 | INFO | fairseq.trainer | begin training epoch 93
2022-03-04 12:55:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:56:04 | INFO | train_inner | epoch 093:     11 / 393 loss=5.764, nll_loss=3.923, ppl=15.17, wps=24717.8, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=36100, lr=0.000166436, gnorm=0.839, loss_scale=8, train_wall=251, gb_free=12.3, wall=93744
2022-03-04 13:00:21 | INFO | train_inner | epoch 093:    111 / 393 loss=5.656, nll_loss=3.798, ppl=13.91, wps=25477.2, ups=0.39, wpb=65536, bsz=128, num_updates=36200, lr=0.000166206, gnorm=0.837, loss_scale=16, train_wall=252, gb_free=12.3, wall=94001
2022-03-04 13:02:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 13:04:41 | INFO | train_inner | epoch 093:    212 / 393 loss=5.694, nll_loss=3.842, ppl=14.34, wps=25215.9, ups=0.38, wpb=65530.9, bsz=128, num_updates=36300, lr=0.000165977, gnorm=0.838, loss_scale=8, train_wall=255, gb_free=12.3, wall=94261
2022-03-04 13:08:58 | INFO | train_inner | epoch 093:    312 / 393 loss=5.744, nll_loss=3.901, ppl=14.93, wps=25476.9, ups=0.39, wpb=65536, bsz=128, num_updates=36400, lr=0.000165748, gnorm=0.847, loss_scale=8, train_wall=252, gb_free=12.3, wall=94518
2022-03-04 13:12:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:12:29 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 8.218 | nll_loss 6.647 | ppl 100.25 | wps 66662.1 | wpb 2034.1 | bsz 4 | num_updates 36481 | best_loss 7.641
2022-03-04 13:12:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 36481 updates
2022-03-04 13:12:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 13:12:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 13:12:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 93 @ 36481 updates, score 8.218) (writing took 4.2822040962055326 seconds)
2022-03-04 13:12:33 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-04 13:12:33 | INFO | train | epoch 093 | loss 5.713 | nll_loss 3.864 | ppl 14.56 | wps 25217.7 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 36481 | lr 0.000165564 | gnorm 0.841 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 94733
2022-03-04 13:12:33 | INFO | fairseq.trainer | begin training epoch 94
2022-03-04 13:12:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:13:22 | INFO | train_inner | epoch 094:     19 / 393 loss=5.748, nll_loss=3.904, ppl=14.97, wps=24736, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=36500, lr=0.000165521, gnorm=0.837, loss_scale=8, train_wall=251, gb_free=12.3, wall=94782
2022-03-04 13:17:39 | INFO | train_inner | epoch 094:    119 / 393 loss=5.653, nll_loss=3.794, ppl=13.87, wps=25471.4, ups=0.39, wpb=65530.9, bsz=128, num_updates=36600, lr=0.000165295, gnorm=0.829, loss_scale=8, train_wall=252, gb_free=12.3, wall=95039
2022-03-04 13:21:57 | INFO | train_inner | epoch 094:    219 / 393 loss=5.698, nll_loss=3.847, ppl=14.39, wps=25462.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=36700, lr=0.00016507, gnorm=0.85, loss_scale=8, train_wall=252, gb_free=12.3, wall=95296
2022-03-04 13:25:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 13:26:16 | INFO | train_inner | epoch 094:    320 / 393 loss=5.742, nll_loss=3.898, ppl=14.91, wps=25222.4, ups=0.38, wpb=65536, bsz=128, num_updates=36800, lr=0.000164845, gnorm=0.847, loss_scale=8, train_wall=255, gb_free=12.3, wall=95556
2022-03-04 13:29:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:29:26 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 8.214 | nll_loss 6.646 | ppl 100.16 | wps 66881.5 | wpb 2034.1 | bsz 4 | num_updates 36873 | best_loss 7.641
2022-03-04 13:29:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 36873 updates
2022-03-04 13:29:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 13:29:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 13:29:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 94 @ 36873 updates, score 8.214) (writing took 4.241583616472781 seconds)
2022-03-04 13:29:31 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-04 13:29:31 | INFO | train | epoch 094 | loss 5.711 | nll_loss 3.861 | ppl 14.53 | wps 25214.4 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 36873 | lr 0.000164682 | gnorm 0.843 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 95751
2022-03-04 13:29:31 | INFO | fairseq.trainer | begin training epoch 95
2022-03-04 13:29:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:30:40 | INFO | train_inner | epoch 095:     27 / 393 loss=5.746, nll_loss=3.902, ppl=14.95, wps=24738.8, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=36900, lr=0.000164622, gnorm=0.851, loss_scale=8, train_wall=251, gb_free=12.3, wall=95820
2022-03-04 13:34:57 | INFO | train_inner | epoch 095:    127 / 393 loss=5.651, nll_loss=3.792, ppl=13.85, wps=25479.2, ups=0.39, wpb=65530.2, bsz=128, num_updates=37000, lr=0.000164399, gnorm=0.831, loss_scale=8, train_wall=252, gb_free=12.3, wall=96077
2022-03-04 13:39:15 | INFO | train_inner | epoch 095:    227 / 393 loss=5.703, nll_loss=3.851, ppl=14.43, wps=25473.3, ups=0.39, wpb=65536, bsz=128, num_updates=37100, lr=0.000164177, gnorm=0.848, loss_scale=8, train_wall=252, gb_free=12.3, wall=96334
2022-03-04 13:43:32 | INFO | train_inner | epoch 095:    327 / 393 loss=5.746, nll_loss=3.902, ppl=14.95, wps=25479.9, ups=0.39, wpb=65536, bsz=128, num_updates=37200, lr=0.000163956, gnorm=0.855, loss_scale=8, train_wall=252, gb_free=12.3, wall=96592
2022-03-04 13:46:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:46:24 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 8.232 | nll_loss 6.658 | ppl 100.99 | wps 66558.6 | wpb 2034.1 | bsz 4 | num_updates 37266 | best_loss 7.641
2022-03-04 13:46:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 37266 updates
2022-03-04 13:46:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 13:46:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 13:46:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 95 @ 37266 updates, score 8.232) (writing took 4.2446118434891105 seconds)
2022-03-04 13:46:28 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-04 13:46:28 | INFO | train | epoch 095 | loss 5.708 | nll_loss 3.858 | ppl 14.5 | wps 25286.5 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 37266 | lr 0.000163811 | gnorm 0.847 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 96768
2022-03-04 13:46:28 | INFO | fairseq.trainer | begin training epoch 96
2022-03-04 13:46:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:47:56 | INFO | train_inner | epoch 096:     34 / 393 loss=5.727, nll_loss=3.88, ppl=14.72, wps=24732.9, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=37300, lr=0.000163737, gnorm=0.845, loss_scale=8, train_wall=251, gb_free=12.3, wall=96855
2022-03-04 13:51:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 13:52:15 | INFO | train_inner | epoch 096:    135 / 393 loss=5.651, nll_loss=3.792, ppl=13.85, wps=25227.4, ups=0.38, wpb=65536, bsz=128, num_updates=37400, lr=0.000163517, gnorm=0.842, loss_scale=8, train_wall=255, gb_free=12.3, wall=97115
2022-03-04 13:56:33 | INFO | train_inner | epoch 096:    235 / 393 loss=5.706, nll_loss=3.856, ppl=14.48, wps=25473.5, ups=0.39, wpb=65536, bsz=128, num_updates=37500, lr=0.000163299, gnorm=0.85, loss_scale=8, train_wall=252, gb_free=12.3, wall=97373
2022-03-04 14:00:50 | INFO | train_inner | epoch 096:    335 / 393 loss=5.743, nll_loss=3.898, ppl=14.91, wps=25482.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=37600, lr=0.000163082, gnorm=0.863, loss_scale=8, train_wall=252, gb_free=12.3, wall=97630
2022-03-04 14:03:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:03:21 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 8.241 | nll_loss 6.673 | ppl 102.05 | wps 66406.4 | wpb 2034.1 | bsz 4 | num_updates 37658 | best_loss 7.641
2022-03-04 14:03:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 37658 updates
2022-03-04 14:03:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 14:03:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 14:03:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 96 @ 37658 updates, score 8.241) (writing took 4.242283968254924 seconds)
2022-03-04 14:03:25 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-04 14:03:25 | INFO | train | epoch 096 | loss 5.705 | nll_loss 3.854 | ppl 14.46 | wps 25222.8 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 37658 | lr 0.000162956 | gnorm 0.849 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 97785
2022-03-04 14:03:25 | INFO | fairseq.trainer | begin training epoch 97
2022-03-04 14:03:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:05:13 | INFO | train_inner | epoch 097:     42 / 393 loss=5.711, nll_loss=3.861, ppl=14.53, wps=24748.8, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=37700, lr=0.000162866, gnorm=0.843, loss_scale=8, train_wall=251, gb_free=12.3, wall=97893
2022-03-04 14:09:31 | INFO | train_inner | epoch 097:    142 / 393 loss=5.652, nll_loss=3.793, ppl=13.86, wps=25486.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=37800, lr=0.00016265, gnorm=0.834, loss_scale=8, train_wall=252, gb_free=12.3, wall=98150
2022-03-04 14:13:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 14:13:50 | INFO | train_inner | epoch 097:    243 / 393 loss=5.705, nll_loss=3.854, ppl=14.46, wps=25230.8, ups=0.38, wpb=65536, bsz=128, num_updates=37900, lr=0.000162435, gnorm=0.851, loss_scale=8, train_wall=255, gb_free=12.3, wall=98410
2022-03-04 14:18:08 | INFO | train_inner | epoch 097:    343 / 393 loss=5.747, nll_loss=3.903, ppl=14.96, wps=25475.9, ups=0.39, wpb=65536, bsz=128, num_updates=38000, lr=0.000162221, gnorm=0.851, loss_scale=8, train_wall=252, gb_free=12.3, wall=98667
2022-03-04 14:20:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:20:18 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 8.236 | nll_loss 6.668 | ppl 101.66 | wps 66611.4 | wpb 2034.1 | bsz 4 | num_updates 38050 | best_loss 7.641
2022-03-04 14:20:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 38050 updates
2022-03-04 14:20:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 14:20:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 14:20:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 97 @ 38050 updates, score 8.236) (writing took 4.277590333484113 seconds)
2022-03-04 14:20:23 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-04 14:20:23 | INFO | train | epoch 097 | loss 5.702 | nll_loss 3.852 | ppl 14.44 | wps 25225.3 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 38050 | lr 0.000162115 | gnorm 0.846 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 98803
2022-03-04 14:20:23 | INFO | fairseq.trainer | begin training epoch 98
2022-03-04 14:20:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:22:31 | INFO | train_inner | epoch 098:     50 / 393 loss=5.707, nll_loss=3.857, ppl=14.49, wps=24735.1, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=38100, lr=0.000162008, gnorm=0.852, loss_scale=8, train_wall=251, gb_free=12.3, wall=98931
2022-03-04 14:26:49 | INFO | train_inner | epoch 098:    150 / 393 loss=5.655, nll_loss=3.796, ppl=13.89, wps=25473.1, ups=0.39, wpb=65536, bsz=128, num_updates=38200, lr=0.000161796, gnorm=0.838, loss_scale=8, train_wall=252, gb_free=12.3, wall=99189
2022-03-04 14:31:06 | INFO | train_inner | epoch 098:    250 / 393 loss=5.702, nll_loss=3.851, ppl=14.43, wps=25481.2, ups=0.39, wpb=65536, bsz=128, num_updates=38300, lr=0.000161585, gnorm=0.856, loss_scale=8, train_wall=252, gb_free=12.3, wall=99446
2022-03-04 14:35:23 | INFO | train_inner | epoch 098:    350 / 393 loss=5.747, nll_loss=3.904, ppl=14.97, wps=25478.3, ups=0.39, wpb=65530.2, bsz=128, num_updates=38400, lr=0.000161374, gnorm=0.851, loss_scale=8, train_wall=252, gb_free=12.3, wall=99703
2022-03-04 14:36:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 14:37:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:37:16 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 8.243 | nll_loss 6.686 | ppl 102.94 | wps 66622.5 | wpb 2034.1 | bsz 4 | num_updates 38442 | best_loss 7.641
2022-03-04 14:37:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 38442 updates
2022-03-04 14:37:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 14:37:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 14:37:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 98 @ 38442 updates, score 8.243) (writing took 4.285361168906093 seconds)
2022-03-04 14:37:20 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-04 14:37:20 | INFO | train | epoch 098 | loss 5.701 | nll_loss 3.85 | ppl 14.42 | wps 25220.6 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 38442 | lr 0.000161286 | gnorm 0.849 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 99820
2022-03-04 14:37:20 | INFO | fairseq.trainer | begin training epoch 99
2022-03-04 14:37:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:39:49 | INFO | train_inner | epoch 099:     58 / 393 loss=5.691, nll_loss=3.838, ppl=14.3, wps=24504.2, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=38500, lr=0.000161165, gnorm=0.855, loss_scale=8, train_wall=254, gb_free=12.3, wall=99969
2022-03-04 14:44:07 | INFO | train_inner | epoch 099:    158 / 393 loss=5.661, nll_loss=3.803, ppl=13.96, wps=25485, ups=0.39, wpb=65536, bsz=128, num_updates=38600, lr=0.000160956, gnorm=0.852, loss_scale=8, train_wall=252, gb_free=12.3, wall=100226
2022-03-04 14:48:24 | INFO | train_inner | epoch 099:    258 / 393 loss=5.705, nll_loss=3.855, ppl=14.47, wps=25490.1, ups=0.39, wpb=65530.9, bsz=128, num_updates=38700, lr=0.000160748, gnorm=0.852, loss_scale=8, train_wall=252, gb_free=12.3, wall=100483
2022-03-04 14:52:41 | INFO | train_inner | epoch 099:    358 / 393 loss=5.744, nll_loss=3.9, ppl=14.93, wps=25491.3, ups=0.39, wpb=65536, bsz=128, num_updates=38800, lr=0.00016054, gnorm=0.859, loss_scale=8, train_wall=252, gb_free=12.3, wall=100741
2022-03-04 14:54:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:54:13 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 8.225 | nll_loss 6.658 | ppl 101.01 | wps 66819.6 | wpb 2034.1 | bsz 4 | num_updates 38835 | best_loss 7.641
2022-03-04 14:54:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 38835 updates
2022-03-04 14:54:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 14:54:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 14:54:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 99 @ 38835 updates, score 8.225) (writing took 4.222285029478371 seconds)
2022-03-04 14:54:17 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-04 14:54:17 | INFO | train | epoch 099 | loss 5.698 | nll_loss 3.847 | ppl 14.39 | wps 25297.7 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 38835 | lr 0.000160468 | gnorm 0.854 | loss_scale 8 | train_wall 990 | gb_free 12.3 | wall 100837
2022-03-04 14:54:17 | INFO | fairseq.trainer | begin training epoch 100
2022-03-04 14:54:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:57:04 | INFO | train_inner | epoch 100:     65 / 393 loss=5.676, nll_loss=3.821, ppl=14.13, wps=24744.4, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=38900, lr=0.000160334, gnorm=0.847, loss_scale=8, train_wall=251, gb_free=12.3, wall=101004
2022-03-04 14:59:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 15:01:24 | INFO | train_inner | epoch 100:    166 / 393 loss=5.66, nll_loss=3.802, ppl=13.95, wps=25221.9, ups=0.38, wpb=65536, bsz=128, num_updates=39000, lr=0.000160128, gnorm=0.849, loss_scale=8, train_wall=255, gb_free=12.3, wall=101264
2022-03-04 15:05:41 | INFO | train_inner | epoch 100:    266 / 393 loss=5.708, nll_loss=3.858, ppl=14.5, wps=25480.5, ups=0.39, wpb=65535.4, bsz=128, num_updates=39100, lr=0.000159923, gnorm=0.855, loss_scale=8, train_wall=252, gb_free=12.3, wall=101521
2022-03-04 15:09:59 | INFO | train_inner | epoch 100:    366 / 393 loss=5.742, nll_loss=3.898, ppl=14.9, wps=25469.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=39200, lr=0.000159719, gnorm=0.858, loss_scale=8, train_wall=252, gb_free=12.3, wall=101779
2022-03-04 15:11:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:11:10 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 8.249 | nll_loss 6.688 | ppl 103.08 | wps 66262.7 | wpb 2034.1 | bsz 4 | num_updates 39227 | best_loss 7.641
2022-03-04 15:11:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 39227 updates
2022-03-04 15:11:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 15:11:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 15:11:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 100 @ 39227 updates, score 8.249) (writing took 4.225228068418801 seconds)
2022-03-04 15:11:15 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-04 15:11:15 | INFO | train | epoch 100 | loss 5.696 | nll_loss 3.844 | ppl 14.36 | wps 25217.3 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 39227 | lr 0.000159664 | gnorm 0.853 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 101855
2022-03-04 15:11:15 | INFO | fairseq.trainer | begin training epoch 101
2022-03-04 15:11:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:14:23 | INFO | train_inner | epoch 101:     73 / 393 loss=5.671, nll_loss=3.816, ppl=14.08, wps=24731.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=39300, lr=0.000159516, gnorm=0.858, loss_scale=8, train_wall=251, gb_free=12.3, wall=102042
2022-03-04 15:18:40 | INFO | train_inner | epoch 101:    173 / 393 loss=5.668, nll_loss=3.812, ppl=14.04, wps=25473.7, ups=0.39, wpb=65530.2, bsz=128, num_updates=39400, lr=0.000159313, gnorm=0.857, loss_scale=8, train_wall=252, gb_free=12.3, wall=102300
2022-03-04 15:22:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 15:23:00 | INFO | train_inner | epoch 101:    274 / 393 loss=5.702, nll_loss=3.851, ppl=14.43, wps=25229.9, ups=0.38, wpb=65536, bsz=128, num_updates=39500, lr=0.000159111, gnorm=0.855, loss_scale=8, train_wall=255, gb_free=12.3, wall=102559
2022-03-04 15:27:17 | INFO | train_inner | epoch 101:    374 / 393 loss=5.749, nll_loss=3.905, ppl=14.98, wps=25479.3, ups=0.39, wpb=65536, bsz=128, num_updates=39600, lr=0.00015891, gnorm=0.86, loss_scale=8, train_wall=252, gb_free=12.3, wall=102817
2022-03-04 15:28:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:28:08 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 8.25 | nll_loss 6.678 | ppl 102.38 | wps 66844.2 | wpb 2034.1 | bsz 4 | num_updates 39619 | best_loss 7.641
2022-03-04 15:28:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 39619 updates
2022-03-04 15:28:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 15:28:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 15:28:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 101 @ 39619 updates, score 8.25) (writing took 4.243527544662356 seconds)
2022-03-04 15:28:12 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-04 15:28:12 | INFO | train | epoch 101 | loss 5.694 | nll_loss 3.842 | ppl 14.34 | wps 25223.2 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 39619 | lr 0.000158872 | gnorm 0.858 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 102872
2022-03-04 15:28:12 | INFO | fairseq.trainer | begin training epoch 102
2022-03-04 15:28:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:31:40 | INFO | train_inner | epoch 102:     81 / 393 loss=5.649, nll_loss=3.789, ppl=13.83, wps=24749.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=39700, lr=0.00015871, gnorm=0.851, loss_scale=8, train_wall=251, gb_free=12.3, wall=103080
2022-03-04 15:35:58 | INFO | train_inner | epoch 102:    181 / 393 loss=5.659, nll_loss=3.801, ppl=13.94, wps=25474.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=39800, lr=0.000158511, gnorm=0.861, loss_scale=8, train_wall=252, gb_free=12.3, wall=103338
2022-03-04 15:40:15 | INFO | train_inner | epoch 102:    281 / 393 loss=5.702, nll_loss=3.852, ppl=14.44, wps=25481.8, ups=0.39, wpb=65536, bsz=128, num_updates=39900, lr=0.000158312, gnorm=0.85, loss_scale=8, train_wall=252, gb_free=12.3, wall=103595
2022-03-04 15:44:32 | INFO | train_inner | epoch 102:    381 / 393 loss=5.756, nll_loss=3.913, ppl=15.07, wps=25476.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=40000, lr=0.000158114, gnorm=0.853, loss_scale=8, train_wall=252, gb_free=12.3, wall=103852
2022-03-04 15:45:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:45:05 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 8.257 | nll_loss 6.689 | ppl 103.15 | wps 66289.5 | wpb 2034.1 | bsz 4 | num_updates 40012 | best_loss 7.641
2022-03-04 15:45:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 40012 updates
2022-03-04 15:45:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 15:45:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 15:45:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 102 @ 40012 updates, score 8.257) (writing took 4.242821724154055 seconds)
2022-03-04 15:45:09 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-04 15:45:09 | INFO | train | epoch 102 | loss 5.692 | nll_loss 3.839 | ppl 14.31 | wps 25287.6 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 40012 | lr 0.00015809 | gnorm 0.852 | loss_scale 16 | train_wall 991 | gb_free 12.3 | wall 103889
2022-03-04 15:45:09 | INFO | fairseq.trainer | begin training epoch 103
2022-03-04 15:45:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:47:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 15:48:59 | INFO | train_inner | epoch 103:     89 / 393 loss=5.634, nll_loss=3.772, ppl=13.66, wps=24488, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=40100, lr=0.000157917, gnorm=0.857, loss_scale=8, train_wall=254, gb_free=12.3, wall=104118
2022-03-04 15:53:16 | INFO | train_inner | epoch 103:    189 / 393 loss=5.669, nll_loss=3.813, ppl=14.05, wps=25474.2, ups=0.39, wpb=65535.4, bsz=128, num_updates=40200, lr=0.00015772, gnorm=0.861, loss_scale=8, train_wall=252, gb_free=12.3, wall=104376
2022-03-04 15:57:33 | INFO | train_inner | epoch 103:    289 / 393 loss=5.702, nll_loss=3.851, ppl=14.43, wps=25470.6, ups=0.39, wpb=65536, bsz=128, num_updates=40300, lr=0.000157524, gnorm=0.854, loss_scale=8, train_wall=252, gb_free=12.3, wall=104633
2022-03-04 16:01:50 | INFO | train_inner | epoch 103:    389 / 393 loss=5.76, nll_loss=3.919, ppl=15.12, wps=25476.1, ups=0.39, wpb=65530.9, bsz=128, num_updates=40400, lr=0.000157329, gnorm=0.868, loss_scale=8, train_wall=252, gb_free=12.3, wall=104890
2022-03-04 16:01:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:02:03 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 8.232 | nll_loss 6.674 | ppl 102.12 | wps 66621.4 | wpb 2034.1 | bsz 4 | num_updates 40404 | best_loss 7.641
2022-03-04 16:02:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 40404 updates
2022-03-04 16:02:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 16:02:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 16:02:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 103 @ 40404 updates, score 8.232) (writing took 4.220058125443757 seconds)
2022-03-04 16:02:07 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-04 16:02:07 | INFO | train | epoch 103 | loss 5.689 | nll_loss 3.836 | ppl 14.28 | wps 25216.8 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 40404 | lr 0.000157321 | gnorm 0.86 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 104907
2022-03-04 16:02:07 | INFO | fairseq.trainer | begin training epoch 104
2022-03-04 16:02:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:06:14 | INFO | train_inner | epoch 104:     96 / 393 loss=5.622, nll_loss=3.758, ppl=13.52, wps=24746.4, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=40500, lr=0.000157135, gnorm=0.85, loss_scale=8, train_wall=251, gb_free=12.3, wall=105154
2022-03-04 16:10:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:10:34 | INFO | train_inner | epoch 104:    197 / 393 loss=5.666, nll_loss=3.809, ppl=14.01, wps=25225.4, ups=0.38, wpb=65536, bsz=128, num_updates=40600, lr=0.000156941, gnorm=0.853, loss_scale=8, train_wall=255, gb_free=12.3, wall=105414
2022-03-04 16:14:51 | INFO | train_inner | epoch 104:    297 / 393 loss=5.716, nll_loss=3.867, ppl=14.59, wps=25479.5, ups=0.39, wpb=65530.2, bsz=128, num_updates=40700, lr=0.000156748, gnorm=0.862, loss_scale=8, train_wall=252, gb_free=12.3, wall=105671
2022-03-04 16:18:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:19:00 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 8.235 | nll_loss 6.678 | ppl 102.41 | wps 66267.7 | wpb 2034.1 | bsz 4 | num_updates 40796 | best_loss 7.641
2022-03-04 16:19:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 40796 updates
2022-03-04 16:19:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 16:19:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 16:19:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 104 @ 40796 updates, score 8.235) (writing took 4.215060422196984 seconds)
2022-03-04 16:19:04 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-04 16:19:04 | INFO | train | epoch 104 | loss 5.687 | nll_loss 3.834 | ppl 14.26 | wps 25222.4 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 40796 | lr 0.000156564 | gnorm 0.856 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 105924
2022-03-04 16:19:04 | INFO | fairseq.trainer | begin training epoch 105
2022-03-04 16:19:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:19:15 | INFO | train_inner | epoch 105:      4 / 393 loss=5.745, nll_loss=3.901, ppl=14.94, wps=24734.7, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=40800, lr=0.000156556, gnorm=0.865, loss_scale=8, train_wall=251, gb_free=12.3, wall=105935
2022-03-04 16:23:32 | INFO | train_inner | epoch 105:    104 / 393 loss=5.621, nll_loss=3.757, ppl=13.52, wps=25489.5, ups=0.39, wpb=65536, bsz=128, num_updates=40900, lr=0.000156365, gnorm=0.859, loss_scale=8, train_wall=252, gb_free=12.3, wall=106192
2022-03-04 16:27:49 | INFO | train_inner | epoch 105:    204 / 393 loss=5.667, nll_loss=3.81, ppl=14.03, wps=25475.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=41000, lr=0.000156174, gnorm=0.859, loss_scale=8, train_wall=252, gb_free=12.3, wall=106449
2022-03-04 16:32:06 | INFO | train_inner | epoch 105:    304 / 393 loss=5.709, nll_loss=3.859, ppl=14.51, wps=25480.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=41100, lr=0.000155984, gnorm=0.87, loss_scale=8, train_wall=252, gb_free=12.3, wall=106706
2022-03-04 16:32:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:35:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:35:57 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 8.237 | nll_loss 6.675 | ppl 102.18 | wps 66625.6 | wpb 2034.1 | bsz 4 | num_updates 41188 | best_loss 7.641
2022-03-04 16:35:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 41188 updates
2022-03-04 16:35:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 16:36:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 16:36:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 105 @ 41188 updates, score 8.237) (writing took 4.19866160210222 seconds)
2022-03-04 16:36:02 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-04 16:36:02 | INFO | train | epoch 105 | loss 5.686 | nll_loss 3.832 | ppl 14.24 | wps 25226.3 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 41188 | lr 0.000155817 | gnorm 0.866 | loss_scale 8 | train_wall 990 | gb_free 12.3 | wall 106941
2022-03-04 16:36:02 | INFO | fairseq.trainer | begin training epoch 106
2022-03-04 16:36:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:36:33 | INFO | train_inner | epoch 106:     12 / 393 loss=5.738, nll_loss=3.893, ppl=14.86, wps=24501.4, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=41200, lr=0.000155794, gnorm=0.873, loss_scale=8, train_wall=254, gb_free=12.3, wall=106972
2022-03-04 16:40:50 | INFO | train_inner | epoch 106:    112 / 393 loss=5.624, nll_loss=3.76, ppl=13.55, wps=25460.2, ups=0.39, wpb=65536, bsz=128, num_updates=41300, lr=0.000155606, gnorm=0.846, loss_scale=8, train_wall=253, gb_free=12.3, wall=107230
2022-03-04 16:45:08 | INFO | train_inner | epoch 106:    212 / 393 loss=5.67, nll_loss=3.814, ppl=14.06, wps=25420.2, ups=0.39, wpb=65536, bsz=128, num_updates=41400, lr=0.000155417, gnorm=0.852, loss_scale=8, train_wall=253, gb_free=12.3, wall=107488
2022-03-04 16:49:25 | INFO | train_inner | epoch 106:    312 / 393 loss=5.717, nll_loss=3.868, ppl=14.61, wps=25467.4, ups=0.39, wpb=65536, bsz=128, num_updates=41500, lr=0.00015523, gnorm=0.873, loss_scale=8, train_wall=252, gb_free=12.3, wall=107745
2022-03-04 16:52:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:52:56 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 8.247 | nll_loss 6.685 | ppl 102.88 | wps 66801.7 | wpb 2034.1 | bsz 4 | num_updates 41581 | best_loss 7.641
2022-03-04 16:52:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 41581 updates
2022-03-04 16:52:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 16:53:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 16:53:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 106 @ 41581 updates, score 8.247) (writing took 4.308175703510642 seconds)
2022-03-04 16:53:00 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-04 16:53:00 | INFO | train | epoch 106 | loss 5.684 | nll_loss 3.83 | ppl 14.22 | wps 25260.9 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 41581 | lr 0.000155079 | gnorm 0.861 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 107960
2022-03-04 16:53:00 | INFO | fairseq.trainer | begin training epoch 107
2022-03-04 16:53:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:53:49 | INFO | train_inner | epoch 107:     19 / 393 loss=5.717, nll_loss=3.869, ppl=14.61, wps=24728.1, ups=0.38, wpb=65243.5, bsz=127.4, num_updates=41600, lr=0.000155043, gnorm=0.877, loss_scale=8, train_wall=251, gb_free=12.3, wall=108009
2022-03-04 16:56:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:58:09 | INFO | train_inner | epoch 107:    120 / 393 loss=5.625, nll_loss=3.761, ppl=13.56, wps=25227.6, ups=0.38, wpb=65535.4, bsz=128, num_updates=41700, lr=0.000154857, gnorm=0.847, loss_scale=8, train_wall=255, gb_free=12.3, wall=108269
2022-03-04 17:02:26 | INFO | train_inner | epoch 107:    220 / 393 loss=5.67, nll_loss=3.813, ppl=14.06, wps=25468.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=41800, lr=0.000154672, gnorm=0.858, loss_scale=8, train_wall=252, gb_free=12.3, wall=108526
2022-03-04 17:06:43 | INFO | train_inner | epoch 107:    320 / 393 loss=5.712, nll_loss=3.862, ppl=14.54, wps=25469.8, ups=0.39, wpb=65536, bsz=128, num_updates=41900, lr=0.000154487, gnorm=0.874, loss_scale=8, train_wall=252, gb_free=12.3, wall=108783
2022-03-04 17:09:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:09:53 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 8.239 | nll_loss 6.667 | ppl 101.6 | wps 66497.9 | wpb 2034.1 | bsz 4 | num_updates 41973 | best_loss 7.641
2022-03-04 17:09:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 41973 updates
2022-03-04 17:09:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 17:09:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 17:09:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 107 @ 41973 updates, score 8.239) (writing took 4.221240075305104 seconds)
2022-03-04 17:09:58 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-04 17:09:58 | INFO | train | epoch 107 | loss 5.682 | nll_loss 3.827 | ppl 14.19 | wps 25219.8 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 41973 | lr 0.000154353 | gnorm 0.863 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 108977
2022-03-04 17:09:58 | INFO | fairseq.trainer | begin training epoch 108
2022-03-04 17:09:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:11:07 | INFO | train_inner | epoch 108:     27 / 393 loss=5.718, nll_loss=3.869, ppl=14.61, wps=24741.7, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=42000, lr=0.000154303, gnorm=0.873, loss_scale=8, train_wall=251, gb_free=12.3, wall=109047
2022-03-04 17:15:24 | INFO | train_inner | epoch 108:    127 / 393 loss=5.626, nll_loss=3.763, ppl=13.57, wps=25478.5, ups=0.39, wpb=65536, bsz=128, num_updates=42100, lr=0.00015412, gnorm=0.862, loss_scale=8, train_wall=252, gb_free=12.3, wall=109304
2022-03-04 17:19:42 | INFO | train_inner | epoch 108:    227 / 393 loss=5.67, nll_loss=3.814, ppl=14.06, wps=25470.2, ups=0.39, wpb=65530.2, bsz=128, num_updates=42200, lr=0.000153937, gnorm=0.865, loss_scale=16, train_wall=252, gb_free=12.3, wall=109561
2022-03-04 17:23:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:24:01 | INFO | train_inner | epoch 108:    328 / 393 loss=5.716, nll_loss=3.867, ppl=14.59, wps=25220.5, ups=0.38, wpb=65536, bsz=128, num_updates=42300, lr=0.000153755, gnorm=0.863, loss_scale=8, train_wall=255, gb_free=12.3, wall=109821
2022-03-04 17:26:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:26:51 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 8.234 | nll_loss 6.668 | ppl 101.66 | wps 66623.2 | wpb 2034.1 | bsz 4 | num_updates 42365 | best_loss 7.641
2022-03-04 17:26:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 42365 updates
2022-03-04 17:26:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 17:26:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 17:26:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 108 @ 42365 updates, score 8.234) (writing took 4.226222571916878 seconds)
2022-03-04 17:26:55 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-04 17:26:55 | INFO | train | epoch 108 | loss 5.679 | nll_loss 3.825 | ppl 14.17 | wps 25217.6 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 42365 | lr 0.000153637 | gnorm 0.866 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 109995
2022-03-04 17:26:55 | INFO | fairseq.trainer | begin training epoch 109
2022-03-04 17:26:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:28:25 | INFO | train_inner | epoch 109:     35 / 393 loss=5.697, nll_loss=3.845, ppl=14.37, wps=24738, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=42400, lr=0.000153574, gnorm=0.865, loss_scale=8, train_wall=251, gb_free=12.3, wall=110085
2022-03-04 17:32:43 | INFO | train_inner | epoch 109:    135 / 393 loss=5.635, nll_loss=3.773, ppl=13.67, wps=25473.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=42500, lr=0.000153393, gnorm=0.865, loss_scale=8, train_wall=252, gb_free=12.3, wall=110342
2022-03-04 17:37:00 | INFO | train_inner | epoch 109:    235 / 393 loss=5.671, nll_loss=3.815, ppl=14.07, wps=25483.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=42600, lr=0.000153213, gnorm=0.867, loss_scale=8, train_wall=252, gb_free=12.3, wall=110600
2022-03-04 17:41:17 | INFO | train_inner | epoch 109:    335 / 393 loss=5.72, nll_loss=3.872, ppl=14.64, wps=25482.8, ups=0.39, wpb=65536, bsz=128, num_updates=42700, lr=0.000153033, gnorm=0.873, loss_scale=8, train_wall=252, gb_free=12.3, wall=110857
2022-03-04 17:43:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:43:48 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 8.228 | nll_loss 6.66 | ppl 101.16 | wps 66591.3 | wpb 2034.1 | bsz 4 | num_updates 42758 | best_loss 7.641
2022-03-04 17:43:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 42758 updates
2022-03-04 17:43:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 17:43:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 17:43:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 109 @ 42758 updates, score 8.228) (writing took 4.232217893004417 seconds)
2022-03-04 17:43:52 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-04 17:43:52 | INFO | train | epoch 109 | loss 5.678 | nll_loss 3.823 | ppl 14.15 | wps 25288.3 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 42758 | lr 0.00015293 | gnorm 0.867 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 111012
2022-03-04 17:43:52 | INFO | fairseq.trainer | begin training epoch 110
2022-03-04 17:43:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:45:41 | INFO | train_inner | epoch 110:     42 / 393 loss=5.686, nll_loss=3.832, ppl=14.24, wps=24740.1, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=42800, lr=0.000152854, gnorm=0.863, loss_scale=16, train_wall=251, gb_free=12.3, wall=111120
2022-03-04 17:46:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:50:00 | INFO | train_inner | epoch 110:    143 / 393 loss=5.628, nll_loss=3.765, ppl=13.6, wps=25229.5, ups=0.39, wpb=65530.2, bsz=128, num_updates=42900, lr=0.000152676, gnorm=0.861, loss_scale=8, train_wall=255, gb_free=12.3, wall=111380
2022-03-04 17:54:17 | INFO | train_inner | epoch 110:    243 / 393 loss=5.674, nll_loss=3.819, ppl=14.11, wps=25486.3, ups=0.39, wpb=65536, bsz=128, num_updates=43000, lr=0.000152499, gnorm=0.873, loss_scale=8, train_wall=252, gb_free=12.3, wall=111637
2022-03-04 17:58:35 | INFO | train_inner | epoch 110:    343 / 393 loss=5.721, nll_loss=3.873, ppl=14.66, wps=25483.4, ups=0.39, wpb=65536, bsz=128, num_updates=43100, lr=0.000152322, gnorm=0.871, loss_scale=8, train_wall=252, gb_free=12.3, wall=111894
2022-03-04 18:00:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:00:45 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 8.251 | nll_loss 6.691 | ppl 103.29 | wps 67270.2 | wpb 2034.1 | bsz 4 | num_updates 43150 | best_loss 7.641
2022-03-04 18:00:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 43150 updates
2022-03-04 18:00:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 18:00:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 18:00:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 110 @ 43150 updates, score 8.251) (writing took 4.270205221138895 seconds)
2022-03-04 18:00:50 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-04 18:00:50 | INFO | train | epoch 110 | loss 5.677 | nll_loss 3.822 | ppl 14.14 | wps 25226.3 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 43150 | lr 0.000152233 | gnorm 0.869 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 112030
2022-03-04 18:00:50 | INFO | fairseq.trainer | begin training epoch 111
2022-03-04 18:00:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:02:58 | INFO | train_inner | epoch 111:     50 / 393 loss=5.671, nll_loss=3.815, ppl=14.07, wps=24743.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=43200, lr=0.000152145, gnorm=0.877, loss_scale=8, train_wall=251, gb_free=12.3, wall=112158
2022-03-04 18:07:16 | INFO | train_inner | epoch 111:    150 / 393 loss=5.64, nll_loss=3.778, ppl=13.72, wps=25473.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=43300, lr=0.000151969, gnorm=0.868, loss_scale=8, train_wall=252, gb_free=12.3, wall=112415
2022-03-04 18:08:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:11:35 | INFO | train_inner | epoch 111:    251 / 393 loss=5.67, nll_loss=3.814, ppl=14.06, wps=25232.3, ups=0.39, wpb=65535.4, bsz=128, num_updates=43400, lr=0.000151794, gnorm=0.882, loss_scale=8, train_wall=255, gb_free=12.3, wall=112675
2022-03-04 18:15:53 | INFO | train_inner | epoch 111:    351 / 393 loss=5.724, nll_loss=3.876, ppl=14.68, wps=25473.4, ups=0.39, wpb=65536, bsz=128, num_updates=43500, lr=0.00015162, gnorm=0.878, loss_scale=8, train_wall=252, gb_free=12.3, wall=112932
2022-03-04 18:17:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:17:43 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 8.23 | nll_loss 6.663 | ppl 101.31 | wps 66578.4 | wpb 2034.1 | bsz 4 | num_updates 43542 | best_loss 7.641
2022-03-04 18:17:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 43542 updates
2022-03-04 18:17:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 18:17:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 18:17:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 111 @ 43542 updates, score 8.23) (writing took 4.2257680194452405 seconds)
2022-03-04 18:17:47 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-04 18:17:47 | INFO | train | epoch 111 | loss 5.675 | nll_loss 3.819 | ppl 14.11 | wps 25221.6 | ups 0.39 | wpb 65461.4 | bsz 127.9 | num_updates 43542 | lr 0.000151546 | gnorm 0.876 | loss_scale 8 | train_wall 991 | gb_free 12.3 | wall 113047
2022-03-04 18:17:47 | INFO | fairseq.trainer | begin training epoch 112
2022-03-04 18:17:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:20:16 | INFO | train_inner | epoch 112:     58 / 393 loss=5.664, nll_loss=3.806, ppl=13.99, wps=24735.4, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=43600, lr=0.000151446, gnorm=0.875, loss_scale=8, train_wall=251, gb_free=12.3, wall=113196
2022-03-04 18:24:34 | INFO | train_inner | epoch 112:    158 / 393 loss=5.639, nll_loss=3.778, ppl=13.72, wps=25440.9, ups=0.39, wpb=65535.4, bsz=128, num_updates=43700, lr=0.000151272, gnorm=0.863, loss_scale=8, train_wall=253, gb_free=12.3, wall=113454
2022-03-04 18:28:52 | INFO | train_inner | epoch 112:    258 / 393 loss=5.678, nll_loss=3.823, ppl=14.15, wps=25386.5, ups=0.39, wpb=65536, bsz=128, num_updates=43800, lr=0.000151099, gnorm=0.874, loss_scale=8, train_wall=253, gb_free=12.3, wall=113712
2022-03-04 18:32:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:33:13 | INFO | train_inner | epoch 112:    359 / 393 loss=5.717, nll_loss=3.868, ppl=14.6, wps=25110.1, ups=0.38, wpb=65536, bsz=128, num_updates=43900, lr=0.000150927, gnorm=0.868, loss_scale=8, train_wall=256, gb_free=12.3, wall=113973
2022-03-04 18:34:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:34:43 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 8.243 | nll_loss 6.68 | ppl 102.53 | wps 66325.1 | wpb 2034.1 | bsz 4 | num_updates 43934 | best_loss 7.641
2022-03-04 18:34:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 43934 updates
2022-03-04 18:34:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 18:34:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 18:34:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 112 @ 43934 updates, score 8.243) (writing took 4.294209050014615 seconds)
2022-03-04 18:34:47 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-04 18:34:47 | INFO | train | epoch 112 | loss 5.673 | nll_loss 3.817 | ppl 14.09 | wps 25148.2 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 43934 | lr 0.000150869 | gnorm 0.868 | loss_scale 8 | train_wall 993 | gb_free 12.3 | wall 114067
2022-03-04 18:34:47 | INFO | fairseq.trainer | begin training epoch 113
2022-03-04 18:34:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:37:38 | INFO | train_inner | epoch 113:     66 / 393 loss=5.65, nll_loss=3.791, ppl=13.84, wps=24642.2, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=44000, lr=0.000150756, gnorm=0.864, loss_scale=8, train_wall=252, gb_free=12.3, wall=114238
2022-03-04 18:41:56 | INFO | train_inner | epoch 113:    166 / 393 loss=5.64, nll_loss=3.779, ppl=13.72, wps=25416, ups=0.39, wpb=65536, bsz=128, num_updates=44100, lr=0.000150585, gnorm=0.862, loss_scale=8, train_wall=253, gb_free=12.3, wall=114496
2022-03-04 18:46:13 | INFO | train_inner | epoch 113:    266 / 393 loss=5.678, nll_loss=3.823, ppl=14.15, wps=25429.6, ups=0.39, wpb=65536, bsz=128, num_updates=44200, lr=0.000150414, gnorm=0.877, loss_scale=8, train_wall=253, gb_free=12.3, wall=114753
2022-03-04 18:50:31 | INFO | train_inner | epoch 113:    366 / 393 loss=5.72, nll_loss=3.871, ppl=14.63, wps=25426, ups=0.39, wpb=65535.4, bsz=128, num_updates=44300, lr=0.000150244, gnorm=0.874, loss_scale=8, train_wall=253, gb_free=12.3, wall=115011
2022-03-04 18:51:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:51:43 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 8.256 | nll_loss 6.694 | ppl 103.57 | wps 66695.1 | wpb 2034.1 | bsz 4 | num_updates 44327 | best_loss 7.641
2022-03-04 18:51:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 44327 updates
2022-03-04 18:51:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 18:51:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 18:51:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 113 @ 44327 updates, score 8.256) (writing took 4.29397804941982 seconds)
2022-03-04 18:51:47 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-04 18:51:47 | INFO | train | epoch 113 | loss 5.671 | nll_loss 3.814 | ppl 14.07 | wps 25226.8 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 44327 | lr 0.000150199 | gnorm 0.871 | loss_scale 8 | train_wall 993 | gb_free 12.3 | wall 115087
2022-03-04 18:51:47 | INFO | fairseq.trainer | begin training epoch 114
2022-03-04 18:51:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:54:56 | INFO | train_inner | epoch 114:     73 / 393 loss=5.643, nll_loss=3.782, ppl=13.76, wps=24677.8, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=44400, lr=0.000150075, gnorm=0.878, loss_scale=16, train_wall=252, gb_free=12.3, wall=115275
2022-03-04 18:55:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:59:16 | INFO | train_inner | epoch 114:    174 / 393 loss=5.637, nll_loss=3.775, ppl=13.69, wps=25151.1, ups=0.38, wpb=65536, bsz=128, num_updates=44500, lr=0.000149906, gnorm=0.877, loss_scale=8, train_wall=256, gb_free=12.3, wall=115536
2022-03-04 19:03:34 | INFO | train_inner | epoch 114:    274 / 393 loss=5.677, nll_loss=3.822, ppl=14.14, wps=25394.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=44600, lr=0.000149738, gnorm=0.878, loss_scale=8, train_wall=253, gb_free=12.3, wall=115794
2022-03-04 19:07:52 | INFO | train_inner | epoch 114:    374 / 393 loss=5.724, nll_loss=3.877, ppl=14.69, wps=25391.4, ups=0.39, wpb=65536, bsz=128, num_updates=44700, lr=0.000149571, gnorm=0.886, loss_scale=8, train_wall=253, gb_free=12.3, wall=116052
2022-03-04 19:08:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:08:44 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 8.257 | nll_loss 6.693 | ppl 103.46 | wps 65743.3 | wpb 2034.1 | bsz 4 | num_updates 44719 | best_loss 7.641
2022-03-04 19:08:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 44719 updates
2022-03-04 19:08:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 19:08:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 19:08:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 114 @ 44719 updates, score 8.257) (writing took 4.375210070982575 seconds)
2022-03-04 19:08:48 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-04 19:08:48 | INFO | train | epoch 114 | loss 5.669 | nll_loss 3.812 | ppl 14.05 | wps 25141.1 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 44719 | lr 0.000149539 | gnorm 0.878 | loss_scale 8 | train_wall 993 | gb_free 12.3 | wall 116108
2022-03-04 19:08:48 | INFO | fairseq.trainer | begin training epoch 115
2022-03-04 19:08:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:12:17 | INFO | train_inner | epoch 115:     81 / 393 loss=5.634, nll_loss=3.771, ppl=13.66, wps=24654.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=44800, lr=0.000149404, gnorm=0.875, loss_scale=8, train_wall=252, gb_free=12.3, wall=116317
2022-03-04 19:16:35 | INFO | train_inner | epoch 115:    181 / 393 loss=5.639, nll_loss=3.778, ppl=13.72, wps=25415.2, ups=0.39, wpb=65536, bsz=128, num_updates=44900, lr=0.000149237, gnorm=0.864, loss_scale=8, train_wall=253, gb_free=12.3, wall=116575
2022-03-04 19:18:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:20:55 | INFO | train_inner | epoch 115:    282 / 393 loss=5.679, nll_loss=3.824, ppl=14.16, wps=25169.2, ups=0.38, wpb=65530.9, bsz=128, num_updates=45000, lr=0.000149071, gnorm=0.869, loss_scale=8, train_wall=255, gb_free=12.3, wall=116835
2022-03-04 19:25:13 | INFO | train_inner | epoch 115:    382 / 393 loss=5.723, nll_loss=3.875, ppl=14.68, wps=25414.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=45100, lr=0.000148906, gnorm=0.887, loss_scale=8, train_wall=253, gb_free=12.3, wall=117093
2022-03-04 19:25:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:25:44 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 8.253 | nll_loss 6.688 | ppl 103.11 | wps 66251.8 | wpb 2034.1 | bsz 4 | num_updates 45111 | best_loss 7.641
2022-03-04 19:25:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 45111 updates
2022-03-04 19:25:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 19:25:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 19:25:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 115 @ 45111 updates, score 8.253) (writing took 4.37784519046545 seconds)
2022-03-04 19:25:48 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-04 19:25:48 | INFO | train | epoch 115 | loss 5.667 | nll_loss 3.81 | ppl 14.02 | wps 25156 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 45111 | lr 0.000148888 | gnorm 0.873 | loss_scale 8 | train_wall 993 | gb_free 12.3 | wall 117128
2022-03-04 19:25:48 | INFO | fairseq.trainer | begin training epoch 116
2022-03-04 19:25:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:29:38 | INFO | train_inner | epoch 116:     89 / 393 loss=5.623, nll_loss=3.759, ppl=13.54, wps=24664.8, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=45200, lr=0.000148741, gnorm=0.875, loss_scale=8, train_wall=252, gb_free=12.3, wall=117357
2022-03-04 19:33:56 | INFO | train_inner | epoch 116:    189 / 393 loss=5.641, nll_loss=3.779, ppl=13.73, wps=25389.1, ups=0.39, wpb=65530.2, bsz=128, num_updates=45300, lr=0.000148577, gnorm=0.881, loss_scale=8, train_wall=253, gb_free=12.3, wall=117616
2022-03-04 19:38:14 | INFO | train_inner | epoch 116:    289 / 393 loss=5.678, nll_loss=3.823, ppl=14.15, wps=25406.3, ups=0.39, wpb=65536, bsz=128, num_updates=45400, lr=0.000148413, gnorm=0.869, loss_scale=8, train_wall=253, gb_free=12.3, wall=117874
2022-03-04 19:42:32 | INFO | train_inner | epoch 116:    389 / 393 loss=5.727, nll_loss=3.879, ppl=14.72, wps=25375.6, ups=0.39, wpb=65536, bsz=128, num_updates=45500, lr=0.00014825, gnorm=0.882, loss_scale=16, train_wall=253, gb_free=12.3, wall=118132
2022-03-04 19:42:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:42:44 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 8.266 | nll_loss 6.698 | ppl 103.84 | wps 66243.8 | wpb 2034.1 | bsz 4 | num_updates 45504 | best_loss 7.641
2022-03-04 19:42:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 45504 updates
2022-03-04 19:42:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 19:42:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 19:42:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 116 @ 45504 updates, score 8.266) (writing took 4.231944588944316 seconds)
2022-03-04 19:42:49 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-04 19:42:49 | INFO | train | epoch 116 | loss 5.666 | nll_loss 3.809 | ppl 14.01 | wps 25205.5 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 45504 | lr 0.000148243 | gnorm 0.878 | loss_scale 16 | train_wall 994 | gb_free 12.3 | wall 118149
2022-03-04 19:42:49 | INFO | fairseq.trainer | begin training epoch 117
2022-03-04 19:42:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:43:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:46:59 | INFO | train_inner | epoch 117:     97 / 393 loss=5.607, nll_loss=3.74, ppl=13.36, wps=24429, ups=0.37, wpb=65248.6, bsz=127.4, num_updates=45600, lr=0.000148087, gnorm=0.871, loss_scale=8, train_wall=254, gb_free=12.3, wall=118399
2022-03-04 19:51:17 | INFO | train_inner | epoch 117:    197 / 393 loss=5.643, nll_loss=3.782, ppl=13.76, wps=25387.3, ups=0.39, wpb=65536, bsz=128, num_updates=45700, lr=0.000147925, gnorm=0.868, loss_scale=8, train_wall=253, gb_free=12.3, wall=118657
2022-03-04 19:55:35 | INFO | train_inner | epoch 117:    297 / 393 loss=5.682, nll_loss=3.828, ppl=14.2, wps=25386.8, ups=0.39, wpb=65536, bsz=128, num_updates=45800, lr=0.000147764, gnorm=0.884, loss_scale=8, train_wall=253, gb_free=12.3, wall=118915
2022-03-04 19:59:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:59:45 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 8.254 | nll_loss 6.685 | ppl 102.92 | wps 65975.7 | wpb 2034.1 | bsz 4 | num_updates 45896 | best_loss 7.641
2022-03-04 19:59:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 45896 updates
2022-03-04 19:59:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 19:59:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 19:59:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 117 @ 45896 updates, score 8.254) (writing took 4.237764512188733 seconds)
2022-03-04 19:59:50 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-04 19:59:50 | INFO | train | epoch 117 | loss 5.664 | nll_loss 3.807 | ppl 13.99 | wps 25135.8 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 45896 | lr 0.000147609 | gnorm 0.877 | loss_scale 8 | train_wall 994 | gb_free 12.3 | wall 119169
2022-03-04 19:59:50 | INFO | fairseq.trainer | begin training epoch 118
2022-03-04 19:59:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:00:00 | INFO | train_inner | epoch 118:      4 / 393 loss=5.723, nll_loss=3.876, ppl=14.68, wps=24651.3, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=45900, lr=0.000147602, gnorm=0.888, loss_scale=8, train_wall=252, gb_free=12.3, wall=119180
2022-03-04 20:04:18 | INFO | train_inner | epoch 118:    104 / 393 loss=5.604, nll_loss=3.737, ppl=13.33, wps=25401.6, ups=0.39, wpb=65536, bsz=128, num_updates=46000, lr=0.000147442, gnorm=0.871, loss_scale=8, train_wall=253, gb_free=12.3, wall=119438
2022-03-04 20:07:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:08:38 | INFO | train_inner | epoch 118:    205 / 393 loss=5.644, nll_loss=3.783, ppl=13.77, wps=25158.4, ups=0.38, wpb=65535.4, bsz=128, num_updates=46100, lr=0.000147282, gnorm=0.879, loss_scale=8, train_wall=255, gb_free=12.3, wall=119698
2022-03-04 20:12:56 | INFO | train_inner | epoch 118:    305 / 393 loss=5.686, nll_loss=3.832, ppl=14.24, wps=25400.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=46200, lr=0.000147122, gnorm=0.88, loss_scale=8, train_wall=253, gb_free=12.3, wall=119956
2022-03-04 20:16:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:16:46 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 8.252 | nll_loss 6.691 | ppl 103.33 | wps 66467.5 | wpb 2034.1 | bsz 4 | num_updates 46288 | best_loss 7.641
2022-03-04 20:16:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 46288 updates
2022-03-04 20:16:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 20:16:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 20:16:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 118 @ 46288 updates, score 8.252) (writing took 4.305329490453005 seconds)
2022-03-04 20:16:50 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-04 20:16:50 | INFO | train | epoch 118 | loss 5.662 | nll_loss 3.804 | ppl 13.97 | wps 25149.2 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 46288 | lr 0.000146983 | gnorm 0.88 | loss_scale 8 | train_wall 993 | gb_free 12.3 | wall 120190
2022-03-04 20:16:50 | INFO | fairseq.trainer | begin training epoch 119
2022-03-04 20:16:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:17:21 | INFO | train_inner | epoch 119:     12 / 393 loss=5.709, nll_loss=3.859, ppl=14.51, wps=24674.1, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=46300, lr=0.000146964, gnorm=0.892, loss_scale=8, train_wall=252, gb_free=12.3, wall=120221
2022-03-04 20:21:39 | INFO | train_inner | epoch 119:    112 / 393 loss=5.603, nll_loss=3.735, ppl=13.32, wps=25407, ups=0.39, wpb=65536, bsz=128, num_updates=46400, lr=0.000146805, gnorm=0.879, loss_scale=8, train_wall=253, gb_free=12.3, wall=120479
2022-03-04 20:25:57 | INFO | train_inner | epoch 119:    212 / 393 loss=5.644, nll_loss=3.783, ppl=13.76, wps=25413.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=46500, lr=0.000146647, gnorm=0.885, loss_scale=8, train_wall=253, gb_free=12.3, wall=120737
2022-03-04 20:30:15 | INFO | train_inner | epoch 119:    312 / 393 loss=5.687, nll_loss=3.833, ppl=14.26, wps=25408.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=46600, lr=0.00014649, gnorm=0.889, loss_scale=16, train_wall=253, gb_free=12.3, wall=120994
2022-03-04 20:31:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:33:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:33:46 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 8.259 | nll_loss 6.699 | ppl 103.9 | wps 65810 | wpb 2034.1 | bsz 4 | num_updates 46680 | best_loss 7.641
2022-03-04 20:33:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 46680 updates
2022-03-04 20:33:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 20:33:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 20:33:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 119 @ 46680 updates, score 8.259) (writing took 4.245818855240941 seconds)
2022-03-04 20:33:50 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-04 20:33:50 | INFO | train | epoch 119 | loss 5.66 | nll_loss 3.801 | ppl 13.94 | wps 25155.5 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 46680 | lr 0.000146364 | gnorm 0.887 | loss_scale 8 | train_wall 993 | gb_free 12.3 | wall 121210
2022-03-04 20:33:50 | INFO | fairseq.trainer | begin training epoch 120
2022-03-04 20:33:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:34:42 | INFO | train_inner | epoch 120:     20 / 393 loss=5.699, nll_loss=3.847, ppl=14.39, wps=24437.8, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=46700, lr=0.000146333, gnorm=0.897, loss_scale=8, train_wall=254, gb_free=12.3, wall=121261
2022-03-04 20:38:59 | INFO | train_inner | epoch 120:    120 / 393 loss=5.607, nll_loss=3.74, ppl=13.36, wps=25421.2, ups=0.39, wpb=65536, bsz=128, num_updates=46800, lr=0.000146176, gnorm=0.869, loss_scale=8, train_wall=253, gb_free=12.3, wall=121519
2022-03-04 20:43:17 | INFO | train_inner | epoch 120:    220 / 393 loss=5.645, nll_loss=3.785, ppl=13.78, wps=25407.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=46900, lr=0.00014602, gnorm=0.879, loss_scale=8, train_wall=253, gb_free=12.3, wall=121777
2022-03-04 20:47:35 | INFO | train_inner | epoch 120:    320 / 393 loss=5.693, nll_loss=3.84, ppl=14.32, wps=25426.3, ups=0.39, wpb=65536, bsz=128, num_updates=47000, lr=0.000145865, gnorm=0.891, loss_scale=8, train_wall=253, gb_free=12.3, wall=122035
2022-03-04 20:50:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:50:45 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 8.268 | nll_loss 6.704 | ppl 104.26 | wps 66521.5 | wpb 2034.1 | bsz 4 | num_updates 47073 | best_loss 7.641
2022-03-04 20:50:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 47073 updates
2022-03-04 20:50:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 20:50:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 20:50:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 120 @ 47073 updates, score 8.268) (writing took 4.246265982277691 seconds)
2022-03-04 20:50:50 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-04 20:50:50 | INFO | train | epoch 120 | loss 5.658 | nll_loss 3.8 | ppl 13.93 | wps 25229.6 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 47073 | lr 0.000145752 | gnorm 0.882 | loss_scale 8 | train_wall 993 | gb_free 12.3 | wall 122230
2022-03-04 20:50:50 | INFO | fairseq.trainer | begin training epoch 121
2022-03-04 20:50:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:51:59 | INFO | train_inner | epoch 121:     27 / 393 loss=5.679, nll_loss=3.824, ppl=14.16, wps=24690.7, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=47100, lr=0.00014571, gnorm=0.888, loss_scale=8, train_wall=252, gb_free=12.3, wall=122299
2022-03-04 20:55:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:56:20 | INFO | train_inner | epoch 121:    128 / 393 loss=5.601, nll_loss=3.734, ppl=13.3, wps=25167.4, ups=0.38, wpb=65530.2, bsz=128, num_updates=47200, lr=0.000145556, gnorm=0.872, loss_scale=8, train_wall=255, gb_free=12.3, wall=122560
2022-03-04 21:00:38 | INFO | train_inner | epoch 121:    228 / 393 loss=5.642, nll_loss=3.781, ppl=13.75, wps=25409.8, ups=0.39, wpb=65536, bsz=128, num_updates=47300, lr=0.000145402, gnorm=0.879, loss_scale=8, train_wall=253, gb_free=12.3, wall=122818
2022-03-04 21:04:55 | INFO | train_inner | epoch 121:    328 / 393 loss=5.698, nll_loss=3.846, ppl=14.38, wps=25418.7, ups=0.39, wpb=65536, bsz=128, num_updates=47400, lr=0.000145248, gnorm=0.886, loss_scale=8, train_wall=253, gb_free=12.3, wall=123075
2022-03-04 21:07:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:07:45 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 8.256 | nll_loss 6.69 | ppl 103.22 | wps 65539.9 | wpb 2034.1 | bsz 4 | num_updates 47465 | best_loss 7.641
2022-03-04 21:07:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 47465 updates
2022-03-04 21:07:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 21:07:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 21:07:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 121 @ 47465 updates, score 8.256) (writing took 4.270406954921782 seconds)
2022-03-04 21:07:50 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-04 21:07:50 | INFO | train | epoch 121 | loss 5.656 | nll_loss 3.797 | ppl 13.9 | wps 25158.9 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 47465 | lr 0.000145149 | gnorm 0.881 | loss_scale 8 | train_wall 993 | gb_free 12.3 | wall 123250
2022-03-04 21:07:50 | INFO | fairseq.trainer | begin training epoch 122
2022-03-04 21:07:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:09:20 | INFO | train_inner | epoch 122:     35 / 393 loss=5.676, nll_loss=3.821, ppl=14.13, wps=24674.2, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=47500, lr=0.000145095, gnorm=0.88, loss_scale=8, train_wall=252, gb_free=12.3, wall=123340
2022-03-04 21:13:38 | INFO | train_inner | epoch 122:    135 / 393 loss=5.604, nll_loss=3.736, ppl=13.33, wps=25396.4, ups=0.39, wpb=65536, bsz=128, num_updates=47600, lr=0.000144943, gnorm=0.865, loss_scale=8, train_wall=253, gb_free=12.3, wall=123598
2022-03-04 21:17:56 | INFO | train_inner | epoch 122:    235 / 393 loss=5.656, nll_loss=3.797, ppl=13.9, wps=25374, ups=0.39, wpb=65530.2, bsz=128, num_updates=47700, lr=0.000144791, gnorm=0.883, loss_scale=16, train_wall=253, gb_free=12.3, wall=123856
2022-03-04 21:18:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:22:17 | INFO | train_inner | epoch 122:    336 / 393 loss=5.687, nll_loss=3.833, ppl=14.25, wps=25173.3, ups=0.38, wpb=65536, bsz=128, num_updates=47800, lr=0.000144639, gnorm=0.888, loss_scale=8, train_wall=255, gb_free=12.3, wall=124116
2022-03-04 21:24:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:24:46 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 8.233 | nll_loss 6.667 | ppl 101.6 | wps 66529.2 | wpb 2034.1 | bsz 4 | num_updates 47857 | best_loss 7.641
2022-03-04 21:24:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 47857 updates
2022-03-04 21:24:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 21:24:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 21:24:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 122 @ 47857 updates, score 8.233) (writing took 4.289837475866079 seconds)
2022-03-04 21:24:50 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-04 21:24:50 | INFO | train | epoch 122 | loss 5.654 | nll_loss 3.795 | ppl 13.88 | wps 25148.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 47857 | lr 0.000144553 | gnorm 0.88 | loss_scale 8 | train_wall 993 | gb_free 12.3 | wall 124270
2022-03-04 21:24:50 | INFO | fairseq.trainer | begin training epoch 123
2022-03-04 21:24:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:26:41 | INFO | train_inner | epoch 123:     43 / 393 loss=5.665, nll_loss=3.808, ppl=14, wps=24680.2, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=47900, lr=0.000144488, gnorm=0.892, loss_scale=8, train_wall=252, gb_free=12.3, wall=124381
2022-03-04 21:30:59 | INFO | train_inner | epoch 123:    143 / 393 loss=5.614, nll_loss=3.748, ppl=13.44, wps=25396.1, ups=0.39, wpb=65536, bsz=128, num_updates=48000, lr=0.000144338, gnorm=0.871, loss_scale=8, train_wall=253, gb_free=12.3, wall=124639
2022-03-04 21:35:17 | INFO | train_inner | epoch 123:    243 / 393 loss=5.65, nll_loss=3.79, ppl=13.83, wps=25411.3, ups=0.39, wpb=65536, bsz=128, num_updates=48100, lr=0.000144187, gnorm=0.891, loss_scale=8, train_wall=253, gb_free=12.3, wall=124897
2022-03-04 21:39:35 | INFO | train_inner | epoch 123:    343 / 393 loss=5.694, nll_loss=3.841, ppl=14.34, wps=25413.6, ups=0.39, wpb=65530.2, bsz=128, num_updates=48200, lr=0.000144038, gnorm=0.9, loss_scale=8, train_wall=253, gb_free=12.3, wall=125155
2022-03-04 21:41:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:41:46 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 8.27 | nll_loss 6.702 | ppl 104.12 | wps 66606 | wpb 2034.1 | bsz 4 | num_updates 48250 | best_loss 7.641
2022-03-04 21:41:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 48250 updates
2022-03-04 21:41:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 21:41:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 21:41:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 123 @ 48250 updates, score 8.27) (writing took 4.211921097710729 seconds)
2022-03-04 21:41:50 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-04 21:41:50 | INFO | train | epoch 123 | loss 5.654 | nll_loss 3.794 | ppl 13.88 | wps 25216.4 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 48250 | lr 0.000143963 | gnorm 0.89 | loss_scale 16 | train_wall 993 | gb_free 12.3 | wall 125290
2022-03-04 21:41:50 | INFO | fairseq.trainer | begin training epoch 124
2022-03-04 21:41:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:43:59 | INFO | train_inner | epoch 124:     50 / 393 loss=5.65, nll_loss=3.79, ppl=13.83, wps=24668.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=48300, lr=0.000143889, gnorm=0.897, loss_scale=16, train_wall=252, gb_free=12.3, wall=125419
2022-03-04 21:47:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:48:20 | INFO | train_inner | epoch 124:    151 / 393 loss=5.609, nll_loss=3.743, ppl=13.39, wps=25140.2, ups=0.38, wpb=65535.4, bsz=128, num_updates=48400, lr=0.00014374, gnorm=0.874, loss_scale=8, train_wall=256, gb_free=12.3, wall=125680
2022-03-04 21:52:38 | INFO | train_inner | epoch 124:    251 / 393 loss=5.654, nll_loss=3.794, ppl=13.87, wps=25409.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=48500, lr=0.000143592, gnorm=0.884, loss_scale=8, train_wall=253, gb_free=12.3, wall=125938
2022-03-04 21:56:56 | INFO | train_inner | epoch 124:    351 / 393 loss=5.697, nll_loss=3.845, ppl=14.37, wps=25427.2, ups=0.39, wpb=65536, bsz=128, num_updates=48600, lr=0.000143444, gnorm=0.886, loss_scale=8, train_wall=253, gb_free=12.3, wall=126195
2022-03-04 21:58:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:58:46 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 8.264 | nll_loss 6.693 | ppl 103.47 | wps 66331.2 | wpb 2034.1 | bsz 4 | num_updates 48642 | best_loss 7.641
2022-03-04 21:58:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 48642 updates
2022-03-04 21:58:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 21:58:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 21:58:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 124 @ 48642 updates, score 8.264) (writing took 4.166294529102743 seconds)
2022-03-04 21:58:50 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-04 21:58:50 | INFO | train | epoch 124 | loss 5.651 | nll_loss 3.791 | ppl 13.85 | wps 25154.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 48642 | lr 0.000143382 | gnorm 0.885 | loss_scale 8 | train_wall 993 | gb_free 12.3 | wall 126310
2022-03-04 21:58:50 | INFO | fairseq.trainer | begin training epoch 125
2022-03-04 21:58:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:01:20 | INFO | train_inner | epoch 125:     58 / 393 loss=5.642, nll_loss=3.781, ppl=13.75, wps=24663.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=48700, lr=0.000143296, gnorm=0.899, loss_scale=8, train_wall=252, gb_free=12.3, wall=126460
2022-03-04 22:05:38 | INFO | train_inner | epoch 125:    158 / 393 loss=5.615, nll_loss=3.749, ppl=13.45, wps=25384.8, ups=0.39, wpb=65530.2, bsz=128, num_updates=48800, lr=0.00014315, gnorm=0.886, loss_scale=8, train_wall=253, gb_free=12.3, wall=126718
2022-03-04 22:09:57 | INFO | train_inner | epoch 125:    258 / 393 loss=5.652, nll_loss=3.792, ppl=13.85, wps=25375.6, ups=0.39, wpb=65536, bsz=128, num_updates=48900, lr=0.000143003, gnorm=0.898, loss_scale=16, train_wall=253, gb_free=12.3, wall=126976
2022-03-04 22:10:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:14:17 | INFO | train_inner | epoch 125:    359 / 393 loss=5.691, nll_loss=3.837, ppl=14.3, wps=25130.1, ups=0.38, wpb=65536, bsz=128, num_updates=49000, lr=0.000142857, gnorm=0.899, loss_scale=8, train_wall=255, gb_free=12.3, wall=127237
2022-03-04 22:15:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:15:47 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 8.277 | nll_loss 6.715 | ppl 105.08 | wps 66564.7 | wpb 2034.1 | bsz 4 | num_updates 49034 | best_loss 7.641
2022-03-04 22:15:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 49034 updates
2022-03-04 22:15:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 22:15:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 22:15:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 125 @ 49034 updates, score 8.277) (writing took 4.235377064906061 seconds)
2022-03-04 22:15:52 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-04 22:15:52 | INFO | train | epoch 125 | loss 5.65 | nll_loss 3.79 | ppl 13.83 | wps 25127.5 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 49034 | lr 0.000142808 | gnorm 0.894 | loss_scale 8 | train_wall 992 | gb_free 12.3 | wall 127331
2022-03-04 22:15:52 | INFO | fairseq.trainer | begin training epoch 126
2022-03-04 22:15:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:18:42 | INFO | train_inner | epoch 126:     66 / 393 loss=5.637, nll_loss=3.775, ppl=13.69, wps=24665.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=49100, lr=0.000142712, gnorm=0.89, loss_scale=8, train_wall=252, gb_free=12.3, wall=127502
2022-03-04 22:23:00 | INFO | train_inner | epoch 126:    166 / 393 loss=5.612, nll_loss=3.746, ppl=13.42, wps=25396.5, ups=0.39, wpb=65536, bsz=128, num_updates=49200, lr=0.000142566, gnorm=0.874, loss_scale=8, train_wall=253, gb_free=12.3, wall=127760
2022-03-04 22:27:18 | INFO | train_inner | epoch 126:    266 / 393 loss=5.663, nll_loss=3.805, ppl=13.97, wps=25362.2, ups=0.39, wpb=65530.9, bsz=128, num_updates=49300, lr=0.000142422, gnorm=0.89, loss_scale=8, train_wall=253, gb_free=12.3, wall=128018
2022-03-04 22:31:37 | INFO | train_inner | epoch 126:    366 / 393 loss=5.692, nll_loss=3.839, ppl=14.31, wps=25367.8, ups=0.39, wpb=65536, bsz=128, num_updates=49400, lr=0.000142278, gnorm=0.889, loss_scale=8, train_wall=253, gb_free=12.3, wall=128277
2022-03-04 22:32:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:32:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:32:48 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 8.264 | nll_loss 6.692 | ppl 103.38 | wps 66299.5 | wpb 2034.1 | bsz 4 | num_updates 49426 | best_loss 7.641
2022-03-04 22:32:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 49426 updates
2022-03-04 22:32:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 22:32:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 22:32:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 126 @ 49426 updates, score 8.264) (writing took 4.162791145034134 seconds)
2022-03-04 22:32:53 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-04 22:32:53 | INFO | train | epoch 126 | loss 5.648 | nll_loss 3.788 | ppl 13.81 | wps 25131.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 49426 | lr 0.00014224 | gnorm 0.887 | loss_scale 8 | train_wall 994 | gb_free 12.3 | wall 128353
2022-03-04 22:32:53 | INFO | fairseq.trainer | begin training epoch 127
2022-03-04 22:32:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:36:04 | INFO | train_inner | epoch 127:     74 / 393 loss=5.621, nll_loss=3.756, ppl=13.51, wps=24450.9, ups=0.37, wpb=65248.6, bsz=127.4, num_updates=49500, lr=0.000142134, gnorm=0.889, loss_scale=8, train_wall=254, gb_free=12.3, wall=128543
2022-03-04 22:40:22 | INFO | train_inner | epoch 127:    174 / 393 loss=5.619, nll_loss=3.754, ppl=13.49, wps=25393.6, ups=0.39, wpb=65530.9, bsz=128, num_updates=49600, lr=0.00014199, gnorm=0.876, loss_scale=8, train_wall=253, gb_free=12.3, wall=128801
2022-03-04 22:44:40 | INFO | train_inner | epoch 127:    274 / 393 loss=5.654, nll_loss=3.795, ppl=13.88, wps=25389.6, ups=0.39, wpb=65536, bsz=128, num_updates=49700, lr=0.000141848, gnorm=0.901, loss_scale=8, train_wall=253, gb_free=12.3, wall=129060
2022-03-04 22:48:58 | INFO | train_inner | epoch 127:    374 / 393 loss=5.7, nll_loss=3.848, ppl=14.4, wps=25399.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=49800, lr=0.000141705, gnorm=0.888, loss_scale=8, train_wall=253, gb_free=12.3, wall=129318
2022-03-04 22:49:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:49:49 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 8.249 | nll_loss 6.692 | ppl 103.39 | wps 66168.4 | wpb 2034.1 | bsz 4 | num_updates 49819 | best_loss 7.641
2022-03-04 22:49:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 49819 updates
2022-03-04 22:49:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 22:49:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 22:49:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 127 @ 49819 updates, score 8.249) (writing took 4.259140077978373 seconds)
2022-03-04 22:49:53 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-04 22:49:53 | INFO | train | epoch 127 | loss 5.647 | nll_loss 3.787 | ppl 13.8 | wps 25207.4 | ups 0.39 | wpb 65461.6 | bsz 127.9 | num_updates 49819 | lr 0.000141678 | gnorm 0.889 | loss_scale 8 | train_wall 993 | gb_free 12.3 | wall 129373
2022-03-04 22:49:53 | INFO | fairseq.trainer | begin training epoch 128
2022-03-04 22:49:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:53:22 | INFO | train_inner | epoch 128:     81 / 393 loss=5.618, nll_loss=3.753, ppl=13.48, wps=24660, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=49900, lr=0.000141563, gnorm=0.905, loss_scale=8, train_wall=252, gb_free=12.3, wall=129582
2022-03-04 22:55:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:57:43 | INFO | train_inner | epoch 128:    182 / 393 loss=5.617, nll_loss=3.752, ppl=13.47, wps=25162.4, ups=0.38, wpb=65536, bsz=128, num_updates=50000, lr=0.000141421, gnorm=0.888, loss_scale=8, train_wall=255, gb_free=12.3, wall=129843
2022-03-04 22:57:43 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2022-03-04 22:57:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:57:46 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 8.31 | nll_loss 6.748 | ppl 107.46 | wps 66108.2 | wpb 2034.1 | bsz 4 | num_updates 50000 | best_loss 7.641
2022-03-04 22:57:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 50000 updates
2022-03-04 22:57:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 22:57:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt
2022-03-04 22:57:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#4/checkpoint_last.pt (epoch 128 @ 50000 updates, score 8.31) (writing took 4.274088063277304 seconds)
2022-03-04 22:57:50 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-04 22:57:50 | INFO | train | epoch 128 | loss 5.606 | nll_loss 3.739 | ppl 13.36 | wps 24860.5 | ups 0.38 | wpb 65536 | bsz 128 | num_updates 50000 | lr 0.000141421 | gnorm 0.894 | loss_scale 8 | train_wall 460 | gb_free 12.3 | wall 129850
2022-03-04 22:57:50 | INFO | fairseq_cli.train | done training in 129849.1 seconds
