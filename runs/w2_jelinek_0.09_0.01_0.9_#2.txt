Sender: LSF System <lsfadmin@eu-g2-20>
Subject: Job 202625140: <w2_jelinek_0.09_0.01_0.9_#2> in cluster <euler> Exited

Job <w2_jelinek_0.09_0.01_0.9_#2> was submitted from host <eu-login-14> by user <andriusb> in cluster <euler> at Mon Jan 31 08:51:56 2022
Job was executed on host(s) <eu-g2-20>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Mon Jan 31 08:52:13 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Mon Jan 31 08:52:13 2022
Terminated at Tue Feb  1 04:52:20 2022
Results reported at Tue Feb  1 04:52:20 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-2-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.09, 0.01, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 16 --save-interval 40 --max-update 50000
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   72897.17 sec.
    Max Memory :                                 5193 MB
    Average Memory :                             2781.12 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               14807.00 MB
    Max Swap :                                   45 MB
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   72008 sec.
    Turnaround time :                            72024 sec.

The output (if any) follows:

2022-01-31 08:52:21 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 40, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.5, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-2-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.09, 0.01, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.5, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-01-31 08:52:21 | INFO | fairseq.tasks.language_modeling | dictionary: 76624 types
2022-01-31 08:52:23 | INFO | fairseq.data.data_utils | loaded 36,718 examples from: data-bin/wikitext-2-raw-full/train
Calculating frequency stats:
  0%|          | 0/36718 [00:00<?, ?it/s]  4%|▍         | 1387/36718 [00:00<00:02, 13866.51it/s]  8%|▊         | 2774/36718 [00:00<00:02, 13143.34it/s] 12%|█▏        | 4293/36718 [00:00<00:02, 14048.62it/s] 16%|█▌        | 5924/36718 [00:00<00:02, 14919.46it/s] 20%|██        | 7420/36718 [00:00<00:02, 14268.50it/s] 24%|██▍       | 8854/36718 [00:00<00:02, 13865.38it/s] 28%|██▊       | 10281/36718 [00:00<00:01, 13984.51it/s] 32%|███▏      | 11684/36718 [00:00<00:01, 13516.77it/s] 36%|███▌      | 13092/36718 [00:00<00:01, 13684.23it/s] 40%|███▉      | 14512/36718 [00:01<00:01, 13835.06it/s] 43%|████▎     | 15929/36718 [00:01<00:01, 13933.42it/s] 47%|████▋     | 17325/36718 [00:01<00:01, 13862.99it/s] 51%|█████     | 18790/36718 [00:01<00:01, 14096.12it/s] 55%|█████▌    | 20277/36718 [00:01<00:01, 14319.26it/s] 59%|█████▉    | 21711/36718 [00:01<00:01, 14083.75it/s] 63%|██████▎   | 23217/36718 [00:01<00:00, 14370.68it/s] 68%|██████▊   | 24877/36718 [00:01<00:00, 15029.23it/s] 72%|███████▏  | 26382/36718 [00:01<00:00, 14633.07it/s] 76%|███████▌  | 27849/36718 [00:01<00:00, 14166.46it/s] 80%|███████▉  | 29304/36718 [00:02<00:00, 14275.49it/s] 84%|████████▎ | 30736/36718 [00:02<00:00, 13864.05it/s] 87%|████████▋ | 32127/36718 [00:02<00:00, 13522.61it/s] 91%|█████████ | 33483/36718 [00:02<00:00, 13492.45it/s] 95%|█████████▌| 34952/36718 [00:02<00:00, 13838.87it/s] 99%|█████████▉| 36339/36718 [00:02<00:00, 13844.16it/s]100%|██████████| 36718/36718 [00:02<00:00, 14007.34it/s]

gathering stats for n=1
  0%|          | 0/36718 [00:00<?, ?it/s]  7%|▋         | 2720/36718 [00:00<00:01, 27173.53it/s] 16%|█▌        | 5866/36718 [00:00<00:01, 29689.16it/s] 24%|██▍       | 8835/36718 [00:00<00:01, 27869.32it/s] 32%|███▏      | 11635/36718 [00:00<00:00, 27814.15it/s] 39%|███▉      | 14430/36718 [00:00<00:00, 27850.03it/s] 47%|████▋     | 17220/36718 [00:00<00:00, 27827.81it/s] 55%|█████▍    | 20071/36718 [00:00<00:00, 28046.65it/s] 62%|██████▏   | 22879/36718 [00:00<00:00, 27702.63it/s] 71%|███████   | 25948/36718 [00:00<00:00, 28621.11it/s] 78%|███████▊  | 28814/36718 [00:01<00:00, 28062.66it/s] 86%|████████▌ | 31625/36718 [00:01<00:00, 27407.18it/s] 94%|█████████▎| 34371/36718 [00:01<00:00, 27035.58it/s]100%|██████████| 36718/36718 [00:01<00:00, 27748.77it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 63.88it/s]2022-01-31 08:52:36 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(76624, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=76624, bias=False)
  )
)
2022-01-31 08:52:36 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-01-31 08:52:36 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-01-31 08:52:36 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-01-31 08:52:36 | INFO | fairseq_cli.train | num. shared model params: 58,145,792 (num. trained: 58,145,792)
2022-01-31 08:52:36 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-01-31 08:52:36 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-2-raw-full/valid
2022-01-31 08:52:36 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-01-31 08:52:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-31 08:52:36 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2022-01-31 08:52:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-01-31 08:52:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-01-31 08:52:36 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-01-31 08:52:36 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint_last.pt
2022-01-31 08:52:36 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint_last.pt
2022-01-31 08:52:36 | INFO | fairseq.trainer | loading train data for epoch 1
2022-01-31 08:52:36 | INFO | fairseq.data.data_utils | loaded 36,718 examples from: data-bin/wikitext-2-raw-full/train
2022-01-31 08:52:36 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-01-31 08:52:36 | INFO | fairseq.trainer | begin training epoch 1
2022-01-31 08:52:36 | INFO | fairseq_cli.train | Start iterating over samples

2022-01-31 08:58:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
/cluster/home/andriusb/fq/fairseq/fairseq/utils.py:372: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-01-31 08:58:28 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.679 | ppl 26227.6 | wps 8024.7 | wpb 2034.1 | bsz 4 | num_updates 64
2022-01-31 08:58:28 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-01-31 08:58:28 | INFO | train | epoch 001 | loss 16.131 | ppl 71746.6 | wps 5954 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 64 | lr 8.0984e-06 | gnorm 3.273 | train_wall 323 | gb_free 6.1 | wall 352
KL Stats: Epoch 1 Divergences: Uniform: 0.5172925508133422 Unigram: 3.685318486297919
2022-01-31 08:58:28 | INFO | fairseq.trainer | begin training epoch 2
2022-01-31 08:58:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:01:30 | INFO | train_inner | epoch 002:     36 / 64 loss=15.584, ppl=49132, wps=6129.2, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=100, lr=1.25975e-05, gnorm=2.682, train_wall=505, gb_free=6.1, wall=535
2022-01-31 09:03:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:04:18 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.667 | ppl 13005.3 | wps 8007.5 | wpb 2034.1 | bsz 4 | num_updates 128
2022-01-31 09:04:18 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-01-31 09:04:18 | INFO | train | epoch 002 | loss 14.402 | ppl 21650.6 | wps 5958.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 128 | lr 1.60968e-05 | gnorm 1.512 | train_wall 322 | gb_free 6.1 | wall 703
KL Stats: Epoch 2 Divergences: Uniform: 0.5355066972682596 Unigram: 2.4150737509746527
2022-01-31 09:04:18 | INFO | fairseq.trainer | begin training epoch 3
2022-01-31 09:04:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:09:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:10:09 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 12.836 | ppl 7313.14 | wps 7974.9 | wpb 2034.1 | bsz 4 | num_updates 192
2022-01-31 09:10:09 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-01-31 09:10:09 | INFO | train | epoch 003 | loss 13.489 | ppl 11496.4 | wps 5958.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 192 | lr 2.40952e-05 | gnorm 1.219 | train_wall 322 | gb_free 6.1 | wall 1053
KL Stats: Epoch 3 Divergences: Uniform: 0.5218326599133369 Unigram: 1.7313023709848727
2022-01-31 09:10:09 | INFO | fairseq.trainer | begin training epoch 4
2022-01-31 09:10:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:10:50 | INFO | train_inner | epoch 004:      8 / 64 loss=13.623, ppl=12620.1, wps=5828.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=200, lr=2.5095e-05, gnorm=1.248, train_wall=503, gb_free=6.1, wall=1094
2022-01-31 09:15:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:15:58 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.98 | ppl 4038.3 | wps 8082.8 | wpb 2034.1 | bsz 4 | num_updates 256
2022-01-31 09:15:58 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-01-31 09:15:58 | INFO | train | epoch 004 | loss 12.532 | ppl 5920.87 | wps 5979.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 256 | lr 3.20936e-05 | gnorm 0.972 | train_wall 321 | gb_free 6.1 | wall 1402
KL Stats: Epoch 4 Divergences: Uniform: 0.6079998372450861 Unigram: 1.1139984562616057
2022-01-31 09:15:58 | INFO | fairseq.trainer | begin training epoch 5
2022-01-31 09:15:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:19:39 | INFO | train_inner | epoch 005:     44 / 64 loss=12.178, ppl=4634.5, wps=6170.5, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=300, lr=3.75925e-05, gnorm=0.853, train_wall=501, gb_free=6.1, wall=1624
2022-01-31 09:21:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:21:46 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.455 | ppl 2807.14 | wps 7982.7 | wpb 2034.1 | bsz 4 | num_updates 320
2022-01-31 09:21:46 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-01-31 09:21:46 | INFO | train | epoch 005 | loss 11.725 | ppl 3385.99 | wps 6002.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 320 | lr 4.0092e-05 | gnorm 0.694 | train_wall 320 | gb_free 6.1 | wall 1750
KL Stats: Epoch 5 Divergences: Uniform: 0.8527021028177504 Unigram: 0.6570025670544285
2022-01-31 09:21:46 | INFO | fairseq.trainer | begin training epoch 6
2022-01-31 09:21:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:27:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:27:37 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.208 | ppl 2365.91 | wps 7987.5 | wpb 2034.1 | bsz 4 | num_updates 384
2022-01-31 09:27:37 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-01-31 09:27:37 | INFO | train | epoch 006 | loss 11.288 | ppl 2500.35 | wps 5953.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 384 | lr 4.80904e-05 | gnorm 0.587 | train_wall 323 | gb_free 6.1 | wall 2101
KL Stats: Epoch 6 Divergences: Uniform: 1.158623902415679 Unigram: 0.4516596964502991
2022-01-31 09:27:37 | INFO | fairseq.trainer | begin training epoch 7
2022-01-31 09:27:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:28:58 | INFO | train_inner | epoch 007:     16 / 64 loss=11.311, ppl=2540.2, wps=5832.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=400, lr=5.009e-05, gnorm=0.586, train_wall=503, gb_free=6.1, wall=2182
2022-01-31 09:33:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:33:28 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.065 | ppl 2142.7 | wps 7936.4 | wpb 2034.1 | bsz 4 | num_updates 448
2022-01-31 09:33:28 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-01-31 09:33:28 | INFO | train | epoch 007 | loss 11.085 | ppl 2172.54 | wps 5944.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 448 | lr 5.60888e-05 | gnorm 0.524 | train_wall 323 | gb_free 6.1 | wall 2452
KL Stats: Epoch 7 Divergences: Uniform: 1.392642527964186 Unigram: 0.45654338669681677
2022-01-31 09:33:28 | INFO | fairseq.trainer | begin training epoch 8
2022-01-31 09:33:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:37:53 | INFO | train_inner | epoch 008:     52 / 64 loss=11.024, ppl=2082.14, wps=6115.1, ups=0.19, wpb=32686.1, bsz=63.8, num_updates=500, lr=6.25875e-05, gnorm=0.517, train_wall=506, gb_free=6.1, wall=2717
2022-01-31 09:38:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:39:20 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.962 | ppl 1994.18 | wps 7931.6 | wpb 2034.1 | bsz 4 | num_updates 512
2022-01-31 09:39:20 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-01-31 09:39:20 | INFO | train | epoch 008 | loss 10.972 | ppl 2009.09 | wps 5937.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 512 | lr 6.40872e-05 | gnorm 0.512 | train_wall 323 | gb_free 6.1 | wall 2804
KL Stats: Epoch 8 Divergences: Uniform: 1.5156018024966074 Unigram: 0.5258306782085844
2022-01-31 09:39:20 | INFO | fairseq.trainer | begin training epoch 9
2022-01-31 09:39:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:44:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:45:10 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.846 | ppl 1841.01 | wps 8023.7 | wpb 2034.1 | bsz 4 | num_updates 576
2022-01-31 09:45:10 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-01-31 09:45:10 | INFO | train | epoch 009 | loss 10.868 | ppl 1868.46 | wps 5961.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 576 | lr 7.20856e-05 | gnorm 0.486 | train_wall 322 | gb_free 6.1 | wall 3155
KL Stats: Epoch 9 Divergences: Uniform: 1.5656219075321887 Unigram: 0.6215845768683442
2022-01-31 09:45:10 | INFO | fairseq.trainer | begin training epoch 10
2022-01-31 09:45:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:47:12 | INFO | train_inner | epoch 010:     24 / 64 loss=10.859, ppl=1856.76, wps=5824.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=600, lr=7.5085e-05, gnorm=0.488, train_wall=503, gb_free=6.1, wall=3277
2022-01-31 09:50:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:51:02 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.743 | ppl 1713.51 | wps 7977.1 | wpb 2034.1 | bsz 4 | num_updates 640
2022-01-31 09:51:02 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-01-31 09:51:02 | INFO | train | epoch 010 | loss 10.759 | ppl 1733.43 | wps 5944.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 640 | lr 8.0084e-05 | gnorm 0.484 | train_wall 323 | gb_free 6.1 | wall 3506
KL Stats: Epoch 10 Divergences: Uniform: 1.5928129414902545 Unigram: 0.726575061791323
2022-01-31 09:51:02 | INFO | fairseq.trainer | begin training epoch 11
2022-01-31 09:51:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 09:56:06 | INFO | train_inner | epoch 011:     60 / 64 loss=10.684, ppl=1645.03, wps=6122, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=700, lr=8.75825e-05, gnorm=0.494, train_wall=505, gb_free=6.1, wall=3810
2022-01-31 09:56:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 09:56:53 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.636 | ppl 1591.08 | wps 8015.4 | wpb 2034.1 | bsz 4 | num_updates 704
2022-01-31 09:56:53 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-01-31 09:56:53 | INFO | train | epoch 011 | loss 10.644 | ppl 1600.58 | wps 5954.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 704 | lr 8.80824e-05 | gnorm 0.499 | train_wall 323 | gb_free 6.1 | wall 3857
KL Stats: Epoch 11 Divergences: Uniform: 1.6123376178889608 Unigram: 0.8304129090247766
2022-01-31 09:56:53 | INFO | fairseq.trainer | begin training epoch 12
2022-01-31 09:56:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:02:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:02:44 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.529 | ppl 1477.1 | wps 7996.6 | wpb 2034.1 | bsz 4 | num_updates 768
2022-01-31 10:02:44 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-01-31 10:02:44 | INFO | train | epoch 012 | loss 10.528 | ppl 1476.5 | wps 5946.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 768 | lr 9.60808e-05 | gnorm 0.478 | train_wall 323 | gb_free 6.1 | wall 4208
KL Stats: Epoch 12 Divergences: Uniform: 1.6237976215068786 Unigram: 0.931897225138668
2022-01-31 10:02:44 | INFO | fairseq.trainer | begin training epoch 13
2022-01-31 10:02:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:05:27 | INFO | train_inner | epoch 013:     32 / 64 loss=10.504, ppl=1452.23, wps=5818.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=800, lr=0.00010008, gnorm=0.492, train_wall=504, gb_free=6.1, wall=4371
2022-01-31 10:08:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:08:35 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 10.437 | ppl 1386.69 | wps 8020.3 | wpb 2034.1 | bsz 4 | num_updates 832
2022-01-31 10:08:35 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-01-31 10:08:35 | INFO | train | epoch 013 | loss 10.414 | ppl 1364.48 | wps 5942.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 832 | lr 0.000104079 | gnorm 0.518 | train_wall 323 | gb_free 6.1 | wall 4559
KL Stats: Epoch 13 Divergences: Uniform: 1.6509153539214831 Unigram: 1.0206337089915303
2022-01-31 10:08:35 | INFO | fairseq.trainer | begin training epoch 14
2022-01-31 10:08:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:13:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:14:26 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 10.344 | ppl 1300.03 | wps 8011.3 | wpb 2034.1 | bsz 4 | num_updates 896
2022-01-31 10:14:26 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-01-31 10:14:26 | INFO | train | epoch 014 | loss 10.304 | ppl 1263.76 | wps 5952.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 896 | lr 0.000112078 | gnorm 0.554 | train_wall 323 | gb_free 6.1 | wall 4910
KL Stats: Epoch 14 Divergences: Uniform: 1.677293718416745 Unigram: 1.103458534226754
2022-01-31 10:14:26 | INFO | fairseq.trainer | begin training epoch 15
2022-01-31 10:14:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:14:47 | INFO | train_inner | epoch 015:      4 / 64 loss=10.326, ppl=1283.69, wps=5821.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=900, lr=0.000112578, gnorm=0.536, train_wall=504, gb_free=6.1, wall=4931
2022-01-31 10:19:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:20:17 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 10.275 | ppl 1239.39 | wps 7969.1 | wpb 2034.1 | bsz 4 | num_updates 960
2022-01-31 10:20:17 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-01-31 10:20:17 | INFO | train | epoch 015 | loss 10.192 | ppl 1169.65 | wps 5948.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 960 | lr 0.000120076 | gnorm 0.532 | train_wall 323 | gb_free 6.1 | wall 5261
KL Stats: Epoch 15 Divergences: Uniform: 1.7021763677656307 Unigram: 1.1782900924106943
2022-01-31 10:20:17 | INFO | fairseq.trainer | begin training epoch 16
2022-01-31 10:20:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:23:40 | INFO | train_inner | epoch 016:     40 / 64 loss=10.151, ppl=1136.82, wps=6121.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=1000, lr=0.000125075, gnorm=0.553, train_wall=505, gb_free=6.1, wall=5465
2022-01-31 10:25:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:26:08 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 10.196 | ppl 1172.82 | wps 8023.7 | wpb 2034.1 | bsz 4 | num_updates 1024
2022-01-31 10:26:08 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-01-31 10:26:08 | INFO | train | epoch 016 | loss 10.086 | ppl 1086.63 | wps 5951.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1024 | lr 0.000128074 | gnorm 0.553 | train_wall 323 | gb_free 6.1 | wall 5612
KL Stats: Epoch 16 Divergences: Uniform: 1.7317401962284507 Unigram: 1.2510396704586992
2022-01-31 10:26:08 | INFO | fairseq.trainer | begin training epoch 17
2022-01-31 10:26:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:31:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:31:59 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.102 | ppl 1099.16 | wps 7975.1 | wpb 2034.1 | bsz 4 | num_updates 1088
2022-01-31 10:31:59 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-01-31 10:31:59 | INFO | train | epoch 017 | loss 9.979 | ppl 1009.13 | wps 5954.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1088 | lr 0.000136073 | gnorm 0.544 | train_wall 322 | gb_free 6.1 | wall 5963
KL Stats: Epoch 17 Divergences: Uniform: 1.7665163546840297 Unigram: 1.3139824541843683
2022-01-31 10:31:59 | INFO | fairseq.trainer | begin training epoch 18
2022-01-31 10:31:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:33:00 | INFO | train_inner | epoch 018:     12 / 64 loss=9.993, ppl=1019.04, wps=5825.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=1100, lr=0.000137573, gnorm=0.548, train_wall=503, gb_free=6.1, wall=6024
2022-01-31 10:37:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:37:49 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.036 | ppl 1050.04 | wps 8044.4 | wpb 2034.1 | bsz 4 | num_updates 1152
2022-01-31 10:37:49 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-01-31 10:37:49 | INFO | train | epoch 018 | loss 9.878 | ppl 941.25 | wps 5959.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1152 | lr 0.000144071 | gnorm 0.569 | train_wall 322 | gb_free 6.1 | wall 6314
KL Stats: Epoch 18 Divergences: Uniform: 1.8017846695645565 Unigram: 1.375909205706343
2022-01-31 10:37:49 | INFO | fairseq.trainer | begin training epoch 19
2022-01-31 10:37:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:41:51 | INFO | train_inner | epoch 019:     48 / 64 loss=9.829, ppl=909.47, wps=6149.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=1200, lr=0.00015007, gnorm=0.541, train_wall=503, gb_free=6.1, wall=6556
2022-01-31 10:43:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:43:38 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.965 | ppl 999.23 | wps 8043.8 | wpb 2034.1 | bsz 4 | num_updates 1216
2022-01-31 10:43:38 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-01-31 10:43:38 | INFO | train | epoch 019 | loss 9.775 | ppl 876.26 | wps 5993.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1216 | lr 0.00015207 | gnorm 0.528 | train_wall 320 | gb_free 6.1 | wall 6662
KL Stats: Epoch 19 Divergences: Uniform: 1.8337019048581586 Unigram: 1.4378193049657308
2022-01-31 10:43:38 | INFO | fairseq.trainer | begin training epoch 20
2022-01-31 10:43:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:49:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:49:27 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.879 | ppl 941.94 | wps 8010.2 | wpb 2034.1 | bsz 4 | num_updates 1280
2022-01-31 10:49:27 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-01-31 10:49:27 | INFO | train | epoch 020 | loss 9.679 | ppl 819.72 | wps 5975.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1280 | lr 0.000160068 | gnorm 0.554 | train_wall 321 | gb_free 6.1 | wall 7012
KL Stats: Epoch 20 Divergences: Uniform: 1.8650812480484598 Unigram: 1.4935647577801334
2022-01-31 10:49:27 | INFO | fairseq.trainer | begin training epoch 21
2022-01-31 10:49:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 10:51:09 | INFO | train_inner | epoch 021:     20 / 64 loss=9.674, ppl=816.96, wps=5847, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=1300, lr=0.000162568, gnorm=0.549, train_wall=502, gb_free=6.1, wall=7113
2022-01-31 10:54:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 10:55:18 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.836 | ppl 914.09 | wps 8019.3 | wpb 2034.1 | bsz 4 | num_updates 1344
2022-01-31 10:55:18 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-01-31 10:55:18 | INFO | train | epoch 021 | loss 9.585 | ppl 767.88 | wps 5960.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1344 | lr 0.000168066 | gnorm 0.522 | train_wall 322 | gb_free 6.1 | wall 7362
KL Stats: Epoch 21 Divergences: Uniform: 1.8959466477684905 Unigram: 1.5474696312805731
2022-01-31 10:55:18 | INFO | fairseq.trainer | begin training epoch 22
2022-01-31 10:55:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:00:02 | INFO | train_inner | epoch 022:     56 / 64 loss=9.532, ppl=740.38, wps=6128.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=1400, lr=0.000175065, gnorm=0.529, train_wall=505, gb_free=6.1, wall=7646
2022-01-31 11:00:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:01:09 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.772 | ppl 874.3 | wps 7984.7 | wpb 2034.1 | bsz 4 | num_updates 1408
2022-01-31 11:01:09 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-01-31 11:01:09 | INFO | train | epoch 022 | loss 9.496 | ppl 721.95 | wps 5950.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1408 | lr 0.000176065 | gnorm 0.542 | train_wall 323 | gb_free 6.1 | wall 7713
KL Stats: Epoch 22 Divergences: Uniform: 1.9216239144417957 Unigram: 1.599239671066291
2022-01-31 11:01:09 | INFO | fairseq.trainer | begin training epoch 23
2022-01-31 11:01:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:06:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:07:00 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.713 | ppl 839.35 | wps 8013.6 | wpb 2034.1 | bsz 4 | num_updates 1472
2022-01-31 11:07:00 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-01-31 11:07:00 | INFO | train | epoch 023 | loss 9.409 | ppl 679.96 | wps 5948.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1472 | lr 0.000184063 | gnorm 0.515 | train_wall 323 | gb_free 6.1 | wall 8064
KL Stats: Epoch 23 Divergences: Uniform: 1.949994775913474 Unigram: 1.646232332910266
2022-01-31 11:07:00 | INFO | fairseq.trainer | begin training epoch 24
2022-01-31 11:07:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:09:22 | INFO | train_inner | epoch 024:     28 / 64 loss=9.394, ppl=672.87, wps=5825.1, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=1500, lr=0.000187563, gnorm=0.526, train_wall=504, gb_free=6.1, wall=8206
2022-01-31 11:12:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:12:50 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.654 | ppl 805.76 | wps 8024.7 | wpb 2034.1 | bsz 4 | num_updates 1536
2022-01-31 11:12:50 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-01-31 11:12:50 | INFO | train | epoch 024 | loss 9.326 | ppl 641.69 | wps 5961.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1536 | lr 0.000192062 | gnorm 0.546 | train_wall 322 | gb_free 6.1 | wall 8414
KL Stats: Epoch 24 Divergences: Uniform: 1.9727753792875775 Unigram: 1.6889862349654534
2022-01-31 11:12:50 | INFO | fairseq.trainer | begin training epoch 25
2022-01-31 11:12:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:18:13 | INFO | train_inner | epoch 025:     64 / 64 loss=9.272, ppl=618.13, wps=6131.9, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=1600, lr=0.00020006, gnorm=0.53, train_wall=503, gb_free=6.1, wall=8738
2022-01-31 11:18:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:18:41 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.624 | ppl 789.3 | wps 7948.4 | wpb 2034.1 | bsz 4 | num_updates 1600
2022-01-31 11:18:41 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-01-31 11:18:41 | INFO | train | epoch 025 | loss 9.244 | ppl 606.39 | wps 5957.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1600 | lr 0.00020006 | gnorm 0.521 | train_wall 322 | gb_free 6.1 | wall 8765
KL Stats: Epoch 25 Divergences: Uniform: 2.0026716159493776 Unigram: 1.7330858693671025
2022-01-31 11:18:41 | INFO | fairseq.trainer | begin training epoch 26
2022-01-31 11:18:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:24:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:24:31 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.576 | ppl 763.5 | wps 8029.1 | wpb 2034.1 | bsz 4 | num_updates 1664
2022-01-31 11:24:31 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-01-31 11:24:31 | INFO | train | epoch 026 | loss 9.163 | ppl 573.26 | wps 5962.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1664 | lr 0.000208058 | gnorm 0.534 | train_wall 322 | gb_free 6.1 | wall 9115
KL Stats: Epoch 26 Divergences: Uniform: 2.0169362838777287 Unigram: 1.7723366416299602
2022-01-31 11:24:31 | INFO | fairseq.trainer | begin training epoch 27
2022-01-31 11:24:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:27:34 | INFO | train_inner | epoch 027:     36 / 64 loss=9.135, ppl=562.15, wps=5834.5, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=1700, lr=0.000212558, gnorm=0.523, train_wall=504, gb_free=6.1, wall=9298
2022-01-31 11:29:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:30:21 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.543 | ppl 745.9 | wps 8010.9 | wpb 2034.1 | bsz 4 | num_updates 1728
2022-01-31 11:30:21 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-01-31 11:30:21 | INFO | train | epoch 027 | loss 9.082 | ppl 542.07 | wps 5966.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1728 | lr 0.000216057 | gnorm 0.514 | train_wall 322 | gb_free 6.1 | wall 9465
KL Stats: Epoch 27 Divergences: Uniform: 2.0451686198162173 Unigram: 1.8091918034813852
2022-01-31 11:30:21 | INFO | fairseq.trainer | begin training epoch 28
2022-01-31 11:30:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:35:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:36:12 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.514 | ppl 731.27 | wps 8001.1 | wpb 2034.1 | bsz 4 | num_updates 1792
2022-01-31 11:36:12 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-01-31 11:36:12 | INFO | train | epoch 028 | loss 9.005 | ppl 513.67 | wps 5952.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1792 | lr 0.000224055 | gnorm 0.522 | train_wall 323 | gb_free 6.1 | wall 9816
KL Stats: Epoch 28 Divergences: Uniform: 2.0734511726547016 Unigram: 1.846561328919263
2022-01-31 11:36:12 | INFO | fairseq.trainer | begin training epoch 29
2022-01-31 11:36:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:36:53 | INFO | train_inner | epoch 029:      8 / 64 loss=9.02, ppl=519.29, wps=5827.3, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=1800, lr=0.000225055, gnorm=0.527, train_wall=503, gb_free=6.1, wall=9857
2022-01-31 11:41:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:42:02 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.483 | ppl 715.84 | wps 8004.3 | wpb 2034.1 | bsz 4 | num_updates 1856
2022-01-31 11:42:02 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-01-31 11:42:02 | INFO | train | epoch 029 | loss 8.926 | ppl 486.44 | wps 5963 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1856 | lr 0.000232054 | gnorm 0.529 | train_wall 322 | gb_free 6.1 | wall 10167
KL Stats: Epoch 29 Divergences: Uniform: 2.0946086788516123 Unigram: 1.882260890309672
2022-01-31 11:42:02 | INFO | fairseq.trainer | begin training epoch 30
2022-01-31 11:42:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:45:46 | INFO | train_inner | epoch 030:     44 / 64 loss=8.893, ppl=475.35, wps=6135.7, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=1900, lr=0.000237553, gnorm=0.517, train_wall=504, gb_free=6.1, wall=10390
2022-01-31 11:47:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:47:53 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 9.462 | ppl 705.19 | wps 8021.5 | wpb 2034.1 | bsz 4 | num_updates 1920
2022-01-31 11:47:53 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-01-31 11:47:53 | INFO | train | epoch 030 | loss 8.848 | ppl 460.8 | wps 5959.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1920 | lr 0.000240052 | gnorm 0.516 | train_wall 322 | gb_free 6.1 | wall 10517
KL Stats: Epoch 30 Divergences: Uniform: 2.1129783086653977 Unigram: 1.9192933076442849
2022-01-31 11:47:53 | INFO | fairseq.trainer | begin training epoch 31
2022-01-31 11:47:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:53:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:53:44 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.411 | ppl 680.78 | wps 7971.2 | wpb 2034.1 | bsz 4 | num_updates 1984
2022-01-31 11:53:44 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-01-31 11:53:44 | INFO | train | epoch 031 | loss 8.769 | ppl 436.27 | wps 5953.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1984 | lr 0.00024805 | gnorm 0.494 | train_wall 323 | gb_free 6.1 | wall 10868
KL Stats: Epoch 31 Divergences: Uniform: 2.1328234181528987 Unigram: 1.951347608752532
2022-01-31 11:53:44 | INFO | fairseq.trainer | begin training epoch 32
2022-01-31 11:53:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 11:55:05 | INFO | train_inner | epoch 032:     16 / 64 loss=8.77, ppl=436.67, wps=5826.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=2000, lr=0.00025005, gnorm=0.502, train_wall=503, gb_free=6.1, wall=10949
2022-01-31 11:59:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 11:59:35 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 9.375 | ppl 663.8 | wps 8040.3 | wpb 2034.1 | bsz 4 | num_updates 2048
2022-01-31 11:59:35 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-01-31 11:59:35 | INFO | train | epoch 032 | loss 8.695 | ppl 414.53 | wps 5954.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2048 | lr 0.000256049 | gnorm 0.507 | train_wall 323 | gb_free 6.1 | wall 11219
KL Stats: Epoch 32 Divergences: Uniform: 2.1587278027032886 Unigram: 1.985949289624129
2022-01-31 11:59:35 | INFO | fairseq.trainer | begin training epoch 33
2022-01-31 11:59:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:03:58 | INFO | train_inner | epoch 033:     52 / 64 loss=8.658, ppl=404.03, wps=6135.1, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=2100, lr=0.000262548, gnorm=0.507, train_wall=504, gb_free=6.1, wall=11482
2022-01-31 12:04:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:05:24 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 9.362 | ppl 658.15 | wps 8007.2 | wpb 2034.1 | bsz 4 | num_updates 2112
2022-01-31 12:05:24 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-01-31 12:05:24 | INFO | train | epoch 033 | loss 8.621 | ppl 393.67 | wps 5968.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2112 | lr 0.000264047 | gnorm 0.505 | train_wall 322 | gb_free 6.1 | wall 11569
KL Stats: Epoch 33 Divergences: Uniform: 2.185260094275646 Unigram: 2.0201879666214566
2022-01-31 12:05:24 | INFO | fairseq.trainer | begin training epoch 34
2022-01-31 12:05:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:10:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:11:14 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 9.346 | ppl 650.72 | wps 8097.4 | wpb 2034.1 | bsz 4 | num_updates 2176
2022-01-31 12:11:14 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-01-31 12:11:14 | INFO | train | epoch 034 | loss 8.545 | ppl 373.54 | wps 5974.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2176 | lr 0.000272046 | gnorm 0.507 | train_wall 322 | gb_free 6.1 | wall 11918
KL Stats: Epoch 34 Divergences: Uniform: 2.2078339469416957 Unigram: 2.0538029326791505
2022-01-31 12:11:14 | INFO | fairseq.trainer | begin training epoch 35
2022-01-31 12:11:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:13:15 | INFO | train_inner | epoch 035:     24 / 64 loss=8.533, ppl=370.29, wps=5850.9, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=2200, lr=0.000275045, gnorm=0.507, train_wall=501, gb_free=6.1, wall=12039
2022-01-31 12:16:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:17:02 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 9.316 | ppl 637.46 | wps 8066.3 | wpb 2034.1 | bsz 4 | num_updates 2240
2022-01-31 12:17:02 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-01-31 12:17:02 | INFO | train | epoch 035 | loss 8.474 | ppl 355.45 | wps 5997.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2240 | lr 0.000280044 | gnorm 0.504 | train_wall 320 | gb_free 6.1 | wall 12266
KL Stats: Epoch 35 Divergences: Uniform: 2.229343265943193 Unigram: 2.0819022033249492
2022-01-31 12:17:02 | INFO | fairseq.trainer | begin training epoch 36
2022-01-31 12:17:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:22:06 | INFO | train_inner | epoch 036:     60 / 64 loss=8.429, ppl=344.68, wps=6153.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=2300, lr=0.000287543, gnorm=0.497, train_wall=503, gb_free=6.1, wall=12570
2022-01-31 12:22:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:22:52 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 9.286 | ppl 624.13 | wps 8015.6 | wpb 2034.1 | bsz 4 | num_updates 2304
2022-01-31 12:22:52 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-01-31 12:22:52 | INFO | train | epoch 036 | loss 8.399 | ppl 337.59 | wps 5968 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2304 | lr 0.000288042 | gnorm 0.494 | train_wall 322 | gb_free 6.1 | wall 12616
KL Stats: Epoch 36 Divergences: Uniform: 2.250290759235184 Unigram: 2.1151387002591924
2022-01-31 12:22:52 | INFO | fairseq.trainer | begin training epoch 37
2022-01-31 12:22:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:28:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:28:43 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.3 | ppl 630.17 | wps 8010 | wpb 2034.1 | bsz 4 | num_updates 2368
2022-01-31 12:28:43 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-01-31 12:28:43 | INFO | train | epoch 037 | loss 8.33 | ppl 321.79 | wps 5954.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2368 | lr 0.000296041 | gnorm 0.506 | train_wall 323 | gb_free 6.1 | wall 12967
KL Stats: Epoch 37 Divergences: Uniform: 2.271226338370874 Unigram: 2.1479327047977033
2022-01-31 12:28:43 | INFO | fairseq.trainer | begin training epoch 38
2022-01-31 12:28:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:31:25 | INFO | train_inner | epoch 038:     32 / 64 loss=8.309, ppl=317.05, wps=5827.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=2400, lr=0.00030004, gnorm=0.503, train_wall=503, gb_free=6.1, wall=13130
2022-01-31 12:34:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:34:34 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 9.282 | ppl 622.59 | wps 7975.8 | wpb 2034.1 | bsz 4 | num_updates 2432
2022-01-31 12:34:34 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-01-31 12:34:34 | INFO | train | epoch 038 | loss 8.262 | ppl 306.95 | wps 5954.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2432 | lr 0.000304039 | gnorm 0.507 | train_wall 322 | gb_free 6.1 | wall 13318
KL Stats: Epoch 38 Divergences: Uniform: 2.3018023846046796 Unigram: 2.1705607920289833
2022-01-31 12:34:34 | INFO | fairseq.trainer | begin training epoch 39
2022-01-31 12:34:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:39:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:40:24 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.266 | ppl 615.56 | wps 7990.3 | wpb 2034.1 | bsz 4 | num_updates 2496
2022-01-31 12:40:24 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-01-31 12:40:24 | INFO | train | epoch 039 | loss 8.193 | ppl 292.56 | wps 5956.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2496 | lr 0.000312038 | gnorm 0.494 | train_wall 322 | gb_free 6.1 | wall 13669
KL Stats: Epoch 39 Divergences: Uniform: 2.311048492981072 Unigram: 2.206612928570825
2022-01-31 12:40:24 | INFO | fairseq.trainer | begin training epoch 40
2022-01-31 12:40:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:40:45 | INFO | train_inner | epoch 040:      4 / 64 loss=8.215, ppl=297.05, wps=5827.7, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=2500, lr=0.000312538, gnorm=0.5, train_wall=503, gb_free=6.1, wall=13689
2022-01-31 12:45:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:46:15 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.246 | ppl 607.32 | wps 7976.8 | wpb 2034.1 | bsz 4 | num_updates 2560
2022-01-31 12:46:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 2560 updates
2022-01-31 12:46:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint40.pt
2022-01-31 12:46:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint40.pt
2022-01-31 12:46:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint40.pt (epoch 40 @ 2560 updates, score 9.246) (writing took 5.354029951617122 seconds)
2022-01-31 12:46:20 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-01-31 12:46:20 | INFO | train | epoch 040 | loss 8.125 | ppl 279.09 | wps 5868.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2560 | lr 0.000320036 | gnorm 0.498 | train_wall 322 | gb_free 6.1 | wall 14024
KL Stats: Epoch 40 Divergences: Uniform: 2.3392316898239276 Unigram: 2.233293056382214
2022-01-31 12:46:20 | INFO | fairseq.trainer | begin training epoch 41
2022-01-31 12:46:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:49:43 | INFO | train_inner | epoch 041:     40 / 64 loss=8.101, ppl=274.59, wps=6072.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=2600, lr=0.000325035, gnorm=0.497, train_wall=504, gb_free=6.1, wall=14227
2022-01-31 12:51:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:52:11 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.24 | ppl 604.83 | wps 7987 | wpb 2034.1 | bsz 4 | num_updates 2624 | best_loss 9.24
2022-01-31 12:52:11 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-01-31 12:52:11 | INFO | train | epoch 041 | loss 8.061 | ppl 267.01 | wps 5957.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2624 | lr 0.000328034 | gnorm 0.5 | train_wall 322 | gb_free 6.1 | wall 14375
KL Stats: Epoch 41 Divergences: Uniform: 2.3539291284267394 Unigram: 2.2586266373296975
2022-01-31 12:52:11 | INFO | fairseq.trainer | begin training epoch 42
2022-01-31 12:52:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:57:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 12:58:02 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.213 | ppl 593.27 | wps 7992.2 | wpb 2034.1 | bsz 4 | num_updates 2688 | best_loss 9.213
2022-01-31 12:58:02 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-01-31 12:58:02 | INFO | train | epoch 042 | loss 7.997 | ppl 255.42 | wps 5956.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2688 | lr 0.000336033 | gnorm 0.507 | train_wall 322 | gb_free 6.1 | wall 14726
KL Stats: Epoch 42 Divergences: Uniform: 2.373181215456418 Unigram: 2.2907259681200483
2022-01-31 12:58:02 | INFO | fairseq.trainer | begin training epoch 43
2022-01-31 12:58:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 12:59:03 | INFO | train_inner | epoch 043:     12 / 64 loss=8.003, ppl=256.56, wps=5825.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=2700, lr=0.000337533, gnorm=0.506, train_wall=503, gb_free=6.1, wall=14787
2022-01-31 13:03:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:03:52 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.242 | ppl 605.34 | wps 8022.1 | wpb 2034.1 | bsz 4 | num_updates 2752 | best_loss 9.242
2022-01-31 13:03:52 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-01-31 13:03:52 | INFO | train | epoch 043 | loss 7.932 | ppl 244.17 | wps 5955.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2752 | lr 0.000344031 | gnorm 0.497 | train_wall 323 | gb_free 6.1 | wall 15076
KL Stats: Epoch 43 Divergences: Uniform: 2.39493089001601 Unigram: 2.3174577780527588
2022-01-31 13:03:52 | INFO | fairseq.trainer | begin training epoch 44
2022-01-31 13:03:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:07:56 | INFO | train_inner | epoch 044:     48 / 64 loss=7.897, ppl=238.43, wps=6126.4, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=2800, lr=0.00035003, gnorm=0.499, train_wall=505, gb_free=6.1, wall=15320
2022-01-31 13:09:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:09:43 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.242 | ppl 605.69 | wps 7983.1 | wpb 2034.1 | bsz 4 | num_updates 2816 | best_loss 9.242
2022-01-31 13:09:43 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-01-31 13:09:43 | INFO | train | epoch 044 | loss 7.872 | ppl 234.26 | wps 5951.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2816 | lr 0.00035203 | gnorm 0.502 | train_wall 323 | gb_free 6.1 | wall 15427
KL Stats: Epoch 44 Divergences: Uniform: 2.412821130862619 Unigram: 2.340842516116444
2022-01-31 13:09:43 | INFO | fairseq.trainer | begin training epoch 45
2022-01-31 13:09:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:15:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:15:34 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.221 | ppl 596.66 | wps 8003.5 | wpb 2034.1 | bsz 4 | num_updates 2880 | best_loss 9.221
2022-01-31 13:15:34 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-01-31 13:15:34 | INFO | train | epoch 045 | loss 7.81 | ppl 224.36 | wps 5949 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2880 | lr 0.000360028 | gnorm 0.504 | train_wall 323 | gb_free 6.1 | wall 15778
KL Stats: Epoch 45 Divergences: Uniform: 2.4316907512099277 Unigram: 2.371139778331074
2022-01-31 13:15:34 | INFO | fairseq.trainer | begin training epoch 46
2022-01-31 13:15:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:17:16 | INFO | train_inner | epoch 046:     20 / 64 loss=7.809, ppl=224.28, wps=5823.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=2900, lr=0.000362528, gnorm=0.504, train_wall=504, gb_free=6.1, wall=15880
2022-01-31 13:20:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:21:25 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.227 | ppl 599.36 | wps 7990.1 | wpb 2034.1 | bsz 4 | num_updates 2944 | best_loss 9.227
2022-01-31 13:21:25 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-01-31 13:21:25 | INFO | train | epoch 046 | loss 7.751 | ppl 215.37 | wps 5948.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2944 | lr 0.000368026 | gnorm 0.507 | train_wall 323 | gb_free 6.1 | wall 16130
KL Stats: Epoch 46 Divergences: Uniform: 2.44571838345043 Unigram: 2.389719107073781
2022-01-31 13:21:25 | INFO | fairseq.trainer | begin training epoch 47
2022-01-31 13:21:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:26:10 | INFO | train_inner | epoch 047:     56 / 64 loss=7.72, ppl=210.81, wps=6120, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3000, lr=0.000375025, gnorm=0.499, train_wall=505, gb_free=6.1, wall=16414
2022-01-31 13:26:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:27:17 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.208 | ppl 591.33 | wps 8000.8 | wpb 2034.1 | bsz 4 | num_updates 3008 | best_loss 9.208
2022-01-31 13:27:17 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-01-31 13:27:17 | INFO | train | epoch 047 | loss 7.692 | ppl 206.72 | wps 5948.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3008 | lr 0.000376025 | gnorm 0.492 | train_wall 323 | gb_free 6.1 | wall 16481
KL Stats: Epoch 47 Divergences: Uniform: 2.4685463246058954 Unigram: 2.412590927406782
2022-01-31 13:27:17 | INFO | fairseq.trainer | begin training epoch 48
2022-01-31 13:27:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:32:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:33:07 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.206 | ppl 590.43 | wps 7971.6 | wpb 2034.1 | bsz 4 | num_updates 3072 | best_loss 9.206
2022-01-31 13:33:07 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-01-31 13:33:07 | INFO | train | epoch 048 | loss 7.635 | ppl 198.75 | wps 5955.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3072 | lr 0.000384023 | gnorm 0.508 | train_wall 322 | gb_free 6.1 | wall 16831
KL Stats: Epoch 48 Divergences: Uniform: 2.4871440295690546 Unigram: 2.442350126085311
2022-01-31 13:33:07 | INFO | fairseq.trainer | begin training epoch 49
2022-01-31 13:33:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:35:29 | INFO | train_inner | epoch 049:     28 / 64 loss=7.617, ppl=196.34, wps=5826.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=3100, lr=0.000387523, gnorm=0.504, train_wall=503, gb_free=6.1, wall=16973
2022-01-31 13:38:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:38:57 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.233 | ppl 601.7 | wps 8070.1 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 9.233
2022-01-31 13:38:57 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-01-31 13:38:57 | INFO | train | epoch 049 | loss 7.578 | ppl 191.08 | wps 5965.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3136 | lr 0.000392022 | gnorm 0.507 | train_wall 322 | gb_free 6.1 | wall 17181
KL Stats: Epoch 49 Divergences: Uniform: 2.492513298935431 Unigram: 2.4626103808389894
2022-01-31 13:38:57 | INFO | fairseq.trainer | begin training epoch 50
2022-01-31 13:38:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:44:18 | INFO | train_inner | epoch 050:     64 / 64 loss=7.553, ppl=187.82, wps=6166.6, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=3200, lr=0.00040002, gnorm=0.52, train_wall=500, gb_free=6.1, wall=17502
2022-01-31 13:44:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:44:45 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.242 | ppl 605.51 | wps 8077.2 | wpb 2034.1 | bsz 4 | num_updates 3200 | best_loss 9.242
2022-01-31 13:44:45 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-01-31 13:44:45 | INFO | train | epoch 050 | loss 7.526 | ppl 184.35 | wps 6007.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3200 | lr 0.00040002 | gnorm 0.524 | train_wall 320 | gb_free 6.1 | wall 17529
KL Stats: Epoch 50 Divergences: Uniform: 2.5132236073295435 Unigram: 2.4790649263794244
2022-01-31 13:44:45 | INFO | fairseq.trainer | begin training epoch 51
2022-01-31 13:44:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:50:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:50:34 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.26 | ppl 613.29 | wps 8004.5 | wpb 2034.1 | bsz 4 | num_updates 3264 | best_loss 9.246
2022-01-31 13:50:34 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-01-31 13:50:34 | INFO | train | epoch 051 | loss 7.47 | ppl 177.31 | wps 5977.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3264 | lr 0.000408018 | gnorm 0.505 | train_wall 321 | gb_free 6.1 | wall 17879
KL Stats: Epoch 51 Divergences: Uniform: 2.5384572158640775 Unigram: 2.5002152715531447
2022-01-31 13:50:34 | INFO | fairseq.trainer | begin training epoch 52
2022-01-31 13:50:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 13:53:37 | INFO | train_inner | epoch 052:     36 / 64 loss=7.446, ppl=174.39, wps=5845.9, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=3300, lr=0.000412518, gnorm=0.504, train_wall=503, gb_free=6.1, wall=18061
2022-01-31 13:55:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 13:56:25 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.248 | ppl 607.87 | wps 8003.8 | wpb 2034.1 | bsz 4 | num_updates 3328 | best_loss 9.246
2022-01-31 13:56:25 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-01-31 13:56:25 | INFO | train | epoch 052 | loss 7.418 | ppl 171.02 | wps 5961.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3328 | lr 0.000416017 | gnorm 0.512 | train_wall 322 | gb_free 6.1 | wall 18229
KL Stats: Epoch 52 Divergences: Uniform: 2.5459241317845533 Unigram: 2.5297980139216367
2022-01-31 13:56:25 | INFO | fairseq.trainer | begin training epoch 53
2022-01-31 13:56:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:01:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:02:15 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.215 | ppl 594.48 | wps 7972.5 | wpb 2034.1 | bsz 4 | num_updates 3392 | best_loss 9.215
2022-01-31 14:02:15 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-01-31 14:02:15 | INFO | train | epoch 053 | loss 7.367 | ppl 165.04 | wps 5958.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3392 | lr 0.000424015 | gnorm 0.499 | train_wall 322 | gb_free 6.1 | wall 18579
KL Stats: Epoch 53 Divergences: Uniform: 2.571177017408321 Unigram: 2.546479536065831
2022-01-31 14:02:15 | INFO | fairseq.trainer | begin training epoch 54
2022-01-31 14:02:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:02:56 | INFO | train_inner | epoch 054:      8 / 64 loss=7.38, ppl=166.53, wps=5831.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=3400, lr=0.000425015, gnorm=0.51, train_wall=503, gb_free=6.1, wall=18620
2022-01-31 14:07:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:08:07 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.28 | ppl 621.83 | wps 7984.2 | wpb 2034.1 | bsz 4 | num_updates 3456 | best_loss 9.246
2022-01-31 14:08:07 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-01-31 14:08:07 | INFO | train | epoch 054 | loss 7.316 | ppl 159.36 | wps 5937.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3456 | lr 0.000432014 | gnorm 0.515 | train_wall 324 | gb_free 6.1 | wall 18931
KL Stats: Epoch 54 Divergences: Uniform: 2.575243285746771 Unigram: 2.5651617547537664
2022-01-31 14:08:07 | INFO | fairseq.trainer | begin training epoch 55
2022-01-31 14:08:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:11:51 | INFO | train_inner | epoch 055:     44 / 64 loss=7.289, ppl=156.37, wps=6110.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3500, lr=0.000437513, gnorm=0.516, train_wall=506, gb_free=6.1, wall=19155
2022-01-31 14:13:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:13:58 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.271 | ppl 617.99 | wps 7965.2 | wpb 2034.1 | bsz 4 | num_updates 3520 | best_loss 9.246
2022-01-31 14:13:58 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-01-31 14:13:58 | INFO | train | epoch 055 | loss 7.27 | ppl 154.31 | wps 5945.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3520 | lr 0.000440012 | gnorm 0.533 | train_wall 323 | gb_free 6.1 | wall 19283
KL Stats: Epoch 55 Divergences: Uniform: 2.592504079321782 Unigram: 2.5896378388893746
2022-01-31 14:13:58 | INFO | fairseq.trainer | begin training epoch 56
2022-01-31 14:13:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:19:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:19:50 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.38 | ppl 666.5 | wps 7981.7 | wpb 2034.1 | bsz 4 | num_updates 3584 | best_loss 9.246
2022-01-31 14:19:50 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-01-31 14:19:50 | INFO | train | epoch 056 | loss 7.22 | ppl 149.05 | wps 5946.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3584 | lr 0.00044801 | gnorm 0.515 | train_wall 323 | gb_free 6.1 | wall 19634
KL Stats: Epoch 56 Divergences: Uniform: 2.588788986840991 Unigram: 2.6029715724272884
2022-01-31 14:19:50 | INFO | fairseq.trainer | begin training epoch 57
2022-01-31 14:19:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:21:11 | INFO | train_inner | epoch 057:     16 / 64 loss=7.224, ppl=149.48, wps=5822.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=3600, lr=0.00045001, gnorm=0.525, train_wall=504, gb_free=6.1, wall=19715
2022-01-31 14:25:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:25:40 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.356 | ppl 655.47 | wps 8001.1 | wpb 2034.1 | bsz 4 | num_updates 3648 | best_loss 9.246
2022-01-31 14:25:40 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-01-31 14:25:40 | INFO | train | epoch 057 | loss 7.173 | ppl 144.27 | wps 5954.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3648 | lr 0.000456009 | gnorm 0.526 | train_wall 323 | gb_free 6.1 | wall 19985
KL Stats: Epoch 57 Divergences: Uniform: 2.6248513855407145 Unigram: 2.6314225590498883
2022-01-31 14:25:40 | INFO | fairseq.trainer | begin training epoch 58
2022-01-31 14:25:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:30:05 | INFO | train_inner | epoch 058:     52 / 64 loss=7.149, ppl=141.94, wps=6120.7, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3700, lr=0.000462508, gnorm=0.528, train_wall=505, gb_free=6.1, wall=20249
2022-01-31 14:31:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:31:32 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.348 | ppl 651.56 | wps 7996.1 | wpb 2034.1 | bsz 4 | num_updates 3712 | best_loss 9.246
2022-01-31 14:31:32 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-01-31 14:31:32 | INFO | train | epoch 058 | loss 7.129 | ppl 140 | wps 5944.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3712 | lr 0.000464007 | gnorm 0.529 | train_wall 323 | gb_free 6.1 | wall 20336
KL Stats: Epoch 58 Divergences: Uniform: 2.6320366394874313 Unigram: 2.6464217880594427
2022-01-31 14:31:32 | INFO | fairseq.trainer | begin training epoch 59
2022-01-31 14:31:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:36:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:37:23 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.444 | ppl 696.67 | wps 7948.9 | wpb 2034.1 | bsz 4 | num_updates 3776 | best_loss 9.246
2022-01-31 14:37:23 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-01-31 14:37:23 | INFO | train | epoch 059 | loss 7.084 | ppl 135.7 | wps 5952 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3776 | lr 0.000472006 | gnorm 0.527 | train_wall 323 | gb_free 6.1 | wall 20687
KL Stats: Epoch 59 Divergences: Uniform: 2.6483546816609564 Unigram: 2.661073766599165
2022-01-31 14:37:23 | INFO | fairseq.trainer | begin training epoch 60
2022-01-31 14:37:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:39:24 | INFO | train_inner | epoch 060:     24 / 64 loss=7.078, ppl=135.14, wps=5825.5, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=3800, lr=0.000475005, gnorm=0.533, train_wall=503, gb_free=6.1, wall=20808
2022-01-31 14:42:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:43:13 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.374 | ppl 663.38 | wps 7961.7 | wpb 2034.1 | bsz 4 | num_updates 3840 | best_loss 9.246
2022-01-31 14:43:13 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-01-31 14:43:13 | INFO | train | epoch 060 | loss 7.039 | ppl 131.52 | wps 5954.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3840 | lr 0.000480004 | gnorm 0.536 | train_wall 322 | gb_free 6.1 | wall 21038
KL Stats: Epoch 60 Divergences: Uniform: 2.663031051172662 Unigram: 2.686090431264174
2022-01-31 14:43:13 | INFO | fairseq.trainer | begin training epoch 61
2022-01-31 14:43:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:48:18 | INFO | train_inner | epoch 061:     60 / 64 loss=7.019, ppl=129.71, wps=6123.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3900, lr=0.000487503, gnorm=0.535, train_wall=505, gb_free=6.1, wall=21342
2022-01-31 14:48:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:49:04 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.434 | ppl 691.88 | wps 8010.4 | wpb 2034.1 | bsz 4 | num_updates 3904 | best_loss 9.246
2022-01-31 14:49:04 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-01-31 14:49:04 | INFO | train | epoch 061 | loss 6.996 | ppl 127.67 | wps 5951.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3904 | lr 0.000488002 | gnorm 0.542 | train_wall 323 | gb_free 6.1 | wall 21389
KL Stats: Epoch 61 Divergences: Uniform: 2.6812445053882827 Unigram: 2.6953474898577476
2022-01-31 14:49:04 | INFO | fairseq.trainer | begin training epoch 62
2022-01-31 14:49:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:54:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 14:54:55 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.409 | ppl 679.96 | wps 7985 | wpb 2034.1 | bsz 4 | num_updates 3968 | best_loss 9.246
2022-01-31 14:54:55 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-01-31 14:54:55 | INFO | train | epoch 062 | loss 6.955 | ppl 124.04 | wps 5952.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3968 | lr 0.000496001 | gnorm 0.538 | train_wall 323 | gb_free 6.1 | wall 21739
KL Stats: Epoch 62 Divergences: Uniform: 2.687262237415915 Unigram: 2.7204856309581746
2022-01-31 14:54:55 | INFO | fairseq.trainer | begin training epoch 63
2022-01-31 14:54:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 14:57:38 | INFO | train_inner | epoch 063:     32 / 64 loss=6.929, ppl=121.85, wps=5821.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4000, lr=0.0005, gnorm=0.538, train_wall=504, gb_free=6.1, wall=21902
2022-01-31 15:00:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:00:47 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.431 | ppl 690.24 | wps 7970.2 | wpb 2034.1 | bsz 4 | num_updates 4032 | best_loss 9.246
2022-01-31 15:00:47 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-01-31 15:00:47 | INFO | train | epoch 063 | loss 6.912 | ppl 120.4 | wps 5943 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4032 | lr 0.000498012 | gnorm 0.536 | train_wall 323 | gb_free 6.1 | wall 22091
KL Stats: Epoch 63 Divergences: Uniform: 2.7028314626010532 Unigram: 2.7365208688017213
2022-01-31 15:00:47 | INFO | fairseq.trainer | begin training epoch 64
2022-01-31 15:00:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:06:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:06:38 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.458 | ppl 703.14 | wps 8035.8 | wpb 2034.1 | bsz 4 | num_updates 4096 | best_loss 9.246
2022-01-31 15:06:38 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-01-31 15:06:38 | INFO | train | epoch 064 | loss 6.869 | ppl 116.89 | wps 5944.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4096 | lr 0.000494106 | gnorm 0.546 | train_wall 323 | gb_free 6.1 | wall 22442
KL Stats: Epoch 64 Divergences: Uniform: 2.71180065716313 Unigram: 2.753621684356737
2022-01-31 15:06:38 | INFO | fairseq.trainer | begin training epoch 65
2022-01-31 15:06:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:06:58 | INFO | train_inner | epoch 065:      4 / 64 loss=6.896, ppl=119.1, wps=5816.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4100, lr=0.000493865, gnorm=0.544, train_wall=504, gb_free=6.1, wall=22463
2022-01-31 15:12:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:12:27 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.504 | ppl 726.05 | wps 8103.1 | wpb 2034.1 | bsz 4 | num_updates 4160 | best_loss 9.246
2022-01-31 15:12:27 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-01-31 15:12:27 | INFO | train | epoch 065 | loss 6.826 | ppl 113.49 | wps 5982.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4160 | lr 0.00049029 | gnorm 0.552 | train_wall 321 | gb_free 6.1 | wall 22791
KL Stats: Epoch 65 Divergences: Uniform: 2.7159273904850365 Unigram: 2.771189072160683
2022-01-31 15:12:27 | INFO | fairseq.trainer | begin training epoch 66
2022-01-31 15:12:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:15:49 | INFO | train_inner | epoch 066:     40 / 64 loss=6.802, ppl=111.57, wps=6165.6, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4200, lr=0.00048795, gnorm=0.555, train_wall=502, gb_free=6.1, wall=22993
2022-01-31 15:17:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:18:15 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.478 | ppl 713.33 | wps 8061.8 | wpb 2034.1 | bsz 4 | num_updates 4224 | best_loss 9.246
2022-01-31 15:18:15 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-01-31 15:18:15 | INFO | train | epoch 066 | loss 6.786 | ppl 110.38 | wps 6003.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4224 | lr 0.000486562 | gnorm 0.558 | train_wall 320 | gb_free 6.1 | wall 23139
KL Stats: Epoch 66 Divergences: Uniform: 2.7363678512708693 Unigram: 2.7854401653626732
2022-01-31 15:18:15 | INFO | fairseq.trainer | begin training epoch 67
2022-01-31 15:18:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:23:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:24:05 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.455 | ppl 701.94 | wps 8021.8 | wpb 2034.1 | bsz 4 | num_updates 4288 | best_loss 9.246
2022-01-31 15:24:05 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-01-31 15:24:05 | INFO | train | epoch 067 | loss 6.745 | ppl 107.3 | wps 5963.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4288 | lr 0.000482917 | gnorm 0.554 | train_wall 322 | gb_free 6.1 | wall 23490
KL Stats: Epoch 67 Divergences: Uniform: 2.7525707326144895 Unigram: 2.8078312264301486
2022-01-31 15:24:05 | INFO | fairseq.trainer | begin training epoch 68
2022-01-31 15:24:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:25:06 | INFO | train_inner | epoch 068:     12 / 64 loss=6.755, ppl=107.98, wps=5843.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4300, lr=0.000482243, gnorm=0.556, train_wall=502, gb_free=6.1, wall=23551
2022-01-31 15:29:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:29:56 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.527 | ppl 737.87 | wps 7965.2 | wpb 2034.1 | bsz 4 | num_updates 4352 | best_loss 9.246
2022-01-31 15:29:56 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-01-31 15:29:56 | INFO | train | epoch 068 | loss 6.707 | ppl 104.48 | wps 5955.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4352 | lr 0.000479353 | gnorm 0.562 | train_wall 322 | gb_free 6.1 | wall 23840
KL Stats: Epoch 68 Divergences: Uniform: 2.769761139056548 Unigram: 2.826800064674442
2022-01-31 15:29:56 | INFO | fairseq.trainer | begin training epoch 69
2022-01-31 15:29:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:34:00 | INFO | train_inner | epoch 069:     48 / 64 loss=6.689, ppl=103.2, wps=6125.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4400, lr=0.000476731, gnorm=0.555, train_wall=505, gb_free=6.1, wall=24084
2022-01-31 15:35:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:35:47 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.544 | ppl 746.61 | wps 7958.7 | wpb 2034.1 | bsz 4 | num_updates 4416 | best_loss 9.246
2022-01-31 15:35:47 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-01-31 15:35:47 | INFO | train | epoch 069 | loss 6.67 | ppl 101.83 | wps 5950.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4416 | lr 0.000475867 | gnorm 0.556 | train_wall 323 | gb_free 6.1 | wall 24191
KL Stats: Epoch 69 Divergences: Uniform: 2.778441064473104 Unigram: 2.8393852453574677
2022-01-31 15:35:47 | INFO | fairseq.trainer | begin training epoch 70
2022-01-31 15:35:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:41:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:41:38 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.603 | ppl 777.69 | wps 7963.3 | wpb 2034.1 | bsz 4 | num_updates 4480 | best_loss 9.246
2022-01-31 15:41:38 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-01-31 15:41:38 | INFO | train | epoch 070 | loss 6.635 | ppl 99.37 | wps 5954.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4480 | lr 0.000472456 | gnorm 0.554 | train_wall 322 | gb_free 6.1 | wall 24542
KL Stats: Epoch 70 Divergences: Uniform: 2.7869951213882387 Unigram: 2.847321428408159
2022-01-31 15:41:38 | INFO | fairseq.trainer | begin training epoch 71
2022-01-31 15:41:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:43:19 | INFO | train_inner | epoch 071:     20 / 64 loss=6.63, ppl=99.02, wps=5825.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4500, lr=0.000471405, gnorm=0.562, train_wall=503, gb_free=6.1, wall=24644
2022-01-31 15:47:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:47:29 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.576 | ppl 763.45 | wps 7984 | wpb 2034.1 | bsz 4 | num_updates 4544 | best_loss 9.246
2022-01-31 15:47:29 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-01-31 15:47:29 | INFO | train | epoch 071 | loss 6.602 | ppl 97.14 | wps 5944.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4544 | lr 0.000469117 | gnorm 0.576 | train_wall 323 | gb_free 6.1 | wall 24893
KL Stats: Epoch 71 Divergences: Uniform: 2.7953830080671516 Unigram: 2.872068287899798
2022-01-31 15:47:29 | INFO | fairseq.trainer | begin training epoch 72
2022-01-31 15:47:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:52:13 | INFO | train_inner | epoch 072:     56 / 64 loss=6.586, ppl=96.07, wps=6119.5, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4600, lr=0.000466252, gnorm=0.568, train_wall=505, gb_free=6.1, wall=25178
2022-01-31 15:52:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:53:20 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.558 | ppl 753.89 | wps 7960.3 | wpb 2034.1 | bsz 4 | num_updates 4608 | best_loss 9.246
2022-01-31 15:53:20 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-01-31 15:53:20 | INFO | train | epoch 072 | loss 6.566 | ppl 94.72 | wps 5949.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4608 | lr 0.000465847 | gnorm 0.562 | train_wall 323 | gb_free 6.1 | wall 25244
KL Stats: Epoch 72 Divergences: Uniform: 2.8144443693343026 Unigram: 2.887326286995723
2022-01-31 15:53:20 | INFO | fairseq.trainer | begin training epoch 73
2022-01-31 15:53:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 15:58:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 15:59:11 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.631 | ppl 792.77 | wps 7984 | wpb 2034.1 | bsz 4 | num_updates 4672 | best_loss 9.246
2022-01-31 15:59:11 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-01-31 15:59:11 | INFO | train | epoch 073 | loss 6.534 | ppl 92.67 | wps 5949.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4672 | lr 0.000462646 | gnorm 0.565 | train_wall 323 | gb_free 6.1 | wall 25595
KL Stats: Epoch 73 Divergences: Uniform: 2.8112855059170374 Unigram: 2.9000206296305313
2022-01-31 15:59:11 | INFO | fairseq.trainer | begin training epoch 74
2022-01-31 15:59:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:01:33 | INFO | train_inner | epoch 074:     28 / 64 loss=6.523, ppl=91.95, wps=5821.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4700, lr=0.000461266, gnorm=0.565, train_wall=504, gb_free=6.1, wall=25738
2022-01-31 16:04:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:05:02 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.583 | ppl 766.81 | wps 7991.3 | wpb 2034.1 | bsz 4 | num_updates 4736 | best_loss 9.246
2022-01-31 16:05:02 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-01-31 16:05:02 | INFO | train | epoch 074 | loss 6.501 | ppl 90.57 | wps 5953.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4736 | lr 0.000459509 | gnorm 0.568 | train_wall 323 | gb_free 6.1 | wall 25946
KL Stats: Epoch 74 Divergences: Uniform: 2.8285110012669064 Unigram: 2.9194100935936396
2022-01-31 16:05:02 | INFO | fairseq.trainer | begin training epoch 75
2022-01-31 16:05:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:10:26 | INFO | train_inner | epoch 075:     64 / 64 loss=6.492, ppl=90.01, wps=6118.5, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=4800, lr=0.000456435, gnorm=0.575, train_wall=504, gb_free=6.1, wall=26270
2022-01-31 16:10:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:10:54 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.724 | ppl 845.67 | wps 7969.9 | wpb 2034.1 | bsz 4 | num_updates 4800 | best_loss 9.246
2022-01-31 16:10:54 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-01-31 16:10:54 | INFO | train | epoch 075 | loss 6.472 | ppl 88.76 | wps 5941.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4800 | lr 0.000456435 | gnorm 0.576 | train_wall 323 | gb_free 6.1 | wall 26298
KL Stats: Epoch 75 Divergences: Uniform: 2.8262790088594305 Unigram: 2.9311146187983352
2022-01-31 16:10:54 | INFO | fairseq.trainer | begin training epoch 76
2022-01-31 16:10:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:16:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:16:44 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 9.696 | ppl 829.72 | wps 7988.5 | wpb 2034.1 | bsz 4 | num_updates 4864 | best_loss 9.246
2022-01-31 16:16:44 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-01-31 16:16:44 | INFO | train | epoch 076 | loss 6.442 | ppl 86.97 | wps 5953.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4864 | lr 0.000453423 | gnorm 0.584 | train_wall 323 | gb_free 6.1 | wall 26649
KL Stats: Epoch 76 Divergences: Uniform: 2.839954368072354 Unigram: 2.951171327049977
2022-01-31 16:16:44 | INFO | fairseq.trainer | begin training epoch 77
2022-01-31 16:16:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:19:47 | INFO | train_inner | epoch 077:     36 / 64 loss=6.417, ppl=85.47, wps=5824.9, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=4900, lr=0.000451754, gnorm=0.585, train_wall=505, gb_free=6.1, wall=26831
2022-01-31 16:22:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:22:35 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 9.683 | ppl 822.03 | wps 7986.9 | wpb 2034.1 | bsz 4 | num_updates 4928 | best_loss 9.246
2022-01-31 16:22:35 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-01-31 16:22:35 | INFO | train | epoch 077 | loss 6.413 | ppl 85.24 | wps 5954.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4928 | lr 0.000450469 | gnorm 0.593 | train_wall 322 | gb_free 6.1 | wall 26999
KL Stats: Epoch 77 Divergences: Uniform: 2.8446341214897473 Unigram: 2.97305975360337
2022-01-31 16:22:35 | INFO | fairseq.trainer | begin training epoch 78
2022-01-31 16:22:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:27:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:28:26 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 9.665 | ppl 811.8 | wps 7954.1 | wpb 2034.1 | bsz 4 | num_updates 4992 | best_loss 9.246
2022-01-31 16:28:26 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-01-31 16:28:26 | INFO | train | epoch 078 | loss 6.385 | ppl 83.55 | wps 5954.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4992 | lr 0.000447572 | gnorm 0.595 | train_wall 322 | gb_free 6.1 | wall 27350
KL Stats: Epoch 78 Divergences: Uniform: 2.8578261340717397 Unigram: 2.981098338227456
2022-01-31 16:28:26 | INFO | fairseq.trainer | begin training epoch 79
2022-01-31 16:28:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:29:07 | INFO | train_inner | epoch 079:      8 / 64 loss=6.4, ppl=84.44, wps=5829.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=5000, lr=0.000447214, gnorm=0.597, train_wall=503, gb_free=6.1, wall=27391
2022-01-31 16:33:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:34:17 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 9.737 | ppl 853.54 | wps 7988.4 | wpb 2034.1 | bsz 4 | num_updates 5056 | best_loss 9.246
2022-01-31 16:34:17 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-01-31 16:34:17 | INFO | train | epoch 079 | loss 6.354 | ppl 81.82 | wps 5950.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5056 | lr 0.00044473 | gnorm 0.584 | train_wall 323 | gb_free 6.1 | wall 27701
KL Stats: Epoch 79 Divergences: Uniform: 2.8578794912608316 Unigram: 2.988907619100436
2022-01-31 16:34:17 | INFO | fairseq.trainer | begin training epoch 80
2022-01-31 16:34:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:38:00 | INFO | train_inner | epoch 080:     44 / 64 loss=6.338, ppl=80.89, wps=6122.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5100, lr=0.000442807, gnorm=0.583, train_wall=505, gb_free=6.1, wall=27924
2022-01-31 16:39:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:40:07 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 9.67 | ppl 814.8 | wps 8078 | wpb 2034.1 | bsz 4 | num_updates 5120 | best_loss 9.246
2022-01-31 16:40:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 5120 updates
2022-01-31 16:40:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint80.pt
2022-01-31 16:40:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint80.pt
2022-01-31 16:40:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint80.pt (epoch 80 @ 5120 updates, score 9.67) (writing took 3.525436796247959 seconds)
2022-01-31 16:40:10 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-01-31 16:40:10 | INFO | train | epoch 080 | loss 6.329 | ppl 80.4 | wps 5908.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5120 | lr 0.000441942 | gnorm 0.595 | train_wall 322 | gb_free 6.1 | wall 28055
KL Stats: Epoch 80 Divergences: Uniform: 2.8694258485430884 Unigram: 3.003587363404377
2022-01-31 16:40:11 | INFO | fairseq.trainer | begin training epoch 81
2022-01-31 16:40:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:45:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:45:59 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 9.679 | ppl 819.77 | wps 8081.8 | wpb 2034.1 | bsz 4 | num_updates 5184 | best_loss 9.246
2022-01-31 16:45:59 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-01-31 16:45:59 | INFO | train | epoch 081 | loss 6.303 | ppl 78.95 | wps 6001.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5184 | lr 0.000439205 | gnorm 0.606 | train_wall 320 | gb_free 6.1 | wall 28403
KL Stats: Epoch 81 Divergences: Uniform: 2.890615850356331 Unigram: 3.0291461778438413
2022-01-31 16:45:59 | INFO | fairseq.trainer | begin training epoch 82
2022-01-31 16:45:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:47:20 | INFO | train_inner | epoch 082:     16 / 64 loss=6.31, ppl=79.32, wps=5828.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5200, lr=0.000438529, gnorm=0.61, train_wall=500, gb_free=6.1, wall=28484
2022-01-31 16:51:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:51:49 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.682 | ppl 821.24 | wps 8005.9 | wpb 2034.1 | bsz 4 | num_updates 5248 | best_loss 9.246
2022-01-31 16:51:49 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-01-31 16:51:49 | INFO | train | epoch 082 | loss 6.278 | ppl 77.61 | wps 5958.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5248 | lr 0.000436519 | gnorm 0.608 | train_wall 322 | gb_free 6.1 | wall 28753
KL Stats: Epoch 82 Divergences: Uniform: 2.8985036248055978 Unigram: 3.0428841215633997
2022-01-31 16:51:49 | INFO | fairseq.trainer | begin training epoch 83
2022-01-31 16:51:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 16:56:12 | INFO | train_inner | epoch 083:     52 / 64 loss=6.264, ppl=76.86, wps=6136.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5300, lr=0.000434372, gnorm=0.612, train_wall=504, gb_free=6.1, wall=29016
2022-01-31 16:57:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 16:57:39 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 9.62 | ppl 787.11 | wps 7990.6 | wpb 2034.1 | bsz 4 | num_updates 5312 | best_loss 9.246
2022-01-31 16:57:39 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-01-31 16:57:39 | INFO | train | epoch 083 | loss 6.254 | ppl 76.32 | wps 5966 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5312 | lr 0.000433881 | gnorm 0.618 | train_wall 322 | gb_free 6.1 | wall 29103
KL Stats: Epoch 83 Divergences: Uniform: 2.90211026498174 Unigram: 3.0555249795985655
2022-01-31 16:57:39 | INFO | fairseq.trainer | begin training epoch 84
2022-01-31 16:57:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:03:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:03:30 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 9.696 | ppl 829.53 | wps 7999.2 | wpb 2034.1 | bsz 4 | num_updates 5376 | best_loss 9.246
2022-01-31 17:03:30 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-01-31 17:03:30 | INFO | train | epoch 084 | loss 6.228 | ppl 74.94 | wps 5956.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5376 | lr 0.000431291 | gnorm 0.602 | train_wall 322 | gb_free 6.1 | wall 29454
KL Stats: Epoch 84 Divergences: Uniform: 2.90920185862887 Unigram: 3.0673680220822783
2022-01-31 17:03:30 | INFO | fairseq.trainer | begin training epoch 85
2022-01-31 17:03:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:05:31 | INFO | train_inner | epoch 085:     24 / 64 loss=6.217, ppl=74.41, wps=5830.5, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5400, lr=0.000430331, gnorm=0.606, train_wall=503, gb_free=6.1, wall=29575
2022-01-31 17:08:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:09:20 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 9.713 | ppl 839.5 | wps 8023.7 | wpb 2034.1 | bsz 4 | num_updates 5440 | best_loss 9.246
2022-01-31 17:09:20 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-01-31 17:09:20 | INFO | train | epoch 085 | loss 6.204 | ppl 73.74 | wps 5961 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5440 | lr 0.000428746 | gnorm 0.614 | train_wall 322 | gb_free 6.1 | wall 29804
KL Stats: Epoch 85 Divergences: Uniform: 2.922017007841304 Unigram: 3.0761343846572964
2022-01-31 17:09:20 | INFO | fairseq.trainer | begin training epoch 86
2022-01-31 17:09:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:14:24 | INFO | train_inner | epoch 086:     60 / 64 loss=6.201, ppl=73.59, wps=6133.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5500, lr=0.000426401, gnorm=0.615, train_wall=504, gb_free=6.1, wall=30108
2022-01-31 17:14:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:15:11 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 9.732 | ppl 850.35 | wps 7912.2 | wpb 2034.1 | bsz 4 | num_updates 5504 | best_loss 9.246
2022-01-31 17:15:11 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-01-31 17:15:11 | INFO | train | epoch 086 | loss 6.18 | ppl 72.51 | wps 5956.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5504 | lr 0.000426246 | gnorm 0.617 | train_wall 322 | gb_free 6.1 | wall 30155
KL Stats: Epoch 86 Divergences: Uniform: 2.919603231373333 Unigram: 3.0963559125298348
2022-01-31 17:15:11 | INFO | fairseq.trainer | begin training epoch 87
2022-01-31 17:15:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:20:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:21:02 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 9.739 | ppl 854.44 | wps 7973.8 | wpb 2034.1 | bsz 4 | num_updates 5568 | best_loss 9.246
2022-01-31 17:21:02 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-01-31 17:21:02 | INFO | train | epoch 087 | loss 6.159 | ppl 71.45 | wps 5951.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5568 | lr 0.00042379 | gnorm 0.638 | train_wall 323 | gb_free 6.1 | wall 30506
KL Stats: Epoch 87 Divergences: Uniform: 2.9265122665493317 Unigram: 3.109895170713815
2022-01-31 17:21:02 | INFO | fairseq.trainer | begin training epoch 88
2022-01-31 17:21:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:23:44 | INFO | train_inner | epoch 088:     32 / 64 loss=6.146, ppl=70.81, wps=5821.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=5600, lr=0.000422577, gnorm=0.635, train_wall=503, gb_free=6.1, wall=30668
2022-01-31 17:26:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:26:53 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 9.717 | ppl 841.33 | wps 8006.9 | wpb 2034.1 | bsz 4 | num_updates 5632 | best_loss 9.246
2022-01-31 17:26:53 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-01-31 17:26:53 | INFO | train | epoch 088 | loss 6.137 | ppl 70.36 | wps 5954.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5632 | lr 0.000421375 | gnorm 0.635 | train_wall 323 | gb_free 6.1 | wall 30857
KL Stats: Epoch 88 Divergences: Uniform: 2.9324901219320147 Unigram: 3.1185036241029347
2022-01-31 17:26:53 | INFO | fairseq.trainer | begin training epoch 89
2022-01-31 17:26:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:32:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:32:43 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 9.737 | ppl 853.06 | wps 7993.9 | wpb 2034.1 | bsz 4 | num_updates 5696 | best_loss 9.246
2022-01-31 17:32:43 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-01-31 17:32:43 | INFO | train | epoch 089 | loss 6.118 | ppl 69.44 | wps 5955.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5696 | lr 0.000419001 | gnorm 0.644 | train_wall 322 | gb_free 6.1 | wall 31207
KL Stats: Epoch 89 Divergences: Uniform: 2.9409127239645505 Unigram: 3.131990047438749
2022-01-31 17:32:43 | INFO | fairseq.trainer | begin training epoch 90
2022-01-31 17:32:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:33:04 | INFO | train_inner | epoch 090:      4 / 64 loss=6.128, ppl=69.96, wps=5826.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5700, lr=0.000418854, gnorm=0.644, train_wall=503, gb_free=6.1, wall=31228
2022-01-31 17:38:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:38:35 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 9.781 | ppl 879.89 | wps 7954.4 | wpb 2034.1 | bsz 4 | num_updates 5760 | best_loss 9.246
2022-01-31 17:38:35 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-01-31 17:38:35 | INFO | train | epoch 090 | loss 6.095 | ppl 68.38 | wps 5939.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5760 | lr 0.000416667 | gnorm 0.631 | train_wall 323 | gb_free 6.1 | wall 31559
KL Stats: Epoch 90 Divergences: Uniform: 2.9464852430002457 Unigram: 3.1402575321002795
2022-01-31 17:38:35 | INFO | fairseq.trainer | begin training epoch 91
2022-01-31 17:38:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:41:58 | INFO | train_inner | epoch 091:     40 / 64 loss=6.077, ppl=67.51, wps=6111.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5800, lr=0.000415227, gnorm=0.626, train_wall=506, gb_free=6.1, wall=31763
2022-01-31 17:43:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:44:26 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 9.804 | ppl 894 | wps 7972.9 | wpb 2034.1 | bsz 4 | num_updates 5824 | best_loss 9.246
2022-01-31 17:44:26 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-01-31 17:44:26 | INFO | train | epoch 091 | loss 6.073 | ppl 67.33 | wps 5949.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5824 | lr 0.000414371 | gnorm 0.633 | train_wall 323 | gb_free 6.1 | wall 31910
KL Stats: Epoch 91 Divergences: Uniform: 2.9541651263958713 Unigram: 3.162081193140862
2022-01-31 17:44:26 | INFO | fairseq.trainer | begin training epoch 92
2022-01-31 17:44:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:49:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:50:17 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 9.805 | ppl 894.32 | wps 8021.2 | wpb 2034.1 | bsz 4 | num_updates 5888 | best_loss 9.246
2022-01-31 17:50:17 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-01-31 17:50:17 | INFO | train | epoch 092 | loss 6.054 | ppl 66.45 | wps 5947.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5888 | lr 0.000412113 | gnorm 0.665 | train_wall 323 | gb_free 6.1 | wall 32261
KL Stats: Epoch 92 Divergences: Uniform: 2.9608603573565055 Unigram: 3.1701059523445734
2022-01-31 17:50:17 | INFO | fairseq.trainer | begin training epoch 93
2022-01-31 17:50:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 17:51:18 | INFO | train_inner | epoch 093:     12 / 64 loss=6.063, ppl=66.88, wps=5824.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5900, lr=0.000411693, gnorm=0.656, train_wall=504, gb_free=6.1, wall=32322
2022-01-31 17:55:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 17:56:08 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 9.788 | ppl 884.31 | wps 7970.1 | wpb 2034.1 | bsz 4 | num_updates 5952 | best_loss 9.246
2022-01-31 17:56:08 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-01-31 17:56:08 | INFO | train | epoch 093 | loss 6.036 | ppl 65.63 | wps 5954.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5952 | lr 0.000409891 | gnorm 0.649 | train_wall 322 | gb_free 6.1 | wall 32612
KL Stats: Epoch 93 Divergences: Uniform: 2.9630128706607404 Unigram: 3.1805304436486823
2022-01-31 17:56:08 | INFO | fairseq.trainer | begin training epoch 94
2022-01-31 17:56:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:00:10 | INFO | train_inner | epoch 094:     48 / 64 loss=6.022, ppl=64.99, wps=6149.7, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=6000, lr=0.000408248, gnorm=0.656, train_wall=503, gb_free=6.1, wall=32854
2022-01-31 18:01:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:01:56 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 9.828 | ppl 908.81 | wps 8041.7 | wpb 2034.1 | bsz 4 | num_updates 6016 | best_loss 9.246
2022-01-31 18:01:56 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-01-31 18:01:56 | INFO | train | epoch 094 | loss 6.015 | ppl 64.66 | wps 5998.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6016 | lr 0.000407705 | gnorm 0.658 | train_wall 320 | gb_free 6.1 | wall 32960
KL Stats: Epoch 94 Divergences: Uniform: 2.966390253118658 Unigram: 3.198641726878607
2022-01-31 18:01:56 | INFO | fairseq.trainer | begin training epoch 95
2022-01-31 18:01:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:07:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:07:44 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 9.896 | ppl 952.69 | wps 8153.3 | wpb 2034.1 | bsz 4 | num_updates 6080 | best_loss 9.246
2022-01-31 18:07:44 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-01-31 18:07:44 | INFO | train | epoch 095 | loss 5.996 | ppl 63.84 | wps 6009.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6080 | lr 0.000405554 | gnorm 0.648 | train_wall 320 | gb_free 6.1 | wall 33308
KL Stats: Epoch 95 Divergences: Uniform: 2.9713336326152127 Unigram: 3.2089014736959474
2022-01-31 18:07:44 | INFO | fairseq.trainer | begin training epoch 96
2022-01-31 18:07:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:09:24 | INFO | train_inner | epoch 096:     20 / 64 loss=5.995, ppl=63.79, wps=5880.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6100, lr=0.000404888, gnorm=0.654, train_wall=499, gb_free=6.1, wall=33408
2022-01-31 18:13:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:13:30 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 9.799 | ppl 890.73 | wps 8106.2 | wpb 2034.1 | bsz 4 | num_updates 6144 | best_loss 9.246
2022-01-31 18:13:30 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-01-31 18:13:30 | INFO | train | epoch 096 | loss 5.98 | ppl 63.13 | wps 6028.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6144 | lr 0.000403436 | gnorm 0.673 | train_wall 319 | gb_free 6.1 | wall 33654
KL Stats: Epoch 96 Divergences: Uniform: 2.981195968783701 Unigram: 3.2182568683658133
2022-01-31 18:13:30 | INFO | fairseq.trainer | begin training epoch 97
2022-01-31 18:13:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:18:13 | INFO | train_inner | epoch 097:     56 / 64 loss=5.975, ppl=62.88, wps=6178.5, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=6200, lr=0.00040161, gnorm=0.683, train_wall=501, gb_free=6.1, wall=33937
2022-01-31 18:18:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:19:19 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 9.845 | ppl 919.52 | wps 8024.1 | wpb 2034.1 | bsz 4 | num_updates 6208 | best_loss 9.246
2022-01-31 18:19:19 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-01-31 18:19:19 | INFO | train | epoch 097 | loss 5.963 | ppl 62.37 | wps 5985.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6208 | lr 0.000401351 | gnorm 0.684 | train_wall 321 | gb_free 6.1 | wall 34003
KL Stats: Epoch 97 Divergences: Uniform: 2.9905779922585247 Unigram: 3.23319567323948
2022-01-31 18:19:19 | INFO | fairseq.trainer | begin training epoch 98
2022-01-31 18:19:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:24:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:25:08 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 9.856 | ppl 926.53 | wps 8047.8 | wpb 2034.1 | bsz 4 | num_updates 6272 | best_loss 9.246
2022-01-31 18:25:08 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-01-31 18:25:08 | INFO | train | epoch 098 | loss 5.942 | ppl 61.49 | wps 5987.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6272 | lr 0.000399298 | gnorm 0.674 | train_wall 321 | gb_free 6.1 | wall 34352
KL Stats: Epoch 98 Divergences: Uniform: 2.986913658490035 Unigram: 3.2348868696625055
2022-01-31 18:25:08 | INFO | fairseq.trainer | begin training epoch 99
2022-01-31 18:25:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:27:29 | INFO | train_inner | epoch 099:     28 / 64 loss=5.933, ppl=61.1, wps=5857, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=6300, lr=0.00039841, gnorm=0.674, train_wall=501, gb_free=6.1, wall=34494
2022-01-31 18:30:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:30:58 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 9.821 | ppl 904.67 | wps 7969 | wpb 2034.1 | bsz 4 | num_updates 6336 | best_loss 9.246
2022-01-31 18:30:58 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-01-31 18:30:58 | INFO | train | epoch 099 | loss 5.925 | ppl 60.74 | wps 5972.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6336 | lr 0.000397276 | gnorm 0.687 | train_wall 321 | gb_free 6.1 | wall 34702
KL Stats: Epoch 99 Divergences: Uniform: 2.9997902692184413 Unigram: 3.2533463750264406
2022-01-31 18:30:58 | INFO | fairseq.trainer | begin training epoch 100
2022-01-31 18:30:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:36:21 | INFO | train_inner | epoch 100:     64 / 64 loss=5.928, ppl=60.89, wps=6132.7, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=6400, lr=0.000395285, gnorm=0.694, train_wall=503, gb_free=6.1, wall=35025
2022-01-31 18:36:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:36:48 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 9.791 | ppl 886.05 | wps 7984.1 | wpb 2034.1 | bsz 4 | num_updates 6400 | best_loss 9.246
2022-01-31 18:36:48 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-01-31 18:36:48 | INFO | train | epoch 100 | loss 5.911 | ppl 60.17 | wps 5955.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6400 | lr 0.000395285 | gnorm 0.691 | train_wall 322 | gb_free 6.1 | wall 35053
KL Stats: Epoch 100 Divergences: Uniform: 2.9996437611603177 Unigram: 3.258376529573389
2022-01-31 18:36:48 | INFO | fairseq.trainer | begin training epoch 101
2022-01-31 18:36:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:42:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:42:39 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 9.87 | ppl 935.89 | wps 7937.6 | wpb 2034.1 | bsz 4 | num_updates 6464 | best_loss 9.246
2022-01-31 18:42:39 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-01-31 18:42:39 | INFO | train | epoch 101 | loss 5.891 | ppl 59.33 | wps 5950.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6464 | lr 0.000393323 | gnorm 0.691 | train_wall 323 | gb_free 6.1 | wall 35404
KL Stats: Epoch 101 Divergences: Uniform: 3.0136383977721546 Unigram: 3.2770153218991904
2022-01-31 18:42:39 | INFO | fairseq.trainer | begin training epoch 102
2022-01-31 18:42:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:45:43 | INFO | train_inner | epoch 102:     36 / 64 loss=5.877, ppl=58.77, wps=5819.4, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=6500, lr=0.000392232, gnorm=0.704, train_wall=505, gb_free=6.1, wall=35587
2022-01-31 18:48:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:48:31 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 9.869 | ppl 935.37 | wps 7920.6 | wpb 2034.1 | bsz 4 | num_updates 6528 | best_loss 9.246
2022-01-31 18:48:31 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-01-31 18:48:31 | INFO | train | epoch 102 | loss 5.877 | ppl 58.79 | wps 5932.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6528 | lr 0.00039139 | gnorm 0.713 | train_wall 324 | gb_free 6.1 | wall 35756
KL Stats: Epoch 102 Divergences: Uniform: 3.0074707747458143 Unigram: 3.285721747969396
2022-01-31 18:48:31 | INFO | fairseq.trainer | begin training epoch 103
2022-01-31 18:48:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:53:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 18:54:23 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 9.877 | ppl 940.37 | wps 8022.2 | wpb 2034.1 | bsz 4 | num_updates 6592 | best_loss 9.246
2022-01-31 18:54:23 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-01-31 18:54:23 | INFO | train | epoch 103 | loss 5.86 | ppl 58.1 | wps 5948.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6592 | lr 0.000389486 | gnorm 0.699 | train_wall 323 | gb_free 6.1 | wall 36107
KL Stats: Epoch 103 Divergences: Uniform: 3.019424094658558 Unigram: 3.2963978197645516
2022-01-31 18:54:23 | INFO | fairseq.trainer | begin training epoch 104
2022-01-31 18:54:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 18:55:03 | INFO | train_inner | epoch 104:      8 / 64 loss=5.868, ppl=58.39, wps=5817.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6600, lr=0.000389249, gnorm=0.699, train_wall=504, gb_free=6.1, wall=36147
2022-01-31 18:59:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:00:10 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 9.893 | ppl 950.83 | wps 8100.8 | wpb 2034.1 | bsz 4 | num_updates 6656 | best_loss 9.246
2022-01-31 19:00:10 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-01-31 19:00:10 | INFO | train | epoch 104 | loss 5.847 | ppl 57.56 | wps 6016.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6656 | lr 0.000387609 | gnorm 0.72 | train_wall 319 | gb_free 6.1 | wall 36454
KL Stats: Epoch 104 Divergences: Uniform: 3.0147308982640864 Unigram: 3.309288526290372
2022-01-31 19:00:10 | INFO | fairseq.trainer | begin training epoch 105
2022-01-31 19:00:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:03:51 | INFO | train_inner | epoch 105:     44 / 64 loss=5.835, ppl=57.07, wps=6188.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=6700, lr=0.000386334, gnorm=0.72, train_wall=500, gb_free=6.1, wall=36675
2022-01-31 19:05:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:05:57 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 9.835 | ppl 913.61 | wps 8092.3 | wpb 2034.1 | bsz 4 | num_updates 6720 | best_loss 9.246
2022-01-31 19:05:57 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-01-31 19:05:57 | INFO | train | epoch 105 | loss 5.831 | ppl 56.91 | wps 6010.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6720 | lr 0.000385758 | gnorm 0.717 | train_wall 320 | gb_free 6.1 | wall 36801
KL Stats: Epoch 105 Divergences: Uniform: 3.02241235725413 Unigram: 3.313443933547046
2022-01-31 19:05:57 | INFO | fairseq.trainer | begin training epoch 106
2022-01-31 19:05:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:11:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:11:44 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 9.876 | ppl 939.47 | wps 8103.1 | wpb 2034.1 | bsz 4 | num_updates 6784 | best_loss 9.246
2022-01-31 19:11:44 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-01-31 19:11:44 | INFO | train | epoch 106 | loss 5.815 | ppl 56.29 | wps 6019.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6784 | lr 0.000383934 | gnorm 0.72 | train_wall 319 | gb_free 6.1 | wall 37148
KL Stats: Epoch 106 Divergences: Uniform: 3.02000442061274 Unigram: 3.3225626371936485
2022-01-31 19:11:44 | INFO | fairseq.trainer | begin training epoch 107
2022-01-31 19:11:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:13:04 | INFO | train_inner | epoch 107:     16 / 64 loss=5.818, ppl=56.42, wps=5889.3, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6800, lr=0.000383482, gnorm=0.717, train_wall=498, gb_free=6.1, wall=37229
2022-01-31 19:17:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:17:32 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 9.881 | ppl 943.24 | wps 8057.6 | wpb 2034.1 | bsz 4 | num_updates 6848 | best_loss 9.246
2022-01-31 19:17:32 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-01-31 19:17:32 | INFO | train | epoch 107 | loss 5.799 | ppl 55.67 | wps 6010.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6848 | lr 0.000382136 | gnorm 0.709 | train_wall 319 | gb_free 6.1 | wall 37496
KL Stats: Epoch 107 Divergences: Uniform: 3.0309695071556066 Unigram: 3.338660753625158
2022-01-31 19:17:32 | INFO | fairseq.trainer | begin training epoch 108
2022-01-31 19:17:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:21:54 | INFO | train_inner | epoch 108:     52 / 64 loss=5.795, ppl=55.5, wps=6174.5, ups=0.19, wpb=32686.1, bsz=63.8, num_updates=6900, lr=0.000380693, gnorm=0.721, train_wall=501, gb_free=6.1, wall=37758
2022-01-31 19:22:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:23:20 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 9.919 | ppl 968.25 | wps 8058.8 | wpb 2034.1 | bsz 4 | num_updates 6912 | best_loss 9.246
2022-01-31 19:23:20 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-01-31 19:23:20 | INFO | train | epoch 108 | loss 5.788 | ppl 55.26 | wps 5994.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6912 | lr 0.000380363 | gnorm 0.737 | train_wall 320 | gb_free 6.1 | wall 37844
KL Stats: Epoch 108 Divergences: Uniform: 3.03347689154092 Unigram: 3.3434450013659767
2022-01-31 19:23:20 | INFO | fairseq.trainer | begin training epoch 109
2022-01-31 19:23:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:28:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:29:08 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 9.946 | ppl 986.28 | wps 8033.1 | wpb 2034.1 | bsz 4 | num_updates 6976 | best_loss 9.246
2022-01-31 19:29:08 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-01-31 19:29:08 | INFO | train | epoch 109 | loss 5.773 | ppl 54.68 | wps 6001.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6976 | lr 0.000378614 | gnorm 0.73 | train_wall 320 | gb_free 6.1 | wall 38192
KL Stats: Epoch 109 Divergences: Uniform: 3.0385399153642108 Unigram: 3.3572704321111617
2022-01-31 19:29:08 | INFO | fairseq.trainer | begin training epoch 110
2022-01-31 19:29:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:31:09 | INFO | train_inner | epoch 110:     24 / 64 loss=5.767, ppl=54.47, wps=5872.3, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=7000, lr=0.000377964, gnorm=0.738, train_wall=499, gb_free=6.1, wall=38313
2022-01-31 19:34:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:34:54 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 9.966 | ppl 1000.12 | wps 8162.8 | wpb 2034.1 | bsz 4 | num_updates 7040 | best_loss 9.246
2022-01-31 19:34:54 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-01-31 19:34:54 | INFO | train | epoch 110 | loss 5.759 | ppl 54.17 | wps 6031.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7040 | lr 0.000376889 | gnorm 0.734 | train_wall 319 | gb_free 6.1 | wall 38539
KL Stats: Epoch 110 Divergences: Uniform: 3.0451966794501186 Unigram: 3.3682384829599488
2022-01-31 19:34:54 | INFO | fairseq.trainer | begin training epoch 111
2022-01-31 19:34:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:39:56 | INFO | train_inner | epoch 111:     60 / 64 loss=5.759, ppl=54.15, wps=6201.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7100, lr=0.000375293, gnorm=0.744, train_wall=499, gb_free=6.1, wall=38840
2022-01-31 19:40:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:40:42 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 9.937 | ppl 980.21 | wps 8045 | wpb 2034.1 | bsz 4 | num_updates 7104 | best_loss 9.246
2022-01-31 19:40:42 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-01-31 19:40:42 | INFO | train | epoch 111 | loss 5.747 | ppl 53.69 | wps 6011.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7104 | lr 0.000375188 | gnorm 0.758 | train_wall 319 | gb_free 6.1 | wall 38886
KL Stats: Epoch 111 Divergences: Uniform: 3.0494284835117846 Unigram: 3.3816936833034132
2022-01-31 19:40:42 | INFO | fairseq.trainer | begin training epoch 112
2022-01-31 19:40:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:46:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:46:34 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 9.938 | ppl 980.67 | wps 7954.4 | wpb 2034.1 | bsz 4 | num_updates 7168 | best_loss 9.246
2022-01-31 19:46:34 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-01-31 19:46:34 | INFO | train | epoch 112 | loss 5.733 | ppl 53.17 | wps 5940 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7168 | lr 0.000373509 | gnorm 0.747 | train_wall 323 | gb_free 6.1 | wall 39238
KL Stats: Epoch 112 Divergences: Uniform: 3.0453683405096035 Unigram: 3.39058479212995
2022-01-31 19:46:34 | INFO | fairseq.trainer | begin training epoch 113
2022-01-31 19:46:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:49:17 | INFO | train_inner | epoch 113:     32 / 64 loss=5.721, ppl=52.74, wps=5812.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7200, lr=0.000372678, gnorm=0.748, train_wall=505, gb_free=6.1, wall=39401
2022-01-31 19:51:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:52:25 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 9.962 | ppl 997.22 | wps 7968.2 | wpb 2034.1 | bsz 4 | num_updates 7232 | best_loss 9.246
2022-01-31 19:52:25 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-01-31 19:52:25 | INFO | train | epoch 113 | loss 5.717 | ppl 52.6 | wps 5941.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7232 | lr 0.000371853 | gnorm 0.748 | train_wall 323 | gb_free 6.1 | wall 39589
KL Stats: Epoch 113 Divergences: Uniform: 3.0625494583627675 Unigram: 3.399464567237783
2022-01-31 19:52:25 | INFO | fairseq.trainer | begin training epoch 114
2022-01-31 19:52:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:57:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 19:58:16 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 9.954 | ppl 991.75 | wps 7959.8 | wpb 2034.1 | bsz 4 | num_updates 7296 | best_loss 9.246
2022-01-31 19:58:16 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-01-31 19:58:16 | INFO | train | epoch 114 | loss 5.705 | ppl 52.16 | wps 5945.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7296 | lr 0.000370218 | gnorm 0.752 | train_wall 323 | gb_free 6.1 | wall 39941
KL Stats: Epoch 114 Divergences: Uniform: 3.056717262340026 Unigram: 3.4063503651210483
2022-01-31 19:58:16 | INFO | fairseq.trainer | begin training epoch 115
2022-01-31 19:58:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 19:58:37 | INFO | train_inner | epoch 115:      4 / 64 loss=5.717, ppl=52.59, wps=5820.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=7300, lr=0.000370117, gnorm=0.753, train_wall=504, gb_free=6.1, wall=39961
2022-01-31 20:03:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:04:11 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 9.966 | ppl 999.98 | wps 7821.5 | wpb 2034.1 | bsz 4 | num_updates 7360 | best_loss 9.246
2022-01-31 20:04:11 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-01-31 20:04:11 | INFO | train | epoch 115 | loss 5.692 | ppl 51.71 | wps 5891.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7360 | lr 0.000368605 | gnorm 0.763 | train_wall 326 | gb_free 6.1 | wall 40295
KL Stats: Epoch 115 Divergences: Uniform: 3.062692138495786 Unigram: 3.413147659657322
2022-01-31 20:04:11 | INFO | fairseq.trainer | begin training epoch 116
2022-01-31 20:04:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:07:38 | INFO | train_inner | epoch 116:     40 / 64 loss=5.679, ppl=51.23, wps=6036.9, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=7400, lr=0.000367607, gnorm=0.759, train_wall=512, gb_free=6.1, wall=40502
2022-01-31 20:09:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:10:09 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 9.935 | ppl 979.11 | wps 7734 | wpb 2034.1 | bsz 4 | num_updates 7424 | best_loss 9.246
2022-01-31 20:10:09 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-01-31 20:10:09 | INFO | train | epoch 116 | loss 5.68 | ppl 51.27 | wps 5837.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7424 | lr 0.000367013 | gnorm 0.766 | train_wall 329 | gb_free 6.1 | wall 40653
KL Stats: Epoch 116 Divergences: Uniform: 3.064045496744629 Unigram: 3.4177973114856868
2022-01-31 20:10:09 | INFO | fairseq.trainer | begin training epoch 117
2022-01-31 20:10:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:15:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:16:06 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 10 | ppl 1024.18 | wps 7799.9 | wpb 2034.1 | bsz 4 | num_updates 7488 | best_loss 9.246
2022-01-31 20:16:06 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-01-31 20:16:06 | INFO | train | epoch 117 | loss 5.669 | ppl 50.88 | wps 5843.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7488 | lr 0.000365441 | gnorm 0.773 | train_wall 328 | gb_free 6.1 | wall 41010
KL Stats: Epoch 117 Divergences: Uniform: 3.0629582802983744 Unigram: 3.4318073903831756
2022-01-31 20:16:06 | INFO | fairseq.trainer | begin training epoch 118
2022-01-31 20:16:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:17:08 | INFO | train_inner | epoch 118:     12 / 64 loss=5.674, ppl=51.05, wps=5719.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7500, lr=0.000365148, gnorm=0.78, train_wall=512, gb_free=6.1, wall=41072
2022-01-31 20:21:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:22:04 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 9.986 | ppl 1014.15 | wps 7805.1 | wpb 2034.1 | bsz 4 | num_updates 7552 | best_loss 9.246
2022-01-31 20:22:04 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-01-31 20:22:04 | INFO | train | epoch 118 | loss 5.656 | ppl 50.42 | wps 5841.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7552 | lr 0.000363889 | gnorm 0.781 | train_wall 329 | gb_free 6.1 | wall 41368
KL Stats: Epoch 118 Divergences: Uniform: 3.0735088211665724 Unigram: 3.4409717074299833
2022-01-31 20:22:04 | INFO | fairseq.trainer | begin training epoch 119
2022-01-31 20:22:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:26:11 | INFO | train_inner | epoch 119:     48 / 64 loss=5.646, ppl=50.08, wps=6020.4, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=7600, lr=0.000362738, gnorm=0.775, train_wall=513, gb_free=6.1, wall=41615
2022-01-31 20:27:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:28:00 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 9.971 | ppl 1003.45 | wps 7836 | wpb 2034.1 | bsz 4 | num_updates 7616 | best_loss 9.246
2022-01-31 20:28:00 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-01-31 20:28:00 | INFO | train | epoch 119 | loss 5.644 | ppl 50.01 | wps 5859 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7616 | lr 0.000362357 | gnorm 0.783 | train_wall 328 | gb_free 6.1 | wall 41724
KL Stats: Epoch 119 Divergences: Uniform: 3.078349062351984 Unigram: 3.453722884694153
2022-01-31 20:28:00 | INFO | fairseq.trainer | begin training epoch 120
2022-01-31 20:28:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:33:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:33:57 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 9.984 | ppl 1012.61 | wps 7795.9 | wpb 2034.1 | bsz 4 | num_updates 7680 | best_loss 9.246
2022-01-31 20:33:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 7680 updates
2022-01-31 20:33:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint120.pt
2022-01-31 20:34:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint120.pt
2022-01-31 20:39:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint120.pt (epoch 120 @ 7680 updates, score 9.984) (writing took 357.7872103191912 seconds)
2022-01-31 20:39:55 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-01-31 20:39:55 | INFO | train | epoch 120 | loss 5.634 | ppl 49.67 | wps 2921.1 | ups 0.09 | wpb 32634.8 | bsz 63.8 | num_updates 7680 | lr 0.000360844 | gnorm 0.786 | train_wall 328 | gb_free 6.1 | wall 42439
KL Stats: Epoch 120 Divergences: Uniform: 3.0841357559825493 Unigram: 3.457134184284947
2022-01-31 20:39:55 | INFO | fairseq.trainer | begin training epoch 121
2022-01-31 20:39:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:41:38 | INFO | train_inner | epoch 121:     20 / 64 loss=5.635, ppl=49.71, wps=3516.5, ups=0.11, wpb=32600.8, bsz=63.7, num_updates=7700, lr=0.000360375, gnorm=0.785, train_wall=512, gb_free=6.1, wall=42542
2022-01-31 20:45:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:45:51 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 9.998 | ppl 1022.53 | wps 7865.3 | wpb 2034.1 | bsz 4 | num_updates 7744 | best_loss 9.246
2022-01-31 20:45:51 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-01-31 20:45:51 | INFO | train | epoch 121 | loss 5.621 | ppl 49.23 | wps 5870.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7744 | lr 0.00035935 | gnorm 0.772 | train_wall 327 | gb_free 6.1 | wall 42795
KL Stats: Epoch 121 Divergences: Uniform: 3.0837452827534886 Unigram: 3.4635553600202917
2022-01-31 20:45:51 | INFO | fairseq.trainer | begin training epoch 122
2022-01-31 20:45:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:50:40 | INFO | train_inner | epoch 122:     56 / 64 loss=5.619, ppl=49.16, wps=6034.4, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=7800, lr=0.000358057, gnorm=0.794, train_wall=512, gb_free=6.1, wall=43084
2022-01-31 20:51:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:51:47 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 9.95 | ppl 989.2 | wps 7856.7 | wpb 2034.1 | bsz 4 | num_updates 7808 | best_loss 9.246
2022-01-31 20:51:47 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-01-31 20:51:47 | INFO | train | epoch 122 | loss 5.612 | ppl 48.89 | wps 5864.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7808 | lr 0.000357874 | gnorm 0.81 | train_wall 327 | gb_free 6.1 | wall 43151
KL Stats: Epoch 122 Divergences: Uniform: 3.089669705495483 Unigram: 3.4775312468574886
2022-01-31 20:51:47 | INFO | fairseq.trainer | begin training epoch 123
2022-01-31 20:51:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 20:57:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 20:57:41 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 10.023 | ppl 1040.73 | wps 7930.5 | wpb 2034.1 | bsz 4 | num_updates 7872 | best_loss 9.246
2022-01-31 20:57:41 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-01-31 20:57:41 | INFO | train | epoch 123 | loss 5.6 | ppl 48.51 | wps 5899.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7872 | lr 0.000356416 | gnorm 0.811 | train_wall 326 | gb_free 6.1 | wall 43505
KL Stats: Epoch 123 Divergences: Uniform: 3.086775258018131 Unigram: 3.4861835337087306
2022-01-31 20:57:41 | INFO | fairseq.trainer | begin training epoch 124
2022-01-31 20:57:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:00:04 | INFO | train_inner | epoch 124:     28 / 64 loss=5.593, ppl=48.26, wps=5772.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7900, lr=0.000355784, gnorm=0.806, train_wall=508, gb_free=6.1, wall=43648
2022-01-31 21:03:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:03:35 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 10.005 | ppl 1027.41 | wps 7834.4 | wpb 2034.1 | bsz 4 | num_updates 7936 | best_loss 9.246
2022-01-31 21:03:35 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-01-31 21:03:35 | INFO | train | epoch 124 | loss 5.588 | ppl 48.1 | wps 5909.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7936 | lr 0.000354976 | gnorm 0.801 | train_wall 325 | gb_free 6.1 | wall 43859
KL Stats: Epoch 124 Divergences: Uniform: 3.076812669218142 Unigram: 3.4853046724545496
2022-01-31 21:03:35 | INFO | fairseq.trainer | begin training epoch 125
2022-01-31 21:03:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:09:03 | INFO | train_inner | epoch 125:     64 / 64 loss=5.591, ppl=48.21, wps=6051, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=8000, lr=0.000353553, gnorm=0.813, train_wall=509, gb_free=6.1, wall=44187
2022-01-31 21:09:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:09:31 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 10.049 | ppl 1059.55 | wps 7778 | wpb 2034.1 | bsz 4 | num_updates 8000 | best_loss 9.246
2022-01-31 21:09:31 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-01-31 21:09:31 | INFO | train | epoch 125 | loss 5.577 | ppl 47.75 | wps 5859.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8000 | lr 0.000353553 | gnorm 0.816 | train_wall 327 | gb_free 6.1 | wall 44215
KL Stats: Epoch 125 Divergences: Uniform: 3.090824957905586 Unigram: 3.4981979701553936
2022-01-31 21:09:31 | INFO | fairseq.trainer | begin training epoch 126
2022-01-31 21:09:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:15:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:15:28 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 9.973 | ppl 1005.3 | wps 7829.7 | wpb 2034.1 | bsz 4 | num_updates 8064 | best_loss 9.246
2022-01-31 21:15:28 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-01-31 21:15:28 | INFO | train | epoch 126 | loss 5.567 | ppl 47.4 | wps 5854.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8064 | lr 0.000352148 | gnorm 0.801 | train_wall 328 | gb_free 6.1 | wall 44572
KL Stats: Epoch 126 Divergences: Uniform: 3.097918617952891 Unigram: 3.5052430565178367
2022-01-31 21:15:28 | INFO | fairseq.trainer | begin training epoch 127
2022-01-31 21:15:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:18:33 | INFO | train_inner | epoch 127:     36 / 64 loss=5.555, ppl=47.02, wps=5728.9, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=8100, lr=0.000351364, gnorm=0.824, train_wall=513, gb_free=6.1, wall=44758
2022-01-31 21:20:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:21:24 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 10.003 | ppl 1026.13 | wps 7907.4 | wpb 2034.1 | bsz 4 | num_updates 8128 | best_loss 9.246
2022-01-31 21:21:24 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-01-31 21:21:24 | INFO | train | epoch 127 | loss 5.558 | ppl 47.11 | wps 5871.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8128 | lr 0.000350758 | gnorm 0.844 | train_wall 327 | gb_free 6.1 | wall 44928
KL Stats: Epoch 127 Divergences: Uniform: 3.0916964797194777 Unigram: 3.510811431430151
2022-01-31 21:21:24 | INFO | fairseq.trainer | begin training epoch 128
2022-01-31 21:21:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:26:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:27:20 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 10.019 | ppl 1037.89 | wps 7806.2 | wpb 2034.1 | bsz 4 | num_updates 8192 | best_loss 9.246
2022-01-31 21:27:20 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-01-31 21:27:20 | INFO | train | epoch 128 | loss 5.546 | ppl 46.72 | wps 5852 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8192 | lr 0.000349386 | gnorm 0.838 | train_wall 328 | gb_free 6.1 | wall 45285
KL Stats: Epoch 128 Divergences: Uniform: 3.0960420328301956 Unigram: 3.5200384903419724
2022-01-31 21:27:20 | INFO | fairseq.trainer | begin training epoch 129
2022-01-31 21:27:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:28:02 | INFO | train_inner | epoch 129:      8 / 64 loss=5.554, ppl=46.97, wps=5735.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8200, lr=0.000349215, gnorm=0.835, train_wall=511, gb_free=6.1, wall=45326
2022-01-31 21:32:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:33:17 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 10.013 | ppl 1033.11 | wps 7849.2 | wpb 2034.1 | bsz 4 | num_updates 8256 | best_loss 9.246
2022-01-31 21:33:17 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-01-31 21:33:17 | INFO | train | epoch 129 | loss 5.54 | ppl 46.52 | wps 5850.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8256 | lr 0.000348029 | gnorm 0.841 | train_wall 328 | gb_free 6.1 | wall 45642
KL Stats: Epoch 129 Divergences: Uniform: 3.0944421038435026 Unigram: 3.525352858777476
2022-01-31 21:33:17 | INFO | fairseq.trainer | begin training epoch 130
2022-01-31 21:33:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:37:04 | INFO | train_inner | epoch 130:     44 / 64 loss=5.527, ppl=46.11, wps=6024.7, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=8300, lr=0.000347105, gnorm=0.831, train_wall=513, gb_free=6.1, wall=45868
2022-01-31 21:38:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:39:14 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 10.087 | ppl 1087.68 | wps 7806.8 | wpb 2034.1 | bsz 4 | num_updates 8320 | best_loss 9.246
2022-01-31 21:39:14 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-01-31 21:39:14 | INFO | train | epoch 130 | loss 5.526 | ppl 46.08 | wps 5856.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8320 | lr 0.000346688 | gnorm 0.824 | train_wall 328 | gb_free 6.1 | wall 45998
KL Stats: Epoch 130 Divergences: Uniform: 3.096336222943654 Unigram: 3.5374585601207262
2022-01-31 21:39:14 | INFO | fairseq.trainer | begin training epoch 131
2022-01-31 21:39:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:44:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:45:11 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 10.055 | ppl 1063.81 | wps 7830.7 | wpb 2034.1 | bsz 4 | num_updates 8384 | best_loss 9.246
2022-01-31 21:45:11 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-01-31 21:45:11 | INFO | train | epoch 131 | loss 5.518 | ppl 45.83 | wps 5858.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8384 | lr 0.000345362 | gnorm 0.845 | train_wall 328 | gb_free 6.1 | wall 46355
KL Stats: Epoch 131 Divergences: Uniform: 3.096469563798697 Unigram: 3.539068610838557
2022-01-31 21:45:11 | INFO | fairseq.trainer | begin training epoch 132
2022-01-31 21:45:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:46:33 | INFO | train_inner | epoch 132:     16 / 64 loss=5.521, ppl=45.93, wps=5732.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=8400, lr=0.000345033, gnorm=0.847, train_wall=511, gb_free=6.1, wall=46437
2022-01-31 21:50:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:51:06 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 10.078 | ppl 1080.82 | wps 7840.2 | wpb 2034.1 | bsz 4 | num_updates 8448 | best_loss 9.246
2022-01-31 21:51:06 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-01-31 21:51:06 | INFO | train | epoch 132 | loss 5.509 | ppl 45.55 | wps 5870.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8448 | lr 0.000344051 | gnorm 0.876 | train_wall 327 | gb_free 6.1 | wall 46711
KL Stats: Epoch 132 Divergences: Uniform: 3.1037304387640754 Unigram: 3.5483974909919977
2022-01-31 21:51:06 | INFO | fairseq.trainer | begin training epoch 133
2022-01-31 21:51:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 21:55:35 | INFO | train_inner | epoch 133:     52 / 64 loss=5.503, ppl=45.35, wps=6031.2, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=8500, lr=0.000342997, gnorm=0.865, train_wall=513, gb_free=6.1, wall=46979
2022-01-31 21:56:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 21:57:03 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 10.048 | ppl 1058.94 | wps 7812.9 | wpb 2034.1 | bsz 4 | num_updates 8512 | best_loss 9.246
2022-01-31 21:57:03 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-01-31 21:57:03 | INFO | train | epoch 133 | loss 5.498 | ppl 45.19 | wps 5848.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8512 | lr 0.000342755 | gnorm 0.856 | train_wall 328 | gb_free 6.1 | wall 47068
KL Stats: Epoch 133 Divergences: Uniform: 3.114140632784319 Unigram: 3.5550237927392385
2022-01-31 21:57:03 | INFO | fairseq.trainer | begin training epoch 134
2022-01-31 21:57:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:02:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:03:01 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 10.132 | ppl 1122.15 | wps 7797.2 | wpb 2034.1 | bsz 4 | num_updates 8576 | best_loss 9.246
2022-01-31 22:03:01 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-01-31 22:03:01 | INFO | train | epoch 134 | loss 5.49 | ppl 44.95 | wps 5844.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8576 | lr 0.000341474 | gnorm 0.879 | train_wall 328 | gb_free 6.1 | wall 47425
KL Stats: Epoch 134 Divergences: Uniform: 3.1078433724863497 Unigram: 3.558811212929947
2022-01-31 22:03:01 | INFO | fairseq.trainer | begin training epoch 135
2022-01-31 22:03:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:05:05 | INFO | train_inner | epoch 135:     24 / 64 loss=5.489, ppl=44.92, wps=5720.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8600, lr=0.000340997, gnorm=0.872, train_wall=512, gb_free=6.1, wall=47549
2022-01-31 22:08:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:08:57 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 10.029 | ppl 1044.72 | wps 7871.5 | wpb 2034.1 | bsz 4 | num_updates 8640 | best_loss 9.246
2022-01-31 22:08:57 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-01-31 22:08:57 | INFO | train | epoch 135 | loss 5.481 | ppl 44.65 | wps 5857.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8640 | lr 0.000340207 | gnorm 0.859 | train_wall 328 | gb_free 6.1 | wall 47782
KL Stats: Epoch 135 Divergences: Uniform: 3.111670229840268 Unigram: 3.5685190033817418
2022-01-31 22:08:57 | INFO | fairseq.trainer | begin training epoch 136
2022-01-31 22:08:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:14:07 | INFO | train_inner | epoch 136:     60 / 64 loss=5.481, ppl=44.67, wps=6027.1, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=8700, lr=0.000339032, gnorm=0.867, train_wall=513, gb_free=6.1, wall=48091
2022-01-31 22:14:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:14:54 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 10.076 | ppl 1079.26 | wps 7870.2 | wpb 2034.1 | bsz 4 | num_updates 8704 | best_loss 9.246
2022-01-31 22:14:54 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-01-31 22:14:54 | INFO | train | epoch 136 | loss 5.472 | ppl 44.39 | wps 5858.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8704 | lr 0.000338954 | gnorm 0.872 | train_wall 328 | gb_free 6.1 | wall 48138
KL Stats: Epoch 136 Divergences: Uniform: 3.1115972082508017 Unigram: 3.5743016246367283
2022-01-31 22:14:54 | INFO | fairseq.trainer | begin training epoch 137
2022-01-31 22:14:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:20:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:20:46 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 10.121 | ppl 1113.42 | wps 7962.3 | wpb 2034.1 | bsz 4 | num_updates 8768 | best_loss 9.246
2022-01-31 22:20:46 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-01-31 22:20:46 | INFO | train | epoch 137 | loss 5.465 | ppl 44.16 | wps 5926.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8768 | lr 0.000337715 | gnorm 0.887 | train_wall 324 | gb_free 6.1 | wall 48491
KL Stats: Epoch 137 Divergences: Uniform: 3.114532469221607 Unigram: 3.5864768893889094
2022-01-31 22:20:46 | INFO | fairseq.trainer | begin training epoch 138
2022-01-31 22:20:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:23:29 | INFO | train_inner | epoch 138:     32 / 64 loss=5.455, ppl=43.87, wps=5802, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8800, lr=0.0003371, gnorm=0.875, train_wall=505, gb_free=6.1, wall=48653
2022-01-31 22:26:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:26:38 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 10.124 | ppl 1115.96 | wps 7902.3 | wpb 2034.1 | bsz 4 | num_updates 8832 | best_loss 9.246
2022-01-31 22:26:38 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-01-31 22:26:38 | INFO | train | epoch 138 | loss 5.454 | ppl 43.83 | wps 5934.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8832 | lr 0.000336489 | gnorm 0.889 | train_wall 323 | gb_free 6.1 | wall 48842
KL Stats: Epoch 138 Divergences: Uniform: 3.117839864238971 Unigram: 3.5910906533479583
2022-01-31 22:26:38 | INFO | fairseq.trainer | begin training epoch 139
2022-01-31 22:26:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:32:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:32:34 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 10.07 | ppl 1074.55 | wps 7827.5 | wpb 2034.1 | bsz 4 | num_updates 8896 | best_loss 9.246
2022-01-31 22:32:34 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-01-31 22:32:34 | INFO | train | epoch 139 | loss 5.445 | ppl 43.55 | wps 5864.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8896 | lr 0.000335276 | gnorm 0.895 | train_wall 327 | gb_free 6.1 | wall 49199
KL Stats: Epoch 139 Divergences: Uniform: 3.1199963792292107 Unigram: 3.6010009246670402
2022-01-31 22:32:34 | INFO | fairseq.trainer | begin training epoch 140
2022-01-31 22:32:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:32:55 | INFO | train_inner | epoch 140:      4 / 64 loss=5.453, ppl=43.79, wps=5755.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=8900, lr=0.000335201, gnorm=0.903, train_wall=509, gb_free=6.1, wall=49219
2022-01-31 22:38:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:38:29 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 10.085 | ppl 1086.12 | wps 7840.7 | wpb 2034.1 | bsz 4 | num_updates 8960 | best_loss 9.246
2022-01-31 22:38:29 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-01-31 22:38:29 | INFO | train | epoch 140 | loss 5.437 | ppl 43.31 | wps 5885 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8960 | lr 0.000334077 | gnorm 0.907 | train_wall 326 | gb_free 6.1 | wall 49554
KL Stats: Epoch 140 Divergences: Uniform: 3.114179465805643 Unigram: 3.6032917672033262
2022-01-31 22:38:29 | INFO | fairseq.trainer | begin training epoch 141
2022-01-31 22:38:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:41:56 | INFO | train_inner | epoch 141:     40 / 64 loss=5.429, ppl=43.09, wps=6046.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9000, lr=0.000333333, gnorm=0.913, train_wall=511, gb_free=6.1, wall=49760
2022-01-31 22:43:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:44:26 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 10.13 | ppl 1120.89 | wps 7859.9 | wpb 2034.1 | bsz 4 | num_updates 9024 | best_loss 9.246
2022-01-31 22:44:26 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-01-31 22:44:26 | INFO | train | epoch 141 | loss 5.429 | ppl 43.08 | wps 5861.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9024 | lr 0.00033289 | gnorm 0.9 | train_wall 328 | gb_free 6.1 | wall 49910
KL Stats: Epoch 141 Divergences: Uniform: 3.1216654506620705 Unigram: 3.6082183314703116
2022-01-31 22:44:26 | INFO | fairseq.trainer | begin training epoch 142
2022-01-31 22:44:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:49:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:50:22 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 10.069 | ppl 1073.79 | wps 7808.9 | wpb 2034.1 | bsz 4 | num_updates 9088 | best_loss 9.246
2022-01-31 22:50:22 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-01-31 22:50:22 | INFO | train | epoch 142 | loss 5.419 | ppl 42.79 | wps 5855.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9088 | lr 0.000331716 | gnorm 0.915 | train_wall 328 | gb_free 6.1 | wall 50267
KL Stats: Epoch 142 Divergences: Uniform: 3.1228926772283967 Unigram: 3.6120658966365835
2022-01-31 22:50:22 | INFO | fairseq.trainer | begin training epoch 143
2022-01-31 22:50:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 22:51:24 | INFO | train_inner | epoch 143:     12 / 64 loss=5.421, ppl=42.84, wps=5731.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=9100, lr=0.000331497, gnorm=0.907, train_wall=512, gb_free=6.1, wall=50329
2022-01-31 22:55:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 22:56:19 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 10.074 | ppl 1077.97 | wps 7848.6 | wpb 2034.1 | bsz 4 | num_updates 9152 | best_loss 9.246
2022-01-31 22:56:19 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-01-31 22:56:19 | INFO | train | epoch 143 | loss 5.414 | ppl 42.62 | wps 5863.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9152 | lr 0.000330554 | gnorm 0.917 | train_wall 327 | gb_free 6.1 | wall 50623
KL Stats: Epoch 143 Divergences: Uniform: 3.13257279959034 Unigram: 3.6190608785885785
2022-01-31 22:56:19 | INFO | fairseq.trainer | begin training epoch 144
2022-01-31 22:56:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:00:26 | INFO | train_inner | epoch 144:     48 / 64 loss=5.41, ppl=42.51, wps=6032.4, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=9200, lr=0.00032969, gnorm=0.91, train_wall=512, gb_free=6.1, wall=50870
2022-01-31 23:01:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:02:15 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 10.129 | ppl 1119.96 | wps 7828.7 | wpb 2034.1 | bsz 4 | num_updates 9216 | best_loss 9.246
2022-01-31 23:02:15 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-01-31 23:02:15 | INFO | train | epoch 144 | loss 5.406 | ppl 42.39 | wps 5860.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9216 | lr 0.000329404 | gnorm 0.907 | train_wall 328 | gb_free 6.1 | wall 50979
KL Stats: Epoch 144 Divergences: Uniform: 3.122829273442295 Unigram: 3.620045999924024
2022-01-31 23:02:15 | INFO | fairseq.trainer | begin training epoch 145
2022-01-31 23:02:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:07:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:08:12 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 10.148 | ppl 1134.8 | wps 7825.5 | wpb 2034.1 | bsz 4 | num_updates 9280 | best_loss 9.246
2022-01-31 23:08:12 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-01-31 23:08:12 | INFO | train | epoch 145 | loss 5.398 | ppl 42.17 | wps 5855.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9280 | lr 0.000328266 | gnorm 0.937 | train_wall 328 | gb_free 6.1 | wall 51336
KL Stats: Epoch 145 Divergences: Uniform: 3.133467826428437 Unigram: 3.6309601271147205
2022-01-31 23:08:12 | INFO | fairseq.trainer | begin training epoch 146
2022-01-31 23:08:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:09:55 | INFO | train_inner | epoch 146:     20 / 64 loss=5.394, ppl=42.06, wps=5733.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=9300, lr=0.000327913, gnorm=0.933, train_wall=511, gb_free=6.1, wall=51439
2022-01-31 23:13:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:14:07 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 10.106 | ppl 1102.12 | wps 7794.9 | wpb 2034.1 | bsz 4 | num_updates 9344 | best_loss 9.246
2022-01-31 23:14:07 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-01-31 23:14:07 | INFO | train | epoch 146 | loss 5.388 | ppl 41.88 | wps 5873.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9344 | lr 0.00032714 | gnorm 0.945 | train_wall 327 | gb_free 6.1 | wall 51692
KL Stats: Epoch 146 Divergences: Uniform: 3.1301713620510516 Unigram: 3.635953028467672
2022-01-31 23:14:07 | INFO | fairseq.trainer | begin training epoch 147
2022-01-31 23:14:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:18:55 | INFO | train_inner | epoch 147:     56 / 64 loss=5.39, ppl=41.93, wps=6046, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9400, lr=0.000326164, gnorm=0.942, train_wall=511, gb_free=6.1, wall=51979
2022-01-31 23:19:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:20:03 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 10.126 | ppl 1117.28 | wps 7799.8 | wpb 2034.1 | bsz 4 | num_updates 9408 | best_loss 9.246
2022-01-31 23:20:03 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-01-31 23:20:03 | INFO | train | epoch 147 | loss 5.381 | ppl 41.67 | wps 5869.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9408 | lr 0.000326025 | gnorm 0.943 | train_wall 327 | gb_free 6.1 | wall 52047
KL Stats: Epoch 147 Divergences: Uniform: 3.1374342874373746 Unigram: 3.64663195049186
2022-01-31 23:20:03 | INFO | fairseq.trainer | begin training epoch 148
2022-01-31 23:20:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:25:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:26:00 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 10.11 | ppl 1105.44 | wps 7811.4 | wpb 2034.1 | bsz 4 | num_updates 9472 | best_loss 9.246
2022-01-31 23:26:00 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-01-31 23:26:00 | INFO | train | epoch 148 | loss 5.373 | ppl 41.45 | wps 5860.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9472 | lr 0.000324922 | gnorm 0.941 | train_wall 327 | gb_free 6.1 | wall 52404
KL Stats: Epoch 148 Divergences: Uniform: 3.1352670913371203 Unigram: 3.650489253887747
2022-01-31 23:26:00 | INFO | fairseq.trainer | begin training epoch 149
2022-01-31 23:26:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:28:23 | INFO | train_inner | epoch 149:     28 / 64 loss=5.368, ppl=41.31, wps=5736.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=9500, lr=0.000324443, gnorm=0.943, train_wall=511, gb_free=6.1, wall=52548
2022-01-31 23:31:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:31:55 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 10.123 | ppl 1114.99 | wps 7821.3 | wpb 2034.1 | bsz 4 | num_updates 9536 | best_loss 9.246
2022-01-31 23:31:55 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-01-31 23:31:55 | INFO | train | epoch 149 | loss 5.368 | ppl 41.29 | wps 5873.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9536 | lr 0.00032383 | gnorm 0.975 | train_wall 327 | gb_free 6.1 | wall 52759
KL Stats: Epoch 149 Divergences: Uniform: 3.1419838030392255 Unigram: 3.6569394030361453
2022-01-31 23:31:55 | INFO | fairseq.trainer | begin training epoch 150
2022-01-31 23:31:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:37:23 | INFO | train_inner | epoch 150:     64 / 64 loss=5.37, ppl=41.34, wps=6042.1, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=9600, lr=0.000322749, gnorm=0.981, train_wall=510, gb_free=6.1, wall=53087
2022-01-31 23:37:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:37:51 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 10.162 | ppl 1145.35 | wps 7852 | wpb 2034.1 | bsz 4 | num_updates 9600 | best_loss 9.246
2022-01-31 23:37:51 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-01-31 23:37:51 | INFO | train | epoch 150 | loss 5.359 | ppl 41.03 | wps 5871.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9600 | lr 0.000322749 | gnorm 0.967 | train_wall 327 | gb_free 6.1 | wall 53115
KL Stats: Epoch 150 Divergences: Uniform: 3.139678197175833 Unigram: 3.6605557191123235
2022-01-31 23:37:51 | INFO | fairseq.trainer | begin training epoch 151
2022-01-31 23:37:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:43:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:43:45 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 10.176 | ppl 1156.71 | wps 7916.9 | wpb 2034.1 | bsz 4 | num_updates 9664 | best_loss 9.246
2022-01-31 23:43:45 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-01-31 23:43:45 | INFO | train | epoch 151 | loss 5.351 | ppl 40.82 | wps 5893.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9664 | lr 0.000321678 | gnorm 0.969 | train_wall 326 | gb_free 6.1 | wall 53469
KL Stats: Epoch 151 Divergences: Uniform: 3.138224029879648 Unigram: 3.665370863699881
2022-01-31 23:43:45 | INFO | fairseq.trainer | begin training epoch 152
2022-01-31 23:43:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:46:49 | INFO | train_inner | epoch 152:     36 / 64 loss=5.339, ppl=40.47, wps=5777.9, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=9700, lr=0.000321081, gnorm=0.97, train_wall=509, gb_free=6.1, wall=53653
2022-01-31 23:49:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:49:38 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 10.146 | ppl 1132.94 | wps 7815.2 | wpb 2034.1 | bsz 4 | num_updates 9728 | best_loss 9.246
2022-01-31 23:49:38 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-01-31 23:49:38 | INFO | train | epoch 152 | loss 5.344 | ppl 40.62 | wps 5915.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9728 | lr 0.000320618 | gnorm 0.975 | train_wall 324 | gb_free 6.1 | wall 53822
KL Stats: Epoch 152 Divergences: Uniform: 3.1477865279646804 Unigram: 3.6749283161578155
2022-01-31 23:49:38 | INFO | fairseq.trainer | begin training epoch 153
2022-01-31 23:49:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:55:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-01-31 23:55:34 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 10.137 | ppl 1125.81 | wps 7829.5 | wpb 2034.1 | bsz 4 | num_updates 9792 | best_loss 9.246
2022-01-31 23:55:34 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-01-31 23:55:34 | INFO | train | epoch 153 | loss 5.336 | ppl 40.39 | wps 5867.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9792 | lr 0.000319569 | gnorm 0.954 | train_wall 327 | gb_free 6.1 | wall 54178
KL Stats: Epoch 153 Divergences: Uniform: 3.1489724360515527 Unigram: 3.6805198993674395
2022-01-31 23:55:34 | INFO | fairseq.trainer | begin training epoch 154
2022-01-31 23:55:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-01-31 23:56:15 | INFO | train_inner | epoch 154:      8 / 64 loss=5.344, ppl=40.61, wps=5750.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=9800, lr=0.000319438, gnorm=0.961, train_wall=510, gb_free=6.1, wall=54220
2022-02-01 00:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:01:31 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 10.194 | ppl 1171.08 | wps 7853.3 | wpb 2034.1 | bsz 4 | num_updates 9856 | best_loss 9.246
2022-02-01 00:01:31 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-02-01 00:01:31 | INFO | train | epoch 154 | loss 5.332 | ppl 40.27 | wps 5862.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9856 | lr 0.000318529 | gnorm 0.997 | train_wall 328 | gb_free 6.1 | wall 54535
KL Stats: Epoch 154 Divergences: Uniform: 3.1526098467221115 Unigram: 3.6799221807091276
2022-02-01 00:01:31 | INFO | fairseq.trainer | begin training epoch 155
2022-02-01 00:01:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:05:17 | INFO | train_inner | epoch 155:     44 / 64 loss=5.323, ppl=40.02, wps=6035.1, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=9900, lr=0.000317821, gnorm=0.996, train_wall=512, gb_free=6.1, wall=54761
2022-02-01 00:06:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:07:26 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 10.185 | ppl 1164.28 | wps 7920.7 | wpb 2034.1 | bsz 4 | num_updates 9920 | best_loss 9.246
2022-02-01 00:07:26 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-02-01 00:07:26 | INFO | train | epoch 155 | loss 5.324 | ppl 40.06 | wps 5873.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9920 | lr 0.0003175 | gnorm 0.998 | train_wall 327 | gb_free 6.1 | wall 54890
KL Stats: Epoch 155 Divergences: Uniform: 3.144106154752952 Unigram: 3.6947574393250813
2022-02-01 00:07:26 | INFO | fairseq.trainer | begin training epoch 156
2022-02-01 00:07:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:12:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:13:19 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 10.157 | ppl 1142.04 | wps 7898.4 | wpb 2034.1 | bsz 4 | num_updates 9984 | best_loss 9.246
2022-02-01 00:13:19 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-02-01 00:13:19 | INFO | train | epoch 156 | loss 5.318 | ppl 39.9 | wps 5915.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9984 | lr 0.000316481 | gnorm 1.012 | train_wall 325 | gb_free 6.1 | wall 55243
KL Stats: Epoch 156 Divergences: Uniform: 3.1490253064561515 Unigram: 3.6845379121241124
2022-02-01 00:13:19 | INFO | fairseq.trainer | begin training epoch 157
2022-02-01 00:13:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:14:41 | INFO | train_inner | epoch 157:     16 / 64 loss=5.324, ppl=40.05, wps=5780.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10000, lr=0.000316228, gnorm=1.011, train_wall=507, gb_free=6.1, wall=55325
2022-02-01 00:18:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:19:13 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 10.128 | ppl 1119.04 | wps 7840.9 | wpb 2034.1 | bsz 4 | num_updates 10048 | best_loss 9.246
2022-02-01 00:19:13 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-02-01 00:19:13 | INFO | train | epoch 157 | loss 5.31 | ppl 39.68 | wps 5900.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10048 | lr 0.000315472 | gnorm 1.017 | train_wall 325 | gb_free 6.1 | wall 55597
KL Stats: Epoch 157 Divergences: Uniform: 3.1475109914330126 Unigram: 3.6988670225943454
2022-02-01 00:19:13 | INFO | fairseq.trainer | begin training epoch 158
2022-02-01 00:19:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:23:41 | INFO | train_inner | epoch 158:     52 / 64 loss=5.302, ppl=39.46, wps=6055.2, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=10100, lr=0.000314658, gnorm=0.99, train_wall=510, gb_free=6.1, wall=55865
2022-02-01 00:24:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:25:09 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 10.212 | ppl 1186.47 | wps 7835.3 | wpb 2034.1 | bsz 4 | num_updates 10112 | best_loss 9.246
2022-02-01 00:25:09 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-02-01 00:25:09 | INFO | train | epoch 158 | loss 5.302 | ppl 39.46 | wps 5869.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10112 | lr 0.000314472 | gnorm 0.967 | train_wall 327 | gb_free 6.1 | wall 55953
KL Stats: Epoch 158 Divergences: Uniform: 3.1480411072598526 Unigram: 3.7040085168187478
2022-02-01 00:25:09 | INFO | fairseq.trainer | begin training epoch 159
2022-02-01 00:25:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:30:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:31:06 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 10.23 | ppl 1201.39 | wps 7857.2 | wpb 2034.1 | bsz 4 | num_updates 10176 | best_loss 9.246
2022-02-01 00:31:06 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-02-01 00:31:06 | INFO | train | epoch 159 | loss 5.296 | ppl 39.29 | wps 5859.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10176 | lr 0.000313481 | gnorm 1.015 | train_wall 328 | gb_free 6.1 | wall 56310
KL Stats: Epoch 159 Divergences: Uniform: 3.1430791221191976 Unigram: 3.7161618646282224
2022-02-01 00:31:06 | INFO | fairseq.trainer | begin training epoch 160
2022-02-01 00:31:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:33:09 | INFO | train_inner | epoch 160:     24 / 64 loss=5.296, ppl=39.29, wps=5732.4, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=10200, lr=0.000313112, gnorm=1.013, train_wall=511, gb_free=6.1, wall=56433
2022-02-01 00:36:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:37:02 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 10.195 | ppl 1172.52 | wps 7840.1 | wpb 2034.1 | bsz 4 | num_updates 10240 | best_loss 9.246
2022-02-01 00:37:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 10240 updates
2022-02-01 00:37:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint160.pt
2022-02-01 00:37:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint160.pt
2022-02-01 00:37:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint160.pt (epoch 160 @ 10240 updates, score 10.195) (writing took 3.7570846006274223 seconds)
2022-02-01 00:37:06 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-02-01 00:37:06 | INFO | train | epoch 160 | loss 5.292 | ppl 39.18 | wps 5799.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10240 | lr 0.0003125 | gnorm 1.032 | train_wall 328 | gb_free 6.1 | wall 56670
KL Stats: Epoch 160 Divergences: Uniform: 3.1510534993394765 Unigram: 3.71617796872914
2022-02-01 00:37:06 | INFO | fairseq.trainer | begin training epoch 161
2022-02-01 00:37:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:42:14 | INFO | train_inner | epoch 161:     60 / 64 loss=5.293, ppl=39.2, wps=5997.6, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=10300, lr=0.000311588, gnorm=1.031, train_wall=512, gb_free=6.1, wall=56978
2022-02-01 00:42:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:43:02 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 10.156 | ppl 1141.02 | wps 7806.3 | wpb 2034.1 | bsz 4 | num_updates 10304 | best_loss 9.246
2022-02-01 00:43:02 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-02-01 00:43:02 | INFO | train | epoch 161 | loss 5.285 | ppl 38.99 | wps 5868.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10304 | lr 0.000311528 | gnorm 1.021 | train_wall 327 | gb_free 6.1 | wall 57026
KL Stats: Epoch 161 Divergences: Uniform: 3.158335517577685 Unigram: 3.72012645617306
2022-02-01 00:43:02 | INFO | fairseq.trainer | begin training epoch 162
2022-02-01 00:43:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:48:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:48:57 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 10.195 | ppl 1172.35 | wps 7810 | wpb 2034.1 | bsz 4 | num_updates 10368 | best_loss 9.246
2022-02-01 00:48:57 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-02-01 00:48:57 | INFO | train | epoch 162 | loss 5.277 | ppl 38.78 | wps 5871.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10368 | lr 0.000310565 | gnorm 1.03 | train_wall 327 | gb_free 6.1 | wall 57381
KL Stats: Epoch 162 Divergences: Uniform: 3.161009652107703 Unigram: 3.726535426527989
2022-02-01 00:48:57 | INFO | fairseq.trainer | begin training epoch 163
2022-02-01 00:48:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 00:51:42 | INFO | train_inner | epoch 163:     32 / 64 loss=5.264, ppl=38.44, wps=5740, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10400, lr=0.000310087, gnorm=1.026, train_wall=510, gb_free=6.1, wall=57546
2022-02-01 00:54:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 00:54:54 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 10.261 | ppl 1226.92 | wps 7854.4 | wpb 2034.1 | bsz 4 | num_updates 10432 | best_loss 9.246
2022-02-01 00:54:54 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-02-01 00:54:54 | INFO | train | epoch 163 | loss 5.273 | ppl 38.68 | wps 5862 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10432 | lr 0.000309611 | gnorm 1.032 | train_wall 328 | gb_free 6.1 | wall 57738
KL Stats: Epoch 163 Divergences: Uniform: 3.15149764108806 Unigram: 3.7293601410684194
2022-02-01 00:54:54 | INFO | fairseq.trainer | begin training epoch 164
2022-02-01 00:54:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:00:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:00:50 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 10.168 | ppl 1150.78 | wps 7825.7 | wpb 2034.1 | bsz 4 | num_updates 10496 | best_loss 9.246
2022-02-01 01:00:50 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-02-01 01:00:50 | INFO | train | epoch 164 | loss 5.266 | ppl 38.49 | wps 5865.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10496 | lr 0.000308665 | gnorm 1.053 | train_wall 327 | gb_free 6.1 | wall 58094
KL Stats: Epoch 164 Divergences: Uniform: 3.1513544582971402 Unigram: 3.733933988910646
2022-02-01 01:00:50 | INFO | fairseq.trainer | begin training epoch 165
2022-02-01 01:00:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:01:10 | INFO | train_inner | epoch 165:      4 / 64 loss=5.281, ppl=38.88, wps=5737.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=10500, lr=0.000308607, gnorm=1.05, train_wall=511, gb_free=6.1, wall=58115
2022-02-01 01:06:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:06:45 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 10.201 | ppl 1176.75 | wps 7891.9 | wpb 2034.1 | bsz 4 | num_updates 10560 | best_loss 9.246
2022-02-01 01:06:45 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-02-01 01:06:45 | INFO | train | epoch 165 | loss 5.258 | ppl 38.27 | wps 5874.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10560 | lr 0.000307729 | gnorm 1.054 | train_wall 327 | gb_free 6.1 | wall 58449
KL Stats: Epoch 165 Divergences: Uniform: 3.156471710547944 Unigram: 3.741887934304215
2022-02-01 01:06:45 | INFO | fairseq.trainer | begin training epoch 166
2022-02-01 01:06:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:10:11 | INFO | train_inner | epoch 166:     40 / 64 loss=5.251, ppl=38.09, wps=6043.8, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=10600, lr=0.000307148, gnorm=1.073, train_wall=512, gb_free=6.1, wall=58655
2022-02-01 01:12:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:12:41 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 10.114 | ppl 1107.97 | wps 7837.2 | wpb 2034.1 | bsz 4 | num_updates 10624 | best_loss 9.246
2022-02-01 01:12:41 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-02-01 01:12:41 | INFO | train | epoch 166 | loss 5.254 | ppl 38.16 | wps 5869.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10624 | lr 0.0003068 | gnorm 1.079 | train_wall 327 | gb_free 6.1 | wall 58805
KL Stats: Epoch 166 Divergences: Uniform: 3.159389989195111 Unigram: 3.7476906918230664
2022-02-01 01:12:41 | INFO | fairseq.trainer | begin training epoch 167
2022-02-01 01:12:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:18:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:18:37 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 10.139 | ppl 1127.82 | wps 7804.1 | wpb 2034.1 | bsz 4 | num_updates 10688 | best_loss 9.246
2022-02-01 01:18:37 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-02-01 01:18:37 | INFO | train | epoch 167 | loss 5.247 | ppl 37.98 | wps 5862.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10688 | lr 0.00030588 | gnorm 1.065 | train_wall 327 | gb_free 6.1 | wall 59161
KL Stats: Epoch 167 Divergences: Uniform: 3.1548258114326706 Unigram: 3.7444339634522406
2022-02-01 01:18:37 | INFO | fairseq.trainer | begin training epoch 168
2022-02-01 01:18:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:19:39 | INFO | train_inner | epoch 168:     12 / 64 loss=5.249, ppl=38.03, wps=5739.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10700, lr=0.000305709, gnorm=1.055, train_wall=511, gb_free=6.1, wall=59223
2022-02-01 01:24:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:24:33 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 10.22 | ppl 1192.55 | wps 7851.7 | wpb 2034.1 | bsz 4 | num_updates 10752 | best_loss 9.246
2022-02-01 01:24:33 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-02-01 01:24:33 | INFO | train | epoch 168 | loss 5.241 | ppl 37.82 | wps 5872.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10752 | lr 0.000304969 | gnorm 1.056 | train_wall 327 | gb_free 6.1 | wall 59517
KL Stats: Epoch 168 Divergences: Uniform: 3.16180785549941 Unigram: 3.7506524654054876
2022-02-01 01:24:33 | INFO | fairseq.trainer | begin training epoch 169
2022-02-01 01:24:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:28:40 | INFO | train_inner | epoch 169:     48 / 64 loss=5.241, ppl=37.81, wps=6043.1, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=10800, lr=0.00030429, gnorm=1.076, train_wall=512, gb_free=6.1, wall=59764
2022-02-01 01:30:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:30:29 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 10.181 | ppl 1160.99 | wps 7813.2 | wpb 2034.1 | bsz 4 | num_updates 10816 | best_loss 9.246
2022-02-01 01:30:29 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-02-01 01:30:29 | INFO | train | epoch 169 | loss 5.236 | ppl 37.68 | wps 5867.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10816 | lr 0.000304065 | gnorm 1.082 | train_wall 327 | gb_free 6.1 | wall 59873
KL Stats: Epoch 169 Divergences: Uniform: 3.16005977058326 Unigram: 3.7566339754814897
2022-02-01 01:30:29 | INFO | fairseq.trainer | begin training epoch 170
2022-02-01 01:30:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:35:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:36:22 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 10.22 | ppl 1192.61 | wps 7920.7 | wpb 2034.1 | bsz 4 | num_updates 10880 | best_loss 9.246
2022-02-01 01:36:22 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-02-01 01:36:22 | INFO | train | epoch 170 | loss 5.232 | ppl 37.57 | wps 5911.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10880 | lr 0.00030317 | gnorm 1.119 | train_wall 325 | gb_free 6.1 | wall 60226
KL Stats: Epoch 170 Divergences: Uniform: 3.167935037422434 Unigram: 3.7641906196468944
2022-02-01 01:36:22 | INFO | fairseq.trainer | begin training epoch 171
2022-02-01 01:36:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:38:04 | INFO | train_inner | epoch 171:     20 / 64 loss=5.225, ppl=37.41, wps=5776.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10900, lr=0.000302891, gnorm=1.113, train_wall=507, gb_free=6.1, wall=60328
2022-02-01 01:41:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:42:15 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 10.207 | ppl 1182.25 | wps 7844.7 | wpb 2034.1 | bsz 4 | num_updates 10944 | best_loss 9.246
2022-02-01 01:42:15 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-02-01 01:42:15 | INFO | train | epoch 171 | loss 5.225 | ppl 37.41 | wps 5912.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10944 | lr 0.000302282 | gnorm 1.125 | train_wall 324 | gb_free 6.1 | wall 60580
KL Stats: Epoch 171 Divergences: Uniform: 3.162335560261489 Unigram: 3.7644736666283607
2022-02-01 01:42:15 | INFO | fairseq.trainer | begin training epoch 172
2022-02-01 01:42:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:47:03 | INFO | train_inner | epoch 172:     56 / 64 loss=5.227, ppl=37.46, wps=6060, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11000, lr=0.000301511, gnorm=1.099, train_wall=510, gb_free=6.1, wall=60868
2022-02-01 01:47:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:48:11 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 10.204 | ppl 1179.87 | wps 7856.3 | wpb 2034.1 | bsz 4 | num_updates 11008 | best_loss 9.246
2022-02-01 01:48:11 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-02-01 01:48:11 | INFO | train | epoch 172 | loss 5.218 | ppl 37.21 | wps 5870.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11008 | lr 0.000301402 | gnorm 1.094 | train_wall 327 | gb_free 6.1 | wall 60935
KL Stats: Epoch 172 Divergences: Uniform: 3.1693925541134025 Unigram: 3.7705039571668113
2022-02-01 01:48:11 | INFO | fairseq.trainer | begin training epoch 173
2022-02-01 01:48:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:53:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 01:54:07 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 10.167 | ppl 1149.27 | wps 7862.6 | wpb 2034.1 | bsz 4 | num_updates 11072 | best_loss 9.246
2022-02-01 01:54:07 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-02-01 01:54:07 | INFO | train | epoch 173 | loss 5.214 | ppl 37.13 | wps 5869.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11072 | lr 0.000300529 | gnorm 1.147 | train_wall 327 | gb_free 6.1 | wall 61291
KL Stats: Epoch 173 Divergences: Uniform: 3.167246454089047 Unigram: 3.7749430577701006
2022-02-01 01:54:07 | INFO | fairseq.trainer | begin training epoch 174
2022-02-01 01:54:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 01:56:31 | INFO | train_inner | epoch 174:     28 / 64 loss=5.208, ppl=36.97, wps=5741.3, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=11100, lr=0.00030015, gnorm=1.139, train_wall=511, gb_free=6.1, wall=61435
2022-02-01 01:59:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:00:03 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 10.221 | ppl 1193.88 | wps 7870.4 | wpb 2034.1 | bsz 4 | num_updates 11136 | best_loss 9.246
2022-02-01 02:00:03 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-02-01 02:00:03 | INFO | train | epoch 174 | loss 5.209 | ppl 37 | wps 5872.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11136 | lr 0.000299665 | gnorm 1.133 | train_wall 327 | gb_free 6.1 | wall 61647
KL Stats: Epoch 174 Divergences: Uniform: 3.1646548739762186 Unigram: 3.783897610701194
2022-02-01 02:00:03 | INFO | fairseq.trainer | begin training epoch 175
2022-02-01 02:00:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:05:31 | INFO | train_inner | epoch 175:     64 / 64 loss=5.213, ppl=37.09, wps=6043.7, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=11200, lr=0.000298807, gnorm=1.136, train_wall=510, gb_free=6.1, wall=61975
2022-02-01 02:05:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:05:59 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 10.246 | ppl 1214.58 | wps 7847.3 | wpb 2034.1 | bsz 4 | num_updates 11200 | best_loss 9.246
2022-02-01 02:05:59 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-02-01 02:05:59 | INFO | train | epoch 175 | loss 5.203 | ppl 36.82 | wps 5871 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11200 | lr 0.000298807 | gnorm 1.128 | train_wall 327 | gb_free 6.1 | wall 62003
KL Stats: Epoch 175 Divergences: Uniform: 3.1747484809284576 Unigram: 3.7846221077246396
2022-02-01 02:05:59 | INFO | fairseq.trainer | begin training epoch 176
2022-02-01 02:05:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:11:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:11:54 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 10.231 | ppl 1201.99 | wps 7808.2 | wpb 2034.1 | bsz 4 | num_updates 11264 | best_loss 9.246
2022-02-01 02:11:54 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-02-01 02:11:54 | INFO | train | epoch 176 | loss 5.198 | ppl 36.71 | wps 5875.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11264 | lr 0.000297957 | gnorm 1.136 | train_wall 327 | gb_free 6.1 | wall 62358
KL Stats: Epoch 176 Divergences: Uniform: 3.1697322154013357 Unigram: 3.7887824058956006
2022-02-01 02:11:54 | INFO | fairseq.trainer | begin training epoch 177
2022-02-01 02:11:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:14:59 | INFO | train_inner | epoch 177:     36 / 64 loss=5.185, ppl=36.38, wps=5752.3, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=11300, lr=0.000297482, gnorm=1.127, train_wall=511, gb_free=6.1, wall=62543
2022-02-01 02:17:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:17:49 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 10.224 | ppl 1196.03 | wps 7835.2 | wpb 2034.1 | bsz 4 | num_updates 11328 | best_loss 9.246
2022-02-01 02:17:49 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-02-01 02:17:49 | INFO | train | epoch 177 | loss 5.192 | ppl 36.56 | wps 5876.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11328 | lr 0.000297114 | gnorm 1.13 | train_wall 327 | gb_free 6.1 | wall 62714
KL Stats: Epoch 177 Divergences: Uniform: 3.170895565471488 Unigram: 3.7928213436128404
2022-02-01 02:17:49 | INFO | fairseq.trainer | begin training epoch 178
2022-02-01 02:17:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:23:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:23:45 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 10.23 | ppl 1201.39 | wps 7798.3 | wpb 2034.1 | bsz 4 | num_updates 11392 | best_loss 9.246
2022-02-01 02:23:45 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-02-01 02:23:45 | INFO | train | epoch 178 | loss 5.192 | ppl 36.54 | wps 5874.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11392 | lr 0.000296278 | gnorm 1.203 | train_wall 327 | gb_free 6.1 | wall 63069
KL Stats: Epoch 178 Divergences: Uniform: 3.1791057690376183 Unigram: 3.7987952126538125
2022-02-01 02:23:45 | INFO | fairseq.trainer | begin training epoch 179
2022-02-01 02:23:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:24:26 | INFO | train_inner | epoch 179:      8 / 64 loss=5.198, ppl=36.71, wps=5744.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=11400, lr=0.000296174, gnorm=1.185, train_wall=510, gb_free=6.1, wall=63110
2022-02-01 02:29:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:29:41 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 10.187 | ppl 1165.68 | wps 7863.5 | wpb 2034.1 | bsz 4 | num_updates 11456 | best_loss 9.246
2022-02-01 02:29:41 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-02-01 02:29:41 | INFO | train | epoch 179 | loss 5.182 | ppl 36.29 | wps 5864.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11456 | lr 0.00029545 | gnorm 1.16 | train_wall 327 | gb_free 6.1 | wall 63425
KL Stats: Epoch 179 Divergences: Uniform: 3.179375766808332 Unigram: 3.8035008846999165
2022-02-01 02:29:41 | INFO | fairseq.trainer | begin training epoch 180
2022-02-01 02:29:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:33:27 | INFO | train_inner | epoch 180:     44 / 64 loss=5.177, ppl=36.18, wps=6039.3, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=11500, lr=0.000294884, gnorm=1.155, train_wall=512, gb_free=6.1, wall=63652
2022-02-01 02:35:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:35:37 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 10.172 | ppl 1153.89 | wps 7854.1 | wpb 2034.1 | bsz 4 | num_updates 11520 | best_loss 9.246
2022-02-01 02:35:37 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-02-01 02:35:37 | INFO | train | epoch 180 | loss 5.178 | ppl 36.19 | wps 5871.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11520 | lr 0.000294628 | gnorm 1.17 | train_wall 327 | gb_free 6.1 | wall 63781
KL Stats: Epoch 180 Divergences: Uniform: 3.1717641813935993 Unigram: 3.8072895474609565
2022-02-01 02:35:37 | INFO | fairseq.trainer | begin training epoch 181
2022-02-01 02:35:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:41:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:41:33 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 10.19 | ppl 1168.22 | wps 7833.5 | wpb 2034.1 | bsz 4 | num_updates 11584 | best_loss 9.246
2022-02-01 02:41:33 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-02-01 02:41:33 | INFO | train | epoch 181 | loss 5.17 | ppl 36.01 | wps 5870.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11584 | lr 0.000293813 | gnorm 1.16 | train_wall 327 | gb_free 6.1 | wall 64137
KL Stats: Epoch 181 Divergences: Uniform: 3.177378616595448 Unigram: 3.8118337992292695
2022-02-01 02:41:33 | INFO | fairseq.trainer | begin training epoch 182
2022-02-01 02:41:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:42:55 | INFO | train_inner | epoch 182:     16 / 64 loss=5.171, ppl=36.03, wps=5741.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=11600, lr=0.00029361, gnorm=1.167, train_wall=511, gb_free=6.1, wall=64219
2022-02-01 02:47:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:47:29 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 10.198 | ppl 1174.72 | wps 7798.7 | wpb 2034.1 | bsz 4 | num_updates 11648 | best_loss 9.246
2022-02-01 02:47:29 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-02-01 02:47:29 | INFO | train | epoch 182 | loss 5.169 | ppl 35.97 | wps 5866.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11648 | lr 0.000293005 | gnorm 1.193 | train_wall 327 | gb_free 6.1 | wall 64493
KL Stats: Epoch 182 Divergences: Uniform: 3.182958410519576 Unigram: 3.811672530912898
2022-02-01 02:47:29 | INFO | fairseq.trainer | begin training epoch 183
2022-02-01 02:47:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:51:56 | INFO | train_inner | epoch 183:     52 / 64 loss=5.166, ppl=35.89, wps=6043, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=11700, lr=0.000292353, gnorm=1.197, train_wall=511, gb_free=6.1, wall=64760
2022-02-01 02:52:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:53:24 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 10.206 | ppl 1181.2 | wps 7812.5 | wpb 2034.1 | bsz 4 | num_updates 11712 | best_loss 9.246
2022-02-01 02:53:24 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-02-01 02:53:24 | INFO | train | epoch 183 | loss 5.161 | ppl 35.77 | wps 5869.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11712 | lr 0.000292203 | gnorm 1.186 | train_wall 327 | gb_free 6.1 | wall 64849
KL Stats: Epoch 183 Divergences: Uniform: 3.180219092794487 Unigram: 3.8182755778008337
2022-02-01 02:53:25 | INFO | fairseq.trainer | begin training epoch 184
2022-02-01 02:53:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 02:58:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 02:59:18 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 10.158 | ppl 1142.63 | wps 7929.7 | wpb 2034.1 | bsz 4 | num_updates 11776 | best_loss 9.246
2022-02-01 02:59:18 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-02-01 02:59:18 | INFO | train | epoch 184 | loss 5.159 | ppl 35.72 | wps 5906.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11776 | lr 0.000291408 | gnorm 1.237 | train_wall 325 | gb_free 6.1 | wall 65202
KL Stats: Epoch 184 Divergences: Uniform: 3.179069445919472 Unigram: 3.8201804522985467
2022-02-01 02:59:18 | INFO | fairseq.trainer | begin training epoch 185
2022-02-01 02:59:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:01:21 | INFO | train_inner | epoch 185:     24 / 64 loss=5.159, ppl=35.72, wps=5773.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=11800, lr=0.000291111, gnorm=1.225, train_wall=508, gb_free=6.1, wall=65325
2022-02-01 03:04:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:05:11 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 10.211 | ppl 1184.92 | wps 7953.8 | wpb 2034.1 | bsz 4 | num_updates 11840 | best_loss 9.246
2022-02-01 03:05:11 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-02-01 03:05:11 | INFO | train | epoch 185 | loss 5.154 | ppl 35.6 | wps 5921.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11840 | lr 0.000290619 | gnorm 1.195 | train_wall 324 | gb_free 6.1 | wall 65555
KL Stats: Epoch 185 Divergences: Uniform: 3.178993884690645 Unigram: 3.825198452683274
2022-02-01 03:05:11 | INFO | fairseq.trainer | begin training epoch 186
2022-02-01 03:05:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:10:19 | INFO | train_inner | epoch 186:     60 / 64 loss=5.154, ppl=35.61, wps=6074.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11900, lr=0.000289886, gnorm=1.214, train_wall=509, gb_free=6.1, wall=65863
2022-02-01 03:10:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:11:06 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 10.152 | ppl 1137.74 | wps 7886.4 | wpb 2034.1 | bsz 4 | num_updates 11904 | best_loss 9.246
2022-02-01 03:11:06 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-02-01 03:11:06 | INFO | train | epoch 186 | loss 5.151 | ppl 35.52 | wps 5887.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11904 | lr 0.000289837 | gnorm 1.235 | train_wall 326 | gb_free 6.1 | wall 65910
KL Stats: Epoch 186 Divergences: Uniform: 3.1775096567588226 Unigram: 3.8243792362865
2022-02-01 03:11:06 | INFO | fairseq.trainer | begin training epoch 187
2022-02-01 03:11:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:16:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:17:01 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 10.269 | ppl 1233.48 | wps 7864.1 | wpb 2034.1 | bsz 4 | num_updates 11968 | best_loss 9.246
2022-02-01 03:17:01 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-02-01 03:17:01 | INFO | train | epoch 187 | loss 5.144 | ppl 35.36 | wps 5876.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11968 | lr 0.000289061 | gnorm 1.215 | train_wall 327 | gb_free 6.1 | wall 66265
KL Stats: Epoch 187 Divergences: Uniform: 3.1799654653746448 Unigram: 3.834803896166196
2022-02-01 03:17:01 | INFO | fairseq.trainer | begin training epoch 188
2022-02-01 03:17:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:19:46 | INFO | train_inner | epoch 188:     32 / 64 loss=5.137, ppl=35.18, wps=5748, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12000, lr=0.000288675, gnorm=1.203, train_wall=510, gb_free=6.1, wall=66430
2022-02-01 03:22:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:22:57 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 10.219 | ppl 1191.62 | wps 7836.2 | wpb 2034.1 | bsz 4 | num_updates 12032 | best_loss 9.246
2022-02-01 03:22:57 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-02-01 03:22:57 | INFO | train | epoch 188 | loss 5.138 | ppl 35.2 | wps 5873 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12032 | lr 0.000288291 | gnorm 1.222 | train_wall 327 | gb_free 6.1 | wall 66621
KL Stats: Epoch 188 Divergences: Uniform: 3.1785500497221153 Unigram: 3.8384672724507514
2022-02-01 03:22:57 | INFO | fairseq.trainer | begin training epoch 189
2022-02-01 03:22:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:28:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:28:51 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 10.201 | ppl 1177.41 | wps 7879.8 | wpb 2034.1 | bsz 4 | num_updates 12096 | best_loss 9.246
2022-02-01 03:28:51 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-02-01 03:28:51 | INFO | train | epoch 189 | loss 5.134 | ppl 35.12 | wps 5901.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12096 | lr 0.000287527 | gnorm 1.268 | train_wall 325 | gb_free 6.1 | wall 66975
KL Stats: Epoch 189 Divergences: Uniform: 3.183505595973794 Unigram: 3.8421271093887484
2022-02-01 03:28:51 | INFO | fairseq.trainer | begin training epoch 190
2022-02-01 03:28:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:29:11 | INFO | train_inner | epoch 190:      4 / 64 loss=5.14, ppl=35.27, wps=5765.5, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=12100, lr=0.00028748, gnorm=1.268, train_wall=508, gb_free=6.1, wall=66995
2022-02-01 03:34:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:34:45 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 10.245 | ppl 1213.46 | wps 7858.3 | wpb 2034.1 | bsz 4 | num_updates 12160 | best_loss 9.246
2022-02-01 03:34:45 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-02-01 03:34:45 | INFO | train | epoch 190 | loss 5.129 | ppl 34.99 | wps 5886.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12160 | lr 0.00028677 | gnorm 1.235 | train_wall 326 | gb_free 6.1 | wall 67330
KL Stats: Epoch 190 Divergences: Uniform: 3.1838642796356837 Unigram: 3.8461705168458655
2022-02-01 03:34:45 | INFO | fairseq.trainer | begin training epoch 191
2022-02-01 03:34:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:38:12 | INFO | train_inner | epoch 191:     40 / 64 loss=5.119, ppl=34.74, wps=6045.5, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=12200, lr=0.000286299, gnorm=1.251, train_wall=511, gb_free=6.1, wall=67536
2022-02-01 03:40:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:40:42 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 10.203 | ppl 1178.97 | wps 7842.7 | wpb 2034.1 | bsz 4 | num_updates 12224 | best_loss 9.246
2022-02-01 03:40:42 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-02-01 03:40:42 | INFO | train | epoch 191 | loss 5.126 | ppl 34.91 | wps 5861.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12224 | lr 0.000286018 | gnorm 1.261 | train_wall 327 | gb_free 6.1 | wall 67686
KL Stats: Epoch 191 Divergences: Uniform: 3.184101226009161 Unigram: 3.846745972732835
2022-02-01 03:40:42 | INFO | fairseq.trainer | begin training epoch 192
2022-02-01 03:40:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:46:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:46:37 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 10.195 | ppl 1172.39 | wps 7835.4 | wpb 2034.1 | bsz 4 | num_updates 12288 | best_loss 9.246
2022-02-01 03:46:37 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-02-01 03:46:37 | INFO | train | epoch 192 | loss 5.122 | ppl 34.83 | wps 5879.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12288 | lr 0.000285272 | gnorm 1.272 | train_wall 326 | gb_free 6.1 | wall 68041
KL Stats: Epoch 192 Divergences: Uniform: 3.1823351248814378 Unigram: 3.8438328492392766
2022-02-01 03:46:37 | INFO | fairseq.trainer | begin training epoch 193
2022-02-01 03:46:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:47:39 | INFO | train_inner | epoch 193:     12 / 64 loss=5.128, ppl=34.97, wps=5749.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12300, lr=0.000285133, gnorm=1.268, train_wall=510, gb_free=6.1, wall=68103
2022-02-01 03:52:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:52:32 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 10.15 | ppl 1136.23 | wps 7833.5 | wpb 2034.1 | bsz 4 | num_updates 12352 | best_loss 9.246
2022-02-01 03:52:32 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-02-01 03:52:32 | INFO | train | epoch 193 | loss 5.118 | ppl 34.72 | wps 5881.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12352 | lr 0.000284532 | gnorm 1.294 | train_wall 326 | gb_free 6.1 | wall 68396
KL Stats: Epoch 193 Divergences: Uniform: 3.1914390736667517 Unigram: 3.852600357345195
2022-02-01 03:52:32 | INFO | fairseq.trainer | begin training epoch 194
2022-02-01 03:52:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 03:56:39 | INFO | train_inner | epoch 194:     48 / 64 loss=5.114, ppl=34.63, wps=6053.6, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=12400, lr=0.000283981, gnorm=1.293, train_wall=511, gb_free=6.1, wall=68643
2022-02-01 03:57:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 03:58:27 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 10.182 | ppl 1161.79 | wps 7870.4 | wpb 2034.1 | bsz 4 | num_updates 12416 | best_loss 9.246
2022-02-01 03:58:27 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-02-01 03:58:27 | INFO | train | epoch 194 | loss 5.113 | ppl 34.6 | wps 5880.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12416 | lr 0.000283798 | gnorm 1.318 | train_wall 327 | gb_free 6.1 | wall 68751
KL Stats: Epoch 194 Divergences: Uniform: 3.1890637539145263 Unigram: 3.8575371569969152
2022-02-01 03:58:27 | INFO | fairseq.trainer | begin training epoch 195
2022-02-01 03:58:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:03:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:04:23 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 10.218 | ppl 1191.36 | wps 7866.1 | wpb 2034.1 | bsz 4 | num_updates 12480 | best_loss 9.246
2022-02-01 04:04:23 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-02-01 04:04:23 | INFO | train | epoch 195 | loss 5.108 | ppl 34.49 | wps 5870 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12480 | lr 0.000283069 | gnorm 1.298 | train_wall 327 | gb_free 6.1 | wall 69107
KL Stats: Epoch 195 Divergences: Uniform: 3.1885844526743816 Unigram: 3.858804741066612
2022-02-01 04:04:23 | INFO | fairseq.trainer | begin training epoch 196
2022-02-01 04:04:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:06:06 | INFO | train_inner | epoch 196:     20 / 64 loss=5.106, ppl=34.45, wps=5744.3, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12500, lr=0.000282843, gnorm=1.311, train_wall=510, gb_free=6.1, wall=69210
2022-02-01 04:09:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:10:19 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 10.21 | ppl 1184.84 | wps 7840.3 | wpb 2034.1 | bsz 4 | num_updates 12544 | best_loss 9.246
2022-02-01 04:10:19 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-02-01 04:10:19 | INFO | train | epoch 196 | loss 5.106 | ppl 34.43 | wps 5869.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12544 | lr 0.000282346 | gnorm 1.336 | train_wall 327 | gb_free 6.1 | wall 69463
KL Stats: Epoch 196 Divergences: Uniform: 3.1877433700239823 Unigram: 3.8604513818933017
2022-02-01 04:10:19 | INFO | fairseq.trainer | begin training epoch 197
2022-02-01 04:10:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:15:07 | INFO | train_inner | epoch 197:     56 / 64 loss=5.105, ppl=34.41, wps=6047.4, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=12600, lr=0.000281718, gnorm=1.34, train_wall=511, gb_free=6.1, wall=69751
2022-02-01 04:15:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:16:14 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 10.211 | ppl 1185.54 | wps 7879.6 | wpb 2034.1 | bsz 4 | num_updates 12608 | best_loss 9.246
2022-02-01 04:16:14 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-02-01 04:16:14 | INFO | train | epoch 197 | loss 5.099 | ppl 34.28 | wps 5881.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12608 | lr 0.000281629 | gnorm 1.328 | train_wall 326 | gb_free 6.1 | wall 69818
KL Stats: Epoch 197 Divergences: Uniform: 3.1865663248012184 Unigram: 3.8635427002582983
2022-02-01 04:16:14 | INFO | fairseq.trainer | begin training epoch 198
2022-02-01 04:16:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:21:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:22:09 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 10.205 | ppl 1180.13 | wps 7868.4 | wpb 2034.1 | bsz 4 | num_updates 12672 | best_loss 9.246
2022-02-01 04:22:09 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-02-01 04:22:09 | INFO | train | epoch 198 | loss 5.095 | ppl 34.18 | wps 5886.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12672 | lr 0.000280917 | gnorm 1.337 | train_wall 326 | gb_free 6.1 | wall 70173
KL Stats: Epoch 198 Divergences: Uniform: 3.1929393927891203 Unigram: 3.8711975379567964
2022-02-01 04:22:09 | INFO | fairseq.trainer | begin training epoch 199
2022-02-01 04:22:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:24:32 | INFO | train_inner | epoch 199:     28 / 64 loss=5.091, ppl=34.07, wps=5765.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=12700, lr=0.000280607, gnorm=1.332, train_wall=508, gb_free=6.1, wall=70316
2022-02-01 04:27:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:28:01 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 10.221 | ppl 1193.68 | wps 7956.8 | wpb 2034.1 | bsz 4 | num_updates 12736 | best_loss 9.246
2022-02-01 04:28:01 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-02-01 04:28:01 | INFO | train | epoch 199 | loss 5.091 | ppl 34.09 | wps 5925.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12736 | lr 0.00028021 | gnorm 1.314 | train_wall 324 | gb_free 6.1 | wall 70526
KL Stats: Epoch 199 Divergences: Uniform: 3.191141017214542 Unigram: 3.8772058656254824
2022-02-01 04:28:01 | INFO | fairseq.trainer | begin training epoch 200
2022-02-01 04:28:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:33:27 | INFO | train_inner | epoch 200:     64 / 64 loss=5.099, ppl=34.27, wps=6089.7, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=12800, lr=0.000279508, gnorm=1.341, train_wall=506, gb_free=6.1, wall=70851
2022-02-01 04:33:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:33:55 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 10.241 | ppl 1209.92 | wps 7858.5 | wpb 2034.1 | bsz 4 | num_updates 12800 | best_loss 9.246
2022-02-01 04:33:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 12800 updates
2022-02-01 04:33:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint200.pt
2022-02-01 04:33:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint200.pt
2022-02-01 04:33:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint200.pt (epoch 200 @ 12800 updates, score 10.241) (writing took 3.6377953849732876 seconds)
2022-02-01 04:33:59 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-02-01 04:33:59 | INFO | train | epoch 200 | loss 5.087 | ppl 33.99 | wps 5845.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12800 | lr 0.000279508 | gnorm 1.357 | train_wall 325 | gb_free 6.1 | wall 70883
KL Stats: Epoch 200 Divergences: Uniform: 3.190097251998759 Unigram: 3.876024122403243
2022-02-01 04:33:59 | INFO | fairseq.trainer | begin training epoch 201
2022-02-01 04:33:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:39:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:39:54 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 10.238 | ppl 1207.87 | wps 7851.9 | wpb 2034.1 | bsz 4 | num_updates 12864 | best_loss 9.246
2022-02-01 04:39:54 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-02-01 04:39:54 | INFO | train | epoch 201 | loss 5.084 | ppl 33.93 | wps 5872.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12864 | lr 0.000278812 | gnorm 1.335 | train_wall 327 | gb_free 6.1 | wall 71239
KL Stats: Epoch 201 Divergences: Uniform: 3.188080827236387 Unigram: 3.8778765306333094
2022-02-01 04:39:54 | INFO | fairseq.trainer | begin training epoch 202
2022-02-01 04:39:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:43:00 | INFO | train_inner | epoch 202:     36 / 64 loss=5.071, ppl=33.62, wps=5708.8, ups=0.17, wpb=32679.4, bsz=63.8, num_updates=12900, lr=0.000278423, gnorm=1.352, train_wall=512, gb_free=6.1, wall=71424
2022-02-01 04:45:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:45:51 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 10.206 | ppl 1181.42 | wps 7845.3 | wpb 2034.1 | bsz 4 | num_updates 12928 | best_loss 9.246
2022-02-01 04:45:51 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-02-01 04:45:51 | INFO | train | epoch 202 | loss 5.08 | ppl 33.82 | wps 5863.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12928 | lr 0.000278121 | gnorm 1.358 | train_wall 327 | gb_free 6.1 | wall 71595
KL Stats: Epoch 202 Divergences: Uniform: 3.1914965566718942 Unigram: 3.887394019142554
2022-02-01 04:45:51 | INFO | fairseq.trainer | begin training epoch 203
2022-02-01 04:45:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-01 04:51:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-01 04:51:47 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 10.225 | ppl 1196.91 | wps 7982.2 | wpb 2034.1 | bsz 4 | num_updates 12992 | best_loss 9.246
2022-02-01 04:51:47 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-02-01 04:51:47 | INFO | train | epoch 203 | loss 5.074 | ppl 33.68 | wps 5867.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12992 | lr 0.000277435 | gnorm 1.364 | train_wall 328 | gb_free 6.1 | wall 71951
KL Stats: Epoch 203 Divergences: Uniform: 3.19078468916631 Unigram: 3.887174998290582
2022-02-01 04:51:47 | INFO | fairseq.trainer | begin training epoch 204
2022-02-01 04:51:47 | INFO | fairseq_cli.train | Start iterating over samples
User defined signal 2
Sender: LSF System <lsfadmin@eu-g3-004>
Subject: Job 202993779: <w2_jelinek_0.09_0.01_0.9_#2> in cluster <euler> Exited

Job <w2_jelinek_0.09_0.01_0.9_#2> was submitted from host <eu-login-26> by user <andriusb> in cluster <euler> at Wed Feb  2 06:10:42 2022
Job was executed on host(s) <eu-g3-004>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Wed Feb  2 06:11:20 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Feb  2 06:11:20 2022
Terminated at Thu Feb  3 02:11:38 2022
Results reported at Thu Feb  3 02:11:38 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-2-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.09, 0.01, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 16 --save-interval 40 --seed 20002 --max-update 50000
------------------------------------------------------------

TERM_RUNLIMIT: job killed after reaching LSF run time limit.
Exited with exit code 140.

Resource usage summary:

    CPU time :                                   71987.21 sec.
    Max Memory :                                 4781 MB
    Average Memory :                             2703.64 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               15219.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   72017 sec.
    Turnaround time :                            72056 sec.

The output (if any) follows:

2022-02-02 06:11:26 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 20002, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 40, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.5, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-2-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 20002, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.09, 0.01, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.5, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-02 06:11:26 | INFO | fairseq.tasks.language_modeling | dictionary: 76624 types
2022-02-02 06:11:27 | INFO | fairseq.data.data_utils | loaded 36,718 examples from: data-bin/wikitext-2-raw-full/train
Calculating frequency stats:
  0%|          | 0/36718 [00:00<?, ?it/s]  3%|▎         | 1163/36718 [00:00<00:03, 11614.12it/s]  6%|▋         | 2325/36718 [00:00<00:03, 10467.24it/s] 10%|▉         | 3594/36718 [00:00<00:02, 11422.34it/s] 13%|█▎        | 4867/36718 [00:00<00:02, 11916.03it/s] 17%|█▋        | 6147/36718 [00:00<00:02, 12222.70it/s] 20%|██        | 7375/36718 [00:00<00:02, 11508.80it/s] 23%|██▎       | 8537/36718 [00:00<00:02, 11186.21it/s] 26%|██▋       | 9693/36718 [00:00<00:02, 11296.56it/s] 29%|██▉       | 10829/36718 [00:00<00:02, 11262.11it/s] 33%|███▎      | 12030/36718 [00:01<00:02, 11478.84it/s] 36%|███▌      | 13182/36718 [00:01<00:02, 11331.66it/s] 39%|███▉      | 14390/36718 [00:01<00:01, 11541.28it/s] 42%|████▏     | 15547/36718 [00:01<00:01, 11428.47it/s] 45%|████▌     | 16692/36718 [00:01<00:01, 11091.82it/s] 49%|████▊     | 17888/36718 [00:01<00:01, 11336.49it/s] 52%|█████▏    | 19094/36718 [00:01<00:01, 11540.14it/s] 55%|█████▌    | 20251/36718 [00:01<00:01, 11523.94it/s] 58%|█████▊    | 21406/36718 [00:01<00:01, 11231.91it/s] 62%|██████▏   | 22599/36718 [00:01<00:01, 11434.94it/s] 65%|██████▌   | 23869/36718 [00:02<00:01, 11805.03it/s] 69%|██████▊   | 25212/36718 [00:02<00:00, 12284.91it/s] 72%|███████▏  | 26443/36718 [00:02<00:00, 11728.07it/s] 75%|███████▌  | 27623/36718 [00:02<00:00, 11289.69it/s] 79%|███████▊  | 28842/36718 [00:02<00:00, 11544.32it/s] 82%|████████▏ | 30003/36718 [00:02<00:00, 11486.23it/s] 85%|████████▍ | 31156/36718 [00:02<00:00, 10859.28it/s] 88%|████████▊ | 32251/36718 [00:02<00:00, 10826.59it/s] 91%|█████████ | 33340/36718 [00:02<00:00, 10827.13it/s] 94%|█████████▍| 34537/36718 [00:03<00:00, 11152.45it/s] 97%|█████████▋| 35657/36718 [00:03<00:00, 11063.17it/s]100%|██████████| 36718/36718 [00:03<00:00, 11366.13it/s]

gathering stats for n=1
  0%|          | 0/36718 [00:00<?, ?it/s]  5%|▌         | 2012/36718 [00:00<00:01, 20101.42it/s] 12%|█▏        | 4249/36718 [00:00<00:01, 21432.23it/s] 18%|█▊        | 6496/36718 [00:00<00:01, 21897.70it/s] 24%|██▎       | 8686/36718 [00:00<00:01, 21323.05it/s] 30%|██▉       | 10834/36718 [00:00<00:01, 21376.30it/s] 35%|███▌      | 12974/36718 [00:00<00:01, 21355.91it/s] 41%|████▏     | 15231/36718 [00:00<00:00, 21743.11it/s] 47%|████▋     | 17407/36718 [00:00<00:00, 21296.82it/s] 53%|█████▎    | 19641/36718 [00:00<00:00, 21613.65it/s] 59%|█████▉    | 21805/36718 [00:01<00:00, 21189.76it/s] 66%|██████▌   | 24137/36718 [00:01<00:00, 21822.39it/s] 72%|███████▏  | 26323/36718 [00:01<00:00, 21732.32it/s] 78%|███████▊  | 28499/36718 [00:01<00:00, 21522.12it/s] 83%|████████▎ | 30654/36718 [00:01<00:00, 21134.21it/s] 89%|████████▉ | 32770/36718 [00:01<00:00, 20680.78it/s] 95%|█████████▌| 34915/36718 [00:01<00:00, 20896.29it/s]100%|██████████| 36718/36718 [00:01<00:00, 21250.11it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 302.14it/s]2022-02-02 06:11:36 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(76624, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=76624, bias=False)
  )
)
2022-02-02 06:11:36 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-02 06:11:36 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-02 06:11:36 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-02-02 06:11:36 | INFO | fairseq_cli.train | num. shared model params: 58,145,792 (num. trained: 58,145,792)
2022-02-02 06:11:36 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-02 06:11:36 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-2-raw-full/valid
2022-02-02 06:11:36 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-02 06:11:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-02 06:11:36 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-02-02 06:11:36 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-02 06:11:36 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-02 06:11:36 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-02-02 06:11:36 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint_last.pt
2022-02-02 06:11:36 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint_last.pt
2022-02-02 06:11:36 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-02 06:11:36 | INFO | fairseq.data.data_utils | loaded 36,718 examples from: data-bin/wikitext-2-raw-full/train
2022-02-02 06:11:36 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp
2022-02-02 06:11:36 | INFO | fairseq.trainer | begin training epoch 1
2022-02-02 06:11:36 | INFO | fairseq_cli.train | Start iterating over samples

2022-02-02 06:17:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
/cluster/home/andriusb/fq/fairseq/fairseq/utils.py:372: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2022-02-02 06:17:32 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.667 | ppl 26021 | wps 8003.8 | wpb 2034.1 | bsz 4 | num_updates 64
2022-02-02 06:17:32 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-02 06:17:32 | INFO | train | epoch 001 | loss 16.119 | ppl 71170.9 | wps 5916.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 64 | lr 8.0984e-06 | gnorm 3.157 | train_wall 324 | gb_free 6.1 | wall 356
KL Stats: Epoch 1 Divergences: Uniform: 0.5181508472216589 Unigram: 3.6924147420706244
2022-02-02 06:17:32 | INFO | fairseq.trainer | begin training epoch 2
2022-02-02 06:17:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 06:20:36 | INFO | train_inner | epoch 002:     36 / 64 loss=15.585, ppl=49158.5, wps=6089.5, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=100, lr=1.25975e-05, gnorm=2.629, train_wall=507, gb_free=6.1, wall=540
2022-02-02 06:22:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 06:23:25 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.573 | ppl 12186.3 | wps 7998.5 | wpb 2034.1 | bsz 4 | num_updates 128
2022-02-02 06:23:25 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-02 06:23:25 | INFO | train | epoch 002 | loss 14.411 | ppl 21776.9 | wps 5916.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 128 | lr 1.60968e-05 | gnorm 1.585 | train_wall 324 | gb_free 6.1 | wall 709
KL Stats: Epoch 2 Divergences: Uniform: 0.5219036608974086 Unigram: 2.425390205381184
2022-02-02 06:23:25 | INFO | fairseq.trainer | begin training epoch 3
2022-02-02 06:23:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 06:28:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 06:29:18 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 12.747 | ppl 6872.11 | wps 7927 | wpb 2034.1 | bsz 4 | num_updates 192
2022-02-02 06:29:18 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-02 06:29:18 | INFO | train | epoch 003 | loss 13.475 | ppl 11386.7 | wps 5917.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 192 | lr 2.40952e-05 | gnorm 1.292 | train_wall 324 | gb_free 6.1 | wall 1062
KL Stats: Epoch 3 Divergences: Uniform: 0.5079609465186351 Unigram: 1.723944299230563
2022-02-02 06:29:18 | INFO | fairseq.trainer | begin training epoch 4
2022-02-02 06:29:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 06:29:59 | INFO | train_inner | epoch 004:      8 / 64 loss=13.611, ppl=12509.5, wps=5789.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=200, lr=2.5095e-05, gnorm=1.326, train_wall=506, gb_free=6.1, wall=1103
2022-02-02 06:34:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 06:35:10 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.915 | ppl 3861.86 | wps 7984.8 | wpb 2034.1 | bsz 4 | num_updates 256
2022-02-02 06:35:10 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-02 06:35:10 | INFO | train | epoch 004 | loss 12.52 | ppl 5874.38 | wps 5927.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 256 | lr 3.20936e-05 | gnorm 1.026 | train_wall 323 | gb_free 6.1 | wall 1414
KL Stats: Epoch 4 Divergences: Uniform: 0.5968590479821986 Unigram: 1.0935956084405483
2022-02-02 06:35:10 | INFO | fairseq.trainer | begin training epoch 5
2022-02-02 06:35:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 06:38:54 | INFO | train_inner | epoch 005:     44 / 64 loss=12.175, ppl=4623, wps=6102.6, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=300, lr=3.75925e-05, gnorm=0.899, train_wall=506, gb_free=6.1, wall=1638
2022-02-02 06:40:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 06:41:02 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 11.4 | ppl 2701.97 | wps 7970.3 | wpb 2034.1 | bsz 4 | num_updates 320
2022-02-02 06:41:02 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-02 06:41:02 | INFO | train | epoch 005 | loss 11.718 | ppl 3368.93 | wps 5934.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 320 | lr 4.0092e-05 | gnorm 0.728 | train_wall 323 | gb_free 6.1 | wall 1766
KL Stats: Epoch 5 Divergences: Uniform: 0.8452211994094103 Unigram: 0.6204730733151019
2022-02-02 06:41:02 | INFO | fairseq.trainer | begin training epoch 6
2022-02-02 06:41:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 06:46:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 06:46:54 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.184 | ppl 2326.27 | wps 7974.7 | wpb 2034.1 | bsz 4 | num_updates 384
2022-02-02 06:46:54 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-02 06:46:54 | INFO | train | epoch 006 | loss 11.286 | ppl 2496.38 | wps 5926.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 384 | lr 4.80904e-05 | gnorm 0.578 | train_wall 323 | gb_free 6.1 | wall 2119
KL Stats: Epoch 6 Divergences: Uniform: 1.1657886452744708 Unigram: 0.4142561443530344
2022-02-02 06:46:54 | INFO | fairseq.trainer | begin training epoch 7
2022-02-02 06:46:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 06:48:16 | INFO | train_inner | epoch 007:     16 / 64 loss=11.304, ppl=2528.59, wps=5799.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=400, lr=5.009e-05, gnorm=0.591, train_wall=505, gb_free=6.1, wall=2201
2022-02-02 06:52:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 06:52:47 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.064 | ppl 2140.78 | wps 8008.2 | wpb 2034.1 | bsz 4 | num_updates 448
2022-02-02 06:52:47 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-02 06:52:47 | INFO | train | epoch 007 | loss 11.087 | ppl 2174.98 | wps 5926.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 448 | lr 5.60888e-05 | gnorm 0.521 | train_wall 323 | gb_free 6.1 | wall 2471
KL Stats: Epoch 7 Divergences: Uniform: 1.3956437264343817 Unigram: 0.4407133404752492
2022-02-02 06:52:47 | INFO | fairseq.trainer | begin training epoch 8
2022-02-02 06:52:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 06:57:12 | INFO | train_inner | epoch 008:     52 / 64 loss=11.024, ppl=2082.98, wps=6097.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=500, lr=6.25875e-05, gnorm=0.503, train_wall=506, gb_free=6.1, wall=2736
2022-02-02 06:58:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 06:58:40 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.958 | ppl 1989.01 | wps 7960.3 | wpb 2034.1 | bsz 4 | num_updates 512
2022-02-02 06:58:40 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-02 06:58:40 | INFO | train | epoch 008 | loss 10.971 | ppl 2007.88 | wps 5922.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 512 | lr 6.40872e-05 | gnorm 0.503 | train_wall 324 | gb_free 6.1 | wall 2824
KL Stats: Epoch 8 Divergences: Uniform: 1.5141560336086308 Unigram: 0.5208845827455686
2022-02-02 06:58:40 | INFO | fairseq.trainer | begin training epoch 9
2022-02-02 06:58:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:04:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 07:04:33 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.849 | ppl 1844.13 | wps 7940.9 | wpb 2034.1 | bsz 4 | num_updates 576
2022-02-02 07:04:33 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-02 07:04:33 | INFO | train | epoch 009 | loss 10.868 | ppl 1868.63 | wps 5913.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 576 | lr 7.20856e-05 | gnorm 0.496 | train_wall 324 | gb_free 6.1 | wall 3177
KL Stats: Epoch 9 Divergences: Uniform: 1.566327553455744 Unigram: 0.6165342466385694
2022-02-02 07:04:33 | INFO | fairseq.trainer | begin training epoch 10
2022-02-02 07:04:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:06:36 | INFO | train_inner | epoch 010:     24 / 64 loss=10.854, ppl=1850.4, wps=5786.9, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=600, lr=7.5085e-05, gnorm=0.505, train_wall=506, gb_free=6.1, wall=3300
2022-02-02 07:09:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 07:10:26 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.74 | ppl 1710.22 | wps 7997.1 | wpb 2034.1 | bsz 4 | num_updates 640
2022-02-02 07:10:26 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-02 07:10:26 | INFO | train | epoch 010 | loss 10.758 | ppl 1731.71 | wps 5912.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 640 | lr 8.0084e-05 | gnorm 0.505 | train_wall 324 | gb_free 6.1 | wall 3530
KL Stats: Epoch 10 Divergences: Uniform: 1.5922945991813264 Unigram: 0.7241834033292185
2022-02-02 07:10:26 | INFO | fairseq.trainer | begin training epoch 11
2022-02-02 07:10:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:15:33 | INFO | train_inner | epoch 011:     60 / 64 loss=10.684, ppl=1645, wps=6084.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=700, lr=8.75825e-05, gnorm=0.489, train_wall=507, gb_free=6.1, wall=3837
2022-02-02 07:15:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:16:19 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.643 | ppl 1598.63 | wps 7959.2 | wpb 2034.1 | bsz 4 | num_updates 704
2022-02-02 07:16:19 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-02 07:16:19 | INFO | train | epoch 011 | loss 10.644 | ppl 1599.93 | wps 5914.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 704 | lr 8.80824e-05 | gnorm 0.489 | train_wall 324 | gb_free 6.1 | wall 3883
KL Stats: Epoch 11 Divergences: Uniform: 1.608104747081406 Unigram: 0.8333381222345223
2022-02-02 07:16:19 | INFO | fairseq.trainer | begin training epoch 12
2022-02-02 07:16:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:21:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:22:12 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.535 | ppl 1483.74 | wps 7977.8 | wpb 2034.1 | bsz 4 | num_updates 768
2022-02-02 07:22:12 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-02 07:22:12 | INFO | train | epoch 012 | loss 10.532 | ppl 1480.89 | wps 5915.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 768 | lr 9.60808e-05 | gnorm 0.545 | train_wall 324 | gb_free 6.1 | wall 4237
KL Stats: Epoch 12 Divergences: Uniform: 1.6249796121955393 Unigram: 0.9305080969599264
2022-02-02 07:22:12 | INFO | fairseq.trainer | begin training epoch 13
2022-02-02 07:22:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:24:56 | INFO | train_inner | epoch 013:     32 / 64 loss=10.507, ppl=1455.16, wps=5788.4, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=800, lr=0.00010008, gnorm=0.524, train_wall=506, gb_free=6.1, wall=4400
2022-02-02 07:27:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 07:28:05 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 10.436 | ppl 1385.46 | wps 7974.4 | wpb 2034.1 | bsz 4 | num_updates 832
2022-02-02 07:28:05 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-02 07:28:05 | INFO | train | epoch 013 | loss 10.418 | ppl 1367.81 | wps 5914.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 832 | lr 0.000104079 | gnorm 0.493 | train_wall 324 | gb_free 6.1 | wall 4590
KL Stats: Epoch 13 Divergences: Uniform: 1.6480556014797922 Unigram: 1.0171287116399088
2022-02-02 07:28:05 | INFO | fairseq.trainer | begin training epoch 14
2022-02-02 07:28:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:33:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:33:59 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 10.38 | ppl 1332.59 | wps 7970 | wpb 2034.1 | bsz 4 | num_updates 896
2022-02-02 07:33:59 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-02 07:33:59 | INFO | train | epoch 014 | loss 10.307 | ppl 1266.48 | wps 5911.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 896 | lr 0.000112078 | gnorm 0.531 | train_wall 324 | gb_free 6.1 | wall 4943
KL Stats: Epoch 14 Divergences: Uniform: 1.671335578440985 Unigram: 1.0995424429787248
2022-02-02 07:33:59 | INFO | fairseq.trainer | begin training epoch 15
2022-02-02 07:33:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:34:19 | INFO | train_inner | epoch 015:      4 / 64 loss=10.331, ppl=1288.22, wps=5786.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=900, lr=0.000112578, gnorm=0.524, train_wall=506, gb_free=6.1, wall=4963
2022-02-02 07:39:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:39:51 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 10.274 | ppl 1237.88 | wps 7980.2 | wpb 2034.1 | bsz 4 | num_updates 960
2022-02-02 07:39:51 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-02 07:39:51 | INFO | train | epoch 015 | loss 10.197 | ppl 1173.73 | wps 5920.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 960 | lr 0.000120076 | gnorm 0.533 | train_wall 324 | gb_free 6.1 | wall 5296
KL Stats: Epoch 15 Divergences: Uniform: 1.705320279015153 Unigram: 1.1719093011635122
2022-02-02 07:39:51 | INFO | fairseq.trainer | begin training epoch 16
2022-02-02 07:39:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:43:16 | INFO | train_inner | epoch 016:     40 / 64 loss=10.153, ppl=1138.67, wps=6089.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=1000, lr=0.000125075, gnorm=0.518, train_wall=507, gb_free=6.1, wall=5500
2022-02-02 07:45:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:45:45 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 10.191 | ppl 1169.35 | wps 7962.2 | wpb 2034.1 | bsz 4 | num_updates 1024
2022-02-02 07:45:45 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-02 07:45:45 | INFO | train | epoch 016 | loss 10.085 | ppl 1085.95 | wps 5915.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1024 | lr 0.000128074 | gnorm 0.498 | train_wall 324 | gb_free 6.1 | wall 5649
KL Stats: Epoch 16 Divergences: Uniform: 1.736748878923036 Unigram: 1.2415802386195407
2022-02-02 07:45:45 | INFO | fairseq.trainer | begin training epoch 17
2022-02-02 07:45:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:51:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 07:51:38 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 10.099 | ppl 1096.93 | wps 7944.7 | wpb 2034.1 | bsz 4 | num_updates 1088
2022-02-02 07:51:38 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-02 07:51:38 | INFO | train | epoch 017 | loss 9.977 | ppl 1007.65 | wps 5913.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1088 | lr 0.000136073 | gnorm 0.506 | train_wall 324 | gb_free 6.1 | wall 6002
KL Stats: Epoch 17 Divergences: Uniform: 1.767284607072433 Unigram: 1.310497310867428
2022-02-02 07:51:38 | INFO | fairseq.trainer | begin training epoch 18
2022-02-02 07:51:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 07:52:39 | INFO | train_inner | epoch 018:     12 / 64 loss=9.991, ppl=1017.58, wps=5785.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=1100, lr=0.000137573, gnorm=0.506, train_wall=506, gb_free=6.1, wall=6064
2022-02-02 07:57:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 07:57:31 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 10.025 | ppl 1042.16 | wps 7953.2 | wpb 2034.1 | bsz 4 | num_updates 1152
2022-02-02 07:57:31 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-02 07:57:31 | INFO | train | epoch 018 | loss 9.873 | ppl 937.88 | wps 5917.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1152 | lr 0.000144071 | gnorm 0.552 | train_wall 324 | gb_free 6.1 | wall 6355
KL Stats: Epoch 18 Divergences: Uniform: 1.8023523125604053 Unigram: 1.3734696110354176
2022-02-02 07:57:31 | INFO | fairseq.trainer | begin training epoch 19
2022-02-02 07:57:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:01:36 | INFO | train_inner | epoch 019:     48 / 64 loss=9.82, ppl=903.64, wps=6084.4, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=1200, lr=0.00015007, gnorm=0.531, train_wall=507, gb_free=6.1, wall=6601
2022-02-02 08:02:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 08:03:24 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.975 | ppl 1006.52 | wps 7958.2 | wpb 2034.1 | bsz 4 | num_updates 1216
2022-02-02 08:03:24 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-02 08:03:24 | INFO | train | epoch 019 | loss 9.77 | ppl 873 | wps 5904.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1216 | lr 0.00015207 | gnorm 0.509 | train_wall 325 | gb_free 6.1 | wall 6709
KL Stats: Epoch 19 Divergences: Uniform: 1.834345629413129 Unigram: 1.4398646540923272
2022-02-02 08:03:24 | INFO | fairseq.trainer | begin training epoch 20
2022-02-02 08:03:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:08:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:09:18 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.895 | ppl 951.95 | wps 7956.8 | wpb 2034.1 | bsz 4 | num_updates 1280
2022-02-02 08:09:18 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-02 08:09:18 | INFO | train | epoch 020 | loss 9.674 | ppl 817.15 | wps 5912 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1280 | lr 0.000160068 | gnorm 0.545 | train_wall 324 | gb_free 6.1 | wall 7062
KL Stats: Epoch 20 Divergences: Uniform: 1.869839910819591 Unigram: 1.4942823056374663
2022-02-02 08:09:18 | INFO | fairseq.trainer | begin training epoch 21
2022-02-02 08:09:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:11:00 | INFO | train_inner | epoch 021:     20 / 64 loss=9.677, ppl=818.71, wps=5781.2, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=1300, lr=0.000162568, gnorm=0.538, train_wall=506, gb_free=6.1, wall=7165
2022-02-02 08:14:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:15:11 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 9.843 | ppl 918.59 | wps 7950.1 | wpb 2034.1 | bsz 4 | num_updates 1344
2022-02-02 08:15:11 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-02 08:15:11 | INFO | train | epoch 021 | loss 9.584 | ppl 767.28 | wps 5912.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1344 | lr 0.000168066 | gnorm 0.551 | train_wall 324 | gb_free 6.1 | wall 7415
KL Stats: Epoch 21 Divergences: Uniform: 1.89039626885561 Unigram: 1.5513802947227657
2022-02-02 08:15:11 | INFO | fairseq.trainer | begin training epoch 22
2022-02-02 08:15:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:19:57 | INFO | train_inner | epoch 022:     56 / 64 loss=9.528, ppl=738.02, wps=6090.7, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=1400, lr=0.000175065, gnorm=0.552, train_wall=507, gb_free=6.1, wall=7701
2022-02-02 08:20:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:21:04 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 9.788 | ppl 884.26 | wps 7947.5 | wpb 2034.1 | bsz 4 | num_updates 1408
2022-02-02 08:21:04 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-02 08:21:04 | INFO | train | epoch 022 | loss 9.493 | ppl 720.52 | wps 5920.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1408 | lr 0.000176065 | gnorm 0.539 | train_wall 324 | gb_free 6.1 | wall 7768
KL Stats: Epoch 22 Divergences: Uniform: 1.9192537755580992 Unigram: 1.6010088371373365
2022-02-02 08:21:04 | INFO | fairseq.trainer | begin training epoch 23
2022-02-02 08:21:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:26:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:26:57 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 9.726 | ppl 846.82 | wps 7972 | wpb 2034.1 | bsz 4 | num_updates 1472
2022-02-02 08:26:57 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-02 08:26:57 | INFO | train | epoch 023 | loss 9.405 | ppl 678.03 | wps 5917.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1472 | lr 0.000184063 | gnorm 0.51 | train_wall 324 | gb_free 6.1 | wall 8121
KL Stats: Epoch 23 Divergences: Uniform: 1.9495561455177615 Unigram: 1.646188275844745
2022-02-02 08:26:57 | INFO | fairseq.trainer | begin training epoch 24
2022-02-02 08:26:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:29:20 | INFO | train_inner | epoch 024:     28 / 64 loss=9.392, ppl=671.84, wps=5790.6, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=1500, lr=0.000187563, gnorm=0.524, train_wall=506, gb_free=6.1, wall=8264
2022-02-02 08:32:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 08:32:50 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 9.674 | ppl 816.76 | wps 7961.7 | wpb 2034.1 | bsz 4 | num_updates 1536
2022-02-02 08:32:50 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-02 08:32:50 | INFO | train | epoch 024 | loss 9.324 | ppl 641 | wps 5916.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1536 | lr 0.000192062 | gnorm 0.551 | train_wall 324 | gb_free 6.1 | wall 8474
KL Stats: Epoch 24 Divergences: Uniform: 1.9719878228251477 Unigram: 1.6889669729030454
2022-02-02 08:32:50 | INFO | fairseq.trainer | begin training epoch 25
2022-02-02 08:32:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:38:15 | INFO | train_inner | epoch 025:     64 / 64 loss=9.269, ppl=617.16, wps=6087.2, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=1600, lr=0.00020006, gnorm=0.539, train_wall=506, gb_free=6.1, wall=8800
2022-02-02 08:38:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:38:43 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 9.633 | ppl 794.26 | wps 7938.2 | wpb 2034.1 | bsz 4 | num_updates 1600
2022-02-02 08:38:43 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-02 08:38:43 | INFO | train | epoch 025 | loss 9.243 | ppl 605.81 | wps 5916.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1600 | lr 0.00020006 | gnorm 0.533 | train_wall 324 | gb_free 6.1 | wall 8827
KL Stats: Epoch 25 Divergences: Uniform: 1.9949694904178394 Unigram: 1.7335619271410025
2022-02-02 08:38:43 | INFO | fairseq.trainer | begin training epoch 26
2022-02-02 08:38:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:44:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:44:35 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 9.582 | ppl 766.36 | wps 7986.5 | wpb 2034.1 | bsz 4 | num_updates 1664
2022-02-02 08:44:35 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-02 08:44:35 | INFO | train | epoch 026 | loss 9.159 | ppl 571.73 | wps 5924.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1664 | lr 0.000208058 | gnorm 0.513 | train_wall 324 | gb_free 6.1 | wall 9180
KL Stats: Epoch 26 Divergences: Uniform: 2.0251857236497304 Unigram: 1.774749302409037
2022-02-02 08:44:35 | INFO | fairseq.trainer | begin training epoch 27
2022-02-02 08:44:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:47:39 | INFO | train_inner | epoch 027:     36 / 64 loss=9.131, ppl=560.75, wps=5794.6, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=1700, lr=0.000212558, gnorm=0.516, train_wall=506, gb_free=6.1, wall=9364
2022-02-02 08:50:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:50:29 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 9.55 | ppl 749.49 | wps 7985.1 | wpb 2034.1 | bsz 4 | num_updates 1728
2022-02-02 08:50:29 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-02 08:50:29 | INFO | train | epoch 027 | loss 9.081 | ppl 541.55 | wps 5913.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1728 | lr 0.000216057 | gnorm 0.53 | train_wall 324 | gb_free 6.1 | wall 9533
KL Stats: Epoch 27 Divergences: Uniform: 2.0470729169451922 Unigram: 1.812297027374552
2022-02-02 08:50:29 | INFO | fairseq.trainer | begin training epoch 28
2022-02-02 08:50:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:55:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 08:56:21 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 9.502 | ppl 724.88 | wps 7976.9 | wpb 2034.1 | bsz 4 | num_updates 1792
2022-02-02 08:56:21 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-02 08:56:21 | INFO | train | epoch 028 | loss 9.001 | ppl 512.33 | wps 5920.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1792 | lr 0.000224055 | gnorm 0.524 | train_wall 324 | gb_free 6.1 | wall 9886
KL Stats: Epoch 28 Divergences: Uniform: 2.06851692618765 Unigram: 1.848819948226888
2022-02-02 08:56:21 | INFO | fairseq.trainer | begin training epoch 29
2022-02-02 08:56:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 08:57:02 | INFO | train_inner | epoch 029:      8 / 64 loss=9.015, ppl=517.22, wps=5790.6, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=1800, lr=0.000225055, gnorm=0.528, train_wall=506, gb_free=6.1, wall=9927
2022-02-02 09:01:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:02:15 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 9.468 | ppl 708.19 | wps 8007.3 | wpb 2034.1 | bsz 4 | num_updates 1856
2022-02-02 09:02:15 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-02 09:02:15 | INFO | train | epoch 029 | loss 8.922 | ppl 485.19 | wps 5909.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1856 | lr 0.000232054 | gnorm 0.515 | train_wall 325 | gb_free 6.1 | wall 10239
KL Stats: Epoch 29 Divergences: Uniform: 2.0947845112309156 Unigram: 1.8850112153962089
2022-02-02 09:02:15 | INFO | fairseq.trainer | begin training epoch 30
2022-02-02 09:02:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:05:59 | INFO | train_inner | epoch 030:     44 / 64 loss=8.889, ppl=474.15, wps=6087.3, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=1900, lr=0.000237553, gnorm=0.517, train_wall=507, gb_free=6.1, wall=10463
2022-02-02 09:07:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:08:07 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 9.446 | ppl 697.65 | wps 7971.8 | wpb 2034.1 | bsz 4 | num_updates 1920
2022-02-02 09:08:07 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-02 09:08:07 | INFO | train | epoch 030 | loss 8.846 | ppl 460.07 | wps 5922.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1920 | lr 0.000240052 | gnorm 0.516 | train_wall 324 | gb_free 6.1 | wall 10592
KL Stats: Epoch 30 Divergences: Uniform: 2.116989067380713 Unigram: 1.9206195924211173
2022-02-02 09:08:07 | INFO | fairseq.trainer | begin training epoch 31
2022-02-02 09:08:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:13:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 09:14:00 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 9.401 | ppl 676.08 | wps 7998.8 | wpb 2034.1 | bsz 4 | num_updates 1984
2022-02-02 09:14:00 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-02 09:14:00 | INFO | train | epoch 031 | loss 8.771 | ppl 436.8 | wps 5920.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 1984 | lr 0.00024805 | gnorm 0.524 | train_wall 324 | gb_free 6.1 | wall 10945
KL Stats: Epoch 31 Divergences: Uniform: 2.1390425723973667 Unigram: 1.9540864481589448
2022-02-02 09:14:00 | INFO | fairseq.trainer | begin training epoch 32
2022-02-02 09:14:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:15:22 | INFO | train_inner | epoch 032:     16 / 64 loss=8.775, ppl=437.99, wps=5793, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=2000, lr=0.00025005, gnorm=0.523, train_wall=505, gb_free=6.1, wall=11026
2022-02-02 09:19:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:19:52 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 9.372 | ppl 662.43 | wps 7988.6 | wpb 2034.1 | bsz 4 | num_updates 2048
2022-02-02 09:19:52 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-02 09:19:52 | INFO | train | epoch 032 | loss 8.693 | ppl 413.92 | wps 5931.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2048 | lr 0.000256049 | gnorm 0.519 | train_wall 323 | gb_free 6.1 | wall 11297
KL Stats: Epoch 32 Divergences: Uniform: 2.1590920102051294 Unigram: 1.9884326129926875
2022-02-02 09:19:52 | INFO | fairseq.trainer | begin training epoch 33
2022-02-02 09:19:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:24:17 | INFO | train_inner | epoch 033:     52 / 64 loss=8.65, ppl=401.83, wps=6101.4, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=2100, lr=0.000262548, gnorm=0.509, train_wall=506, gb_free=6.1, wall=11562
2022-02-02 09:25:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:25:45 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 9.352 | ppl 653.47 | wps 7980.6 | wpb 2034.1 | bsz 4 | num_updates 2112
2022-02-02 09:25:45 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-02-02 09:25:45 | INFO | train | epoch 033 | loss 8.619 | ppl 393.28 | wps 5924.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2112 | lr 0.000264047 | gnorm 0.506 | train_wall 324 | gb_free 6.1 | wall 11649
KL Stats: Epoch 33 Divergences: Uniform: 2.1792100143601933 Unigram: 2.02202643884641
2022-02-02 09:25:45 | INFO | fairseq.trainer | begin training epoch 34
2022-02-02 09:25:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:31:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:31:37 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 9.325 | ppl 641.55 | wps 8008.8 | wpb 2034.1 | bsz 4 | num_updates 2176
2022-02-02 09:31:37 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-02-02 09:31:37 | INFO | train | epoch 034 | loss 8.547 | ppl 373.96 | wps 5936.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2176 | lr 0.000272046 | gnorm 0.5 | train_wall 323 | gb_free 6.1 | wall 12001
KL Stats: Epoch 34 Divergences: Uniform: 2.2014853129956227 Unigram: 2.0534536618420245
2022-02-02 09:31:37 | INFO | fairseq.trainer | begin training epoch 35
2022-02-02 09:31:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:33:39 | INFO | train_inner | epoch 035:     24 / 64 loss=8.542, ppl=372.61, wps=5806.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=2200, lr=0.000275045, gnorm=0.495, train_wall=504, gb_free=6.1, wall=12123
2022-02-02 09:37:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:37:29 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 9.302 | ppl 631.34 | wps 7994.1 | wpb 2034.1 | bsz 4 | num_updates 2240
2022-02-02 09:37:29 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-02-02 09:37:29 | INFO | train | epoch 035 | loss 8.474 | ppl 355.55 | wps 5932.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2240 | lr 0.000280044 | gnorm 0.494 | train_wall 323 | gb_free 6.1 | wall 12353
KL Stats: Epoch 35 Divergences: Uniform: 2.2295950301424745 Unigram: 2.0855258739740647
2022-02-02 09:37:29 | INFO | fairseq.trainer | begin training epoch 36
2022-02-02 09:37:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:42:34 | INFO | train_inner | epoch 036:     60 / 64 loss=8.431, ppl=345.04, wps=6104.4, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=2300, lr=0.000287543, gnorm=0.508, train_wall=506, gb_free=6.1, wall=12659
2022-02-02 09:42:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:43:21 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 9.283 | ppl 622.83 | wps 7997.7 | wpb 2034.1 | bsz 4 | num_updates 2304
2022-02-02 09:43:21 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-02-02 09:43:21 | INFO | train | epoch 036 | loss 8.403 | ppl 338.48 | wps 5931.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2304 | lr 0.000288042 | gnorm 0.513 | train_wall 323 | gb_free 6.1 | wall 12705
KL Stats: Epoch 36 Divergences: Uniform: 2.2535941088617615 Unigram: 2.11613213989505
2022-02-02 09:43:21 | INFO | fairseq.trainer | begin training epoch 37
2022-02-02 09:43:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:48:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 09:49:13 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 9.259 | ppl 612.77 | wps 7974.6 | wpb 2034.1 | bsz 4 | num_updates 2368
2022-02-02 09:49:13 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-02-02 09:49:13 | INFO | train | epoch 037 | loss 8.333 | ppl 322.4 | wps 5925.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2368 | lr 0.000296041 | gnorm 0.508 | train_wall 323 | gb_free 6.1 | wall 13058
KL Stats: Epoch 37 Divergences: Uniform: 2.271590650637204 Unigram: 2.1468684021441398
2022-02-02 09:49:13 | INFO | fairseq.trainer | begin training epoch 38
2022-02-02 09:49:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 09:51:56 | INFO | train_inner | epoch 038:     32 / 64 loss=8.313, ppl=318.03, wps=5799.2, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=2400, lr=0.00030004, gnorm=0.505, train_wall=505, gb_free=6.1, wall=13221
2022-02-02 09:54:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 09:55:06 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 9.217 | ppl 594.92 | wps 7976 | wpb 2034.1 | bsz 4 | num_updates 2432
2022-02-02 09:55:06 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-02-02 09:55:06 | INFO | train | epoch 038 | loss 8.262 | ppl 307.01 | wps 5929.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2432 | lr 0.000304039 | gnorm 0.49 | train_wall 323 | gb_free 6.1 | wall 13410
KL Stats: Epoch 38 Divergences: Uniform: 2.294073616617053 Unigram: 2.1751034635466784
2022-02-02 09:55:06 | INFO | fairseq.trainer | begin training epoch 39
2022-02-02 09:55:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:00:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:00:58 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 9.227 | ppl 599.27 | wps 7983.5 | wpb 2034.1 | bsz 4 | num_updates 2496
2022-02-02 10:00:58 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-02-02 10:00:58 | INFO | train | epoch 039 | loss 8.196 | ppl 293.24 | wps 5923.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2496 | lr 0.000312038 | gnorm 0.499 | train_wall 324 | gb_free 6.1 | wall 13762
KL Stats: Epoch 39 Divergences: Uniform: 2.3099565144067755 Unigram: 2.207526473643459
2022-02-02 10:00:58 | INFO | fairseq.trainer | begin training epoch 40
2022-02-02 10:00:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:01:19 | INFO | train_inner | epoch 040:      4 / 64 loss=8.213, ppl=296.78, wps=5798.6, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=2500, lr=0.000312538, gnorm=0.494, train_wall=505, gb_free=6.1, wall=13783
2022-02-02 10:06:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:06:50 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 9.243 | ppl 606.07 | wps 7981.8 | wpb 2034.1 | bsz 4 | num_updates 2560
2022-02-02 10:06:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 2560 updates
2022-02-02 10:06:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint40.pt
2022-02-02 10:06:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint40.pt
2022-02-02 10:06:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint40.pt (epoch 40 @ 2560 updates, score 9.243) (writing took 5.205136084929109 seconds)
2022-02-02 10:06:56 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-02-02 10:06:56 | INFO | train | epoch 040 | loss 8.13 | ppl 280.23 | wps 5844.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2560 | lr 0.000320036 | gnorm 0.515 | train_wall 323 | gb_free 6.1 | wall 14120
KL Stats: Epoch 40 Divergences: Uniform: 2.331223084246165 Unigram: 2.235732244058974
2022-02-02 10:06:56 | INFO | fairseq.trainer | begin training epoch 41
2022-02-02 10:06:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:10:19 | INFO | train_inner | epoch 041:     40 / 64 loss=8.102, ppl=274.74, wps=6044.5, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=2600, lr=0.000325035, gnorm=0.506, train_wall=506, gb_free=6.1, wall=14324
2022-02-02 10:12:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:12:48 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 9.202 | ppl 588.94 | wps 7972.4 | wpb 2034.1 | bsz 4 | num_updates 2624 | best_loss 9.202
2022-02-02 10:12:48 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-02-02 10:12:48 | INFO | train | epoch 041 | loss 8.064 | ppl 267.54 | wps 5932 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2624 | lr 0.000328034 | gnorm 0.504 | train_wall 323 | gb_free 6.1 | wall 14472
KL Stats: Epoch 41 Divergences: Uniform: 2.349534604690152 Unigram: 2.262333010018692
2022-02-02 10:12:48 | INFO | fairseq.trainer | begin training epoch 42
2022-02-02 10:12:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:18:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:18:39 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 9.204 | ppl 589.82 | wps 7993.2 | wpb 2034.1 | bsz 4 | num_updates 2688 | best_loss 9.204
2022-02-02 10:18:39 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-02-02 10:18:39 | INFO | train | epoch 042 | loss 7.999 | ppl 255.74 | wps 5941 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2688 | lr 0.000336033 | gnorm 0.489 | train_wall 323 | gb_free 6.1 | wall 14824
KL Stats: Epoch 42 Divergences: Uniform: 2.3762621403375417 Unigram: 2.2883122910196074
2022-02-02 10:18:39 | INFO | fairseq.trainer | begin training epoch 43
2022-02-02 10:18:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:19:40 | INFO | train_inner | epoch 043:     12 / 64 loss=8.008, ppl=257.38, wps=5809.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=2700, lr=0.000337533, gnorm=0.495, train_wall=504, gb_free=6.1, wall=14885
2022-02-02 10:24:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:24:31 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 9.208 | ppl 591.55 | wps 7980.8 | wpb 2034.1 | bsz 4 | num_updates 2752 | best_loss 9.208
2022-02-02 10:24:31 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-02-02 10:24:31 | INFO | train | epoch 043 | loss 7.935 | ppl 244.78 | wps 5935.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2752 | lr 0.000344031 | gnorm 0.491 | train_wall 323 | gb_free 6.1 | wall 15175
KL Stats: Epoch 43 Divergences: Uniform: 2.3874285929053825 Unigram: 2.3116627823126565
2022-02-02 10:24:31 | INFO | fairseq.trainer | begin training epoch 44
2022-02-02 10:24:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:28:36 | INFO | train_inner | epoch 044:     48 / 64 loss=7.906, ppl=239.88, wps=6105.7, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=2800, lr=0.00035003, gnorm=0.503, train_wall=505, gb_free=6.1, wall=15420
2022-02-02 10:29:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:30:23 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 9.165 | ppl 574.08 | wps 7986.1 | wpb 2034.1 | bsz 4 | num_updates 2816 | best_loss 9.165
2022-02-02 10:30:23 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-02-02 10:30:23 | INFO | train | epoch 044 | loss 7.875 | ppl 234.71 | wps 5931 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2816 | lr 0.00035203 | gnorm 0.506 | train_wall 323 | gb_free 6.1 | wall 15528
KL Stats: Epoch 44 Divergences: Uniform: 2.408187881978522 Unigram: 2.3395944860740285
2022-02-02 10:30:23 | INFO | fairseq.trainer | begin training epoch 45
2022-02-02 10:30:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:35:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:36:15 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 9.178 | ppl 579.21 | wps 7988.9 | wpb 2034.1 | bsz 4 | num_updates 2880 | best_loss 9.178
2022-02-02 10:36:15 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-02-02 10:36:15 | INFO | train | epoch 045 | loss 7.816 | ppl 225.34 | wps 5943.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2880 | lr 0.000360028 | gnorm 0.5 | train_wall 322 | gb_free 6.1 | wall 15879
KL Stats: Epoch 45 Divergences: Uniform: 2.4292974514374985 Unigram: 2.3600787096324094
2022-02-02 10:36:15 | INFO | fairseq.trainer | begin training epoch 46
2022-02-02 10:36:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:37:56 | INFO | train_inner | epoch 046:     20 / 64 loss=7.808, ppl=224.06, wps=5813.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=2900, lr=0.000362528, gnorm=0.499, train_wall=504, gb_free=6.1, wall=15981
2022-02-02 10:41:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:42:06 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 9.217 | ppl 595.24 | wps 8002.5 | wpb 2034.1 | bsz 4 | num_updates 2944 | best_loss 9.217
2022-02-02 10:42:06 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-02-02 10:42:06 | INFO | train | epoch 046 | loss 7.756 | ppl 216.16 | wps 5936.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 2944 | lr 0.000368026 | gnorm 0.51 | train_wall 323 | gb_free 6.1 | wall 16231
KL Stats: Epoch 46 Divergences: Uniform: 2.451115851607579 Unigram: 2.3907253842749596
2022-02-02 10:42:06 | INFO | fairseq.trainer | begin training epoch 47
2022-02-02 10:42:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:46:52 | INFO | train_inner | epoch 047:     56 / 64 loss=7.728, ppl=212.03, wps=6106.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3000, lr=0.000375025, gnorm=0.507, train_wall=505, gb_free=6.1, wall=16516
2022-02-02 10:47:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:47:58 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 9.222 | ppl 597.37 | wps 8026.9 | wpb 2034.1 | bsz 4 | num_updates 3008 | best_loss 9.222
2022-02-02 10:47:58 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-02-02 10:47:58 | INFO | train | epoch 047 | loss 7.696 | ppl 207.3 | wps 5936.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3008 | lr 0.000376025 | gnorm 0.503 | train_wall 323 | gb_free 6.1 | wall 16583
KL Stats: Epoch 47 Divergences: Uniform: 2.4607563809294515 Unigram: 2.4148695502765856
2022-02-02 10:47:58 | INFO | fairseq.trainer | begin training epoch 48
2022-02-02 10:47:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:53:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:53:51 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 9.188 | ppl 583.15 | wps 7995.3 | wpb 2034.1 | bsz 4 | num_updates 3072 | best_loss 9.188
2022-02-02 10:53:51 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-02-02 10:53:51 | INFO | train | epoch 048 | loss 7.638 | ppl 199.19 | wps 5929.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3072 | lr 0.000384023 | gnorm 0.498 | train_wall 323 | gb_free 6.1 | wall 16935
KL Stats: Epoch 48 Divergences: Uniform: 2.480493284150118 Unigram: 2.437043817107856
2022-02-02 10:53:51 | INFO | fairseq.trainer | begin training epoch 49
2022-02-02 10:53:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 10:56:13 | INFO | train_inner | epoch 049:     28 / 64 loss=7.619, ppl=196.54, wps=5805.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=3100, lr=0.000387523, gnorm=0.494, train_wall=504, gb_free=6.1, wall=17078
2022-02-02 10:59:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 10:59:43 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 9.224 | ppl 597.88 | wps 8008.9 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 9.224
2022-02-02 10:59:43 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-02-02 10:59:43 | INFO | train | epoch 049 | loss 7.582 | ppl 191.54 | wps 5933.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3136 | lr 0.000392022 | gnorm 0.497 | train_wall 323 | gb_free 6.1 | wall 17287
KL Stats: Epoch 49 Divergences: Uniform: 2.4973981529672677 Unigram: 2.4589017965610536
2022-02-02 10:59:43 | INFO | fairseq.trainer | begin training epoch 50
2022-02-02 10:59:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:05:07 | INFO | train_inner | epoch 050:     64 / 64 loss=7.556, ppl=188.16, wps=6112.4, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=3200, lr=0.00040002, gnorm=0.504, train_wall=504, gb_free=6.1, wall=17611
2022-02-02 11:05:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:05:34 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 9.217 | ppl 594.97 | wps 8004.7 | wpb 2034.1 | bsz 4 | num_updates 3200 | best_loss 9.217
2022-02-02 11:05:34 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-02-02 11:05:34 | INFO | train | epoch 050 | loss 7.527 | ppl 184.38 | wps 5945.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3200 | lr 0.00040002 | gnorm 0.503 | train_wall 322 | gb_free 6.1 | wall 17638
KL Stats: Epoch 50 Divergences: Uniform: 2.520803852817825 Unigram: 2.478977715127795
2022-02-02 11:05:34 | INFO | fairseq.trainer | begin training epoch 51
2022-02-02 11:05:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:10:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:11:25 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 9.232 | ppl 601.3 | wps 7985.1 | wpb 2034.1 | bsz 4 | num_updates 3264 | best_loss 9.232
2022-02-02 11:11:25 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-02-02 11:11:25 | INFO | train | epoch 051 | loss 7.475 | ppl 177.9 | wps 5940.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3264 | lr 0.000408018 | gnorm 0.517 | train_wall 323 | gb_free 6.1 | wall 17990
KL Stats: Epoch 51 Divergences: Uniform: 2.521127303547188 Unigram: 2.4967042615664465
2022-02-02 11:11:25 | INFO | fairseq.trainer | begin training epoch 52
2022-02-02 11:11:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:14:29 | INFO | train_inner | epoch 052:     36 / 64 loss=7.445, ppl=174.28, wps=5811.4, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=3300, lr=0.000412518, gnorm=0.513, train_wall=505, gb_free=6.1, wall=18173
2022-02-02 11:16:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:17:18 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 9.255 | ppl 610.91 | wps 7989.3 | wpb 2034.1 | bsz 4 | num_updates 3328 | best_loss 9.243
2022-02-02 11:17:18 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-02-02 11:17:18 | INFO | train | epoch 052 | loss 7.422 | ppl 171.55 | wps 5931.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3328 | lr 0.000416017 | gnorm 0.513 | train_wall 323 | gb_free 6.1 | wall 18342
KL Stats: Epoch 52 Divergences: Uniform: 2.543735090489394 Unigram: 2.520642338569462
2022-02-02 11:17:18 | INFO | fairseq.trainer | begin training epoch 53
2022-02-02 11:17:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:22:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:23:09 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 9.251 | ppl 609.48 | wps 8002.8 | wpb 2034.1 | bsz 4 | num_updates 3392 | best_loss 9.243
2022-02-02 11:23:09 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-02-02 11:23:09 | INFO | train | epoch 053 | loss 7.369 | ppl 165.27 | wps 5944.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3392 | lr 0.000424015 | gnorm 0.51 | train_wall 322 | gb_free 6.1 | wall 18693
KL Stats: Epoch 53 Divergences: Uniform: 2.562493088824242 Unigram: 2.545015173385898
2022-02-02 11:23:09 | INFO | fairseq.trainer | begin training epoch 54
2022-02-02 11:23:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:23:50 | INFO | train_inner | epoch 054:      8 / 64 loss=7.389, ppl=167.64, wps=5814.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=3400, lr=0.000425015, gnorm=0.514, train_wall=503, gb_free=6.1, wall=18734
2022-02-02 11:28:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:29:01 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 9.264 | ppl 614.81 | wps 8010.2 | wpb 2034.1 | bsz 4 | num_updates 3456 | best_loss 9.243
2022-02-02 11:29:01 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-02-02 11:29:01 | INFO | train | epoch 054 | loss 7.318 | ppl 159.61 | wps 5939.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3456 | lr 0.000432014 | gnorm 0.502 | train_wall 323 | gb_free 6.1 | wall 19045
KL Stats: Epoch 54 Divergences: Uniform: 2.5748169461304062 Unigram: 2.5603791483232725
2022-02-02 11:29:01 | INFO | fairseq.trainer | begin training epoch 55
2022-02-02 11:29:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:32:45 | INFO | train_inner | epoch 055:     44 / 64 loss=7.292, ppl=156.76, wps=6107, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=3500, lr=0.000437513, gnorm=0.502, train_wall=505, gb_free=6.1, wall=19269
2022-02-02 11:34:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:34:52 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 9.268 | ppl 616.7 | wps 8017.7 | wpb 2034.1 | bsz 4 | num_updates 3520 | best_loss 9.243
2022-02-02 11:34:52 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-02-02 11:34:52 | INFO | train | epoch 055 | loss 7.271 | ppl 154.44 | wps 5936.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3520 | lr 0.000440012 | gnorm 0.511 | train_wall 323 | gb_free 6.1 | wall 19397
KL Stats: Epoch 55 Divergences: Uniform: 2.5889764606714567 Unigram: 2.5816304294257706
2022-02-02 11:34:52 | INFO | fairseq.trainer | begin training epoch 56
2022-02-02 11:34:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:40:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:40:44 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.254 | ppl 610.62 | wps 7988.8 | wpb 2034.1 | bsz 4 | num_updates 3584 | best_loss 9.243
2022-02-02 11:40:44 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-02-02 11:40:44 | INFO | train | epoch 056 | loss 7.222 | ppl 149.31 | wps 5939.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3584 | lr 0.00044801 | gnorm 0.523 | train_wall 323 | gb_free 6.1 | wall 19748
KL Stats: Epoch 56 Divergences: Uniform: 2.6102376200070236 Unigram: 2.6027153161595358
2022-02-02 11:40:44 | INFO | fairseq.trainer | begin training epoch 57
2022-02-02 11:40:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:42:06 | INFO | train_inner | epoch 057:     16 / 64 loss=7.222, ppl=149.34, wps=5812.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=3600, lr=0.00045001, gnorm=0.52, train_wall=504, gb_free=6.1, wall=19830
2022-02-02 11:46:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:46:36 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.313 | ppl 635.97 | wps 8006.5 | wpb 2034.1 | bsz 4 | num_updates 3648 | best_loss 9.243
2022-02-02 11:46:36 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-02-02 11:46:36 | INFO | train | epoch 057 | loss 7.176 | ppl 144.6 | wps 5931.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3648 | lr 0.000456009 | gnorm 0.515 | train_wall 323 | gb_free 6.1 | wall 20100
KL Stats: Epoch 57 Divergences: Uniform: 2.6147294400058954 Unigram: 2.6215391091331077
2022-02-02 11:46:36 | INFO | fairseq.trainer | begin training epoch 58
2022-02-02 11:46:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:51:01 | INFO | train_inner | epoch 058:     52 / 64 loss=7.155, ppl=142.56, wps=6108, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=3700, lr=0.000462508, gnorm=0.527, train_wall=505, gb_free=6.1, wall=20365
2022-02-02 11:52:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:52:28 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.337 | ppl 646.77 | wps 7991.5 | wpb 2034.1 | bsz 4 | num_updates 3712 | best_loss 9.243
2022-02-02 11:52:28 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-02-02 11:52:28 | INFO | train | epoch 058 | loss 7.129 | ppl 140.02 | wps 5940.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3712 | lr 0.000464007 | gnorm 0.535 | train_wall 323 | gb_free 6.1 | wall 20452
KL Stats: Epoch 58 Divergences: Uniform: 2.6303901290343275 Unigram: 2.6410329554592087
2022-02-02 11:52:28 | INFO | fairseq.trainer | begin training epoch 59
2022-02-02 11:52:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 11:57:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 11:58:19 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.27 | ppl 617.19 | wps 8038.3 | wpb 2034.1 | bsz 4 | num_updates 3776 | best_loss 9.243
2022-02-02 11:58:19 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-02-02 11:58:19 | INFO | train | epoch 059 | loss 7.084 | ppl 135.69 | wps 5947.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3776 | lr 0.000472006 | gnorm 0.528 | train_wall 322 | gb_free 6.1 | wall 20803
KL Stats: Epoch 59 Divergences: Uniform: 2.6567768537357974 Unigram: 2.6600107827082877
2022-02-02 11:58:19 | INFO | fairseq.trainer | begin training epoch 60
2022-02-02 11:58:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:00:21 | INFO | train_inner | epoch 060:     24 / 64 loss=7.074, ppl=134.76, wps=5816.5, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=3800, lr=0.000475005, gnorm=0.529, train_wall=503, gb_free=6.1, wall=20925
2022-02-02 12:03:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 12:04:10 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.365 | ppl 659.24 | wps 8038.9 | wpb 2034.1 | bsz 4 | num_updates 3840 | best_loss 9.243
2022-02-02 12:04:10 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-02-02 12:04:10 | INFO | train | epoch 060 | loss 7.038 | ppl 131.45 | wps 5947.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3840 | lr 0.000480004 | gnorm 0.532 | train_wall 322 | gb_free 6.1 | wall 21154
KL Stats: Epoch 60 Divergences: Uniform: 2.660053583975638 Unigram: 2.6813288943480766
2022-02-02 12:04:10 | INFO | fairseq.trainer | begin training epoch 61
2022-02-02 12:04:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:09:15 | INFO | train_inner | epoch 061:     60 / 64 loss=7.02, ppl=129.8, wps=6118.6, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=3900, lr=0.000487503, gnorm=0.544, train_wall=504, gb_free=6.1, wall=21459
2022-02-02 12:09:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:10:01 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.332 | ppl 644.42 | wps 8001.3 | wpb 2034.1 | bsz 4 | num_updates 3904 | best_loss 9.243
2022-02-02 12:10:01 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-02-02 12:10:01 | INFO | train | epoch 061 | loss 6.998 | ppl 127.82 | wps 5944.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3904 | lr 0.000488002 | gnorm 0.551 | train_wall 322 | gb_free 6.1 | wall 21506
KL Stats: Epoch 61 Divergences: Uniform: 2.6763970506916137 Unigram: 2.7005093954850765
2022-02-02 12:10:01 | INFO | fairseq.trainer | begin training epoch 62
2022-02-02 12:10:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:15:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:15:53 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.368 | ppl 660.92 | wps 7982.3 | wpb 2034.1 | bsz 4 | num_updates 3968 | best_loss 9.243
2022-02-02 12:15:53 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-02-02 12:15:53 | INFO | train | epoch 062 | loss 6.953 | ppl 123.92 | wps 5941.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 3968 | lr 0.000496001 | gnorm 0.535 | train_wall 322 | gb_free 6.1 | wall 21857
KL Stats: Epoch 62 Divergences: Uniform: 2.68377928441755 Unigram: 2.7153074175211995
2022-02-02 12:15:53 | INFO | fairseq.trainer | begin training epoch 63
2022-02-02 12:15:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:18:36 | INFO | train_inner | epoch 063:     32 / 64 loss=6.929, ppl=121.82, wps=5816.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=4000, lr=0.0005, gnorm=0.538, train_wall=503, gb_free=6.1, wall=22020
2022-02-02 12:21:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:21:44 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.312 | ppl 635.43 | wps 8015.6 | wpb 2034.1 | bsz 4 | num_updates 4032 | best_loss 9.243
2022-02-02 12:21:44 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-02-02 12:21:44 | INFO | train | epoch 063 | loss 6.912 | ppl 120.46 | wps 5946.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4032 | lr 0.000498012 | gnorm 0.545 | train_wall 322 | gb_free 6.1 | wall 22209
KL Stats: Epoch 63 Divergences: Uniform: 2.7110205795095066 Unigram: 2.734286559088411
2022-02-02 12:21:44 | INFO | fairseq.trainer | begin training epoch 64
2022-02-02 12:21:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:27:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:27:36 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.357 | ppl 655.59 | wps 7998.9 | wpb 2034.1 | bsz 4 | num_updates 4096 | best_loss 9.243
2022-02-02 12:27:36 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-02-02 12:27:36 | INFO | train | epoch 064 | loss 6.869 | ppl 116.87 | wps 5945.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4096 | lr 0.000494106 | gnorm 0.542 | train_wall 322 | gb_free 6.1 | wall 22560
KL Stats: Epoch 64 Divergences: Uniform: 2.7142401878113116 Unigram: 2.7514494813604986
2022-02-02 12:27:36 | INFO | fairseq.trainer | begin training epoch 65
2022-02-02 12:27:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:27:56 | INFO | train_inner | epoch 065:      4 / 64 loss=6.896, ppl=119.06, wps=5819, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4100, lr=0.000493865, gnorm=0.543, train_wall=503, gb_free=6.1, wall=22580
2022-02-02 12:33:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:33:28 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.398 | ppl 674.49 | wps 7966.5 | wpb 2034.1 | bsz 4 | num_updates 4160 | best_loss 9.243
2022-02-02 12:33:28 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-02-02 12:33:28 | INFO | train | epoch 065 | loss 6.828 | ppl 113.63 | wps 5933.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4160 | lr 0.00049029 | gnorm 0.543 | train_wall 323 | gb_free 6.1 | wall 22912
KL Stats: Epoch 65 Divergences: Uniform: 2.7293703961361793 Unigram: 2.775064506056676
2022-02-02 12:33:28 | INFO | fairseq.trainer | begin training epoch 66
2022-02-02 12:33:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:36:51 | INFO | train_inner | epoch 066:     40 / 64 loss=6.801, ppl=111.48, wps=6107.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4200, lr=0.00048795, gnorm=0.547, train_wall=505, gb_free=6.1, wall=23115
2022-02-02 12:38:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:39:19 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.443 | ppl 696.18 | wps 7998.9 | wpb 2034.1 | bsz 4 | num_updates 4224 | best_loss 9.243
2022-02-02 12:39:19 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-02-02 12:39:19 | INFO | train | epoch 066 | loss 6.785 | ppl 110.27 | wps 5946.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4224 | lr 0.000486562 | gnorm 0.55 | train_wall 322 | gb_free 6.1 | wall 23263
KL Stats: Epoch 66 Divergences: Uniform: 2.7422483550459873 Unigram: 2.787974651686308
2022-02-02 12:39:19 | INFO | fairseq.trainer | begin training epoch 67
2022-02-02 12:39:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:44:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:45:10 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.385 | ppl 668.37 | wps 7990.9 | wpb 2034.1 | bsz 4 | num_updates 4288 | best_loss 9.243
2022-02-02 12:45:10 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-02-02 12:45:10 | INFO | train | epoch 067 | loss 6.745 | ppl 107.25 | wps 5945.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4288 | lr 0.000482917 | gnorm 0.543 | train_wall 322 | gb_free 6.1 | wall 23614
KL Stats: Epoch 67 Divergences: Uniform: 2.7501581732135745 Unigram: 2.8114159713572415
2022-02-02 12:45:10 | INFO | fairseq.trainer | begin training epoch 68
2022-02-02 12:45:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:46:11 | INFO | train_inner | epoch 068:     12 / 64 loss=6.755, ppl=107.98, wps=5818.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4300, lr=0.000482243, gnorm=0.542, train_wall=503, gb_free=6.1, wall=23675
2022-02-02 12:50:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:51:02 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.453 | ppl 700.87 | wps 8035.7 | wpb 2034.1 | bsz 4 | num_updates 4352 | best_loss 9.243
2022-02-02 12:51:02 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-02-02 12:51:02 | INFO | train | epoch 068 | loss 6.708 | ppl 104.52 | wps 5941.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4352 | lr 0.000479353 | gnorm 0.551 | train_wall 323 | gb_free 6.1 | wall 23966
KL Stats: Epoch 68 Divergences: Uniform: 2.7601851337600483 Unigram: 2.8224591116771878
2022-02-02 12:51:02 | INFO | fairseq.trainer | begin training epoch 69
2022-02-02 12:51:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 12:55:06 | INFO | train_inner | epoch 069:     48 / 64 loss=6.689, ppl=103.16, wps=6110.2, ups=0.19, wpb=32686.1, bsz=63.8, num_updates=4400, lr=0.000476731, gnorm=0.56, train_wall=505, gb_free=6.1, wall=24210
2022-02-02 12:56:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 12:56:54 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.512 | ppl 730.16 | wps 8005.7 | wpb 2034.1 | bsz 4 | num_updates 4416 | best_loss 9.243
2022-02-02 12:56:54 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-02-02 12:56:54 | INFO | train | epoch 069 | loss 6.669 | ppl 101.78 | wps 5933.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4416 | lr 0.000475867 | gnorm 0.556 | train_wall 323 | gb_free 6.1 | wall 24318
KL Stats: Epoch 69 Divergences: Uniform: 2.7706165394145965 Unigram: 2.8458014495416157
2022-02-02 12:56:54 | INFO | fairseq.trainer | begin training epoch 70
2022-02-02 12:56:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:02:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:02:46 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.453 | ppl 700.83 | wps 8011.1 | wpb 2034.1 | bsz 4 | num_updates 4480 | best_loss 9.243
2022-02-02 13:02:46 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-02-02 13:02:46 | INFO | train | epoch 070 | loss 6.632 | ppl 99.19 | wps 5934.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4480 | lr 0.000472456 | gnorm 0.558 | train_wall 323 | gb_free 6.1 | wall 24670
KL Stats: Epoch 70 Divergences: Uniform: 2.789733839098528 Unigram: 2.857332003788582
2022-02-02 13:02:46 | INFO | fairseq.trainer | begin training epoch 71
2022-02-02 13:02:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:04:27 | INFO | train_inner | epoch 071:     20 / 64 loss=6.628, ppl=98.93, wps=5809.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4500, lr=0.000471405, gnorm=0.556, train_wall=504, gb_free=6.1, wall=24771
2022-02-02 13:08:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:08:37 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.467 | ppl 707.67 | wps 7973.1 | wpb 2034.1 | bsz 4 | num_updates 4544 | best_loss 9.243
2022-02-02 13:08:37 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-02-02 13:08:37 | INFO | train | epoch 071 | loss 6.6 | ppl 96.97 | wps 5940.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4544 | lr 0.000469117 | gnorm 0.567 | train_wall 323 | gb_free 6.1 | wall 25021
KL Stats: Epoch 71 Divergences: Uniform: 2.792528659991566 Unigram: 2.875029902007386
2022-02-02 13:08:37 | INFO | fairseq.trainer | begin training epoch 72
2022-02-02 13:08:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:13:22 | INFO | train_inner | epoch 072:     56 / 64 loss=6.584, ppl=95.91, wps=6110, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=4600, lr=0.000466252, gnorm=0.566, train_wall=505, gb_free=6.1, wall=25306
2022-02-02 13:14:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:14:29 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.467 | ppl 707.94 | wps 8007.8 | wpb 2034.1 | bsz 4 | num_updates 4608 | best_loss 9.243
2022-02-02 13:14:29 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-02-02 13:14:29 | INFO | train | epoch 072 | loss 6.563 | ppl 94.57 | wps 5939.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4608 | lr 0.000465847 | gnorm 0.563 | train_wall 323 | gb_free 6.1 | wall 25373
KL Stats: Epoch 72 Divergences: Uniform: 2.8071683503975087 Unigram: 2.893045007415311
2022-02-02 13:14:29 | INFO | fairseq.trainer | begin training epoch 73
2022-02-02 13:14:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:19:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:20:20 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.517 | ppl 732.84 | wps 8019.9 | wpb 2034.1 | bsz 4 | num_updates 4672 | best_loss 9.243
2022-02-02 13:20:20 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-02-02 13:20:20 | INFO | train | epoch 073 | loss 6.527 | ppl 92.25 | wps 5938.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4672 | lr 0.000462646 | gnorm 0.552 | train_wall 323 | gb_free 6.1 | wall 25725
KL Stats: Epoch 73 Divergences: Uniform: 2.8109989729236613 Unigram: 2.90684959426013
2022-02-02 13:20:20 | INFO | fairseq.trainer | begin training epoch 74
2022-02-02 13:20:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:22:43 | INFO | train_inner | epoch 074:     28 / 64 loss=6.517, ppl=91.6, wps=5813, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=4700, lr=0.000461266, gnorm=0.559, train_wall=504, gb_free=6.1, wall=25867
2022-02-02 13:25:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:26:12 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.566 | ppl 757.88 | wps 7980.4 | wpb 2034.1 | bsz 4 | num_updates 4736 | best_loss 9.243
2022-02-02 13:26:12 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-02-02 13:26:12 | INFO | train | epoch 074 | loss 6.499 | ppl 90.43 | wps 5934.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4736 | lr 0.000459509 | gnorm 0.571 | train_wall 323 | gb_free 6.1 | wall 26077
KL Stats: Epoch 74 Divergences: Uniform: 2.824333480978033 Unigram: 2.924094715490904
2022-02-02 13:26:12 | INFO | fairseq.trainer | begin training epoch 75
2022-02-02 13:26:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:31:36 | INFO | train_inner | epoch 075:     64 / 64 loss=6.489, ppl=89.84, wps=6108.9, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=4800, lr=0.000456435, gnorm=0.576, train_wall=504, gb_free=6.1, wall=26401
2022-02-02 13:31:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 13:32:04 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.535 | ppl 741.85 | wps 8008.7 | wpb 2034.1 | bsz 4 | num_updates 4800 | best_loss 9.243
2022-02-02 13:32:04 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-02-02 13:32:04 | INFO | train | epoch 075 | loss 6.467 | ppl 88.47 | wps 5943.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4800 | lr 0.000456435 | gnorm 0.577 | train_wall 322 | gb_free 6.1 | wall 26428
KL Stats: Epoch 75 Divergences: Uniform: 2.8312584997228765 Unigram: 2.9435509838530924
2022-02-02 13:32:04 | INFO | fairseq.trainer | begin training epoch 76
2022-02-02 13:32:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:37:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 13:37:56 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 9.541 | ppl 745.13 | wps 8004.3 | wpb 2034.1 | bsz 4 | num_updates 4864 | best_loss 9.243
2022-02-02 13:37:56 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-02-02 13:37:56 | INFO | train | epoch 076 | loss 6.436 | ppl 86.58 | wps 5930.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4864 | lr 0.000453423 | gnorm 0.569 | train_wall 323 | gb_free 6.1 | wall 26780
KL Stats: Epoch 76 Divergences: Uniform: 2.8500425564782437 Unigram: 2.961182330589108
2022-02-02 13:37:56 | INFO | fairseq.trainer | begin training epoch 77
2022-02-02 13:37:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:40:59 | INFO | train_inner | epoch 077:     36 / 64 loss=6.417, ppl=85.44, wps=5806.8, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=4900, lr=0.000451754, gnorm=0.577, train_wall=506, gb_free=6.1, wall=26964
2022-02-02 13:43:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:43:48 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 9.549 | ppl 748.99 | wps 7982.2 | wpb 2034.1 | bsz 4 | num_updates 4928 | best_loss 9.243
2022-02-02 13:43:48 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-02-02 13:43:48 | INFO | train | epoch 077 | loss 6.409 | ppl 85 | wps 5927.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4928 | lr 0.000450469 | gnorm 0.589 | train_wall 323 | gb_free 6.1 | wall 27133
KL Stats: Epoch 77 Divergences: Uniform: 2.8527101297882256 Unigram: 2.972397860875407
2022-02-02 13:43:48 | INFO | fairseq.trainer | begin training epoch 78
2022-02-02 13:43:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:49:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 13:49:41 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 9.541 | ppl 745.06 | wps 8006.6 | wpb 2034.1 | bsz 4 | num_updates 4992 | best_loss 9.243
2022-02-02 13:49:41 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-02-02 13:49:41 | INFO | train | epoch 078 | loss 6.379 | ppl 83.25 | wps 5920.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 4992 | lr 0.000447572 | gnorm 0.585 | train_wall 324 | gb_free 6.1 | wall 27485
KL Stats: Epoch 78 Divergences: Uniform: 2.8627669670504567 Unigram: 2.9888550914556147
2022-02-02 13:49:41 | INFO | fairseq.trainer | begin training epoch 79
2022-02-02 13:49:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:50:22 | INFO | train_inner | epoch 079:      8 / 64 loss=6.391, ppl=83.93, wps=5793.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=5000, lr=0.000447214, gnorm=0.586, train_wall=505, gb_free=6.1, wall=27526
2022-02-02 13:55:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 13:55:33 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 9.566 | ppl 758.09 | wps 7985.2 | wpb 2034.1 | bsz 4 | num_updates 5056 | best_loss 9.243
2022-02-02 13:55:33 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-02-02 13:55:33 | INFO | train | epoch 079 | loss 6.351 | ppl 81.63 | wps 5929.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5056 | lr 0.00044473 | gnorm 0.589 | train_wall 323 | gb_free 6.1 | wall 27838
KL Stats: Epoch 79 Divergences: Uniform: 2.8693710664511256 Unigram: 3.0057237371570844
2022-02-02 13:55:33 | INFO | fairseq.trainer | begin training epoch 80
2022-02-02 13:55:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 13:59:18 | INFO | train_inner | epoch 080:     44 / 64 loss=6.335, ppl=80.72, wps=6101.2, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=5100, lr=0.000442807, gnorm=0.585, train_wall=506, gb_free=6.1, wall=28062
2022-02-02 14:00:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 14:01:26 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 9.584 | ppl 767.73 | wps 7959.4 | wpb 2034.1 | bsz 4 | num_updates 5120 | best_loss 9.243
2022-02-02 14:01:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 5120 updates
2022-02-02 14:01:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint80.pt
2022-02-02 14:01:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint80.pt
2022-02-02 14:01:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint80.pt (epoch 80 @ 5120 updates, score 9.584) (writing took 3.188158340752125 seconds)
2022-02-02 14:01:29 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-02-02 14:01:29 | INFO | train | epoch 080 | loss 6.326 | ppl 80.21 | wps 5871.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5120 | lr 0.000441942 | gnorm 0.591 | train_wall 323 | gb_free 6.1 | wall 28193
KL Stats: Epoch 80 Divergences: Uniform: 2.8754532524494927 Unigram: 3.0135488096365015
2022-02-02 14:01:29 | INFO | fairseq.trainer | begin training epoch 81
2022-02-02 14:01:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:06:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 14:07:21 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 9.563 | ppl 756.36 | wps 7982.2 | wpb 2034.1 | bsz 4 | num_updates 5184 | best_loss 9.243
2022-02-02 14:07:21 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-02-02 14:07:21 | INFO | train | epoch 081 | loss 6.3 | ppl 78.79 | wps 5927.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5184 | lr 0.000439205 | gnorm 0.601 | train_wall 323 | gb_free 6.1 | wall 28546
KL Stats: Epoch 81 Divergences: Uniform: 2.8912955169618475 Unigram: 3.036244151542518
2022-02-02 14:07:21 | INFO | fairseq.trainer | begin training epoch 82
2022-02-02 14:07:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:08:43 | INFO | train_inner | epoch 082:     16 / 64 loss=6.301, ppl=78.85, wps=5767.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=5200, lr=0.000438529, gnorm=0.597, train_wall=505, gb_free=6.1, wall=28627
2022-02-02 14:12:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 14:13:13 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.603 | ppl 777.75 | wps 7979.3 | wpb 2034.1 | bsz 4 | num_updates 5248 | best_loss 9.243
2022-02-02 14:13:13 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-02-02 14:13:13 | INFO | train | epoch 082 | loss 6.27 | ppl 77.18 | wps 5932.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5248 | lr 0.000436519 | gnorm 0.601 | train_wall 323 | gb_free 6.1 | wall 28898
KL Stats: Epoch 82 Divergences: Uniform: 2.8997601817249987 Unigram: 3.0401909216954026
2022-02-02 14:13:13 | INFO | fairseq.trainer | begin training epoch 83
2022-02-02 14:13:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:17:39 | INFO | train_inner | epoch 083:     52 / 64 loss=6.262, ppl=76.77, wps=6095.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=5300, lr=0.000434372, gnorm=0.61, train_wall=506, gb_free=6.1, wall=29163
2022-02-02 14:18:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 14:19:06 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 9.609 | ppl 781.12 | wps 7973 | wpb 2034.1 | bsz 4 | num_updates 5312 | best_loss 9.243
2022-02-02 14:19:06 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-02-02 14:19:06 | INFO | train | epoch 083 | loss 6.247 | ppl 75.97 | wps 5918.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5312 | lr 0.000433881 | gnorm 0.605 | train_wall 324 | gb_free 6.1 | wall 29251
KL Stats: Epoch 83 Divergences: Uniform: 2.9046715646651498 Unigram: 3.057977535435744
2022-02-02 14:19:06 | INFO | fairseq.trainer | begin training epoch 84
2022-02-02 14:19:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:24:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 14:24:59 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 9.64 | ppl 797.81 | wps 8015.2 | wpb 2034.1 | bsz 4 | num_updates 5376 | best_loss 9.243
2022-02-02 14:24:59 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-02-02 14:24:59 | INFO | train | epoch 084 | loss 6.223 | ppl 74.68 | wps 5922.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5376 | lr 0.000431291 | gnorm 0.617 | train_wall 324 | gb_free 6.1 | wall 29603
KL Stats: Epoch 84 Divergences: Uniform: 2.9090101009890392 Unigram: 3.0739574980534714
2022-02-02 14:24:59 | INFO | fairseq.trainer | begin training epoch 85
2022-02-02 14:24:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:27:01 | INFO | train_inner | epoch 085:     24 / 64 loss=6.209, ppl=73.99, wps=5798.5, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5400, lr=0.000430331, gnorm=0.611, train_wall=505, gb_free=6.1, wall=29725
2022-02-02 14:30:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 14:30:51 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 9.693 | ppl 827.94 | wps 7970.3 | wpb 2034.1 | bsz 4 | num_updates 5440 | best_loss 9.243
2022-02-02 14:30:51 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-02-02 14:30:51 | INFO | train | epoch 085 | loss 6.198 | ppl 73.41 | wps 5932.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5440 | lr 0.000428746 | gnorm 0.61 | train_wall 323 | gb_free 6.1 | wall 29955
KL Stats: Epoch 85 Divergences: Uniform: 2.9121552557083854 Unigram: 3.0840330945085643
2022-02-02 14:30:51 | INFO | fairseq.trainer | begin training epoch 86
2022-02-02 14:30:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:35:58 | INFO | train_inner | epoch 086:     60 / 64 loss=6.197, ppl=73.38, wps=6090.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5500, lr=0.000426401, gnorm=0.622, train_wall=507, gb_free=6.1, wall=30262
2022-02-02 14:36:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 14:36:44 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 9.678 | ppl 819.25 | wps 7969.2 | wpb 2034.1 | bsz 4 | num_updates 5504 | best_loss 9.243
2022-02-02 14:36:44 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-02-02 14:36:44 | INFO | train | epoch 086 | loss 6.177 | ppl 72.36 | wps 5912 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5504 | lr 0.000426246 | gnorm 0.628 | train_wall 324 | gb_free 6.1 | wall 30309
KL Stats: Epoch 86 Divergences: Uniform: 2.9237191561746747 Unigram: 3.0918645156101814
2022-02-02 14:36:44 | INFO | fairseq.trainer | begin training epoch 87
2022-02-02 14:36:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:42:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 14:42:37 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 9.708 | ppl 836.12 | wps 7960.2 | wpb 2034.1 | bsz 4 | num_updates 5568 | best_loss 9.243
2022-02-02 14:42:37 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-02-02 14:42:37 | INFO | train | epoch 087 | loss 6.152 | ppl 71.1 | wps 5923.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5568 | lr 0.00042379 | gnorm 0.62 | train_wall 323 | gb_free 6.1 | wall 30661
KL Stats: Epoch 87 Divergences: Uniform: 2.9242777925991663 Unigram: 3.115742946274271
2022-02-02 14:42:37 | INFO | fairseq.trainer | begin training epoch 88
2022-02-02 14:42:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:45:20 | INFO | train_inner | epoch 088:     32 / 64 loss=6.136, ppl=70.3, wps=5795.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=5600, lr=0.000422577, gnorm=0.623, train_wall=505, gb_free=6.1, wall=30825
2022-02-02 14:48:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 14:48:30 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 9.7 | ppl 831.79 | wps 7970.6 | wpb 2034.1 | bsz 4 | num_updates 5632 | best_loss 9.243
2022-02-02 14:48:30 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-02-02 14:48:30 | INFO | train | epoch 088 | loss 6.131 | ppl 70.1 | wps 5922.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5632 | lr 0.000421375 | gnorm 0.632 | train_wall 324 | gb_free 6.1 | wall 31014
KL Stats: Epoch 88 Divergences: Uniform: 2.934160890761239 Unigram: 3.1201417004855747
2022-02-02 14:48:30 | INFO | fairseq.trainer | begin training epoch 89
2022-02-02 14:48:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:53:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 14:54:22 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 9.634 | ppl 794.52 | wps 8007.6 | wpb 2034.1 | bsz 4 | num_updates 5696 | best_loss 9.243
2022-02-02 14:54:22 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-02-02 14:54:22 | INFO | train | epoch 089 | loss 6.112 | ppl 69.15 | wps 5925 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5696 | lr 0.000419001 | gnorm 0.628 | train_wall 324 | gb_free 6.1 | wall 31366
KL Stats: Epoch 89 Divergences: Uniform: 2.9415530252604816 Unigram: 3.1443557761480583
2022-02-02 14:54:22 | INFO | fairseq.trainer | begin training epoch 90
2022-02-02 14:54:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 14:54:43 | INFO | train_inner | epoch 090:      4 / 64 loss=6.128, ppl=69.92, wps=5797.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5700, lr=0.000418854, gnorm=0.633, train_wall=505, gb_free=6.1, wall=31387
2022-02-02 14:59:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:00:14 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 9.714 | ppl 839.76 | wps 7997.5 | wpb 2034.1 | bsz 4 | num_updates 5760 | best_loss 9.243
2022-02-02 15:00:14 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-02-02 15:00:14 | INFO | train | epoch 090 | loss 6.092 | ppl 68.22 | wps 5929.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5760 | lr 0.000416667 | gnorm 0.652 | train_wall 323 | gb_free 6.1 | wall 31719
KL Stats: Epoch 90 Divergences: Uniform: 2.944327602691244 Unigram: 3.1487692619396537
2022-02-02 15:00:14 | INFO | fairseq.trainer | begin training epoch 91
2022-02-02 15:00:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:03:38 | INFO | train_inner | epoch 091:     40 / 64 loss=6.075, ppl=67.42, wps=6100.2, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=5800, lr=0.000415227, gnorm=0.644, train_wall=506, gb_free=6.1, wall=31923
2022-02-02 15:05:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:06:07 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 9.667 | ppl 813.2 | wps 7971.1 | wpb 2034.1 | bsz 4 | num_updates 5824 | best_loss 9.243
2022-02-02 15:06:07 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-02-02 15:06:07 | INFO | train | epoch 091 | loss 6.07 | ppl 67.16 | wps 5926.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5824 | lr 0.000414371 | gnorm 0.643 | train_wall 323 | gb_free 6.1 | wall 32071
KL Stats: Epoch 91 Divergences: Uniform: 2.9495978173392645 Unigram: 3.1595909545752825
2022-02-02 15:06:07 | INFO | fairseq.trainer | begin training epoch 92
2022-02-02 15:06:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:11:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 15:11:59 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 9.706 | ppl 835.46 | wps 7966.1 | wpb 2034.1 | bsz 4 | num_updates 5888 | best_loss 9.243
2022-02-02 15:11:59 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-02-02 15:11:59 | INFO | train | epoch 092 | loss 6.048 | ppl 66.16 | wps 5925.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5888 | lr 0.000412113 | gnorm 0.639 | train_wall 323 | gb_free 6.1 | wall 32424
KL Stats: Epoch 92 Divergences: Uniform: 2.957596921941941 Unigram: 3.1699151334580558
2022-02-02 15:11:59 | INFO | fairseq.trainer | begin training epoch 93
2022-02-02 15:11:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:13:00 | INFO | train_inner | epoch 093:     12 / 64 loss=6.053, ppl=66.39, wps=5799.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=5900, lr=0.000411693, gnorm=0.646, train_wall=505, gb_free=6.1, wall=32485
2022-02-02 15:17:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:17:51 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 9.742 | ppl 856.36 | wps 7972.5 | wpb 2034.1 | bsz 4 | num_updates 5952 | best_loss 9.243
2022-02-02 15:17:51 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-02-02 15:17:51 | INFO | train | epoch 093 | loss 6.029 | ppl 65.32 | wps 5932.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 5952 | lr 0.000409891 | gnorm 0.663 | train_wall 323 | gb_free 6.1 | wall 32776
KL Stats: Epoch 93 Divergences: Uniform: 2.9617125299100713 Unigram: 3.1822964029739658
2022-02-02 15:17:51 | INFO | fairseq.trainer | begin training epoch 94
2022-02-02 15:17:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:21:56 | INFO | train_inner | epoch 094:     48 / 64 loss=6.021, ppl=64.95, wps=6099.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=6000, lr=0.000408248, gnorm=0.662, train_wall=506, gb_free=6.1, wall=33021
2022-02-02 15:23:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:23:44 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 9.714 | ppl 839.78 | wps 7973.1 | wpb 2034.1 | bsz 4 | num_updates 6016 | best_loss 9.243
2022-02-02 15:23:44 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-02-02 15:23:44 | INFO | train | epoch 094 | loss 6.012 | ppl 64.52 | wps 5924.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6016 | lr 0.000407705 | gnorm 0.661 | train_wall 324 | gb_free 6.1 | wall 33128
KL Stats: Epoch 94 Divergences: Uniform: 2.963795058101852 Unigram: 3.1981597304397087
2022-02-02 15:23:44 | INFO | fairseq.trainer | begin training epoch 95
2022-02-02 15:23:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:29:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:29:36 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 9.743 | ppl 856.91 | wps 7992.3 | wpb 2034.1 | bsz 4 | num_updates 6080 | best_loss 9.243
2022-02-02 15:29:36 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-02-02 15:29:36 | INFO | train | epoch 095 | loss 5.99 | ppl 63.56 | wps 5924.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6080 | lr 0.000405554 | gnorm 0.652 | train_wall 324 | gb_free 6.1 | wall 33481
KL Stats: Epoch 95 Divergences: Uniform: 2.974456531230048 Unigram: 3.205265152681895
2022-02-02 15:29:36 | INFO | fairseq.trainer | begin training epoch 96
2022-02-02 15:29:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:31:19 | INFO | train_inner | epoch 096:     20 / 64 loss=5.987, ppl=63.42, wps=5797.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=6100, lr=0.000404888, gnorm=0.654, train_wall=505, gb_free=6.1, wall=33583
2022-02-02 15:35:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:35:29 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 9.772 | ppl 874.61 | wps 7986.8 | wpb 2034.1 | bsz 4 | num_updates 6144 | best_loss 9.243
2022-02-02 15:35:29 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-02-02 15:35:29 | INFO | train | epoch 096 | loss 5.974 | ppl 62.84 | wps 5928.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6144 | lr 0.000403436 | gnorm 0.66 | train_wall 323 | gb_free 6.1 | wall 33833
KL Stats: Epoch 96 Divergences: Uniform: 2.974886109973926 Unigram: 3.217300693564451
2022-02-02 15:35:29 | INFO | fairseq.trainer | begin training epoch 97
2022-02-02 15:35:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:40:14 | INFO | train_inner | epoch 097:     56 / 64 loss=5.971, ppl=62.71, wps=6097.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=6200, lr=0.00040161, gnorm=0.672, train_wall=506, gb_free=6.1, wall=34119
2022-02-02 15:40:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:41:21 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 9.705 | ppl 834.44 | wps 8010.3 | wpb 2034.1 | bsz 4 | num_updates 6208 | best_loss 9.243
2022-02-02 15:41:21 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-02-02 15:41:21 | INFO | train | epoch 097 | loss 5.956 | ppl 62.08 | wps 5924 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6208 | lr 0.000401351 | gnorm 0.676 | train_wall 324 | gb_free 6.1 | wall 34186
KL Stats: Epoch 97 Divergences: Uniform: 2.986686181450652 Unigram: 3.2319272522179223
2022-02-02 15:41:21 | INFO | fairseq.trainer | begin training epoch 98
2022-02-02 15:41:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:46:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 15:47:13 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 9.768 | ppl 872.08 | wps 7974 | wpb 2034.1 | bsz 4 | num_updates 6272 | best_loss 9.243
2022-02-02 15:47:13 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-02-02 15:47:13 | INFO | train | epoch 098 | loss 5.938 | ppl 61.32 | wps 5931.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6272 | lr 0.000399298 | gnorm 0.671 | train_wall 323 | gb_free 6.1 | wall 34538
KL Stats: Epoch 98 Divergences: Uniform: 2.982816387058839 Unigram: 3.2395124310553998
2022-02-02 15:47:13 | INFO | fairseq.trainer | begin training epoch 99
2022-02-02 15:47:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:49:36 | INFO | train_inner | epoch 099:     28 / 64 loss=5.929, ppl=60.94, wps=5805.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=6300, lr=0.00039841, gnorm=0.669, train_wall=504, gb_free=6.1, wall=34680
2022-02-02 15:52:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:53:05 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 9.811 | ppl 898.07 | wps 8002.4 | wpb 2034.1 | bsz 4 | num_updates 6336 | best_loss 9.243
2022-02-02 15:53:05 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-02-02 15:53:05 | INFO | train | epoch 099 | loss 5.919 | ppl 60.5 | wps 5932.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6336 | lr 0.000397276 | gnorm 0.669 | train_wall 323 | gb_free 6.1 | wall 34890
KL Stats: Epoch 99 Divergences: Uniform: 2.986584767414843 Unigram: 3.2449179462893403
2022-02-02 15:53:06 | INFO | fairseq.trainer | begin training epoch 100
2022-02-02 15:53:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 15:58:31 | INFO | train_inner | epoch 100:     64 / 64 loss=5.919, ppl=60.49, wps=6096.3, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=6400, lr=0.000395285, gnorm=0.684, train_wall=505, gb_free=6.1, wall=35215
2022-02-02 15:58:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 15:58:58 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 9.825 | ppl 907.17 | wps 7986.7 | wpb 2034.1 | bsz 4 | num_updates 6400 | best_loss 9.243
2022-02-02 15:58:58 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-02-02 15:58:58 | INFO | train | epoch 100 | loss 5.904 | ppl 59.86 | wps 5922.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6400 | lr 0.000395285 | gnorm 0.688 | train_wall 324 | gb_free 6.1 | wall 35242
KL Stats: Epoch 100 Divergences: Uniform: 2.9911294698821034 Unigram: 3.2624353687338865
2022-02-02 15:58:58 | INFO | fairseq.trainer | begin training epoch 101
2022-02-02 15:58:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:04:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:04:51 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 9.801 | ppl 892.34 | wps 7960.8 | wpb 2034.1 | bsz 4 | num_updates 6464 | best_loss 9.243
2022-02-02 16:04:51 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-02-02 16:04:51 | INFO | train | epoch 101 | loss 5.887 | ppl 59.18 | wps 5924.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6464 | lr 0.000393323 | gnorm 0.69 | train_wall 323 | gb_free 6.1 | wall 35595
KL Stats: Epoch 101 Divergences: Uniform: 2.9994073397733345 Unigram: 3.275748063340168
2022-02-02 16:04:51 | INFO | fairseq.trainer | begin training epoch 102
2022-02-02 16:04:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:07:55 | INFO | train_inner | epoch 102:     36 / 64 loss=5.87, ppl=58.47, wps=5797.5, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=6500, lr=0.000392232, gnorm=0.688, train_wall=506, gb_free=6.1, wall=35779
2022-02-02 16:10:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:10:43 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 9.805 | ppl 894.54 | wps 7994.2 | wpb 2034.1 | bsz 4 | num_updates 6528 | best_loss 9.243
2022-02-02 16:10:43 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-02-02 16:10:43 | INFO | train | epoch 102 | loss 5.871 | ppl 58.54 | wps 5923.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6528 | lr 0.00039139 | gnorm 0.706 | train_wall 324 | gb_free 6.1 | wall 35948
KL Stats: Epoch 102 Divergences: Uniform: 3.0076461289122642 Unigram: 3.2812403489683026
2022-02-02 16:10:43 | INFO | fairseq.trainer | begin training epoch 103
2022-02-02 16:10:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:16:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:16:35 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 9.775 | ppl 876.38 | wps 7977.1 | wpb 2034.1 | bsz 4 | num_updates 6592 | best_loss 9.243
2022-02-02 16:16:35 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-02-02 16:16:35 | INFO | train | epoch 103 | loss 5.856 | ppl 57.94 | wps 5929.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6592 | lr 0.000389486 | gnorm 0.711 | train_wall 323 | gb_free 6.1 | wall 36300
KL Stats: Epoch 103 Divergences: Uniform: 3.008572290698211 Unigram: 3.296197997587181
2022-02-02 16:16:36 | INFO | fairseq.trainer | begin training epoch 104
2022-02-02 16:16:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:17:16 | INFO | train_inner | epoch 104:      8 / 64 loss=5.868, ppl=58.39, wps=5803.3, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6600, lr=0.000389249, gnorm=0.715, train_wall=504, gb_free=6.1, wall=36341
2022-02-02 16:22:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:22:27 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 9.864 | ppl 931.68 | wps 8003.3 | wpb 2034.1 | bsz 4 | num_updates 6656 | best_loss 9.243
2022-02-02 16:22:27 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-02-02 16:22:27 | INFO | train | epoch 104 | loss 5.839 | ppl 57.23 | wps 5936.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6656 | lr 0.000387609 | gnorm 0.693 | train_wall 323 | gb_free 6.1 | wall 36652
KL Stats: Epoch 104 Divergences: Uniform: 3.0049052142165684 Unigram: 3.3037049475109845
2022-02-02 16:22:27 | INFO | fairseq.trainer | begin training epoch 105
2022-02-02 16:22:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:26:12 | INFO | train_inner | epoch 105:     44 / 64 loss=5.823, ppl=56.6, wps=6101.6, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=6700, lr=0.000386334, gnorm=0.708, train_wall=506, gb_free=6.1, wall=36876
2022-02-02 16:27:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:28:20 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 9.847 | ppl 920.76 | wps 7998.1 | wpb 2034.1 | bsz 4 | num_updates 6720 | best_loss 9.243
2022-02-02 16:28:20 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-02-02 16:28:20 | INFO | train | epoch 105 | loss 5.825 | ppl 56.67 | wps 5920.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6720 | lr 0.000385758 | gnorm 0.715 | train_wall 324 | gb_free 6.1 | wall 37004
KL Stats: Epoch 105 Divergences: Uniform: 3.0134611520246564 Unigram: 3.313627067587929
2022-02-02 16:28:20 | INFO | fairseq.trainer | begin training epoch 106
2022-02-02 16:28:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:33:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:34:13 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 9.861 | ppl 930.14 | wps 7959 | wpb 2034.1 | bsz 4 | num_updates 6784 | best_loss 9.243
2022-02-02 16:34:13 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-02-02 16:34:13 | INFO | train | epoch 106 | loss 5.81 | ppl 56.1 | wps 5922.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6784 | lr 0.000383934 | gnorm 0.717 | train_wall 324 | gb_free 6.1 | wall 37357
KL Stats: Epoch 106 Divergences: Uniform: 3.0156854406442317 Unigram: 3.325886781830498
2022-02-02 16:34:13 | INFO | fairseq.trainer | begin training epoch 107
2022-02-02 16:34:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:35:35 | INFO | train_inner | epoch 107:     16 / 64 loss=5.815, ppl=56.28, wps=5793.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=6800, lr=0.000383482, gnorm=0.715, train_wall=505, gb_free=6.1, wall=37439
2022-02-02 16:39:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:40:05 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 9.834 | ppl 912.99 | wps 8005.5 | wpb 2034.1 | bsz 4 | num_updates 6848 | best_loss 9.243
2022-02-02 16:40:05 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-02-02 16:40:05 | INFO | train | epoch 107 | loss 5.795 | ppl 55.53 | wps 5929.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6848 | lr 0.000382136 | gnorm 0.737 | train_wall 323 | gb_free 6.1 | wall 37709
KL Stats: Epoch 107 Divergences: Uniform: 3.0228376585380055 Unigram: 3.340905459997022
2022-02-02 16:40:05 | INFO | fairseq.trainer | begin training epoch 108
2022-02-02 16:40:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:44:30 | INFO | train_inner | epoch 108:     52 / 64 loss=5.788, ppl=55.24, wps=6105.3, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=6900, lr=0.000380693, gnorm=0.729, train_wall=505, gb_free=6.1, wall=37974
2022-02-02 16:45:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:45:57 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 9.878 | ppl 940.77 | wps 7992.3 | wpb 2034.1 | bsz 4 | num_updates 6912 | best_loss 9.243
2022-02-02 16:45:57 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-02-02 16:45:57 | INFO | train | epoch 108 | loss 5.781 | ppl 54.99 | wps 5934.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6912 | lr 0.000380363 | gnorm 0.717 | train_wall 323 | gb_free 6.1 | wall 38061
KL Stats: Epoch 108 Divergences: Uniform: 3.022049611798933 Unigram: 3.3446454936220973
2022-02-02 16:45:57 | INFO | fairseq.trainer | begin training epoch 109
2022-02-02 16:45:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:51:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:51:50 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 9.873 | ppl 937.62 | wps 7991.2 | wpb 2034.1 | bsz 4 | num_updates 6976 | best_loss 9.243
2022-02-02 16:51:50 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-02-02 16:51:50 | INFO | train | epoch 109 | loss 5.766 | ppl 54.43 | wps 5921.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 6976 | lr 0.000378614 | gnorm 0.724 | train_wall 324 | gb_free 6.1 | wall 38414
KL Stats: Epoch 109 Divergences: Uniform: 3.0338563038622386 Unigram: 3.360714399174378
2022-02-02 16:51:50 | INFO | fairseq.trainer | begin training epoch 110
2022-02-02 16:51:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 16:53:52 | INFO | train_inner | epoch 110:     24 / 64 loss=5.759, ppl=54.14, wps=5798.2, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=7000, lr=0.000377964, gnorm=0.724, train_wall=505, gb_free=6.1, wall=38536
2022-02-02 16:57:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 16:57:42 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 9.872 | ppl 937.13 | wps 8012.7 | wpb 2034.1 | bsz 4 | num_updates 7040 | best_loss 9.243
2022-02-02 16:57:42 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-02-02 16:57:42 | INFO | train | epoch 110 | loss 5.75 | ppl 53.83 | wps 5934.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7040 | lr 0.000376889 | gnorm 0.727 | train_wall 323 | gb_free 6.1 | wall 38766
KL Stats: Epoch 110 Divergences: Uniform: 3.0287233831826392 Unigram: 3.361564265427672
2022-02-02 16:57:42 | INFO | fairseq.trainer | begin training epoch 111
2022-02-02 16:57:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:02:47 | INFO | train_inner | epoch 111:     60 / 64 loss=5.754, ppl=53.98, wps=6106.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=7100, lr=0.000375293, gnorm=0.725, train_wall=505, gb_free=6.1, wall=39072
2022-02-02 17:03:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:03:34 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 9.807 | ppl 895.95 | wps 8002.7 | wpb 2034.1 | bsz 4 | num_updates 7104 | best_loss 9.243
2022-02-02 17:03:34 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-02-02 17:03:34 | INFO | train | epoch 111 | loss 5.739 | ppl 53.39 | wps 5933.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7104 | lr 0.000375188 | gnorm 0.726 | train_wall 323 | gb_free 6.1 | wall 39118
KL Stats: Epoch 111 Divergences: Uniform: 3.0457628576479325 Unigram: 3.3743885021658406
2022-02-02 17:03:34 | INFO | fairseq.trainer | begin training epoch 112
2022-02-02 17:03:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:08:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:09:26 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 9.914 | ppl 964.49 | wps 8012.8 | wpb 2034.1 | bsz 4 | num_updates 7168 | best_loss 9.243
2022-02-02 17:09:26 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-02-02 17:09:26 | INFO | train | epoch 112 | loss 5.725 | ppl 52.9 | wps 5931.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7168 | lr 0.000373509 | gnorm 0.729 | train_wall 323 | gb_free 6.1 | wall 39470
KL Stats: Epoch 112 Divergences: Uniform: 3.0390689065072256 Unigram: 3.3807486408401264
2022-02-02 17:09:26 | INFO | fairseq.trainer | begin training epoch 113
2022-02-02 17:09:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:12:09 | INFO | train_inner | epoch 113:     32 / 64 loss=5.714, ppl=52.5, wps=5806, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7200, lr=0.000372678, gnorm=0.732, train_wall=504, gb_free=6.1, wall=39633
2022-02-02 17:14:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 17:15:18 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 9.835 | ppl 913.51 | wps 7981.5 | wpb 2034.1 | bsz 4 | num_updates 7232 | best_loss 9.243
2022-02-02 17:15:18 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-02-02 17:15:18 | INFO | train | epoch 113 | loss 5.712 | ppl 52.41 | wps 5929.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7232 | lr 0.000371853 | gnorm 0.745 | train_wall 323 | gb_free 6.1 | wall 39822
KL Stats: Epoch 113 Divergences: Uniform: 3.0451106417724096 Unigram: 3.3956492303796626
2022-02-02 17:15:18 | INFO | fairseq.trainer | begin training epoch 114
2022-02-02 17:15:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:20:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:21:10 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 9.928 | ppl 974.19 | wps 8010.3 | wpb 2034.1 | bsz 4 | num_updates 7296 | best_loss 9.243
2022-02-02 17:21:10 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-02-02 17:21:10 | INFO | train | epoch 114 | loss 5.7 | ppl 51.99 | wps 5931.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7296 | lr 0.000370218 | gnorm 0.75 | train_wall 323 | gb_free 6.1 | wall 40174
KL Stats: Epoch 114 Divergences: Uniform: 3.0404770292814605 Unigram: 3.396745786527184
2022-02-02 17:21:10 | INFO | fairseq.trainer | begin training epoch 115
2022-02-02 17:21:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:21:31 | INFO | train_inner | epoch 115:      4 / 64 loss=5.711, ppl=52.38, wps=5801.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=7300, lr=0.000370117, gnorm=0.753, train_wall=505, gb_free=6.1, wall=40195
2022-02-02 17:26:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:27:02 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 9.826 | ppl 907.74 | wps 7974.8 | wpb 2034.1 | bsz 4 | num_updates 7360 | best_loss 9.243
2022-02-02 17:27:02 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-02-02 17:27:02 | INFO | train | epoch 115 | loss 5.688 | ppl 51.54 | wps 5936.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7360 | lr 0.000368605 | gnorm 0.763 | train_wall 323 | gb_free 6.1 | wall 40526
KL Stats: Epoch 115 Divergences: Uniform: 3.046624644618536 Unigram: 3.4074671789110895
2022-02-02 17:27:02 | INFO | fairseq.trainer | begin training epoch 116
2022-02-02 17:27:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:30:25 | INFO | train_inner | epoch 116:     40 / 64 loss=5.675, ppl=51.08, wps=6109.3, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7400, lr=0.000367607, gnorm=0.753, train_wall=505, gb_free=6.1, wall=40730
2022-02-02 17:32:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:32:54 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 9.918 | ppl 967.47 | wps 7967.7 | wpb 2034.1 | bsz 4 | num_updates 7424 | best_loss 9.243
2022-02-02 17:32:54 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-02-02 17:32:54 | INFO | train | epoch 116 | loss 5.673 | ppl 51.03 | wps 5938.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7424 | lr 0.000367013 | gnorm 0.75 | train_wall 323 | gb_free 6.1 | wall 40878
KL Stats: Epoch 116 Divergences: Uniform: 3.04760615284899 Unigram: 3.412715185074821
2022-02-02 17:32:54 | INFO | fairseq.trainer | begin training epoch 117
2022-02-02 17:32:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:38:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 17:38:46 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 9.878 | ppl 941.17 | wps 8007.3 | wpb 2034.1 | bsz 4 | num_updates 7488 | best_loss 9.243
2022-02-02 17:38:46 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-02-02 17:38:46 | INFO | train | epoch 117 | loss 5.662 | ppl 50.63 | wps 5932.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7488 | lr 0.000365441 | gnorm 0.771 | train_wall 323 | gb_free 6.1 | wall 41230
KL Stats: Epoch 117 Divergences: Uniform: 3.061177817546575 Unigram: 3.4271646179837543
2022-02-02 17:38:46 | INFO | fairseq.trainer | begin training epoch 118
2022-02-02 17:38:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:39:47 | INFO | train_inner | epoch 118:     12 / 64 loss=5.666, ppl=50.77, wps=5808.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7500, lr=0.000365148, gnorm=0.767, train_wall=504, gb_free=6.1, wall=41291
2022-02-02 17:44:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:44:37 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 9.896 | ppl 952.58 | wps 8007.4 | wpb 2034.1 | bsz 4 | num_updates 7552 | best_loss 9.243
2022-02-02 17:44:37 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-02-02 17:44:37 | INFO | train | epoch 118 | loss 5.65 | ppl 50.21 | wps 5942.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7552 | lr 0.000363889 | gnorm 0.768 | train_wall 323 | gb_free 6.1 | wall 41582
KL Stats: Epoch 118 Divergences: Uniform: 3.057683682473458 Unigram: 3.4347472442714997
2022-02-02 17:44:37 | INFO | fairseq.trainer | begin training epoch 119
2022-02-02 17:44:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:48:42 | INFO | train_inner | epoch 119:     48 / 64 loss=5.646, ppl=50.06, wps=6109.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7600, lr=0.000362738, gnorm=0.778, train_wall=505, gb_free=6.1, wall=41826
2022-02-02 17:50:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:50:29 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 9.835 | ppl 913.12 | wps 8000.5 | wpb 2034.1 | bsz 4 | num_updates 7616 | best_loss 9.243
2022-02-02 17:50:29 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-02-02 17:50:29 | INFO | train | epoch 119 | loss 5.639 | ppl 49.84 | wps 5936.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7616 | lr 0.000362357 | gnorm 0.776 | train_wall 323 | gb_free 6.1 | wall 41933
KL Stats: Epoch 119 Divergences: Uniform: 3.064436050293131 Unigram: 3.4468336070425183
2022-02-02 17:50:29 | INFO | fairseq.trainer | begin training epoch 120
2022-02-02 17:50:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:55:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 17:56:21 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 9.933 | ppl 977.63 | wps 8004.9 | wpb 2034.1 | bsz 4 | num_updates 7680 | best_loss 9.243
2022-02-02 17:56:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 7680 updates
2022-02-02 17:56:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint120.pt
2022-02-02 17:56:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint120.pt
2022-02-02 17:56:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint120.pt (epoch 120 @ 7680 updates, score 9.933) (writing took 3.1776067037135363 seconds)
2022-02-02 17:56:24 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-02-02 17:56:24 | INFO | train | epoch 120 | loss 5.627 | ppl 49.42 | wps 5886 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7680 | lr 0.000360844 | gnorm 0.772 | train_wall 323 | gb_free 6.1 | wall 42288
KL Stats: Epoch 120 Divergences: Uniform: 3.0607535275045734 Unigram: 3.450177343379536
2022-02-02 17:56:24 | INFO | fairseq.trainer | begin training epoch 121
2022-02-02 17:56:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 17:58:06 | INFO | train_inner | epoch 121:     20 / 64 loss=5.626, ppl=49.4, wps=5780.4, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7700, lr=0.000360375, gnorm=0.771, train_wall=504, gb_free=6.1, wall=42390
2022-02-02 18:01:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:02:16 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 9.908 | ppl 960.84 | wps 8022.8 | wpb 2034.1 | bsz 4 | num_updates 7744 | best_loss 9.243
2022-02-02 18:02:16 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-02-02 18:02:16 | INFO | train | epoch 121 | loss 5.616 | ppl 49.06 | wps 5936.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7744 | lr 0.00035935 | gnorm 0.798 | train_wall 323 | gb_free 6.1 | wall 42640
KL Stats: Epoch 121 Divergences: Uniform: 3.0679962475247984 Unigram: 3.46116161092461
2022-02-02 18:02:16 | INFO | fairseq.trainer | begin training epoch 122
2022-02-02 18:02:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:07:01 | INFO | train_inner | epoch 122:     56 / 64 loss=5.61, ppl=48.84, wps=6110.1, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=7800, lr=0.000358057, gnorm=0.792, train_wall=505, gb_free=6.1, wall=42925
2022-02-02 18:07:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:08:07 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 9.919 | ppl 967.88 | wps 7966.3 | wpb 2034.1 | bsz 4 | num_updates 7808 | best_loss 9.243
2022-02-02 18:08:07 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-02-02 18:08:07 | INFO | train | epoch 122 | loss 5.602 | ppl 48.57 | wps 5938.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7808 | lr 0.000357874 | gnorm 0.783 | train_wall 323 | gb_free 6.1 | wall 42992
KL Stats: Epoch 122 Divergences: Uniform: 3.0686837728426712 Unigram: 3.46207022349463
2022-02-02 18:08:07 | INFO | fairseq.trainer | begin training epoch 123
2022-02-02 18:08:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:13:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:14:00 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 9.921 | ppl 969.67 | wps 8022.4 | wpb 2034.1 | bsz 4 | num_updates 7872 | best_loss 9.243
2022-02-02 18:14:00 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-02-02 18:14:00 | INFO | train | epoch 123 | loss 5.592 | ppl 48.24 | wps 5931.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7872 | lr 0.000356416 | gnorm 0.786 | train_wall 323 | gb_free 6.1 | wall 43344
KL Stats: Epoch 123 Divergences: Uniform: 3.067126617926891 Unigram: 3.4720204546353153
2022-02-02 18:14:00 | INFO | fairseq.trainer | begin training epoch 124
2022-02-02 18:14:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:16:22 | INFO | train_inner | epoch 124:     28 / 64 loss=5.588, ppl=48.09, wps=5803.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=7900, lr=0.000355784, gnorm=0.791, train_wall=504, gb_free=6.1, wall=43487
2022-02-02 18:19:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:19:52 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 9.946 | ppl 986.32 | wps 7968 | wpb 2034.1 | bsz 4 | num_updates 7936 | best_loss 9.243
2022-02-02 18:19:52 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-02-02 18:19:52 | INFO | train | epoch 124 | loss 5.581 | ppl 47.87 | wps 5926.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 7936 | lr 0.000354976 | gnorm 0.79 | train_wall 323 | gb_free 6.1 | wall 43696
KL Stats: Epoch 124 Divergences: Uniform: 3.077079172299483 Unigram: 3.4848860991673103
2022-02-02 18:19:52 | INFO | fairseq.trainer | begin training epoch 125
2022-02-02 18:19:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:25:17 | INFO | train_inner | epoch 125:     64 / 64 loss=5.585, ppl=48.02, wps=6097.8, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=8000, lr=0.000353553, gnorm=0.822, train_wall=505, gb_free=6.1, wall=44021
2022-02-02 18:25:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:25:44 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 9.9 | ppl 955.2 | wps 7990.6 | wpb 2034.1 | bsz 4 | num_updates 8000 | best_loss 9.243
2022-02-02 18:25:44 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-02-02 18:25:44 | INFO | train | epoch 125 | loss 5.576 | ppl 47.71 | wps 5930.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8000 | lr 0.000353553 | gnorm 0.846 | train_wall 323 | gb_free 6.1 | wall 44048
KL Stats: Epoch 125 Divergences: Uniform: 3.073852504035181 Unigram: 3.4826409648483825
2022-02-02 18:25:44 | INFO | fairseq.trainer | begin training epoch 126
2022-02-02 18:25:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:31:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:31:36 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 9.967 | ppl 1001.18 | wps 7983.7 | wpb 2034.1 | bsz 4 | num_updates 8064 | best_loss 9.243
2022-02-02 18:31:36 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-02-02 18:31:36 | INFO | train | epoch 126 | loss 5.564 | ppl 47.32 | wps 5929.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8064 | lr 0.000352148 | gnorm 0.802 | train_wall 323 | gb_free 6.1 | wall 44401
KL Stats: Epoch 126 Divergences: Uniform: 3.0761181535691193 Unigram: 3.4998440139303195
2022-02-02 18:31:36 | INFO | fairseq.trainer | begin training epoch 127
2022-02-02 18:31:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:34:40 | INFO | train_inner | epoch 127:     36 / 64 loss=5.546, ppl=46.73, wps=5805.1, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=8100, lr=0.000351364, gnorm=0.808, train_wall=506, gb_free=6.1, wall=44584
2022-02-02 18:37:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:37:29 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 9.93 | ppl 975.8 | wps 7985.5 | wpb 2034.1 | bsz 4 | num_updates 8128 | best_loss 9.243
2022-02-02 18:37:29 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-02-02 18:37:29 | INFO | train | epoch 127 | loss 5.551 | ppl 46.89 | wps 5930.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8128 | lr 0.000350758 | gnorm 0.824 | train_wall 323 | gb_free 6.1 | wall 44753
KL Stats: Epoch 127 Divergences: Uniform: 3.082207426091485 Unigram: 3.5051463800591995
2022-02-02 18:37:29 | INFO | fairseq.trainer | begin training epoch 128
2022-02-02 18:37:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:42:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:43:21 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 9.942 | ppl 983.41 | wps 7989.9 | wpb 2034.1 | bsz 4 | num_updates 8192 | best_loss 9.243
2022-02-02 18:43:21 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-02-02 18:43:21 | INFO | train | epoch 128 | loss 5.54 | ppl 46.54 | wps 5926.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8192 | lr 0.000349386 | gnorm 0.814 | train_wall 323 | gb_free 6.1 | wall 45105
KL Stats: Epoch 128 Divergences: Uniform: 3.0825781612675613 Unigram: 3.5130693272278672
2022-02-02 18:43:21 | INFO | fairseq.trainer | begin training epoch 129
2022-02-02 18:43:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:44:02 | INFO | train_inner | epoch 129:      8 / 64 loss=5.552, ppl=46.9, wps=5799.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8200, lr=0.000349215, gnorm=0.818, train_wall=505, gb_free=6.1, wall=45146
2022-02-02 18:48:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:49:13 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 9.96 | ppl 996.3 | wps 8000.1 | wpb 2034.1 | bsz 4 | num_updates 8256 | best_loss 9.243
2022-02-02 18:49:13 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-02-02 18:49:13 | INFO | train | epoch 129 | loss 5.531 | ppl 46.25 | wps 5932.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8256 | lr 0.000348029 | gnorm 0.839 | train_wall 323 | gb_free 6.1 | wall 45457
KL Stats: Epoch 129 Divergences: Uniform: 3.084498907267862 Unigram: 3.522375373484768
2022-02-02 18:49:13 | INFO | fairseq.trainer | begin training epoch 130
2022-02-02 18:49:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 18:52:57 | INFO | train_inner | epoch 130:     44 / 64 loss=5.525, ppl=46.04, wps=6101.5, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=8300, lr=0.000347105, gnorm=0.836, train_wall=506, gb_free=6.1, wall=45682
2022-02-02 18:54:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 18:55:05 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 9.93 | ppl 975.81 | wps 7987.8 | wpb 2034.1 | bsz 4 | num_updates 8320 | best_loss 9.243
2022-02-02 18:55:05 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-02-02 18:55:05 | INFO | train | epoch 130 | loss 5.521 | ppl 45.93 | wps 5927.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8320 | lr 0.000346688 | gnorm 0.839 | train_wall 323 | gb_free 6.1 | wall 45810
KL Stats: Epoch 130 Divergences: Uniform: 3.0887173313169654 Unigram: 3.5265895942555923
2022-02-02 18:55:05 | INFO | fairseq.trainer | begin training epoch 131
2022-02-02 18:55:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:00:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:00:57 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 10.001 | ppl 1024.46 | wps 8008.3 | wpb 2034.1 | bsz 4 | num_updates 8384 | best_loss 9.243
2022-02-02 19:00:57 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-02-02 19:00:57 | INFO | train | epoch 131 | loss 5.514 | ppl 45.69 | wps 5938.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8384 | lr 0.000345362 | gnorm 0.869 | train_wall 323 | gb_free 6.1 | wall 46161
KL Stats: Epoch 131 Divergences: Uniform: 3.088890551885816 Unigram: 3.5274731117092766
2022-02-02 19:00:57 | INFO | fairseq.trainer | begin training epoch 132
2022-02-02 19:00:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:02:19 | INFO | train_inner | epoch 132:     16 / 64 loss=5.514, ppl=45.71, wps=5808.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=8400, lr=0.000345033, gnorm=0.866, train_wall=504, gb_free=6.1, wall=46243
2022-02-02 19:06:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:06:49 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 9.961 | ppl 996.85 | wps 7993.8 | wpb 2034.1 | bsz 4 | num_updates 8448 | best_loss 9.243
2022-02-02 19:06:49 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-02-02 19:06:49 | INFO | train | epoch 132 | loss 5.503 | ppl 45.35 | wps 5929.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8448 | lr 0.000344051 | gnorm 0.853 | train_wall 323 | gb_free 6.1 | wall 46514
KL Stats: Epoch 132 Divergences: Uniform: 3.098810521327703 Unigram: 3.5411154420652022
2022-02-02 19:06:49 | INFO | fairseq.trainer | begin training epoch 133
2022-02-02 19:06:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:11:14 | INFO | train_inner | epoch 133:     52 / 64 loss=5.499, ppl=45.23, wps=6102, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=8500, lr=0.000342997, gnorm=0.851, train_wall=506, gb_free=6.1, wall=46779
2022-02-02 19:12:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:12:41 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 9.963 | ppl 998.01 | wps 7993.5 | wpb 2034.1 | bsz 4 | num_updates 8512 | best_loss 9.243
2022-02-02 19:12:41 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-02-02 19:12:41 | INFO | train | epoch 133 | loss 5.492 | ppl 45.01 | wps 5933.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8512 | lr 0.000342755 | gnorm 0.847 | train_wall 323 | gb_free 6.1 | wall 46866
KL Stats: Epoch 133 Divergences: Uniform: 3.091759527835431 Unigram: 3.543969453518728
2022-02-02 19:12:41 | INFO | fairseq.trainer | begin training epoch 134
2022-02-02 19:12:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:18:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:18:33 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 10.022 | ppl 1040.05 | wps 8031.2 | wpb 2034.1 | bsz 4 | num_updates 8576 | best_loss 9.243
2022-02-02 19:18:33 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-02-02 19:18:33 | INFO | train | epoch 134 | loss 5.484 | ppl 44.75 | wps 5943.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8576 | lr 0.000341474 | gnorm 0.864 | train_wall 323 | gb_free 6.1 | wall 47217
KL Stats: Epoch 134 Divergences: Uniform: 3.0989382746375194 Unigram: 3.5571891510163844
2022-02-02 19:18:33 | INFO | fairseq.trainer | begin training epoch 135
2022-02-02 19:18:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:20:35 | INFO | train_inner | epoch 135:     24 / 64 loss=5.479, ppl=44.59, wps=5816.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8600, lr=0.000340997, gnorm=0.862, train_wall=503, gb_free=6.1, wall=47339
2022-02-02 19:23:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:24:24 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 9.965 | ppl 999.49 | wps 7956.9 | wpb 2034.1 | bsz 4 | num_updates 8640 | best_loss 9.243
2022-02-02 19:24:24 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-02-02 19:24:24 | INFO | train | epoch 135 | loss 5.474 | ppl 44.44 | wps 5939 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8640 | lr 0.000340207 | gnorm 0.872 | train_wall 323 | gb_free 6.1 | wall 47569
KL Stats: Epoch 135 Divergences: Uniform: 3.1011845102534523 Unigram: 3.559568852358165
2022-02-02 19:24:24 | INFO | fairseq.trainer | begin training epoch 136
2022-02-02 19:24:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:29:30 | INFO | train_inner | epoch 136:     60 / 64 loss=5.473, ppl=44.42, wps=6105.6, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=8700, lr=0.000339032, gnorm=0.881, train_wall=505, gb_free=6.1, wall=47874
2022-02-02 19:29:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:30:16 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 9.994 | ppl 1019.71 | wps 7993.5 | wpb 2034.1 | bsz 4 | num_updates 8704 | best_loss 9.243
2022-02-02 19:30:16 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-02-02 19:30:16 | INFO | train | epoch 136 | loss 5.465 | ppl 44.18 | wps 5934.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8704 | lr 0.000338954 | gnorm 0.888 | train_wall 323 | gb_free 6.1 | wall 47921
KL Stats: Epoch 136 Divergences: Uniform: 3.1012411868894993 Unigram: 3.5671356393134284
2022-02-02 19:30:16 | INFO | fairseq.trainer | begin training epoch 137
2022-02-02 19:30:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:35:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:36:09 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 9.963 | ppl 997.84 | wps 8000.4 | wpb 2034.1 | bsz 4 | num_updates 8768 | best_loss 9.243
2022-02-02 19:36:09 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-02-02 19:36:09 | INFO | train | epoch 137 | loss 5.457 | ppl 43.92 | wps 5923.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8768 | lr 0.000337715 | gnorm 0.872 | train_wall 324 | gb_free 6.1 | wall 48273
KL Stats: Epoch 137 Divergences: Uniform: 3.100891096561261 Unigram: 3.57331227102758
2022-02-02 19:36:09 | INFO | fairseq.trainer | begin training epoch 138
2022-02-02 19:36:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:38:52 | INFO | train_inner | epoch 138:     32 / 64 loss=5.448, ppl=43.64, wps=5799.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=8800, lr=0.0003371, gnorm=0.873, train_wall=505, gb_free=6.1, wall=48437
2022-02-02 19:41:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:42:01 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 10.037 | ppl 1050.59 | wps 7968.3 | wpb 2034.1 | bsz 4 | num_updates 8832 | best_loss 9.243
2022-02-02 19:42:01 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-02-02 19:42:01 | INFO | train | epoch 138 | loss 5.449 | ppl 43.68 | wps 5926.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8832 | lr 0.000336489 | gnorm 0.885 | train_wall 323 | gb_free 6.1 | wall 48626
KL Stats: Epoch 138 Divergences: Uniform: 3.0990610373480503 Unigram: 3.5790850454788443
2022-02-02 19:42:01 | INFO | fairseq.trainer | begin training epoch 139
2022-02-02 19:42:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:47:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:47:54 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 10.017 | ppl 1036.41 | wps 8010.3 | wpb 2034.1 | bsz 4 | num_updates 8896 | best_loss 9.243
2022-02-02 19:47:54 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-02-02 19:47:54 | INFO | train | epoch 139 | loss 5.437 | ppl 43.32 | wps 5928.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8896 | lr 0.000335276 | gnorm 0.869 | train_wall 323 | gb_free 6.1 | wall 48978
KL Stats: Epoch 139 Divergences: Uniform: 3.1070854338973137 Unigram: 3.5894678504637385
2022-02-02 19:47:54 | INFO | fairseq.trainer | begin training epoch 140
2022-02-02 19:47:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:48:14 | INFO | train_inner | epoch 140:      4 / 64 loss=5.449, ppl=43.68, wps=5800.2, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=8900, lr=0.000335201, gnorm=0.879, train_wall=505, gb_free=6.1, wall=48999
2022-02-02 19:53:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:53:46 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 10.024 | ppl 1040.99 | wps 7968 | wpb 2034.1 | bsz 4 | num_updates 8960 | best_loss 9.243
2022-02-02 19:53:46 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-02-02 19:53:46 | INFO | train | epoch 140 | loss 5.432 | ppl 43.18 | wps 5928.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 8960 | lr 0.000334077 | gnorm 0.892 | train_wall 323 | gb_free 6.1 | wall 49330
KL Stats: Epoch 140 Divergences: Uniform: 3.1038939873483304 Unigram: 3.5951666925094203
2022-02-02 19:53:46 | INFO | fairseq.trainer | begin training epoch 141
2022-02-02 19:53:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 19:57:10 | INFO | train_inner | epoch 141:     40 / 64 loss=5.421, ppl=42.85, wps=6097.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9000, lr=0.000333333, gnorm=0.904, train_wall=506, gb_free=6.1, wall=49534
2022-02-02 19:59:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 19:59:39 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 10.012 | ppl 1032.8 | wps 7966.2 | wpb 2034.1 | bsz 4 | num_updates 9024 | best_loss 9.243
2022-02-02 19:59:39 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-02-02 19:59:39 | INFO | train | epoch 141 | loss 5.423 | ppl 42.9 | wps 5924.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9024 | lr 0.00033289 | gnorm 0.905 | train_wall 323 | gb_free 6.1 | wall 49683
KL Stats: Epoch 141 Divergences: Uniform: 3.1082891615771526 Unigram: 3.597727168786277
2022-02-02 19:59:39 | INFO | fairseq.trainer | begin training epoch 142
2022-02-02 19:59:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:05:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:05:31 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 10.04 | ppl 1053.05 | wps 7993.5 | wpb 2034.1 | bsz 4 | num_updates 9088 | best_loss 9.243
2022-02-02 20:05:31 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-02-02 20:05:31 | INFO | train | epoch 142 | loss 5.416 | ppl 42.68 | wps 5932.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9088 | lr 0.000331716 | gnorm 0.914 | train_wall 323 | gb_free 6.1 | wall 50035
KL Stats: Epoch 142 Divergences: Uniform: 3.1058289027986556 Unigram: 3.599413675569821
2022-02-02 20:05:31 | INFO | fairseq.trainer | begin training epoch 143
2022-02-02 20:05:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:06:32 | INFO | train_inner | epoch 143:     12 / 64 loss=5.417, ppl=42.73, wps=5805.3, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=9100, lr=0.000331497, gnorm=0.904, train_wall=504, gb_free=6.1, wall=50096
2022-02-02 20:10:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:11:22 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 9.987 | ppl 1015.02 | wps 7993.4 | wpb 2034.1 | bsz 4 | num_updates 9152 | best_loss 9.243
2022-02-02 20:11:22 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-02-02 20:11:22 | INFO | train | epoch 143 | loss 5.407 | ppl 42.43 | wps 5938.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9152 | lr 0.000330554 | gnorm 0.895 | train_wall 323 | gb_free 6.1 | wall 50387
KL Stats: Epoch 143 Divergences: Uniform: 3.1136944378634004 Unigram: 3.610652457646566
2022-02-02 20:11:22 | INFO | fairseq.trainer | begin training epoch 144
2022-02-02 20:11:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:15:27 | INFO | train_inner | epoch 144:     48 / 64 loss=5.404, ppl=42.35, wps=6105.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9200, lr=0.00032969, gnorm=0.908, train_wall=505, gb_free=6.1, wall=50631
2022-02-02 20:16:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 20:17:15 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 10.002 | ppl 1025.22 | wps 7946.6 | wpb 2034.1 | bsz 4 | num_updates 9216 | best_loss 9.243
2022-02-02 20:17:15 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-02-02 20:17:15 | INFO | train | epoch 144 | loss 5.401 | ppl 42.25 | wps 5926.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9216 | lr 0.000329404 | gnorm 0.934 | train_wall 323 | gb_free 6.1 | wall 50739
KL Stats: Epoch 144 Divergences: Uniform: 3.1128090383527125 Unigram: 3.6170812975921156
2022-02-02 20:17:15 | INFO | fairseq.trainer | begin training epoch 145
2022-02-02 20:17:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:22:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 20:23:07 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 10.041 | ppl 1053.23 | wps 7967.3 | wpb 2034.1 | bsz 4 | num_updates 9280 | best_loss 9.243
2022-02-02 20:23:07 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-02-02 20:23:07 | INFO | train | epoch 145 | loss 5.393 | ppl 42.02 | wps 5924.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9280 | lr 0.000328266 | gnorm 0.929 | train_wall 323 | gb_free 6.1 | wall 51092
KL Stats: Epoch 145 Divergences: Uniform: 3.1116361767252023 Unigram: 3.620180781978636
2022-02-02 20:23:07 | INFO | fairseq.trainer | begin training epoch 146
2022-02-02 20:23:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:24:49 | INFO | train_inner | epoch 146:     20 / 64 loss=5.392, ppl=41.99, wps=5798.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=9300, lr=0.000327913, gnorm=0.93, train_wall=505, gb_free=6.1, wall=51193
2022-02-02 20:28:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:28:59 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 10.08 | ppl 1082.2 | wps 7988.2 | wpb 2034.1 | bsz 4 | num_updates 9344 | best_loss 9.243
2022-02-02 20:28:59 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-02-02 20:28:59 | INFO | train | epoch 146 | loss 5.382 | ppl 41.69 | wps 5932.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9344 | lr 0.00032714 | gnorm 0.924 | train_wall 323 | gb_free 6.1 | wall 51444
KL Stats: Epoch 146 Divergences: Uniform: 3.113543590438985 Unigram: 3.631239205109401
2022-02-02 20:28:59 | INFO | fairseq.trainer | begin training epoch 147
2022-02-02 20:28:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:33:45 | INFO | train_inner | epoch 147:     56 / 64 loss=5.385, ppl=41.78, wps=6099.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9400, lr=0.000326164, gnorm=0.913, train_wall=506, gb_free=6.1, wall=51729
2022-02-02 20:34:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:34:52 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 9.998 | ppl 1022.91 | wps 7998.3 | wpb 2034.1 | bsz 4 | num_updates 9408 | best_loss 9.243
2022-02-02 20:34:52 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-02-02 20:34:52 | INFO | train | epoch 147 | loss 5.376 | ppl 41.53 | wps 5927.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9408 | lr 0.000326025 | gnorm 0.904 | train_wall 323 | gb_free 6.1 | wall 51796
KL Stats: Epoch 147 Divergences: Uniform: 3.120764826543057 Unigram: 3.6300830671748106
2022-02-02 20:34:52 | INFO | fairseq.trainer | begin training epoch 148
2022-02-02 20:34:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:40:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:40:44 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 10.033 | ppl 1047.62 | wps 7963.6 | wpb 2034.1 | bsz 4 | num_updates 9472 | best_loss 9.243
2022-02-02 20:40:44 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-02-02 20:40:44 | INFO | train | epoch 148 | loss 5.368 | ppl 41.31 | wps 5932.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9472 | lr 0.000324922 | gnorm 0.952 | train_wall 323 | gb_free 6.1 | wall 52148
KL Stats: Epoch 148 Divergences: Uniform: 3.123708034164896 Unigram: 3.6445589285663007
2022-02-02 20:40:44 | INFO | fairseq.trainer | begin training epoch 149
2022-02-02 20:40:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:43:07 | INFO | train_inner | epoch 149:     28 / 64 loss=5.362, ppl=41.11, wps=5801.3, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=9500, lr=0.000324443, gnorm=0.948, train_wall=505, gb_free=6.1, wall=52291
2022-02-02 20:46:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:46:37 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 10.039 | ppl 1051.94 | wps 8001.6 | wpb 2034.1 | bsz 4 | num_updates 9536 | best_loss 9.243
2022-02-02 20:46:37 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-02-02 20:46:37 | INFO | train | epoch 149 | loss 5.362 | ppl 41.14 | wps 5921.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9536 | lr 0.00032383 | gnorm 0.951 | train_wall 324 | gb_free 6.1 | wall 52501
KL Stats: Epoch 149 Divergences: Uniform: 3.1209722981154524 Unigram: 3.6424578646792343
2022-02-02 20:46:37 | INFO | fairseq.trainer | begin training epoch 150
2022-02-02 20:46:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:52:02 | INFO | train_inner | epoch 150:     64 / 64 loss=5.366, ppl=41.23, wps=6094.4, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=9600, lr=0.000322749, gnorm=0.957, train_wall=505, gb_free=6.1, wall=52826
2022-02-02 20:52:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:52:29 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 10.048 | ppl 1058.52 | wps 7951.7 | wpb 2034.1 | bsz 4 | num_updates 9600 | best_loss 9.243
2022-02-02 20:52:29 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-02-02 20:52:29 | INFO | train | epoch 150 | loss 5.354 | ppl 40.91 | wps 5922.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9600 | lr 0.000322749 | gnorm 0.957 | train_wall 324 | gb_free 6.1 | wall 52854
KL Stats: Epoch 150 Divergences: Uniform: 3.130626031255066 Unigram: 3.651927552221089
2022-02-02 20:52:29 | INFO | fairseq.trainer | begin training epoch 151
2022-02-02 20:52:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 20:57:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 20:58:22 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 10.077 | ppl 1080.03 | wps 7960.5 | wpb 2034.1 | bsz 4 | num_updates 9664 | best_loss 9.243
2022-02-02 20:58:22 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-02-02 20:58:22 | INFO | train | epoch 151 | loss 5.347 | ppl 40.71 | wps 5915.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9664 | lr 0.000321678 | gnorm 0.968 | train_wall 324 | gb_free 6.1 | wall 53207
KL Stats: Epoch 151 Divergences: Uniform: 3.128527412825143 Unigram: 3.6564853063330247
2022-02-02 20:58:22 | INFO | fairseq.trainer | begin training epoch 152
2022-02-02 20:58:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:01:26 | INFO | train_inner | epoch 152:     36 / 64 loss=5.334, ppl=40.34, wps=5791.2, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=9700, lr=0.000321081, gnorm=0.961, train_wall=507, gb_free=6.1, wall=53390
2022-02-02 21:03:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:04:15 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 10.078 | ppl 1080.82 | wps 7997.3 | wpb 2034.1 | bsz 4 | num_updates 9728 | best_loss 9.243
2022-02-02 21:04:15 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-02-02 21:04:15 | INFO | train | epoch 152 | loss 5.34 | ppl 40.51 | wps 5929.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9728 | lr 0.000320618 | gnorm 0.97 | train_wall 323 | gb_free 6.1 | wall 53559
KL Stats: Epoch 152 Divergences: Uniform: 3.129606123982246 Unigram: 3.6617483343487573
2022-02-02 21:04:15 | INFO | fairseq.trainer | begin training epoch 153
2022-02-02 21:04:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:09:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:10:07 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 10.082 | ppl 1083.64 | wps 7963.3 | wpb 2034.1 | bsz 4 | num_updates 9792 | best_loss 9.243
2022-02-02 21:10:07 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-02-02 21:10:07 | INFO | train | epoch 153 | loss 5.332 | ppl 40.29 | wps 5926.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9792 | lr 0.000319569 | gnorm 0.969 | train_wall 323 | gb_free 6.1 | wall 53911
KL Stats: Epoch 153 Divergences: Uniform: 3.126607684733547 Unigram: 3.6627681404550607
2022-02-02 21:10:07 | INFO | fairseq.trainer | begin training epoch 154
2022-02-02 21:10:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:10:48 | INFO | train_inner | epoch 154:      8 / 64 loss=5.34, ppl=40.51, wps=5802.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=9800, lr=0.000319438, gnorm=0.978, train_wall=504, gb_free=6.1, wall=53952
2022-02-02 21:15:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:15:59 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 10.088 | ppl 1088.34 | wps 7971.9 | wpb 2034.1 | bsz 4 | num_updates 9856 | best_loss 9.243
2022-02-02 21:15:59 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-02-02 21:15:59 | INFO | train | epoch 154 | loss 5.327 | ppl 40.15 | wps 5928.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9856 | lr 0.000318529 | gnorm 0.982 | train_wall 323 | gb_free 6.1 | wall 54264
KL Stats: Epoch 154 Divergences: Uniform: 3.1301347962814177 Unigram: 3.675069533185963
2022-02-02 21:15:59 | INFO | fairseq.trainer | begin training epoch 155
2022-02-02 21:15:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:19:43 | INFO | train_inner | epoch 155:     44 / 64 loss=5.319, ppl=39.91, wps=6103.7, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=9900, lr=0.000317821, gnorm=0.994, train_wall=505, gb_free=6.1, wall=54488
2022-02-02 21:21:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:21:51 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 10.034 | ppl 1048.67 | wps 8017.8 | wpb 2034.1 | bsz 4 | num_updates 9920 | best_loss 9.243
2022-02-02 21:21:51 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-02-02 21:21:51 | INFO | train | epoch 155 | loss 5.319 | ppl 39.93 | wps 5939.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9920 | lr 0.0003175 | gnorm 0.999 | train_wall 323 | gb_free 6.1 | wall 54615
KL Stats: Epoch 155 Divergences: Uniform: 3.1344823457376725 Unigram: 3.6836872886034633
2022-02-02 21:21:51 | INFO | fairseq.trainer | begin training epoch 156
2022-02-02 21:21:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:27:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:27:43 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 10.079 | ppl 1081.29 | wps 8002.8 | wpb 2034.1 | bsz 4 | num_updates 9984 | best_loss 9.243
2022-02-02 21:27:43 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-02-02 21:27:43 | INFO | train | epoch 156 | loss 5.312 | ppl 39.74 | wps 5924.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 9984 | lr 0.000316481 | gnorm 0.987 | train_wall 324 | gb_free 6.1 | wall 54968
KL Stats: Epoch 156 Divergences: Uniform: 3.130094344913298 Unigram: 3.689510892562426
2022-02-02 21:27:43 | INFO | fairseq.trainer | begin training epoch 157
2022-02-02 21:27:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:29:05 | INFO | train_inner | epoch 157:     16 / 64 loss=5.315, ppl=39.8, wps=5801.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=10000, lr=0.000316228, gnorm=0.984, train_wall=505, gb_free=6.1, wall=55049
2022-02-02 21:33:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:33:36 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 10.044 | ppl 1055.91 | wps 8027.8 | wpb 2034.1 | bsz 4 | num_updates 10048 | best_loss 9.243
2022-02-02 21:33:36 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-02-02 21:33:36 | INFO | train | epoch 157 | loss 5.305 | ppl 39.53 | wps 5930.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10048 | lr 0.000315472 | gnorm 1.012 | train_wall 323 | gb_free 6.1 | wall 55320
KL Stats: Epoch 157 Divergences: Uniform: 3.133744268590908 Unigram: 3.691913602196583
2022-02-02 21:33:36 | INFO | fairseq.trainer | begin training epoch 158
2022-02-02 21:33:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:38:00 | INFO | train_inner | epoch 158:     52 / 64 loss=5.306, ppl=39.57, wps=6107.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=10100, lr=0.000314658, gnorm=1.021, train_wall=505, gb_free=6.1, wall=55585
2022-02-02 21:39:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:39:27 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 10.047 | ppl 1058.09 | wps 8020.6 | wpb 2034.1 | bsz 4 | num_updates 10112 | best_loss 9.243
2022-02-02 21:39:27 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-02-02 21:39:27 | INFO | train | epoch 158 | loss 5.3 | ppl 39.39 | wps 5942 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10112 | lr 0.000314472 | gnorm 1.008 | train_wall 323 | gb_free 6.1 | wall 55671
KL Stats: Epoch 158 Divergences: Uniform: 3.1351907787670097 Unigram: 3.698747602929235
2022-02-02 21:39:27 | INFO | fairseq.trainer | begin training epoch 159
2022-02-02 21:39:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:44:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:45:19 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 10.094 | ppl 1093.15 | wps 7990 | wpb 2034.1 | bsz 4 | num_updates 10176 | best_loss 9.243
2022-02-02 21:45:19 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-02-02 21:45:19 | INFO | train | epoch 159 | loss 5.292 | ppl 39.18 | wps 5931.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10176 | lr 0.000313481 | gnorm 1.001 | train_wall 323 | gb_free 6.1 | wall 56024
KL Stats: Epoch 159 Divergences: Uniform: 3.1386550836863645 Unigram: 3.7003430772028763
2022-02-02 21:45:19 | INFO | fairseq.trainer | begin training epoch 160
2022-02-02 21:45:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:47:22 | INFO | train_inner | epoch 160:     24 / 64 loss=5.285, ppl=38.99, wps=5806.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10200, lr=0.000313112, gnorm=0.995, train_wall=504, gb_free=6.1, wall=56146
2022-02-02 21:50:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:51:11 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 10.103 | ppl 1100.12 | wps 8030 | wpb 2034.1 | bsz 4 | num_updates 10240 | best_loss 9.243
2022-02-02 21:51:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 10240 updates
2022-02-02 21:51:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint160.pt
2022-02-02 21:51:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint160.pt
2022-02-02 21:51:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint160.pt (epoch 160 @ 10240 updates, score 10.103) (writing took 3.153644242323935 seconds)
2022-02-02 21:51:14 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-02-02 21:51:14 | INFO | train | epoch 160 | loss 5.284 | ppl 38.95 | wps 5879.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10240 | lr 0.0003125 | gnorm 1.014 | train_wall 323 | gb_free 6.1 | wall 56379
KL Stats: Epoch 160 Divergences: Uniform: 3.1394289070058283 Unigram: 3.7055291376058865
2022-02-02 21:51:15 | INFO | fairseq.trainer | begin training epoch 161
2022-02-02 21:51:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 21:56:20 | INFO | train_inner | epoch 161:     60 / 64 loss=5.291, ppl=39.14, wps=6069.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=10300, lr=0.000311588, gnorm=1.039, train_wall=505, gb_free=6.1, wall=56684
2022-02-02 21:56:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 21:57:07 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 10.065 | ppl 1071.42 | wps 7986.6 | wpb 2034.1 | bsz 4 | num_updates 10304 | best_loss 9.243
2022-02-02 21:57:07 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-02-02 21:57:07 | INFO | train | epoch 161 | loss 5.282 | ppl 38.9 | wps 5933.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10304 | lr 0.000311528 | gnorm 1.052 | train_wall 323 | gb_free 6.1 | wall 56731
KL Stats: Epoch 161 Divergences: Uniform: 3.1470057344203837 Unigram: 3.7130842835332785
2022-02-02 21:57:07 | INFO | fairseq.trainer | begin training epoch 162
2022-02-02 21:57:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:02:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:02:58 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 10.053 | ppl 1062.66 | wps 8007.8 | wpb 2034.1 | bsz 4 | num_updates 10368 | best_loss 9.243
2022-02-02 22:02:58 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-02-02 22:02:58 | INFO | train | epoch 162 | loss 5.275 | ppl 38.73 | wps 5934.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10368 | lr 0.000310565 | gnorm 1.045 | train_wall 323 | gb_free 6.1 | wall 57083
KL Stats: Epoch 162 Divergences: Uniform: 3.1459543433022192 Unigram: 3.714792702391684
2022-02-02 22:02:58 | INFO | fairseq.trainer | begin training epoch 163
2022-02-02 22:02:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:05:42 | INFO | train_inner | epoch 163:     32 / 64 loss=5.262, ppl=38.38, wps=5803.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10400, lr=0.000310087, gnorm=1.041, train_wall=504, gb_free=6.1, wall=57246
2022-02-02 22:08:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-02 22:08:51 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 10.115 | ppl 1109.04 | wps 7988.7 | wpb 2034.1 | bsz 4 | num_updates 10432 | best_loss 9.243
2022-02-02 22:08:51 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-02-02 22:08:51 | INFO | train | epoch 163 | loss 5.267 | ppl 38.51 | wps 5919.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10432 | lr 0.000309611 | gnorm 1.039 | train_wall 324 | gb_free 6.1 | wall 57436
KL Stats: Epoch 163 Divergences: Uniform: 3.146643818932748 Unigram: 3.720520906748901
2022-02-02 22:08:51 | INFO | fairseq.trainer | begin training epoch 164
2022-02-02 22:08:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:14:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:14:44 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 10.068 | ppl 1073.72 | wps 7981 | wpb 2034.1 | bsz 4 | num_updates 10496 | best_loss 9.243
2022-02-02 22:14:44 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-02-02 22:14:44 | INFO | train | epoch 164 | loss 5.262 | ppl 38.38 | wps 5924 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10496 | lr 0.000308665 | gnorm 1.04 | train_wall 324 | gb_free 6.1 | wall 57788
KL Stats: Epoch 164 Divergences: Uniform: 3.146465067751292 Unigram: 3.7221863148109784
2022-02-02 22:14:44 | INFO | fairseq.trainer | begin training epoch 165
2022-02-02 22:14:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:15:04 | INFO | train_inner | epoch 165:      4 / 64 loss=5.273, ppl=38.67, wps=5795.5, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=10500, lr=0.000308607, gnorm=1.043, train_wall=505, gb_free=6.1, wall=57809
2022-02-02 22:20:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:20:36 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 10.135 | ppl 1124.61 | wps 7992.4 | wpb 2034.1 | bsz 4 | num_updates 10560 | best_loss 9.243
2022-02-02 22:20:36 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-02-02 22:20:36 | INFO | train | epoch 165 | loss 5.256 | ppl 38.21 | wps 5930.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10560 | lr 0.000307729 | gnorm 1.075 | train_wall 323 | gb_free 6.1 | wall 58140
KL Stats: Epoch 165 Divergences: Uniform: 3.135283806881691 Unigram: 3.7283556826422597
2022-02-02 22:20:36 | INFO | fairseq.trainer | begin training epoch 166
2022-02-02 22:20:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:24:00 | INFO | train_inner | epoch 166:     40 / 64 loss=5.244, ppl=37.9, wps=6101.8, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=10600, lr=0.000307148, gnorm=1.068, train_wall=506, gb_free=6.1, wall=58344
2022-02-02 22:26:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:26:29 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 10.151 | ppl 1137.14 | wps 7980.3 | wpb 2034.1 | bsz 4 | num_updates 10624 | best_loss 9.243
2022-02-02 22:26:29 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-02-02 22:26:29 | INFO | train | epoch 166 | loss 5.249 | ppl 38.04 | wps 5925 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10624 | lr 0.0003068 | gnorm 1.063 | train_wall 323 | gb_free 6.1 | wall 58493
KL Stats: Epoch 166 Divergences: Uniform: 3.140691006495801 Unigram: 3.735473296719554
2022-02-02 22:26:29 | INFO | fairseq.trainer | begin training epoch 167
2022-02-02 22:26:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:31:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:32:21 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 10.123 | ppl 1114.81 | wps 7989.8 | wpb 2034.1 | bsz 4 | num_updates 10688 | best_loss 9.243
2022-02-02 22:32:21 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-02-02 22:32:21 | INFO | train | epoch 167 | loss 5.244 | ppl 37.9 | wps 5922.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10688 | lr 0.00030588 | gnorm 1.107 | train_wall 324 | gb_free 6.1 | wall 58846
KL Stats: Epoch 167 Divergences: Uniform: 3.144028057482737 Unigram: 3.7371249279338836
2022-02-02 22:32:21 | INFO | fairseq.trainer | begin training epoch 168
2022-02-02 22:32:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:33:22 | INFO | train_inner | epoch 168:     12 / 64 loss=5.248, ppl=37.99, wps=5794.4, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=10700, lr=0.000305709, gnorm=1.094, train_wall=505, gb_free=6.1, wall=58907
2022-02-02 22:37:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:38:14 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 10.164 | ppl 1147.15 | wps 7992.1 | wpb 2034.1 | bsz 4 | num_updates 10752 | best_loss 9.243
2022-02-02 22:38:14 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-02-02 22:38:14 | INFO | train | epoch 168 | loss 5.238 | ppl 37.75 | wps 5919.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10752 | lr 0.000304969 | gnorm 1.078 | train_wall 324 | gb_free 6.1 | wall 59198
KL Stats: Epoch 168 Divergences: Uniform: 3.144222959644573 Unigram: 3.7449251872086813
2022-02-02 22:38:14 | INFO | fairseq.trainer | begin training epoch 169
2022-02-02 22:38:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:42:19 | INFO | train_inner | epoch 169:     48 / 64 loss=5.235, ppl=37.67, wps=6092.6, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=10800, lr=0.00030429, gnorm=1.08, train_wall=506, gb_free=6.1, wall=59443
2022-02-02 22:43:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:44:06 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 10.101 | ppl 1098.56 | wps 7965.5 | wpb 2034.1 | bsz 4 | num_updates 10816 | best_loss 9.243
2022-02-02 22:44:06 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-02-02 22:44:06 | INFO | train | epoch 169 | loss 5.231 | ppl 37.56 | wps 5927.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10816 | lr 0.000304065 | gnorm 1.082 | train_wall 323 | gb_free 6.1 | wall 59551
KL Stats: Epoch 169 Divergences: Uniform: 3.146284653883323 Unigram: 3.7424202325129268
2022-02-02 22:44:06 | INFO | fairseq.trainer | begin training epoch 170
2022-02-02 22:44:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:49:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:49:59 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 10.141 | ppl 1129.44 | wps 7964.1 | wpb 2034.1 | bsz 4 | num_updates 10880 | best_loss 9.243
2022-02-02 22:49:59 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-02-02 22:49:59 | INFO | train | epoch 170 | loss 5.229 | ppl 37.5 | wps 5923.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10880 | lr 0.00030317 | gnorm 1.117 | train_wall 324 | gb_free 6.1 | wall 59903
KL Stats: Epoch 170 Divergences: Uniform: 3.148356865206878 Unigram: 3.749783722919408
2022-02-02 22:49:59 | INFO | fairseq.trainer | begin training epoch 171
2022-02-02 22:49:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 22:51:41 | INFO | train_inner | epoch 171:     20 / 64 loss=5.228, ppl=37.47, wps=5800.1, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=10900, lr=0.000302891, gnorm=1.118, train_wall=505, gb_free=6.1, wall=60005
2022-02-02 22:55:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 22:55:51 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 10.136 | ppl 1125.4 | wps 7989.7 | wpb 2034.1 | bsz 4 | num_updates 10944 | best_loss 9.243
2022-02-02 22:55:51 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-02-02 22:55:51 | INFO | train | epoch 171 | loss 5.222 | ppl 37.33 | wps 5933.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 10944 | lr 0.000302282 | gnorm 1.125 | train_wall 323 | gb_free 6.1 | wall 60255
KL Stats: Epoch 171 Divergences: Uniform: 3.15007580665287 Unigram: 3.756117263054033
2022-02-02 22:55:51 | INFO | fairseq.trainer | begin training epoch 172
2022-02-02 22:55:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:00:37 | INFO | train_inner | epoch 172:     56 / 64 loss=5.222, ppl=37.33, wps=6098.6, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=11000, lr=0.000301511, gnorm=1.128, train_wall=506, gb_free=6.1, wall=60541
2022-02-02 23:01:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:01:44 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 10.128 | ppl 1118.96 | wps 8015.3 | wpb 2034.1 | bsz 4 | num_updates 11008 | best_loss 9.243
2022-02-02 23:01:44 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-02-02 23:01:44 | INFO | train | epoch 172 | loss 5.216 | ppl 37.17 | wps 5924.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11008 | lr 0.000301402 | gnorm 1.129 | train_wall 324 | gb_free 6.1 | wall 60608
KL Stats: Epoch 172 Divergences: Uniform: 3.1572661975516882 Unigram: 3.766225573755189
2022-02-02 23:01:44 | INFO | fairseq.trainer | begin training epoch 173
2022-02-02 23:01:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:07:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:07:36 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 10.094 | ppl 1093.05 | wps 7975.7 | wpb 2034.1 | bsz 4 | num_updates 11072 | best_loss 9.243
2022-02-02 23:07:36 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-02-02 23:07:36 | INFO | train | epoch 173 | loss 5.211 | ppl 37.04 | wps 5929.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11072 | lr 0.000300529 | gnorm 1.129 | train_wall 323 | gb_free 6.1 | wall 60960
KL Stats: Epoch 173 Divergences: Uniform: 3.1536555894469878 Unigram: 3.762536505408357
2022-02-02 23:07:36 | INFO | fairseq.trainer | begin training epoch 174
2022-02-02 23:07:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:09:58 | INFO | train_inner | epoch 174:     28 / 64 loss=5.206, ppl=36.91, wps=5803.4, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=11100, lr=0.00030015, gnorm=1.114, train_wall=504, gb_free=6.1, wall=61103
2022-02-02 23:13:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:13:28 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 10.111 | ppl 1106.09 | wps 8006.4 | wpb 2034.1 | bsz 4 | num_updates 11136 | best_loss 9.243
2022-02-02 23:13:28 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-02-02 23:13:28 | INFO | train | epoch 174 | loss 5.204 | ppl 36.85 | wps 5929.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11136 | lr 0.000299665 | gnorm 1.09 | train_wall 323 | gb_free 6.1 | wall 61312
KL Stats: Epoch 174 Divergences: Uniform: 3.1552722301686336 Unigram: 3.767898699891234
2022-02-02 23:13:28 | INFO | fairseq.trainer | begin training epoch 175
2022-02-02 23:13:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:18:53 | INFO | train_inner | epoch 175:     64 / 64 loss=5.21, ppl=37.02, wps=6101.3, ups=0.19, wpb=32600.8, bsz=63.7, num_updates=11200, lr=0.000298807, gnorm=1.126, train_wall=505, gb_free=6.1, wall=61637
2022-02-02 23:18:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:19:20 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 10.183 | ppl 1162.16 | wps 7974.4 | wpb 2034.1 | bsz 4 | num_updates 11200 | best_loss 9.243
2022-02-02 23:19:20 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-02-02 23:19:20 | INFO | train | epoch 175 | loss 5.201 | ppl 36.79 | wps 5930.9 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11200 | lr 0.000298807 | gnorm 1.144 | train_wall 323 | gb_free 6.1 | wall 61665
KL Stats: Epoch 175 Divergences: Uniform: 3.1518210320275886 Unigram: 3.772476653654841
2022-02-02 23:19:20 | INFO | fairseq.trainer | begin training epoch 176
2022-02-02 23:19:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:24:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:25:12 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 10.151 | ppl 1136.85 | wps 8005.7 | wpb 2034.1 | bsz 4 | num_updates 11264 | best_loss 9.243
2022-02-02 23:25:12 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-02-02 23:25:12 | INFO | train | epoch 176 | loss 5.195 | ppl 36.63 | wps 5935.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11264 | lr 0.000297957 | gnorm 1.149 | train_wall 323 | gb_free 6.1 | wall 62016
KL Stats: Epoch 176 Divergences: Uniform: 3.154745027006228 Unigram: 3.778129583774436
2022-02-02 23:25:12 | INFO | fairseq.trainer | begin training epoch 177
2022-02-02 23:25:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:28:15 | INFO | train_inner | epoch 177:     36 / 64 loss=5.184, ppl=36.36, wps=5809.6, ups=0.18, wpb=32682.8, bsz=63.8, num_updates=11300, lr=0.000297482, gnorm=1.144, train_wall=505, gb_free=6.1, wall=62200
2022-02-02 23:30:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:31:04 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 10.145 | ppl 1131.98 | wps 7979.2 | wpb 2034.1 | bsz 4 | num_updates 11328 | best_loss 9.243
2022-02-02 23:31:04 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-02-02 23:31:04 | INFO | train | epoch 177 | loss 5.19 | ppl 36.51 | wps 5937.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11328 | lr 0.000297114 | gnorm 1.159 | train_wall 323 | gb_free 6.1 | wall 62368
KL Stats: Epoch 177 Divergences: Uniform: 3.1544833191754393 Unigram: 3.7813631253299635
2022-02-02 23:31:04 | INFO | fairseq.trainer | begin training epoch 178
2022-02-02 23:31:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:36:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:36:56 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 10.187 | ppl 1165.73 | wps 8003.8 | wpb 2034.1 | bsz 4 | num_updates 11392 | best_loss 9.243
2022-02-02 23:36:56 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-02-02 23:36:56 | INFO | train | epoch 178 | loss 5.185 | ppl 36.39 | wps 5932 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11392 | lr 0.000296278 | gnorm 1.163 | train_wall 323 | gb_free 6.1 | wall 62720
KL Stats: Epoch 178 Divergences: Uniform: 3.1568866023687168 Unigram: 3.7878384198604875
2022-02-02 23:36:56 | INFO | fairseq.trainer | begin training epoch 179
2022-02-02 23:36:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:37:37 | INFO | train_inner | epoch 179:      8 / 64 loss=5.191, ppl=36.53, wps=5806.7, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=11400, lr=0.000296174, gnorm=1.175, train_wall=504, gb_free=6.1, wall=62761
2022-02-02 23:42:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:42:48 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 10.155 | ppl 1140.36 | wps 7981.1 | wpb 2034.1 | bsz 4 | num_updates 11456 | best_loss 9.243
2022-02-02 23:42:48 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-02-02 23:42:48 | INFO | train | epoch 179 | loss 5.178 | ppl 36.21 | wps 5925.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11456 | lr 0.00029545 | gnorm 1.166 | train_wall 323 | gb_free 6.1 | wall 63073
KL Stats: Epoch 179 Divergences: Uniform: 3.159428088970836 Unigram: 3.7924603431742296
2022-02-02 23:42:48 | INFO | fairseq.trainer | begin training epoch 180
2022-02-02 23:42:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:46:33 | INFO | train_inner | epoch 180:     44 / 64 loss=5.17, ppl=36, wps=6092.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11500, lr=0.000294884, gnorm=1.148, train_wall=506, gb_free=6.1, wall=63297
2022-02-02 23:48:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:48:41 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 10.132 | ppl 1122.47 | wps 7982.7 | wpb 2034.1 | bsz 4 | num_updates 11520 | best_loss 9.243
2022-02-02 23:48:41 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-02-02 23:48:41 | INFO | train | epoch 180 | loss 5.172 | ppl 36.06 | wps 5922.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11520 | lr 0.000294628 | gnorm 1.161 | train_wall 324 | gb_free 6.1 | wall 63425
KL Stats: Epoch 180 Divergences: Uniform: 3.1625909684659956 Unigram: 3.7959072844907014
2022-02-02 23:48:41 | INFO | fairseq.trainer | begin training epoch 181
2022-02-02 23:48:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:54:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-02 23:54:33 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 10.141 | ppl 1129.03 | wps 7969.7 | wpb 2034.1 | bsz 4 | num_updates 11584 | best_loss 9.243
2022-02-02 23:54:33 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-02-02 23:54:33 | INFO | train | epoch 181 | loss 5.171 | ppl 36.02 | wps 5928.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11584 | lr 0.000293813 | gnorm 1.196 | train_wall 323 | gb_free 6.1 | wall 63778
KL Stats: Epoch 181 Divergences: Uniform: 3.163982624590688 Unigram: 3.8031044279370265
2022-02-02 23:54:33 | INFO | fairseq.trainer | begin training epoch 182
2022-02-02 23:54:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-02 23:55:55 | INFO | train_inner | epoch 182:     16 / 64 loss=5.174, ppl=36.1, wps=5801.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=11600, lr=0.00029361, gnorm=1.195, train_wall=505, gb_free=6.1, wall=63859
2022-02-02 23:59:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:00:26 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 10.177 | ppl 1157.72 | wps 7972.4 | wpb 2034.1 | bsz 4 | num_updates 11648 | best_loss 9.243
2022-02-03 00:00:26 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-02-03 00:00:26 | INFO | train | epoch 182 | loss 5.164 | ppl 35.84 | wps 5926 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11648 | lr 0.000293005 | gnorm 1.189 | train_wall 323 | gb_free 6.1 | wall 64130
KL Stats: Epoch 182 Divergences: Uniform: 3.1630004185652445 Unigram: 3.8043305868579713
2022-02-03 00:00:26 | INFO | fairseq.trainer | begin training epoch 183
2022-02-03 00:00:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:04:51 | INFO | train_inner | epoch 183:     52 / 64 loss=5.16, ppl=35.76, wps=6095.9, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11700, lr=0.000292353, gnorm=1.184, train_wall=506, gb_free=6.1, wall=64395
2022-02-03 00:05:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:06:18 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 10.161 | ppl 1145.19 | wps 8002.1 | wpb 2034.1 | bsz 4 | num_updates 11712 | best_loss 9.243
2022-02-03 00:06:18 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-02-03 00:06:18 | INFO | train | epoch 183 | loss 5.16 | ppl 35.75 | wps 5927.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11712 | lr 0.000292203 | gnorm 1.167 | train_wall 323 | gb_free 6.1 | wall 64483
KL Stats: Epoch 183 Divergences: Uniform: 3.163845854233066 Unigram: 3.8134567412703606
2022-02-03 00:06:18 | INFO | fairseq.trainer | begin training epoch 184
2022-02-03 00:06:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:11:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:12:10 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 10.153 | ppl 1138.88 | wps 7974.8 | wpb 2034.1 | bsz 4 | num_updates 11776 | best_loss 9.243
2022-02-03 00:12:10 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-02-03 00:12:10 | INFO | train | epoch 184 | loss 5.155 | ppl 35.63 | wps 5930.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11776 | lr 0.000291408 | gnorm 1.211 | train_wall 323 | gb_free 6.1 | wall 64835
KL Stats: Epoch 184 Divergences: Uniform: 3.162201623730378 Unigram: 3.8088081694686355
2022-02-03 00:12:10 | INFO | fairseq.trainer | begin training epoch 185
2022-02-03 00:12:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:14:13 | INFO | train_inner | epoch 185:     24 / 64 loss=5.155, ppl=35.63, wps=5803.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=11800, lr=0.000291111, gnorm=1.211, train_wall=504, gb_free=6.1, wall=64957
2022-02-03 00:17:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-03 00:18:03 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 10.152 | ppl 1137.51 | wps 7983.7 | wpb 2034.1 | bsz 4 | num_updates 11840 | best_loss 9.243
2022-02-03 00:18:03 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-02-03 00:18:03 | INFO | train | epoch 185 | loss 5.151 | ppl 35.53 | wps 5923.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11840 | lr 0.000290619 | gnorm 1.218 | train_wall 324 | gb_free 6.1 | wall 65187
KL Stats: Epoch 185 Divergences: Uniform: 3.162161961270606 Unigram: 3.8196136789537776
2022-02-03 00:18:03 | INFO | fairseq.trainer | begin training epoch 186
2022-02-03 00:18:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:23:09 | INFO | train_inner | epoch 186:     60 / 64 loss=5.152, ppl=35.55, wps=6095.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=11900, lr=0.000289886, gnorm=1.217, train_wall=506, gb_free=6.1, wall=65493
2022-02-03 00:23:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:23:55 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 10.181 | ppl 1161.2 | wps 7992.3 | wpb 2034.1 | bsz 4 | num_updates 11904 | best_loss 9.243
2022-02-03 00:23:55 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-02-03 00:23:55 | INFO | train | epoch 186 | loss 5.145 | ppl 35.39 | wps 5927.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11904 | lr 0.000289837 | gnorm 1.228 | train_wall 323 | gb_free 6.1 | wall 65540
KL Stats: Epoch 186 Divergences: Uniform: 3.1670215882251807 Unigram: 3.8222141481747482
2022-02-03 00:23:55 | INFO | fairseq.trainer | begin training epoch 187
2022-02-03 00:23:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:29:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:29:48 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 10.141 | ppl 1128.76 | wps 7995.3 | wpb 2034.1 | bsz 4 | num_updates 11968 | best_loss 9.243
2022-02-03 00:29:48 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-02-03 00:29:48 | INFO | train | epoch 187 | loss 5.141 | ppl 35.29 | wps 5926.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 11968 | lr 0.000289061 | gnorm 1.217 | train_wall 323 | gb_free 6.1 | wall 65892
KL Stats: Epoch 187 Divergences: Uniform: 3.166585583820165 Unigram: 3.8225607351361517
2022-02-03 00:29:48 | INFO | fairseq.trainer | begin training epoch 188
2022-02-03 00:29:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:32:31 | INFO | train_inner | epoch 188:     32 / 64 loss=5.133, ppl=35.09, wps=5801.3, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12000, lr=0.000288675, gnorm=1.228, train_wall=505, gb_free=6.1, wall=66055
2022-02-03 00:35:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-03 00:35:40 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 10.148 | ppl 1134.69 | wps 8003.4 | wpb 2034.1 | bsz 4 | num_updates 12032 | best_loss 9.243
2022-02-03 00:35:40 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-02-03 00:35:40 | INFO | train | epoch 188 | loss 5.134 | ppl 35.12 | wps 5932.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12032 | lr 0.000288291 | gnorm 1.226 | train_wall 323 | gb_free 6.1 | wall 66244
KL Stats: Epoch 188 Divergences: Uniform: 3.168558370680794 Unigram: 3.82705964822003
2022-02-03 00:35:40 | INFO | fairseq.trainer | begin training epoch 189
2022-02-03 00:35:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:41:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:41:32 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 10.143 | ppl 1130.4 | wps 7993.7 | wpb 2034.1 | bsz 4 | num_updates 12096 | best_loss 9.243
2022-02-03 00:41:32 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-02-03 00:41:32 | INFO | train | epoch 189 | loss 5.13 | ppl 35.02 | wps 5940.5 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12096 | lr 0.000287527 | gnorm 1.241 | train_wall 323 | gb_free 6.1 | wall 66596
KL Stats: Epoch 189 Divergences: Uniform: 3.1660242144182105 Unigram: 3.8269602212173592
2022-02-03 00:41:32 | INFO | fairseq.trainer | begin training epoch 190
2022-02-03 00:41:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:41:52 | INFO | train_inner | epoch 190:      4 / 64 loss=5.138, ppl=35.22, wps=5809.7, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=12100, lr=0.00028748, gnorm=1.233, train_wall=504, gb_free=6.1, wall=66616
2022-02-03 00:46:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:47:24 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 10.191 | ppl 1168.88 | wps 7994.2 | wpb 2034.1 | bsz 4 | num_updates 12160 | best_loss 9.243
2022-02-03 00:47:24 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-02-03 00:47:24 | INFO | train | epoch 190 | loss 5.126 | ppl 34.92 | wps 5928.6 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12160 | lr 0.00028677 | gnorm 1.245 | train_wall 323 | gb_free 6.1 | wall 66948
KL Stats: Epoch 190 Divergences: Uniform: 3.1685556440526943 Unigram: 3.8338756341168048
2022-02-03 00:47:24 | INFO | fairseq.trainer | begin training epoch 191
2022-02-03 00:47:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:50:48 | INFO | train_inner | epoch 191:     40 / 64 loss=5.117, ppl=34.71, wps=6101.9, ups=0.19, wpb=32682.8, bsz=63.8, num_updates=12200, lr=0.000286299, gnorm=1.255, train_wall=506, gb_free=6.1, wall=67152
2022-02-03 00:52:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:53:16 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 10.167 | ppl 1149.86 | wps 7977.6 | wpb 2034.1 | bsz 4 | num_updates 12224 | best_loss 9.243
2022-02-03 00:53:16 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-02-03 00:53:16 | INFO | train | epoch 191 | loss 5.122 | ppl 34.83 | wps 5931 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12224 | lr 0.000286018 | gnorm 1.259 | train_wall 323 | gb_free 6.1 | wall 67300
KL Stats: Epoch 191 Divergences: Uniform: 3.171716451433252 Unigram: 3.8337122997177264
2022-02-03 00:53:16 | INFO | fairseq.trainer | begin training epoch 192
2022-02-03 00:53:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 00:58:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 00:59:08 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 10.182 | ppl 1161.99 | wps 7991.8 | wpb 2034.1 | bsz 4 | num_updates 12288 | best_loss 9.243
2022-02-03 00:59:08 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-02-03 00:59:08 | INFO | train | epoch 192 | loss 5.119 | ppl 34.75 | wps 5928.2 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12288 | lr 0.000285272 | gnorm 1.295 | train_wall 323 | gb_free 6.1 | wall 67653
KL Stats: Epoch 192 Divergences: Uniform: 3.1747955324593726 Unigram: 3.8415202444730947
2022-02-03 00:59:08 | INFO | fairseq.trainer | begin training epoch 193
2022-02-03 00:59:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:00:09 | INFO | train_inner | epoch 193:     12 / 64 loss=5.122, ppl=34.83, wps=5800.7, ups=0.18, wpb=32594.2, bsz=63.7, num_updates=12300, lr=0.000285133, gnorm=1.286, train_wall=505, gb_free=6.1, wall=67714
2022-02-03 01:04:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:05:00 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 10.194 | ppl 1171.09 | wps 7972.1 | wpb 2034.1 | bsz 4 | num_updates 12352 | best_loss 9.243
2022-02-03 01:05:00 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-02-03 01:05:00 | INFO | train | epoch 193 | loss 5.115 | ppl 34.66 | wps 5931.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12352 | lr 0.000284532 | gnorm 1.295 | train_wall 323 | gb_free 6.1 | wall 68005
KL Stats: Epoch 193 Divergences: Uniform: 3.172870015018631 Unigram: 3.8424381697595886
2022-02-03 01:05:00 | INFO | fairseq.trainer | begin training epoch 194
2022-02-03 01:05:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:09:05 | INFO | train_inner | epoch 194:     48 / 64 loss=5.112, ppl=34.59, wps=6102.1, ups=0.19, wpb=32686.1, bsz=63.8, num_updates=12400, lr=0.000283981, gnorm=1.287, train_wall=506, gb_free=6.1, wall=68249
2022-02-03 01:10:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:10:53 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 10.178 | ppl 1158.1 | wps 8023 | wpb 2034.1 | bsz 4 | num_updates 12416 | best_loss 9.243
2022-02-03 01:10:53 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-02-03 01:10:53 | INFO | train | epoch 194 | loss 5.111 | ppl 34.55 | wps 5930.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12416 | lr 0.000283798 | gnorm 1.285 | train_wall 323 | gb_free 6.1 | wall 68357
KL Stats: Epoch 194 Divergences: Uniform: 3.174538725490206 Unigram: 3.8485197760611176
2022-02-03 01:10:53 | INFO | fairseq.trainer | begin training epoch 195
2022-02-03 01:10:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:16:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:16:45 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 10.208 | ppl 1182.61 | wps 7983.5 | wpb 2034.1 | bsz 4 | num_updates 12480 | best_loss 9.243
2022-02-03 01:16:45 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-02-03 01:16:45 | INFO | train | epoch 195 | loss 5.107 | ppl 34.47 | wps 5923.1 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12480 | lr 0.000283069 | gnorm 1.313 | train_wall 324 | gb_free 6.1 | wall 68710
KL Stats: Epoch 195 Divergences: Uniform: 3.1739021498946363 Unigram: 3.852098276178095
2022-02-03 01:16:45 | INFO | fairseq.trainer | begin training epoch 196
2022-02-03 01:16:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:18:27 | INFO | train_inner | epoch 196:     20 / 64 loss=5.105, ppl=34.41, wps=5799.8, ups=0.18, wpb=32597.5, bsz=63.7, num_updates=12500, lr=0.000282843, gnorm=1.304, train_wall=505, gb_free=6.1, wall=68812
2022-02-03 01:22:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:22:38 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 10.16 | ppl 1144.25 | wps 7957.9 | wpb 2034.1 | bsz 4 | num_updates 12544 | best_loss 9.243
2022-02-03 01:22:38 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-02-03 01:22:38 | INFO | train | epoch 196 | loss 5.101 | ppl 34.31 | wps 5928.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12544 | lr 0.000282346 | gnorm 1.306 | train_wall 323 | gb_free 6.1 | wall 69062
KL Stats: Epoch 196 Divergences: Uniform: 3.1741260019768833 Unigram: 3.852041309270338
2022-02-03 01:22:38 | INFO | fairseq.trainer | begin training epoch 197
2022-02-03 01:22:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:27:23 | INFO | train_inner | epoch 197:     56 / 64 loss=5.103, ppl=34.36, wps=6100.8, ups=0.19, wpb=32679.4, bsz=63.8, num_updates=12600, lr=0.000281718, gnorm=1.321, train_wall=506, gb_free=6.1, wall=69347
2022-02-03 01:28:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:28:30 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 10.197 | ppl 1174.02 | wps 7985.9 | wpb 2034.1 | bsz 4 | num_updates 12608 | best_loss 9.243
2022-02-03 01:28:30 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-02-03 01:28:30 | INFO | train | epoch 197 | loss 5.098 | ppl 34.26 | wps 5932 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12608 | lr 0.000281629 | gnorm 1.319 | train_wall 323 | gb_free 6.1 | wall 69414
KL Stats: Epoch 197 Divergences: Uniform: 3.1760397996576533 Unigram: 3.8540294048683514
2022-02-03 01:28:30 | INFO | fairseq.trainer | begin training epoch 198
2022-02-03 01:28:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:33:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:34:22 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 10.183 | ppl 1162.7 | wps 7953.5 | wpb 2034.1 | bsz 4 | num_updates 12672 | best_loss 9.243
2022-02-03 01:34:22 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-02-03 01:34:22 | INFO | train | epoch 198 | loss 5.093 | ppl 34.13 | wps 5924.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12672 | lr 0.000280917 | gnorm 1.358 | train_wall 323 | gb_free 6.1 | wall 69766
KL Stats: Epoch 198 Divergences: Uniform: 3.178488251689906 Unigram: 3.8598334381858668
2022-02-03 01:34:22 | INFO | fairseq.trainer | begin training epoch 199
2022-02-03 01:34:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:36:45 | INFO | train_inner | epoch 199:     28 / 64 loss=5.085, ppl=33.93, wps=5799.8, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=12700, lr=0.000280607, gnorm=1.337, train_wall=505, gb_free=6.1, wall=69909
2022-02-03 01:39:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:40:14 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 10.215 | ppl 1188.27 | wps 7985.7 | wpb 2034.1 | bsz 4 | num_updates 12736 | best_loss 9.243
2022-02-03 01:40:14 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-02-03 01:40:14 | INFO | train | epoch 199 | loss 5.089 | ppl 34.03 | wps 5928.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12736 | lr 0.00028021 | gnorm 1.331 | train_wall 323 | gb_free 6.1 | wall 70119
KL Stats: Epoch 199 Divergences: Uniform: 3.1736841470571924 Unigram: 3.8652829539759974
2022-02-03 01:40:14 | INFO | fairseq.trainer | begin training epoch 200
2022-02-03 01:40:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:45:39 | INFO | train_inner | epoch 200:     64 / 64 loss=5.099, ppl=34.27, wps=6102, ups=0.19, wpb=32597.5, bsz=63.7, num_updates=12800, lr=0.000279508, gnorm=1.347, train_wall=504, gb_free=6.1, wall=70443
2022-02-03 01:45:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
Traceback (most recent call last):
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2022-02-03 01:46:07 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 10.196 | ppl 1173.23 | wps 7975.6 | wpb 2034.1 | bsz 4 | num_updates 12800 | best_loss 9.243
2022-02-03 01:46:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 12800 updates
2022-02-03 01:46:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint200.pt
2022-02-03 01:46:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint200.pt
2022-02-03 01:46:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w2-jelinek_0.09_0.01_0.9_#2/checkpoint200.pt (epoch 200 @ 12800 updates, score 10.196) (writing took 3.0743150198832154 seconds)
2022-02-03 01:46:10 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-02-03 01:46:10 | INFO | train | epoch 200 | loss 5.085 | ppl 33.94 | wps 5879.8 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12800 | lr 0.000279508 | gnorm 1.35 | train_wall 323 | gb_free 6.1 | wall 70474
KL Stats: Epoch 200 Divergences: Uniform: 3.179796919735918 Unigram: 3.8702996017347693
2022-02-03 01:46:10 | INFO | fairseq.trainer | begin training epoch 201
2022-02-03 01:46:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:51:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:52:02 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 10.209 | ppl 1183.82 | wps 7955 | wpb 2034.1 | bsz 4 | num_updates 12864 | best_loss 9.243
2022-02-03 01:52:02 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-02-03 01:52:02 | INFO | train | epoch 201 | loss 5.081 | ppl 33.84 | wps 5933.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12864 | lr 0.000278812 | gnorm 1.348 | train_wall 323 | gb_free 6.1 | wall 70826
KL Stats: Epoch 201 Divergences: Uniform: 3.176277559384525 Unigram: 3.8691803603716233
2022-02-03 01:52:02 | INFO | fairseq.trainer | begin training epoch 202
2022-02-03 01:52:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 01:55:06 | INFO | train_inner | epoch 202:     36 / 64 loss=5.071, ppl=33.62, wps=5767.3, ups=0.18, wpb=32679.4, bsz=63.8, num_updates=12900, lr=0.000278423, gnorm=1.348, train_wall=506, gb_free=6.1, wall=71010
2022-02-03 01:57:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 01:57:55 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 10.178 | ppl 1158.08 | wps 7992.4 | wpb 2034.1 | bsz 4 | num_updates 12928 | best_loss 9.243
2022-02-03 01:57:55 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-02-03 01:57:55 | INFO | train | epoch 202 | loss 5.078 | ppl 33.78 | wps 5917.4 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12928 | lr 0.000278121 | gnorm 1.368 | train_wall 324 | gb_free 6.1 | wall 71179
KL Stats: Epoch 202 Divergences: Uniform: 3.1832268620087634 Unigram: 3.8738243396791203
2022-02-03 01:57:55 | INFO | fairseq.trainer | begin training epoch 203
2022-02-03 01:57:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 02:03:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 02:03:47 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 10.199 | ppl 1175.48 | wps 7982 | wpb 2034.1 | bsz 4 | num_updates 12992 | best_loss 9.243
2022-02-03 02:03:47 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-02-03 02:03:47 | INFO | train | epoch 203 | loss 5.071 | ppl 33.63 | wps 5931.7 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 12992 | lr 0.000277435 | gnorm 1.34 | train_wall 323 | gb_free 6.1 | wall 71531
KL Stats: Epoch 203 Divergences: Uniform: 3.177170601094293 Unigram: 3.8752247579246957
2022-02-03 02:03:47 | INFO | fairseq.trainer | begin training epoch 204
2022-02-03 02:03:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-03 02:04:28 | INFO | train_inner | epoch 204:      8 / 64 loss=5.078, ppl=33.77, wps=5802.2, ups=0.18, wpb=32600.8, bsz=63.7, num_updates=13000, lr=0.00027735, gnorm=1.356, train_wall=505, gb_free=6.1, wall=71572
2022-02-03 02:09:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-03 02:09:40 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 10.149 | ppl 1135.1 | wps 7976.1 | wpb 2034.1 | bsz 4 | num_updates 13056 | best_loss 9.243
2022-02-03 02:09:40 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-02-03 02:09:40 | INFO | train | epoch 204 | loss 5.068 | ppl 33.55 | wps 5917.3 | ups 0.18 | wpb 32634.8 | bsz 63.8 | num_updates 13056 | lr 0.000276755 | gnorm 1.379 | train_wall 324 | gb_free 6.1 | wall 71884
KL Stats: Epoch 204 Divergences: Uniform: 3.181683828393671 Unigram: 3.8787013789150273
2022-02-03 02:09:40 | INFO | fairseq.trainer | begin training epoch 205
2022-02-03 02:09:40 | INFO | fairseq_cli.train | Start iterating over samples
User defined signal 2
