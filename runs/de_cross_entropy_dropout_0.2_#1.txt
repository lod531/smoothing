Sender: LSF System <lsfadmin@eu-g2-08>
Subject: Job 208027603: <de_cross_entropy_dropout_0.2_#1> in cluster <euler> Done

Job <de_cross_entropy_dropout_0.2_#1> was submitted from host <eu-login-12> by user <andriusb> in cluster <euler> at Sat Mar 12 10:20:45 2022
Job was executed on host(s) <eu-g2-08>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Sat Mar 12 10:21:40 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Sat Mar 12 10:21:40 2022
Terminated at Sat Mar 12 23:38:51 2022
Results reported at Sat Mar 12 23:38:51 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/de --save-dir /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.2 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575611 --fp16 --no-epoch-checkpoints --patience 3 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   50871.56 sec.
    Max Memory :                                 3915 MB
    Average Memory :                             3157.95 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               16085.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   47830 sec.
    Turnaround time :                            47886 sec.

The output (if any) follows:

2022-03-12 10:21:46 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575611, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.2, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/de', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575611, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-12 10:21:46 | INFO | fairseq.tasks.language_modeling | dictionary: 34528 types
2022-03-12 10:21:47 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(34528, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=34528, bias=False)
  )
)
2022-03-12 10:21:47 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-12 10:21:47 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-12 10:21:47 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-12 10:21:47 | INFO | fairseq_cli.train | num. shared model params: 36,592,640 (num. trained: 36,592,640)
2022-03-12 10:21:47 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-12 10:21:47 | INFO | fairseq.data.data_utils | loaded 2,294 examples from: data-bin/de/valid
2022-03-12 10:21:50 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-12 10:21:50 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-12 10:21:50 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-12 10:21:50 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-12 10:21:50 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-12 10:21:50 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-12 10:21:50 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 10:21:50 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 10:21:50 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-12 10:21:50 | INFO | fairseq.data.data_utils | loaded 45,920 examples from: data-bin/de/train
2022-03-12 10:21:51 | INFO | fairseq.trainer | begin training epoch 1
2022-03-12 10:21:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 10:21:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-12 10:21:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 10:22:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 10:22:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 10:25:34 | INFO | train_inner | epoch 001:    104 / 392 loss=14.679, ppl=26229.4, wps=31746.9, ups=0.48, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.401, loss_scale=8, train_wall=199, gb_free=9.8, wall=224
2022-03-12 10:29:07 | INFO | train_inner | epoch 001:    204 / 392 loss=13.077, ppl=8641.03, wps=30751.5, ups=0.47, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=0.741, loss_scale=16, train_wall=190, gb_free=9.8, wall=437
2022-03-12 10:32:42 | INFO | train_inner | epoch 001:    304 / 392 loss=12.114, ppl=4432.62, wps=30600.6, ups=0.47, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.467, loss_scale=32, train_wall=191, gb_free=9.8, wall=652
2022-03-12 10:35:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 10:36:09 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.401 | ppl 2703.51 | wps 53410.7 | wpb 511.9 | bsz 1 | num_updates 388
2022-03-12 10:36:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-12 10:36:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 10:36:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 10:36:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 1 @ 388 updates, score 11.401) (writing took 2.110675973992329 seconds)
2022-03-12 10:36:11 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-12 10:36:11 | INFO | train | epoch 001 | loss 12.909 | ppl 7689.89 | wps 30087.2 | ups 0.46 | wpb 65404.5 | bsz 127.7 | num_updates 388 | lr 4.85903e-05 | gnorm 1.019 | loss_scale 64 | train_wall 743 | gb_free 9.8 | wall 861
2022-03-12 10:36:11 | INFO | fairseq.trainer | begin training epoch 2
2022-03-12 10:36:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 10:36:36 | INFO | train_inner | epoch 002:     12 / 392 loss=11.579, ppl=3058.49, wps=27685.5, ups=0.43, wpb=65022.4, bsz=127, num_updates=400, lr=5.009e-05, gnorm=0.389, loss_scale=64, train_wall=185, gb_free=9.8, wall=886
2022-03-12 10:39:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 10:40:13 | INFO | train_inner | epoch 002:    113 / 392 loss=11.28, ppl=2486.85, wps=30209.9, ups=0.46, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.394, loss_scale=32, train_wall=193, gb_free=9.8, wall=1103
2022-03-12 10:43:45 | INFO | train_inner | epoch 002:    213 / 392 loss=10.95, ppl=1978.45, wps=30989.3, ups=0.47, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.428, loss_scale=64, train_wall=188, gb_free=9.8, wall=1315
2022-03-12 10:44:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 10:47:20 | INFO | train_inner | epoch 002:    314 / 392 loss=10.579, ppl=1529.74, wps=30501.1, ups=0.47, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.537, loss_scale=32, train_wall=191, gb_free=9.8, wall=1530
2022-03-12 10:49:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 10:50:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 10:50:28 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.052 | ppl 1061.58 | wps 54363.7 | wpb 511.9 | bsz 1 | num_updates 777 | best_loss 10.052
2022-03-12 10:50:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 777 updates
2022-03-12 10:50:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 10:50:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 10:50:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 2 @ 777 updates, score 10.052) (writing took 2.240259693993721 seconds)
2022-03-12 10:50:30 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-12 10:50:30 | INFO | train | epoch 002 | loss 10.826 | ppl 1815.7 | wps 29620.4 | ups 0.45 | wpb 65404.8 | bsz 127.7 | num_updates 777 | lr 9.72056e-05 | gnorm 0.467 | loss_scale 32 | train_wall 741 | gb_free 9.8 | wall 1720
2022-03-12 10:50:30 | INFO | fairseq.trainer | begin training epoch 3
2022-03-12 10:50:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 10:51:19 | INFO | train_inner | epoch 003:     23 / 392 loss=10.262, ppl=1227.54, wps=27149, ups=0.42, wpb=65029.1, bsz=127, num_updates=800, lr=0.00010008, gnorm=0.551, loss_scale=32, train_wall=190, gb_free=9.8, wall=1769
2022-03-12 10:54:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 10:54:54 | INFO | train_inner | epoch 003:    124 / 392 loss=9.988, ppl=1015.35, wps=30508.1, ups=0.47, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.61, loss_scale=32, train_wall=191, gb_free=9.8, wall=1984
2022-03-12 10:58:27 | INFO | train_inner | epoch 003:    224 / 392 loss=9.756, ppl=864.44, wps=30761.2, ups=0.47, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.664, loss_scale=32, train_wall=190, gb_free=9.8, wall=2197
2022-03-12 10:59:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:02:00 | INFO | train_inner | epoch 003:    325 / 392 loss=9.554, ppl=751.72, wps=30842.4, ups=0.47, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.681, loss_scale=32, train_wall=189, gb_free=9.8, wall=2410
2022-03-12 11:03:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:04:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 11:04:42 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.2 | ppl 588.08 | wps 55682.1 | wpb 511.9 | bsz 1 | num_updates 1166 | best_loss 9.2
2022-03-12 11:04:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1166 updates
2022-03-12 11:04:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 11:04:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 11:04:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 3 @ 1166 updates, score 9.2) (writing took 2.1697919220023323 seconds)
2022-03-12 11:04:44 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-12 11:04:44 | INFO | train | epoch 003 | loss 9.727 | ppl 847.4 | wps 29791.9 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 1166 | lr 0.000145821 | gnorm 0.663 | loss_scale 32 | train_wall 738 | gb_free 9.8 | wall 2574
2022-03-12 11:04:44 | INFO | fairseq.trainer | begin training epoch 4
2022-03-12 11:04:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 11:05:58 | INFO | train_inner | epoch 004:     34 / 392 loss=9.367, ppl=660.29, wps=27322, ups=0.42, wpb=65025.8, bsz=127, num_updates=1200, lr=0.00015007, gnorm=0.73, loss_scale=32, train_wall=189, gb_free=9.8, wall=2648
2022-03-12 11:08:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:09:33 | INFO | train_inner | epoch 004:    135 / 392 loss=9.187, ppl=582.94, wps=30428, ups=0.46, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.791, loss_scale=32, train_wall=192, gb_free=9.8, wall=2863
2022-03-12 11:13:08 | INFO | train_inner | epoch 004:    235 / 392 loss=9.027, ppl=521.7, wps=30497.6, ups=0.47, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.793, loss_scale=32, train_wall=191, gb_free=9.8, wall=3078
2022-03-12 11:13:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:16:42 | INFO | train_inner | epoch 004:    336 / 392 loss=8.87, ppl=468.04, wps=30588.1, ups=0.47, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.817, loss_scale=32, train_wall=191, gb_free=9.8, wall=3292
2022-03-12 11:18:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 11:19:04 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.522 | ppl 367.58 | wps 52451.5 | wpb 511.9 | bsz 1 | num_updates 1556 | best_loss 8.522
2022-03-12 11:19:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1556 updates
2022-03-12 11:19:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 11:19:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 11:19:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 4 @ 1556 updates, score 8.522) (writing took 2.2001617689966224 seconds)
2022-03-12 11:19:06 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-12 11:19:06 | INFO | train | epoch 004 | loss 9.012 | ppl 516.18 | wps 29586.3 | ups 0.45 | wpb 65405.2 | bsz 127.7 | num_updates 1556 | lr 0.000194561 | gnorm 0.807 | loss_scale 64 | train_wall 744 | gb_free 9.8 | wall 3436
2022-03-12 11:19:06 | INFO | fairseq.trainer | begin training epoch 5
2022-03-12 11:19:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 11:19:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:20:43 | INFO | train_inner | epoch 005:     45 / 392 loss=8.696, ppl=414.64, wps=27040.7, ups=0.42, wpb=65025.8, bsz=127, num_updates=1600, lr=0.00020006, gnorm=0.879, loss_scale=32, train_wall=190, gb_free=9.8, wall=3533
2022-03-12 11:24:16 | INFO | train_inner | epoch 005:    145 / 392 loss=8.527, ppl=368.78, wps=30717.5, ups=0.47, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.843, loss_scale=64, train_wall=190, gb_free=9.8, wall=3746
2022-03-12 11:24:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:27:52 | INFO | train_inner | epoch 005:    246 / 392 loss=8.38, ppl=333.1, wps=30321.5, ups=0.46, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.85, loss_scale=32, train_wall=192, gb_free=9.8, wall=3962
2022-03-12 11:29:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:31:26 | INFO | train_inner | epoch 005:    347 / 392 loss=8.244, ppl=303.22, wps=30638.8, ups=0.47, wpb=65532.7, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.871, loss_scale=32, train_wall=190, gb_free=9.8, wall=4176
2022-03-12 11:32:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 11:33:22 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.958 | ppl 248.7 | wps 54793.7 | wpb 511.9 | bsz 1 | num_updates 1945 | best_loss 7.958
2022-03-12 11:33:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1945 updates
2022-03-12 11:33:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 11:33:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 11:33:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 5 @ 1945 updates, score 7.958) (writing took 2.119985697005177 seconds)
2022-03-12 11:33:24 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-12 11:33:24 | INFO | train | epoch 005 | loss 8.384 | ppl 334.15 | wps 29645.6 | ups 0.45 | wpb 65404.8 | bsz 127.7 | num_updates 1945 | lr 0.000243176 | gnorm 0.852 | loss_scale 32 | train_wall 741 | gb_free 9.8 | wall 4295
2022-03-12 11:33:25 | INFO | fairseq.trainer | begin training epoch 6
2022-03-12 11:33:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 11:34:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:35:19 | INFO | train_inner | epoch 006:     56 / 392 loss=8.093, ppl=273.12, wps=27856.5, ups=0.43, wpb=65029.1, bsz=127, num_updates=2000, lr=0.00025005, gnorm=0.842, loss_scale=32, train_wall=184, gb_free=9.8, wall=4409
2022-03-12 11:38:44 | INFO | train_inner | epoch 006:    156 / 392 loss=7.966, ppl=250.12, wps=31977.8, ups=0.49, wpb=65532.7, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.837, loss_scale=32, train_wall=182, gb_free=9.8, wall=4614
2022-03-12 11:39:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:42:12 | INFO | train_inner | epoch 006:    257 / 392 loss=7.865, ppl=233.14, wps=31592, ups=0.48, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.851, loss_scale=32, train_wall=184, gb_free=9.8, wall=4822
2022-03-12 11:44:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:45:40 | INFO | train_inner | epoch 006:    358 / 392 loss=7.769, ppl=218.08, wps=31500.6, ups=0.48, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.816, loss_scale=32, train_wall=185, gb_free=9.8, wall=5030
2022-03-12 11:46:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 11:47:11 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.547 | ppl 187 | wps 55828.1 | wpb 511.9 | bsz 1 | num_updates 2334 | best_loss 7.547
2022-03-12 11:47:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2334 updates
2022-03-12 11:47:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 11:47:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 11:47:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 6 @ 2334 updates, score 7.547) (writing took 2.0490233860036824 seconds)
2022-03-12 11:47:13 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-12 11:47:13 | INFO | train | epoch 006 | loss 7.88 | ppl 235.57 | wps 30700.8 | ups 0.47 | wpb 65404.8 | bsz 127.7 | num_updates 2334 | lr 0.000291792 | gnorm 0.839 | loss_scale 32 | train_wall 713 | gb_free 9.8 | wall 5123
2022-03-12 11:47:13 | INFO | fairseq.trainer | begin training epoch 7
2022-03-12 11:47:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 11:49:29 | INFO | train_inner | epoch 007:     66 / 392 loss=7.653, ppl=201.3, wps=28421.6, ups=0.44, wpb=65029.1, bsz=127, num_updates=2400, lr=0.00030004, gnorm=0.842, loss_scale=64, train_wall=181, gb_free=9.8, wall=5259
2022-03-12 11:49:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:52:56 | INFO | train_inner | epoch 007:    167 / 392 loss=7.554, ppl=187.95, wps=31532.7, ups=0.48, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.815, loss_scale=32, train_wall=185, gb_free=9.8, wall=5467
2022-03-12 11:54:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:56:23 | INFO | train_inner | epoch 007:    268 / 392 loss=7.479, ppl=178.45, wps=31769.3, ups=0.48, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.801, loss_scale=32, train_wall=183, gb_free=9.8, wall=5673
2022-03-12 11:58:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 11:59:51 | INFO | train_inner | epoch 007:    369 / 392 loss=7.403, ppl=169.21, wps=31490.9, ups=0.48, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.777, loss_scale=32, train_wall=185, gb_free=9.8, wall=5881
2022-03-12 12:00:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 12:01:02 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.22 | ppl 149.08 | wps 54280.8 | wpb 511.9 | bsz 1 | num_updates 2723 | best_loss 7.22
2022-03-12 12:01:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2723 updates
2022-03-12 12:01:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 12:01:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 12:01:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 7 @ 2723 updates, score 7.22) (writing took 2.0874904129887 seconds)
2022-03-12 12:01:04 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-12 12:01:04 | INFO | train | epoch 007 | loss 7.497 | ppl 180.63 | wps 30618.4 | ups 0.47 | wpb 65404.8 | bsz 127.7 | num_updates 2723 | lr 0.000340407 | gnorm 0.807 | loss_scale 32 | train_wall 715 | gb_free 9.8 | wall 5954
2022-03-12 12:01:04 | INFO | fairseq.trainer | begin training epoch 8
2022-03-12 12:01:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 12:03:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-12 12:03:50 | INFO | train_inner | epoch 008:     78 / 392 loss=7.283, ppl=155.75, wps=27203.9, ups=0.42, wpb=65025.8, bsz=127, num_updates=2800, lr=0.00035003, gnorm=0.784, loss_scale=32, train_wall=190, gb_free=9.8, wall=6120
2022-03-12 12:04:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 12:07:19 | INFO | train_inner | epoch 008:    179 / 392 loss=7.226, ppl=149.76, wps=31281.8, ups=0.48, wpb=65532.7, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.766, loss_scale=16, train_wall=186, gb_free=9.8, wall=6329
2022-03-12 12:09:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 12:10:49 | INFO | train_inner | epoch 008:    280 / 392 loss=7.153, ppl=142.33, wps=31198.4, ups=0.48, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.753, loss_scale=16, train_wall=187, gb_free=9.8, wall=6540
2022-03-12 12:14:15 | INFO | train_inner | epoch 008:    380 / 392 loss=7.094, ppl=136.62, wps=31847.5, ups=0.49, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.753, loss_scale=32, train_wall=183, gb_free=9.8, wall=6745
2022-03-12 12:14:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 12:15:02 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 6.911 | ppl 120.37 | wps 54695.3 | wpb 511.9 | bsz 1 | num_updates 3112 | best_loss 6.911
2022-03-12 12:15:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3112 updates
2022-03-12 12:15:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 12:15:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 12:15:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 8 @ 3112 updates, score 6.911) (writing took 2.0891746190027334 seconds)
2022-03-12 12:15:04 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-12 12:15:04 | INFO | train | epoch 008 | loss 7.175 | ppl 144.52 | wps 30297.4 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 3112 | lr 0.000389022 | gnorm 0.76 | loss_scale 32 | train_wall 723 | gb_free 9.8 | wall 6794
2022-03-12 12:15:04 | INFO | fairseq.trainer | begin training epoch 9
2022-03-12 12:15:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 12:15:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 12:18:07 | INFO | train_inner | epoch 009:     89 / 392 loss=6.952, ppl=123.83, wps=28025.8, ups=0.43, wpb=65029.1, bsz=127, num_updates=3200, lr=0.00040002, gnorm=0.759, loss_scale=16, train_wall=183, gb_free=9.8, wall=6977
2022-03-12 12:21:34 | INFO | train_inner | epoch 009:    189 / 392 loss=6.912, ppl=120.39, wps=31653.3, ups=0.48, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.751, loss_scale=32, train_wall=184, gb_free=9.8, wall=7184
2022-03-12 12:22:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 12:25:02 | INFO | train_inner | epoch 009:    290 / 392 loss=6.86, ppl=116.18, wps=31557.3, ups=0.48, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.743, loss_scale=16, train_wall=184, gb_free=9.8, wall=7392
2022-03-12 12:28:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 12:28:30 | INFO | train_inner | epoch 009:    391 / 392 loss=6.794, ppl=110.97, wps=31454.1, ups=0.48, wpb=65532.7, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.742, loss_scale=16, train_wall=185, gb_free=9.8, wall=7600
2022-03-12 12:28:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 12:28:54 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 6.643 | ppl 99.95 | wps 55647.6 | wpb 511.9 | bsz 1 | num_updates 3501 | best_loss 6.643
2022-03-12 12:28:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3501 updates
2022-03-12 12:28:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 12:28:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 12:28:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 9 @ 3501 updates, score 6.643) (writing took 2.052678860985907 seconds)
2022-03-12 12:28:56 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-12 12:28:56 | INFO | train | epoch 009 | loss 6.875 | ppl 117.36 | wps 30574.5 | ups 0.47 | wpb 65404.8 | bsz 127.7 | num_updates 3501 | lr 0.000437637 | gnorm 0.748 | loss_scale 16 | train_wall 716 | gb_free 9.8 | wall 7626
2022-03-12 12:28:56 | INFO | fairseq.trainer | begin training epoch 10
2022-03-12 12:28:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 12:32:19 | INFO | train_inner | epoch 010:     99 / 392 loss=6.666, ppl=101.55, wps=28413.7, ups=0.44, wpb=65029.1, bsz=127, num_updates=3600, lr=0.00045001, gnorm=0.728, loss_scale=16, train_wall=180, gb_free=9.8, wall=7829
2022-03-12 12:34:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 12:35:47 | INFO | train_inner | epoch 010:    200 / 392 loss=6.646, ppl=100.16, wps=31495.1, ups=0.48, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.71, loss_scale=16, train_wall=185, gb_free=9.8, wall=8037
2022-03-12 12:39:12 | INFO | train_inner | epoch 010:    300 / 392 loss=6.617, ppl=98.19, wps=31970.5, ups=0.49, wpb=65532.7, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.709, loss_scale=32, train_wall=182, gb_free=9.8, wall=8242
2022-03-12 12:40:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 12:42:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 12:42:43 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 6.461 | ppl 88.07 | wps 55472.5 | wpb 511.9 | bsz 1 | num_updates 3891 | best_loss 6.461
2022-03-12 12:42:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3891 updates
2022-03-12 12:42:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 12:42:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 12:42:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 10 @ 3891 updates, score 6.461) (writing took 2.0623798810120206 seconds)
2022-03-12 12:42:45 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-12 12:42:45 | INFO | train | epoch 010 | loss 6.63 | ppl 99.02 | wps 30754.9 | ups 0.47 | wpb 65405.2 | bsz 127.7 | num_updates 3891 | lr 0.000486378 | gnorm 0.711 | loss_scale 16 | train_wall 713 | gb_free 9.8 | wall 8456
2022-03-12 12:42:46 | INFO | fairseq.trainer | begin training epoch 11
2022-03-12 12:42:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 12:43:04 | INFO | train_inner | epoch 011:      9 / 392 loss=6.577, ppl=95.48, wps=28046.3, ups=0.43, wpb=65029.1, bsz=127, num_updates=3900, lr=0.000487503, gnorm=0.707, loss_scale=16, train_wall=183, gb_free=9.8, wall=8474
2022-03-12 12:46:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 12:46:32 | INFO | train_inner | epoch 011:    110 / 392 loss=6.467, ppl=88.48, wps=31552, ups=0.48, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.68, loss_scale=16, train_wall=184, gb_free=9.8, wall=8682
2022-03-12 12:49:58 | INFO | train_inner | epoch 011:    210 / 392 loss=6.448, ppl=87.3, wps=31803.6, ups=0.49, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.676, loss_scale=16, train_wall=183, gb_free=9.8, wall=8888
2022-03-12 12:50:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 12:53:25 | INFO | train_inner | epoch 011:    311 / 392 loss=6.43, ppl=86.24, wps=31658.7, ups=0.48, wpb=65532.7, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.653, loss_scale=16, train_wall=184, gb_free=9.8, wall=9095
2022-03-12 12:55:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 12:56:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 12:56:34 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.31 | ppl 79.35 | wps 55634.6 | wpb 511.9 | bsz 1 | num_updates 4280 | best_loss 6.31
2022-03-12 12:56:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4280 updates
2022-03-12 12:56:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 12:56:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 12:56:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 11 @ 4280 updates, score 6.31) (writing took 2.209864517993992 seconds)
2022-03-12 12:56:36 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-12 12:56:36 | INFO | train | epoch 011 | loss 6.439 | ppl 86.78 | wps 30631.6 | ups 0.47 | wpb 65404.8 | bsz 127.7 | num_updates 4280 | lr 0.000483368 | gnorm 0.668 | loss_scale 16 | train_wall 715 | gb_free 9.8 | wall 9286
2022-03-12 12:56:36 | INFO | fairseq.trainer | begin training epoch 12
2022-03-12 12:56:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 12:57:17 | INFO | train_inner | epoch 012:     20 / 392 loss=6.382, ppl=83.39, wps=27996.7, ups=0.43, wpb=65029.1, bsz=127, num_updates=4300, lr=0.000482243, gnorm=0.654, loss_scale=16, train_wall=184, gb_free=9.8, wall=9327
2022-03-12 13:00:43 | INFO | train_inner | epoch 012:    120 / 392 loss=6.292, ppl=78.34, wps=31868.6, ups=0.49, wpb=65532.7, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.629, loss_scale=32, train_wall=182, gb_free=9.8, wall=9533
2022-03-12 13:01:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:04:08 | INFO | train_inner | epoch 012:    221 / 392 loss=6.288, ppl=78.15, wps=31905.8, ups=0.49, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.641, loss_scale=16, train_wall=182, gb_free=9.8, wall=9738
2022-03-12 13:05:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:07:36 | INFO | train_inner | epoch 012:    322 / 392 loss=6.263, ppl=76.79, wps=31599.6, ups=0.48, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.616, loss_scale=16, train_wall=184, gb_free=9.8, wall=9946
2022-03-12 13:09:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 13:10:21 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.2 | ppl 73.52 | wps 55050.4 | wpb 511.9 | bsz 1 | num_updates 4670 | best_loss 6.2
2022-03-12 13:10:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4670 updates
2022-03-12 13:10:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 13:10:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 13:10:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 12 @ 4670 updates, score 6.2) (writing took 2.1191994060063735 seconds)
2022-03-12 13:10:23 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-12 13:10:23 | INFO | train | epoch 012 | loss 6.277 | ppl 77.53 | wps 30829.6 | ups 0.47 | wpb 65405.2 | bsz 127.7 | num_updates 4670 | lr 0.000462745 | gnorm 0.63 | loss_scale 16 | train_wall 711 | gb_free 9.8 | wall 10114
2022-03-12 13:10:23 | INFO | fairseq.trainer | begin training epoch 13
2022-03-12 13:10:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 13:11:26 | INFO | train_inner | epoch 013:     30 / 392 loss=6.217, ppl=74.41, wps=28229.3, ups=0.43, wpb=65029.1, bsz=127, num_updates=4700, lr=0.000461266, gnorm=0.623, loss_scale=32, train_wall=182, gb_free=9.8, wall=10176
2022-03-12 13:11:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:14:54 | INFO | train_inner | epoch 013:    131 / 392 loss=6.158, ppl=71.4, wps=31432.6, ups=0.48, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.599, loss_scale=16, train_wall=185, gb_free=9.8, wall=10385
2022-03-12 13:18:20 | INFO | train_inner | epoch 013:    231 / 392 loss=6.145, ppl=70.75, wps=31836.4, ups=0.49, wpb=65532.7, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.609, loss_scale=32, train_wall=183, gb_free=9.8, wall=10590
2022-03-12 13:19:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:21:52 | INFO | train_inner | epoch 013:    332 / 392 loss=6.146, ppl=70.79, wps=30967.2, ups=0.47, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.613, loss_scale=16, train_wall=188, gb_free=9.8, wall=10802
2022-03-12 13:23:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:23:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 13:24:19 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.105 | ppl 68.84 | wps 54822.2 | wpb 511.9 | bsz 1 | num_updates 5059 | best_loss 6.105
2022-03-12 13:24:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5059 updates
2022-03-12 13:24:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 13:24:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 13:24:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 13 @ 5059 updates, score 6.105) (writing took 2.2160831460205372 seconds)
2022-03-12 13:24:21 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-12 13:24:21 | INFO | train | epoch 013 | loss 6.146 | ppl 70.8 | wps 30366.7 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 5059 | lr 0.000444598 | gnorm 0.607 | loss_scale 16 | train_wall 721 | gb_free 9.8 | wall 10951
2022-03-12 13:24:21 | INFO | fairseq.trainer | begin training epoch 14
2022-03-12 13:24:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 13:25:48 | INFO | train_inner | epoch 014:     41 / 392 loss=6.084, ppl=67.86, wps=27493.1, ups=0.42, wpb=65025.8, bsz=127, num_updates=5100, lr=0.000442807, gnorm=0.608, loss_scale=16, train_wall=187, gb_free=9.8, wall=11039
2022-03-12 13:29:19 | INFO | train_inner | epoch 014:    141 / 392 loss=6.027, ppl=65.21, wps=31057.8, ups=0.47, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.588, loss_scale=32, train_wall=188, gb_free=9.8, wall=11250
2022-03-12 13:30:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:32:52 | INFO | train_inner | epoch 014:    242 / 392 loss=6.049, ppl=66.2, wps=30847.6, ups=0.47, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.612, loss_scale=16, train_wall=189, gb_free=9.8, wall=11462
2022-03-12 13:34:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:36:24 | INFO | train_inner | epoch 014:    343 / 392 loss=6.048, ppl=66.19, wps=30949.3, ups=0.47, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.596, loss_scale=16, train_wall=188, gb_free=9.8, wall=11674
2022-03-12 13:38:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 13:38:29 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.026 | ppl 65.18 | wps 54654 | wpb 511.9 | bsz 1 | num_updates 5449 | best_loss 6.026
2022-03-12 13:38:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5449 updates
2022-03-12 13:38:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 13:38:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 13:38:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 14 @ 5449 updates, score 6.026) (writing took 2.1195757129753474 seconds)
2022-03-12 13:38:31 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-12 13:38:31 | INFO | train | epoch 014 | loss 6.039 | ppl 65.77 | wps 30019.3 | ups 0.46 | wpb 65405.2 | bsz 127.7 | num_updates 5449 | lr 0.000428392 | gnorm 0.601 | loss_scale 16 | train_wall 733 | gb_free 9.8 | wall 11801
2022-03-12 13:38:31 | INFO | fairseq.trainer | begin training epoch 15
2022-03-12 13:38:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 13:39:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:40:20 | INFO | train_inner | epoch 015:     52 / 392 loss=5.985, ppl=63.34, wps=27490.2, ups=0.42, wpb=65029.1, bsz=127, num_updates=5500, lr=0.000426401, gnorm=0.601, loss_scale=16, train_wall=187, gb_free=9.8, wall=11910
2022-03-12 13:43:51 | INFO | train_inner | epoch 015:    152 / 392 loss=5.952, ppl=61.9, wps=31084.6, ups=0.47, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.58, loss_scale=16, train_wall=188, gb_free=9.8, wall=12121
2022-03-12 13:44:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:47:22 | INFO | train_inner | epoch 015:    253 / 392 loss=5.955, ppl=62.03, wps=30999.8, ups=0.47, wpb=65532.7, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.591, loss_scale=16, train_wall=188, gb_free=9.8, wall=12333
2022-03-12 13:49:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:50:55 | INFO | train_inner | epoch 015:    354 / 392 loss=5.953, ppl=61.93, wps=30777.4, ups=0.47, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.583, loss_scale=16, train_wall=190, gb_free=9.8, wall=12545
2022-03-12 13:52:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 13:52:36 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.982 | ppl 63.21 | wps 55885 | wpb 511.9 | bsz 1 | num_updates 5838 | best_loss 5.982
2022-03-12 13:52:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5838 updates
2022-03-12 13:52:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 13:52:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 13:52:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 15 @ 5838 updates, score 5.982) (writing took 2.0646974190021865 seconds)
2022-03-12 13:52:38 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-12 13:52:38 | INFO | train | epoch 015 | loss 5.95 | ppl 61.81 | wps 30051.3 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 5838 | lr 0.000413874 | gnorm 0.589 | loss_scale 16 | train_wall 731 | gb_free 9.8 | wall 12648
2022-03-12 13:52:38 | INFO | fairseq.trainer | begin training epoch 16
2022-03-12 13:52:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 13:54:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 13:54:50 | INFO | train_inner | epoch 016:     63 / 392 loss=5.891, ppl=59.34, wps=27746.8, ups=0.43, wpb=65029.1, bsz=127, num_updates=5900, lr=0.000411693, gnorm=0.595, loss_scale=16, train_wall=186, gb_free=9.8, wall=12780
2022-03-12 13:58:19 | INFO | train_inner | epoch 016:    163 / 392 loss=5.87, ppl=58.5, wps=31246.2, ups=0.48, wpb=65532.7, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.589, loss_scale=16, train_wall=187, gb_free=9.8, wall=12990
2022-03-12 13:59:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:01:51 | INFO | train_inner | epoch 016:    264 / 392 loss=5.881, ppl=58.92, wps=31051.1, ups=0.47, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.599, loss_scale=16, train_wall=188, gb_free=9.8, wall=13201
2022-03-12 14:04:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:05:18 | INFO | train_inner | epoch 016:    365 / 392 loss=5.871, ppl=58.51, wps=31571.6, ups=0.48, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.586, loss_scale=16, train_wall=184, gb_free=9.8, wall=13408
2022-03-12 14:06:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 14:06:38 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.93 | ppl 60.97 | wps 54211.7 | wpb 511.9 | bsz 1 | num_updates 6227 | best_loss 5.93
2022-03-12 14:06:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6227 updates
2022-03-12 14:06:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 14:06:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 14:06:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 16 @ 6227 updates, score 5.93) (writing took 2.0542272299935576 seconds)
2022-03-12 14:06:40 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-12 14:06:40 | INFO | train | epoch 016 | loss 5.873 | ppl 58.62 | wps 30210.8 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 6227 | lr 0.000400738 | gnorm 0.593 | loss_scale 16 | train_wall 726 | gb_free 9.8 | wall 13490
2022-03-12 14:06:40 | INFO | fairseq.trainer | begin training epoch 17
2022-03-12 14:06:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 14:09:12 | INFO | train_inner | epoch 017:     73 / 392 loss=5.817, ppl=56.38, wps=27844.1, ups=0.43, wpb=65025.8, bsz=127, num_updates=6300, lr=0.00039841, gnorm=0.59, loss_scale=16, train_wall=184, gb_free=9.8, wall=13642
2022-03-12 14:09:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:12:42 | INFO | train_inner | epoch 017:    174 / 392 loss=5.806, ppl=55.93, wps=31114.5, ups=0.47, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.589, loss_scale=16, train_wall=187, gb_free=9.8, wall=13852
2022-03-12 14:14:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:16:13 | INFO | train_inner | epoch 017:    275 / 392 loss=5.81, ppl=56.09, wps=31045, ups=0.47, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.577, loss_scale=16, train_wall=188, gb_free=9.8, wall=14063
2022-03-12 14:19:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:19:44 | INFO | train_inner | epoch 017:    376 / 392 loss=5.817, ppl=56.39, wps=31084.6, ups=0.47, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.59, loss_scale=16, train_wall=187, gb_free=9.8, wall=14274
2022-03-12 14:20:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 14:20:39 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 5.888 | ppl 59.22 | wps 54058.2 | wpb 511.9 | bsz 1 | num_updates 6616 | best_loss 5.888
2022-03-12 14:20:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6616 updates
2022-03-12 14:20:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 14:20:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 14:20:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 17 @ 6616 updates, score 5.888) (writing took 2.0884053490008228 seconds)
2022-03-12 14:20:41 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-12 14:20:41 | INFO | train | epoch 017 | loss 5.807 | ppl 55.98 | wps 30228.3 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 6616 | lr 0.000388779 | gnorm 0.585 | loss_scale 16 | train_wall 725 | gb_free 9.8 | wall 14332
2022-03-12 14:20:42 | INFO | fairseq.trainer | begin training epoch 18
2022-03-12 14:20:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 14:23:36 | INFO | train_inner | epoch 018:     84 / 392 loss=5.736, ppl=53.3, wps=28055.3, ups=0.43, wpb=65029.1, bsz=127, num_updates=6700, lr=0.000386334, gnorm=0.59, loss_scale=16, train_wall=183, gb_free=9.8, wall=14506
2022-03-12 14:24:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:27:08 | INFO | train_inner | epoch 018:    185 / 392 loss=5.746, ppl=53.68, wps=30856.9, ups=0.47, wpb=65532.7, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.585, loss_scale=16, train_wall=189, gb_free=9.8, wall=14718
2022-03-12 14:28:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:30:38 | INFO | train_inner | epoch 018:    286 / 392 loss=5.758, ppl=54.13, wps=31248.8, ups=0.48, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.582, loss_scale=16, train_wall=186, gb_free=9.8, wall=14928
2022-03-12 14:33:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:34:11 | INFO | train_inner | epoch 018:    387 / 392 loss=5.765, ppl=54.37, wps=30746.8, ups=0.47, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.59, loss_scale=16, train_wall=190, gb_free=9.8, wall=15141
2022-03-12 14:34:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 14:34:44 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 5.852 | ppl 57.77 | wps 54679.6 | wpb 511.9 | bsz 1 | num_updates 7005 | best_loss 5.852
2022-03-12 14:34:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7005 updates
2022-03-12 14:34:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 14:34:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 14:34:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 18 @ 7005 updates, score 5.852) (writing took 2.04834866602323 seconds)
2022-03-12 14:34:46 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-12 14:34:46 | INFO | train | epoch 018 | loss 5.749 | ppl 53.79 | wps 30135.1 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 7005 | lr 0.00037783 | gnorm 0.588 | loss_scale 16 | train_wall 728 | gb_free 9.8 | wall 15176
2022-03-12 14:34:46 | INFO | fairseq.trainer | begin training epoch 19
2022-03-12 14:34:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 14:38:05 | INFO | train_inner | epoch 019:     95 / 392 loss=5.672, ppl=50.97, wps=27854.7, ups=0.43, wpb=65029.1, bsz=127, num_updates=7100, lr=0.000375293, gnorm=0.576, loss_scale=16, train_wall=185, gb_free=9.8, wall=15375
2022-03-12 14:38:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:41:36 | INFO | train_inner | epoch 019:    196 / 392 loss=5.696, ppl=51.84, wps=30961.3, ups=0.47, wpb=65532.7, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.585, loss_scale=16, train_wall=188, gb_free=9.8, wall=15586
2022-03-12 14:42:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:45:08 | INFO | train_inner | epoch 019:    297 / 392 loss=5.71, ppl=52.35, wps=30972.6, ups=0.47, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.584, loss_scale=16, train_wall=188, gb_free=9.8, wall=15798
2022-03-12 14:47:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:48:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 14:48:48 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 5.814 | ppl 56.26 | wps 55043 | wpb 511.9 | bsz 1 | num_updates 7394 | best_loss 5.814
2022-03-12 14:48:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7394 updates
2022-03-12 14:48:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 14:48:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 14:48:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 19 @ 7394 updates, score 5.814) (writing took 2.0675064440001734 seconds)
2022-03-12 14:48:50 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-12 14:48:50 | INFO | train | epoch 019 | loss 5.698 | ppl 51.9 | wps 30127.1 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 7394 | lr 0.000367756 | gnorm 0.583 | loss_scale 16 | train_wall 728 | gb_free 9.8 | wall 16020
2022-03-12 14:48:50 | INFO | fairseq.trainer | begin training epoch 20
2022-03-12 14:48:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 14:49:03 | INFO | train_inner | epoch 020:      6 / 392 loss=5.708, ppl=52.28, wps=27676.8, ups=0.43, wpb=65029.1, bsz=127, num_updates=7400, lr=0.000367607, gnorm=0.592, loss_scale=16, train_wall=186, gb_free=9.8, wall=16033
2022-03-12 14:52:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:52:34 | INFO | train_inner | epoch 020:    107 / 392 loss=5.636, ppl=49.71, wps=31103.4, ups=0.47, wpb=65532.7, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.583, loss_scale=16, train_wall=187, gb_free=9.8, wall=16244
2022-03-12 14:56:01 | INFO | train_inner | epoch 020:    207 / 392 loss=5.645, ppl=50.04, wps=31586.1, ups=0.48, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.581, loss_scale=16, train_wall=184, gb_free=9.8, wall=16451
2022-03-12 14:56:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 14:59:33 | INFO | train_inner | epoch 020:    308 / 392 loss=5.661, ppl=50.62, wps=30857, ups=0.47, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.587, loss_scale=16, train_wall=189, gb_free=9.8, wall=16664
2022-03-12 15:01:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 15:02:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 15:02:50 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 5.787 | ppl 55.22 | wps 54543.2 | wpb 511.9 | bsz 1 | num_updates 7783 | best_loss 5.787
2022-03-12 15:02:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7783 updates
2022-03-12 15:02:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 15:02:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 15:02:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 20 @ 7783 updates, score 5.787) (writing took 2.1192402919987217 seconds)
2022-03-12 15:02:52 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-12 15:02:52 | INFO | train | epoch 020 | loss 5.65 | ppl 50.22 | wps 30213 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 7783 | lr 0.000358448 | gnorm 0.585 | loss_scale 16 | train_wall 726 | gb_free 9.8 | wall 16862
2022-03-12 15:02:52 | INFO | fairseq.trainer | begin training epoch 21
2022-03-12 15:02:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 15:03:28 | INFO | train_inner | epoch 021:     17 / 392 loss=5.65, ppl=50.2, wps=27689.4, ups=0.43, wpb=65029.1, bsz=127, num_updates=7800, lr=0.000358057, gnorm=0.591, loss_scale=16, train_wall=186, gb_free=9.8, wall=16898
2022-03-12 15:06:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 15:06:58 | INFO | train_inner | epoch 021:    118 / 392 loss=5.591, ppl=48.21, wps=31318.6, ups=0.48, wpb=65536, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.576, loss_scale=16, train_wall=186, gb_free=9.8, wall=17108
2022-03-12 15:10:27 | INFO | train_inner | epoch 021:    218 / 392 loss=5.6, ppl=48.5, wps=31285.6, ups=0.48, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.586, loss_scale=16, train_wall=186, gb_free=9.8, wall=17317
2022-03-12 15:11:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 15:14:00 | INFO | train_inner | epoch 021:    319 / 392 loss=5.622, ppl=49.24, wps=30850.9, ups=0.47, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.59, loss_scale=16, train_wall=189, gb_free=9.8, wall=17530
2022-03-12 15:15:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 15:16:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 15:16:54 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 5.765 | ppl 54.38 | wps 54319.3 | wpb 511.9 | bsz 1 | num_updates 8172 | best_loss 5.765
2022-03-12 15:16:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8172 updates
2022-03-12 15:16:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 15:16:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 15:16:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 21 @ 8172 updates, score 5.765) (writing took 2.048998770012986 seconds)
2022-03-12 15:16:56 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-12 15:16:56 | INFO | train | epoch 021 | loss 5.609 | ppl 48.81 | wps 30140.7 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 8172 | lr 0.000349813 | gnorm 0.588 | loss_scale 16 | train_wall 728 | gb_free 9.8 | wall 17707
2022-03-12 15:16:57 | INFO | fairseq.trainer | begin training epoch 22
2022-03-12 15:16:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 15:17:55 | INFO | train_inner | epoch 022:     28 / 392 loss=5.604, ppl=48.63, wps=27587.1, ups=0.42, wpb=65025.8, bsz=127, num_updates=8200, lr=0.000349215, gnorm=0.587, loss_scale=16, train_wall=187, gb_free=9.8, wall=17765
2022-03-12 15:20:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 15:21:27 | INFO | train_inner | epoch 022:    129 / 392 loss=5.553, ppl=46.93, wps=30883, ups=0.47, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.58, loss_scale=16, train_wall=189, gb_free=9.8, wall=17978
2022-03-12 15:24:55 | INFO | train_inner | epoch 022:    229 / 392 loss=5.578, ppl=47.76, wps=31539.9, ups=0.48, wpb=65532.7, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.586, loss_scale=32, train_wall=185, gb_free=9.8, wall=18185
2022-03-12 15:24:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 15:28:27 | INFO | train_inner | epoch 022:    330 / 392 loss=5.584, ppl=47.96, wps=30986.8, ups=0.47, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.594, loss_scale=16, train_wall=188, gb_free=9.8, wall=18397
2022-03-12 15:29:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 15:30:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 15:30:58 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 5.749 | ppl 53.77 | wps 53890.3 | wpb 511.9 | bsz 1 | num_updates 8561 | best_loss 5.749
2022-03-12 15:30:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8561 updates
2022-03-12 15:30:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 15:30:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 15:31:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 22 @ 8561 updates, score 5.749) (writing took 2.085705595993204 seconds)
2022-03-12 15:31:00 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-12 15:31:00 | INFO | train | epoch 022 | loss 5.571 | ppl 47.53 | wps 30160.8 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 8561 | lr 0.000341773 | gnorm 0.586 | loss_scale 16 | train_wall 727 | gb_free 9.8 | wall 18550
2022-03-12 15:31:00 | INFO | fairseq.trainer | begin training epoch 23
2022-03-12 15:31:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 15:32:23 | INFO | train_inner | epoch 023:     39 / 392 loss=5.551, ppl=46.89, wps=27521.5, ups=0.42, wpb=65029.1, bsz=127, num_updates=8600, lr=0.000340997, gnorm=0.592, loss_scale=16, train_wall=187, gb_free=9.8, wall=18633
2022-03-12 15:34:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 15:35:54 | INFO | train_inner | epoch 023:    140 / 392 loss=5.522, ppl=45.94, wps=31109.9, ups=0.47, wpb=65536, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.587, loss_scale=16, train_wall=187, gb_free=9.8, wall=18844
2022-03-12 15:39:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 15:39:26 | INFO | train_inner | epoch 023:    241 / 392 loss=5.538, ppl=46.45, wps=30927, ups=0.47, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.586, loss_scale=16, train_wall=188, gb_free=9.8, wall=19056
2022-03-12 15:41:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 15:42:55 | INFO | train_inner | epoch 023:    342 / 392 loss=5.551, ppl=46.89, wps=31216.4, ups=0.48, wpb=65532.7, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.589, loss_scale=8, train_wall=187, gb_free=9.8, wall=19266
2022-03-12 15:44:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 15:45:03 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 5.732 | ppl 53.13 | wps 53941.7 | wpb 511.9 | bsz 1 | num_updates 8950 | best_loss 5.732
2022-03-12 15:45:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 8950 updates
2022-03-12 15:45:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 15:45:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 15:45:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 23 @ 8950 updates, score 5.732) (writing took 2.1199313399847597 seconds)
2022-03-12 15:45:05 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-12 15:45:05 | INFO | train | epoch 023 | loss 5.536 | ppl 46.4 | wps 30107.6 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 8950 | lr 0.000334263 | gnorm 0.591 | loss_scale 8 | train_wall 728 | gb_free 9.8 | wall 19395
2022-03-12 15:45:05 | INFO | fairseq.trainer | begin training epoch 24
2022-03-12 15:45:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 15:46:49 | INFO | train_inner | epoch 024:     50 / 392 loss=5.522, ppl=45.96, wps=27819.2, ups=0.43, wpb=65029.1, bsz=127, num_updates=9000, lr=0.000333333, gnorm=0.6, loss_scale=16, train_wall=185, gb_free=9.8, wall=19499
2022-03-12 15:50:17 | INFO | train_inner | epoch 024:    150 / 392 loss=5.486, ppl=44.82, wps=31510.4, ups=0.48, wpb=65532.7, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.588, loss_scale=16, train_wall=185, gb_free=9.8, wall=19707
2022-03-12 15:50:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 15:53:49 | INFO | train_inner | epoch 024:    251 / 392 loss=5.514, ppl=45.71, wps=30889.8, ups=0.47, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.591, loss_scale=16, train_wall=189, gb_free=9.8, wall=19919
2022-03-12 15:55:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 15:57:20 | INFO | train_inner | epoch 024:    352 / 392 loss=5.519, ppl=45.86, wps=31158.5, ups=0.48, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.592, loss_scale=16, train_wall=187, gb_free=9.8, wall=20130
2022-03-12 15:58:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 15:59:03 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 5.725 | ppl 52.88 | wps 55861.7 | wpb 511.9 | bsz 1 | num_updates 9340 | best_loss 5.725
2022-03-12 15:59:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9340 updates
2022-03-12 15:59:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 15:59:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 15:59:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 24 @ 9340 updates, score 5.725) (writing took 2.0743595180101693 seconds)
2022-03-12 15:59:05 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-12 15:59:05 | INFO | train | epoch 024 | loss 5.504 | ppl 45.39 | wps 30353.4 | ups 0.46 | wpb 65405.2 | bsz 127.7 | num_updates 9340 | lr 0.00032721 | gnorm 0.591 | loss_scale 16 | train_wall 725 | gb_free 9.8 | wall 20236
2022-03-12 15:59:06 | INFO | fairseq.trainer | begin training epoch 25
2022-03-12 15:59:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 16:00:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:01:10 | INFO | train_inner | epoch 025:     61 / 392 loss=5.469, ppl=44.28, wps=28218.1, ups=0.43, wpb=65025.8, bsz=127, num_updates=9400, lr=0.000326164, gnorm=0.595, loss_scale=16, train_wall=182, gb_free=9.8, wall=20360
2022-03-12 16:04:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:04:36 | INFO | train_inner | epoch 025:    162 / 392 loss=5.464, ppl=44.15, wps=31842.4, ups=0.49, wpb=65536, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.603, loss_scale=16, train_wall=182, gb_free=9.8, wall=20566
2022-03-12 16:08:04 | INFO | train_inner | epoch 025:    262 / 392 loss=5.485, ppl=44.78, wps=31435, ups=0.48, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.594, loss_scale=16, train_wall=185, gb_free=9.8, wall=20775
2022-03-12 16:09:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:11:36 | INFO | train_inner | epoch 025:    363 / 392 loss=5.492, ppl=45.01, wps=31023.6, ups=0.47, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.593, loss_scale=16, train_wall=188, gb_free=9.8, wall=20986
2022-03-12 16:12:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 16:12:59 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 5.697 | ppl 51.89 | wps 54671.8 | wpb 511.9 | bsz 1 | num_updates 9729 | best_loss 5.697
2022-03-12 16:12:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9729 updates
2022-03-12 16:12:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 16:13:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 16:13:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 25 @ 9729 updates, score 5.697) (writing took 2.052284725010395 seconds)
2022-03-12 16:13:01 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-12 16:13:01 | INFO | train | epoch 025 | loss 5.475 | ppl 44.46 | wps 30456.2 | ups 0.47 | wpb 65404.8 | bsz 127.7 | num_updates 9729 | lr 0.000320602 | gnorm 0.597 | loss_scale 16 | train_wall 719 | gb_free 9.8 | wall 21071
2022-03-12 16:13:01 | INFO | fairseq.trainer | begin training epoch 26
2022-03-12 16:13:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 16:13:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:15:31 | INFO | train_inner | epoch 026:     72 / 392 loss=5.437, ppl=43.32, wps=27606.2, ups=0.42, wpb=65029.1, bsz=127, num_updates=9800, lr=0.000319438, gnorm=0.596, loss_scale=16, train_wall=186, gb_free=9.8, wall=21221
2022-03-12 16:18:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:19:04 | INFO | train_inner | epoch 026:    173 / 392 loss=5.439, ppl=43.38, wps=30790.9, ups=0.47, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.589, loss_scale=16, train_wall=189, gb_free=9.8, wall=21434
2022-03-12 16:22:32 | INFO | train_inner | epoch 026:    273 / 392 loss=5.455, ppl=43.87, wps=31468.8, ups=0.48, wpb=65532.7, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.59, loss_scale=16, train_wall=185, gb_free=9.8, wall=21642
2022-03-12 16:23:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:26:05 | INFO | train_inner | epoch 026:    374 / 392 loss=5.47, ppl=44.32, wps=30861.4, ups=0.47, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.593, loss_scale=16, train_wall=189, gb_free=9.8, wall=21855
2022-03-12 16:26:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 16:27:04 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 5.686 | ppl 51.49 | wps 54451.9 | wpb 511.9 | bsz 1 | num_updates 10118 | best_loss 5.686
2022-03-12 16:27:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10118 updates
2022-03-12 16:27:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 16:27:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 16:27:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 26 @ 10118 updates, score 5.686) (writing took 2.0540380640304647 seconds)
2022-03-12 16:27:06 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-12 16:27:06 | INFO | train | epoch 026 | loss 5.448 | ppl 43.64 | wps 30088.7 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 10118 | lr 0.000314378 | gnorm 0.593 | loss_scale 16 | train_wall 729 | gb_free 9.8 | wall 21917
2022-03-12 16:27:06 | INFO | fairseq.trainer | begin training epoch 27
2022-03-12 16:27:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 16:28:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:29:59 | INFO | train_inner | epoch 027:     83 / 392 loss=5.401, ppl=42.26, wps=27694.4, ups=0.43, wpb=65025.8, bsz=127, num_updates=10200, lr=0.000313112, gnorm=0.605, loss_scale=16, train_wall=186, gb_free=9.8, wall=22090
2022-03-12 16:32:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:33:31 | INFO | train_inner | epoch 027:    184 / 392 loss=5.41, ppl=42.51, wps=30991.9, ups=0.47, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.595, loss_scale=16, train_wall=188, gb_free=9.8, wall=22301
2022-03-12 16:36:58 | INFO | train_inner | epoch 027:    284 / 392 loss=5.435, ppl=43.28, wps=31601.4, ups=0.48, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.602, loss_scale=16, train_wall=184, gb_free=9.8, wall=22508
2022-03-12 16:37:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:40:29 | INFO | train_inner | epoch 027:    385 / 392 loss=5.449, ppl=43.69, wps=31119.5, ups=0.47, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.601, loss_scale=16, train_wall=187, gb_free=9.8, wall=22719
2022-03-12 16:40:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 16:41:06 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 5.677 | ppl 51.16 | wps 54410.9 | wpb 511.9 | bsz 1 | num_updates 10507 | best_loss 5.677
2022-03-12 16:41:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10507 updates
2022-03-12 16:41:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 16:41:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 16:41:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 27 @ 10507 updates, score 5.677) (writing took 2.0133314990089275 seconds)
2022-03-12 16:41:08 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-12 16:41:08 | INFO | train | epoch 027 | loss 5.422 | ppl 42.87 | wps 30244.8 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 10507 | lr 0.000308504 | gnorm 0.6 | loss_scale 16 | train_wall 725 | gb_free 9.8 | wall 22758
2022-03-12 16:41:08 | INFO | fairseq.trainer | begin training epoch 28
2022-03-12 16:41:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 16:42:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:44:24 | INFO | train_inner | epoch 028:     94 / 392 loss=5.371, ppl=41.37, wps=27713.4, ups=0.43, wpb=65025.8, bsz=127, num_updates=10600, lr=0.000307148, gnorm=0.604, loss_scale=16, train_wall=185, gb_free=9.8, wall=22954
2022-03-12 16:46:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:47:54 | INFO | train_inner | epoch 028:    195 / 392 loss=5.386, ppl=41.81, wps=31088.7, ups=0.47, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.59, loss_scale=16, train_wall=187, gb_free=9.8, wall=23164
2022-03-12 16:51:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:51:25 | INFO | train_inner | epoch 028:    296 / 392 loss=5.416, ppl=42.71, wps=31044.6, ups=0.47, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.599, loss_scale=16, train_wall=188, gb_free=9.8, wall=23376
2022-03-12 16:54:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 16:55:08 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 5.666 | ppl 50.76 | wps 54612.3 | wpb 511.9 | bsz 1 | num_updates 10896 | best_loss 5.666
2022-03-12 16:55:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10896 updates
2022-03-12 16:55:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 16:55:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 16:55:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 28 @ 10896 updates, score 5.666) (writing took 2.061859718989581 seconds)
2022-03-12 16:55:10 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-12 16:55:10 | INFO | train | epoch 028 | loss 5.398 | ppl 42.17 | wps 30217.5 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 10896 | lr 0.000302947 | gnorm 0.598 | loss_scale 16 | train_wall 725 | gb_free 9.8 | wall 23600
2022-03-12 16:55:10 | INFO | fairseq.trainer | begin training epoch 29
2022-03-12 16:55:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 16:55:18 | INFO | train_inner | epoch 029:      4 / 392 loss=5.419, ppl=42.78, wps=27961.8, ups=0.43, wpb=65029.1, bsz=127, num_updates=10900, lr=0.000302891, gnorm=0.604, loss_scale=16, train_wall=184, gb_free=9.8, wall=23608
2022-03-12 16:56:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 16:58:51 | INFO | train_inner | epoch 029:    105 / 392 loss=5.339, ppl=40.49, wps=30809.4, ups=0.47, wpb=65536, bsz=128, num_updates=11000, lr=0.000301511, gnorm=0.595, loss_scale=16, train_wall=189, gb_free=9.8, wall=23821
2022-03-12 17:00:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 17:02:21 | INFO | train_inner | epoch 029:    206 / 392 loss=5.373, ppl=41.43, wps=31179.3, ups=0.48, wpb=65532.7, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.604, loss_scale=16, train_wall=187, gb_free=9.8, wall=24031
2022-03-12 17:05:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 17:05:46 | INFO | train_inner | epoch 029:    307 / 392 loss=5.387, ppl=41.86, wps=31947.7, ups=0.49, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.602, loss_scale=16, train_wall=182, gb_free=9.8, wall=24236
2022-03-12 17:08:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 17:08:59 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 5.653 | ppl 50.33 | wps 56430.9 | wpb 511.9 | bsz 1 | num_updates 11285 | best_loss 5.653
2022-03-12 17:08:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11285 updates
2022-03-12 17:08:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 17:09:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 17:09:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 29 @ 11285 updates, score 5.653) (writing took 2.0554759289952926 seconds)
2022-03-12 17:09:01 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-12 17:09:01 | INFO | train | epoch 029 | loss 5.375 | ppl 41.51 | wps 30588.3 | ups 0.47 | wpb 65404.8 | bsz 127.7 | num_updates 11285 | lr 0.00029768 | gnorm 0.602 | loss_scale 16 | train_wall 716 | gb_free 9.8 | wall 24431
2022-03-12 17:09:01 | INFO | fairseq.trainer | begin training epoch 30
2022-03-12 17:09:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 17:09:32 | INFO | train_inner | epoch 030:     15 / 392 loss=5.395, ppl=42.07, wps=28803.5, ups=0.44, wpb=65029.1, bsz=127, num_updates=11300, lr=0.000297482, gnorm=0.61, loss_scale=16, train_wall=178, gb_free=9.8, wall=24462
2022-03-12 17:09:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 17:13:03 | INFO | train_inner | epoch 030:    116 / 392 loss=5.332, ppl=40.28, wps=31099.3, ups=0.47, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.602, loss_scale=16, train_wall=187, gb_free=9.8, wall=24673
2022-03-12 17:14:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 17:16:32 | INFO | train_inner | epoch 030:    217 / 392 loss=5.352, ppl=40.83, wps=31243.9, ups=0.48, wpb=65536, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.614, loss_scale=16, train_wall=186, gb_free=9.8, wall=24882
2022-03-12 17:18:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 17:20:05 | INFO | train_inner | epoch 030:    318 / 392 loss=5.371, ppl=41.39, wps=30780.1, ups=0.47, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.592, loss_scale=16, train_wall=190, gb_free=9.8, wall=25095
2022-03-12 17:22:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 17:23:02 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 5.637 | ppl 49.76 | wps 54899.5 | wpb 511.9 | bsz 1 | num_updates 11674 | best_loss 5.637
2022-03-12 17:23:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11674 updates
2022-03-12 17:23:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 17:23:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 17:23:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 30 @ 11674 updates, score 5.637) (writing took 2.126720939995721 seconds)
2022-03-12 17:23:04 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-12 17:23:04 | INFO | train | epoch 030 | loss 5.355 | ppl 40.93 | wps 30207.8 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 11674 | lr 0.000292678 | gnorm 0.605 | loss_scale 16 | train_wall 726 | gb_free 9.8 | wall 25274
2022-03-12 17:23:04 | INFO | fairseq.trainer | begin training epoch 31
2022-03-12 17:23:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 17:23:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 17:24:00 | INFO | train_inner | epoch 031:     27 / 392 loss=5.354, ppl=40.89, wps=27652.8, ups=0.43, wpb=65025.8, bsz=127, num_updates=11700, lr=0.000292353, gnorm=0.611, loss_scale=16, train_wall=186, gb_free=9.8, wall=25330
2022-03-12 17:27:31 | INFO | train_inner | epoch 031:    127 / 392 loss=5.302, ppl=39.45, wps=31168.1, ups=0.48, wpb=65532.7, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.602, loss_scale=16, train_wall=187, gb_free=9.8, wall=25541
2022-03-12 17:28:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 17:31:01 | INFO | train_inner | epoch 031:    228 / 392 loss=5.354, ppl=40.89, wps=31086.5, ups=0.47, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.603, loss_scale=16, train_wall=187, gb_free=9.8, wall=25752
2022-03-12 17:33:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 17:34:34 | INFO | train_inner | epoch 031:    329 / 392 loss=5.344, ppl=40.62, wps=30853.5, ups=0.47, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.606, loss_scale=16, train_wall=189, gb_free=9.8, wall=25964
2022-03-12 17:36:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 17:37:08 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 5.64 | ppl 49.88 | wps 53596.8 | wpb 511.9 | bsz 1 | num_updates 12063 | best_loss 5.637
2022-03-12 17:37:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12063 updates
2022-03-12 17:37:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 17:37:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 17:37:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt (epoch 31 @ 12063 updates, score 5.64) (writing took 1.2309758839546703 seconds)
2022-03-12 17:37:09 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-12 17:37:09 | INFO | train | epoch 031 | loss 5.335 | ppl 40.37 | wps 30102.6 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 12063 | lr 0.00028792 | gnorm 0.605 | loss_scale 16 | train_wall 729 | gb_free 9.8 | wall 26119
2022-03-12 17:37:09 | INFO | fairseq.trainer | begin training epoch 32
2022-03-12 17:37:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 17:38:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 17:38:30 | INFO | train_inner | epoch 032:     38 / 392 loss=5.334, ppl=40.34, wps=27543.1, ups=0.42, wpb=65029.1, bsz=127, num_updates=12100, lr=0.00028748, gnorm=0.611, loss_scale=16, train_wall=187, gb_free=9.8, wall=26200
2022-03-12 17:41:58 | INFO | train_inner | epoch 032:    138 / 392 loss=5.296, ppl=39.28, wps=31466, ups=0.48, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.603, loss_scale=16, train_wall=185, gb_free=9.8, wall=26408
2022-03-12 17:42:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 17:45:30 | INFO | train_inner | epoch 032:    239 / 392 loss=5.319, ppl=39.91, wps=31015.3, ups=0.47, wpb=65532.7, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.597, loss_scale=16, train_wall=188, gb_free=9.8, wall=26620
2022-03-12 17:46:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 17:48:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 17:49:02 | INFO | train_inner | epoch 032:    341 / 392 loss=5.325, ppl=40.1, wps=30913.7, ups=0.47, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.6, loss_scale=8, train_wall=188, gb_free=9.8, wall=26832
2022-03-12 17:50:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 17:51:10 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 5.623 | ppl 49.3 | wps 55304.6 | wpb 511.9 | bsz 1 | num_updates 12451 | best_loss 5.623
2022-03-12 17:51:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12451 updates
2022-03-12 17:51:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 17:51:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 17:51:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 32 @ 12451 updates, score 5.623) (writing took 2.237160261021927 seconds)
2022-03-12 17:51:12 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-12 17:51:12 | INFO | train | epoch 032 | loss 5.316 | ppl 39.82 | wps 30103.7 | ups 0.46 | wpb 65404.5 | bsz 127.7 | num_updates 12451 | lr 0.000283399 | gnorm 0.604 | loss_scale 8 | train_wall 727 | gb_free 9.8 | wall 26962
2022-03-12 17:51:12 | INFO | fairseq.trainer | begin training epoch 33
2022-03-12 17:51:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 17:52:56 | INFO | train_inner | epoch 033:     49 / 392 loss=5.305, ppl=39.53, wps=27773.4, ups=0.43, wpb=65029.1, bsz=127, num_updates=12500, lr=0.000282843, gnorm=0.615, loss_scale=8, train_wall=185, gb_free=9.8, wall=27066
2022-03-12 17:56:25 | INFO | train_inner | epoch 033:    149 / 392 loss=5.279, ppl=38.83, wps=31388.5, ups=0.48, wpb=65532.7, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.603, loss_scale=16, train_wall=186, gb_free=9.8, wall=27275
2022-03-12 17:57:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 17:59:57 | INFO | train_inner | epoch 033:    250 / 392 loss=5.309, ppl=39.65, wps=30857.2, ups=0.47, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.612, loss_scale=8, train_wall=189, gb_free=9.8, wall=27487
2022-03-12 18:03:25 | INFO | train_inner | epoch 033:    350 / 392 loss=5.314, ppl=39.78, wps=31458, ups=0.48, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.607, loss_scale=16, train_wall=185, gb_free=9.8, wall=27695
2022-03-12 18:04:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 18:05:13 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 5.619 | ppl 49.15 | wps 54530.3 | wpb 511.9 | bsz 1 | num_updates 12842 | best_loss 5.619
2022-03-12 18:05:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12842 updates
2022-03-12 18:05:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 18:05:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 18:05:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 33 @ 12842 updates, score 5.619) (writing took 2.172450926969759 seconds)
2022-03-12 18:05:16 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-12 18:05:16 | INFO | train | epoch 033 | loss 5.299 | ppl 39.36 | wps 30306.1 | ups 0.46 | wpb 65405.5 | bsz 127.7 | num_updates 12842 | lr 0.000279051 | gnorm 0.61 | loss_scale 16 | train_wall 727 | gb_free 9.8 | wall 27806
2022-03-12 18:05:16 | INFO | fairseq.trainer | begin training epoch 34
2022-03-12 18:05:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 18:06:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 18:07:20 | INFO | train_inner | epoch 034:     59 / 392 loss=5.277, ppl=38.78, wps=27714.6, ups=0.43, wpb=65029.1, bsz=127, num_updates=12900, lr=0.000278423, gnorm=0.616, loss_scale=16, train_wall=185, gb_free=9.8, wall=27930
2022-03-12 18:10:48 | INFO | train_inner | epoch 034:    159 / 392 loss=5.269, ppl=38.56, wps=31484, ups=0.48, wpb=65532.7, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.61, loss_scale=16, train_wall=185, gb_free=9.8, wall=28138
2022-03-12 18:11:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 18:14:20 | INFO | train_inner | epoch 034:    260 / 392 loss=5.291, ppl=39.15, wps=30958, ups=0.47, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.608, loss_scale=16, train_wall=188, gb_free=9.8, wall=28350
2022-03-12 18:15:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 18:17:50 | INFO | train_inner | epoch 034:    361 / 392 loss=5.3, ppl=39.38, wps=31172.6, ups=0.48, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.604, loss_scale=16, train_wall=187, gb_free=9.8, wall=28560
2022-03-12 18:18:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 18:19:16 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 5.609 | ppl 48.82 | wps 54296.6 | wpb 511.9 | bsz 1 | num_updates 13231 | best_loss 5.609
2022-03-12 18:19:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13231 updates
2022-03-12 18:19:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 18:19:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 18:19:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 34 @ 13231 updates, score 5.609) (writing took 2.068741427967325 seconds)
2022-03-12 18:19:18 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-12 18:19:18 | INFO | train | epoch 034 | loss 5.281 | ppl 38.88 | wps 30216.2 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 13231 | lr 0.000274918 | gnorm 0.609 | loss_scale 16 | train_wall 725 | gb_free 9.8 | wall 28648
2022-03-12 18:19:18 | INFO | fairseq.trainer | begin training epoch 35
2022-03-12 18:19:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 18:20:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 18:21:41 | INFO | train_inner | epoch 035:     70 / 392 loss=5.247, ppl=37.97, wps=28176.4, ups=0.43, wpb=65029.1, bsz=127, num_updates=13300, lr=0.000274204, gnorm=0.621, loss_scale=16, train_wall=182, gb_free=9.8, wall=28791
2022-03-12 18:24:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 18:25:07 | INFO | train_inner | epoch 035:    171 / 392 loss=5.25, ppl=38.05, wps=31749.8, ups=0.48, wpb=65532.7, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.61, loss_scale=16, train_wall=183, gb_free=9.8, wall=28997
2022-03-12 18:27:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 18:28:35 | INFO | train_inner | epoch 035:    272 / 392 loss=5.283, ppl=38.93, wps=31581.2, ups=0.48, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.617, loss_scale=8, train_wall=184, gb_free=9.8, wall=29205
2022-03-12 18:32:00 | INFO | train_inner | epoch 035:    372 / 392 loss=5.289, ppl=39.09, wps=31974.3, ups=0.49, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.621, loss_scale=8, train_wall=182, gb_free=9.8, wall=29410
2022-03-12 18:32:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 18:33:03 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 5.598 | ppl 48.43 | wps 55737.1 | wpb 511.9 | bsz 1 | num_updates 13620 | best_loss 5.598
2022-03-12 18:33:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13620 updates
2022-03-12 18:33:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 18:33:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 18:33:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 35 @ 13620 updates, score 5.598) (writing took 2.04958535998594 seconds)
2022-03-12 18:33:05 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-12 18:33:05 | INFO | train | epoch 035 | loss 5.265 | ppl 38.45 | wps 30757.7 | ups 0.47 | wpb 65404.8 | bsz 127.7 | num_updates 13620 | lr 0.000270964 | gnorm 0.617 | loss_scale 16 | train_wall 712 | gb_free 9.8 | wall 29475
2022-03-12 18:33:05 | INFO | fairseq.trainer | begin training epoch 36
2022-03-12 18:33:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 18:35:48 | INFO | train_inner | epoch 036:     80 / 392 loss=5.22, ppl=37.28, wps=28412.2, ups=0.44, wpb=65025.8, bsz=127, num_updates=13700, lr=0.000270172, gnorm=0.619, loss_scale=16, train_wall=181, gb_free=9.8, wall=29639
2022-03-12 18:37:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 18:39:16 | INFO | train_inner | epoch 036:    181 / 392 loss=5.247, ppl=37.97, wps=31564.8, ups=0.48, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.606, loss_scale=16, train_wall=184, gb_free=9.8, wall=29846
2022-03-12 18:41:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 18:42:44 | INFO | train_inner | epoch 036:    282 / 392 loss=5.26, ppl=38.33, wps=31491.7, ups=0.48, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.613, loss_scale=16, train_wall=185, gb_free=9.8, wall=30054
2022-03-12 18:46:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 18:46:15 | INFO | train_inner | epoch 036:    383 / 392 loss=5.277, ppl=38.76, wps=31084.8, ups=0.47, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.612, loss_scale=16, train_wall=187, gb_free=9.8, wall=30265
2022-03-12 18:46:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 18:46:56 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 5.607 | ppl 48.73 | wps 54195.3 | wpb 511.9 | bsz 1 | num_updates 14009 | best_loss 5.598
2022-03-12 18:46:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14009 updates
2022-03-12 18:46:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 18:46:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 18:46:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt (epoch 36 @ 14009 updates, score 5.607) (writing took 1.1673798349802382 seconds)
2022-03-12 18:46:57 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-12 18:46:57 | INFO | train | epoch 036 | loss 5.25 | ppl 38.06 | wps 30567.8 | ups 0.47 | wpb 65404.8 | bsz 127.7 | num_updates 14009 | lr 0.000267175 | gnorm 0.614 | loss_scale 16 | train_wall 717 | gb_free 9.8 | wall 30307
2022-03-12 18:46:57 | INFO | fairseq.trainer | begin training epoch 37
2022-03-12 18:46:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 18:50:08 | INFO | train_inner | epoch 037:     91 / 392 loss=5.209, ppl=36.99, wps=27931.3, ups=0.43, wpb=65029.1, bsz=127, num_updates=14100, lr=0.000266312, gnorm=0.615, loss_scale=16, train_wall=185, gb_free=9.8, wall=30498
2022-03-12 18:51:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 18:51:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 18:53:42 | INFO | train_inner | epoch 037:    193 / 392 loss=5.228, ppl=37.49, wps=30590.2, ups=0.47, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.613, loss_scale=8, train_wall=191, gb_free=9.8, wall=30712
2022-03-12 18:57:11 | INFO | train_inner | epoch 037:    293 / 392 loss=5.248, ppl=38, wps=31324.2, ups=0.48, wpb=65532.7, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.607, loss_scale=16, train_wall=186, gb_free=9.8, wall=30921
2022-03-12 19:00:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 19:00:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 19:01:01 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 5.597 | ppl 48.39 | wps 54688.1 | wpb 511.9 | bsz 1 | num_updates 14398 | best_loss 5.597
2022-03-12 19:01:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14398 updates
2022-03-12 19:01:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 19:01:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 19:01:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 37 @ 14398 updates, score 5.597) (writing took 2.0899354349821806 seconds)
2022-03-12 19:01:03 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-12 19:01:03 | INFO | train | epoch 037 | loss 5.236 | ppl 37.69 | wps 30088.3 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 14398 | lr 0.000263541 | gnorm 0.611 | loss_scale 16 | train_wall 729 | gb_free 9.8 | wall 31153
2022-03-12 19:01:03 | INFO | fairseq.trainer | begin training epoch 38
2022-03-12 19:01:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 19:01:07 | INFO | train_inner | epoch 038:      2 / 392 loss=5.264, ppl=38.43, wps=27578.8, ups=0.42, wpb=65029.1, bsz=127, num_updates=14400, lr=0.000263523, gnorm=0.617, loss_scale=16, train_wall=187, gb_free=9.8, wall=31157
2022-03-12 19:04:31 | INFO | train_inner | epoch 038:    102 / 392 loss=5.192, ppl=36.55, wps=32089, ups=0.49, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.625, loss_scale=16, train_wall=181, gb_free=9.8, wall=31361
2022-03-12 19:05:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 19:06:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 19:08:05 | INFO | train_inner | epoch 038:    204 / 392 loss=5.215, ppl=37.14, wps=30640.7, ups=0.47, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.609, loss_scale=8, train_wall=190, gb_free=9.8, wall=31575
2022-03-12 19:11:33 | INFO | train_inner | epoch 038:    304 / 392 loss=5.238, ppl=37.74, wps=31502.5, ups=0.48, wpb=65532.7, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.612, loss_scale=16, train_wall=185, gb_free=9.8, wall=31783
2022-03-12 19:14:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 19:15:00 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 5.591 | ppl 48.19 | wps 54316.5 | wpb 511.9 | bsz 1 | num_updates 14788 | best_loss 5.591
2022-03-12 19:15:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14788 updates
2022-03-12 19:15:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 19:15:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 19:15:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 38 @ 14788 updates, score 5.591) (writing took 2.03342500096187 seconds)
2022-03-12 19:15:02 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-12 19:15:02 | INFO | train | epoch 038 | loss 5.222 | ppl 37.33 | wps 30387.8 | ups 0.46 | wpb 65405.2 | bsz 127.7 | num_updates 14788 | lr 0.000260043 | gnorm 0.616 | loss_scale 16 | train_wall 723 | gb_free 9.8 | wall 31992
2022-03-12 19:15:02 | INFO | fairseq.trainer | begin training epoch 39
2022-03-12 19:15:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 19:15:27 | INFO | train_inner | epoch 039:     12 / 392 loss=5.237, ppl=37.7, wps=27797.7, ups=0.43, wpb=65029.1, bsz=127, num_updates=14800, lr=0.000259938, gnorm=0.621, loss_scale=16, train_wall=185, gb_free=9.8, wall=32017
2022-03-12 19:15:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 19:18:57 | INFO | train_inner | epoch 039:    113 / 392 loss=5.181, ppl=36.27, wps=31282.8, ups=0.48, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.606, loss_scale=16, train_wall=186, gb_free=9.8, wall=32227
2022-03-12 19:20:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 19:22:27 | INFO | train_inner | epoch 039:    214 / 392 loss=5.198, ppl=36.72, wps=31126.6, ups=0.47, wpb=65532.7, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.616, loss_scale=16, train_wall=187, gb_free=9.8, wall=32437
2022-03-12 19:24:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 19:25:59 | INFO | train_inner | epoch 039:    315 / 392 loss=5.229, ppl=37.51, wps=30998.2, ups=0.47, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.61, loss_scale=16, train_wall=188, gb_free=9.8, wall=32649
2022-03-12 19:28:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 19:29:00 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 5.58 | ppl 47.84 | wps 54852.1 | wpb 511.9 | bsz 1 | num_updates 15177 | best_loss 5.58
2022-03-12 19:29:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15177 updates
2022-03-12 19:29:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 19:29:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 19:29:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 39 @ 15177 updates, score 5.58) (writing took 2.0226570689701475 seconds)
2022-03-12 19:29:02 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-12 19:29:02 | INFO | train | epoch 039 | loss 5.209 | ppl 36.98 | wps 30298 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 15177 | lr 0.000256689 | gnorm 0.613 | loss_scale 16 | train_wall 724 | gb_free 9.8 | wall 32832
2022-03-12 19:29:02 | INFO | fairseq.trainer | begin training epoch 40
2022-03-12 19:29:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 19:29:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 19:29:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 19:29:54 | INFO | train_inner | epoch 040:     25 / 392 loss=5.214, ppl=37.11, wps=27638.5, ups=0.43, wpb=65029.1, bsz=127, num_updates=15200, lr=0.000256495, gnorm=0.622, loss_scale=8, train_wall=186, gb_free=9.8, wall=32884
2022-03-12 19:33:24 | INFO | train_inner | epoch 040:    125 / 392 loss=5.17, ppl=36, wps=31268, ups=0.48, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.616, loss_scale=8, train_wall=186, gb_free=9.8, wall=33094
2022-03-12 19:36:51 | INFO | train_inner | epoch 040:    225 / 392 loss=5.196, ppl=36.66, wps=31628.3, ups=0.48, wpb=65532.7, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.619, loss_scale=16, train_wall=184, gb_free=9.8, wall=33301
2022-03-12 19:38:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 19:40:22 | INFO | train_inner | epoch 040:    326 / 392 loss=5.213, ppl=37.08, wps=30972, ups=0.47, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.615, loss_scale=16, train_wall=188, gb_free=9.8, wall=33512
2022-03-12 19:42:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 19:43:01 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 5.579 | ppl 47.79 | wps 54599.2 | wpb 511.9 | bsz 1 | num_updates 15566 | best_loss 5.579
2022-03-12 19:43:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15566 updates
2022-03-12 19:43:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 19:43:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 19:43:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 40 @ 15566 updates, score 5.579) (writing took 2.0435345959849656 seconds)
2022-03-12 19:43:03 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-12 19:43:03 | INFO | train | epoch 040 | loss 5.196 | ppl 36.66 | wps 30243.9 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 15566 | lr 0.000253461 | gnorm 0.62 | loss_scale 16 | train_wall 725 | gb_free 9.8 | wall 33673
2022-03-12 19:43:03 | INFO | fairseq.trainer | begin training epoch 41
2022-03-12 19:43:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 19:43:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 19:44:16 | INFO | train_inner | epoch 041:     35 / 392 loss=5.2, ppl=36.76, wps=27857.9, ups=0.43, wpb=65029.1, bsz=127, num_updates=15600, lr=0.000253185, gnorm=0.627, loss_scale=16, train_wall=185, gb_free=9.8, wall=33746
2022-03-12 19:47:45 | INFO | train_inner | epoch 041:    135 / 392 loss=5.169, ppl=35.97, wps=31247.1, ups=0.48, wpb=65532.7, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.62, loss_scale=16, train_wall=187, gb_free=9.8, wall=33956
2022-03-12 19:48:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 19:51:15 | INFO | train_inner | epoch 041:    236 / 392 loss=5.186, ppl=36.4, wps=31230.6, ups=0.48, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.612, loss_scale=16, train_wall=186, gb_free=9.8, wall=34165
2022-03-12 19:52:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 19:54:45 | INFO | train_inner | epoch 041:    337 / 392 loss=5.203, ppl=36.84, wps=31226.2, ups=0.48, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.618, loss_scale=16, train_wall=187, gb_free=9.8, wall=34375
2022-03-12 19:56:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 19:57:01 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 5.578 | ppl 47.76 | wps 54334.8 | wpb 511.9 | bsz 1 | num_updates 15955 | best_loss 5.578
2022-03-12 19:57:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 15955 updates
2022-03-12 19:57:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 19:57:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 19:57:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 41 @ 15955 updates, score 5.578) (writing took 2.139779652003199 seconds)
2022-03-12 19:57:03 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-12 19:57:03 | INFO | train | epoch 041 | loss 5.185 | ppl 36.37 | wps 30292.2 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 15955 | lr 0.000250352 | gnorm 0.62 | loss_scale 16 | train_wall 723 | gb_free 9.8 | wall 34513
2022-03-12 19:57:03 | INFO | fairseq.trainer | begin training epoch 42
2022-03-12 19:57:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 19:57:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 19:58:39 | INFO | train_inner | epoch 042:     46 / 392 loss=5.172, ppl=36.04, wps=27847.8, ups=0.43, wpb=65029.1, bsz=127, num_updates=16000, lr=0.00025, gnorm=0.63, loss_scale=16, train_wall=184, gb_free=9.8, wall=34609
2022-03-12 20:02:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 20:02:10 | INFO | train_inner | epoch 042:    147 / 392 loss=5.156, ppl=35.64, wps=31029.7, ups=0.47, wpb=65536, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.618, loss_scale=16, train_wall=188, gb_free=9.8, wall=34820
2022-03-12 20:04:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 20:05:38 | INFO | train_inner | epoch 042:    248 / 392 loss=5.171, ppl=36.03, wps=31528.9, ups=0.48, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.616, loss_scale=8, train_wall=185, gb_free=9.8, wall=35028
2022-03-12 20:09:07 | INFO | train_inner | epoch 042:    348 / 392 loss=5.198, ppl=36.72, wps=31263.2, ups=0.48, wpb=65532.7, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.62, loss_scale=8, train_wall=186, gb_free=9.8, wall=35237
2022-03-12 20:10:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 20:11:01 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 5.57 | ppl 47.51 | wps 55141.9 | wpb 511.9 | bsz 1 | num_updates 16344 | best_loss 5.57
2022-03-12 20:11:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16344 updates
2022-03-12 20:11:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 20:11:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 20:11:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 42 @ 16344 updates, score 5.57) (writing took 2.0867378690163605 seconds)
2022-03-12 20:11:03 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-12 20:11:03 | INFO | train | epoch 042 | loss 5.172 | ppl 36.06 | wps 30295.3 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 16344 | lr 0.000247355 | gnorm 0.62 | loss_scale 16 | train_wall 724 | gb_free 9.8 | wall 35353
2022-03-12 20:11:03 | INFO | fairseq.trainer | begin training epoch 43
2022-03-12 20:11:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 20:13:02 | INFO | train_inner | epoch 043:     56 / 392 loss=5.154, ppl=35.62, wps=27774.8, ups=0.43, wpb=65029.1, bsz=127, num_updates=16400, lr=0.000246932, gnorm=0.628, loss_scale=16, train_wall=185, gb_free=9.8, wall=35472
2022-03-12 20:14:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 20:15:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 20:16:33 | INFO | train_inner | epoch 043:    158 / 392 loss=5.146, ppl=35.41, wps=30965.4, ups=0.47, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.621, loss_scale=8, train_wall=188, gb_free=9.8, wall=35683
2022-03-12 20:20:03 | INFO | train_inner | epoch 043:    258 / 392 loss=5.169, ppl=35.97, wps=31272.3, ups=0.48, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.624, loss_scale=16, train_wall=186, gb_free=9.8, wall=35893
2022-03-12 20:23:30 | INFO | train_inner | epoch 043:    358 / 392 loss=5.186, ppl=36.4, wps=31585, ups=0.48, wpb=65532.7, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.622, loss_scale=16, train_wall=184, gb_free=9.8, wall=36100
2022-03-12 20:23:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 20:24:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 20:25:03 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 5.566 | ppl 47.38 | wps 54954.6 | wpb 511.9 | bsz 1 | num_updates 16733 | best_loss 5.566
2022-03-12 20:25:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16733 updates
2022-03-12 20:25:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 20:25:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 20:25:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 43 @ 16733 updates, score 5.566) (writing took 2.212339834019076 seconds)
2022-03-12 20:25:05 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-12 20:25:05 | INFO | train | epoch 043 | loss 5.162 | ppl 35.79 | wps 30203.3 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 16733 | lr 0.000244463 | gnorm 0.625 | loss_scale 16 | train_wall 726 | gb_free 9.8 | wall 36195
2022-03-12 20:25:05 | INFO | fairseq.trainer | begin training epoch 44
2022-03-12 20:25:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 20:27:26 | INFO | train_inner | epoch 044:     67 / 392 loss=5.139, ppl=35.23, wps=27573.7, ups=0.42, wpb=65029.1, bsz=127, num_updates=16800, lr=0.000243975, gnorm=0.634, loss_scale=16, train_wall=187, gb_free=9.8, wall=36336
2022-03-12 20:28:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 20:30:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 20:30:59 | INFO | train_inner | epoch 044:    169 / 392 loss=5.131, ppl=35.05, wps=30801.6, ups=0.47, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.629, loss_scale=8, train_wall=189, gb_free=9.8, wall=36549
2022-03-12 20:34:29 | INFO | train_inner | epoch 044:    269 / 392 loss=5.168, ppl=35.96, wps=31181.5, ups=0.48, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.621, loss_scale=8, train_wall=187, gb_free=9.8, wall=36759
2022-03-12 20:37:57 | INFO | train_inner | epoch 044:    369 / 392 loss=5.169, ppl=35.98, wps=31575, ups=0.48, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.624, loss_scale=16, train_wall=184, gb_free=9.8, wall=36967
2022-03-12 20:38:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 20:39:07 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 5.568 | ppl 47.44 | wps 53747.3 | wpb 511.9 | bsz 1 | num_updates 17123 | best_loss 5.566
2022-03-12 20:39:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17123 updates
2022-03-12 20:39:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 20:39:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 20:39:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt (epoch 44 @ 17123 updates, score 5.568) (writing took 1.1653898539952934 seconds)
2022-03-12 20:39:09 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-12 20:39:09 | INFO | train | epoch 044 | loss 5.151 | ppl 35.53 | wps 30245.3 | ups 0.46 | wpb 65405.2 | bsz 127.7 | num_updates 17123 | lr 0.000241663 | gnorm 0.628 | loss_scale 16 | train_wall 727 | gb_free 9.8 | wall 37039
2022-03-12 20:39:09 | INFO | fairseq.trainer | begin training epoch 45
2022-03-12 20:39:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 20:39:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 20:40:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 20:41:53 | INFO | train_inner | epoch 045:     79 / 392 loss=5.122, ppl=34.83, wps=27500.6, ups=0.42, wpb=65025.8, bsz=127, num_updates=17200, lr=0.000241121, gnorm=0.629, loss_scale=8, train_wall=188, gb_free=9.8, wall=37203
2022-03-12 20:45:22 | INFO | train_inner | epoch 045:    179 / 392 loss=5.126, ppl=34.92, wps=31325.7, ups=0.48, wpb=65536, bsz=128, num_updates=17300, lr=0.000240424, gnorm=0.63, loss_scale=16, train_wall=186, gb_free=9.8, wall=37412
2022-03-12 20:46:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 20:48:53 | INFO | train_inner | epoch 045:    280 / 392 loss=5.152, ppl=35.57, wps=31155.2, ups=0.48, wpb=65532.7, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.634, loss_scale=8, train_wall=187, gb_free=9.8, wall=37623
2022-03-12 20:52:22 | INFO | train_inner | epoch 045:    380 / 392 loss=5.171, ppl=36.02, wps=31231.6, ups=0.48, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.62, loss_scale=16, train_wall=187, gb_free=9.8, wall=37832
2022-03-12 20:52:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 20:53:09 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 5.569 | ppl 47.46 | wps 54679.8 | wpb 511.9 | bsz 1 | num_updates 17512 | best_loss 5.566
2022-03-12 20:53:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17512 updates
2022-03-12 20:53:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 20:53:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 20:53:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt (epoch 45 @ 17512 updates, score 5.569) (writing took 1.142826897033956 seconds)
2022-03-12 20:53:10 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-12 20:53:10 | INFO | train | epoch 045 | loss 5.141 | ppl 35.27 | wps 30224 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 17512 | lr 0.000238964 | gnorm 0.628 | loss_scale 16 | train_wall 726 | gb_free 9.8 | wall 37881
2022-03-12 20:53:11 | INFO | fairseq.trainer | begin training epoch 46
2022-03-12 20:53:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 20:55:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 20:56:16 | INFO | train_inner | epoch 046:     89 / 392 loss=5.102, ppl=34.34, wps=27875.9, ups=0.43, wpb=65025.8, bsz=127, num_updates=17600, lr=0.000238366, gnorm=0.634, loss_scale=16, train_wall=185, gb_free=9.8, wall=38066
2022-03-12 20:59:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 20:59:48 | INFO | train_inner | epoch 046:    190 / 392 loss=5.127, ppl=34.95, wps=30878.2, ups=0.47, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.624, loss_scale=8, train_wall=189, gb_free=9.8, wall=38278
2022-03-12 21:03:15 | INFO | train_inner | epoch 046:    290 / 392 loss=5.141, ppl=35.28, wps=31591.1, ups=0.48, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.621, loss_scale=8, train_wall=184, gb_free=9.8, wall=38485
2022-03-12 21:06:42 | INFO | train_inner | epoch 046:    390 / 392 loss=5.153, ppl=35.59, wps=31649.4, ups=0.48, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.631, loss_scale=16, train_wall=184, gb_free=9.8, wall=38693
2022-03-12 21:06:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 21:07:09 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 5.562 | ppl 47.23 | wps 54404.4 | wpb 511.9 | bsz 1 | num_updates 17902 | best_loss 5.562
2022-03-12 21:07:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 17902 updates
2022-03-12 21:07:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 21:07:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 21:07:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 46 @ 17902 updates, score 5.562) (writing took 2.0558785569737665 seconds)
2022-03-12 21:07:11 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-12 21:07:11 | INFO | train | epoch 046 | loss 5.13 | ppl 35.02 | wps 30354.2 | ups 0.46 | wpb 65405.2 | bsz 127.7 | num_updates 17902 | lr 0.000236347 | gnorm 0.627 | loss_scale 16 | train_wall 724 | gb_free 9.8 | wall 38721
2022-03-12 21:07:11 | INFO | fairseq.trainer | begin training epoch 47
2022-03-12 21:07:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 21:08:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 21:10:36 | INFO | train_inner | epoch 047:     99 / 392 loss=5.089, ppl=34.03, wps=27785.9, ups=0.43, wpb=65029.1, bsz=127, num_updates=18000, lr=0.000235702, gnorm=0.629, loss_scale=16, train_wall=185, gb_free=9.8, wall=38927
2022-03-12 21:13:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 21:14:06 | INFO | train_inner | epoch 047:    200 / 392 loss=5.114, ppl=34.63, wps=31283.7, ups=0.48, wpb=65532.7, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.635, loss_scale=16, train_wall=186, gb_free=9.8, wall=39136
2022-03-12 21:17:33 | INFO | train_inner | epoch 047:    300 / 392 loss=5.134, ppl=35.11, wps=31655.1, ups=0.48, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.63, loss_scale=16, train_wall=184, gb_free=9.8, wall=39343
2022-03-12 21:18:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 21:20:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 21:21:08 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 5.56 | ppl 47.16 | wps 55422.1 | wpb 511.9 | bsz 1 | num_updates 18291 | best_loss 5.56
2022-03-12 21:21:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18291 updates
2022-03-12 21:21:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 21:21:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 21:21:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 47 @ 18291 updates, score 5.56) (writing took 2.0768398320069537 seconds)
2022-03-12 21:21:10 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-12 21:21:10 | INFO | train | epoch 047 | loss 5.121 | ppl 34.8 | wps 30330 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 18291 | lr 0.00023382 | gnorm 0.629 | loss_scale 16 | train_wall 723 | gb_free 9.8 | wall 39560
2022-03-12 21:21:10 | INFO | fairseq.trainer | begin training epoch 48
2022-03-12 21:21:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 21:21:29 | INFO | train_inner | epoch 048:      9 / 392 loss=5.142, ppl=35.31, wps=27604.6, ups=0.42, wpb=65029.1, bsz=127, num_updates=18300, lr=0.000233762, gnorm=0.627, loss_scale=16, train_wall=187, gb_free=9.8, wall=39579
2022-03-12 21:23:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 21:24:59 | INFO | train_inner | epoch 048:    110 / 392 loss=5.081, ppl=33.84, wps=31130.6, ups=0.48, wpb=65532.7, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.621, loss_scale=16, train_wall=187, gb_free=9.8, wall=39789
2022-03-12 21:27:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 21:28:31 | INFO | train_inner | epoch 048:    211 / 392 loss=5.109, ppl=34.5, wps=30909.8, ups=0.47, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.628, loss_scale=16, train_wall=189, gb_free=9.8, wall=40001
2022-03-12 21:31:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 21:32:01 | INFO | train_inner | epoch 048:    312 / 392 loss=5.129, ppl=34.98, wps=31153.5, ups=0.48, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.626, loss_scale=16, train_wall=187, gb_free=9.8, wall=40212
2022-03-12 21:34:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 21:35:10 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 5.556 | ppl 47.06 | wps 55793.6 | wpb 511.9 | bsz 1 | num_updates 18680 | best_loss 5.556
2022-03-12 21:35:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18680 updates
2022-03-12 21:35:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 21:35:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 21:35:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 48 @ 18680 updates, score 5.556) (writing took 2.0319975719903596 seconds)
2022-03-12 21:35:12 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-12 21:35:12 | INFO | train | epoch 048 | loss 5.112 | ppl 34.57 | wps 30188.3 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 18680 | lr 0.000231372 | gnorm 0.626 | loss_scale 16 | train_wall 727 | gb_free 9.8 | wall 40403
2022-03-12 21:35:13 | INFO | fairseq.trainer | begin training epoch 49
2022-03-12 21:35:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 21:35:54 | INFO | train_inner | epoch 049:     20 / 392 loss=5.122, ppl=34.83, wps=27933.4, ups=0.43, wpb=65029.1, bsz=127, num_updates=18700, lr=0.000231249, gnorm=0.632, loss_scale=16, train_wall=185, gb_free=9.8, wall=40444
2022-03-12 21:36:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 21:38:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 21:39:28 | INFO | train_inner | epoch 049:    122 / 392 loss=5.074, ppl=33.69, wps=30630.6, ups=0.47, wpb=65532.7, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.627, loss_scale=8, train_wall=190, gb_free=9.8, wall=40658
2022-03-12 21:42:56 | INFO | train_inner | epoch 049:    222 / 392 loss=5.104, ppl=34.38, wps=31495.8, ups=0.48, wpb=65536, bsz=128, num_updates=18900, lr=0.000230022, gnorm=0.63, loss_scale=8, train_wall=185, gb_free=9.8, wall=40866
2022-03-12 21:43:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 21:46:26 | INFO | train_inner | epoch 049:    323 / 392 loss=5.118, ppl=34.72, wps=31299.4, ups=0.48, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.625, loss_scale=8, train_wall=186, gb_free=9.8, wall=41076
2022-03-12 21:48:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 21:49:12 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 5.555 | ppl 47.02 | wps 54546.1 | wpb 511.9 | bsz 1 | num_updates 19069 | best_loss 5.555
2022-03-12 21:49:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19069 updates
2022-03-12 21:49:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 21:49:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 21:49:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 49 @ 19069 updates, score 5.555) (writing took 2.0330562070012093 seconds)
2022-03-12 21:49:14 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-12 21:49:14 | INFO | train | epoch 049 | loss 5.102 | ppl 34.35 | wps 30248.2 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 19069 | lr 0.000229 | gnorm 0.629 | loss_scale 16 | train_wall 725 | gb_free 9.8 | wall 41244
2022-03-12 21:49:14 | INFO | fairseq.trainer | begin training epoch 50
2022-03-12 21:49:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 21:50:18 | INFO | train_inner | epoch 050:     31 / 392 loss=5.104, ppl=34.39, wps=27953.8, ups=0.43, wpb=65029.1, bsz=127, num_updates=19100, lr=0.000228814, gnorm=0.631, loss_scale=16, train_wall=184, gb_free=9.8, wall=41308
2022-03-12 21:52:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 21:53:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 21:53:52 | INFO | train_inner | epoch 050:    133 / 392 loss=5.066, ppl=33.49, wps=30707.5, ups=0.47, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.642, loss_scale=8, train_wall=190, gb_free=9.8, wall=41522
2022-03-12 21:57:19 | INFO | train_inner | epoch 050:    233 / 392 loss=5.096, ppl=34.19, wps=31563.2, ups=0.48, wpb=65532.7, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.63, loss_scale=8, train_wall=184, gb_free=9.8, wall=41729
2022-03-12 21:59:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 22:00:51 | INFO | train_inner | epoch 050:    334 / 392 loss=5.115, ppl=34.65, wps=30981.7, ups=0.47, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.631, loss_scale=8, train_wall=188, gb_free=9.8, wall=41941
2022-03-12 22:02:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 22:03:13 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 5.554 | ppl 46.99 | wps 54307.3 | wpb 511.9 | bsz 1 | num_updates 19458 | best_loss 5.554
2022-03-12 22:03:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19458 updates
2022-03-12 22:03:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 22:03:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 22:03:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 50 @ 19458 updates, score 5.554) (writing took 2.0315865189768374 seconds)
2022-03-12 22:03:15 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-12 22:03:15 | INFO | train | epoch 050 | loss 5.094 | ppl 34.16 | wps 30224.6 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 19458 | lr 0.0002267 | gnorm 0.636 | loss_scale 8 | train_wall 725 | gb_free 9.8 | wall 42085
2022-03-12 22:03:15 | INFO | fairseq.trainer | begin training epoch 51
2022-03-12 22:03:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 22:04:42 | INFO | train_inner | epoch 051:     42 / 392 loss=5.098, ppl=34.24, wps=28115, ups=0.43, wpb=65029.1, bsz=127, num_updates=19500, lr=0.000226455, gnorm=0.64, loss_scale=16, train_wall=182, gb_free=9.8, wall=42172
2022-03-12 22:08:12 | INFO | train_inner | epoch 051:    142 / 392 loss=5.063, ppl=33.43, wps=31284.8, ups=0.48, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.631, loss_scale=16, train_wall=186, gb_free=9.8, wall=42382
2022-03-12 22:09:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 22:11:41 | INFO | train_inner | epoch 051:    243 / 392 loss=5.087, ppl=34, wps=31351.7, ups=0.48, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.629, loss_scale=16, train_wall=186, gb_free=9.8, wall=42591
2022-03-12 22:13:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 22:14:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 22:15:13 | INFO | train_inner | epoch 051:    345 / 392 loss=5.113, ppl=34.61, wps=30848.8, ups=0.47, wpb=65532.7, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.625, loss_scale=8, train_wall=189, gb_free=9.8, wall=42803
2022-03-12 22:16:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 22:17:13 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 5.55 | ppl 46.84 | wps 54312.8 | wpb 511.9 | bsz 1 | num_updates 19847 | best_loss 5.55
2022-03-12 22:17:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 19847 updates
2022-03-12 22:17:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 22:17:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 22:17:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 51 @ 19847 updates, score 5.55) (writing took 2.1013474719948135 seconds)
2022-03-12 22:17:15 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-12 22:17:15 | INFO | train | epoch 051 | loss 5.086 | ppl 33.96 | wps 30304.6 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 19847 | lr 0.000224467 | gnorm 0.629 | loss_scale 8 | train_wall 723 | gb_free 9.8 | wall 42925
2022-03-12 22:17:15 | INFO | fairseq.trainer | begin training epoch 52
2022-03-12 22:17:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 22:19:04 | INFO | train_inner | epoch 052:     53 / 392 loss=5.074, ppl=33.68, wps=28160.5, ups=0.43, wpb=65029.1, bsz=127, num_updates=19900, lr=0.000224168, gnorm=0.643, loss_scale=8, train_wall=182, gb_free=9.8, wall=43034
2022-03-12 22:22:20 | INFO | train_inner | epoch 052:    153 / 392 loss=5.057, ppl=33.29, wps=33402.9, ups=0.51, wpb=65532.7, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.63, loss_scale=16, train_wall=174, gb_free=9.8, wall=43230
2022-03-12 22:23:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 22:23:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 22:25:41 | INFO | train_inner | epoch 052:    255 / 392 loss=5.082, ppl=33.86, wps=32695, ups=0.5, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.629, loss_scale=8, train_wall=177, gb_free=9.8, wall=43431
2022-03-12 22:28:57 | INFO | train_inner | epoch 052:    355 / 392 loss=5.103, ppl=34.37, wps=33416.1, ups=0.51, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.632, loss_scale=16, train_wall=174, gb_free=9.8, wall=43627
2022-03-12 22:29:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 22:30:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 22:30:30 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 5.549 | ppl 46.82 | wps 57444.6 | wpb 511.9 | bsz 1 | num_updates 20236 | best_loss 5.549
2022-03-12 22:30:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20236 updates
2022-03-12 22:30:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 22:30:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 22:30:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 52 @ 20236 updates, score 5.549) (writing took 2.19060975802131 seconds)
2022-03-12 22:30:33 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-12 22:30:33 | INFO | train | epoch 052 | loss 5.078 | ppl 33.77 | wps 31898.1 | ups 0.49 | wpb 65404.8 | bsz 127.7 | num_updates 20236 | lr 0.000222299 | gnorm 0.635 | loss_scale 8 | train_wall 684 | gb_free 9.8 | wall 43723
2022-03-12 22:30:33 | INFO | fairseq.trainer | begin training epoch 53
2022-03-12 22:30:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 22:32:38 | INFO | train_inner | epoch 053:     64 / 392 loss=5.054, ppl=33.23, wps=29355.3, ups=0.45, wpb=65029.1, bsz=127, num_updates=20300, lr=0.000221948, gnorm=0.642, loss_scale=8, train_wall=174, gb_free=9.8, wall=43848
2022-03-12 22:34:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 22:35:56 | INFO | train_inner | epoch 053:    165 / 392 loss=5.057, ppl=33.29, wps=33076.1, ups=0.5, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.644, loss_scale=8, train_wall=175, gb_free=9.8, wall=44047
2022-03-12 22:39:13 | INFO | train_inner | epoch 053:    265 / 392 loss=5.077, ppl=33.76, wps=33402.8, ups=0.51, wpb=65532.7, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.626, loss_scale=16, train_wall=174, gb_free=9.8, wall=44243
2022-03-12 22:42:30 | INFO | train_inner | epoch 053:    365 / 392 loss=5.096, ppl=34.2, wps=33141.6, ups=0.51, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.635, loss_scale=16, train_wall=175, gb_free=9.8, wall=44440
2022-03-12 22:42:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 22:43:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 22:43:45 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 5.548 | ppl 46.77 | wps 57390.2 | wpb 511.9 | bsz 1 | num_updates 20626 | best_loss 5.548
2022-03-12 22:43:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20626 updates
2022-03-12 22:43:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 22:43:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 22:43:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 53 @ 20626 updates, score 5.548) (writing took 2.1776943810400553 seconds)
2022-03-12 22:43:47 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-12 22:43:47 | INFO | train | epoch 053 | loss 5.07 | ppl 33.58 | wps 32110.7 | ups 0.49 | wpb 65405.2 | bsz 127.7 | num_updates 20626 | lr 0.000220187 | gnorm 0.636 | loss_scale 16 | train_wall 681 | gb_free 9.8 | wall 44517
2022-03-12 22:43:47 | INFO | fairseq.trainer | begin training epoch 54
2022-03-12 22:43:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 22:46:13 | INFO | train_inner | epoch 054:     74 / 392 loss=5.04, ppl=32.91, wps=29189.1, ups=0.45, wpb=65025.8, bsz=127, num_updates=20700, lr=0.000219793, gnorm=0.636, loss_scale=16, train_wall=175, gb_free=9.8, wall=44663
2022-03-12 22:47:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 22:48:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 22:49:35 | INFO | train_inner | epoch 054:    176 / 392 loss=5.052, ppl=33.17, wps=32523.8, ups=0.5, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.628, loss_scale=8, train_wall=178, gb_free=9.8, wall=44865
2022-03-12 22:52:52 | INFO | train_inner | epoch 054:    276 / 392 loss=5.074, ppl=33.68, wps=33223.8, ups=0.51, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.642, loss_scale=16, train_wall=175, gb_free=9.8, wall=45062
2022-03-12 22:56:10 | INFO | train_inner | epoch 054:    376 / 392 loss=5.089, ppl=34.03, wps=33055.1, ups=0.5, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.632, loss_scale=16, train_wall=176, gb_free=9.8, wall=45260
2022-03-12 22:56:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 22:57:03 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 5.539 | ppl 46.5 | wps 56401.1 | wpb 511.9 | bsz 1 | num_updates 21016 | best_loss 5.539
2022-03-12 22:57:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21016 updates
2022-03-12 22:57:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 22:57:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt
2022-03-12 22:57:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_best.pt (epoch 54 @ 21016 updates, score 5.539) (writing took 2.0970647369977087 seconds)
2022-03-12 22:57:06 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-12 22:57:06 | INFO | train | epoch 054 | loss 5.062 | ppl 33.42 | wps 31941.2 | ups 0.49 | wpb 65405.2 | bsz 127.7 | num_updates 21016 | lr 0.000218135 | gnorm 0.635 | loss_scale 16 | train_wall 685 | gb_free 9.8 | wall 45316
2022-03-12 22:57:06 | INFO | fairseq.trainer | begin training epoch 55
2022-03-12 22:57:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 22:57:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 22:59:55 | INFO | train_inner | epoch 055:     85 / 392 loss=5.03, ppl=32.67, wps=28984.4, ups=0.45, wpb=65029.1, bsz=127, num_updates=21100, lr=0.0002177, gnorm=0.637, loss_scale=16, train_wall=176, gb_free=9.8, wall=45485
2022-03-12 23:01:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-12 23:03:15 | INFO | train_inner | epoch 055:    186 / 392 loss=5.047, ppl=33.06, wps=32714.7, ups=0.5, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.632, loss_scale=16, train_wall=177, gb_free=9.8, wall=45685
2022-03-12 23:04:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 23:06:35 | INFO | train_inner | epoch 055:    287 / 392 loss=5.066, ppl=33.5, wps=32676.3, ups=0.5, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.634, loss_scale=8, train_wall=178, gb_free=9.8, wall=45886
2022-03-12 23:10:02 | INFO | train_inner | epoch 055:    387 / 392 loss=5.077, ppl=33.76, wps=31773, ups=0.48, wpb=65532.7, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.637, loss_scale=16, train_wall=183, gb_free=9.8, wall=46092
2022-03-12 23:10:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 23:10:34 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 5.543 | ppl 46.64 | wps 54225.9 | wpb 511.9 | bsz 1 | num_updates 21405 | best_loss 5.539
2022-03-12 23:10:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 21405 updates
2022-03-12 23:10:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 23:10:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 23:10:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt (epoch 55 @ 21405 updates, score 5.543) (writing took 1.2467246379819699 seconds)
2022-03-12 23:10:36 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-12 23:10:36 | INFO | train | epoch 055 | loss 5.055 | ppl 33.25 | wps 31401.8 | ups 0.48 | wpb 65404.8 | bsz 127.7 | num_updates 21405 | lr 0.000216144 | gnorm 0.635 | loss_scale 16 | train_wall 695 | gb_free 9.8 | wall 46126
2022-03-12 23:10:36 | INFO | fairseq.trainer | begin training epoch 56
2022-03-12 23:10:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 23:11:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 23:13:58 | INFO | train_inner | epoch 056:     96 / 392 loss=5.011, ppl=32.24, wps=27527.2, ups=0.42, wpb=65029.1, bsz=127, num_updates=21500, lr=0.000215666, gnorm=0.645, loss_scale=8, train_wall=188, gb_free=9.8, wall=46328
2022-03-12 23:17:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 23:17:31 | INFO | train_inner | epoch 056:    197 / 392 loss=5.043, ppl=32.98, wps=30811.4, ups=0.47, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.635, loss_scale=8, train_wall=189, gb_free=9.8, wall=46541
2022-03-12 23:21:00 | INFO | train_inner | epoch 056:    297 / 392 loss=5.067, ppl=33.52, wps=31239.7, ups=0.48, wpb=65532.7, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.637, loss_scale=8, train_wall=186, gb_free=9.8, wall=46751
2022-03-12 23:22:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 23:24:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 23:24:42 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 5.55 | ppl 46.86 | wps 54569.3 | wpb 511.9 | bsz 1 | num_updates 21794 | best_loss 5.539
2022-03-12 23:24:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 21794 updates
2022-03-12 23:24:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 23:24:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 23:24:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt (epoch 56 @ 21794 updates, score 5.55) (writing took 1.2164424729999155 seconds)
2022-03-12 23:24:43 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-12 23:24:43 | INFO | train | epoch 056 | loss 5.047 | ppl 33.06 | wps 30019.1 | ups 0.46 | wpb 65404.8 | bsz 127.7 | num_updates 21794 | lr 0.000214206 | gnorm 0.639 | loss_scale 8 | train_wall 731 | gb_free 9.8 | wall 46973
2022-03-12 23:24:43 | INFO | fairseq.trainer | begin training epoch 57
2022-03-12 23:24:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-12 23:24:56 | INFO | train_inner | epoch 057:      6 / 392 loss=5.069, ppl=33.57, wps=27607.3, ups=0.42, wpb=65029.1, bsz=127, num_updates=21800, lr=0.000214176, gnorm=0.645, loss_scale=8, train_wall=187, gb_free=9.8, wall=46986
2022-03-12 23:28:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 23:28:28 | INFO | train_inner | epoch 057:    107 / 392 loss=5.003, ppl=32.08, wps=30861.3, ups=0.47, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.643, loss_scale=8, train_wall=189, gb_free=9.8, wall=47198
2022-03-12 23:31:57 | INFO | train_inner | epoch 057:    207 / 392 loss=5.038, ppl=32.85, wps=31368.8, ups=0.48, wpb=65536, bsz=128, num_updates=22000, lr=0.000213201, gnorm=0.64, loss_scale=8, train_wall=186, gb_free=9.8, wall=47407
2022-03-12 23:33:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-12 23:35:29 | INFO | train_inner | epoch 057:    308 / 392 loss=5.065, ppl=33.47, wps=30918.7, ups=0.47, wpb=65532.7, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.644, loss_scale=8, train_wall=188, gb_free=9.8, wall=47619
2022-03-12 23:38:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-12 23:38:48 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 5.544 | ppl 46.66 | wps 54489.4 | wpb 511.9 | bsz 1 | num_updates 22184 | best_loss 5.539
2022-03-12 23:38:48 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-12 23:38:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 22184 updates
2022-03-12 23:38:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 23:38:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt
2022-03-12 23:38:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/de_cross_entropy_dropout_0.2_#1/checkpoint_last.pt (epoch 57 @ 22184 updates, score 5.544) (writing took 1.2327358180191368 seconds)
2022-03-12 23:38:49 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-12 23:38:49 | INFO | train | epoch 057 | loss 5.041 | ppl 32.91 | wps 30167.2 | ups 0.46 | wpb 65405.2 | bsz 127.7 | num_updates 22184 | lr 0.000212315 | gnorm 0.642 | loss_scale 16 | train_wall 729 | gb_free 9.8 | wall 47819
2022-03-12 23:38:49 | INFO | fairseq_cli.train | done training in 47818.4 seconds
