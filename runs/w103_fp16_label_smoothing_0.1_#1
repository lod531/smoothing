Sender: LSF System <lsfadmin@eu-g3-076>
Subject: Job 204577716: <w103_fp16_label_smoothing_0.1_#1> in cluster <euler> Exited

Job <w103_fp16_label_smoothing_0.1_#1> was submitted from host <eu-login-08> by user <andriusb> in cluster <euler> at Fri Feb 11 08:44:30 2022
Job was executed on host(s) <eu-g3-076>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Fri Feb 11 09:26:50 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 11 09:26:50 2022
Terminated at Fri Feb 11 09:27:25 2022
Results reported at Fri Feb 11 09:27:25 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 13521651 --fp16 --max-update 5000000
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.84 sec.
    Max Memory :                                 4097 MB
    Average Memory :                             310.00 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               15903.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   35 sec.
    Turnaround time :                            2575 sec.

The output (if any) follows:

2022-02-11 09:27:00 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 13521651, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 5000000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.5, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 13521651, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.5, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-11 09:27:02 | INFO | fairseq.tasks.language_modeling | dictionary: 623952 types
2022-02-11 09:27:08 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(623952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=623952, bias=False)
  )
)
2022-02-11 09:27:08 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-11 09:27:08 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-11 09:27:08 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-02-11 09:27:08 | INFO | fairseq_cli.train | num. shared model params: 338,377,728 (num. trained: 338,377,728)
2022-02-11 09:27:08 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-11 09:27:08 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-full/valid
2022-02-11 09:27:17 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-11 09:27:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 09:27:17 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-11 09:27:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 09:27:17 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-11 09:27:17 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None
2022-02-11 09:27:17 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-02-11 09:27:17 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-02-11 09:27:17 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-11 09:27:20 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103-raw-full/train
2022-02-11 09:27:20 | INFO | fairseq.trainer | begin training epoch 1
2022-02-11 09:27:20 | INFO | fairseq_cli.train | Start iterating over samples
> /cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py(50)label_smoothed_nll_loss()
-> loss = (1.0 - epsilon - eps_i) * nll_loss + eps_i * smooth_loss
(Pdb) 
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 81, in forward
    loss, nll_loss = self.compute_loss(model, net_output, sample, reduce=reduce)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 112, in compute_loss
    loss, nll_loss = label_smoothed_nll_loss(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 50, in label_smoothed_nll_loss
    loss = (1.0 - epsilon - eps_i) * nll_loss + eps_i * smooth_loss
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 50, in label_smoothed_nll_loss
    loss = (1.0 - epsilon - eps_i) * nll_loss + eps_i * smooth_loss
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
Sender: LSF System <lsfadmin@eu-g3-071>
Subject: Job 204594658: <w103_fp16_label_smoothing_0.1_#1> in cluster <euler> Exited

Job <w103_fp16_label_smoothing_0.1_#1> was submitted from host <eu-login-08> by user <andriusb> in cluster <euler> at Fri Feb 11 12:05:14 2022
Job was executed on host(s) <eu-g3-071>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Fri Feb 11 14:05:33 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 11 14:05:33 2022
Terminated at Fri Feb 11 15:15:28 2022
Results reported at Fri Feb 11 15:15:28 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.5 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.5 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 13521651 --fp16 --max-update 5000000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   4177.39 sec.
    Max Memory :                                 3652 MB
    Average Memory :                             3076.76 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               16348.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   4195 sec.
    Turnaround time :                            11414 sec.

The output (if any) follows:

2022-02-11 14:05:44 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 13521651, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 5000000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.5, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 13521651, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.5, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-11 14:05:45 | INFO | fairseq.tasks.language_modeling | dictionary: 623952 types
2022-02-11 14:05:51 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(623952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=623952, bias=False)
  )
)
2022-02-11 14:05:51 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-11 14:05:51 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-11 14:05:51 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-02-11 14:05:51 | INFO | fairseq_cli.train | num. shared model params: 338,377,728 (num. trained: 338,377,728)
2022-02-11 14:05:51 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-11 14:05:51 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-full/valid
2022-02-11 14:05:59 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-11 14:05:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 14:05:59 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-11 14:05:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 14:05:59 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-11 14:05:59 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None
2022-02-11 14:05:59 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-02-11 14:05:59 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-02-11 14:05:59 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-11 14:05:59 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103-raw-full/train
2022-02-11 14:06:00 | INFO | fairseq.trainer | begin training epoch 1
2022-02-11 14:06:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 14:06:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-11 14:06:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 14:06:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 14:06:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 14:16:11 | INFO | train_inner | epoch 001:    104 / 1576 loss=18.551, nll_loss=18.394, ppl=344367, wps=11738.2, ups=0.18, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=2.606, loss_scale=8, train_wall=598, gb_free=8.8, wall=612
2022-02-11 14:25:30 | INFO | train_inner | epoch 001:    204 / 1576 loss=16.271, nll_loss=15.863, ppl=59578.7, wps=11735.2, ups=0.18, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.594, loss_scale=8, train_wall=548, gb_free=8.8, wall=1171
2022-02-11 14:34:48 | INFO | train_inner | epoch 001:    304 / 1576 loss=14.047, nll_loss=13.369, ppl=10582.7, wps=11735.6, ups=0.18, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.234, loss_scale=16, train_wall=548, gb_free=8.8, wall=1729
2022-02-11 14:44:07 | INFO | train_inner | epoch 001:    404 / 1576 loss=12.343, nll_loss=11.388, ppl=2680.34, wps=11736.5, ups=0.18, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.733, loss_scale=16, train_wall=548, gb_free=8.8, wall=2288
2022-02-11 14:53:25 | INFO | train_inner | epoch 001:    504 / 1576 loss=11.725, nll_loss=10.611, ppl=1563.6, wps=11735.6, ups=0.18, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.459, loss_scale=16, train_wall=548, gb_free=8.8, wall=2846
2022-02-11 15:02:43 | INFO | train_inner | epoch 001:    604 / 1576 loss=11.511, nll_loss=10.339, ppl=1295.39, wps=11738.4, ups=0.18, wpb=65532.3, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.422, loss_scale=32, train_wall=548, gb_free=8.8, wall=3404
2022-02-11 15:12:02 | INFO | train_inner | epoch 001:    704 / 1576 loss=11.32, nll_loss=10.12, ppl=1113.03, wps=11734, ups=0.18, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.429, loss_scale=32, train_wall=548, gb_free=8.8, wall=3963
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 79, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 368, in forward
    x, attn = self.self_attn(
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/multihead_attention.py", line 190, in forward
    k_proj_weight=self.k_proj.weight,
  File "/cluster/home/andriusb/fq/env/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1164, in __getattr__
    def __getattr__(self, name: str) -> Union[Tensor, 'Module']:
KeyboardInterrupt
Sender: LSF System <lsfadmin@eu-g3-071>
Subject: Job 204616323: <w103_fp16_label_smoothing_0.1_#1> in cluster <euler> Done

Job <w103_fp16_label_smoothing_0.1_#1> was submitted from host <eu-login-08> by user <andriusb> in cluster <euler> at Fri Feb 11 16:38:37 2022
Job was executed on host(s) <eu-g3-071>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Fri Feb 11 17:30:12 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Fri Feb 11 17:30:12 2022
Terminated at Mon Feb 14 23:03:46 2022
Results reported at Mon Feb 14 23:03:46 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-full --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 1024 --update-freq 64 --seed 13521651 --fp16 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   279080.97 sec.
    Max Memory :                                 19401 MB
    Average Memory :                             4553.88 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               599.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   279214 sec.
    Turnaround time :                            282309 sec.

The output (if any) follows:

2022-02-11 17:30:21 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 13521651, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [64], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-full', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 13521651, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
2022-02-11 17:30:22 | INFO | fairseq.tasks.language_modeling | dictionary: 623952 types
2022-02-11 17:30:29 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(623952, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=623952, bias=False)
  )
)
2022-02-11 17:30:29 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-02-11 17:30:29 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-02-11 17:30:29 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-02-11 17:30:29 | INFO | fairseq_cli.train | num. shared model params: 338,377,728 (num. trained: 338,377,728)
2022-02-11 17:30:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-02-11 17:30:29 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-full/valid
2022-02-11 17:30:37 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-02-11 17:30:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 17:30:37 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-02-11 17:30:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-02-11 17:30:37 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-02-11 17:30:37 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None
2022-02-11 17:30:37 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-02-11 17:30:37 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint_last.pt
2022-02-11 17:30:37 | INFO | fairseq.trainer | loading train data for epoch 1
2022-02-11 17:30:41 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103-raw-full/train
2022-02-11 17:30:41 | INFO | fairseq.trainer | begin training epoch 1
2022-02-11 17:30:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 17:30:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-02-11 17:31:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 17:31:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 17:31:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-11 17:32:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-11 17:40:53 | INFO | train_inner | epoch 001:    105 / 1576 loss=18.223, nll_loss=18.027, ppl=267046, wps=11685.2, ups=0.18, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.176, loss_scale=4, train_wall=598, gb_free=8.8, wall=616
2022-02-11 17:50:09 | INFO | train_inner | epoch 001:    205 / 1576 loss=15.898, nll_loss=15.448, ppl=44700.2, wps=11794.1, ups=0.18, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.613, loss_scale=4, train_wall=545, gb_free=8.8, wall=1171
2022-02-11 17:59:24 | INFO | train_inner | epoch 001:    305 / 1576 loss=13.711, nll_loss=12.992, ppl=8147.05, wps=11801.8, ups=0.18, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.146, loss_scale=8, train_wall=545, gb_free=8.8, wall=1727
2022-02-11 18:08:39 | INFO | train_inner | epoch 001:    405 / 1576 loss=12.071, nll_loss=11.085, ppl=2171.59, wps=11807.9, ups=0.18, wpb=65536, bsz=128, num_updates=400, lr=5.009e-05, gnorm=0.654, loss_scale=8, train_wall=544, gb_free=8.8, wall=2282
2022-02-11 18:17:54 | INFO | train_inner | epoch 001:    505 / 1576 loss=11.437, nll_loss=10.292, ppl=1253.56, wps=11806.1, ups=0.18, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.472, loss_scale=8, train_wall=545, gb_free=8.8, wall=2837
2022-02-11 18:27:10 | INFO | train_inner | epoch 001:    605 / 1576 loss=11.119, nll_loss=9.907, ppl=960.18, wps=11800, ups=0.18, wpb=65532.3, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.527, loss_scale=16, train_wall=545, gb_free=8.8, wall=3392
2022-02-11 18:36:25 | INFO | train_inner | epoch 001:    705 / 1576 loss=10.866, nll_loss=9.617, ppl=785.4, wps=11797.1, ups=0.18, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.587, loss_scale=16, train_wall=545, gb_free=8.8, wall=3948
2022-02-11 18:45:41 | INFO | train_inner | epoch 001:    805 / 1576 loss=10.655, nll_loss=9.377, ppl=664.83, wps=11799.6, ups=0.18, wpb=65536, bsz=128, num_updates=800, lr=0.00010008, gnorm=0.679, loss_scale=32, train_wall=545, gb_free=8.8, wall=4503
2022-02-11 18:54:56 | INFO | train_inner | epoch 001:    905 / 1576 loss=10.464, nll_loss=9.161, ppl=572.55, wps=11792.3, ups=0.18, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.751, loss_scale=32, train_wall=545, gb_free=8.8, wall=5059
2022-02-11 19:04:12 | INFO | train_inner | epoch 001:   1005 / 1576 loss=10.27, nll_loss=8.942, ppl=491.71, wps=11792.1, ups=0.18, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.802, loss_scale=32, train_wall=545, gb_free=8.8, wall=5615
2022-02-11 19:08:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 19:13:33 | INFO | train_inner | epoch 001:   1106 / 1576 loss=10.118, nll_loss=8.771, ppl=436.81, wps=11678.2, ups=0.18, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.79, loss_scale=32, train_wall=551, gb_free=8.8, wall=6176
2022-02-11 19:22:49 | INFO | train_inner | epoch 001:   1206 / 1576 loss=9.974, nll_loss=8.608, ppl=390.26, wps=11786.7, ups=0.18, wpb=65536, bsz=128, num_updates=1200, lr=0.00015007, gnorm=0.854, loss_scale=32, train_wall=545, gb_free=8.8, wall=6732
2022-02-11 19:32:05 | INFO | train_inner | epoch 001:   1306 / 1576 loss=9.841, nll_loss=8.458, ppl=351.77, wps=11792.7, ups=0.18, wpb=65536, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.865, loss_scale=32, train_wall=545, gb_free=8.8, wall=7288
2022-02-11 19:34:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 19:41:27 | INFO | train_inner | epoch 001:   1407 / 1576 loss=9.725, nll_loss=8.327, ppl=321.22, wps=11670.4, ups=0.18, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.854, loss_scale=32, train_wall=551, gb_free=8.8, wall=7849
2022-02-11 19:50:43 | INFO | train_inner | epoch 001:   1507 / 1576 loss=9.61, nll_loss=8.197, ppl=293.39, wps=11778.3, ups=0.18, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.842, loss_scale=32, train_wall=546, gb_free=8.8, wall=8406
2022-02-11 19:57:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 19:57:09 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.33 | nll_loss 7.864 | ppl 232.92 | wps 32462.8 | wpb 1021.8 | bsz 2 | num_updates 1569
2022-02-11 19:57:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1569 updates
2022-02-11 19:57:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint1.pt
2022-02-11 19:57:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint1.pt
2022-02-11 19:57:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint1.pt (epoch 1 @ 1569 updates, score 9.33) (writing took 27.807995917275548 seconds)
2022-02-11 19:57:36 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-02-11 19:57:36 | INFO | train | epoch 001 | loss 11.509 | nll_loss 10.377 | ppl 1330.1 | wps 11726.2 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 1569 | lr 0.000196186 | gnorm 0.97 | loss_scale 32 | train_wall 8612 | gb_free 8.8 | wall 8819
2022-02-11 19:57:37 | INFO | fairseq.trainer | begin training epoch 2
2022-02-11 19:57:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 20:00:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 20:00:34 | INFO | train_inner | epoch 002:     32 / 1576 loss=9.5, nll_loss=8.073, ppl=269.37, wps=10988.3, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=1600, lr=0.00020006, gnorm=0.866, loss_scale=32, train_wall=546, gb_free=8.8, wall=8997
2022-02-11 20:09:50 | INFO | train_inner | epoch 002:    132 / 1576 loss=9.374, nll_loss=7.93, ppl=243.88, wps=11789.7, ups=0.18, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.818, loss_scale=32, train_wall=545, gb_free=8.8, wall=9553
2022-02-11 20:19:06 | INFO | train_inner | epoch 002:    232 / 1576 loss=9.283, nll_loss=7.828, ppl=227.23, wps=11787.9, ups=0.18, wpb=65532.3, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.831, loss_scale=32, train_wall=545, gb_free=8.8, wall=10109
2022-02-11 20:24:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 20:28:27 | INFO | train_inner | epoch 002:    333 / 1576 loss=9.195, nll_loss=7.728, ppl=211.98, wps=11678.6, ups=0.18, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.826, loss_scale=32, train_wall=550, gb_free=8.8, wall=10670
2022-02-11 20:37:43 | INFO | train_inner | epoch 002:    433 / 1576 loss=9.107, nll_loss=7.629, ppl=197.96, wps=11792.1, ups=0.18, wpb=65536, bsz=128, num_updates=2000, lr=0.00025005, gnorm=0.791, loss_scale=32, train_wall=545, gb_free=8.8, wall=11226
2022-02-11 20:46:59 | INFO | train_inner | epoch 002:    533 / 1576 loss=9.037, nll_loss=7.55, ppl=187.45, wps=11789.6, ups=0.18, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.796, loss_scale=32, train_wall=545, gb_free=8.8, wall=11781
2022-02-11 20:50:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 20:56:20 | INFO | train_inner | epoch 002:    634 / 1576 loss=8.968, nll_loss=7.472, ppl=177.54, wps=11673.5, ups=0.18, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.757, loss_scale=32, train_wall=551, gb_free=8.8, wall=12343
2022-02-11 21:05:36 | INFO | train_inner | epoch 002:    734 / 1576 loss=8.887, nll_loss=7.381, ppl=166.68, wps=11796.3, ups=0.18, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.772, loss_scale=32, train_wall=545, gb_free=8.8, wall=12898
2022-02-11 21:14:52 | INFO | train_inner | epoch 002:    834 / 1576 loss=8.811, nll_loss=7.295, ppl=157.02, wps=11789.9, ups=0.18, wpb=65536, bsz=128, num_updates=2400, lr=0.00030004, gnorm=0.731, loss_scale=64, train_wall=545, gb_free=8.8, wall=13454
2022-02-11 21:15:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-11 21:20:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 21:24:18 | INFO | train_inner | epoch 002:    936 / 1576 loss=8.747, nll_loss=7.222, ppl=149.3, wps=11566, ups=0.18, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.742, loss_scale=16, train_wall=556, gb_free=8.8, wall=14021
2022-02-11 21:33:34 | INFO | train_inner | epoch 002:   1036 / 1576 loss=8.689, nll_loss=7.157, ppl=142.7, wps=11797.3, ups=0.18, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.719, loss_scale=16, train_wall=545, gb_free=8.8, wall=14576
2022-02-11 21:42:50 | INFO | train_inner | epoch 002:   1136 / 1576 loss=8.624, nll_loss=7.084, ppl=135.66, wps=11795.6, ups=0.18, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.708, loss_scale=16, train_wall=545, gb_free=8.8, wall=15132
2022-02-11 21:52:06 | INFO | train_inner | epoch 002:   1236 / 1576 loss=8.568, nll_loss=7.021, ppl=129.89, wps=11780.4, ups=0.18, wpb=65536, bsz=128, num_updates=2800, lr=0.00035003, gnorm=0.71, loss_scale=32, train_wall=546, gb_free=8.8, wall=15688
2022-02-11 22:00:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 22:01:27 | INFO | train_inner | epoch 002:   1337 / 1576 loss=8.517, nll_loss=6.963, ppl=124.79, wps=11673.8, ups=0.18, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.704, loss_scale=16, train_wall=551, gb_free=8.8, wall=16250
2022-02-11 22:10:42 | INFO | train_inner | epoch 002:   1437 / 1576 loss=8.458, nll_loss=6.896, ppl=119.11, wps=11803.2, ups=0.18, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.682, loss_scale=16, train_wall=545, gb_free=8.8, wall=16805
2022-02-11 22:19:58 | INFO | train_inner | epoch 002:   1537 / 1576 loss=8.407, nll_loss=6.839, ppl=114.52, wps=11801, ups=0.18, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.692, loss_scale=16, train_wall=545, gb_free=8.8, wall=17360
2022-02-11 22:23:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-11 22:23:36 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 8.211 | nll_loss 6.597 | ppl 96.79 | wps 32415.1 | wpb 1021.8 | bsz 2 | num_updates 3139 | best_loss 8.211
2022-02-11 22:23:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3139 updates
2022-02-11 22:23:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint2.pt
2022-02-11 22:23:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint2.pt
2022-02-11 22:24:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint2.pt (epoch 2 @ 3139 updates, score 8.211) (writing took 28.209414175711572 seconds)
2022-02-11 22:24:05 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-02-11 22:24:05 | INFO | train | epoch 002 | loss 8.845 | nll_loss 7.333 | ppl 161.28 | wps 11701.4 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 3139 | lr 0.000392397 | gnorm 0.751 | loss_scale 16 | train_wall 8587 | gb_free 8.8 | wall 17607
2022-02-11 22:24:05 | INFO | fairseq.trainer | begin training epoch 3
2022-02-11 22:24:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-11 22:25:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 22:29:49 | INFO | train_inner | epoch 003:     62 / 1576 loss=8.338, nll_loss=6.761, ppl=108.46, wps=10989.3, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=3200, lr=0.00040002, gnorm=0.66, loss_scale=16, train_wall=545, gb_free=8.8, wall=17951
2022-02-11 22:39:05 | INFO | train_inner | epoch 003:    162 / 1576 loss=8.287, nll_loss=6.703, ppl=104.22, wps=11795.5, ups=0.18, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.667, loss_scale=16, train_wall=545, gb_free=8.8, wall=18507
2022-02-11 22:48:20 | INFO | train_inner | epoch 003:    262 / 1576 loss=8.245, nll_loss=6.656, ppl=100.84, wps=11809.1, ups=0.18, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.664, loss_scale=16, train_wall=544, gb_free=8.8, wall=19062
2022-02-11 22:57:35 | INFO | train_inner | epoch 003:    362 / 1576 loss=8.201, nll_loss=6.607, ppl=97.5, wps=11795, ups=0.18, wpb=65532.3, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.656, loss_scale=32, train_wall=545, gb_free=8.8, wall=19618
2022-02-11 23:06:51 | INFO | train_inner | epoch 003:    462 / 1576 loss=8.182, nll_loss=6.585, ppl=96.01, wps=11793.2, ups=0.18, wpb=65536, bsz=128, num_updates=3600, lr=0.00045001, gnorm=0.651, loss_scale=32, train_wall=545, gb_free=8.8, wall=20173
2022-02-11 23:11:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 23:16:12 | INFO | train_inner | epoch 003:    563 / 1576 loss=8.147, nll_loss=6.546, ppl=93.46, wps=11685.3, ups=0.18, wpb=65536, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.646, loss_scale=16, train_wall=550, gb_free=8.8, wall=20734
2022-02-11 23:25:27 | INFO | train_inner | epoch 003:    663 / 1576 loss=8.114, nll_loss=6.508, ppl=91.02, wps=11804.7, ups=0.18, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.633, loss_scale=16, train_wall=545, gb_free=8.8, wall=21289
2022-02-11 23:34:42 | INFO | train_inner | epoch 003:    763 / 1576 loss=8.076, nll_loss=6.466, ppl=88.41, wps=11800.7, ups=0.18, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.606, loss_scale=16, train_wall=545, gb_free=8.8, wall=21845
2022-02-11 23:43:58 | INFO | train_inner | epoch 003:    863 / 1576 loss=8.052, nll_loss=6.439, ppl=86.77, wps=11796.5, ups=0.18, wpb=65536, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.62, loss_scale=32, train_wall=545, gb_free=8.8, wall=22400
2022-02-11 23:51:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-11 23:53:19 | INFO | train_inner | epoch 003:    964 / 1576 loss=8.014, nll_loss=6.396, ppl=84.24, wps=11681.2, ups=0.18, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.594, loss_scale=16, train_wall=550, gb_free=8.8, wall=22961
2022-02-12 00:02:34 | INFO | train_inner | epoch 003:   1064 / 1576 loss=7.987, nll_loss=6.367, ppl=82.52, wps=11801.1, ups=0.18, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.597, loss_scale=16, train_wall=545, gb_free=8.8, wall=23517
2022-02-12 00:11:49 | INFO | train_inner | epoch 003:   1164 / 1576 loss=7.956, nll_loss=6.332, ppl=80.54, wps=11805.6, ups=0.18, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.562, loss_scale=16, train_wall=545, gb_free=8.8, wall=24072
2022-02-12 00:21:05 | INFO | train_inner | epoch 003:   1264 / 1576 loss=7.927, nll_loss=6.299, ppl=78.75, wps=11798.9, ups=0.18, wpb=65536, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.563, loss_scale=32, train_wall=545, gb_free=8.8, wall=24627
2022-02-12 00:30:20 | INFO | train_inner | epoch 003:   1364 / 1576 loss=7.9, nll_loss=6.27, ppl=77.15, wps=11802.2, ups=0.18, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.543, loss_scale=32, train_wall=545, gb_free=8.8, wall=25182
2022-02-12 00:38:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-02-12 00:39:41 | INFO | train_inner | epoch 003:   1465 / 1576 loss=7.873, nll_loss=6.238, ppl=75.5, wps=11678.5, ups=0.18, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.54, loss_scale=32, train_wall=551, gb_free=8.8, wall=25744
2022-02-12 00:48:57 | INFO | train_inner | epoch 003:   1565 / 1576 loss=7.855, nll_loss=6.218, ppl=74.45, wps=11793.4, ups=0.18, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.536, loss_scale=32, train_wall=545, gb_free=8.8, wall=26299
2022-02-12 00:49:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 00:49:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 00:50:00 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.699 | nll_loss 6.02 | ppl 64.91 | wps 32510.6 | wpb 1021.8 | bsz 2 | num_updates 4710 | best_loss 7.699
2022-02-12 00:50:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4710 updates
2022-02-12 00:50:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint3.pt
2022-02-12 00:50:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint3.pt
2022-02-12 00:50:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint3.pt (epoch 3 @ 4710 updates, score 7.699) (writing took 55.3538909656927 seconds)
2022-02-12 00:50:55 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-02-12 00:50:55 | INFO | train | epoch 003 | loss 8.063 | nll_loss 6.452 | ppl 87.56 | wps 11679 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 4710 | lr 0.000460776 | gnorm 0.609 | loss_scale 16 | train_wall 8582 | gb_free 8.8 | wall 26418
2022-02-12 00:50:55 | INFO | fairseq.trainer | begin training epoch 4
2022-02-12 00:50:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 00:59:15 | INFO | train_inner | epoch 004:     90 / 1576 loss=7.753, nll_loss=6.103, ppl=68.74, wps=10510.1, ups=0.16, wpb=64962.6, bsz=126.9, num_updates=4800, lr=0.000456435, gnorm=0.54, loss_scale=16, train_wall=545, gb_free=8.8, wall=26917
2022-02-12 01:08:30 | INFO | train_inner | epoch 004:    190 / 1576 loss=7.732, nll_loss=6.079, ppl=67.62, wps=11799.9, ups=0.18, wpb=65536, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.54, loss_scale=16, train_wall=545, gb_free=8.8, wall=27473
2022-02-12 01:17:46 | INFO | train_inner | epoch 004:    290 / 1576 loss=7.722, nll_loss=6.069, ppl=67.14, wps=11799, ups=0.18, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.503, loss_scale=32, train_wall=545, gb_free=8.8, wall=28028
2022-02-12 01:27:02 | INFO | train_inner | epoch 004:    390 / 1576 loss=7.714, nll_loss=6.059, ppl=66.69, wps=11790.8, ups=0.18, wpb=65536, bsz=128, num_updates=5100, lr=0.000442807, gnorm=0.535, loss_scale=32, train_wall=545, gb_free=8.8, wall=28584
2022-02-12 01:36:17 | INFO | train_inner | epoch 004:    490 / 1576 loss=7.697, nll_loss=6.041, ppl=65.86, wps=11794.9, ups=0.18, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.505, loss_scale=32, train_wall=545, gb_free=8.8, wall=29140
2022-02-12 01:37:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 01:45:38 | INFO | train_inner | epoch 004:    591 / 1576 loss=7.695, nll_loss=6.039, ppl=65.75, wps=11691.5, ups=0.18, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.513, loss_scale=16, train_wall=550, gb_free=8.8, wall=29700
2022-02-12 01:54:53 | INFO | train_inner | epoch 004:    691 / 1576 loss=7.678, nll_loss=6.02, ppl=64.88, wps=11802.6, ups=0.18, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.509, loss_scale=16, train_wall=545, gb_free=8.8, wall=30256
2022-02-12 02:01:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 02:04:14 | INFO | train_inner | epoch 004:    792 / 1576 loss=7.667, nll_loss=6.008, ppl=64.36, wps=11686.4, ups=0.18, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.51, loss_scale=16, train_wall=550, gb_free=8.8, wall=30816
2022-02-12 02:13:29 | INFO | train_inner | epoch 004:    892 / 1576 loss=7.659, nll_loss=6, ppl=63.98, wps=11804.6, ups=0.18, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.489, loss_scale=16, train_wall=545, gb_free=8.8, wall=31372
2022-02-12 02:22:44 | INFO | train_inner | epoch 004:    992 / 1576 loss=7.655, nll_loss=5.994, ppl=63.75, wps=11798.5, ups=0.18, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.502, loss_scale=16, train_wall=545, gb_free=8.8, wall=31927
2022-02-12 02:27:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 02:32:05 | INFO | train_inner | epoch 004:   1093 / 1576 loss=7.637, nll_loss=5.975, ppl=62.9, wps=11684, ups=0.18, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.483, loss_scale=16, train_wall=550, gb_free=8.8, wall=32488
2022-02-12 02:41:21 | INFO | train_inner | epoch 004:   1193 / 1576 loss=7.627, nll_loss=5.963, ppl=62.39, wps=11795.8, ups=0.18, wpb=65536, bsz=128, num_updates=5900, lr=0.000411693, gnorm=0.51, loss_scale=16, train_wall=545, gb_free=8.8, wall=33043
2022-02-12 02:50:36 | INFO | train_inner | epoch 004:   1293 / 1576 loss=7.613, nll_loss=5.949, ppl=61.76, wps=11806.6, ups=0.18, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.493, loss_scale=16, train_wall=545, gb_free=8.8, wall=33599
2022-02-12 02:51:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 02:59:57 | INFO | train_inner | epoch 004:   1394 / 1576 loss=7.597, nll_loss=5.931, ppl=61, wps=11690.6, ups=0.18, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.486, loss_scale=16, train_wall=550, gb_free=8.8, wall=34159
2022-02-12 03:09:12 | INFO | train_inner | epoch 004:   1494 / 1576 loss=7.583, nll_loss=5.914, ppl=60.3, wps=11805.7, ups=0.18, wpb=65532.3, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.473, loss_scale=16, train_wall=545, gb_free=8.8, wall=34714
2022-02-12 03:16:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 03:16:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 03:16:49 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 7.456 | nll_loss 5.733 | ppl 53.2 | wps 32483.3 | wpb 1021.8 | bsz 2 | num_updates 6281 | best_loss 7.456
2022-02-12 03:16:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6281 updates
2022-02-12 03:16:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint4.pt
2022-02-12 03:16:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint4.pt
2022-02-12 03:17:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint4.pt (epoch 4 @ 6281 updates, score 7.456) (writing took 27.984641578048468 seconds)
2022-02-12 03:17:17 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-02-12 03:17:17 | INFO | train | epoch 004 | loss 7.663 | nll_loss 6.003 | ppl 64.13 | wps 11717.7 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 6281 | lr 0.000399012 | gnorm 0.504 | loss_scale 16 | train_wall 8580 | gb_free 8.8 | wall 35199
2022-02-12 03:17:17 | INFO | fairseq.trainer | begin training epoch 5
2022-02-12 03:17:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 03:19:02 | INFO | train_inner | epoch 005:     19 / 1576 loss=7.553, nll_loss=5.88, ppl=58.91, wps=11001.9, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=6300, lr=0.00039841, gnorm=0.506, loss_scale=16, train_wall=545, gb_free=8.8, wall=35305
2022-02-12 03:28:18 | INFO | train_inner | epoch 005:    119 / 1576 loss=7.473, nll_loss=5.79, ppl=55.35, wps=11800.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.471, loss_scale=16, train_wall=545, gb_free=8.8, wall=35860
2022-02-12 03:37:33 | INFO | train_inner | epoch 005:    219 / 1576 loss=7.477, nll_loss=5.795, ppl=55.51, wps=11794.5, ups=0.18, wpb=65536, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.493, loss_scale=16, train_wall=545, gb_free=8.8, wall=36416
2022-02-12 03:40:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 03:46:54 | INFO | train_inner | epoch 005:    320 / 1576 loss=7.473, nll_loss=5.791, ppl=55.36, wps=11689.7, ups=0.18, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.473, loss_scale=16, train_wall=550, gb_free=8.8, wall=36976
2022-02-12 03:56:09 | INFO | train_inner | epoch 005:    420 / 1576 loss=7.47, nll_loss=5.788, ppl=55.24, wps=11808.9, ups=0.18, wpb=65536, bsz=128, num_updates=6700, lr=0.000386334, gnorm=0.483, loss_scale=16, train_wall=544, gb_free=8.8, wall=37531
2022-02-12 04:04:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 04:05:29 | INFO | train_inner | epoch 005:    521 / 1576 loss=7.461, nll_loss=5.776, ppl=54.81, wps=11693.8, ups=0.18, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.477, loss_scale=16, train_wall=550, gb_free=8.8, wall=38092
2022-02-12 04:14:44 | INFO | train_inner | epoch 005:    621 / 1576 loss=7.469, nll_loss=5.787, ppl=55.2, wps=11808.6, ups=0.18, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.488, loss_scale=16, train_wall=544, gb_free=8.8, wall=38647
2022-02-12 04:23:59 | INFO | train_inner | epoch 005:    721 / 1576 loss=7.468, nll_loss=5.785, ppl=55.16, wps=11806.2, ups=0.18, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.477, loss_scale=16, train_wall=545, gb_free=8.8, wall=39202
2022-02-12 04:29:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 04:33:20 | INFO | train_inner | epoch 005:    822 / 1576 loss=7.457, nll_loss=5.773, ppl=54.69, wps=11692.4, ups=0.18, wpb=65536, bsz=128, num_updates=7100, lr=0.000375293, gnorm=0.488, loss_scale=16, train_wall=550, gb_free=8.8, wall=39762
2022-02-12 04:37:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 04:42:40 | INFO | train_inner | epoch 005:    923 / 1576 loss=7.451, nll_loss=5.766, ppl=54.41, wps=11698.7, ups=0.18, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.499, loss_scale=8, train_wall=550, gb_free=8.8, wall=40323
2022-02-12 04:51:55 | INFO | train_inner | epoch 005:   1023 / 1576 loss=7.444, nll_loss=5.758, ppl=54.11, wps=11813, ups=0.18, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.473, loss_scale=8, train_wall=544, gb_free=8.8, wall=40877
2022-02-12 05:01:10 | INFO | train_inner | epoch 005:   1123 / 1576 loss=7.445, nll_loss=5.759, ppl=54.16, wps=11811.1, ups=0.18, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.471, loss_scale=16, train_wall=544, gb_free=8.8, wall=41432
2022-02-12 05:07:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 05:10:30 | INFO | train_inner | epoch 005:   1224 / 1576 loss=7.436, nll_loss=5.75, ppl=53.83, wps=11693.1, ups=0.18, wpb=65536, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.475, loss_scale=8, train_wall=550, gb_free=8.8, wall=41993
2022-02-12 05:19:45 | INFO | train_inner | epoch 005:   1324 / 1576 loss=7.434, nll_loss=5.747, ppl=53.72, wps=11816.3, ups=0.18, wpb=65536, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.462, loss_scale=8, train_wall=544, gb_free=8.8, wall=42547
2022-02-12 05:28:59 | INFO | train_inner | epoch 005:   1424 / 1576 loss=7.42, nll_loss=5.732, ppl=53.13, wps=11815.3, ups=0.18, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.489, loss_scale=8, train_wall=544, gb_free=8.8, wall=43102
2022-02-12 05:38:14 | INFO | train_inner | epoch 005:   1524 / 1576 loss=7.439, nll_loss=5.753, ppl=53.95, wps=11808.7, ups=0.18, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.458, loss_scale=16, train_wall=544, gb_free=8.8, wall=43657
2022-02-12 05:42:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 05:43:05 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 7.325 | nll_loss 5.633 | ppl 49.64 | wps 32545.7 | wpb 1021.8 | bsz 2 | num_updates 7852 | best_loss 7.325
2022-02-12 05:43:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7852 updates
2022-02-12 05:43:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint5.pt
2022-02-12 05:43:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint5.pt
2022-02-12 05:43:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint5.pt (epoch 5 @ 7852 updates, score 7.325) (writing took 28.017137371003628 seconds)
2022-02-12 05:43:33 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-02-12 05:43:33 | INFO | train | epoch 005 | loss 7.453 | nll_loss 5.769 | ppl 54.53 | wps 11724.9 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 7852 | lr 0.00035687 | gnorm 0.48 | loss_scale 16 | train_wall 8575 | gb_free 8.8 | wall 43975
2022-02-12 05:43:33 | INFO | fairseq.trainer | begin training epoch 6
2022-02-12 05:43:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 05:47:59 | INFO | train_inner | epoch 006:     48 / 1576 loss=7.373, nll_loss=5.68, ppl=51.25, wps=11106.5, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=7900, lr=0.000355784, gnorm=0.48, loss_scale=16, train_wall=539, gb_free=8.8, wall=44242
2022-02-12 05:55:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 05:57:20 | INFO | train_inner | epoch 006:    149 / 1576 loss=7.32, nll_loss=5.619, ppl=49.14, wps=11681.9, ups=0.18, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.476, loss_scale=16, train_wall=550, gb_free=8.8, wall=44803
2022-02-12 06:06:36 | INFO | train_inner | epoch 006:    249 / 1576 loss=7.324, nll_loss=5.623, ppl=49.28, wps=11803, ups=0.18, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.495, loss_scale=16, train_wall=545, gb_free=8.8, wall=45358
2022-02-12 06:15:51 | INFO | train_inner | epoch 006:    349 / 1576 loss=7.318, nll_loss=5.617, ppl=49.07, wps=11802.2, ups=0.18, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.475, loss_scale=16, train_wall=545, gb_free=8.8, wall=45913
2022-02-12 06:21:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 06:25:12 | INFO | train_inner | epoch 006:    450 / 1576 loss=7.322, nll_loss=5.621, ppl=49.2, wps=11686.6, ups=0.18, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.482, loss_scale=16, train_wall=550, gb_free=8.8, wall=46474
2022-02-12 06:33:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 06:34:32 | INFO | train_inner | epoch 006:    551 / 1576 loss=7.341, nll_loss=5.642, ppl=49.95, wps=11694.2, ups=0.18, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.472, loss_scale=8, train_wall=550, gb_free=8.8, wall=47035
2022-02-12 06:43:47 | INFO | train_inner | epoch 006:    651 / 1576 loss=7.34, nll_loss=5.641, ppl=49.9, wps=11813.7, ups=0.18, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.497, loss_scale=8, train_wall=544, gb_free=8.8, wall=47589
2022-02-12 06:53:02 | INFO | train_inner | epoch 006:    751 / 1576 loss=7.32, nll_loss=5.62, ppl=49.17, wps=11811.9, ups=0.18, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.448, loss_scale=8, train_wall=544, gb_free=8.8, wall=48144
2022-02-12 07:02:17 | INFO | train_inner | epoch 006:    851 / 1576 loss=7.334, nll_loss=5.635, ppl=49.7, wps=11806.7, ups=0.18, wpb=65532.3, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.479, loss_scale=16, train_wall=544, gb_free=8.8, wall=48699
2022-02-12 07:07:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 07:11:37 | INFO | train_inner | epoch 006:    952 / 1576 loss=7.318, nll_loss=5.617, ppl=49.08, wps=11693.7, ups=0.18, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.477, loss_scale=8, train_wall=550, gb_free=8.8, wall=49260
2022-02-12 07:20:52 | INFO | train_inner | epoch 006:   1052 / 1576 loss=7.328, nll_loss=5.629, ppl=49.5, wps=11810.1, ups=0.18, wpb=65536, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.45, loss_scale=8, train_wall=544, gb_free=8.8, wall=49815
2022-02-12 07:30:06 | INFO | train_inner | epoch 006:   1152 / 1576 loss=7.325, nll_loss=5.625, ppl=49.37, wps=11824.5, ups=0.18, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.483, loss_scale=8, train_wall=544, gb_free=8.8, wall=50369
2022-02-12 07:31:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 07:39:26 | INFO | train_inner | epoch 006:   1253 / 1576 loss=7.316, nll_loss=5.615, ppl=49.02, wps=11698.2, ups=0.18, wpb=65536, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.478, loss_scale=8, train_wall=550, gb_free=8.8, wall=50929
2022-02-12 07:48:41 | INFO | train_inner | epoch 006:   1353 / 1576 loss=7.328, nll_loss=5.629, ppl=49.49, wps=11811.3, ups=0.18, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.48, loss_scale=8, train_wall=544, gb_free=8.8, wall=51484
2022-02-12 07:57:56 | INFO | train_inner | epoch 006:   1453 / 1576 loss=7.315, nll_loss=5.614, ppl=48.99, wps=11809.9, ups=0.18, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.451, loss_scale=16, train_wall=544, gb_free=8.8, wall=52039
2022-02-12 07:59:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 08:07:17 | INFO | train_inner | epoch 006:   1554 / 1576 loss=7.31, nll_loss=5.609, ppl=48.8, wps=11695.6, ups=0.18, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.466, loss_scale=8, train_wall=550, gb_free=8.8, wall=52599
2022-02-12 08:09:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 08:09:21 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.229 | nll_loss 5.504 | ppl 45.39 | wps 32582.6 | wpb 1021.8 | bsz 2 | num_updates 9422 | best_loss 7.229
2022-02-12 08:09:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 9422 updates
2022-02-12 08:09:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint6.pt
2022-02-12 08:09:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint6.pt
2022-02-12 08:09:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint6.pt (epoch 6 @ 9422 updates, score 7.229) (writing took 28.673704688437283 seconds)
2022-02-12 08:09:49 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-02-12 08:09:49 | INFO | train | epoch 006 | loss 7.324 | nll_loss 5.624 | ppl 49.3 | wps 11717.1 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 9422 | lr 0.000325783 | gnorm 0.475 | loss_scale 8 | train_wall 8573 | gb_free 8.8 | wall 52752
2022-02-12 08:09:49 | INFO | fairseq.trainer | begin training epoch 7
2022-02-12 08:09:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 08:17:02 | INFO | train_inner | epoch 007:     78 / 1576 loss=7.236, nll_loss=5.525, ppl=46.05, wps=11094.5, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=9500, lr=0.000324443, gnorm=0.509, loss_scale=8, train_wall=539, gb_free=8.8, wall=53185
2022-02-12 08:26:17 | INFO | train_inner | epoch 007:    178 / 1576 loss=7.223, nll_loss=5.51, ppl=45.57, wps=11809.7, ups=0.18, wpb=65532.3, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.468, loss_scale=16, train_wall=544, gb_free=8.8, wall=53740
2022-02-12 08:32:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 08:35:38 | INFO | train_inner | epoch 007:    279 / 1576 loss=7.224, nll_loss=5.511, ppl=45.59, wps=11691.8, ups=0.18, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.468, loss_scale=8, train_wall=550, gb_free=8.8, wall=54300
2022-02-12 08:44:53 | INFO | train_inner | epoch 007:    379 / 1576 loss=7.227, nll_loss=5.514, ppl=45.69, wps=11809.6, ups=0.18, wpb=65536, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.495, loss_scale=8, train_wall=544, gb_free=8.8, wall=54855
2022-02-12 08:54:08 | INFO | train_inner | epoch 007:    479 / 1576 loss=7.227, nll_loss=5.515, ppl=45.74, wps=11808.3, ups=0.18, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.45, loss_scale=8, train_wall=544, gb_free=8.8, wall=55410
2022-02-12 08:56:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 09:03:28 | INFO | train_inner | epoch 007:    580 / 1576 loss=7.234, nll_loss=5.523, ppl=45.98, wps=11696.5, ups=0.18, wpb=65536, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.466, loss_scale=8, train_wall=550, gb_free=8.8, wall=55970
2022-02-12 09:12:43 | INFO | train_inner | epoch 007:    680 / 1576 loss=7.249, nll_loss=5.539, ppl=46.51, wps=11807.3, ups=0.18, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.479, loss_scale=8, train_wall=544, gb_free=8.8, wall=56525
2022-02-12 09:21:58 | INFO | train_inner | epoch 007:    780 / 1576 loss=7.235, nll_loss=5.524, ppl=46, wps=11811.9, ups=0.18, wpb=65536, bsz=128, num_updates=10200, lr=0.000313112, gnorm=0.475, loss_scale=16, train_wall=544, gb_free=8.8, wall=57080
2022-02-12 09:27:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 09:31:18 | INFO | train_inner | epoch 007:    881 / 1576 loss=7.235, nll_loss=5.524, ppl=46.03, wps=11689.1, ups=0.18, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.453, loss_scale=8, train_wall=550, gb_free=8.8, wall=57641
2022-02-12 09:40:33 | INFO | train_inner | epoch 007:    981 / 1576 loss=7.244, nll_loss=5.534, ppl=46.33, wps=11812.9, ups=0.18, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.474, loss_scale=8, train_wall=544, gb_free=8.8, wall=58196
2022-02-12 09:49:48 | INFO | train_inner | epoch 007:   1081 / 1576 loss=7.236, nll_loss=5.524, ppl=46.03, wps=11820.9, ups=0.18, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.463, loss_scale=8, train_wall=544, gb_free=8.8, wall=58750
2022-02-12 09:59:02 | INFO | train_inner | epoch 007:   1181 / 1576 loss=7.246, nll_loss=5.537, ppl=46.42, wps=11814, ups=0.18, wpb=65536, bsz=128, num_updates=10600, lr=0.000307148, gnorm=0.471, loss_scale=16, train_wall=544, gb_free=8.8, wall=59305
2022-02-12 10:00:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 10:08:23 | INFO | train_inner | epoch 007:   1282 / 1576 loss=7.233, nll_loss=5.522, ppl=45.94, wps=11689.7, ups=0.18, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.5, loss_scale=8, train_wall=550, gb_free=8.8, wall=59865
2022-02-12 10:17:38 | INFO | train_inner | epoch 007:   1382 / 1576 loss=7.238, nll_loss=5.528, ppl=46.13, wps=11805.7, ups=0.18, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.479, loss_scale=8, train_wall=545, gb_free=8.8, wall=60421
2022-02-12 10:24:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 10:26:58 | INFO | train_inner | epoch 007:   1483 / 1576 loss=7.233, nll_loss=5.522, ppl=45.94, wps=11697.9, ups=0.18, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.503, loss_scale=8, train_wall=550, gb_free=8.8, wall=60981
2022-02-12 10:35:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 10:35:36 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.178 | nll_loss 5.443 | ppl 43.52 | wps 32595.7 | wpb 1021.8 | bsz 2 | num_updates 10993 | best_loss 7.178
2022-02-12 10:35:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10993 updates
2022-02-12 10:35:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint7.pt
2022-02-12 10:35:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint7.pt
2022-02-12 10:36:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint7.pt (epoch 7 @ 10993 updates, score 7.178) (writing took 28.207447359338403 seconds)
2022-02-12 10:36:05 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-02-12 10:36:05 | INFO | train | epoch 007 | loss 7.233 | nll_loss 5.522 | ppl 45.95 | wps 11726 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 10993 | lr 0.000301607 | gnorm 0.474 | loss_scale 8 | train_wall 8573 | gb_free 8.8 | wall 61527
2022-02-12 10:36:05 | INFO | fairseq.trainer | begin training epoch 8
2022-02-12 10:36:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 10:36:44 | INFO | train_inner | epoch 008:      7 / 1576 loss=7.225, nll_loss=5.513, ppl=45.67, wps=11098.5, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=11000, lr=0.000301511, gnorm=0.457, loss_scale=8, train_wall=540, gb_free=8.8, wall=61566
2022-02-12 10:45:58 | INFO | train_inner | epoch 008:    107 / 1576 loss=7.125, nll_loss=5.4, ppl=42.22, wps=11817.1, ups=0.18, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.507, loss_scale=8, train_wall=544, gb_free=8.8, wall=62121
2022-02-12 10:55:13 | INFO | train_inner | epoch 008:    207 / 1576 loss=7.144, nll_loss=5.421, ppl=42.84, wps=11812.3, ups=0.18, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.463, loss_scale=16, train_wall=544, gb_free=8.8, wall=62676
2022-02-12 11:04:28 | INFO | train_inner | epoch 008:    307 / 1576 loss=7.157, nll_loss=5.436, ppl=43.28, wps=11801.5, ups=0.18, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.476, loss_scale=16, train_wall=545, gb_free=8.8, wall=63231
2022-02-12 11:06:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 11:13:49 | INFO | train_inner | epoch 008:    408 / 1576 loss=7.158, nll_loss=5.437, ppl=43.32, wps=11694.4, ups=0.18, wpb=65536, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.491, loss_scale=8, train_wall=550, gb_free=8.8, wall=63791
2022-02-12 11:23:03 | INFO | train_inner | epoch 008:    508 / 1576 loss=7.166, nll_loss=5.447, ppl=43.61, wps=11820.4, ups=0.18, wpb=65532.3, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.448, loss_scale=8, train_wall=544, gb_free=8.8, wall=64346
2022-02-12 11:32:18 | INFO | train_inner | epoch 008:    608 / 1576 loss=7.165, nll_loss=5.445, ppl=43.56, wps=11807.3, ups=0.18, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.476, loss_scale=16, train_wall=544, gb_free=8.8, wall=64901
2022-02-12 11:41:33 | INFO | train_inner | epoch 008:    708 / 1576 loss=7.168, nll_loss=5.448, ppl=43.65, wps=11813.3, ups=0.18, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.451, loss_scale=16, train_wall=544, gb_free=8.8, wall=65455
2022-02-12 11:47:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 11:50:53 | INFO | train_inner | epoch 008:    809 / 1576 loss=7.176, nll_loss=5.458, ppl=43.95, wps=11695.7, ups=0.18, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.457, loss_scale=8, train_wall=550, gb_free=8.8, wall=66016
2022-02-12 12:00:08 | INFO | train_inner | epoch 008:    909 / 1576 loss=7.166, nll_loss=5.446, ppl=43.6, wps=11810.8, ups=0.18, wpb=65536, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.48, loss_scale=8, train_wall=544, gb_free=8.8, wall=66571
2022-02-12 12:09:23 | INFO | train_inner | epoch 008:   1009 / 1576 loss=7.166, nll_loss=5.447, ppl=43.62, wps=11812.3, ups=0.18, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.461, loss_scale=8, train_wall=544, gb_free=8.8, wall=67125
2022-02-12 12:13:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 12:18:43 | INFO | train_inner | epoch 008:   1110 / 1576 loss=7.179, nll_loss=5.461, ppl=44.05, wps=11698.6, ups=0.18, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.458, loss_scale=8, train_wall=550, gb_free=8.8, wall=67686
2022-02-12 12:27:58 | INFO | train_inner | epoch 008:   1210 / 1576 loss=7.179, nll_loss=5.461, ppl=44.04, wps=11809.9, ups=0.18, wpb=65536, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.478, loss_scale=8, train_wall=544, gb_free=8.8, wall=68241
2022-02-12 12:37:13 | INFO | train_inner | epoch 008:   1310 / 1576 loss=7.171, nll_loss=5.452, ppl=43.78, wps=11814.3, ups=0.18, wpb=65536, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.468, loss_scale=16, train_wall=544, gb_free=8.8, wall=68795
2022-02-12 12:38:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 12:46:33 | INFO | train_inner | epoch 008:   1411 / 1576 loss=7.173, nll_loss=5.455, ppl=43.86, wps=11697.8, ups=0.18, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.47, loss_scale=8, train_wall=550, gb_free=8.8, wall=69356
2022-02-12 12:55:48 | INFO | train_inner | epoch 008:   1511 / 1576 loss=7.179, nll_loss=5.461, ppl=44.06, wps=11805.4, ups=0.18, wpb=65536, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.467, loss_scale=8, train_wall=545, gb_free=8.8, wall=69911
2022-02-12 13:01:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 13:01:51 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.13 | nll_loss 5.37 | ppl 41.37 | wps 32413 | wpb 1021.8 | bsz 2 | num_updates 12565 | best_loss 7.13
2022-02-12 13:01:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 12565 updates
2022-02-12 13:01:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint8.pt
2022-02-12 13:01:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint8.pt
2022-02-12 13:02:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint8.pt (epoch 8 @ 12565 updates, score 7.13) (writing took 28.195367695763707 seconds)
2022-02-12 13:02:19 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-02-12 13:02:19 | INFO | train | epoch 008 | loss 7.166 | nll_loss 5.446 | ppl 43.59 | wps 11734.7 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 12565 | lr 0.00028211 | gnorm 0.47 | loss_scale 16 | train_wall 8573 | gb_free 8.8 | wall 70302
2022-02-12 13:02:19 | INFO | fairseq.trainer | begin training epoch 9
2022-02-12 13:02:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 13:05:33 | INFO | train_inner | epoch 009:     35 / 1576 loss=7.143, nll_loss=5.421, ppl=42.83, wps=11100.1, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=12600, lr=0.000281718, gnorm=0.475, loss_scale=16, train_wall=540, gb_free=8.8, wall=70496
2022-02-12 13:09:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 13:14:54 | INFO | train_inner | epoch 009:    136 / 1576 loss=7.082, nll_loss=5.351, ppl=40.81, wps=11692.2, ups=0.18, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.496, loss_scale=8, train_wall=550, gb_free=8.8, wall=71056
2022-02-12 13:24:09 | INFO | train_inner | epoch 009:    236 / 1576 loss=7.088, nll_loss=5.359, ppl=41.03, wps=11807.2, ups=0.18, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.453, loss_scale=8, train_wall=544, gb_free=8.8, wall=71612
2022-02-12 13:33:24 | INFO | train_inner | epoch 009:    336 / 1576 loss=7.095, nll_loss=5.366, ppl=41.25, wps=11807.4, ups=0.18, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.471, loss_scale=16, train_wall=544, gb_free=8.8, wall=72167
2022-02-12 13:36:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 13:42:45 | INFO | train_inner | epoch 009:    437 / 1576 loss=7.106, nll_loss=5.379, ppl=41.6, wps=11690.6, ups=0.18, wpb=65536, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.497, loss_scale=8, train_wall=550, gb_free=8.8, wall=72727
2022-02-12 13:51:59 | INFO | train_inner | epoch 009:    537 / 1576 loss=7.109, nll_loss=5.381, ppl=41.68, wps=11812.5, ups=0.18, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.483, loss_scale=8, train_wall=544, gb_free=8.8, wall=73282
2022-02-12 14:01:14 | INFO | train_inner | epoch 009:    637 / 1576 loss=7.114, nll_loss=5.388, ppl=41.87, wps=11813.9, ups=0.18, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.472, loss_scale=16, train_wall=544, gb_free=8.8, wall=73837
2022-02-12 14:10:30 | INFO | train_inner | epoch 009:    737 / 1576 loss=7.107, nll_loss=5.379, ppl=41.62, wps=11800.5, ups=0.18, wpb=65536, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.481, loss_scale=16, train_wall=545, gb_free=8.8, wall=74392
2022-02-12 14:19:45 | INFO | train_inner | epoch 009:    837 / 1576 loss=7.12, nll_loss=5.394, ppl=42.06, wps=11801.9, ups=0.18, wpb=65536, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.46, loss_scale=16, train_wall=545, gb_free=8.8, wall=74947
2022-02-12 14:24:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 14:29:05 | INFO | train_inner | epoch 009:    938 / 1576 loss=7.121, nll_loss=5.396, ppl=42.1, wps=11690.3, ups=0.18, wpb=65536, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.465, loss_scale=16, train_wall=550, gb_free=8.8, wall=75508
2022-02-12 14:38:20 | INFO | train_inner | epoch 009:   1038 / 1576 loss=7.125, nll_loss=5.4, ppl=42.22, wps=11807, ups=0.18, wpb=65532.3, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.47, loss_scale=16, train_wall=544, gb_free=8.8, wall=76063
2022-02-12 14:38:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 14:47:41 | INFO | train_inner | epoch 009:   1139 / 1576 loss=7.116, nll_loss=5.39, ppl=41.94, wps=11695.6, ups=0.18, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.473, loss_scale=8, train_wall=550, gb_free=8.8, wall=76623
2022-02-12 14:56:56 | INFO | train_inner | epoch 009:   1239 / 1576 loss=7.121, nll_loss=5.395, ppl=42.08, wps=11808.1, ups=0.18, wpb=65536, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.499, loss_scale=8, train_wall=544, gb_free=8.8, wall=77178
2022-02-12 15:06:11 | INFO | train_inner | epoch 009:   1339 / 1576 loss=7.126, nll_loss=5.402, ppl=42.27, wps=11805.9, ups=0.18, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.447, loss_scale=16, train_wall=545, gb_free=8.8, wall=77733
2022-02-12 15:15:26 | INFO | train_inner | epoch 009:   1439 / 1576 loss=7.124, nll_loss=5.399, ppl=42.19, wps=11798.5, ups=0.18, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.469, loss_scale=16, train_wall=545, gb_free=8.8, wall=78289
2022-02-12 15:24:42 | INFO | train_inner | epoch 009:   1539 / 1576 loss=7.131, nll_loss=5.408, ppl=42.45, wps=11802.6, ups=0.18, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.45, loss_scale=16, train_wall=545, gb_free=8.8, wall=78844
2022-02-12 15:24:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 15:28:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 15:28:09 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.096 | nll_loss 5.347 | ppl 40.69 | wps 32365.6 | wpb 1021.8 | bsz 2 | num_updates 14136 | best_loss 7.096
2022-02-12 15:28:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 14136 updates
2022-02-12 15:28:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint9.pt
2022-02-12 15:28:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint9.pt
2022-02-12 15:28:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint9.pt (epoch 9 @ 14136 updates, score 7.096) (writing took 30.390893827192485 seconds)
2022-02-12 15:28:39 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-02-12 15:28:39 | INFO | train | epoch 009 | loss 7.112 | nll_loss 5.385 | ppl 41.8 | wps 11719.4 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 14136 | lr 0.000265972 | gnorm 0.475 | loss_scale 8 | train_wall 8576 | gb_free 8.8 | wall 79082
2022-02-12 15:28:39 | INFO | fairseq.trainer | begin training epoch 10
2022-02-12 15:28:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 15:34:35 | INFO | train_inner | epoch 010:     64 / 1576 loss=7.068, nll_loss=5.336, ppl=40.38, wps=10954.7, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=14200, lr=0.000265372, gnorm=0.502, loss_scale=8, train_wall=545, gb_free=8.8, wall=79437
2022-02-12 15:43:49 | INFO | train_inner | epoch 010:    164 / 1576 loss=7.033, nll_loss=5.296, ppl=39.29, wps=11812.4, ups=0.18, wpb=65532.3, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.467, loss_scale=8, train_wall=544, gb_free=8.8, wall=79992
2022-02-12 15:53:04 | INFO | train_inner | epoch 010:    264 / 1576 loss=7.039, nll_loss=5.303, ppl=39.49, wps=11807.5, ups=0.18, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.486, loss_scale=16, train_wall=544, gb_free=8.8, wall=80547
2022-02-12 15:55:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 16:02:25 | INFO | train_inner | epoch 010:    365 / 1576 loss=7.05, nll_loss=5.316, ppl=39.82, wps=11698.6, ups=0.18, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.461, loss_scale=8, train_wall=549, gb_free=8.8, wall=81107
2022-02-12 16:11:39 | INFO | train_inner | epoch 010:    465 / 1576 loss=7.061, nll_loss=5.327, ppl=40.14, wps=11812.8, ups=0.18, wpb=65536, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.496, loss_scale=8, train_wall=544, gb_free=8.8, wall=81662
2022-02-12 16:20:54 | INFO | train_inner | epoch 010:    565 / 1576 loss=7.067, nll_loss=5.335, ppl=40.36, wps=11812.6, ups=0.18, wpb=65536, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.478, loss_scale=16, train_wall=544, gb_free=8.8, wall=82217
2022-02-12 16:30:10 | INFO | train_inner | epoch 010:    665 / 1576 loss=7.06, nll_loss=5.327, ppl=40.14, wps=11796.2, ups=0.18, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.477, loss_scale=16, train_wall=545, gb_free=8.8, wall=82772
2022-02-12 16:38:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 16:39:31 | INFO | train_inner | epoch 010:    766 / 1576 loss=7.069, nll_loss=5.337, ppl=40.41, wps=11686.8, ups=0.18, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.461, loss_scale=8, train_wall=550, gb_free=8.8, wall=83333
2022-02-12 16:48:45 | INFO | train_inner | epoch 010:    866 / 1576 loss=7.074, nll_loss=5.342, ppl=40.56, wps=11812.4, ups=0.18, wpb=65536, bsz=128, num_updates=15000, lr=0.000258199, gnorm=0.468, loss_scale=8, train_wall=544, gb_free=8.8, wall=83888
2022-02-12 16:58:00 | INFO | train_inner | epoch 010:    966 / 1576 loss=7.072, nll_loss=5.341, ppl=40.52, wps=11810.1, ups=0.18, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.472, loss_scale=8, train_wall=544, gb_free=8.8, wall=84443
2022-02-12 17:07:15 | INFO | train_inner | epoch 010:   1066 / 1576 loss=7.086, nll_loss=5.356, ppl=40.94, wps=11811.6, ups=0.18, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.488, loss_scale=16, train_wall=544, gb_free=8.8, wall=84998
2022-02-12 17:16:30 | INFO | train_inner | epoch 010:   1166 / 1576 loss=7.087, nll_loss=5.358, ppl=41, wps=11806, ups=0.18, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.468, loss_scale=16, train_wall=545, gb_free=8.8, wall=85553
2022-02-12 17:25:46 | INFO | train_inner | epoch 010:   1266 / 1576 loss=7.086, nll_loss=5.356, ppl=40.95, wps=11801.3, ups=0.18, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.47, loss_scale=32, train_wall=545, gb_free=8.8, wall=86108
2022-02-12 17:26:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 17:35:06 | INFO | train_inner | epoch 010:   1367 / 1576 loss=7.089, nll_loss=5.36, ppl=41.06, wps=11686.7, ups=0.18, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.451, loss_scale=16, train_wall=550, gb_free=8.8, wall=86669
2022-02-12 17:38:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 17:44:27 | INFO | train_inner | epoch 010:   1468 / 1576 loss=7.095, nll_loss=5.367, ppl=41.26, wps=11692.7, ups=0.18, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.466, loss_scale=8, train_wall=550, gb_free=8.8, wall=87229
2022-02-12 17:53:42 | INFO | train_inner | epoch 010:   1568 / 1576 loss=7.092, nll_loss=5.363, ppl=41.16, wps=11811.5, ups=0.18, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.473, loss_scale=8, train_wall=544, gb_free=8.8, wall=87784
2022-02-12 17:54:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 17:54:28 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.068 | nll_loss 5.324 | ppl 40.07 | wps 32559.2 | wpb 1021.8 | bsz 2 | num_updates 15708 | best_loss 7.068
2022-02-12 17:54:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 15708 updates
2022-02-12 17:54:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint10.pt
2022-02-12 17:54:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint10.pt
2022-02-12 17:54:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint10.pt (epoch 10 @ 15708 updates, score 7.068) (writing took 27.90116013586521 seconds)
2022-02-12 17:54:56 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-02-12 17:54:56 | INFO | train | epoch 010 | loss 7.069 | nll_loss 5.337 | ppl 40.42 | wps 11731.6 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 15708 | lr 0.000252313 | gnorm 0.473 | loss_scale 8 | train_wall 8575 | gb_free 8.8 | wall 87859
2022-02-12 17:54:56 | INFO | fairseq.trainer | begin training epoch 11
2022-02-12 17:54:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 18:03:26 | INFO | train_inner | epoch 011:     92 / 1576 loss=7.002, nll_loss=5.261, ppl=38.35, wps=11109.8, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=15800, lr=0.000251577, gnorm=0.488, loss_scale=16, train_wall=539, gb_free=8.8, wall=88369
2022-02-12 18:12:42 | INFO | train_inner | epoch 011:    192 / 1576 loss=7.003, nll_loss=5.263, ppl=38.39, wps=11806.5, ups=0.18, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.462, loss_scale=16, train_wall=544, gb_free=8.8, wall=88924
2022-02-12 18:21:57 | INFO | train_inner | epoch 011:    292 / 1576 loss=7.013, nll_loss=5.274, ppl=38.69, wps=11806.8, ups=0.18, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.469, loss_scale=16, train_wall=544, gb_free=8.8, wall=89479
2022-02-12 18:26:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 18:31:17 | INFO | train_inner | epoch 011:    393 / 1576 loss=7.011, nll_loss=5.271, ppl=38.62, wps=11687.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=16100, lr=0.000249222, gnorm=0.494, loss_scale=16, train_wall=550, gb_free=8.8, wall=90040
2022-02-12 18:40:33 | INFO | train_inner | epoch 011:    493 / 1576 loss=7.024, nll_loss=5.286, ppl=39, wps=11798.7, ups=0.18, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.464, loss_scale=16, train_wall=545, gb_free=8.8, wall=90595
2022-02-12 18:40:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 18:49:53 | INFO | train_inner | epoch 011:    594 / 1576 loss=7.034, nll_loss=5.298, ppl=39.33, wps=11696.7, ups=0.18, wpb=65536, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.489, loss_scale=8, train_wall=550, gb_free=8.8, wall=91156
2022-02-12 18:59:08 | INFO | train_inner | epoch 011:    694 / 1576 loss=7.036, nll_loss=5.3, ppl=39.39, wps=11810.1, ups=0.18, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.514, loss_scale=8, train_wall=544, gb_free=8.8, wall=91711
2022-02-12 19:08:23 | INFO | train_inner | epoch 011:    794 / 1576 loss=7.029, nll_loss=5.292, ppl=39.17, wps=11808.7, ups=0.18, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=0.469, loss_scale=16, train_wall=544, gb_free=8.8, wall=92266
2022-02-12 19:17:38 | INFO | train_inner | epoch 011:    894 / 1576 loss=7.036, nll_loss=5.3, ppl=39.39, wps=11805.5, ups=0.18, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.475, loss_scale=16, train_wall=545, gb_free=8.8, wall=92821
2022-02-12 19:26:53 | INFO | train_inner | epoch 011:    994 / 1576 loss=7.046, nll_loss=5.311, ppl=39.7, wps=11802.6, ups=0.18, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.464, loss_scale=16, train_wall=545, gb_free=8.8, wall=93376
2022-02-12 19:29:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 19:36:14 | INFO | train_inner | epoch 011:   1095 / 1576 loss=7.045, nll_loss=5.31, ppl=39.67, wps=11686.2, ups=0.18, wpb=65536, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.473, loss_scale=16, train_wall=550, gb_free=8.8, wall=93937
2022-02-12 19:45:29 | INFO | train_inner | epoch 011:   1195 / 1576 loss=7.056, nll_loss=5.322, ppl=40.01, wps=11804.7, ups=0.18, wpb=65536, bsz=128, num_updates=16900, lr=0.000243252, gnorm=0.483, loss_scale=16, train_wall=545, gb_free=8.8, wall=94492
2022-02-12 19:45:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 19:54:50 | INFO | train_inner | epoch 011:   1296 / 1576 loss=7.05, nll_loss=5.315, ppl=39.82, wps=11690.8, ups=0.18, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.488, loss_scale=8, train_wall=550, gb_free=8.8, wall=95052
2022-02-12 20:04:05 | INFO | train_inner | epoch 011:   1396 / 1576 loss=7.041, nll_loss=5.305, ppl=39.54, wps=11810.3, ups=0.18, wpb=65536, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.489, loss_scale=8, train_wall=544, gb_free=8.8, wall=95607
2022-02-12 20:11:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 20:13:25 | INFO | train_inner | epoch 011:   1497 / 1576 loss=7.047, nll_loss=5.312, ppl=39.73, wps=11696.1, ups=0.18, wpb=65536, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.468, loss_scale=8, train_wall=550, gb_free=8.8, wall=96168
2022-02-12 20:20:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 20:20:45 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.049 | nll_loss 5.3 | ppl 39.39 | wps 32333 | wpb 1021.8 | bsz 2 | num_updates 17279 | best_loss 7.049
2022-02-12 20:20:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 17279 updates
2022-02-12 20:20:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint11.pt
2022-02-12 20:20:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint11.pt
2022-02-12 20:21:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint11.pt (epoch 11 @ 17279 updates, score 7.049) (writing took 28.118269531056285 seconds)
2022-02-12 20:21:13 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-02-12 20:21:13 | INFO | train | epoch 011 | loss 7.033 | nll_loss 5.296 | ppl 39.29 | wps 11723.1 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 17279 | lr 0.00024057 | gnorm 0.48 | loss_scale 8 | train_wall 8576 | gb_free 8.8 | wall 96636
2022-02-12 20:21:14 | INFO | fairseq.trainer | begin training epoch 12
2022-02-12 20:21:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 20:23:10 | INFO | train_inner | epoch 012:     21 / 1576 loss=7.041, nll_loss=5.306, ppl=39.56, wps=11107.8, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=17300, lr=0.000240424, gnorm=0.494, loss_scale=8, train_wall=539, gb_free=8.8, wall=96753
2022-02-12 20:32:25 | INFO | train_inner | epoch 012:    121 / 1576 loss=6.97, nll_loss=5.226, ppl=37.41, wps=11817.4, ups=0.18, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.466, loss_scale=8, train_wall=544, gb_free=8.8, wall=97307
2022-02-12 20:41:40 | INFO | train_inner | epoch 012:    221 / 1576 loss=6.971, nll_loss=5.226, ppl=37.43, wps=11803.6, ups=0.18, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.495, loss_scale=16, train_wall=545, gb_free=8.8, wall=97862
2022-02-12 20:50:55 | INFO | train_inner | epoch 012:    321 / 1576 loss=6.986, nll_loss=5.243, ppl=37.86, wps=11801.5, ups=0.18, wpb=65536, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.459, loss_scale=16, train_wall=545, gb_free=8.8, wall=98418
2022-02-12 21:00:11 | INFO | train_inner | epoch 012:    421 / 1576 loss=6.998, nll_loss=5.256, ppl=38.21, wps=11798.5, ups=0.18, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=0.487, loss_scale=32, train_wall=545, gb_free=8.8, wall=98973
2022-02-12 21:00:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 21:09:31 | INFO | train_inner | epoch 012:    522 / 1576 loss=6.99, nll_loss=5.248, ppl=37.99, wps=11686.8, ups=0.18, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.472, loss_scale=16, train_wall=550, gb_free=8.8, wall=99534
2022-02-12 21:18:46 | INFO | train_inner | epoch 012:    622 / 1576 loss=6.995, nll_loss=5.253, ppl=38.13, wps=11808.7, ups=0.18, wpb=65536, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.471, loss_scale=16, train_wall=544, gb_free=8.8, wall=100089
2022-02-12 21:24:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 21:28:07 | INFO | train_inner | epoch 012:    723 / 1576 loss=7.002, nll_loss=5.261, ppl=38.35, wps=11690.5, ups=0.18, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.464, loss_scale=16, train_wall=550, gb_free=8.8, wall=100649
2022-02-12 21:31:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 21:37:27 | INFO | train_inner | epoch 012:    824 / 1576 loss=7.008, nll_loss=5.267, ppl=38.51, wps=11691.1, ups=0.18, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=0.512, loss_scale=8, train_wall=550, gb_free=8.8, wall=101210
2022-02-12 21:46:42 | INFO | train_inner | epoch 012:    924 / 1576 loss=7.013, nll_loss=5.273, ppl=38.67, wps=11815.7, ups=0.18, wpb=65532.3, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.461, loss_scale=8, train_wall=544, gb_free=8.8, wall=101765
2022-02-12 21:55:57 | INFO | train_inner | epoch 012:   1024 / 1576 loss=7.016, nll_loss=5.277, ppl=38.78, wps=11812, ups=0.18, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.522, loss_scale=16, train_wall=544, gb_free=8.8, wall=102319
2022-02-12 22:05:12 | INFO | train_inner | epoch 012:   1124 / 1576 loss=7.012, nll_loss=5.272, ppl=38.65, wps=11800.3, ups=0.18, wpb=65536, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.496, loss_scale=16, train_wall=545, gb_free=8.8, wall=102875
2022-02-12 22:14:28 | INFO | train_inner | epoch 012:   1224 / 1576 loss=7.008, nll_loss=5.268, ppl=38.54, wps=11803.3, ups=0.18, wpb=65536, bsz=128, num_updates=18500, lr=0.000232495, gnorm=0.468, loss_scale=16, train_wall=545, gb_free=8.8, wall=103430
2022-02-12 22:19:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 22:21:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 22:23:54 | INFO | train_inner | epoch 012:   1326 / 1576 loss=7.018, nll_loss=5.279, ppl=38.83, wps=11579.2, ups=0.18, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.506, loss_scale=8, train_wall=555, gb_free=8.8, wall=103996
2022-02-12 22:33:08 | INFO | train_inner | epoch 012:   1426 / 1576 loss=7.02, nll_loss=5.282, ppl=38.91, wps=11809.5, ups=0.18, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.455, loss_scale=8, train_wall=544, gb_free=8.8, wall=104551
2022-02-12 22:42:23 | INFO | train_inner | epoch 012:   1526 / 1576 loss=7.029, nll_loss=5.293, ppl=39.2, wps=11810.9, ups=0.18, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.493, loss_scale=8, train_wall=544, gb_free=8.8, wall=105106
2022-02-12 22:46:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-12 22:47:03 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.03 | nll_loss 5.264 | ppl 38.41 | wps 32639.5 | wpb 1021.8 | bsz 2 | num_updates 18850 | best_loss 7.03
2022-02-12 22:47:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 18850 updates
2022-02-12 22:47:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint12.pt
2022-02-12 22:47:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint12.pt
2022-02-12 22:47:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint12.pt (epoch 12 @ 18850 updates, score 7.03) (writing took 28.05572437029332 seconds)
2022-02-12 22:47:31 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-02-12 22:47:31 | INFO | train | epoch 012 | loss 7.002 | nll_loss 5.262 | ppl 38.36 | wps 11723.3 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 18850 | lr 0.000230327 | gnorm 0.481 | loss_scale 16 | train_wall 8576 | gb_free 8.8 | wall 105413
2022-02-12 22:47:31 | INFO | fairseq.trainer | begin training epoch 13
2022-02-12 22:47:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-12 22:52:09 | INFO | train_inner | epoch 013:     50 / 1576 loss=6.967, nll_loss=5.222, ppl=37.32, wps=11101.5, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=18900, lr=0.000230022, gnorm=0.478, loss_scale=16, train_wall=540, gb_free=8.8, wall=105691
2022-02-12 23:01:24 | INFO | train_inner | epoch 013:    150 / 1576 loss=6.94, nll_loss=5.191, ppl=36.54, wps=11806.9, ups=0.18, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.491, loss_scale=16, train_wall=544, gb_free=8.8, wall=106246
2022-02-12 23:09:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-12 23:10:44 | INFO | train_inner | epoch 013:    251 / 1576 loss=6.958, nll_loss=5.212, ppl=37.06, wps=11689.8, ups=0.18, wpb=65532.3, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.468, loss_scale=16, train_wall=550, gb_free=8.8, wall=106807
2022-02-12 23:17:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-12 23:20:05 | INFO | train_inner | epoch 013:    352 / 1576 loss=6.962, nll_loss=5.216, ppl=37.16, wps=11691, ups=0.18, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.505, loss_scale=8, train_wall=550, gb_free=8.8, wall=107367
2022-02-12 23:29:19 | INFO | train_inner | epoch 013:    452 / 1576 loss=6.964, nll_loss=5.218, ppl=37.22, wps=11814.2, ups=0.18, wpb=65536, bsz=128, num_updates=19300, lr=0.000227626, gnorm=0.478, loss_scale=8, train_wall=544, gb_free=8.8, wall=107922
2022-02-12 23:38:34 | INFO | train_inner | epoch 013:    552 / 1576 loss=6.966, nll_loss=5.22, ppl=37.28, wps=11808.5, ups=0.18, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.496, loss_scale=8, train_wall=544, gb_free=8.8, wall=108477
2022-02-12 23:47:50 | INFO | train_inner | epoch 013:    652 / 1576 loss=6.976, nll_loss=5.232, ppl=37.57, wps=11805.9, ups=0.18, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.478, loss_scale=16, train_wall=545, gb_free=8.8, wall=109032
2022-02-12 23:57:05 | INFO | train_inner | epoch 013:    752 / 1576 loss=6.988, nll_loss=5.245, ppl=37.91, wps=11806, ups=0.18, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.479, loss_scale=16, train_wall=545, gb_free=8.8, wall=109587
2022-02-12 23:57:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 00:06:25 | INFO | train_inner | epoch 013:    853 / 1576 loss=6.982, nll_loss=5.239, ppl=37.76, wps=11699.7, ups=0.18, wpb=65536, bsz=128, num_updates=19700, lr=0.000225303, gnorm=0.491, loss_scale=8, train_wall=550, gb_free=8.8, wall=110147
2022-02-13 00:15:39 | INFO | train_inner | epoch 013:    953 / 1576 loss=6.983, nll_loss=5.24, ppl=37.8, wps=11817.8, ups=0.18, wpb=65536, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.476, loss_scale=8, train_wall=544, gb_free=8.8, wall=110702
2022-02-13 00:22:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 00:25:00 | INFO | train_inner | epoch 013:   1054 / 1576 loss=6.988, nll_loss=5.246, ppl=37.95, wps=11697.7, ups=0.18, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.503, loss_scale=8, train_wall=550, gb_free=8.8, wall=111262
2022-02-13 00:34:14 | INFO | train_inner | epoch 013:   1154 / 1576 loss=6.999, nll_loss=5.258, ppl=38.27, wps=11815.6, ups=0.18, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.483, loss_scale=8, train_wall=544, gb_free=8.8, wall=111817
2022-02-13 00:43:29 | INFO | train_inner | epoch 013:   1254 / 1576 loss=6.99, nll_loss=5.248, ppl=38, wps=11814.6, ups=0.18, wpb=65536, bsz=128, num_updates=20100, lr=0.00022305, gnorm=0.468, loss_scale=8, train_wall=544, gb_free=8.8, wall=112371
2022-02-13 00:51:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 00:52:49 | INFO | train_inner | epoch 013:   1355 / 1576 loss=6.994, nll_loss=5.253, ppl=38.12, wps=11692.7, ups=0.18, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.488, loss_scale=8, train_wall=550, gb_free=8.8, wall=112932
2022-02-13 01:02:04 | INFO | train_inner | epoch 013:   1455 / 1576 loss=6.995, nll_loss=5.254, ppl=38.15, wps=11816.6, ups=0.18, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.491, loss_scale=8, train_wall=544, gb_free=8.8, wall=113487
2022-02-13 01:11:19 | INFO | train_inner | epoch 013:   1555 / 1576 loss=6.988, nll_loss=5.246, ppl=37.95, wps=11814.5, ups=0.18, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=0.477, loss_scale=8, train_wall=544, gb_free=8.8, wall=114041
2022-02-13 01:13:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 01:13:17 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.018 | nll_loss 5.285 | ppl 39 | wps 32489.5 | wpb 1021.8 | bsz 2 | num_updates 20421 | best_loss 7.018
2022-02-13 01:13:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 20421 updates
2022-02-13 01:13:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint13.pt
2022-02-13 01:13:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint13.pt
2022-02-13 01:13:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint13.pt (epoch 13 @ 20421 updates, score 7.018) (writing took 28.244301847182214 seconds)
2022-02-13 01:13:45 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-02-13 01:13:45 | INFO | train | epoch 013 | loss 6.977 | nll_loss 5.232 | ppl 37.59 | wps 11726.9 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 20421 | lr 0.00022129 | gnorm 0.485 | loss_scale 8 | train_wall 8573 | gb_free 8.8 | wall 114188
2022-02-13 01:13:46 | INFO | fairseq.trainer | begin training epoch 14
2022-02-13 01:13:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 01:21:04 | INFO | train_inner | epoch 014:     79 / 1576 loss=6.928, nll_loss=5.178, ppl=36.19, wps=11102.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=20500, lr=0.000220863, gnorm=0.489, loss_scale=16, train_wall=539, gb_free=8.8, wall=114626
2022-02-13 01:21:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 01:30:24 | INFO | train_inner | epoch 014:    180 / 1576 loss=6.911, nll_loss=5.158, ppl=35.71, wps=11694.2, ups=0.18, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.473, loss_scale=8, train_wall=550, gb_free=8.8, wall=115187
2022-02-13 01:39:39 | INFO | train_inner | epoch 014:    280 / 1576 loss=6.924, nll_loss=5.173, ppl=36.08, wps=11809.6, ups=0.18, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.481, loss_scale=8, train_wall=544, gb_free=8.8, wall=115742
2022-02-13 01:48:54 | INFO | train_inner | epoch 014:    380 / 1576 loss=6.938, nll_loss=5.189, ppl=36.48, wps=11811.4, ups=0.18, wpb=65536, bsz=128, num_updates=20800, lr=0.000219265, gnorm=0.479, loss_scale=16, train_wall=544, gb_free=8.8, wall=116297
2022-02-13 01:58:09 | INFO | train_inner | epoch 014:    480 / 1576 loss=6.939, nll_loss=5.19, ppl=36.51, wps=11800.6, ups=0.18, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.471, loss_scale=16, train_wall=545, gb_free=8.8, wall=116852
2022-02-13 01:58:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 02:07:30 | INFO | train_inner | epoch 014:    581 / 1576 loss=6.948, nll_loss=5.2, ppl=36.75, wps=11694.4, ups=0.18, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.508, loss_scale=8, train_wall=550, gb_free=8.8, wall=117412
2022-02-13 02:16:45 | INFO | train_inner | epoch 014:    681 / 1576 loss=6.95, nll_loss=5.202, ppl=36.81, wps=11811.3, ups=0.18, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.497, loss_scale=8, train_wall=544, gb_free=8.8, wall=117967
2022-02-13 02:22:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 02:26:05 | INFO | train_inner | epoch 014:    782 / 1576 loss=6.959, nll_loss=5.212, ppl=37.08, wps=11698.9, ups=0.18, wpb=65536, bsz=128, num_updates=21200, lr=0.000217186, gnorm=0.485, loss_scale=8, train_wall=550, gb_free=8.8, wall=118527
2022-02-13 02:35:20 | INFO | train_inner | epoch 014:    882 / 1576 loss=6.965, nll_loss=5.22, ppl=37.27, wps=11811.5, ups=0.18, wpb=65536, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.476, loss_scale=8, train_wall=544, gb_free=8.8, wall=119082
2022-02-13 02:44:35 | INFO | train_inner | epoch 014:    982 / 1576 loss=6.962, nll_loss=5.216, ppl=37.16, wps=11810.1, ups=0.18, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.493, loss_scale=8, train_wall=544, gb_free=8.8, wall=119637
2022-02-13 02:47:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 02:53:55 | INFO | train_inner | epoch 014:   1083 / 1576 loss=6.964, nll_loss=5.218, ppl=37.22, wps=11693.8, ups=0.18, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.504, loss_scale=8, train_wall=550, gb_free=8.8, wall=120198
2022-02-13 03:03:10 | INFO | train_inner | epoch 014:   1183 / 1576 loss=6.97, nll_loss=5.225, ppl=37.41, wps=11814.5, ups=0.18, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=0.472, loss_scale=8, train_wall=544, gb_free=8.8, wall=120752
2022-02-13 03:12:24 | INFO | train_inner | epoch 014:   1283 / 1576 loss=6.974, nll_loss=5.23, ppl=37.53, wps=11818.2, ups=0.18, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.483, loss_scale=16, train_wall=544, gb_free=8.8, wall=121307
2022-02-13 03:16:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 03:21:45 | INFO | train_inner | epoch 014:   1384 / 1576 loss=6.971, nll_loss=5.226, ppl=37.43, wps=11699.9, ups=0.18, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.483, loss_scale=8, train_wall=550, gb_free=8.8, wall=121867
2022-02-13 03:30:59 | INFO | train_inner | epoch 014:   1484 / 1576 loss=6.981, nll_loss=5.237, ppl=37.72, wps=11811.3, ups=0.18, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.471, loss_scale=8, train_wall=544, gb_free=8.8, wall=122422
2022-02-13 03:39:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 03:39:32 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.011 | nll_loss 5.243 | ppl 37.87 | wps 32467.6 | wpb 1021.8 | bsz 2 | num_updates 21992 | best_loss 7.011
2022-02-13 03:39:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 21992 updates
2022-02-13 03:39:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint14.pt
2022-02-13 03:39:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint14.pt
2022-02-13 03:40:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint14.pt (epoch 14 @ 21992 updates, score 7.011) (writing took 28.061185502447188 seconds)
2022-02-13 03:40:00 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-02-13 03:40:00 | INFO | train | epoch 014 | loss 6.954 | nll_loss 5.206 | ppl 36.92 | wps 11727.4 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 21992 | lr 0.000213239 | gnorm 0.486 | loss_scale 8 | train_wall 8573 | gb_free 8.8 | wall 122962
2022-02-13 03:40:00 | INFO | fairseq.trainer | begin training epoch 15
2022-02-13 03:40:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 03:40:44 | INFO | train_inner | epoch 015:      8 / 1576 loss=6.977, nll_loss=5.233, ppl=37.6, wps=11108.6, ups=0.17, wpb=64958.8, bsz=126.9, num_updates=22000, lr=0.000213201, gnorm=0.517, loss_scale=8, train_wall=539, gb_free=8.8, wall=123007
2022-02-13 03:49:59 | INFO | train_inner | epoch 015:    108 / 1576 loss=6.893, nll_loss=5.138, ppl=35.21, wps=11806.4, ups=0.18, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.473, loss_scale=16, train_wall=544, gb_free=8.8, wall=123562
2022-02-13 03:58:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 03:59:19 | INFO | train_inner | epoch 015:    209 / 1576 loss=6.899, nll_loss=5.145, ppl=35.38, wps=11704.6, ups=0.18, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.478, loss_scale=8, train_wall=549, gb_free=8.8, wall=124122
2022-02-13 04:08:34 | INFO | train_inner | epoch 015:    309 / 1576 loss=6.904, nll_loss=5.15, ppl=35.51, wps=11819.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.478, loss_scale=8, train_wall=544, gb_free=8.8, wall=124676
2022-02-13 04:17:48 | INFO | train_inner | epoch 015:    409 / 1576 loss=6.916, nll_loss=5.164, ppl=35.86, wps=11826.4, ups=0.18, wpb=65536, bsz=128, num_updates=22400, lr=0.000211289, gnorm=0.495, loss_scale=8, train_wall=544, gb_free=8.8, wall=125230
2022-02-13 04:23:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 04:27:08 | INFO | train_inner | epoch 015:    510 / 1576 loss=6.918, nll_loss=5.166, ppl=35.9, wps=11705.6, ups=0.18, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.499, loss_scale=8, train_wall=549, gb_free=8.8, wall=125790
2022-02-13 04:36:22 | INFO | train_inner | epoch 015:    610 / 1576 loss=6.941, nll_loss=5.193, ppl=36.57, wps=11818.9, ups=0.18, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.472, loss_scale=8, train_wall=544, gb_free=8.8, wall=126345
2022-02-13 04:45:37 | INFO | train_inner | epoch 015:    710 / 1576 loss=6.925, nll_loss=5.174, ppl=36.1, wps=11820.2, ups=0.18, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.484, loss_scale=8, train_wall=544, gb_free=8.8, wall=126899
2022-02-13 04:48:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 04:54:57 | INFO | train_inner | epoch 015:    811 / 1576 loss=6.933, nll_loss=5.183, ppl=36.33, wps=11704.1, ups=0.18, wpb=65536, bsz=128, num_updates=22800, lr=0.000209427, gnorm=0.484, loss_scale=8, train_wall=549, gb_free=8.8, wall=127459
2022-02-13 05:04:11 | INFO | train_inner | epoch 015:    911 / 1576 loss=6.948, nll_loss=5.2, ppl=36.76, wps=11824.4, ups=0.18, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.503, loss_scale=8, train_wall=544, gb_free=8.8, wall=128013
2022-02-13 05:13:25 | INFO | train_inner | epoch 015:   1011 / 1576 loss=6.941, nll_loss=5.193, ppl=36.57, wps=11823.2, ups=0.18, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.476, loss_scale=16, train_wall=544, gb_free=8.8, wall=128568
2022-02-13 05:22:39 | INFO | train_inner | epoch 015:   1111 / 1576 loss=6.943, nll_loss=5.194, ppl=36.61, wps=11820.6, ups=0.18, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.485, loss_scale=16, train_wall=544, gb_free=8.8, wall=129122
2022-02-13 05:26:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 05:32:00 | INFO | train_inner | epoch 015:   1212 / 1576 loss=6.954, nll_loss=5.207, ppl=36.94, wps=11702.6, ups=0.18, wpb=65536, bsz=128, num_updates=23200, lr=0.000207614, gnorm=0.473, loss_scale=8, train_wall=549, gb_free=8.8, wall=129682
2022-02-13 05:41:14 | INFO | train_inner | epoch 015:   1312 / 1576 loss=6.96, nll_loss=5.214, ppl=37.12, wps=11816.2, ups=0.18, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.471, loss_scale=8, train_wall=544, gb_free=8.8, wall=130237
2022-02-13 05:50:29 | INFO | train_inner | epoch 015:   1412 / 1576 loss=6.953, nll_loss=5.206, ppl=36.9, wps=11819.1, ups=0.18, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.49, loss_scale=16, train_wall=544, gb_free=8.8, wall=130791
2022-02-13 05:53:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 05:59:49 | INFO | train_inner | epoch 015:   1513 / 1576 loss=6.961, nll_loss=5.216, ppl=37.16, wps=11704.8, ups=0.18, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.485, loss_scale=8, train_wall=549, gb_free=8.8, wall=131351
2022-02-13 06:05:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 06:05:40 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.994 | nll_loss 5.225 | ppl 37.39 | wps 32678 | wpb 1021.8 | bsz 2 | num_updates 23563 | best_loss 6.994
2022-02-13 06:05:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 23563 updates
2022-02-13 06:05:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint15.pt
2022-02-13 06:05:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint15.pt
2022-02-13 06:06:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint15.pt (epoch 15 @ 23563 updates, score 6.994) (writing took 29.016013548709452 seconds)
2022-02-13 06:06:09 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-02-13 06:06:09 | INFO | train | epoch 015 | loss 6.933 | nll_loss 5.184 | ppl 36.34 | wps 11734.2 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 23563 | lr 0.000206008 | gnorm 0.484 | loss_scale 8 | train_wall 8567 | gb_free 8.8 | wall 131731
2022-02-13 06:06:09 | INFO | fairseq.trainer | begin training epoch 16
2022-02-13 06:06:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 06:09:34 | INFO | train_inner | epoch 016:     37 / 1576 loss=6.927, nll_loss=5.177, ppl=36.17, wps=11093.4, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=23600, lr=0.000205847, gnorm=0.498, loss_scale=8, train_wall=539, gb_free=8.8, wall=131937
2022-02-13 06:18:48 | INFO | train_inner | epoch 016:    137 / 1576 loss=6.867, nll_loss=5.108, ppl=34.5, wps=11822.3, ups=0.18, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.482, loss_scale=16, train_wall=544, gb_free=8.8, wall=132491
2022-02-13 06:20:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 06:28:09 | INFO | train_inner | epoch 016:    238 / 1576 loss=6.882, nll_loss=5.126, ppl=34.91, wps=11700, ups=0.18, wpb=65536, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.489, loss_scale=8, train_wall=549, gb_free=8.8, wall=133051
2022-02-13 06:37:23 | INFO | train_inner | epoch 016:    338 / 1576 loss=6.886, nll_loss=5.13, ppl=35.01, wps=11822.8, ups=0.18, wpb=65536, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.483, loss_scale=8, train_wall=544, gb_free=8.8, wall=133605
2022-02-13 06:46:37 | INFO | train_inner | epoch 016:    438 / 1576 loss=6.893, nll_loss=5.137, ppl=35.2, wps=11818.9, ups=0.18, wpb=65536, bsz=128, num_updates=24000, lr=0.000204124, gnorm=0.49, loss_scale=16, train_wall=544, gb_free=8.8, wall=134160
2022-02-13 06:55:52 | INFO | train_inner | epoch 016:    538 / 1576 loss=6.908, nll_loss=5.155, ppl=35.62, wps=11821.8, ups=0.18, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.481, loss_scale=16, train_wall=544, gb_free=8.8, wall=134714
2022-02-13 06:59:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 07:05:12 | INFO | train_inner | epoch 016:    639 / 1576 loss=6.914, nll_loss=5.161, ppl=35.79, wps=11707.8, ups=0.18, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.483, loss_scale=8, train_wall=549, gb_free=8.8, wall=135274
2022-02-13 07:14:26 | INFO | train_inner | epoch 016:    739 / 1576 loss=6.914, nll_loss=5.162, ppl=35.8, wps=11820.2, ups=0.18, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.487, loss_scale=8, train_wall=544, gb_free=8.8, wall=135829
2022-02-13 07:23:40 | INFO | train_inner | epoch 016:    839 / 1576 loss=6.915, nll_loss=5.162, ppl=35.81, wps=11820.1, ups=0.18, wpb=65536, bsz=128, num_updates=24400, lr=0.000202444, gnorm=0.482, loss_scale=16, train_wall=544, gb_free=8.8, wall=136383
2022-02-13 07:24:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 07:33:00 | INFO | train_inner | epoch 016:    940 / 1576 loss=6.934, nll_loss=5.184, ppl=36.36, wps=11712.3, ups=0.18, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.481, loss_scale=8, train_wall=549, gb_free=8.8, wall=136943
2022-02-13 07:42:14 | INFO | train_inner | epoch 016:   1040 / 1576 loss=6.929, nll_loss=5.178, ppl=36.21, wps=11825.5, ups=0.18, wpb=65536, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.503, loss_scale=8, train_wall=544, gb_free=8.8, wall=137497
2022-02-13 07:50:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 07:51:34 | INFO | train_inner | epoch 016:   1141 / 1576 loss=6.931, nll_loss=5.18, ppl=36.26, wps=11700.5, ups=0.18, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.475, loss_scale=8, train_wall=550, gb_free=8.8, wall=138057
2022-02-13 08:00:49 | INFO | train_inner | epoch 016:   1241 / 1576 loss=6.927, nll_loss=5.177, ppl=36.17, wps=11819.9, ups=0.18, wpb=65536, bsz=128, num_updates=24800, lr=0.000200805, gnorm=0.501, loss_scale=8, train_wall=544, gb_free=8.8, wall=138611
2022-02-13 08:10:03 | INFO | train_inner | epoch 016:   1341 / 1576 loss=6.943, nll_loss=5.194, ppl=36.62, wps=11817.3, ups=0.18, wpb=65532.3, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.492, loss_scale=8, train_wall=544, gb_free=8.8, wall=139166
2022-02-13 08:16:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 08:19:23 | INFO | train_inner | epoch 016:   1442 / 1576 loss=6.939, nll_loss=5.19, ppl=36.51, wps=11707.9, ups=0.18, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.487, loss_scale=8, train_wall=549, gb_free=8.8, wall=139726
2022-02-13 08:28:37 | INFO | train_inner | epoch 016:   1542 / 1576 loss=6.944, nll_loss=5.196, ppl=36.66, wps=11824.8, ups=0.18, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.495, loss_scale=8, train_wall=544, gb_free=8.8, wall=140280
2022-02-13 08:31:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 08:31:48 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.991 | nll_loss 5.226 | ppl 37.44 | wps 32526.5 | wpb 1021.8 | bsz 2 | num_updates 25134 | best_loss 6.991
2022-02-13 08:31:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 25134 updates
2022-02-13 08:31:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint16.pt
2022-02-13 08:31:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint16.pt
2022-02-13 08:32:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint16.pt (epoch 16 @ 25134 updates, score 6.991) (writing took 29.238391916267574 seconds)
2022-02-13 08:32:17 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-02-13 08:32:17 | INFO | train | epoch 016 | loss 6.915 | nll_loss 5.163 | ppl 35.82 | wps 11735.8 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 25134 | lr 0.000199466 | gnorm 0.488 | loss_scale 8 | train_wall 8566 | gb_free 8.8 | wall 140499
2022-02-13 08:32:17 | INFO | fairseq.trainer | begin training epoch 17
2022-02-13 08:32:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 08:38:22 | INFO | train_inner | epoch 017:     66 / 1576 loss=6.884, nll_loss=5.128, ppl=34.98, wps=11100.8, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=25200, lr=0.000199205, gnorm=0.489, loss_scale=8, train_wall=539, gb_free=8.8, wall=140865
2022-02-13 08:42:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 08:47:42 | INFO | train_inner | epoch 017:    167 / 1576 loss=6.847, nll_loss=5.086, ppl=33.96, wps=11713.1, ups=0.18, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.487, loss_scale=8, train_wall=549, gb_free=8.8, wall=141425
2022-02-13 08:56:56 | INFO | train_inner | epoch 017:    267 / 1576 loss=6.874, nll_loss=5.117, ppl=34.69, wps=11828.4, ups=0.18, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.481, loss_scale=8, train_wall=543, gb_free=8.8, wall=141979
2022-02-13 09:06:10 | INFO | train_inner | epoch 017:    367 / 1576 loss=6.873, nll_loss=5.115, ppl=34.65, wps=11824.1, ups=0.18, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=0.487, loss_scale=16, train_wall=544, gb_free=8.8, wall=142533
2022-02-13 09:15:25 | INFO | train_inner | epoch 017:    467 / 1576 loss=6.878, nll_loss=5.121, ppl=34.79, wps=11818.8, ups=0.18, wpb=65532.3, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.493, loss_scale=16, train_wall=544, gb_free=8.8, wall=143087
2022-02-13 09:18:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 09:24:45 | INFO | train_inner | epoch 017:    568 / 1576 loss=6.897, nll_loss=5.142, ppl=35.31, wps=11706.6, ups=0.18, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.48, loss_scale=8, train_wall=549, gb_free=8.8, wall=143647
2022-02-13 09:33:58 | INFO | train_inner | epoch 017:    668 / 1576 loss=6.896, nll_loss=5.142, ppl=35.3, wps=11832.5, ups=0.18, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.48, loss_scale=8, train_wall=543, gb_free=8.8, wall=144201
2022-02-13 09:43:12 | INFO | train_inner | epoch 017:    768 / 1576 loss=6.902, nll_loss=5.148, ppl=35.45, wps=11832.9, ups=0.18, wpb=65536, bsz=128, num_updates=25900, lr=0.000196494, gnorm=0.501, loss_scale=16, train_wall=543, gb_free=8.8, wall=144755
2022-02-13 09:52:27 | INFO | train_inner | epoch 017:    868 / 1576 loss=6.906, nll_loss=5.153, ppl=35.58, wps=11820.5, ups=0.18, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.465, loss_scale=16, train_wall=544, gb_free=8.8, wall=145309
2022-02-13 10:01:41 | INFO | train_inner | epoch 017:    968 / 1576 loss=6.916, nll_loss=5.163, ppl=35.84, wps=11818.3, ups=0.18, wpb=65536, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.484, loss_scale=16, train_wall=544, gb_free=8.8, wall=145864
2022-02-13 10:06:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-13 10:11:01 | INFO | train_inner | epoch 017:   1069 / 1576 loss=6.908, nll_loss=5.155, ppl=35.62, wps=11705.8, ups=0.18, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.492, loss_scale=16, train_wall=549, gb_free=8.8, wall=146424
2022-02-13 10:20:16 | INFO | train_inner | epoch 017:   1169 / 1576 loss=6.902, nll_loss=5.148, ppl=35.45, wps=11820.3, ups=0.18, wpb=65536, bsz=128, num_updates=26300, lr=0.000194994, gnorm=0.471, loss_scale=16, train_wall=544, gb_free=8.8, wall=146978
2022-02-13 10:29:30 | INFO | train_inner | epoch 017:   1269 / 1576 loss=6.923, nll_loss=5.171, ppl=36.04, wps=11819.7, ups=0.18, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.48, loss_scale=16, train_wall=544, gb_free=8.8, wall=147533
2022-02-13 10:30:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-13 10:36:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 10:38:55 | INFO | train_inner | epoch 017:   1371 / 1576 loss=6.932, nll_loss=5.183, ppl=36.32, wps=11594.4, ups=0.18, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.482, loss_scale=8, train_wall=554, gb_free=8.8, wall=148098
2022-02-13 10:48:09 | INFO | train_inner | epoch 017:   1471 / 1576 loss=6.928, nll_loss=5.178, ppl=36.2, wps=11828.2, ups=0.18, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.482, loss_scale=8, train_wall=544, gb_free=8.8, wall=148652
2022-02-13 10:57:23 | INFO | train_inner | epoch 017:   1571 / 1576 loss=6.927, nll_loss=5.177, ppl=36.18, wps=11830.9, ups=0.18, wpb=65536, bsz=128, num_updates=26700, lr=0.000193528, gnorm=0.473, loss_scale=8, train_wall=543, gb_free=8.8, wall=149206
2022-02-13 10:57:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 10:57:53 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.98 | nll_loss 5.201 | ppl 36.79 | wps 32679.1 | wpb 1021.8 | bsz 2 | num_updates 26705 | best_loss 6.98
2022-02-13 10:57:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 26705 updates
2022-02-13 10:57:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint17.pt
2022-02-13 10:58:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint17.pt
2022-02-13 10:58:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint17.pt (epoch 17 @ 26705 updates, score 6.98) (writing took 29.084646145813167 seconds)
2022-02-13 10:58:22 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-02-13 10:58:22 | INFO | train | epoch 017 | loss 6.898 | nll_loss 5.144 | ppl 35.36 | wps 11739.6 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 26705 | lr 0.00019351 | gnorm 0.483 | loss_scale 8 | train_wall 8563 | gb_free 8.8 | wall 149265
2022-02-13 10:58:22 | INFO | fairseq.trainer | begin training epoch 18
2022-02-13 10:58:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 11:07:08 | INFO | train_inner | epoch 018:     95 / 1576 loss=6.841, nll_loss=5.079, ppl=33.8, wps=11101, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=26800, lr=0.000193167, gnorm=0.479, loss_scale=16, train_wall=539, gb_free=8.8, wall=149791
2022-02-13 11:16:23 | INFO | train_inner | epoch 018:    195 / 1576 loss=6.841, nll_loss=5.079, ppl=33.81, wps=11822.5, ups=0.18, wpb=65536, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.484, loss_scale=16, train_wall=544, gb_free=8.8, wall=150345
2022-02-13 11:25:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-13 11:25:43 | INFO | train_inner | epoch 018:    296 / 1576 loss=6.853, nll_loss=5.092, ppl=34.12, wps=11704.4, ups=0.18, wpb=65536, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.491, loss_scale=16, train_wall=549, gb_free=8.8, wall=150905
2022-02-13 11:34:57 | INFO | train_inner | epoch 018:    396 / 1576 loss=6.862, nll_loss=5.103, ppl=34.36, wps=11825.4, ups=0.18, wpb=65536, bsz=128, num_updates=27100, lr=0.000192095, gnorm=0.487, loss_scale=16, train_wall=544, gb_free=8.8, wall=151459
2022-02-13 11:44:11 | INFO | train_inner | epoch 018:    496 / 1576 loss=6.874, nll_loss=5.116, ppl=34.68, wps=11823.9, ups=0.18, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.479, loss_scale=16, train_wall=544, gb_free=8.8, wall=152014
2022-02-13 11:48:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 11:53:30 | INFO | train_inner | epoch 018:    597 / 1576 loss=6.88, nll_loss=5.123, ppl=34.85, wps=11718.4, ups=0.18, wpb=65536, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.497, loss_scale=8, train_wall=549, gb_free=8.8, wall=152573
2022-02-13 12:02:44 | INFO | train_inner | epoch 018:    697 / 1576 loss=6.883, nll_loss=5.127, ppl=34.94, wps=11833.7, ups=0.18, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.491, loss_scale=8, train_wall=543, gb_free=8.8, wall=153127
2022-02-13 12:11:58 | INFO | train_inner | epoch 018:    797 / 1576 loss=6.892, nll_loss=5.137, ppl=35.2, wps=11833.3, ups=0.18, wpb=65536, bsz=128, num_updates=27500, lr=0.000190693, gnorm=0.477, loss_scale=16, train_wall=543, gb_free=8.8, wall=153681
2022-02-13 12:21:12 | INFO | train_inner | epoch 018:    897 / 1576 loss=6.885, nll_loss=5.129, ppl=34.99, wps=11826.1, ups=0.18, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.477, loss_scale=16, train_wall=544, gb_free=8.8, wall=154235
2022-02-13 12:21:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 12:30:32 | INFO | train_inner | epoch 018:    998 / 1576 loss=6.887, nll_loss=5.131, ppl=35.05, wps=11715.2, ups=0.18, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.492, loss_scale=8, train_wall=549, gb_free=8.8, wall=154794
2022-02-13 12:39:46 | INFO | train_inner | epoch 018:   1098 / 1576 loss=6.905, nll_loss=5.152, ppl=35.54, wps=11831.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.513, loss_scale=8, train_wall=543, gb_free=8.8, wall=155348
2022-02-13 12:49:00 | INFO | train_inner | epoch 018:   1198 / 1576 loss=6.904, nll_loss=5.15, ppl=35.52, wps=11829.1, ups=0.18, wpb=65536, bsz=128, num_updates=27900, lr=0.000189321, gnorm=0.48, loss_scale=16, train_wall=544, gb_free=8.8, wall=155902
2022-02-13 12:51:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 12:58:19 | INFO | train_inner | epoch 018:   1299 / 1576 loss=6.904, nll_loss=5.151, ppl=35.52, wps=11716.3, ups=0.18, wpb=65536, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.501, loss_scale=8, train_wall=549, gb_free=8.8, wall=156461
2022-02-13 13:07:33 | INFO | train_inner | epoch 018:   1399 / 1576 loss=6.911, nll_loss=5.158, ppl=35.7, wps=11832.9, ups=0.18, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.494, loss_scale=8, train_wall=543, gb_free=8.8, wall=157015
2022-02-13 13:16:47 | INFO | train_inner | epoch 018:   1499 / 1576 loss=6.908, nll_loss=5.155, ppl=35.63, wps=11829.4, ups=0.18, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.484, loss_scale=16, train_wall=544, gb_free=8.8, wall=157569
2022-02-13 13:23:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 13:23:55 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.97 | nll_loss 5.203 | ppl 36.84 | wps 32709.4 | wpb 1021.8 | bsz 2 | num_updates 28277 | best_loss 6.97
2022-02-13 13:23:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 28277 updates
2022-02-13 13:23:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint18.pt
2022-02-13 13:24:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint18.pt
2022-02-13 13:24:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint18.pt (epoch 18 @ 28277 updates, score 6.97) (writing took 28.931619971059263 seconds)
2022-02-13 13:24:24 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-02-13 13:24:24 | INFO | train | epoch 018 | loss 6.884 | nll_loss 5.127 | ppl 34.95 | wps 11750.7 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 28277 | lr 0.000188054 | gnorm 0.489 | loss_scale 16 | train_wall 8562 | gb_free 8.8 | wall 158027
2022-02-13 13:24:25 | INFO | fairseq.trainer | begin training epoch 19
2022-02-13 13:24:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 13:26:32 | INFO | train_inner | epoch 019:     23 / 1576 loss=6.893, nll_loss=5.138, ppl=35.21, wps=11103.8, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=28300, lr=0.000187978, gnorm=0.501, loss_scale=16, train_wall=539, gb_free=8.8, wall=158154
2022-02-13 13:35:46 | INFO | train_inner | epoch 019:    123 / 1576 loss=6.83, nll_loss=5.067, ppl=33.51, wps=11823.1, ups=0.18, wpb=65536, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.486, loss_scale=16, train_wall=544, gb_free=8.8, wall=158709
2022-02-13 13:39:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-13 13:45:06 | INFO | train_inner | epoch 019:    224 / 1576 loss=6.835, nll_loss=5.072, ppl=33.64, wps=11706.6, ups=0.18, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.478, loss_scale=16, train_wall=549, gb_free=8.8, wall=159269
2022-02-13 13:54:20 | INFO | train_inner | epoch 019:    324 / 1576 loss=6.86, nll_loss=5.1, ppl=34.3, wps=11822, ups=0.18, wpb=65536, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.494, loss_scale=16, train_wall=544, gb_free=8.8, wall=159823
2022-02-13 13:59:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 14:03:40 | INFO | train_inner | epoch 019:    425 / 1576 loss=6.857, nll_loss=5.097, ppl=34.23, wps=11717.7, ups=0.18, wpb=65536, bsz=128, num_updates=28700, lr=0.000186663, gnorm=0.509, loss_scale=8, train_wall=549, gb_free=8.8, wall=160382
2022-02-13 14:12:53 | INFO | train_inner | epoch 019:    525 / 1576 loss=6.844, nll_loss=5.082, ppl=33.87, wps=11834.4, ups=0.18, wpb=65536, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.478, loss_scale=8, train_wall=543, gb_free=8.8, wall=160936
2022-02-13 14:22:07 | INFO | train_inner | epoch 019:    625 / 1576 loss=6.867, nll_loss=5.108, ppl=34.48, wps=11834.6, ups=0.18, wpb=65536, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.484, loss_scale=8, train_wall=543, gb_free=8.8, wall=161490
2022-02-13 14:31:21 | INFO | train_inner | epoch 019:    725 / 1576 loss=6.872, nll_loss=5.114, ppl=34.64, wps=11823.3, ups=0.18, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=0.498, loss_scale=16, train_wall=544, gb_free=8.8, wall=162044
2022-02-13 14:40:35 | INFO | train_inner | epoch 019:    825 / 1576 loss=6.873, nll_loss=5.115, ppl=34.67, wps=11829.4, ups=0.18, wpb=65536, bsz=128, num_updates=29100, lr=0.000185376, gnorm=0.483, loss_scale=16, train_wall=544, gb_free=8.8, wall=162598
2022-02-13 14:48:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-13 14:49:55 | INFO | train_inner | epoch 019:    926 / 1576 loss=6.879, nll_loss=5.121, ppl=34.81, wps=11707.8, ups=0.18, wpb=65536, bsz=128, num_updates=29200, lr=0.000185058, gnorm=0.489, loss_scale=16, train_wall=549, gb_free=8.8, wall=163158
2022-02-13 14:59:10 | INFO | train_inner | epoch 019:   1026 / 1576 loss=6.877, nll_loss=5.12, ppl=34.77, wps=11823.7, ups=0.18, wpb=65536, bsz=128, num_updates=29300, lr=0.000184742, gnorm=0.482, loss_scale=16, train_wall=544, gb_free=8.8, wall=163712
2022-02-13 15:02:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 15:08:29 | INFO | train_inner | epoch 019:   1127 / 1576 loss=6.885, nll_loss=5.128, ppl=34.98, wps=11713.2, ups=0.18, wpb=65536, bsz=128, num_updates=29400, lr=0.000184428, gnorm=0.496, loss_scale=8, train_wall=549, gb_free=8.8, wall=164272
2022-02-13 15:17:43 | INFO | train_inner | epoch 019:   1227 / 1576 loss=6.898, nll_loss=5.143, ppl=35.35, wps=11833.1, ups=0.18, wpb=65536, bsz=128, num_updates=29500, lr=0.000184115, gnorm=0.49, loss_scale=8, train_wall=543, gb_free=8.8, wall=164825
2022-02-13 15:26:57 | INFO | train_inner | epoch 019:   1327 / 1576 loss=6.892, nll_loss=5.137, ppl=35.18, wps=11837.4, ups=0.18, wpb=65536, bsz=128, num_updates=29600, lr=0.000183804, gnorm=0.498, loss_scale=16, train_wall=543, gb_free=8.8, wall=165379
2022-02-13 15:36:11 | INFO | train_inner | epoch 019:   1427 / 1576 loss=6.892, nll_loss=5.137, ppl=35.19, wps=11825.1, ups=0.18, wpb=65536, bsz=128, num_updates=29700, lr=0.000183494, gnorm=0.487, loss_scale=16, train_wall=544, gb_free=8.8, wall=165933
2022-02-13 15:45:25 | INFO | train_inner | epoch 019:   1527 / 1576 loss=6.884, nll_loss=5.127, ppl=34.95, wps=11824.4, ups=0.18, wpb=65536, bsz=128, num_updates=29800, lr=0.000183186, gnorm=0.488, loss_scale=16, train_wall=544, gb_free=8.8, wall=166487
2022-02-13 15:46:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 15:49:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 15:49:58 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.964 | nll_loss 5.198 | ppl 36.71 | wps 32815 | wpb 1021.8 | bsz 2 | num_updates 29848 | best_loss 6.964
2022-02-13 15:49:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 29848 updates
2022-02-13 15:49:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint19.pt
2022-02-13 15:50:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint19.pt
2022-02-13 15:50:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint19.pt (epoch 19 @ 29848 updates, score 6.964) (writing took 29.120598978362978 seconds)
2022-02-13 15:50:27 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-02-13 15:50:27 | INFO | train | epoch 019 | loss 6.87 | nll_loss 5.112 | ppl 34.58 | wps 11742.5 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 29848 | lr 0.000183038 | gnorm 0.49 | loss_scale 8 | train_wall 8562 | gb_free 8.8 | wall 166790
2022-02-13 15:50:28 | INFO | fairseq.trainer | begin training epoch 20
2022-02-13 15:50:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 15:55:15 | INFO | train_inner | epoch 020:     52 / 1576 loss=6.847, nll_loss=5.086, ppl=33.97, wps=11002.6, ups=0.17, wpb=64958.8, bsz=126.9, num_updates=29900, lr=0.000182879, gnorm=0.504, loss_scale=8, train_wall=544, gb_free=8.8, wall=167078
2022-02-13 16:04:29 | INFO | train_inner | epoch 020:    152 / 1576 loss=6.809, nll_loss=5.043, ppl=32.96, wps=11837.4, ups=0.18, wpb=65536, bsz=128, num_updates=30000, lr=0.000182574, gnorm=0.498, loss_scale=8, train_wall=543, gb_free=8.8, wall=167632
2022-02-13 16:13:43 | INFO | train_inner | epoch 020:    252 / 1576 loss=6.827, nll_loss=5.063, ppl=33.43, wps=11833.7, ups=0.18, wpb=65536, bsz=128, num_updates=30100, lr=0.000182271, gnorm=0.487, loss_scale=16, train_wall=543, gb_free=8.8, wall=168185
2022-02-13 16:19:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 16:23:02 | INFO | train_inner | epoch 020:    353 / 1576 loss=6.836, nll_loss=5.073, ppl=33.66, wps=11710.8, ups=0.18, wpb=65536, bsz=128, num_updates=30200, lr=0.000181969, gnorm=0.504, loss_scale=8, train_wall=549, gb_free=8.8, wall=168745
2022-02-13 16:32:16 | INFO | train_inner | epoch 020:    453 / 1576 loss=6.838, nll_loss=5.076, ppl=33.72, wps=11831.4, ups=0.18, wpb=65536, bsz=128, num_updates=30300, lr=0.000181668, gnorm=0.497, loss_scale=8, train_wall=543, gb_free=8.8, wall=169299
2022-02-13 16:41:30 | INFO | train_inner | epoch 020:    553 / 1576 loss=6.85, nll_loss=5.089, ppl=34.04, wps=11831.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=30400, lr=0.000181369, gnorm=0.503, loss_scale=8, train_wall=543, gb_free=8.8, wall=169853
2022-02-13 16:49:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 16:50:50 | INFO | train_inner | epoch 020:    654 / 1576 loss=6.849, nll_loss=5.088, ppl=34, wps=11710.1, ups=0.18, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=0.487, loss_scale=8, train_wall=549, gb_free=8.8, wall=170412
2022-02-13 17:00:04 | INFO | train_inner | epoch 020:    754 / 1576 loss=6.862, nll_loss=5.102, ppl=34.35, wps=11832.4, ups=0.18, wpb=65536, bsz=128, num_updates=30600, lr=0.000180775, gnorm=0.491, loss_scale=8, train_wall=543, gb_free=8.8, wall=170966
2022-02-13 17:09:18 | INFO | train_inner | epoch 020:    854 / 1576 loss=6.867, nll_loss=5.108, ppl=34.49, wps=11831.6, ups=0.18, wpb=65536, bsz=128, num_updates=30700, lr=0.000180481, gnorm=0.506, loss_scale=8, train_wall=543, gb_free=8.8, wall=171520
2022-02-13 17:18:32 | INFO | train_inner | epoch 020:    954 / 1576 loss=6.861, nll_loss=5.102, ppl=34.35, wps=11821.6, ups=0.18, wpb=65536, bsz=128, num_updates=30800, lr=0.000180187, gnorm=0.489, loss_scale=16, train_wall=544, gb_free=8.8, wall=172075
2022-02-13 17:27:47 | INFO | train_inner | epoch 020:   1054 / 1576 loss=6.882, nll_loss=5.125, ppl=34.9, wps=11817.7, ups=0.18, wpb=65536, bsz=128, num_updates=30900, lr=0.000179896, gnorm=0.493, loss_scale=16, train_wall=544, gb_free=8.8, wall=172629
2022-02-13 17:37:01 | INFO | train_inner | epoch 020:   1154 / 1576 loss=6.882, nll_loss=5.126, ppl=34.92, wps=11819.3, ups=0.18, wpb=65536, bsz=128, num_updates=31000, lr=0.000179605, gnorm=0.488, loss_scale=32, train_wall=544, gb_free=8.8, wall=173184
2022-02-13 17:37:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-02-13 17:38:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 17:46:26 | INFO | train_inner | epoch 020:   1256 / 1576 loss=6.872, nll_loss=5.114, ppl=34.63, wps=11604.6, ups=0.18, wpb=65536, bsz=128, num_updates=31100, lr=0.000179316, gnorm=0.506, loss_scale=8, train_wall=554, gb_free=8.8, wall=173748
2022-02-13 17:55:40 | INFO | train_inner | epoch 020:   1356 / 1576 loss=6.885, nll_loss=5.129, ppl=35, wps=11836.2, ups=0.18, wpb=65536, bsz=128, num_updates=31200, lr=0.000179029, gnorm=0.492, loss_scale=8, train_wall=543, gb_free=8.8, wall=174302
2022-02-13 18:04:54 | INFO | train_inner | epoch 020:   1456 / 1576 loss=6.885, nll_loss=5.129, ppl=34.98, wps=11828.8, ups=0.18, wpb=65536, bsz=128, num_updates=31300, lr=0.000178743, gnorm=0.487, loss_scale=16, train_wall=544, gb_free=8.8, wall=174856
2022-02-13 18:14:08 | INFO | train_inner | epoch 020:   1556 / 1576 loss=6.886, nll_loss=5.13, ppl=35.01, wps=11830, ups=0.18, wpb=65536, bsz=128, num_updates=31400, lr=0.000178458, gnorm=0.493, loss_scale=16, train_wall=544, gb_free=8.8, wall=175410
2022-02-13 18:15:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 18:16:00 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.961 | nll_loss 5.176 | ppl 36.16 | wps 32709.8 | wpb 1021.8 | bsz 2 | num_updates 31420 | best_loss 6.961
2022-02-13 18:16:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 31420 updates
2022-02-13 18:16:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint20.pt
2022-02-13 18:16:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint20.pt
2022-02-13 18:16:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint20.pt (epoch 20 @ 31420 updates, score 6.961) (writing took 29.074725141748786 seconds)
2022-02-13 18:16:29 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-02-13 18:16:29 | INFO | train | epoch 020 | loss 6.858 | nll_loss 5.098 | ppl 34.24 | wps 11751.5 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 31420 | lr 0.000178401 | gnorm 0.495 | loss_scale 16 | train_wall 8561 | gb_free 8.8 | wall 175552
2022-02-13 18:16:29 | INFO | fairseq.trainer | begin training epoch 21
2022-02-13 18:16:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 18:17:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 18:23:58 | INFO | train_inner | epoch 021:     81 / 1576 loss=6.818, nll_loss=5.053, ppl=33.21, wps=11004.5, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=31500, lr=0.000178174, gnorm=0.497, loss_scale=8, train_wall=544, gb_free=8.8, wall=176000
2022-02-13 18:33:12 | INFO | train_inner | epoch 021:    181 / 1576 loss=6.797, nll_loss=5.029, ppl=32.65, wps=11830.1, ups=0.18, wpb=65536, bsz=128, num_updates=31600, lr=0.000177892, gnorm=0.497, loss_scale=8, train_wall=543, gb_free=8.8, wall=176554
2022-02-13 18:42:26 | INFO | train_inner | epoch 021:    281 / 1576 loss=6.817, nll_loss=5.052, ppl=33.17, wps=11830.9, ups=0.18, wpb=65536, bsz=128, num_updates=31700, lr=0.000177611, gnorm=0.487, loss_scale=16, train_wall=543, gb_free=8.8, wall=177108
2022-02-13 18:46:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 18:51:45 | INFO | train_inner | epoch 021:    382 / 1576 loss=6.814, nll_loss=5.049, ppl=33.1, wps=11713.1, ups=0.18, wpb=65536, bsz=128, num_updates=31800, lr=0.000177332, gnorm=0.524, loss_scale=8, train_wall=549, gb_free=8.8, wall=177668
2022-02-13 19:00:59 | INFO | train_inner | epoch 021:    482 / 1576 loss=6.832, nll_loss=5.069, ppl=33.56, wps=11832.7, ups=0.18, wpb=65536, bsz=128, num_updates=31900, lr=0.000177054, gnorm=0.483, loss_scale=8, train_wall=543, gb_free=8.8, wall=178222
2022-02-13 19:10:13 | INFO | train_inner | epoch 021:    582 / 1576 loss=6.844, nll_loss=5.082, ppl=33.87, wps=11829, ups=0.18, wpb=65536, bsz=128, num_updates=32000, lr=0.000176777, gnorm=0.499, loss_scale=16, train_wall=544, gb_free=8.8, wall=178776
2022-02-13 19:16:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 19:19:33 | INFO | train_inner | epoch 021:    683 / 1576 loss=6.833, nll_loss=5.069, ppl=33.57, wps=11707.4, ups=0.18, wpb=65536, bsz=128, num_updates=32100, lr=0.000176501, gnorm=0.495, loss_scale=8, train_wall=549, gb_free=8.8, wall=179335
2022-02-13 19:28:47 | INFO | train_inner | epoch 021:    783 / 1576 loss=6.851, nll_loss=5.091, ppl=34.08, wps=11826.8, ups=0.18, wpb=65536, bsz=128, num_updates=32200, lr=0.000176227, gnorm=0.487, loss_scale=8, train_wall=544, gb_free=8.8, wall=179890
2022-02-13 19:38:01 | INFO | train_inner | epoch 021:    883 / 1576 loss=6.857, nll_loss=5.097, ppl=34.22, wps=11831, ups=0.18, wpb=65536, bsz=128, num_updates=32300, lr=0.000175954, gnorm=0.493, loss_scale=8, train_wall=543, gb_free=8.8, wall=180444
2022-02-13 19:43:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 19:47:21 | INFO | train_inner | epoch 021:    984 / 1576 loss=6.852, nll_loss=5.091, ppl=34.09, wps=11711.2, ups=0.18, wpb=65536, bsz=128, num_updates=32400, lr=0.000175682, gnorm=0.495, loss_scale=8, train_wall=549, gb_free=8.8, wall=181003
2022-02-13 19:56:35 | INFO | train_inner | epoch 021:   1084 / 1576 loss=6.86, nll_loss=5.1, ppl=34.3, wps=11829.9, ups=0.18, wpb=65532.3, bsz=128, num_updates=32500, lr=0.000175412, gnorm=0.503, loss_scale=8, train_wall=543, gb_free=8.8, wall=181557
2022-02-13 20:05:49 | INFO | train_inner | epoch 021:   1184 / 1576 loss=6.87, nll_loss=5.112, ppl=34.59, wps=11830.3, ups=0.18, wpb=65536, bsz=128, num_updates=32600, lr=0.000175142, gnorm=0.491, loss_scale=8, train_wall=544, gb_free=8.8, wall=182111
2022-02-13 20:12:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 20:15:08 | INFO | train_inner | epoch 021:   1285 / 1576 loss=6.877, nll_loss=5.12, ppl=34.78, wps=11710.3, ups=0.18, wpb=65536, bsz=128, num_updates=32700, lr=0.000174874, gnorm=0.493, loss_scale=8, train_wall=549, gb_free=8.8, wall=182671
2022-02-13 20:24:22 | INFO | train_inner | epoch 021:   1385 / 1576 loss=6.871, nll_loss=5.113, ppl=34.6, wps=11831.7, ups=0.18, wpb=65536, bsz=128, num_updates=32800, lr=0.000174608, gnorm=0.494, loss_scale=8, train_wall=543, gb_free=8.8, wall=183225
2022-02-13 20:33:36 | INFO | train_inner | epoch 021:   1485 / 1576 loss=6.876, nll_loss=5.119, ppl=34.75, wps=11833.1, ups=0.18, wpb=65536, bsz=128, num_updates=32900, lr=0.000174342, gnorm=0.496, loss_scale=8, train_wall=543, gb_free=8.8, wall=183778
2022-02-13 20:41:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 20:42:02 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.955 | nll_loss 5.183 | ppl 36.33 | wps 32690.1 | wpb 1021.8 | bsz 2 | num_updates 32991 | best_loss 6.955
2022-02-13 20:42:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 32991 updates
2022-02-13 20:42:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint21.pt
2022-02-13 20:42:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint21.pt
2022-02-13 20:42:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint21.pt (epoch 21 @ 32991 updates, score 6.955) (writing took 28.916901983320713 seconds)
2022-02-13 20:42:31 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-02-13 20:42:31 | INFO | train | epoch 021 | loss 6.847 | nll_loss 5.085 | ppl 33.95 | wps 11744.4 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 32991 | lr 0.000174101 | gnorm 0.496 | loss_scale 16 | train_wall 8561 | gb_free 8.8 | wall 184313
2022-02-13 20:42:31 | INFO | fairseq.trainer | begin training epoch 22
2022-02-13 20:42:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 20:43:21 | INFO | train_inner | epoch 022:      9 / 1576 loss=6.879, nll_loss=5.122, ppl=34.82, wps=11107.6, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=33000, lr=0.000174078, gnorm=0.499, loss_scale=16, train_wall=539, gb_free=8.8, wall=184363
2022-02-13 20:48:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 20:52:40 | INFO | train_inner | epoch 022:    110 / 1576 loss=6.777, nll_loss=5.006, ppl=32.14, wps=11712.6, ups=0.18, wpb=65536, bsz=128, num_updates=33100, lr=0.000173814, gnorm=0.497, loss_scale=8, train_wall=549, gb_free=8.8, wall=184923
2022-02-13 21:01:54 | INFO | train_inner | epoch 022:    210 / 1576 loss=6.801, nll_loss=5.033, ppl=32.75, wps=11831.4, ups=0.18, wpb=65536, bsz=128, num_updates=33200, lr=0.000173553, gnorm=0.503, loss_scale=8, train_wall=543, gb_free=8.8, wall=185477
2022-02-13 21:11:08 | INFO | train_inner | epoch 022:    310 / 1576 loss=6.8, nll_loss=5.032, ppl=32.72, wps=11834.3, ups=0.18, wpb=65536, bsz=128, num_updates=33300, lr=0.000173292, gnorm=0.484, loss_scale=8, train_wall=543, gb_free=8.8, wall=186031
2022-02-13 21:12:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 21:20:27 | INFO | train_inner | epoch 022:    411 / 1576 loss=6.818, nll_loss=5.053, ppl=33.2, wps=11715.4, ups=0.18, wpb=65536, bsz=128, num_updates=33400, lr=0.000173032, gnorm=0.499, loss_scale=8, train_wall=549, gb_free=8.8, wall=186590
2022-02-13 21:29:41 | INFO | train_inner | epoch 022:    511 / 1576 loss=6.827, nll_loss=5.063, ppl=33.44, wps=11830.9, ups=0.18, wpb=65536, bsz=128, num_updates=33500, lr=0.000172774, gnorm=0.5, loss_scale=8, train_wall=543, gb_free=8.8, wall=187144
2022-02-13 21:38:55 | INFO | train_inner | epoch 022:    611 / 1576 loss=6.836, nll_loss=5.073, ppl=33.66, wps=11829.2, ups=0.18, wpb=65536, bsz=128, num_updates=33600, lr=0.000172516, gnorm=0.498, loss_scale=16, train_wall=543, gb_free=8.8, wall=187698
2022-02-13 21:48:10 | INFO | train_inner | epoch 022:    711 / 1576 loss=6.841, nll_loss=5.078, ppl=33.79, wps=11818.2, ups=0.18, wpb=65536, bsz=128, num_updates=33700, lr=0.00017226, gnorm=0.493, loss_scale=16, train_wall=544, gb_free=8.8, wall=188252
2022-02-13 21:55:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 21:57:30 | INFO | train_inner | epoch 022:    812 / 1576 loss=6.844, nll_loss=5.082, ppl=33.87, wps=11703.5, ups=0.18, wpb=65536, bsz=128, num_updates=33800, lr=0.000172005, gnorm=0.5, loss_scale=8, train_wall=549, gb_free=8.8, wall=188812
2022-02-13 22:06:44 | INFO | train_inner | epoch 022:    912 / 1576 loss=6.841, nll_loss=5.079, ppl=33.8, wps=11827.9, ups=0.18, wpb=65532.3, bsz=128, num_updates=33900, lr=0.000171751, gnorm=0.503, loss_scale=8, train_wall=544, gb_free=8.8, wall=189366
2022-02-13 22:15:58 | INFO | train_inner | epoch 022:   1012 / 1576 loss=6.844, nll_loss=5.082, ppl=33.88, wps=11829.9, ups=0.18, wpb=65536, bsz=128, num_updates=34000, lr=0.000171499, gnorm=0.498, loss_scale=8, train_wall=544, gb_free=8.8, wall=189920
2022-02-13 22:19:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 22:25:17 | INFO | train_inner | epoch 022:   1113 / 1576 loss=6.852, nll_loss=5.091, ppl=34.09, wps=11715.6, ups=0.18, wpb=65536, bsz=128, num_updates=34100, lr=0.000171247, gnorm=0.499, loss_scale=8, train_wall=549, gb_free=8.8, wall=190480
2022-02-13 22:34:31 | INFO | train_inner | epoch 022:   1213 / 1576 loss=6.861, nll_loss=5.102, ppl=34.33, wps=11829.6, ups=0.18, wpb=65536, bsz=128, num_updates=34200, lr=0.000170996, gnorm=0.495, loss_scale=8, train_wall=544, gb_free=8.8, wall=191034
2022-02-13 22:43:45 | INFO | train_inner | epoch 022:   1313 / 1576 loss=6.857, nll_loss=5.097, ppl=34.23, wps=11831, ups=0.18, wpb=65536, bsz=128, num_updates=34300, lr=0.000170747, gnorm=0.501, loss_scale=16, train_wall=543, gb_free=8.8, wall=191588
2022-02-13 22:44:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 22:53:05 | INFO | train_inner | epoch 022:   1414 / 1576 loss=6.868, nll_loss=5.11, ppl=34.54, wps=11714.1, ups=0.18, wpb=65536, bsz=128, num_updates=34400, lr=0.000170499, gnorm=0.503, loss_scale=8, train_wall=549, gb_free=8.8, wall=192147
2022-02-13 23:02:19 | INFO | train_inner | epoch 022:   1514 / 1576 loss=6.866, nll_loss=5.107, ppl=34.47, wps=11831.4, ups=0.18, wpb=65536, bsz=128, num_updates=34500, lr=0.000170251, gnorm=0.51, loss_scale=8, train_wall=543, gb_free=8.8, wall=192701
2022-02-13 23:07:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-13 23:08:04 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.951 | nll_loss 5.179 | ppl 36.23 | wps 32666.4 | wpb 1021.8 | bsz 2 | num_updates 34562 | best_loss 6.951
2022-02-13 23:08:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 34562 updates
2022-02-13 23:08:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint22.pt
2022-02-13 23:08:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint22.pt
2022-02-13 23:08:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint22.pt (epoch 22 @ 34562 updates, score 6.951) (writing took 28.927682258188725 seconds)
2022-02-13 23:08:33 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-02-13 23:08:33 | INFO | train | epoch 022 | loss 6.836 | nll_loss 5.074 | ppl 33.68 | wps 11743.8 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 34562 | lr 0.000170099 | gnorm 0.499 | loss_scale 16 | train_wall 8561 | gb_free 8.8 | wall 193075
2022-02-13 23:08:33 | INFO | fairseq.trainer | begin training epoch 23
2022-02-13 23:08:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-13 23:12:03 | INFO | train_inner | epoch 023:     38 / 1576 loss=6.837, nll_loss=5.075, ppl=33.7, wps=11108.2, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=34600, lr=0.000170005, gnorm=0.495, loss_scale=16, train_wall=539, gb_free=8.8, wall=193286
2022-02-13 23:16:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 23:21:23 | INFO | train_inner | epoch 023:    139 / 1576 loss=6.781, nll_loss=5.011, ppl=32.25, wps=11711.4, ups=0.18, wpb=65536, bsz=128, num_updates=34700, lr=0.00016976, gnorm=0.504, loss_scale=8, train_wall=549, gb_free=8.8, wall=193846
2022-02-13 23:30:37 | INFO | train_inner | epoch 023:    239 / 1576 loss=6.787, nll_loss=5.018, ppl=32.4, wps=11830.5, ups=0.18, wpb=65536, bsz=128, num_updates=34800, lr=0.000169516, gnorm=0.497, loss_scale=8, train_wall=543, gb_free=8.8, wall=194400
2022-02-13 23:39:51 | INFO | train_inner | epoch 023:    339 / 1576 loss=6.807, nll_loss=5.04, ppl=32.91, wps=11828.7, ups=0.18, wpb=65536, bsz=128, num_updates=34900, lr=0.000169273, gnorm=0.506, loss_scale=8, train_wall=544, gb_free=8.8, wall=194954
2022-02-13 23:46:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-13 23:49:11 | INFO | train_inner | epoch 023:    440 / 1576 loss=6.809, nll_loss=5.043, ppl=32.97, wps=11708.6, ups=0.18, wpb=65536, bsz=128, num_updates=35000, lr=0.000169031, gnorm=0.501, loss_scale=8, train_wall=549, gb_free=8.8, wall=195513
2022-02-13 23:58:25 | INFO | train_inner | epoch 023:    540 / 1576 loss=6.815, nll_loss=5.05, ppl=33.13, wps=11829.5, ups=0.18, wpb=65536, bsz=128, num_updates=35100, lr=0.00016879, gnorm=0.496, loss_scale=8, train_wall=543, gb_free=8.8, wall=196067
2022-02-14 00:07:39 | INFO | train_inner | epoch 023:    640 / 1576 loss=6.818, nll_loss=5.052, ppl=33.18, wps=11828.3, ups=0.18, wpb=65536, bsz=128, num_updates=35200, lr=0.00016855, gnorm=0.507, loss_scale=8, train_wall=544, gb_free=8.8, wall=196621
2022-02-14 00:14:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 00:16:59 | INFO | train_inner | epoch 023:    741 / 1576 loss=6.827, nll_loss=5.063, ppl=33.44, wps=11710, ups=0.18, wpb=65536, bsz=128, num_updates=35300, lr=0.000168311, gnorm=0.495, loss_scale=8, train_wall=549, gb_free=8.8, wall=197181
2022-02-14 00:26:13 | INFO | train_inner | epoch 023:    841 / 1576 loss=6.829, nll_loss=5.065, ppl=33.48, wps=11822.1, ups=0.18, wpb=65536, bsz=128, num_updates=35400, lr=0.000168073, gnorm=0.505, loss_scale=8, train_wall=544, gb_free=8.8, wall=197735
2022-02-14 00:35:27 | INFO | train_inner | epoch 023:    941 / 1576 loss=6.836, nll_loss=5.073, ppl=33.67, wps=11826.3, ups=0.18, wpb=65536, bsz=128, num_updates=35500, lr=0.000167836, gnorm=0.499, loss_scale=8, train_wall=544, gb_free=8.8, wall=198290
2022-02-14 00:44:42 | INFO | train_inner | epoch 023:   1041 / 1576 loss=6.842, nll_loss=5.08, ppl=33.81, wps=11816.8, ups=0.18, wpb=65532.3, bsz=128, num_updates=35600, lr=0.0001676, gnorm=0.507, loss_scale=16, train_wall=544, gb_free=8.8, wall=198844
2022-02-14 00:48:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 00:54:01 | INFO | train_inner | epoch 023:   1142 / 1576 loss=6.845, nll_loss=5.083, ppl=33.9, wps=11706.2, ups=0.18, wpb=65536, bsz=128, num_updates=35700, lr=0.000167365, gnorm=0.507, loss_scale=8, train_wall=549, gb_free=8.8, wall=199404
2022-02-14 01:03:15 | INFO | train_inner | epoch 023:   1242 / 1576 loss=6.852, nll_loss=5.091, ppl=34.09, wps=11829.4, ups=0.18, wpb=65536, bsz=128, num_updates=35800, lr=0.000167132, gnorm=0.492, loss_scale=8, train_wall=544, gb_free=8.8, wall=199958
2022-02-14 01:12:30 | INFO | train_inner | epoch 023:   1342 / 1576 loss=6.843, nll_loss=5.081, ppl=33.84, wps=11827.4, ups=0.18, wpb=65536, bsz=128, num_updates=35900, lr=0.000166899, gnorm=0.502, loss_scale=16, train_wall=544, gb_free=8.8, wall=200512
2022-02-14 01:15:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 01:21:49 | INFO | train_inner | epoch 023:   1443 / 1576 loss=6.863, nll_loss=5.104, ppl=34.38, wps=11712.2, ups=0.18, wpb=65536, bsz=128, num_updates=36000, lr=0.000166667, gnorm=0.502, loss_scale=8, train_wall=549, gb_free=8.8, wall=201072
2022-02-14 01:31:03 | INFO | train_inner | epoch 023:   1543 / 1576 loss=6.859, nll_loss=5.1, ppl=34.29, wps=11832, ups=0.18, wpb=65536, bsz=128, num_updates=36100, lr=0.000166436, gnorm=0.5, loss_scale=8, train_wall=543, gb_free=8.8, wall=201626
2022-02-14 01:34:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 01:34:08 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.949 | nll_loss 5.173 | ppl 36.07 | wps 32694.8 | wpb 1021.8 | bsz 2 | num_updates 36133 | best_loss 6.949
2022-02-14 01:34:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 36133 updates
2022-02-14 01:34:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint23.pt
2022-02-14 01:34:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint23.pt
2022-02-14 01:34:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint23.pt (epoch 23 @ 36133 updates, score 6.949) (writing took 32.190693432465196 seconds)
2022-02-14 01:34:40 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-02-14 01:34:40 | INFO | train | epoch 023 | loss 6.827 | nll_loss 5.063 | ppl 33.43 | wps 11737 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 36133 | lr 0.00016636 | gnorm 0.502 | loss_scale 8 | train_wall 8563 | gb_free 8.8 | wall 201843
2022-02-14 01:34:40 | INFO | fairseq.trainer | begin training epoch 24
2022-02-14 01:34:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 01:40:51 | INFO | train_inner | epoch 024:     67 / 1576 loss=6.792, nll_loss=5.023, ppl=32.51, wps=11043.8, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=36200, lr=0.000166206, gnorm=0.514, loss_scale=16, train_wall=539, gb_free=8.8, wall=202214
2022-02-14 01:41:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 01:50:11 | INFO | train_inner | epoch 024:    168 / 1576 loss=6.78, nll_loss=5.01, ppl=32.21, wps=11711.3, ups=0.18, wpb=65536, bsz=128, num_updates=36300, lr=0.000165977, gnorm=0.504, loss_scale=8, train_wall=549, gb_free=8.8, wall=202773
2022-02-14 01:59:25 | INFO | train_inner | epoch 024:    268 / 1576 loss=6.784, nll_loss=5.014, ppl=32.31, wps=11830.9, ups=0.18, wpb=65536, bsz=128, num_updates=36400, lr=0.000165748, gnorm=0.501, loss_scale=8, train_wall=543, gb_free=8.8, wall=203327
2022-02-14 02:08:39 | INFO | train_inner | epoch 024:    368 / 1576 loss=6.792, nll_loss=5.023, ppl=32.52, wps=11826.5, ups=0.18, wpb=65536, bsz=128, num_updates=36500, lr=0.000165521, gnorm=0.516, loss_scale=16, train_wall=544, gb_free=8.8, wall=203881
2022-02-14 02:17:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 02:17:59 | INFO | train_inner | epoch 024:    469 / 1576 loss=6.798, nll_loss=5.03, ppl=32.67, wps=11708.3, ups=0.18, wpb=65536, bsz=128, num_updates=36600, lr=0.000165295, gnorm=0.502, loss_scale=8, train_wall=549, gb_free=8.8, wall=204441
2022-02-14 02:27:13 | INFO | train_inner | epoch 024:    569 / 1576 loss=6.807, nll_loss=5.04, ppl=32.91, wps=11829.1, ups=0.18, wpb=65536, bsz=128, num_updates=36700, lr=0.00016507, gnorm=0.508, loss_scale=8, train_wall=544, gb_free=8.8, wall=204995
2022-02-14 02:36:26 | INFO | train_inner | epoch 024:    669 / 1576 loss=6.819, nll_loss=5.054, ppl=33.22, wps=11834.1, ups=0.18, wpb=65536, bsz=128, num_updates=36800, lr=0.000164845, gnorm=0.504, loss_scale=8, train_wall=543, gb_free=8.8, wall=205549
2022-02-14 02:45:41 | INFO | train_inner | epoch 024:    769 / 1576 loss=6.831, nll_loss=5.067, ppl=33.53, wps=11828, ups=0.18, wpb=65536, bsz=128, num_updates=36900, lr=0.000164622, gnorm=0.502, loss_scale=16, train_wall=544, gb_free=8.8, wall=206103
2022-02-14 02:46:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 02:55:00 | INFO | train_inner | epoch 024:    870 / 1576 loss=6.824, nll_loss=5.06, ppl=33.36, wps=11715, ups=0.18, wpb=65536, bsz=128, num_updates=37000, lr=0.000164399, gnorm=0.508, loss_scale=8, train_wall=549, gb_free=8.8, wall=206662
2022-02-14 03:04:14 | INFO | train_inner | epoch 024:    970 / 1576 loss=6.829, nll_loss=5.065, ppl=33.48, wps=11824.3, ups=0.18, wpb=65536, bsz=128, num_updates=37100, lr=0.000164177, gnorm=0.504, loss_scale=8, train_wall=544, gb_free=8.8, wall=207217
2022-02-14 03:13:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 03:13:34 | INFO | train_inner | epoch 024:   1071 / 1576 loss=6.832, nll_loss=5.068, ppl=33.56, wps=11707.3, ups=0.18, wpb=65536, bsz=128, num_updates=37200, lr=0.000163956, gnorm=0.504, loss_scale=8, train_wall=549, gb_free=8.8, wall=207777
2022-02-14 03:22:48 | INFO | train_inner | epoch 024:   1171 / 1576 loss=6.835, nll_loss=5.072, ppl=33.63, wps=11832.3, ups=0.18, wpb=65536, bsz=128, num_updates=37300, lr=0.000163737, gnorm=0.512, loss_scale=8, train_wall=543, gb_free=8.8, wall=208330
2022-02-14 03:32:02 | INFO | train_inner | epoch 024:   1271 / 1576 loss=6.849, nll_loss=5.088, ppl=34.01, wps=11828.8, ups=0.18, wpb=65536, bsz=128, num_updates=37400, lr=0.000163517, gnorm=0.515, loss_scale=8, train_wall=544, gb_free=8.8, wall=208884
2022-02-14 03:39:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 03:41:22 | INFO | train_inner | epoch 024:   1372 / 1576 loss=6.856, nll_loss=5.096, ppl=34.19, wps=11709.7, ups=0.18, wpb=65536, bsz=128, num_updates=37500, lr=0.000163299, gnorm=0.512, loss_scale=8, train_wall=549, gb_free=8.8, wall=209444
2022-02-14 03:50:36 | INFO | train_inner | epoch 024:   1472 / 1576 loss=6.843, nll_loss=5.082, ppl=33.87, wps=11831, ups=0.18, wpb=65536, bsz=128, num_updates=37600, lr=0.000163082, gnorm=0.505, loss_scale=8, train_wall=543, gb_free=8.8, wall=209998
2022-02-14 03:59:49 | INFO | train_inner | epoch 024:   1572 / 1576 loss=6.849, nll_loss=5.089, ppl=34.03, wps=11830.6, ups=0.18, wpb=65532.3, bsz=128, num_updates=37700, lr=0.000162866, gnorm=0.525, loss_scale=8, train_wall=543, gb_free=8.8, wall=210552
2022-02-14 04:00:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 04:00:13 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.944 | nll_loss 5.177 | ppl 36.17 | wps 32666.9 | wpb 1021.8 | bsz 2 | num_updates 37704 | best_loss 6.944
2022-02-14 04:00:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 37704 updates
2022-02-14 04:00:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint24.pt
2022-02-14 04:00:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint24.pt
2022-02-14 04:00:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint24.pt (epoch 24 @ 37704 updates, score 6.944) (writing took 29.343236503191292 seconds)
2022-02-14 04:00:43 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-02-14 04:00:43 | INFO | train | epoch 024 | loss 6.819 | nll_loss 5.054 | ppl 33.23 | wps 11742.7 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 37704 | lr 0.000162857 | gnorm 0.509 | loss_scale 8 | train_wall 8561 | gb_free 8.8 | wall 210605
2022-02-14 04:00:43 | INFO | fairseq.trainer | begin training epoch 25
2022-02-14 04:00:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 04:09:35 | INFO | train_inner | epoch 025:     96 / 1576 loss=6.765, nll_loss=4.993, ppl=31.84, wps=11095.3, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=37800, lr=0.00016265, gnorm=0.519, loss_scale=16, train_wall=539, gb_free=8.8, wall=211137
2022-02-14 04:16:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 04:18:55 | INFO | train_inner | epoch 025:    197 / 1576 loss=6.768, nll_loss=4.997, ppl=31.92, wps=11707.4, ups=0.18, wpb=65536, bsz=128, num_updates=37900, lr=0.000162435, gnorm=0.504, loss_scale=8, train_wall=549, gb_free=8.8, wall=211697
2022-02-14 04:28:09 | INFO | train_inner | epoch 025:    297 / 1576 loss=6.776, nll_loss=5.005, ppl=32.12, wps=11828.7, ups=0.18, wpb=65536, bsz=128, num_updates=38000, lr=0.000162221, gnorm=0.5, loss_scale=8, train_wall=544, gb_free=8.8, wall=212251
2022-02-14 04:37:23 | INFO | train_inner | epoch 025:    397 / 1576 loss=6.79, nll_loss=5.021, ppl=32.48, wps=11829.5, ups=0.18, wpb=65536, bsz=128, num_updates=38100, lr=0.000162008, gnorm=0.513, loss_scale=8, train_wall=543, gb_free=8.8, wall=212805
2022-02-14 04:41:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 04:46:42 | INFO | train_inner | epoch 025:    498 / 1576 loss=6.796, nll_loss=5.028, ppl=32.63, wps=11713.2, ups=0.18, wpb=65536, bsz=128, num_updates=38200, lr=0.000161796, gnorm=0.501, loss_scale=8, train_wall=549, gb_free=8.8, wall=213365
2022-02-14 04:55:56 | INFO | train_inner | epoch 025:    598 / 1576 loss=6.803, nll_loss=5.035, ppl=32.79, wps=11825.7, ups=0.18, wpb=65536, bsz=128, num_updates=38300, lr=0.000161585, gnorm=0.508, loss_scale=8, train_wall=544, gb_free=8.8, wall=213919
2022-02-14 05:05:11 | INFO | train_inner | epoch 025:    698 / 1576 loss=6.803, nll_loss=5.036, ppl=32.81, wps=11825, ups=0.18, wpb=65536, bsz=128, num_updates=38400, lr=0.000161374, gnorm=0.5, loss_scale=16, train_wall=544, gb_free=8.8, wall=214473
2022-02-14 05:11:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 05:14:30 | INFO | train_inner | epoch 025:    799 / 1576 loss=6.813, nll_loss=5.047, ppl=33.06, wps=11709.3, ups=0.18, wpb=65536, bsz=128, num_updates=38500, lr=0.000161165, gnorm=0.511, loss_scale=8, train_wall=549, gb_free=8.8, wall=215033
2022-02-14 05:23:44 | INFO | train_inner | epoch 025:    899 / 1576 loss=6.821, nll_loss=5.056, ppl=33.27, wps=11829.1, ups=0.18, wpb=65536, bsz=128, num_updates=38600, lr=0.000160956, gnorm=0.507, loss_scale=8, train_wall=544, gb_free=8.8, wall=215587
2022-02-14 05:32:59 | INFO | train_inner | epoch 025:    999 / 1576 loss=6.826, nll_loss=5.062, ppl=33.41, wps=11826.4, ups=0.18, wpb=65532.3, bsz=128, num_updates=38700, lr=0.000160748, gnorm=0.51, loss_scale=8, train_wall=544, gb_free=8.8, wall=216141
2022-02-14 05:34:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 05:42:18 | INFO | train_inner | epoch 025:   1100 / 1576 loss=6.827, nll_loss=5.063, ppl=33.43, wps=11714.5, ups=0.18, wpb=65536, bsz=128, num_updates=38800, lr=0.00016054, gnorm=0.522, loss_scale=8, train_wall=549, gb_free=8.8, wall=216700
2022-02-14 05:51:32 | INFO | train_inner | epoch 025:   1200 / 1576 loss=6.826, nll_loss=5.063, ppl=33.42, wps=11836.5, ups=0.18, wpb=65536, bsz=128, num_updates=38900, lr=0.000160334, gnorm=0.514, loss_scale=8, train_wall=543, gb_free=8.8, wall=217254
2022-02-14 06:00:46 | INFO | train_inner | epoch 025:   1300 / 1576 loss=6.836, nll_loss=5.074, ppl=33.68, wps=11825.3, ups=0.18, wpb=65536, bsz=128, num_updates=39000, lr=0.000160128, gnorm=0.504, loss_scale=16, train_wall=544, gb_free=8.8, wall=217808
2022-02-14 06:03:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 06:10:06 | INFO | train_inner | epoch 025:   1401 / 1576 loss=6.842, nll_loss=5.08, ppl=33.83, wps=11705.1, ups=0.18, wpb=65536, bsz=128, num_updates=39100, lr=0.000159923, gnorm=0.508, loss_scale=8, train_wall=549, gb_free=8.8, wall=218368
2022-02-14 06:19:20 | INFO | train_inner | epoch 025:   1501 / 1576 loss=6.852, nll_loss=5.092, ppl=34.11, wps=11829.9, ups=0.18, wpb=65536, bsz=128, num_updates=39200, lr=0.000159719, gnorm=0.502, loss_scale=8, train_wall=544, gb_free=8.8, wall=218922
2022-02-14 06:26:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 06:26:17 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.938 | nll_loss 5.158 | ppl 35.71 | wps 32668.3 | wpb 1021.8 | bsz 2 | num_updates 39275 | best_loss 6.938
2022-02-14 06:26:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 39275 updates
2022-02-14 06:26:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint25.pt
2022-02-14 06:26:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint25.pt
2022-02-14 06:26:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint25.pt (epoch 25 @ 39275 updates, score 6.938) (writing took 29.149302852340043 seconds)
2022-02-14 06:26:46 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-02-14 06:26:46 | INFO | train | epoch 025 | loss 6.811 | nll_loss 5.045 | ppl 33.02 | wps 11741.9 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 39275 | lr 0.000159567 | gnorm 0.508 | loss_scale 8 | train_wall 8562 | gb_free 8.8 | wall 219369
2022-02-14 06:26:46 | INFO | fairseq.trainer | begin training epoch 26
2022-02-14 06:26:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 06:27:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 06:29:10 | INFO | train_inner | epoch 026:     26 / 1576 loss=6.815, nll_loss=5.05, ppl=33.13, wps=10999.7, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=39300, lr=0.000159516, gnorm=0.525, loss_scale=8, train_wall=544, gb_free=8.8, wall=219513
2022-02-14 06:38:24 | INFO | train_inner | epoch 026:    126 / 1576 loss=6.76, nll_loss=4.988, ppl=31.73, wps=11831.4, ups=0.18, wpb=65536, bsz=128, num_updates=39400, lr=0.000159313, gnorm=0.508, loss_scale=8, train_wall=543, gb_free=8.8, wall=220067
2022-02-14 06:47:38 | INFO | train_inner | epoch 026:    226 / 1576 loss=6.774, nll_loss=5.003, ppl=32.07, wps=11833, ups=0.18, wpb=65536, bsz=128, num_updates=39500, lr=0.000159111, gnorm=0.509, loss_scale=8, train_wall=543, gb_free=8.8, wall=220621
2022-02-14 06:55:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 06:56:58 | INFO | train_inner | epoch 026:    327 / 1576 loss=6.777, nll_loss=5.006, ppl=32.13, wps=11703.3, ups=0.18, wpb=65536, bsz=128, num_updates=39600, lr=0.00015891, gnorm=0.507, loss_scale=8, train_wall=549, gb_free=8.8, wall=221181
2022-02-14 07:06:12 | INFO | train_inner | epoch 026:    427 / 1576 loss=6.778, nll_loss=5.008, ppl=32.17, wps=11825.1, ups=0.18, wpb=65536, bsz=128, num_updates=39700, lr=0.00015871, gnorm=0.516, loss_scale=8, train_wall=544, gb_free=8.8, wall=221735
2022-02-14 07:15:26 | INFO | train_inner | epoch 026:    527 / 1576 loss=6.794, nll_loss=5.026, ppl=32.57, wps=11826.3, ups=0.18, wpb=65536, bsz=128, num_updates=39800, lr=0.000158511, gnorm=0.518, loss_scale=8, train_wall=544, gb_free=8.8, wall=222289
2022-02-14 07:23:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 07:24:46 | INFO | train_inner | epoch 026:    628 / 1576 loss=6.808, nll_loss=5.042, ppl=32.94, wps=11708.7, ups=0.18, wpb=65536, bsz=128, num_updates=39900, lr=0.000158312, gnorm=0.515, loss_scale=8, train_wall=549, gb_free=8.8, wall=222849
2022-02-14 07:34:00 | INFO | train_inner | epoch 026:    728 / 1576 loss=6.792, nll_loss=5.024, ppl=32.53, wps=11826, ups=0.18, wpb=65532.3, bsz=128, num_updates=40000, lr=0.000158114, gnorm=0.51, loss_scale=8, train_wall=544, gb_free=8.8, wall=223403
2022-02-14 07:43:15 | INFO | train_inner | epoch 026:    828 / 1576 loss=6.808, nll_loss=5.041, ppl=32.93, wps=11824.5, ups=0.18, wpb=65536, bsz=128, num_updates=40100, lr=0.000157917, gnorm=0.514, loss_scale=8, train_wall=544, gb_free=8.8, wall=223957
2022-02-14 07:50:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 07:52:34 | INFO | train_inner | epoch 026:    929 / 1576 loss=6.822, nll_loss=5.057, ppl=33.29, wps=11703.3, ups=0.18, wpb=65536, bsz=128, num_updates=40200, lr=0.00015772, gnorm=0.505, loss_scale=8, train_wall=549, gb_free=8.8, wall=224517
2022-02-14 08:01:49 | INFO | train_inner | epoch 026:   1029 / 1576 loss=6.824, nll_loss=5.06, ppl=33.35, wps=11828.9, ups=0.18, wpb=65536, bsz=128, num_updates=40300, lr=0.000157524, gnorm=0.504, loss_scale=8, train_wall=544, gb_free=8.8, wall=225071
2022-02-14 08:11:03 | INFO | train_inner | epoch 026:   1129 / 1576 loss=6.819, nll_loss=5.054, ppl=33.22, wps=11828.7, ups=0.18, wpb=65536, bsz=128, num_updates=40400, lr=0.000157329, gnorm=0.506, loss_scale=8, train_wall=544, gb_free=8.8, wall=225625
2022-02-14 08:16:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 08:20:22 | INFO | train_inner | epoch 026:   1230 / 1576 loss=6.822, nll_loss=5.057, ppl=33.3, wps=11713.7, ups=0.18, wpb=65536, bsz=128, num_updates=40500, lr=0.000157135, gnorm=0.513, loss_scale=8, train_wall=549, gb_free=8.8, wall=226185
2022-02-14 08:29:36 | INFO | train_inner | epoch 026:   1330 / 1576 loss=6.84, nll_loss=5.078, ppl=33.77, wps=11833.3, ups=0.18, wpb=65536, bsz=128, num_updates=40600, lr=0.000156941, gnorm=0.521, loss_scale=8, train_wall=543, gb_free=8.8, wall=226738
2022-02-14 08:38:50 | INFO | train_inner | epoch 026:   1430 / 1576 loss=6.825, nll_loss=5.06, ppl=33.37, wps=11829.6, ups=0.18, wpb=65536, bsz=128, num_updates=40700, lr=0.000156748, gnorm=0.511, loss_scale=8, train_wall=544, gb_free=8.8, wall=227292
2022-02-14 08:41:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 08:48:09 | INFO | train_inner | epoch 026:   1531 / 1576 loss=6.83, nll_loss=5.067, ppl=33.51, wps=11713.3, ups=0.18, wpb=65536, bsz=128, num_updates=40800, lr=0.000156556, gnorm=0.505, loss_scale=8, train_wall=549, gb_free=8.8, wall=227852
2022-02-14 08:52:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 08:52:20 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.936 | nll_loss 5.164 | ppl 35.86 | wps 32567.1 | wpb 1021.8 | bsz 2 | num_updates 40845 | best_loss 6.936
2022-02-14 08:52:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 40845 updates
2022-02-14 08:52:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint26.pt
2022-02-14 08:52:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint26.pt
2022-02-14 08:52:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint26.pt (epoch 26 @ 40845 updates, score 6.936) (writing took 28.930209091864526 seconds)
2022-02-14 08:52:49 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-02-14 08:52:49 | INFO | train | epoch 026 | loss 6.804 | nll_loss 5.038 | ppl 32.84 | wps 11734.9 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 40845 | lr 0.00015647 | gnorm 0.512 | loss_scale 8 | train_wall 8562 | gb_free 8.8 | wall 228132
2022-02-14 08:52:49 | INFO | fairseq.trainer | begin training epoch 27
2022-02-14 08:52:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 08:57:54 | INFO | train_inner | epoch 027:     55 / 1576 loss=6.786, nll_loss=5.016, ppl=32.36, wps=11106.5, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=40900, lr=0.000156365, gnorm=0.529, loss_scale=8, train_wall=539, gb_free=8.8, wall=228437
2022-02-14 09:07:08 | INFO | train_inner | epoch 027:    155 / 1576 loss=6.752, nll_loss=4.978, ppl=31.52, wps=11829.4, ups=0.18, wpb=65536, bsz=128, num_updates=41000, lr=0.000156174, gnorm=0.517, loss_scale=16, train_wall=543, gb_free=8.8, wall=228991
2022-02-14 09:16:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 09:16:28 | INFO | train_inner | epoch 027:    256 / 1576 loss=6.772, nll_loss=5.001, ppl=32.01, wps=11709.6, ups=0.18, wpb=65536, bsz=128, num_updates=41100, lr=0.000155984, gnorm=0.505, loss_scale=8, train_wall=549, gb_free=8.8, wall=229550
2022-02-14 09:25:42 | INFO | train_inner | epoch 027:    356 / 1576 loss=6.772, nll_loss=5.001, ppl=32.02, wps=11827.7, ups=0.18, wpb=65536, bsz=128, num_updates=41200, lr=0.000155794, gnorm=0.528, loss_scale=8, train_wall=544, gb_free=8.8, wall=230105
2022-02-14 09:34:56 | INFO | train_inner | epoch 027:    456 / 1576 loss=6.787, nll_loss=5.018, ppl=32.4, wps=11827.6, ups=0.18, wpb=65536, bsz=128, num_updates=41300, lr=0.000155606, gnorm=0.515, loss_scale=8, train_wall=544, gb_free=8.8, wall=230659
2022-02-14 09:41:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 09:44:16 | INFO | train_inner | epoch 027:    557 / 1576 loss=6.785, nll_loss=5.015, ppl=32.34, wps=11712.3, ups=0.18, wpb=65536, bsz=128, num_updates=41400, lr=0.000155417, gnorm=0.516, loss_scale=8, train_wall=549, gb_free=8.8, wall=231218
2022-02-14 09:53:30 | INFO | train_inner | epoch 027:    657 / 1576 loss=6.795, nll_loss=5.027, ppl=32.61, wps=11832.2, ups=0.18, wpb=65536, bsz=128, num_updates=41500, lr=0.00015523, gnorm=0.518, loss_scale=8, train_wall=543, gb_free=8.8, wall=231772
2022-02-14 10:02:44 | INFO | train_inner | epoch 027:    757 / 1576 loss=6.797, nll_loss=5.03, ppl=32.66, wps=11827.3, ups=0.18, wpb=65536, bsz=128, num_updates=41600, lr=0.000155043, gnorm=0.506, loss_scale=8, train_wall=544, gb_free=8.8, wall=232326
2022-02-14 10:07:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 10:12:03 | INFO | train_inner | epoch 027:    858 / 1576 loss=6.794, nll_loss=5.026, ppl=32.58, wps=11710.6, ups=0.18, wpb=65536, bsz=128, num_updates=41700, lr=0.000154857, gnorm=0.521, loss_scale=8, train_wall=549, gb_free=8.8, wall=232886
2022-02-14 10:21:17 | INFO | train_inner | epoch 027:    958 / 1576 loss=6.813, nll_loss=5.048, ppl=33.08, wps=11829.4, ups=0.18, wpb=65536, bsz=128, num_updates=41800, lr=0.000154672, gnorm=0.516, loss_scale=8, train_wall=544, gb_free=8.8, wall=233440
2022-02-14 10:30:32 | INFO | train_inner | epoch 027:   1058 / 1576 loss=6.813, nll_loss=5.047, ppl=33.07, wps=11823, ups=0.18, wpb=65536, bsz=128, num_updates=41900, lr=0.000154487, gnorm=0.523, loss_scale=8, train_wall=544, gb_free=8.8, wall=233994
2022-02-14 10:39:46 | INFO | train_inner | epoch 027:   1158 / 1576 loss=6.829, nll_loss=5.065, ppl=33.48, wps=11817.2, ups=0.18, wpb=65536, bsz=128, num_updates=42000, lr=0.000154303, gnorm=0.517, loss_scale=16, train_wall=544, gb_free=8.8, wall=234549
2022-02-14 10:40:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 10:49:06 | INFO | train_inner | epoch 027:   1259 / 1576 loss=6.807, nll_loss=5.04, ppl=32.91, wps=11715.2, ups=0.18, wpb=65536, bsz=128, num_updates=42100, lr=0.00015412, gnorm=0.522, loss_scale=8, train_wall=549, gb_free=8.8, wall=235108
2022-02-14 10:58:20 | INFO | train_inner | epoch 027:   1359 / 1576 loss=6.82, nll_loss=5.055, ppl=33.25, wps=11828.6, ups=0.18, wpb=65532.3, bsz=128, num_updates=42200, lr=0.000153937, gnorm=0.519, loss_scale=8, train_wall=544, gb_free=8.8, wall=235662
2022-02-14 11:07:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 11:07:39 | INFO | train_inner | epoch 027:   1460 / 1576 loss=6.819, nll_loss=5.055, ppl=33.24, wps=11708.9, ups=0.18, wpb=65536, bsz=128, num_updates=42300, lr=0.000153755, gnorm=0.506, loss_scale=8, train_wall=549, gb_free=8.8, wall=236222
2022-02-14 11:16:53 | INFO | train_inner | epoch 027:   1560 / 1576 loss=6.823, nll_loss=5.059, ppl=33.33, wps=11827.2, ups=0.18, wpb=65536, bsz=128, num_updates=42400, lr=0.000153574, gnorm=0.514, loss_scale=8, train_wall=544, gb_free=8.8, wall=236776
2022-02-14 11:18:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 11:18:24 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.933 | nll_loss 5.157 | ppl 35.69 | wps 32615.4 | wpb 1021.8 | bsz 2 | num_updates 42416 | best_loss 6.933
2022-02-14 11:18:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 42416 updates
2022-02-14 11:18:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint27.pt
2022-02-14 11:18:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint27.pt
2022-02-14 11:18:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint27.pt (epoch 27 @ 42416 updates, score 6.933) (writing took 28.96003761421889 seconds)
2022-02-14 11:18:53 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-02-14 11:18:53 | INFO | train | epoch 027 | loss 6.797 | nll_loss 5.029 | ppl 32.66 | wps 11741.7 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 42416 | lr 0.000153545 | gnorm 0.517 | loss_scale 8 | train_wall 8562 | gb_free 8.8 | wall 236896
2022-02-14 11:18:53 | INFO | fairseq.trainer | begin training epoch 28
2022-02-14 11:18:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 11:26:38 | INFO | train_inner | epoch 028:     84 / 1576 loss=6.764, nll_loss=4.992, ppl=31.83, wps=11108.4, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=42500, lr=0.000153393, gnorm=0.515, loss_scale=8, train_wall=539, gb_free=8.8, wall=237361
2022-02-14 11:33:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 11:35:58 | INFO | train_inner | epoch 028:    185 / 1576 loss=6.737, nll_loss=4.961, ppl=31.15, wps=11709.8, ups=0.18, wpb=65536, bsz=128, num_updates=42600, lr=0.000153213, gnorm=0.52, loss_scale=8, train_wall=549, gb_free=8.8, wall=237920
2022-02-14 11:45:12 | INFO | train_inner | epoch 028:    285 / 1576 loss=6.762, nll_loss=4.99, ppl=31.77, wps=11827, ups=0.18, wpb=65536, bsz=128, num_updates=42700, lr=0.000153033, gnorm=0.507, loss_scale=8, train_wall=544, gb_free=8.8, wall=238475
2022-02-14 11:54:26 | INFO | train_inner | epoch 028:    385 / 1576 loss=6.772, nll_loss=5.001, ppl=32.02, wps=11830.5, ups=0.18, wpb=65536, bsz=128, num_updates=42800, lr=0.000152854, gnorm=0.517, loss_scale=8, train_wall=543, gb_free=8.8, wall=239029
2022-02-14 12:00:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 12:03:46 | INFO | train_inner | epoch 028:    486 / 1576 loss=6.776, nll_loss=5.006, ppl=32.13, wps=11710.4, ups=0.18, wpb=65536, bsz=128, num_updates=42900, lr=0.000152676, gnorm=0.514, loss_scale=8, train_wall=549, gb_free=8.8, wall=239588
2022-02-14 12:13:00 | INFO | train_inner | epoch 028:    586 / 1576 loss=6.782, nll_loss=5.013, ppl=32.28, wps=11829.1, ups=0.18, wpb=65536, bsz=128, num_updates=43000, lr=0.000152499, gnorm=0.522, loss_scale=8, train_wall=544, gb_free=8.8, wall=240142
2022-02-14 12:22:14 | INFO | train_inner | epoch 028:    686 / 1576 loss=6.786, nll_loss=5.017, ppl=32.38, wps=11829.6, ups=0.18, wpb=65536, bsz=128, num_updates=43100, lr=0.000152322, gnorm=0.514, loss_scale=8, train_wall=544, gb_free=8.8, wall=240696
2022-02-14 12:24:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 12:31:33 | INFO | train_inner | epoch 028:    787 / 1576 loss=6.798, nll_loss=5.03, ppl=32.68, wps=11712.5, ups=0.18, wpb=65536, bsz=128, num_updates=43200, lr=0.000152145, gnorm=0.532, loss_scale=8, train_wall=549, gb_free=8.8, wall=241256
2022-02-14 12:40:47 | INFO | train_inner | epoch 028:    887 / 1576 loss=6.807, nll_loss=5.04, ppl=32.9, wps=11831.3, ups=0.18, wpb=65536, bsz=128, num_updates=43300, lr=0.000151969, gnorm=0.539, loss_scale=8, train_wall=543, gb_free=8.8, wall=241810
2022-02-14 12:50:01 | INFO | train_inner | epoch 028:    987 / 1576 loss=6.796, nll_loss=5.028, ppl=32.63, wps=11830.9, ups=0.18, wpb=65536, bsz=128, num_updates=43400, lr=0.000151794, gnorm=0.524, loss_scale=16, train_wall=544, gb_free=8.8, wall=242364
2022-02-14 12:52:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 12:59:21 | INFO | train_inner | epoch 028:   1088 / 1576 loss=6.805, nll_loss=5.038, ppl=32.86, wps=11713.9, ups=0.18, wpb=65532.3, bsz=128, num_updates=43500, lr=0.00015162, gnorm=0.525, loss_scale=8, train_wall=549, gb_free=8.8, wall=242923
2022-02-14 13:08:34 | INFO | train_inner | epoch 028:   1188 / 1576 loss=6.814, nll_loss=5.049, ppl=33.11, wps=11833.8, ups=0.18, wpb=65536, bsz=128, num_updates=43600, lr=0.000151446, gnorm=0.523, loss_scale=8, train_wall=543, gb_free=8.8, wall=243477
2022-02-14 13:17:48 | INFO | train_inner | epoch 028:   1288 / 1576 loss=6.809, nll_loss=5.043, ppl=32.98, wps=11829.8, ups=0.18, wpb=65536, bsz=128, num_updates=43700, lr=0.000151272, gnorm=0.522, loss_scale=16, train_wall=544, gb_free=8.8, wall=244031
2022-02-14 13:18:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 13:27:08 | INFO | train_inner | epoch 028:   1389 / 1576 loss=6.811, nll_loss=5.046, ppl=33.03, wps=11715.2, ups=0.18, wpb=65536, bsz=128, num_updates=43800, lr=0.000151099, gnorm=0.518, loss_scale=8, train_wall=549, gb_free=8.8, wall=244590
2022-02-14 13:36:22 | INFO | train_inner | epoch 028:   1489 / 1576 loss=6.819, nll_loss=5.055, ppl=33.23, wps=11829.3, ups=0.18, wpb=65536, bsz=128, num_updates=43900, lr=0.000150927, gnorm=0.519, loss_scale=8, train_wall=544, gb_free=8.8, wall=245144
2022-02-14 13:42:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 13:44:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 13:44:25 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.929 | nll_loss 5.147 | ppl 35.43 | wps 32637.9 | wpb 1021.8 | bsz 2 | num_updates 43986 | best_loss 6.929
2022-02-14 13:44:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 43986 updates
2022-02-14 13:44:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint28.pt
2022-02-14 13:44:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint28.pt
2022-02-14 13:44:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint28.pt (epoch 28 @ 43986 updates, score 6.929) (writing took 28.70554279536009 seconds)
2022-02-14 13:44:54 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-02-14 13:44:54 | INFO | train | epoch 028 | loss 6.791 | nll_loss 5.022 | ppl 32.5 | wps 11737.4 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 43986 | lr 0.00015078 | gnorm 0.52 | loss_scale 8 | train_wall 8561 | gb_free 8.8 | wall 245657
2022-02-14 13:44:54 | INFO | fairseq.trainer | begin training epoch 29
2022-02-14 13:44:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 13:46:12 | INFO | train_inner | epoch 029:     14 / 1576 loss=6.814, nll_loss=5.049, ppl=33.12, wps=11010, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=44000, lr=0.000150756, gnorm=0.524, loss_scale=8, train_wall=544, gb_free=8.8, wall=245734
2022-02-14 13:55:26 | INFO | train_inner | epoch 029:    114 / 1576 loss=6.737, nll_loss=4.961, ppl=31.16, wps=11832.8, ups=0.18, wpb=65536, bsz=128, num_updates=44100, lr=0.000150585, gnorm=0.528, loss_scale=8, train_wall=543, gb_free=8.8, wall=246288
2022-02-14 14:04:40 | INFO | train_inner | epoch 029:    214 / 1576 loss=6.752, nll_loss=4.978, ppl=31.52, wps=11830.6, ups=0.18, wpb=65536, bsz=128, num_updates=44200, lr=0.000150414, gnorm=0.512, loss_scale=8, train_wall=543, gb_free=8.8, wall=246842
2022-02-14 14:13:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 14:13:59 | INFO | train_inner | epoch 029:    315 / 1576 loss=6.751, nll_loss=4.977, ppl=31.49, wps=11708.4, ups=0.18, wpb=65536, bsz=128, num_updates=44300, lr=0.000150244, gnorm=0.517, loss_scale=8, train_wall=549, gb_free=8.8, wall=247402
2022-02-14 14:23:13 | INFO | train_inner | epoch 029:    415 / 1576 loss=6.762, nll_loss=4.99, ppl=31.77, wps=11832.9, ups=0.18, wpb=65536, bsz=128, num_updates=44400, lr=0.000150075, gnorm=0.524, loss_scale=8, train_wall=543, gb_free=8.8, wall=247956
2022-02-14 14:32:27 | INFO | train_inner | epoch 029:    515 / 1576 loss=6.765, nll_loss=4.993, ppl=31.84, wps=11827.1, ups=0.18, wpb=65536, bsz=128, num_updates=44500, lr=0.000149906, gnorm=0.51, loss_scale=8, train_wall=544, gb_free=8.8, wall=248510
2022-02-14 14:41:42 | INFO | train_inner | epoch 029:    615 / 1576 loss=6.784, nll_loss=5.015, ppl=32.33, wps=11824.2, ups=0.18, wpb=65536, bsz=128, num_updates=44600, lr=0.000149738, gnorm=0.512, loss_scale=16, train_wall=544, gb_free=8.8, wall=249064
2022-02-14 14:43:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 14:51:01 | INFO | train_inner | epoch 029:    716 / 1576 loss=6.789, nll_loss=5.02, ppl=32.46, wps=11713.7, ups=0.18, wpb=65536, bsz=128, num_updates=44700, lr=0.000149571, gnorm=0.527, loss_scale=8, train_wall=549, gb_free=8.8, wall=249624
2022-02-14 15:00:15 | INFO | train_inner | epoch 029:    816 / 1576 loss=6.782, nll_loss=5.012, ppl=32.27, wps=11829.6, ups=0.18, wpb=65532.3, bsz=128, num_updates=44800, lr=0.000149404, gnorm=0.518, loss_scale=8, train_wall=544, gb_free=8.8, wall=250178
2022-02-14 15:08:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 15:09:34 | INFO | train_inner | epoch 029:    917 / 1576 loss=6.797, nll_loss=5.03, ppl=32.67, wps=11714.6, ups=0.18, wpb=65536, bsz=128, num_updates=44900, lr=0.000149237, gnorm=0.527, loss_scale=8, train_wall=549, gb_free=8.8, wall=250737
2022-02-14 15:18:48 | INFO | train_inner | epoch 029:   1017 / 1576 loss=6.79, nll_loss=5.021, ppl=32.47, wps=11830.2, ups=0.18, wpb=65536, bsz=128, num_updates=45000, lr=0.000149071, gnorm=0.52, loss_scale=8, train_wall=544, gb_free=8.8, wall=251291
2022-02-14 15:28:03 | INFO | train_inner | epoch 029:   1117 / 1576 loss=6.805, nll_loss=5.038, ppl=32.86, wps=11827.2, ups=0.18, wpb=65536, bsz=128, num_updates=45100, lr=0.000148906, gnorm=0.522, loss_scale=8, train_wall=544, gb_free=8.8, wall=251845
2022-02-14 15:35:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 15:37:22 | INFO | train_inner | epoch 029:   1218 / 1576 loss=6.792, nll_loss=5.024, ppl=32.53, wps=11712.2, ups=0.18, wpb=65536, bsz=128, num_updates=45200, lr=0.000148741, gnorm=0.515, loss_scale=8, train_wall=549, gb_free=8.8, wall=252405
2022-02-14 15:46:36 | INFO | train_inner | epoch 029:   1318 / 1576 loss=6.812, nll_loss=5.046, ppl=33.04, wps=11830.8, ups=0.18, wpb=65536, bsz=128, num_updates=45300, lr=0.000148577, gnorm=0.528, loss_scale=8, train_wall=543, gb_free=8.8, wall=252959
2022-02-14 15:55:50 | INFO | train_inner | epoch 029:   1418 / 1576 loss=6.817, nll_loss=5.053, ppl=33.19, wps=11836.7, ups=0.18, wpb=65536, bsz=128, num_updates=45400, lr=0.000148413, gnorm=0.515, loss_scale=8, train_wall=543, gb_free=8.8, wall=253512
2022-02-14 16:00:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 16:05:09 | INFO | train_inner | epoch 029:   1519 / 1576 loss=6.813, nll_loss=5.047, ppl=33.07, wps=11712.8, ups=0.18, wpb=65536, bsz=128, num_updates=45500, lr=0.00014825, gnorm=0.533, loss_scale=8, train_wall=549, gb_free=8.8, wall=254072
2022-02-14 16:10:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 16:10:27 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.917 | nll_loss 5.154 | ppl 35.62 | wps 32662.3 | wpb 1021.8 | bsz 2 | num_updates 45557 | best_loss 6.917
2022-02-14 16:10:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 45557 updates
2022-02-14 16:10:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint29.pt
2022-02-14 16:10:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint29.pt
2022-02-14 16:10:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint29.pt (epoch 29 @ 45557 updates, score 6.917) (writing took 28.668900042772293 seconds)
2022-02-14 16:10:56 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-02-14 16:10:56 | INFO | train | epoch 029 | loss 6.784 | nll_loss 5.015 | ppl 32.33 | wps 11744.7 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 45557 | lr 0.000148157 | gnorm 0.521 | loss_scale 8 | train_wall 8561 | gb_free 8.8 | wall 254418
2022-02-14 16:10:56 | INFO | fairseq.trainer | begin training epoch 30
2022-02-14 16:10:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 16:14:54 | INFO | train_inner | epoch 030:     43 / 1576 loss=6.781, nll_loss=5.012, ppl=32.26, wps=11116.6, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=45600, lr=0.000148087, gnorm=0.54, loss_scale=8, train_wall=538, gb_free=8.8, wall=254656
2022-02-14 16:24:07 | INFO | train_inner | epoch 030:    143 / 1576 loss=6.746, nll_loss=4.972, ppl=31.38, wps=11842.2, ups=0.18, wpb=65536, bsz=128, num_updates=45700, lr=0.000147925, gnorm=0.525, loss_scale=8, train_wall=543, gb_free=8.8, wall=255210
2022-02-14 16:30:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 16:33:26 | INFO | train_inner | epoch 030:    244 / 1576 loss=6.731, nll_loss=4.954, ppl=30.99, wps=11714.1, ups=0.18, wpb=65536, bsz=128, num_updates=45800, lr=0.000147764, gnorm=0.53, loss_scale=8, train_wall=549, gb_free=8.8, wall=255769
2022-02-14 16:42:40 | INFO | train_inner | epoch 030:    344 / 1576 loss=6.752, nll_loss=4.979, ppl=31.53, wps=11839.3, ups=0.18, wpb=65536, bsz=128, num_updates=45900, lr=0.000147602, gnorm=0.519, loss_scale=8, train_wall=543, gb_free=8.8, wall=256323
2022-02-14 16:51:54 | INFO | train_inner | epoch 030:    444 / 1576 loss=6.753, nll_loss=4.98, ppl=31.56, wps=11838.1, ups=0.18, wpb=65532.3, bsz=128, num_updates=46000, lr=0.000147442, gnorm=0.519, loss_scale=8, train_wall=543, gb_free=8.8, wall=256876
2022-02-14 16:54:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 17:01:13 | INFO | train_inner | epoch 030:    545 / 1576 loss=6.766, nll_loss=4.994, ppl=31.87, wps=11722.1, ups=0.18, wpb=65536, bsz=128, num_updates=46100, lr=0.000147282, gnorm=0.53, loss_scale=8, train_wall=549, gb_free=8.8, wall=257435
2022-02-14 17:09:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-02-14 17:10:32 | INFO | train_inner | epoch 030:    646 / 1576 loss=6.772, nll_loss=5.001, ppl=32.03, wps=11719.3, ups=0.18, wpb=65536, bsz=128, num_updates=46200, lr=0.000147122, gnorm=0.53, loss_scale=4, train_wall=549, gb_free=8.8, wall=257994
2022-02-14 17:19:46 | INFO | train_inner | epoch 030:    746 / 1576 loss=6.775, nll_loss=5.004, ppl=32.1, wps=11836.5, ups=0.18, wpb=65536, bsz=128, num_updates=46300, lr=0.000146964, gnorm=0.517, loss_scale=4, train_wall=543, gb_free=8.8, wall=258548
2022-02-14 17:28:59 | INFO | train_inner | epoch 030:    846 / 1576 loss=6.784, nll_loss=5.015, ppl=32.33, wps=11842, ups=0.18, wpb=65536, bsz=128, num_updates=46400, lr=0.000146805, gnorm=0.528, loss_scale=4, train_wall=543, gb_free=8.8, wall=259102
2022-02-14 17:38:13 | INFO | train_inner | epoch 030:    946 / 1576 loss=6.795, nll_loss=5.027, ppl=32.61, wps=11836.1, ups=0.18, wpb=65536, bsz=128, num_updates=46500, lr=0.000146647, gnorm=0.528, loss_scale=8, train_wall=543, gb_free=8.8, wall=259655
2022-02-14 17:47:26 | INFO | train_inner | epoch 030:   1046 / 1576 loss=6.794, nll_loss=5.026, ppl=32.58, wps=11836.2, ups=0.18, wpb=65536, bsz=128, num_updates=46600, lr=0.00014649, gnorm=0.519, loss_scale=8, train_wall=543, gb_free=8.8, wall=260209
2022-02-14 17:56:40 | INFO | train_inner | epoch 030:   1146 / 1576 loss=6.804, nll_loss=5.038, ppl=32.85, wps=11837.2, ups=0.18, wpb=65536, bsz=128, num_updates=46700, lr=0.000146333, gnorm=0.521, loss_scale=8, train_wall=543, gb_free=8.8, wall=260763
2022-02-14 17:59:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 18:05:59 | INFO | train_inner | epoch 030:   1247 / 1576 loss=6.793, nll_loss=5.026, ppl=32.57, wps=11721.6, ups=0.18, wpb=65536, bsz=128, num_updates=46800, lr=0.000146176, gnorm=0.528, loss_scale=8, train_wall=549, gb_free=8.8, wall=261322
2022-02-14 18:15:13 | INFO | train_inner | epoch 030:   1347 / 1576 loss=6.801, nll_loss=5.034, ppl=32.77, wps=11838.9, ups=0.18, wpb=65536, bsz=128, num_updates=46900, lr=0.00014602, gnorm=0.522, loss_scale=8, train_wall=543, gb_free=8.8, wall=261875
2022-02-14 18:23:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 18:24:32 | INFO | train_inner | epoch 030:   1448 / 1576 loss=6.814, nll_loss=5.049, ppl=33.11, wps=11723.8, ups=0.18, wpb=65536, bsz=128, num_updates=47000, lr=0.000145865, gnorm=0.523, loss_scale=8, train_wall=548, gb_free=8.8, wall=262434
2022-02-14 18:33:45 | INFO | train_inner | epoch 030:   1548 / 1576 loss=6.803, nll_loss=5.036, ppl=32.82, wps=11841.2, ups=0.18, wpb=65536, bsz=128, num_updates=47100, lr=0.00014571, gnorm=0.529, loss_scale=8, train_wall=543, gb_free=8.8, wall=262988
2022-02-14 18:36:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 18:36:22 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.922 | nll_loss 5.161 | ppl 35.78 | wps 32677 | wpb 1021.8 | bsz 2 | num_updates 47128 | best_loss 6.917
2022-02-14 18:36:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 47128 updates
2022-02-14 18:36:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint30.pt
2022-02-14 18:36:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint30.pt
2022-02-14 18:36:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint30.pt (epoch 30 @ 47128 updates, score 6.922) (writing took 19.03147486038506 seconds)
2022-02-14 18:36:41 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-02-14 18:36:41 | INFO | train | epoch 030 | loss 6.778 | nll_loss 5.008 | ppl 32.18 | wps 11765.9 | ups 0.18 | wpb 65499.3 | bsz 127.9 | num_updates 47128 | lr 0.000145667 | gnorm 0.525 | loss_scale 8 | train_wall 8555 | gb_free 8.8 | wall 263164
2022-02-14 18:36:41 | INFO | fairseq.trainer | begin training epoch 31
2022-02-14 18:36:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 18:43:20 | INFO | train_inner | epoch 031:     72 / 1576 loss=6.736, nll_loss=4.96, ppl=31.13, wps=11301.8, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=47200, lr=0.000145556, gnorm=0.537, loss_scale=8, train_wall=538, gb_free=8.8, wall=263562
2022-02-14 18:47:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 18:52:39 | INFO | train_inner | epoch 031:    173 / 1576 loss=6.727, nll_loss=4.95, ppl=30.92, wps=11712.8, ups=0.18, wpb=65536, bsz=128, num_updates=47300, lr=0.000145402, gnorm=0.534, loss_scale=8, train_wall=549, gb_free=8.8, wall=264122
2022-02-14 19:01:53 | INFO | train_inner | epoch 031:    273 / 1576 loss=6.739, nll_loss=4.964, ppl=31.21, wps=11831.4, ups=0.18, wpb=65536, bsz=128, num_updates=47400, lr=0.000145248, gnorm=0.525, loss_scale=8, train_wall=543, gb_free=8.8, wall=264676
2022-02-14 19:11:07 | INFO | train_inner | epoch 031:    373 / 1576 loss=6.749, nll_loss=4.975, ppl=31.46, wps=11832.2, ups=0.18, wpb=65536, bsz=128, num_updates=47500, lr=0.000145095, gnorm=0.538, loss_scale=8, train_wall=543, gb_free=8.8, wall=265230
2022-02-14 19:11:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 19:20:27 | INFO | train_inner | epoch 031:    474 / 1576 loss=6.756, nll_loss=4.983, ppl=31.64, wps=11717.6, ups=0.18, wpb=65536, bsz=128, num_updates=47600, lr=0.000144943, gnorm=0.545, loss_scale=8, train_wall=549, gb_free=8.8, wall=265789
2022-02-14 19:29:40 | INFO | train_inner | epoch 031:    574 / 1576 loss=6.764, nll_loss=4.993, ppl=31.84, wps=11832.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=47700, lr=0.000144791, gnorm=0.519, loss_scale=8, train_wall=543, gb_free=8.8, wall=266343
2022-02-14 19:35:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 19:39:00 | INFO | train_inner | epoch 031:    675 / 1576 loss=6.768, nll_loss=4.997, ppl=31.93, wps=11713, ups=0.18, wpb=65536, bsz=128, num_updates=47800, lr=0.000144639, gnorm=0.54, loss_scale=8, train_wall=549, gb_free=8.8, wall=266902
2022-02-14 19:48:14 | INFO | train_inner | epoch 031:    775 / 1576 loss=6.78, nll_loss=5.01, ppl=32.22, wps=11829.7, ups=0.18, wpb=65536, bsz=128, num_updates=47900, lr=0.000144488, gnorm=0.535, loss_scale=8, train_wall=544, gb_free=8.8, wall=267456
2022-02-14 19:57:28 | INFO | train_inner | epoch 031:    875 / 1576 loss=6.783, nll_loss=5.013, ppl=32.3, wps=11826.3, ups=0.18, wpb=65536, bsz=128, num_updates=48000, lr=0.000144338, gnorm=0.521, loss_scale=8, train_wall=544, gb_free=8.8, wall=268011
2022-02-14 20:03:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 20:06:48 | INFO | train_inner | epoch 031:    976 / 1576 loss=6.785, nll_loss=5.017, ppl=32.37, wps=11708.9, ups=0.18, wpb=65536, bsz=128, num_updates=48100, lr=0.000144187, gnorm=0.523, loss_scale=8, train_wall=549, gb_free=8.8, wall=268570
2022-02-14 20:16:02 | INFO | train_inner | epoch 031:   1076 / 1576 loss=6.792, nll_loss=5.024, ppl=32.54, wps=11828.5, ups=0.18, wpb=65536, bsz=128, num_updates=48200, lr=0.000144038, gnorm=0.522, loss_scale=8, train_wall=544, gb_free=8.8, wall=269124
2022-02-14 20:25:16 | INFO | train_inner | epoch 031:   1176 / 1576 loss=6.797, nll_loss=5.029, ppl=32.66, wps=11828.2, ups=0.18, wpb=65536, bsz=128, num_updates=48300, lr=0.000143889, gnorm=0.524, loss_scale=8, train_wall=544, gb_free=8.8, wall=269678
2022-02-14 20:27:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 20:34:36 | INFO | train_inner | epoch 031:   1277 / 1576 loss=6.796, nll_loss=5.028, ppl=32.63, wps=11706.5, ups=0.18, wpb=65536, bsz=128, num_updates=48400, lr=0.00014374, gnorm=0.525, loss_scale=8, train_wall=549, gb_free=8.8, wall=270238
2022-02-14 20:43:50 | INFO | train_inner | epoch 031:   1377 / 1576 loss=6.788, nll_loss=5.02, ppl=32.44, wps=11823.2, ups=0.18, wpb=65536, bsz=128, num_updates=48500, lr=0.000143592, gnorm=0.516, loss_scale=8, train_wall=544, gb_free=8.8, wall=270793
2022-02-14 20:51:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 20:53:10 | INFO | train_inner | epoch 031:   1478 / 1576 loss=6.796, nll_loss=5.029, ppl=32.64, wps=11704.5, ups=0.18, wpb=65536, bsz=128, num_updates=48600, lr=0.000143444, gnorm=0.535, loss_scale=8, train_wall=549, gb_free=8.8, wall=271352
2022-02-14 21:02:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 21:02:15 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.92 | nll_loss 5.142 | ppl 35.3 | wps 32649.8 | wpb 1021.8 | bsz 2 | num_updates 48698 | best_loss 6.917
2022-02-14 21:02:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 48698 updates
2022-02-14 21:02:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint31.pt
2022-02-14 21:02:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint31.pt
2022-02-14 21:02:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint31.pt (epoch 31 @ 48698 updates, score 6.92) (writing took 19.227858245372772 seconds)
2022-02-14 21:02:35 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-02-14 21:02:35 | INFO | train | epoch 031 | loss 6.772 | nll_loss 5.001 | ppl 32.03 | wps 11747.9 | ups 0.18 | wpb 65499.2 | bsz 127.9 | num_updates 48698 | lr 0.000143299 | gnorm 0.529 | loss_scale 8 | train_wall 8563 | gb_free 8.8 | wall 271917
2022-02-14 21:02:35 | INFO | fairseq.trainer | begin training epoch 32
2022-02-14 21:02:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-02-14 21:02:46 | INFO | train_inner | epoch 032:      2 / 1576 loss=6.806, nll_loss=5.04, ppl=32.9, wps=11282.4, ups=0.17, wpb=64962.6, bsz=126.9, num_updates=48700, lr=0.000143296, gnorm=0.538, loss_scale=8, train_wall=539, gb_free=8.8, wall=271928
2022-02-14 21:12:00 | INFO | train_inner | epoch 032:    102 / 1576 loss=6.722, nll_loss=4.945, ppl=30.8, wps=11823.9, ups=0.18, wpb=65536, bsz=128, num_updates=48800, lr=0.00014315, gnorm=0.522, loss_scale=8, train_wall=544, gb_free=8.8, wall=272483
2022-02-14 21:15:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 21:21:20 | INFO | train_inner | epoch 032:    203 / 1576 loss=6.729, nll_loss=4.953, ppl=30.97, wps=11712.8, ups=0.18, wpb=65536, bsz=128, num_updates=48900, lr=0.000143003, gnorm=0.531, loss_scale=8, train_wall=549, gb_free=8.8, wall=273042
2022-02-14 21:30:34 | INFO | train_inner | epoch 032:    303 / 1576 loss=6.733, nll_loss=4.957, ppl=31.06, wps=11825.7, ups=0.18, wpb=65536, bsz=128, num_updates=49000, lr=0.000142857, gnorm=0.533, loss_scale=8, train_wall=544, gb_free=8.8, wall=273596
2022-02-14 21:39:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 21:39:53 | INFO | train_inner | epoch 032:    404 / 1576 loss=6.74, nll_loss=4.965, ppl=31.23, wps=11711.5, ups=0.18, wpb=65536, bsz=128, num_updates=49100, lr=0.000142712, gnorm=0.519, loss_scale=8, train_wall=549, gb_free=8.8, wall=274156
2022-02-14 21:49:07 | INFO | train_inner | epoch 032:    504 / 1576 loss=6.749, nll_loss=4.975, ppl=31.45, wps=11825.9, ups=0.18, wpb=65536, bsz=128, num_updates=49200, lr=0.000142566, gnorm=0.533, loss_scale=8, train_wall=544, gb_free=8.8, wall=274710
2022-02-14 21:58:22 | INFO | train_inner | epoch 032:    604 / 1576 loss=6.766, nll_loss=4.995, ppl=31.88, wps=11824, ups=0.18, wpb=65536, bsz=128, num_updates=49300, lr=0.000142422, gnorm=0.531, loss_scale=8, train_wall=544, gb_free=8.8, wall=275264
2022-02-14 22:05:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 22:07:41 | INFO | train_inner | epoch 032:    705 / 1576 loss=6.773, nll_loss=5.002, ppl=32.04, wps=11709.5, ups=0.18, wpb=65536, bsz=128, num_updates=49400, lr=0.000142278, gnorm=0.546, loss_scale=8, train_wall=549, gb_free=8.8, wall=275824
2022-02-14 22:16:56 | INFO | train_inner | epoch 032:    805 / 1576 loss=6.766, nll_loss=4.994, ppl=31.87, wps=11821.3, ups=0.18, wpb=65536, bsz=128, num_updates=49500, lr=0.000142134, gnorm=0.542, loss_scale=8, train_wall=544, gb_free=8.8, wall=276378
2022-02-14 22:26:10 | INFO | train_inner | epoch 032:    905 / 1576 loss=6.781, nll_loss=5.011, ppl=32.25, wps=11822.6, ups=0.18, wpb=65536, bsz=128, num_updates=49600, lr=0.00014199, gnorm=0.522, loss_scale=8, train_wall=544, gb_free=8.8, wall=276933
2022-02-14 22:34:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 22:35:30 | INFO | train_inner | epoch 032:   1006 / 1576 loss=6.783, nll_loss=5.013, ppl=32.3, wps=11703.5, ups=0.18, wpb=65536, bsz=128, num_updates=49700, lr=0.000141848, gnorm=0.529, loss_scale=8, train_wall=549, gb_free=8.8, wall=277493
2022-02-14 22:44:44 | INFO | train_inner | epoch 032:   1106 / 1576 loss=6.777, nll_loss=5.007, ppl=32.15, wps=11821.2, ups=0.18, wpb=65532.3, bsz=128, num_updates=49800, lr=0.000141705, gnorm=0.525, loss_scale=8, train_wall=544, gb_free=8.8, wall=278047
2022-02-14 22:53:59 | INFO | train_inner | epoch 032:   1206 / 1576 loss=6.782, nll_loss=5.012, ppl=32.28, wps=11821.7, ups=0.18, wpb=65536, bsz=128, num_updates=49900, lr=0.000141563, gnorm=0.539, loss_scale=8, train_wall=544, gb_free=8.8, wall=278601
2022-02-14 22:58:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-02-14 23:03:19 | INFO | train_inner | epoch 032:   1307 / 1576 loss=6.794, nll_loss=5.026, ppl=32.59, wps=11707.1, ups=0.18, wpb=65536, bsz=128, num_updates=50000, lr=0.000141421, gnorm=0.524, loss_scale=8, train_wall=549, gb_free=8.8, wall=279161
2022-02-14 23:03:19 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2022-02-14 23:03:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-02-14 23:03:25 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.915 | nll_loss 5.147 | ppl 35.43 | wps 32594.9 | wpb 1021.8 | bsz 2 | num_updates 50000 | best_loss 6.915
2022-02-14 23:03:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 50000 updates
2022-02-14 23:03:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-02-14 23:03:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint_best.pt
2022-02-14 23:03:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16-label_smoothing_0.1_#1/checkpoint_best.pt (epoch 32 @ 50000 updates, score 6.915) (writing took 19.411461112089455 seconds)
2022-02-14 23:03:45 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-02-14 23:03:45 | INFO | train | epoch 032 | loss 6.761 | nll_loss 4.989 | ppl 31.75 | wps 11736.4 | ups 0.18 | wpb 65535.7 | bsz 128 | num_updates 50000 | lr 0.000141421 | gnorm 0.531 | loss_scale 8 | train_wall 7107 | gb_free 8.8 | wall 279187
2022-02-14 23:03:45 | INFO | fairseq_cli.train | done training in 279183.5 seconds
