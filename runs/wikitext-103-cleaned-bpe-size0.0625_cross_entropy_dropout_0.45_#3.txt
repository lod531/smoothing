Sender: LSF System <lsfadmin@eu-g3-025>
Subject: Job 210480314: <wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3> in cluster <euler> Done

Job <wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3> was submitted from host <eu-login-43> by user <andriusb> in cluster <euler> at Tue Mar 22 19:44:46 2022
Job was executed on host(s) <eu-g3-025>, in queue <gpu.24h>, as user <andriusb> in cluster <euler> at Tue Mar 22 20:03:33 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Tue Mar 22 20:03:33 2022
Terminated at Tue Mar 22 22:52:43 2022
Results reported at Tue Mar 22 22:52:43 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-cleaned-bpe-size0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.45 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 32 --seed 66575611 --fp16 --no-epoch-checkpoints --no-last-checkpoints --patience 3 --seed 66575613 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   10126.29 sec.
    Max Memory :                                 6132 MB
    Average Memory :                             4878.79 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               13868.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   10149 sec.
    Turnaround time :                            11277 sec.

The output (if any) follows:

2022-03-22 20:03:48 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575613, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.45, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-cleaned-bpe-size0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575613, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-22 20:03:48 | INFO | fairseq.tasks.language_modeling | dictionary: 39136 types
2022-03-22 20:03:49 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=39136, bias=False)
  )
)
2022-03-22 20:03:49 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-22 20:03:49 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-22 20:03:49 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-22 20:03:49 | INFO | fairseq_cli.train | num. shared model params: 38,951,936 (num. trained: 38,951,936)
2022-03-22 20:03:49 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-22 20:03:49 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/valid
2022-03-22 20:03:58 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-22 20:03:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-22 20:03:58 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = NVIDIA GeForce RTX 2080 Ti              
2022-03-22 20:03:58 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-22 20:03:58 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-22 20:03:58 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-22 20:03:58 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_last.pt
2022-03-22 20:03:58 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_last.pt
2022-03-22 20:03:58 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-22 20:03:58 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/train
2022-03-22 20:03:58 | INFO | fairseq.trainer | begin training epoch 1
2022-03-22 20:03:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:04:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 20:04:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 20:05:00 | INFO | train_inner | epoch 001:    102 / 411 loss=14.518, ppl=23468.6, wps=27850.8, ups=1.7, wpb=16384, bsz=32, num_updates=100, lr=1.25975e-05, gnorm=2.851, loss_scale=32, train_wall=57, gb_free=9.7, wall=62
2022-03-22 20:05:59 | INFO | train_inner | epoch 001:    202 / 411 loss=12.675, ppl=6537.61, wps=27891.6, ups=1.7, wpb=16384, bsz=32, num_updates=200, lr=2.5095e-05, gnorm=1.251, loss_scale=32, train_wall=54, gb_free=9.7, wall=121
2022-03-22 20:06:58 | INFO | train_inner | epoch 001:    302 / 411 loss=11.454, ppl=2804.99, wps=27854.1, ups=1.7, wpb=16384, bsz=32, num_updates=300, lr=3.75925e-05, gnorm=0.897, loss_scale=32, train_wall=54, gb_free=9.7, wall=179
2022-03-22 20:07:57 | INFO | train_inner | epoch 001:    402 / 411 loss=10.708, ppl=1672.21, wps=27770.6, ups=1.69, wpb=16384, bsz=32, num_updates=400, lr=5.009e-05, gnorm=0.747, loss_scale=32, train_wall=55, gb_free=9.7, wall=238
2022-03-22 20:08:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:08:07 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.429 | ppl 1378.29 | wps 48571.9 | wpb 511.2 | bsz 1 | num_updates 409
2022-03-22 20:08:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 409 updates
2022-03-22 20:08:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:08:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:08:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 1 @ 409 updates, score 10.429) (writing took 0.9947574958205223 seconds)
2022-03-22 20:08:08 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-22 20:08:08 | INFO | train | epoch 001 | loss 12.3 | ppl 5043.2 | wps 27168.1 | ups 1.66 | wpb 16367.7 | bsz 32 | num_updates 409 | lr 5.12148e-05 | gnorm 1.42 | loss_scale 32 | train_wall 225 | gb_free 9.7 | wall 249
2022-03-22 20:08:08 | INFO | fairseq.trainer | begin training epoch 2
2022-03-22 20:08:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:09:01 | INFO | train_inner | epoch 002:     91 / 411 loss=10.424, ppl=1374.18, wps=25281.6, ups=1.55, wpb=16317.5, bsz=31.9, num_updates=500, lr=6.25875e-05, gnorm=0.686, loss_scale=32, train_wall=54, gb_free=9.7, wall=303
2022-03-22 20:10:00 | INFO | train_inner | epoch 002:    191 / 411 loss=10.276, ppl=1240.13, wps=27844.1, ups=1.7, wpb=16384, bsz=32, num_updates=600, lr=7.5085e-05, gnorm=0.664, loss_scale=64, train_wall=54, gb_free=9.7, wall=362
2022-03-22 20:10:59 | INFO | train_inner | epoch 002:    291 / 411 loss=10.112, ppl=1106.67, wps=27924.7, ups=1.7, wpb=16384, bsz=32, num_updates=700, lr=8.75825e-05, gnorm=0.659, loss_scale=64, train_wall=54, gb_free=9.7, wall=421
2022-03-22 20:11:58 | INFO | train_inner | epoch 002:    391 / 411 loss=9.96, ppl=996.13, wps=27869.5, ups=1.7, wpb=16378.9, bsz=32, num_updates=800, lr=0.00010008, gnorm=0.648, loss_scale=64, train_wall=54, gb_free=9.7, wall=479
2022-03-22 20:12:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:12:14 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.696 | ppl 829.47 | wps 48506.6 | wpb 511.2 | bsz 1 | num_updates 820 | best_loss 9.696
2022-03-22 20:12:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 820 updates
2022-03-22 20:12:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:12:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:12:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 2 @ 820 updates, score 9.696) (writing took 0.960068553686142 seconds)
2022-03-22 20:12:15 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-22 20:12:15 | INFO | train | epoch 002 | loss 10.17 | ppl 1151.96 | wps 27221.3 | ups 1.66 | wpb 16367.8 | bsz 32 | num_updates 820 | lr 0.00010258 | gnorm 0.664 | loss_scale 64 | train_wall 223 | gb_free 9.7 | wall 497
2022-03-22 20:12:15 | INFO | fairseq.trainer | begin training epoch 3
2022-03-22 20:12:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:13:03 | INFO | train_inner | epoch 003:     80 / 411 loss=9.781, ppl=879.97, wps=24921.9, ups=1.53, wpb=16322.6, bsz=31.9, num_updates=900, lr=0.000112578, gnorm=0.644, loss_scale=64, train_wall=55, gb_free=9.7, wall=545
2022-03-22 20:14:02 | INFO | train_inner | epoch 003:    180 / 411 loss=9.625, ppl=789.82, wps=27821.3, ups=1.7, wpb=16378.9, bsz=32, num_updates=1000, lr=0.000125075, gnorm=0.638, loss_scale=64, train_wall=55, gb_free=9.7, wall=604
2022-03-22 20:15:01 | INFO | train_inner | epoch 003:    280 / 411 loss=9.479, ppl=713.64, wps=27851.7, ups=1.7, wpb=16384, bsz=32, num_updates=1100, lr=0.000137573, gnorm=0.663, loss_scale=128, train_wall=54, gb_free=9.7, wall=662
2022-03-22 20:15:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 20:16:00 | INFO | train_inner | epoch 003:    381 / 411 loss=9.353, ppl=653.94, wps=27628.3, ups=1.69, wpb=16384, bsz=32, num_updates=1200, lr=0.00015007, gnorm=0.651, loss_scale=64, train_wall=55, gb_free=9.7, wall=722
2022-03-22 20:16:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:16:22 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.05 | ppl 530.04 | wps 48260 | wpb 511.2 | bsz 1 | num_updates 1230 | best_loss 9.05
2022-03-22 20:16:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1230 updates
2022-03-22 20:16:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:16:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:16:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 3 @ 1230 updates, score 9.05) (writing took 0.9503667112439871 seconds)
2022-03-22 20:16:23 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-22 20:16:23 | INFO | train | epoch 003 | loss 9.524 | ppl 736.16 | wps 27009.9 | ups 1.65 | wpb 16367.8 | bsz 32 | num_updates 1230 | lr 0.000153819 | gnorm 0.652 | loss_scale 64 | train_wall 225 | gb_free 9.7 | wall 745
2022-03-22 20:16:23 | INFO | fairseq.trainer | begin training epoch 4
2022-03-22 20:16:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:17:05 | INFO | train_inner | epoch 004:     70 / 411 loss=9.174, ppl=577.67, wps=25301.4, ups=1.55, wpb=16317.5, bsz=31.9, num_updates=1300, lr=0.000162568, gnorm=0.675, loss_scale=64, train_wall=54, gb_free=9.7, wall=786
2022-03-22 20:18:04 | INFO | train_inner | epoch 004:    170 / 411 loss=9.03, ppl=522.78, wps=27748.4, ups=1.69, wpb=16384, bsz=32, num_updates=1400, lr=0.000175065, gnorm=0.668, loss_scale=64, train_wall=55, gb_free=9.7, wall=845
2022-03-22 20:19:02 | INFO | train_inner | epoch 004:    270 / 411 loss=8.913, ppl=482, wps=27858.4, ups=1.7, wpb=16384, bsz=32, num_updates=1500, lr=0.000187563, gnorm=0.7, loss_scale=64, train_wall=54, gb_free=9.7, wall=904
2022-03-22 20:20:01 | INFO | train_inner | epoch 004:    370 / 411 loss=8.831, ppl=455.52, wps=27909.5, ups=1.7, wpb=16384, bsz=32, num_updates=1600, lr=0.00020006, gnorm=0.673, loss_scale=64, train_wall=54, gb_free=9.7, wall=963
2022-03-22 20:20:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:20:30 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.564 | ppl 378.37 | wps 47453.4 | wpb 511.2 | bsz 1 | num_updates 1641 | best_loss 8.564
2022-03-22 20:20:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1641 updates
2022-03-22 20:20:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:20:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:20:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 4 @ 1641 updates, score 8.564) (writing took 0.9982185531407595 seconds)
2022-03-22 20:20:31 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-22 20:20:31 | INFO | train | epoch 004 | loss 8.943 | ppl 492.08 | wps 27159.4 | ups 1.66 | wpb 16367.8 | bsz 32 | num_updates 1641 | lr 0.000205184 | gnorm 0.674 | loss_scale 128 | train_wall 224 | gb_free 9.7 | wall 993
2022-03-22 20:20:31 | INFO | fairseq.trainer | begin training epoch 5
2022-03-22 20:20:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:21:06 | INFO | train_inner | epoch 005:     59 / 411 loss=8.695, ppl=414.32, wps=25253.9, ups=1.55, wpb=16322.6, bsz=31.9, num_updates=1700, lr=0.000212558, gnorm=0.676, loss_scale=128, train_wall=54, gb_free=9.7, wall=1027
2022-03-22 20:21:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 20:22:05 | INFO | train_inner | epoch 005:    160 / 411 loss=8.575, ppl=381.29, wps=27648.8, ups=1.69, wpb=16384, bsz=32, num_updates=1800, lr=0.000225055, gnorm=0.676, loss_scale=64, train_wall=55, gb_free=9.7, wall=1087
2022-03-22 20:23:04 | INFO | train_inner | epoch 005:    260 / 411 loss=8.508, ppl=364.02, wps=27870.2, ups=1.7, wpb=16384, bsz=32, num_updates=1900, lr=0.000237553, gnorm=0.679, loss_scale=64, train_wall=54, gb_free=9.7, wall=1146
2022-03-22 20:24:03 | INFO | train_inner | epoch 005:    360 / 411 loss=8.415, ppl=341.32, wps=27896.3, ups=1.7, wpb=16384, bsz=32, num_updates=2000, lr=0.00025005, gnorm=0.66, loss_scale=64, train_wall=54, gb_free=9.7, wall=1204
2022-03-22 20:24:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:24:37 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.212 | ppl 296.49 | wps 47848.7 | wpb 511.2 | bsz 1 | num_updates 2051 | best_loss 8.212
2022-03-22 20:24:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 2051 updates
2022-03-22 20:24:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:24:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:24:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 5 @ 2051 updates, score 8.212) (writing took 1.0235639791935682 seconds)
2022-03-22 20:24:38 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-22 20:24:38 | INFO | train | epoch 005 | loss 8.507 | ppl 363.79 | wps 27157.6 | ups 1.66 | wpb 16367.8 | bsz 32 | num_updates 2051 | lr 0.000256424 | gnorm 0.671 | loss_scale 64 | train_wall 223 | gb_free 9.7 | wall 1240
2022-03-22 20:24:38 | INFO | fairseq.trainer | begin training epoch 6
2022-03-22 20:24:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:25:06 | INFO | train_inner | epoch 006:     49 / 411 loss=8.317, ppl=318.84, wps=25709.6, ups=1.58, wpb=16312.3, bsz=31.9, num_updates=2100, lr=0.000262548, gnorm=0.66, loss_scale=64, train_wall=53, gb_free=9.7, wall=1268
2022-03-22 20:26:02 | INFO | train_inner | epoch 006:    149 / 411 loss=8.222, ppl=298.52, wps=29116, ups=1.78, wpb=16384, bsz=32, num_updates=2200, lr=0.000275045, gnorm=0.639, loss_scale=64, train_wall=52, gb_free=9.7, wall=1324
2022-03-22 20:26:58 | INFO | train_inner | epoch 006:    249 / 411 loss=8.166, ppl=287.19, wps=29146.1, ups=1.78, wpb=16384, bsz=32, num_updates=2300, lr=0.000287543, gnorm=0.656, loss_scale=128, train_wall=52, gb_free=9.7, wall=1380
2022-03-22 20:27:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 20:27:55 | INFO | train_inner | epoch 006:    350 / 411 loss=8.111, ppl=276.41, wps=28854.5, ups=1.76, wpb=16384, bsz=32, num_updates=2400, lr=0.00030004, gnorm=0.652, loss_scale=64, train_wall=53, gb_free=9.7, wall=1437
2022-03-22 20:28:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:28:34 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.934 | ppl 244.54 | wps 50487.1 | wpb 511.2 | bsz 1 | num_updates 2461 | best_loss 7.934
2022-03-22 20:28:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2461 updates
2022-03-22 20:28:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:28:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:28:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 6 @ 2461 updates, score 7.934) (writing took 0.9764756467193365 seconds)
2022-03-22 20:28:35 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-22 20:28:35 | INFO | train | epoch 006 | loss 8.16 | ppl 286.12 | wps 28314 | ups 1.73 | wpb 16367.8 | bsz 32 | num_updates 2461 | lr 0.000307663 | gnorm 0.652 | loss_scale 64 | train_wall 214 | gb_free 9.7 | wall 1477
2022-03-22 20:28:35 | INFO | fairseq.trainer | begin training epoch 7
2022-03-22 20:28:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:28:57 | INFO | train_inner | epoch 007:     39 / 411 loss=7.995, ppl=255.15, wps=26346.7, ups=1.61, wpb=16322.6, bsz=31.9, num_updates=2500, lr=0.000312538, gnorm=0.652, loss_scale=64, train_wall=52, gb_free=9.7, wall=1499
2022-03-22 20:29:53 | INFO | train_inner | epoch 007:    139 / 411 loss=7.926, ppl=243.26, wps=29237.4, ups=1.78, wpb=16384, bsz=32, num_updates=2600, lr=0.000325035, gnorm=0.65, loss_scale=64, train_wall=52, gb_free=9.7, wall=1555
2022-03-22 20:30:49 | INFO | train_inner | epoch 007:    239 / 411 loss=7.845, ppl=229.99, wps=29132.5, ups=1.78, wpb=16384, bsz=32, num_updates=2700, lr=0.000337533, gnorm=0.661, loss_scale=64, train_wall=52, gb_free=9.7, wall=1611
2022-03-22 20:31:46 | INFO | train_inner | epoch 007:    339 / 411 loss=7.816, ppl=225.31, wps=29154.5, ups=1.78, wpb=16378.9, bsz=32, num_updates=2800, lr=0.00035003, gnorm=0.66, loss_scale=64, train_wall=52, gb_free=9.7, wall=1667
2022-03-22 20:32:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:32:30 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.657 | ppl 201.89 | wps 50569.1 | wpb 511.2 | bsz 1 | num_updates 2872 | best_loss 7.657
2022-03-22 20:32:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2872 updates
2022-03-22 20:32:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:32:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:32:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 7 @ 2872 updates, score 7.657) (writing took 0.9929842464625835 seconds)
2022-03-22 20:32:31 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-22 20:32:31 | INFO | train | epoch 007 | loss 7.853 | ppl 231.16 | wps 28467.4 | ups 1.74 | wpb 16367.8 | bsz 32 | num_updates 2872 | lr 0.000359028 | gnorm 0.655 | loss_scale 128 | train_wall 214 | gb_free 9.7 | wall 1713
2022-03-22 20:32:31 | INFO | fairseq.trainer | begin training epoch 8
2022-03-22 20:32:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:32:47 | INFO | train_inner | epoch 008:     28 / 411 loss=7.73, ppl=212.31, wps=26461.8, ups=1.62, wpb=16322.6, bsz=31.9, num_updates=2900, lr=0.000362528, gnorm=0.645, loss_scale=128, train_wall=52, gb_free=9.7, wall=1729
2022-03-22 20:33:44 | INFO | train_inner | epoch 008:    128 / 411 loss=7.626, ppl=197.6, wps=29141.3, ups=1.78, wpb=16384, bsz=32, num_updates=3000, lr=0.000375025, gnorm=0.646, loss_scale=128, train_wall=52, gb_free=9.7, wall=1785
2022-03-22 20:34:41 | INFO | train_inner | epoch 008:    228 / 411 loss=7.593, ppl=193.05, wps=28625.9, ups=1.75, wpb=16384, bsz=32, num_updates=3100, lr=0.000387523, gnorm=0.649, loss_scale=128, train_wall=53, gb_free=9.7, wall=1843
2022-03-22 20:35:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 20:35:38 | INFO | train_inner | epoch 008:    329 / 411 loss=7.554, ppl=187.98, wps=28864.4, ups=1.76, wpb=16384, bsz=32, num_updates=3200, lr=0.00040002, gnorm=0.655, loss_scale=64, train_wall=53, gb_free=9.7, wall=1899
2022-03-22 20:36:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:36:28 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.455 | ppl 175.47 | wps 50595.5 | wpb 511.2 | bsz 1 | num_updates 3282 | best_loss 7.455
2022-03-22 20:36:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3282 updates
2022-03-22 20:36:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:36:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:36:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 8 @ 3282 updates, score 7.455) (writing took 0.9620391745120287 seconds)
2022-03-22 20:36:29 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-22 20:36:29 | INFO | train | epoch 008 | loss 7.575 | ppl 190.69 | wps 28227.6 | ups 1.72 | wpb 16367.8 | bsz 32 | num_updates 3282 | lr 0.000410268 | gnorm 0.646 | loss_scale 64 | train_wall 215 | gb_free 9.7 | wall 1951
2022-03-22 20:36:29 | INFO | fairseq.trainer | begin training epoch 9
2022-03-22 20:36:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:36:39 | INFO | train_inner | epoch 009:     18 / 411 loss=7.476, ppl=178.05, wps=26424.5, ups=1.62, wpb=16317.5, bsz=31.9, num_updates=3300, lr=0.000412518, gnorm=0.634, loss_scale=64, train_wall=52, gb_free=9.7, wall=1961
2022-03-22 20:37:35 | INFO | train_inner | epoch 009:    118 / 411 loss=7.359, ppl=164.18, wps=29157.3, ups=1.78, wpb=16378.9, bsz=32, num_updates=3400, lr=0.000425015, gnorm=0.664, loss_scale=64, train_wall=52, gb_free=9.7, wall=2017
2022-03-22 20:38:32 | INFO | train_inner | epoch 009:    218 / 411 loss=7.344, ppl=162.41, wps=29172.3, ups=1.78, wpb=16384, bsz=32, num_updates=3500, lr=0.000437513, gnorm=0.648, loss_scale=64, train_wall=52, gb_free=9.7, wall=2073
2022-03-22 20:39:28 | INFO | train_inner | epoch 009:    318 / 411 loss=7.319, ppl=159.63, wps=29117.1, ups=1.78, wpb=16384, bsz=32, num_updates=3600, lr=0.00045001, gnorm=0.639, loss_scale=64, train_wall=52, gb_free=9.7, wall=2130
2022-03-22 20:40:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:40:25 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.284 | ppl 155.81 | wps 50530.2 | wpb 511.2 | bsz 1 | num_updates 3693 | best_loss 7.284
2022-03-22 20:40:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3693 updates
2022-03-22 20:40:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:40:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:40:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 9 @ 3693 updates, score 7.284) (writing took 1.002993006259203 seconds)
2022-03-22 20:40:26 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-22 20:40:26 | INFO | train | epoch 009 | loss 7.325 | ppl 160.34 | wps 28434.8 | ups 1.74 | wpb 16367.8 | bsz 32 | num_updates 3693 | lr 0.000461633 | gnorm 0.647 | loss_scale 128 | train_wall 214 | gb_free 9.7 | wall 2188
2022-03-22 20:40:26 | INFO | fairseq.trainer | begin training epoch 10
2022-03-22 20:40:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:40:30 | INFO | train_inner | epoch 010:      7 / 411 loss=7.263, ppl=153.64, wps=26413.9, ups=1.62, wpb=16322.6, bsz=31.9, num_updates=3700, lr=0.000462508, gnorm=0.643, loss_scale=128, train_wall=52, gb_free=9.7, wall=2191
2022-03-22 20:41:26 | INFO | train_inner | epoch 010:    107 / 411 loss=7.126, ppl=139.69, wps=29186.6, ups=1.78, wpb=16384, bsz=32, num_updates=3800, lr=0.000475005, gnorm=0.657, loss_scale=128, train_wall=52, gb_free=9.7, wall=2248
2022-03-22 20:42:22 | INFO | train_inner | epoch 010:    207 / 411 loss=7.122, ppl=139.27, wps=29071.3, ups=1.77, wpb=16384, bsz=32, num_updates=3900, lr=0.000487503, gnorm=0.646, loss_scale=128, train_wall=52, gb_free=9.7, wall=2304
2022-03-22 20:43:18 | INFO | train_inner | epoch 010:    307 / 411 loss=7.088, ppl=136.03, wps=29164.1, ups=1.78, wpb=16384, bsz=32, num_updates=4000, lr=0.0005, gnorm=0.642, loss_scale=128, train_wall=52, gb_free=9.7, wall=2360
2022-03-22 20:43:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 20:44:15 | INFO | train_inner | epoch 010:    408 / 411 loss=7.066, ppl=133.99, wps=28847.5, ups=1.76, wpb=16378.9, bsz=32, num_updates=4100, lr=0.000493865, gnorm=0.649, loss_scale=64, train_wall=53, gb_free=9.7, wall=2417
2022-03-22 20:44:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:44:21 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.125 | ppl 139.55 | wps 50393.8 | wpb 511.2 | bsz 1 | num_updates 4103 | best_loss 7.125
2022-03-22 20:44:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 4103 updates
2022-03-22 20:44:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:44:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:44:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 10 @ 4103 updates, score 7.125) (writing took 0.9715053346008062 seconds)
2022-03-22 20:44:22 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-22 20:44:22 | INFO | train | epoch 010 | loss 7.101 | ppl 137.28 | wps 28366.8 | ups 1.73 | wpb 16367.8 | bsz 32 | num_updates 4103 | lr 0.000493684 | gnorm 0.649 | loss_scale 64 | train_wall 214 | gb_free 9.7 | wall 2424
2022-03-22 20:44:22 | INFO | fairseq.trainer | begin training epoch 11
2022-03-22 20:44:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:45:17 | INFO | train_inner | epoch 011:     97 / 411 loss=6.901, ppl=119.49, wps=26441.7, ups=1.62, wpb=16317.5, bsz=31.9, num_updates=4200, lr=0.00048795, gnorm=0.639, loss_scale=64, train_wall=52, gb_free=9.7, wall=2479
2022-03-22 20:46:13 | INFO | train_inner | epoch 011:    197 / 411 loss=6.899, ppl=119.35, wps=29158.2, ups=1.78, wpb=16384, bsz=32, num_updates=4300, lr=0.000482243, gnorm=0.634, loss_scale=64, train_wall=52, gb_free=9.7, wall=2535
2022-03-22 20:47:09 | INFO | train_inner | epoch 011:    297 / 411 loss=6.85, ppl=115.39, wps=29160.2, ups=1.78, wpb=16384, bsz=32, num_updates=4400, lr=0.000476731, gnorm=0.637, loss_scale=64, train_wall=52, gb_free=9.7, wall=2591
2022-03-22 20:48:05 | INFO | train_inner | epoch 011:    397 / 411 loss=6.879, ppl=117.73, wps=29164.8, ups=1.78, wpb=16384, bsz=32, num_updates=4500, lr=0.000471405, gnorm=0.63, loss_scale=64, train_wall=52, gb_free=9.7, wall=2647
2022-03-22 20:48:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:48:18 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.987 | ppl 126.87 | wps 50302.8 | wpb 511.2 | bsz 1 | num_updates 4514 | best_loss 6.987
2022-03-22 20:48:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4514 updates
2022-03-22 20:48:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:48:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:48:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 11 @ 4514 updates, score 6.987) (writing took 1.0012664534151554 seconds)
2022-03-22 20:48:19 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-22 20:48:19 | INFO | train | epoch 011 | loss 6.881 | ppl 117.88 | wps 28443.1 | ups 1.74 | wpb 16367.8 | bsz 32 | num_updates 4514 | lr 0.000470673 | gnorm 0.635 | loss_scale 64 | train_wall 214 | gb_free 9.7 | wall 2661
2022-03-22 20:48:19 | INFO | fairseq.trainer | begin training epoch 12
2022-03-22 20:48:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:49:07 | INFO | train_inner | epoch 012:     86 / 411 loss=6.708, ppl=104.51, wps=26398.3, ups=1.62, wpb=16322.6, bsz=31.9, num_updates=4600, lr=0.000466252, gnorm=0.63, loss_scale=128, train_wall=52, gb_free=9.7, wall=2709
2022-03-22 20:50:04 | INFO | train_inner | epoch 012:    186 / 411 loss=6.689, ppl=103.16, wps=29104.8, ups=1.78, wpb=16384, bsz=32, num_updates=4700, lr=0.000461266, gnorm=0.638, loss_scale=128, train_wall=52, gb_free=9.7, wall=2765
2022-03-22 20:51:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 20:51:00 | INFO | train_inner | epoch 012:    287 / 411 loss=6.703, ppl=104.2, wps=28889.5, ups=1.76, wpb=16378.9, bsz=32, num_updates=4800, lr=0.000456435, gnorm=0.632, loss_scale=64, train_wall=52, gb_free=9.7, wall=2822
2022-03-22 20:51:57 | INFO | train_inner | epoch 012:    387 / 411 loss=6.688, ppl=103.11, wps=29108.1, ups=1.78, wpb=16384, bsz=32, num_updates=4900, lr=0.000451754, gnorm=0.637, loss_scale=64, train_wall=52, gb_free=9.7, wall=2878
2022-03-22 20:52:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:52:14 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.883 | ppl 118.06 | wps 50121.6 | wpb 511.2 | bsz 1 | num_updates 4924 | best_loss 6.883
2022-03-22 20:52:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4924 updates
2022-03-22 20:52:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:52:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:52:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 12 @ 4924 updates, score 6.883) (writing took 0.9755349718034267 seconds)
2022-03-22 20:52:15 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-22 20:52:15 | INFO | train | epoch 012 | loss 6.688 | ppl 103.09 | wps 28362.7 | ups 1.73 | wpb 16367.8 | bsz 32 | num_updates 4924 | lr 0.000450652 | gnorm 0.634 | loss_scale 64 | train_wall 214 | gb_free 9.7 | wall 2897
2022-03-22 20:52:15 | INFO | fairseq.trainer | begin training epoch 13
2022-03-22 20:52:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:52:58 | INFO | train_inner | epoch 013:     76 / 411 loss=6.539, ppl=92.98, wps=26465.2, ups=1.62, wpb=16322.6, bsz=31.9, num_updates=5000, lr=0.000447214, gnorm=0.632, loss_scale=64, train_wall=52, gb_free=9.7, wall=2940
2022-03-22 20:53:54 | INFO | train_inner | epoch 013:    176 / 411 loss=6.518, ppl=91.62, wps=29103.8, ups=1.78, wpb=16378.9, bsz=32, num_updates=5100, lr=0.000442807, gnorm=0.637, loss_scale=64, train_wall=52, gb_free=9.7, wall=2996
2022-03-22 20:54:51 | INFO | train_inner | epoch 013:    276 / 411 loss=6.556, ppl=94.06, wps=29143.7, ups=1.78, wpb=16384, bsz=32, num_updates=5200, lr=0.000438529, gnorm=0.641, loss_scale=64, train_wall=52, gb_free=9.7, wall=3052
2022-03-22 20:55:47 | INFO | train_inner | epoch 013:    376 / 411 loss=6.537, ppl=92.84, wps=29154.4, ups=1.78, wpb=16384, bsz=32, num_updates=5300, lr=0.000434372, gnorm=0.64, loss_scale=64, train_wall=52, gb_free=9.7, wall=3109
2022-03-22 20:56:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 20:56:12 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.805 | ppl 111.84 | wps 47762.8 | wpb 511.2 | bsz 1 | num_updates 5335 | best_loss 6.805
2022-03-22 20:56:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5335 updates
2022-03-22 20:56:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:56:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 20:56:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 13 @ 5335 updates, score 6.805) (writing took 1.0145076885819435 seconds)
2022-03-22 20:56:13 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-22 20:56:13 | INFO | train | epoch 013 | loss 6.53 | ppl 92.4 | wps 28293.7 | ups 1.73 | wpb 16367.8 | bsz 32 | num_updates 5335 | lr 0.000432945 | gnorm 0.637 | loss_scale 128 | train_wall 215 | gb_free 9.7 | wall 3135
2022-03-22 20:56:13 | INFO | fairseq.trainer | begin training epoch 14
2022-03-22 20:56:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 20:56:50 | INFO | train_inner | epoch 014:     65 / 411 loss=6.416, ppl=85.38, wps=25922.1, ups=1.59, wpb=16322.6, bsz=31.9, num_updates=5400, lr=0.000430331, gnorm=0.636, loss_scale=128, train_wall=53, gb_free=9.7, wall=3172
2022-03-22 20:57:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 20:57:47 | INFO | train_inner | epoch 014:    166 / 411 loss=6.396, ppl=84.23, wps=28805.9, ups=1.76, wpb=16378.9, bsz=32, num_updates=5500, lr=0.000426401, gnorm=0.642, loss_scale=64, train_wall=53, gb_free=9.7, wall=3228
2022-03-22 20:58:43 | INFO | train_inner | epoch 014:    266 / 411 loss=6.398, ppl=84.34, wps=29123, ups=1.78, wpb=16384, bsz=32, num_updates=5600, lr=0.000422577, gnorm=0.643, loss_scale=64, train_wall=52, gb_free=9.7, wall=3285
2022-03-22 20:59:39 | INFO | train_inner | epoch 014:    366 / 411 loss=6.409, ppl=84.99, wps=29175.2, ups=1.78, wpb=16384, bsz=32, num_updates=5700, lr=0.000418854, gnorm=0.641, loss_scale=64, train_wall=52, gb_free=9.7, wall=3341
2022-03-22 21:00:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:00:09 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.751 | ppl 107.69 | wps 50299.7 | wpb 511.2 | bsz 1 | num_updates 5745 | best_loss 6.751
2022-03-22 21:00:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5745 updates
2022-03-22 21:00:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:00:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:00:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 14 @ 5745 updates, score 6.751) (writing took 0.9960588701069355 seconds)
2022-03-22 21:00:10 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-22 21:00:10 | INFO | train | epoch 014 | loss 6.395 | ppl 84.18 | wps 28349.5 | ups 1.73 | wpb 16367.8 | bsz 32 | num_updates 5745 | lr 0.00041721 | gnorm 0.641 | loss_scale 64 | train_wall 214 | gb_free 9.7 | wall 3372
2022-03-22 21:00:10 | INFO | fairseq.trainer | begin training epoch 15
2022-03-22 21:00:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:00:41 | INFO | train_inner | epoch 015:     55 / 411 loss=6.302, ppl=78.91, wps=26418.7, ups=1.62, wpb=16322.6, bsz=31.9, num_updates=5800, lr=0.000415227, gnorm=0.643, loss_scale=64, train_wall=52, gb_free=9.7, wall=3403
2022-03-22 21:01:37 | INFO | train_inner | epoch 015:    155 / 411 loss=6.256, ppl=76.45, wps=29117.4, ups=1.78, wpb=16384, bsz=32, num_updates=5900, lr=0.000411693, gnorm=0.645, loss_scale=64, train_wall=52, gb_free=9.7, wall=3459
2022-03-22 21:02:33 | INFO | train_inner | epoch 015:    255 / 411 loss=6.294, ppl=78.44, wps=29112.9, ups=1.78, wpb=16378.9, bsz=32, num_updates=6000, lr=0.000408248, gnorm=0.652, loss_scale=128, train_wall=52, gb_free=9.7, wall=3515
2022-03-22 21:03:30 | INFO | train_inner | epoch 015:    355 / 411 loss=6.314, ppl=79.55, wps=29069.4, ups=1.77, wpb=16384, bsz=32, num_updates=6100, lr=0.000404888, gnorm=0.644, loss_scale=128, train_wall=52, gb_free=9.7, wall=3572
2022-03-22 21:04:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:04:06 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.69 | ppl 103.23 | wps 50095.3 | wpb 511.2 | bsz 1 | num_updates 6156 | best_loss 6.69
2022-03-22 21:04:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 6156 updates
2022-03-22 21:04:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:04:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:04:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 15 @ 6156 updates, score 6.69) (writing took 0.9995746444910765 seconds)
2022-03-22 21:04:07 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-22 21:04:07 | INFO | train | epoch 015 | loss 6.281 | ppl 77.75 | wps 28410.2 | ups 1.74 | wpb 16367.8 | bsz 32 | num_updates 6156 | lr 0.000403042 | gnorm 0.646 | loss_scale 128 | train_wall 214 | gb_free 9.7 | wall 3608
2022-03-22 21:04:07 | INFO | fairseq.trainer | begin training epoch 16
2022-03-22 21:04:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:04:32 | INFO | train_inner | epoch 016:     44 / 411 loss=6.231, ppl=75.13, wps=26428.7, ups=1.62, wpb=16317.5, bsz=31.9, num_updates=6200, lr=0.00040161, gnorm=0.644, loss_scale=128, train_wall=52, gb_free=9.7, wall=3633
2022-03-22 21:05:28 | INFO | train_inner | epoch 016:    144 / 411 loss=6.158, ppl=71.42, wps=29072, ups=1.77, wpb=16384, bsz=32, num_updates=6300, lr=0.00039841, gnorm=0.649, loss_scale=128, train_wall=52, gb_free=9.7, wall=3690
2022-03-22 21:05:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:06:25 | INFO | train_inner | epoch 016:    245 / 411 loss=6.182, ppl=72.59, wps=28868.7, ups=1.76, wpb=16384, bsz=32, num_updates=6400, lr=0.000395285, gnorm=0.653, loss_scale=64, train_wall=53, gb_free=9.7, wall=3746
2022-03-22 21:07:21 | INFO | train_inner | epoch 016:    345 / 411 loss=6.198, ppl=73.4, wps=29157.7, ups=1.78, wpb=16384, bsz=32, num_updates=6500, lr=0.000392232, gnorm=0.653, loss_scale=64, train_wall=52, gb_free=9.7, wall=3803
2022-03-22 21:07:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:08:02 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.662 | ppl 101.25 | wps 50292.3 | wpb 511.2 | bsz 1 | num_updates 6566 | best_loss 6.662
2022-03-22 21:08:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6566 updates
2022-03-22 21:08:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:08:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:08:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 16 @ 6566 updates, score 6.662) (writing took 0.9986708760261536 seconds)
2022-03-22 21:08:03 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-22 21:08:03 | INFO | train | epoch 016 | loss 6.18 | ppl 72.5 | wps 28339.8 | ups 1.73 | wpb 16367.8 | bsz 32 | num_updates 6566 | lr 0.000390256 | gnorm 0.65 | loss_scale 64 | train_wall 214 | gb_free 9.7 | wall 3845
2022-03-22 21:08:04 | INFO | fairseq.trainer | begin training epoch 17
2022-03-22 21:08:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:08:23 | INFO | train_inner | epoch 017:     34 / 411 loss=6.146, ppl=70.83, wps=26432.6, ups=1.62, wpb=16322.6, bsz=31.9, num_updates=6600, lr=0.000389249, gnorm=0.653, loss_scale=64, train_wall=52, gb_free=9.7, wall=3864
2022-03-22 21:09:19 | INFO | train_inner | epoch 017:    134 / 411 loss=6.06, ppl=66.73, wps=29123.2, ups=1.78, wpb=16378.9, bsz=32, num_updates=6700, lr=0.000386334, gnorm=0.651, loss_scale=64, train_wall=52, gb_free=9.7, wall=3921
2022-03-22 21:10:15 | INFO | train_inner | epoch 017:    234 / 411 loss=6.097, ppl=68.46, wps=29164.1, ups=1.78, wpb=16384, bsz=32, num_updates=6800, lr=0.000383482, gnorm=0.653, loss_scale=64, train_wall=52, gb_free=9.7, wall=3977
2022-03-22 21:10:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:11:12 | INFO | train_inner | epoch 017:    335 / 411 loss=6.128, ppl=69.93, wps=28907.5, ups=1.76, wpb=16384, bsz=32, num_updates=6900, lr=0.000380693, gnorm=0.658, loss_scale=64, train_wall=53, gb_free=9.7, wall=4033
2022-03-22 21:11:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:11:59 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.62 | ppl 98.33 | wps 50235.2 | wpb 511.2 | bsz 1 | num_updates 6976 | best_loss 6.62
2022-03-22 21:11:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6976 updates
2022-03-22 21:11:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:12:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:12:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 17 @ 6976 updates, score 6.62) (writing took 1.00095584243536 seconds)
2022-03-22 21:12:00 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-22 21:12:00 | INFO | train | epoch 017 | loss 6.091 | ppl 68.17 | wps 28369.9 | ups 1.73 | wpb 16367.8 | bsz 32 | num_updates 6976 | lr 0.000378614 | gnorm 0.656 | loss_scale 64 | train_wall 214 | gb_free 9.7 | wall 4082
2022-03-22 21:12:00 | INFO | fairseq.trainer | begin training epoch 18
2022-03-22 21:12:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:12:14 | INFO | train_inner | epoch 018:     24 / 411 loss=6.064, ppl=66.92, wps=26391.8, ups=1.62, wpb=16322.6, bsz=31.9, num_updates=7000, lr=0.000377964, gnorm=0.66, loss_scale=64, train_wall=52, gb_free=9.7, wall=4095
2022-03-22 21:13:10 | INFO | train_inner | epoch 018:    124 / 411 loss=5.997, ppl=63.85, wps=29149.4, ups=1.78, wpb=16378.9, bsz=32, num_updates=7100, lr=0.000375293, gnorm=0.659, loss_scale=64, train_wall=52, gb_free=9.7, wall=4152
2022-03-22 21:14:06 | INFO | train_inner | epoch 018:    224 / 411 loss=6.006, ppl=64.26, wps=29075.8, ups=1.77, wpb=16384, bsz=32, num_updates=7200, lr=0.000372678, gnorm=0.663, loss_scale=64, train_wall=52, gb_free=9.7, wall=4208
2022-03-22 21:15:01 | INFO | train_inner | epoch 018:    324 / 411 loss=6.027, ppl=65.22, wps=29793.5, ups=1.82, wpb=16384, bsz=32, num_updates=7300, lr=0.000370117, gnorm=0.666, loss_scale=64, train_wall=51, gb_free=9.7, wall=4263
2022-03-22 21:15:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:15:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:15:52 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.581 | ppl 95.73 | wps 53804.7 | wpb 511.2 | bsz 1 | num_updates 7386 | best_loss 6.581
2022-03-22 21:15:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7386 updates
2022-03-22 21:15:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:15:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:15:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 18 @ 7386 updates, score 6.581) (writing took 0.9323019608855247 seconds)
2022-03-22 21:15:53 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-22 21:15:53 | INFO | train | epoch 018 | loss 6.011 | ppl 64.49 | wps 28817 | ups 1.76 | wpb 16367.8 | bsz 32 | num_updates 7386 | lr 0.000367956 | gnorm 0.665 | loss_scale 64 | train_wall 211 | gb_free 9.7 | wall 4315
2022-03-22 21:15:53 | INFO | fairseq.trainer | begin training epoch 19
2022-03-22 21:15:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:16:00 | INFO | train_inner | epoch 019:     14 / 411 loss=6.013, ppl=64.58, wps=27499.4, ups=1.68, wpb=16322.6, bsz=31.9, num_updates=7400, lr=0.000367607, gnorm=0.673, loss_scale=64, train_wall=50, gb_free=9.7, wall=4322
2022-03-22 21:16:53 | INFO | train_inner | epoch 019:    114 / 411 loss=5.896, ppl=59.55, wps=30912.2, ups=1.89, wpb=16384, bsz=32, num_updates=7500, lr=0.000365148, gnorm=0.672, loss_scale=64, train_wall=49, gb_free=9.7, wall=4375
2022-03-22 21:17:47 | INFO | train_inner | epoch 019:    214 / 411 loss=5.945, ppl=61.59, wps=30803.6, ups=1.88, wpb=16384, bsz=32, num_updates=7600, lr=0.000362738, gnorm=0.671, loss_scale=64, train_wall=49, gb_free=9.7, wall=4428
2022-03-22 21:18:41 | INFO | train_inner | epoch 019:    314 / 411 loss=5.949, ppl=61.76, wps=30320.7, ups=1.85, wpb=16384, bsz=32, num_updates=7700, lr=0.000360375, gnorm=0.671, loss_scale=64, train_wall=50, gb_free=9.7, wall=4482
2022-03-22 21:19:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:19:36 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.554 | ppl 93.99 | wps 53787.5 | wpb 511.2 | bsz 1 | num_updates 7797 | best_loss 6.554
2022-03-22 21:19:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7797 updates
2022-03-22 21:19:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:19:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:19:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 19 @ 7797 updates, score 6.554) (writing took 0.9641021657735109 seconds)
2022-03-22 21:19:37 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-22 21:19:37 | INFO | train | epoch 019 | loss 5.939 | ppl 61.37 | wps 29975.2 | ups 1.83 | wpb 16367.8 | bsz 32 | num_updates 7797 | lr 0.000358126 | gnorm 0.671 | loss_scale 64 | train_wall 203 | gb_free 9.7 | wall 4539
2022-03-22 21:19:37 | INFO | fairseq.trainer | begin training epoch 20
2022-03-22 21:19:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:19:39 | INFO | train_inner | epoch 020:      3 / 411 loss=5.977, ppl=63.01, wps=27982.5, ups=1.71, wpb=16317.5, bsz=31.9, num_updates=7800, lr=0.000358057, gnorm=0.671, loss_scale=64, train_wall=49, gb_free=9.7, wall=4541
2022-03-22 21:20:32 | INFO | train_inner | epoch 020:    103 / 411 loss=5.824, ppl=56.65, wps=30873.7, ups=1.88, wpb=16378.9, bsz=32, num_updates=7900, lr=0.000355784, gnorm=0.677, loss_scale=128, train_wall=49, gb_free=9.7, wall=4594
2022-03-22 21:20:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:21:26 | INFO | train_inner | epoch 020:    204 / 411 loss=5.849, ppl=57.65, wps=30588.8, ups=1.87, wpb=16384, bsz=32, num_updates=8000, lr=0.000353553, gnorm=0.677, loss_scale=64, train_wall=50, gb_free=9.7, wall=4647
2022-03-22 21:22:19 | INFO | train_inner | epoch 020:    304 / 411 loss=5.887, ppl=59.19, wps=30854.8, ups=1.88, wpb=16384, bsz=32, num_updates=8100, lr=0.000351364, gnorm=0.68, loss_scale=64, train_wall=49, gb_free=9.7, wall=4700
2022-03-22 21:23:12 | INFO | train_inner | epoch 020:    404 / 411 loss=5.931, ppl=60.99, wps=30871.4, ups=1.88, wpb=16384, bsz=32, num_updates=8200, lr=0.000349215, gnorm=0.679, loss_scale=64, train_wall=49, gb_free=9.7, wall=4754
2022-03-22 21:23:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:23:20 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.541 | ppl 93.15 | wps 53896.3 | wpb 511.2 | bsz 1 | num_updates 8207 | best_loss 6.541
2022-03-22 21:23:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 8207 updates
2022-03-22 21:23:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:23:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:23:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 20 @ 8207 updates, score 6.541) (writing took 0.9427213668823242 seconds)
2022-03-22 21:23:21 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-22 21:23:21 | INFO | train | epoch 020 | loss 5.874 | ppl 58.65 | wps 30051.3 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 8207 | lr 0.000349066 | gnorm 0.679 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 4762
2022-03-22 21:23:21 | INFO | fairseq.trainer | begin training epoch 21
2022-03-22 21:23:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:24:10 | INFO | train_inner | epoch 021:     93 / 411 loss=5.774, ppl=54.7, wps=28008.9, ups=1.72, wpb=16317.5, bsz=31.9, num_updates=8300, lr=0.000347105, gnorm=0.679, loss_scale=64, train_wall=49, gb_free=9.7, wall=4812
2022-03-22 21:25:03 | INFO | train_inner | epoch 021:    193 / 411 loss=5.807, ppl=56, wps=30905.6, ups=1.89, wpb=16384, bsz=32, num_updates=8400, lr=0.000345033, gnorm=0.686, loss_scale=64, train_wall=49, gb_free=9.7, wall=4865
2022-03-22 21:25:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:25:57 | INFO | train_inner | epoch 021:    294 / 411 loss=5.82, ppl=56.51, wps=30610.1, ups=1.87, wpb=16384, bsz=32, num_updates=8500, lr=0.000342997, gnorm=0.686, loss_scale=64, train_wall=50, gb_free=9.7, wall=4918
2022-03-22 21:26:50 | INFO | train_inner | epoch 021:    394 / 411 loss=5.839, ppl=57.26, wps=30874.4, ups=1.88, wpb=16384, bsz=32, num_updates=8600, lr=0.000340997, gnorm=0.687, loss_scale=64, train_wall=49, gb_free=9.7, wall=4971
2022-03-22 21:26:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:27:03 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.516 | ppl 91.49 | wps 52872.5 | wpb 511.2 | bsz 1 | num_updates 8617 | best_loss 6.516
2022-03-22 21:27:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8617 updates
2022-03-22 21:27:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:27:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:27:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 21 @ 8617 updates, score 6.516) (writing took 0.9336443319916725 seconds)
2022-03-22 21:27:04 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-22 21:27:04 | INFO | train | epoch 021 | loss 5.813 | ppl 56.2 | wps 30065 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 8617 | lr 0.000340661 | gnorm 0.684 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 4986
2022-03-22 21:27:04 | INFO | fairseq.trainer | begin training epoch 22
2022-03-22 21:27:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:27:48 | INFO | train_inner | epoch 022:     83 / 411 loss=5.738, ppl=53.37, wps=27904.7, ups=1.71, wpb=16317.5, bsz=31.9, num_updates=8700, lr=0.000339032, gnorm=0.69, loss_scale=64, train_wall=49, gb_free=9.7, wall=5030
2022-03-22 21:28:41 | INFO | train_inner | epoch 022:    183 / 411 loss=5.723, ppl=52.83, wps=30843.9, ups=1.88, wpb=16384, bsz=32, num_updates=8800, lr=0.0003371, gnorm=0.691, loss_scale=64, train_wall=49, gb_free=9.7, wall=5083
2022-03-22 21:29:34 | INFO | train_inner | epoch 022:    283 / 411 loss=5.781, ppl=54.98, wps=30815.9, ups=1.88, wpb=16384, bsz=32, num_updates=8900, lr=0.000335201, gnorm=0.695, loss_scale=64, train_wall=49, gb_free=9.7, wall=5136
2022-03-22 21:30:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:30:28 | INFO | train_inner | epoch 022:    384 / 411 loss=5.799, ppl=55.69, wps=30549.6, ups=1.86, wpb=16384, bsz=32, num_updates=9000, lr=0.000333333, gnorm=0.695, loss_scale=64, train_wall=50, gb_free=9.7, wall=5190
2022-03-22 21:30:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:30:47 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.501 | ppl 90.59 | wps 53736.1 | wpb 511.2 | bsz 1 | num_updates 9027 | best_loss 6.501
2022-03-22 21:30:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 9027 updates
2022-03-22 21:30:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:30:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:30:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 22 @ 9027 updates, score 6.501) (writing took 0.912119934335351 seconds)
2022-03-22 21:30:47 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-22 21:30:47 | INFO | train | epoch 022 | loss 5.759 | ppl 54.14 | wps 30013.1 | ups 1.83 | wpb 16367.8 | bsz 32 | num_updates 9027 | lr 0.000332834 | gnorm 0.693 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 5209
2022-03-22 21:30:47 | INFO | fairseq.trainer | begin training epoch 23
2022-03-22 21:30:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:31:26 | INFO | train_inner | epoch 023:     73 / 411 loss=5.701, ppl=52.01, wps=28043.5, ups=1.72, wpb=16322.6, bsz=31.9, num_updates=9100, lr=0.000331497, gnorm=0.695, loss_scale=64, train_wall=49, gb_free=9.7, wall=5248
2022-03-22 21:32:19 | INFO | train_inner | epoch 023:    173 / 411 loss=5.696, ppl=51.86, wps=30849.2, ups=1.88, wpb=16384, bsz=32, num_updates=9200, lr=0.00032969, gnorm=0.699, loss_scale=64, train_wall=49, gb_free=9.7, wall=5301
2022-03-22 21:33:12 | INFO | train_inner | epoch 023:    273 / 411 loss=5.72, ppl=52.7, wps=30844.1, ups=1.88, wpb=16384, bsz=32, num_updates=9300, lr=0.000327913, gnorm=0.701, loss_scale=64, train_wall=49, gb_free=9.7, wall=5354
2022-03-22 21:34:06 | INFO | train_inner | epoch 023:    373 / 411 loss=5.73, ppl=53.08, wps=30844.8, ups=1.88, wpb=16378.9, bsz=32, num_updates=9400, lr=0.000326164, gnorm=0.7, loss_scale=64, train_wall=49, gb_free=9.7, wall=5407
2022-03-22 21:34:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:34:30 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.485 | ppl 89.59 | wps 54117.2 | wpb 511.2 | bsz 1 | num_updates 9438 | best_loss 6.485
2022-03-22 21:34:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9438 updates
2022-03-22 21:34:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:34:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:34:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 23 @ 9438 updates, score 6.485) (writing took 0.9112336244434118 seconds)
2022-03-22 21:34:31 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-22 21:34:31 | INFO | train | epoch 023 | loss 5.706 | ppl 52.21 | wps 30109.8 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 9438 | lr 0.000325507 | gnorm 0.7 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 5433
2022-03-22 21:34:31 | INFO | fairseq.trainer | begin training epoch 24
2022-03-22 21:34:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:35:04 | INFO | train_inner | epoch 024:     62 / 411 loss=5.656, ppl=50.43, wps=28006.3, ups=1.72, wpb=16322.6, bsz=31.9, num_updates=9500, lr=0.000324443, gnorm=0.704, loss_scale=128, train_wall=49, gb_free=9.7, wall=5466
2022-03-22 21:35:57 | INFO | train_inner | epoch 024:    162 / 411 loss=5.61, ppl=48.85, wps=30881.2, ups=1.88, wpb=16384, bsz=32, num_updates=9600, lr=0.000322749, gnorm=0.702, loss_scale=128, train_wall=49, gb_free=9.7, wall=5519
2022-03-22 21:36:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:36:51 | INFO | train_inner | epoch 024:    263 / 411 loss=5.673, ppl=51.02, wps=30541.9, ups=1.86, wpb=16384, bsz=32, num_updates=9700, lr=0.000321081, gnorm=0.714, loss_scale=64, train_wall=50, gb_free=9.7, wall=5572
2022-03-22 21:37:44 | INFO | train_inner | epoch 024:    363 / 411 loss=5.704, ppl=52.13, wps=30863, ups=1.88, wpb=16378.9, bsz=32, num_updates=9800, lr=0.000319438, gnorm=0.711, loss_scale=64, train_wall=49, gb_free=9.7, wall=5625
2022-03-22 21:38:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:38:13 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.48 | ppl 89.25 | wps 54317.6 | wpb 511.2 | bsz 1 | num_updates 9848 | best_loss 6.48
2022-03-22 21:38:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9848 updates
2022-03-22 21:38:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:38:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:38:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 24 @ 9848 updates, score 6.48) (writing took 0.9257567413151264 seconds)
2022-03-22 21:38:14 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-22 21:38:14 | INFO | train | epoch 024 | loss 5.659 | ppl 50.52 | wps 30036.2 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 9848 | lr 0.000318659 | gnorm 0.708 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 5656
2022-03-22 21:38:14 | INFO | fairseq.trainer | begin training epoch 25
2022-03-22 21:38:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:38:42 | INFO | train_inner | epoch 025:     52 / 411 loss=5.621, ppl=49.21, wps=27996.3, ups=1.72, wpb=16322.6, bsz=31.9, num_updates=9900, lr=0.000317821, gnorm=0.713, loss_scale=64, train_wall=49, gb_free=9.7, wall=5684
2022-03-22 21:39:35 | INFO | train_inner | epoch 025:    152 / 411 loss=5.571, ppl=47.53, wps=30843.4, ups=1.88, wpb=16384, bsz=32, num_updates=10000, lr=0.000316228, gnorm=0.712, loss_scale=64, train_wall=49, gb_free=9.7, wall=5737
2022-03-22 21:40:29 | INFO | train_inner | epoch 025:    252 / 411 loss=5.615, ppl=49.02, wps=30258.9, ups=1.85, wpb=16378.9, bsz=32, num_updates=10100, lr=0.000314658, gnorm=0.713, loss_scale=64, train_wall=50, gb_free=9.7, wall=5791
2022-03-22 21:40:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:41:23 | INFO | train_inner | epoch 025:    353 / 411 loss=5.666, ppl=50.77, wps=30501.9, ups=1.86, wpb=16384, bsz=32, num_updates=10200, lr=0.000313112, gnorm=0.716, loss_scale=64, train_wall=50, gb_free=9.7, wall=5845
2022-03-22 21:41:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:41:58 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.473 | ppl 88.84 | wps 53910.2 | wpb 511.2 | bsz 1 | num_updates 10258 | best_loss 6.473
2022-03-22 21:41:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 10258 updates
2022-03-22 21:41:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:41:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:41:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 25 @ 10258 updates, score 6.473) (writing took 0.9372537918388844 seconds)
2022-03-22 21:41:59 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-22 21:41:59 | INFO | train | epoch 025 | loss 5.614 | ppl 48.97 | wps 29878.9 | ups 1.83 | wpb 16367.8 | bsz 32 | num_updates 10258 | lr 0.000312226 | gnorm 0.714 | loss_scale 64 | train_wall 203 | gb_free 9.7 | wall 5881
2022-03-22 21:41:59 | INFO | fairseq.trainer | begin training epoch 26
2022-03-22 21:41:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:42:21 | INFO | train_inner | epoch 026:     42 / 411 loss=5.599, ppl=48.45, wps=27956.9, ups=1.71, wpb=16322.6, bsz=31.9, num_updates=10300, lr=0.000311588, gnorm=0.716, loss_scale=64, train_wall=49, gb_free=9.7, wall=5903
2022-03-22 21:43:14 | INFO | train_inner | epoch 026:    142 / 411 loss=5.529, ppl=46.19, wps=30869.8, ups=1.88, wpb=16378.9, bsz=32, num_updates=10400, lr=0.000310087, gnorm=0.716, loss_scale=64, train_wall=49, gb_free=9.7, wall=5956
2022-03-22 21:44:08 | INFO | train_inner | epoch 026:    242 / 411 loss=5.572, ppl=47.59, wps=30776.9, ups=1.88, wpb=16384, bsz=32, num_updates=10500, lr=0.000308607, gnorm=0.726, loss_scale=64, train_wall=49, gb_free=9.7, wall=6009
2022-03-22 21:45:01 | INFO | train_inner | epoch 026:    342 / 411 loss=5.609, ppl=48.82, wps=30863.6, ups=1.88, wpb=16384, bsz=32, num_updates=10600, lr=0.000307148, gnorm=0.723, loss_scale=64, train_wall=49, gb_free=9.7, wall=6062
2022-03-22 21:45:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:45:42 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.464 | ppl 88.27 | wps 54098 | wpb 511.2 | bsz 1 | num_updates 10669 | best_loss 6.464
2022-03-22 21:45:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10669 updates
2022-03-22 21:45:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:45:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:45:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 26 @ 10669 updates, score 6.464) (writing took 0.9378865752369165 seconds)
2022-03-22 21:45:42 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-22 21:45:42 | INFO | train | epoch 026 | loss 5.572 | ppl 47.59 | wps 30089.5 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 10669 | lr 0.000306153 | gnorm 0.722 | loss_scale 128 | train_wall 202 | gb_free 9.7 | wall 6104
2022-03-22 21:45:42 | INFO | fairseq.trainer | begin training epoch 27
2022-03-22 21:45:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:45:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:46:00 | INFO | train_inner | epoch 027:     32 / 411 loss=5.569, ppl=47.46, wps=27720, ups=1.7, wpb=16322.6, bsz=31.9, num_updates=10700, lr=0.000305709, gnorm=0.726, loss_scale=64, train_wall=50, gb_free=9.7, wall=6121
2022-03-22 21:46:53 | INFO | train_inner | epoch 027:    132 / 411 loss=5.498, ppl=45.19, wps=30792.3, ups=1.88, wpb=16378.9, bsz=32, num_updates=10800, lr=0.00030429, gnorm=0.722, loss_scale=64, train_wall=49, gb_free=9.7, wall=6175
2022-03-22 21:47:46 | INFO | train_inner | epoch 027:    232 / 411 loss=5.523, ppl=45.98, wps=30772.1, ups=1.88, wpb=16384, bsz=32, num_updates=10900, lr=0.000302891, gnorm=0.731, loss_scale=64, train_wall=49, gb_free=9.7, wall=6228
2022-03-22 21:48:39 | INFO | train_inner | epoch 027:    332 / 411 loss=5.566, ppl=47.38, wps=30850.7, ups=1.88, wpb=16384, bsz=32, num_updates=11000, lr=0.000301511, gnorm=0.73, loss_scale=64, train_wall=49, gb_free=9.7, wall=6281
2022-03-22 21:49:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:49:25 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.448 | ppl 87.29 | wps 53584.3 | wpb 511.2 | bsz 1 | num_updates 11079 | best_loss 6.448
2022-03-22 21:49:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 11079 updates
2022-03-22 21:49:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:49:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:49:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 27 @ 11079 updates, score 6.448) (writing took 0.8934712391346693 seconds)
2022-03-22 21:49:26 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-22 21:49:26 | INFO | train | epoch 027 | loss 5.533 | ppl 46.29 | wps 29991.8 | ups 1.83 | wpb 16367.8 | bsz 32 | num_updates 11079 | lr 0.000300434 | gnorm 0.728 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 6328
2022-03-22 21:49:26 | INFO | fairseq.trainer | begin training epoch 28
2022-03-22 21:49:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:49:37 | INFO | train_inner | epoch 028:     21 / 411 loss=5.544, ppl=46.65, wps=27958.3, ups=1.71, wpb=16322.6, bsz=31.9, num_updates=11100, lr=0.00030015, gnorm=0.732, loss_scale=64, train_wall=49, gb_free=9.7, wall=6339
2022-03-22 21:50:31 | INFO | train_inner | epoch 028:    121 / 411 loss=5.435, ppl=43.25, wps=30867.8, ups=1.88, wpb=16384, bsz=32, num_updates=11200, lr=0.000298807, gnorm=0.728, loss_scale=64, train_wall=49, gb_free=9.7, wall=6392
2022-03-22 21:50:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:51:24 | INFO | train_inner | epoch 028:    222 / 411 loss=5.497, ppl=45.17, wps=30572.8, ups=1.87, wpb=16378.9, bsz=32, num_updates=11300, lr=0.000297482, gnorm=0.736, loss_scale=64, train_wall=50, gb_free=9.7, wall=6446
2022-03-22 21:52:17 | INFO | train_inner | epoch 028:    322 / 411 loss=5.512, ppl=45.64, wps=30876.2, ups=1.88, wpb=16384, bsz=32, num_updates=11400, lr=0.000296174, gnorm=0.739, loss_scale=64, train_wall=49, gb_free=9.7, wall=6499
2022-03-22 21:53:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:53:09 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.44 | ppl 86.85 | wps 54056 | wpb 511.2 | bsz 1 | num_updates 11489 | best_loss 6.44
2022-03-22 21:53:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 11489 updates
2022-03-22 21:53:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:53:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:53:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 28 @ 11489 updates, score 6.44) (writing took 0.9147341251373291 seconds)
2022-03-22 21:53:10 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-22 21:53:10 | INFO | train | epoch 028 | loss 5.496 | ppl 45.12 | wps 30053.3 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 11489 | lr 0.000295025 | gnorm 0.735 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 6551
2022-03-22 21:53:10 | INFO | fairseq.trainer | begin training epoch 29
2022-03-22 21:53:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:53:15 | INFO | train_inner | epoch 029:     11 / 411 loss=5.539, ppl=46.5, wps=28023.5, ups=1.72, wpb=16322.6, bsz=31.9, num_updates=11500, lr=0.000294884, gnorm=0.74, loss_scale=64, train_wall=49, gb_free=9.7, wall=6557
2022-03-22 21:54:09 | INFO | train_inner | epoch 029:    111 / 411 loss=5.397, ppl=42.15, wps=30819.3, ups=1.88, wpb=16378.9, bsz=32, num_updates=11600, lr=0.00029361, gnorm=0.737, loss_scale=64, train_wall=49, gb_free=9.7, wall=6610
2022-03-22 21:55:02 | INFO | train_inner | epoch 029:    211 / 411 loss=5.455, ppl=43.86, wps=30875.1, ups=1.88, wpb=16384, bsz=32, num_updates=11700, lr=0.000292353, gnorm=0.741, loss_scale=64, train_wall=49, gb_free=9.7, wall=6663
2022-03-22 21:55:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 21:55:55 | INFO | train_inner | epoch 029:    312 / 411 loss=5.481, ppl=44.66, wps=30612.7, ups=1.87, wpb=16384, bsz=32, num_updates=11800, lr=0.000291111, gnorm=0.739, loss_scale=64, train_wall=50, gb_free=9.7, wall=6717
2022-03-22 21:56:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 21:56:52 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.43 | ppl 86.22 | wps 54448.3 | wpb 511.2 | bsz 1 | num_updates 11899 | best_loss 6.43
2022-03-22 21:56:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11899 updates
2022-03-22 21:56:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:56:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 21:56:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 29 @ 11899 updates, score 6.43) (writing took 0.9263497199863195 seconds)
2022-03-22 21:56:53 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-22 21:56:53 | INFO | train | epoch 029 | loss 5.46 | ppl 44 | wps 30053.1 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 11899 | lr 0.000289898 | gnorm 0.741 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 6775
2022-03-22 21:56:53 | INFO | fairseq.trainer | begin training epoch 30
2022-03-22 21:56:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 21:56:53 | INFO | train_inner | epoch 030:      1 / 411 loss=5.514, ppl=45.7, wps=28013.3, ups=1.72, wpb=16322.6, bsz=31.9, num_updates=11900, lr=0.000289886, gnorm=0.748, loss_scale=64, train_wall=49, gb_free=9.7, wall=6775
2022-03-22 21:57:46 | INFO | train_inner | epoch 030:    101 / 411 loss=5.365, ppl=41.22, wps=30913.8, ups=1.89, wpb=16384, bsz=32, num_updates=12000, lr=0.000288675, gnorm=0.741, loss_scale=64, train_wall=49, gb_free=9.7, wall=6828
2022-03-22 21:58:39 | INFO | train_inner | epoch 030:    201 / 411 loss=5.42, ppl=42.82, wps=30873.9, ups=1.88, wpb=16378.9, bsz=32, num_updates=12100, lr=0.00028748, gnorm=0.751, loss_scale=64, train_wall=49, gb_free=9.7, wall=6881
2022-03-22 21:59:33 | INFO | train_inner | epoch 030:    301 / 411 loss=5.451, ppl=43.75, wps=30879.9, ups=1.88, wpb=16384, bsz=32, num_updates=12200, lr=0.000286299, gnorm=0.755, loss_scale=64, train_wall=49, gb_free=9.7, wall=6934
2022-03-22 21:59:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 22:00:26 | INFO | train_inner | epoch 030:    402 / 411 loss=5.47, ppl=44.33, wps=30539.5, ups=1.86, wpb=16384, bsz=32, num_updates=12300, lr=0.000285133, gnorm=0.754, loss_scale=64, train_wall=50, gb_free=9.7, wall=6988
2022-03-22 22:00:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:00:35 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.425 | ppl 85.94 | wps 54013 | wpb 511.2 | bsz 1 | num_updates 12309 | best_loss 6.425
2022-03-22 22:00:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 12309 updates
2022-03-22 22:00:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:00:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:00:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 30 @ 12309 updates, score 6.425) (writing took 0.9366013705730438 seconds)
2022-03-22 22:00:36 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-22 22:00:36 | INFO | train | epoch 030 | loss 5.427 | ppl 43.01 | wps 30053.7 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 12309 | lr 0.000285029 | gnorm 0.751 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 6998
2022-03-22 22:00:36 | INFO | fairseq.trainer | begin training epoch 31
2022-03-22 22:00:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:01:25 | INFO | train_inner | epoch 031:     91 / 411 loss=5.322, ppl=40.01, wps=27988.4, ups=1.71, wpb=16322.6, bsz=31.9, num_updates=12400, lr=0.000283981, gnorm=0.752, loss_scale=64, train_wall=49, gb_free=9.7, wall=7046
2022-03-22 22:02:19 | INFO | train_inner | epoch 031:    191 / 411 loss=5.382, ppl=41.69, wps=30270.7, ups=1.85, wpb=16384, bsz=32, num_updates=12500, lr=0.000282843, gnorm=0.757, loss_scale=64, train_wall=50, gb_free=9.7, wall=7100
2022-03-22 22:03:12 | INFO | train_inner | epoch 031:    291 / 411 loss=5.414, ppl=42.62, wps=30836.7, ups=1.88, wpb=16384, bsz=32, num_updates=12600, lr=0.000281718, gnorm=0.757, loss_scale=64, train_wall=49, gb_free=9.7, wall=7154
2022-03-22 22:04:05 | INFO | train_inner | epoch 031:    391 / 411 loss=5.45, ppl=43.7, wps=30807.3, ups=1.88, wpb=16378.9, bsz=32, num_updates=12700, lr=0.000280607, gnorm=0.754, loss_scale=64, train_wall=49, gb_free=9.7, wall=7207
2022-03-22 22:04:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:04:20 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.423 | ppl 85.82 | wps 54625.8 | wpb 511.2 | bsz 1 | num_updates 12720 | best_loss 6.423
2022-03-22 22:04:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12720 updates
2022-03-22 22:04:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:04:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:04:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 31 @ 12720 updates, score 6.423) (writing took 0.9257717169821262 seconds)
2022-03-22 22:04:21 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-22 22:04:21 | INFO | train | epoch 031 | loss 5.395 | ppl 42.09 | wps 29956.4 | ups 1.83 | wpb 16367.8 | bsz 32 | num_updates 12720 | lr 0.000280386 | gnorm 0.756 | loss_scale 64 | train_wall 203 | gb_free 9.7 | wall 7222
2022-03-22 22:04:21 | INFO | fairseq.trainer | begin training epoch 32
2022-03-22 22:04:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:04:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 22:05:04 | INFO | train_inner | epoch 032:     81 / 411 loss=5.329, ppl=40.19, wps=27721.6, ups=1.7, wpb=16322.6, bsz=31.9, num_updates=12800, lr=0.000279508, gnorm=0.756, loss_scale=64, train_wall=50, gb_free=9.7, wall=7266
2022-03-22 22:05:57 | INFO | train_inner | epoch 032:    181 / 411 loss=5.35, ppl=40.78, wps=30905.6, ups=1.89, wpb=16384, bsz=32, num_updates=12900, lr=0.000278423, gnorm=0.765, loss_scale=64, train_wall=49, gb_free=9.7, wall=7319
2022-03-22 22:06:50 | INFO | train_inner | epoch 032:    281 / 411 loss=5.373, ppl=41.45, wps=30879.2, ups=1.89, wpb=16378.9, bsz=32, num_updates=13000, lr=0.00027735, gnorm=0.765, loss_scale=64, train_wall=49, gb_free=9.7, wall=7372
2022-03-22 22:07:43 | INFO | train_inner | epoch 032:    381 / 411 loss=5.419, ppl=42.78, wps=30803.6, ups=1.88, wpb=16384, bsz=32, num_updates=13100, lr=0.000276289, gnorm=0.764, loss_scale=64, train_wall=49, gb_free=9.7, wall=7425
2022-03-22 22:07:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:08:03 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.422 | ppl 85.72 | wps 54311.3 | wpb 511.2 | bsz 1 | num_updates 13130 | best_loss 6.422
2022-03-22 22:08:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 13130 updates
2022-03-22 22:08:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:08:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:08:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 32 @ 13130 updates, score 6.422) (writing took 0.9180146995931864 seconds)
2022-03-22 22:08:04 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-22 22:08:04 | INFO | train | epoch 032 | loss 5.364 | ppl 41.19 | wps 30043.5 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 13130 | lr 0.000275974 | gnorm 0.762 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 7446
2022-03-22 22:08:04 | INFO | fairseq.trainer | begin training epoch 33
2022-03-22 22:08:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:08:41 | INFO | train_inner | epoch 033:     70 / 411 loss=5.311, ppl=39.71, wps=28045.2, ups=1.72, wpb=16322.6, bsz=31.9, num_updates=13200, lr=0.000275241, gnorm=0.763, loss_scale=64, train_wall=49, gb_free=9.7, wall=7483
2022-03-22 22:09:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 22:09:35 | INFO | train_inner | epoch 033:    171 / 411 loss=5.316, ppl=39.84, wps=30556.2, ups=1.87, wpb=16384, bsz=32, num_updates=13300, lr=0.000274204, gnorm=0.765, loss_scale=64, train_wall=50, gb_free=9.7, wall=7537
2022-03-22 22:10:28 | INFO | train_inner | epoch 033:    271 / 411 loss=5.357, ppl=40.99, wps=30850.2, ups=1.88, wpb=16384, bsz=32, num_updates=13400, lr=0.000273179, gnorm=0.771, loss_scale=64, train_wall=49, gb_free=9.7, wall=7590
2022-03-22 22:11:21 | INFO | train_inner | epoch 033:    371 / 411 loss=5.368, ppl=41.29, wps=30907.2, ups=1.89, wpb=16378.9, bsz=32, num_updates=13500, lr=0.000272166, gnorm=0.779, loss_scale=64, train_wall=49, gb_free=9.7, wall=7643
2022-03-22 22:11:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:11:46 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.418 | ppl 85.48 | wps 54256.8 | wpb 511.2 | bsz 1 | num_updates 13540 | best_loss 6.418
2022-03-22 22:11:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 13540 updates
2022-03-22 22:11:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:11:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:11:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 33 @ 13540 updates, score 6.418) (writing took 0.9100007005035877 seconds)
2022-03-22 22:11:47 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-22 22:11:47 | INFO | train | epoch 033 | loss 5.336 | ppl 40.41 | wps 30058.4 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 13540 | lr 0.000271763 | gnorm 0.77 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 7669
2022-03-22 22:11:47 | INFO | fairseq.trainer | begin training epoch 34
2022-03-22 22:11:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:12:19 | INFO | train_inner | epoch 034:     60 / 411 loss=5.281, ppl=38.87, wps=28040, ups=1.72, wpb=16322.6, bsz=31.9, num_updates=13600, lr=0.000271163, gnorm=0.769, loss_scale=64, train_wall=49, gb_free=9.7, wall=7701
2022-03-22 22:13:12 | INFO | train_inner | epoch 034:    160 / 411 loss=5.274, ppl=38.69, wps=30791.9, ups=1.88, wpb=16384, bsz=32, num_updates=13700, lr=0.000270172, gnorm=0.772, loss_scale=64, train_wall=49, gb_free=9.7, wall=7754
2022-03-22 22:14:06 | INFO | train_inner | epoch 034:    260 / 411 loss=5.308, ppl=39.63, wps=30810.6, ups=1.88, wpb=16378.9, bsz=32, num_updates=13800, lr=0.000269191, gnorm=0.777, loss_scale=64, train_wall=49, gb_free=9.7, wall=7807
2022-03-22 22:14:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 22:14:59 | INFO | train_inner | epoch 034:    361 / 411 loss=5.353, ppl=40.86, wps=30565.3, ups=1.87, wpb=16384, bsz=32, num_updates=13900, lr=0.000268221, gnorm=0.776, loss_scale=64, train_wall=50, gb_free=9.7, wall=7861
2022-03-22 22:15:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:15:30 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.411 | ppl 85.1 | wps 54161.9 | wpb 511.2 | bsz 1 | num_updates 13950 | best_loss 6.411
2022-03-22 22:15:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13950 updates
2022-03-22 22:15:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:15:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:15:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 34 @ 13950 updates, score 6.411) (writing took 0.900001959875226 seconds)
2022-03-22 22:15:31 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-22 22:15:31 | INFO | train | epoch 034 | loss 5.308 | ppl 39.62 | wps 30022 | ups 1.83 | wpb 16367.8 | bsz 32 | num_updates 13950 | lr 0.00026774 | gnorm 0.775 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 7893
2022-03-22 22:15:31 | INFO | fairseq.trainer | begin training epoch 35
2022-03-22 22:15:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:15:58 | INFO | train_inner | epoch 035:     50 / 411 loss=5.291, ppl=39.14, wps=27963.9, ups=1.71, wpb=16322.6, bsz=31.9, num_updates=14000, lr=0.000267261, gnorm=0.781, loss_scale=64, train_wall=49, gb_free=9.7, wall=7919
2022-03-22 22:16:51 | INFO | train_inner | epoch 035:    150 / 411 loss=5.248, ppl=38.01, wps=30833.3, ups=1.88, wpb=16384, bsz=32, num_updates=14100, lr=0.000266312, gnorm=0.779, loss_scale=64, train_wall=49, gb_free=9.7, wall=7972
2022-03-22 22:17:44 | INFO | train_inner | epoch 035:    250 / 411 loss=5.286, ppl=39, wps=30847.7, ups=1.88, wpb=16378.9, bsz=32, num_updates=14200, lr=0.000265372, gnorm=0.785, loss_scale=64, train_wall=49, gb_free=9.7, wall=8026
2022-03-22 22:18:37 | INFO | train_inner | epoch 035:    350 / 411 loss=5.326, ppl=40.11, wps=30909, ups=1.89, wpb=16384, bsz=32, num_updates=14300, lr=0.000264443, gnorm=0.784, loss_scale=64, train_wall=49, gb_free=9.7, wall=8079
2022-03-22 22:18:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 22:19:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:19:13 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.409 | ppl 84.99 | wps 53926.2 | wpb 511.2 | bsz 1 | num_updates 14360 | best_loss 6.409
2022-03-22 22:19:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 14360 updates
2022-03-22 22:19:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:19:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:19:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 35 @ 14360 updates, score 6.409) (writing took 0.9224811159074306 seconds)
2022-03-22 22:19:14 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-22 22:19:14 | INFO | train | epoch 035 | loss 5.283 | ppl 38.92 | wps 30036 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 14360 | lr 0.00026389 | gnorm 0.784 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 8116
2022-03-22 22:19:14 | INFO | fairseq.trainer | begin training epoch 36
2022-03-22 22:19:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:19:36 | INFO | train_inner | epoch 036:     40 / 411 loss=5.274, ppl=38.69, wps=27759.5, ups=1.7, wpb=16322.6, bsz=31.9, num_updates=14400, lr=0.000263523, gnorm=0.787, loss_scale=64, train_wall=49, gb_free=9.7, wall=8137
2022-03-22 22:20:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 22:20:29 | INFO | train_inner | epoch 036:    141 / 411 loss=5.211, ppl=37.04, wps=30511.3, ups=1.86, wpb=16384, bsz=32, num_updates=14500, lr=0.000262613, gnorm=0.786, loss_scale=32, train_wall=50, gb_free=9.7, wall=8191
2022-03-22 22:21:22 | INFO | train_inner | epoch 036:    241 / 411 loss=5.252, ppl=38.12, wps=30849.2, ups=1.88, wpb=16384, bsz=32, num_updates=14600, lr=0.000261712, gnorm=0.786, loss_scale=32, train_wall=49, gb_free=9.7, wall=8244
2022-03-22 22:22:16 | INFO | train_inner | epoch 036:    341 / 411 loss=5.289, ppl=39.09, wps=30768.8, ups=1.88, wpb=16378.9, bsz=32, num_updates=14700, lr=0.00026082, gnorm=0.791, loss_scale=32, train_wall=49, gb_free=9.7, wall=8297
2022-03-22 22:22:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:22:57 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.403 | ppl 84.63 | wps 54217.7 | wpb 511.2 | bsz 1 | num_updates 14770 | best_loss 6.403
2022-03-22 22:22:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14770 updates
2022-03-22 22:22:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:22:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:22:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 36 @ 14770 updates, score 6.403) (writing took 0.9149750526994467 seconds)
2022-03-22 22:22:58 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-22 22:22:58 | INFO | train | epoch 036 | loss 5.257 | ppl 38.23 | wps 30006.5 | ups 1.83 | wpb 16367.8 | bsz 32 | num_updates 14770 | lr 0.000260201 | gnorm 0.788 | loss_scale 32 | train_wall 202 | gb_free 9.7 | wall 8340
2022-03-22 22:22:58 | INFO | fairseq.trainer | begin training epoch 37
2022-03-22 22:22:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:23:15 | INFO | train_inner | epoch 037:     30 / 411 loss=5.269, ppl=38.55, wps=27518.7, ups=1.69, wpb=16322.6, bsz=31.9, num_updates=14800, lr=0.000259938, gnorm=0.791, loss_scale=32, train_wall=50, gb_free=9.7, wall=8357
2022-03-22 22:24:08 | INFO | train_inner | epoch 037:    130 / 411 loss=5.191, ppl=36.53, wps=30840.7, ups=1.88, wpb=16384, bsz=32, num_updates=14900, lr=0.000259064, gnorm=0.79, loss_scale=32, train_wall=49, gb_free=9.7, wall=8410
2022-03-22 22:25:01 | INFO | train_inner | epoch 037:    230 / 411 loss=5.237, ppl=37.72, wps=30850.7, ups=1.88, wpb=16378.9, bsz=32, num_updates=15000, lr=0.000258199, gnorm=0.797, loss_scale=64, train_wall=49, gb_free=9.7, wall=8463
2022-03-22 22:25:54 | INFO | train_inner | epoch 037:    330 / 411 loss=5.264, ppl=38.42, wps=30908.8, ups=1.89, wpb=16384, bsz=32, num_updates=15100, lr=0.000257343, gnorm=0.798, loss_scale=64, train_wall=49, gb_free=9.7, wall=8516
2022-03-22 22:26:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:26:41 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.405 | ppl 84.74 | wps 54128.5 | wpb 511.2 | bsz 1 | num_updates 15181 | best_loss 6.403
2022-03-22 22:26:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 15181 updates
2022-03-22 22:26:41 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-22 22:26:41 | INFO | train | epoch 037 | loss 5.232 | ppl 37.59 | wps 30096.5 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 15181 | lr 0.000256655 | gnorm 0.794 | loss_scale 64 | train_wall 203 | gb_free 9.7 | wall 8563
2022-03-22 22:26:41 | INFO | fairseq.trainer | begin training epoch 38
2022-03-22 22:26:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:26:52 | INFO | train_inner | epoch 038:     19 / 411 loss=5.249, ppl=38.02, wps=28393.3, ups=1.74, wpb=16322.6, bsz=31.9, num_updates=15200, lr=0.000256495, gnorm=0.798, loss_scale=64, train_wall=49, gb_free=9.7, wall=8573
2022-03-22 22:27:45 | INFO | train_inner | epoch 038:    119 / 411 loss=5.163, ppl=35.84, wps=30875.1, ups=1.89, wpb=16378.9, bsz=32, num_updates=15300, lr=0.000255655, gnorm=0.795, loss_scale=64, train_wall=49, gb_free=9.7, wall=8626
2022-03-22 22:28:38 | INFO | train_inner | epoch 038:    219 / 411 loss=5.19, ppl=36.51, wps=30817.4, ups=1.88, wpb=16384, bsz=32, num_updates=15400, lr=0.000254824, gnorm=0.801, loss_scale=64, train_wall=49, gb_free=9.7, wall=8680
2022-03-22 22:29:31 | INFO | train_inner | epoch 038:    319 / 411 loss=5.252, ppl=38.09, wps=30832.9, ups=1.88, wpb=16384, bsz=32, num_updates=15500, lr=0.000254, gnorm=0.801, loss_scale=128, train_wall=49, gb_free=9.7, wall=8733
2022-03-22 22:29:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 22:30:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:30:24 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.412 | ppl 85.16 | wps 53835.8 | wpb 511.2 | bsz 1 | num_updates 15591 | best_loss 6.403
2022-03-22 22:30:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 15591 updates
2022-03-22 22:30:24 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-22 22:30:24 | INFO | train | epoch 038 | loss 5.21 | ppl 37.02 | wps 30149.2 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 15591 | lr 0.000253258 | gnorm 0.801 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 8786
2022-03-22 22:30:24 | INFO | fairseq.trainer | begin training epoch 39
2022-03-22 22:30:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:30:29 | INFO | train_inner | epoch 039:      9 / 411 loss=5.238, ppl=37.74, wps=28212.5, ups=1.73, wpb=16322.6, bsz=31.9, num_updates=15600, lr=0.000253185, gnorm=0.807, loss_scale=64, train_wall=49, gb_free=9.7, wall=8791
2022-03-22 22:31:22 | INFO | train_inner | epoch 039:    109 / 411 loss=5.118, ppl=34.72, wps=30842.6, ups=1.88, wpb=16384, bsz=32, num_updates=15700, lr=0.000252377, gnorm=0.803, loss_scale=64, train_wall=49, gb_free=9.7, wall=8844
2022-03-22 22:32:15 | INFO | train_inner | epoch 039:    209 / 411 loss=5.197, ppl=36.69, wps=30795, ups=1.88, wpb=16378.9, bsz=32, num_updates=15800, lr=0.000251577, gnorm=0.808, loss_scale=64, train_wall=49, gb_free=9.7, wall=8897
2022-03-22 22:33:08 | INFO | train_inner | epoch 039:    309 / 411 loss=5.205, ppl=36.89, wps=30818.6, ups=1.88, wpb=16384, bsz=32, num_updates=15900, lr=0.000250785, gnorm=0.806, loss_scale=64, train_wall=49, gb_free=9.7, wall=8950
2022-03-22 22:33:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 22:34:02 | INFO | train_inner | epoch 039:    410 / 411 loss=5.237, ppl=37.71, wps=30545.4, ups=1.86, wpb=16384, bsz=32, num_updates=16000, lr=0.00025, gnorm=0.811, loss_scale=32, train_wall=50, gb_free=9.7, wall=9004
2022-03-22 22:34:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:34:07 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.4 | ppl 84.46 | wps 53828.7 | wpb 511.2 | bsz 1 | num_updates 16001 | best_loss 6.4
2022-03-22 22:34:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 16001 updates
2022-03-22 22:34:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:34:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:34:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 39 @ 16001 updates, score 6.4) (writing took 0.9433110188692808 seconds)
2022-03-22 22:34:08 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-22 22:34:08 | INFO | train | epoch 039 | loss 5.188 | ppl 36.44 | wps 30005.8 | ups 1.83 | wpb 16367.8 | bsz 32 | num_updates 16001 | lr 0.000249992 | gnorm 0.808 | loss_scale 32 | train_wall 202 | gb_free 9.7 | wall 9009
2022-03-22 22:34:08 | INFO | fairseq.trainer | begin training epoch 40
2022-03-22 22:34:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:35:00 | INFO | train_inner | epoch 040:     99 / 411 loss=5.098, ppl=34.25, wps=27988.6, ups=1.72, wpb=16317.5, bsz=31.9, num_updates=16100, lr=0.000249222, gnorm=0.808, loss_scale=32, train_wall=49, gb_free=9.7, wall=9062
2022-03-22 22:35:53 | INFO | train_inner | epoch 040:    199 / 411 loss=5.163, ppl=35.82, wps=30903.9, ups=1.89, wpb=16384, bsz=32, num_updates=16200, lr=0.000248452, gnorm=0.812, loss_scale=32, train_wall=49, gb_free=9.7, wall=9115
2022-03-22 22:36:46 | INFO | train_inner | epoch 040:    299 / 411 loss=5.185, ppl=36.37, wps=30838.1, ups=1.88, wpb=16384, bsz=32, num_updates=16300, lr=0.000247689, gnorm=0.816, loss_scale=32, train_wall=49, gb_free=9.7, wall=9168
2022-03-22 22:37:40 | INFO | train_inner | epoch 040:    399 / 411 loss=5.211, ppl=37.05, wps=30842.8, ups=1.88, wpb=16384, bsz=32, num_updates=16400, lr=0.000246932, gnorm=0.816, loss_scale=32, train_wall=49, gb_free=9.7, wall=9221
2022-03-22 22:37:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:37:50 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.403 | ppl 84.6 | wps 54161.8 | wpb 511.2 | bsz 1 | num_updates 16412 | best_loss 6.4
2022-03-22 22:37:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 16412 updates
2022-03-22 22:37:50 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-22 22:37:50 | INFO | train | epoch 040 | loss 5.166 | ppl 35.91 | wps 30231.8 | ups 1.85 | wpb 16367.8 | bsz 32 | num_updates 16412 | lr 0.000246842 | gnorm 0.813 | loss_scale 32 | train_wall 202 | gb_free 9.7 | wall 9232
2022-03-22 22:37:50 | INFO | fairseq.trainer | begin training epoch 41
2022-03-22 22:37:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:38:37 | INFO | train_inner | epoch 041:     88 / 411 loss=5.096, ppl=34.2, wps=28440.1, ups=1.74, wpb=16322.6, bsz=31.9, num_updates=16500, lr=0.000246183, gnorm=0.814, loss_scale=64, train_wall=49, gb_free=9.7, wall=9279
2022-03-22 22:39:30 | INFO | train_inner | epoch 041:    188 / 411 loss=5.135, ppl=35.13, wps=30844.2, ups=1.88, wpb=16384, bsz=32, num_updates=16600, lr=0.00024544, gnorm=0.817, loss_scale=64, train_wall=49, gb_free=9.7, wall=9332
2022-03-22 22:40:23 | INFO | train_inner | epoch 041:    288 / 411 loss=5.149, ppl=35.47, wps=30863.8, ups=1.88, wpb=16384, bsz=32, num_updates=16700, lr=0.000244704, gnorm=0.822, loss_scale=64, train_wall=49, gb_free=9.7, wall=9385
2022-03-22 22:41:16 | INFO | train_inner | epoch 041:    388 / 411 loss=5.201, ppl=36.78, wps=30902.9, ups=1.89, wpb=16378.9, bsz=32, num_updates=16800, lr=0.000243975, gnorm=0.821, loss_scale=64, train_wall=49, gb_free=9.7, wall=9438
2022-03-22 22:41:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:41:32 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.392 | ppl 83.98 | wps 54316.7 | wpb 511.2 | bsz 1 | num_updates 16823 | best_loss 6.392
2022-03-22 22:41:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16823 updates
2022-03-22 22:41:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:41:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-22 22:41:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 41 @ 16823 updates, score 6.392) (writing took 0.9312402363866568 seconds)
2022-03-22 22:41:33 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-22 22:41:33 | INFO | train | epoch 041 | loss 5.145 | ppl 35.39 | wps 30134.9 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 16823 | lr 0.000243808 | gnorm 0.818 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 9455
2022-03-22 22:41:33 | INFO | fairseq.trainer | begin training epoch 42
2022-03-22 22:41:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:42:14 | INFO | train_inner | epoch 042:     77 / 411 loss=5.087, ppl=34, wps=28041.7, ups=1.72, wpb=16322.6, bsz=31.9, num_updates=16900, lr=0.000243252, gnorm=0.816, loss_scale=64, train_wall=49, gb_free=9.7, wall=9496
2022-03-22 22:42:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 22:43:08 | INFO | train_inner | epoch 042:    178 / 411 loss=5.099, ppl=34.28, wps=30592.7, ups=1.87, wpb=16384, bsz=32, num_updates=17000, lr=0.000242536, gnorm=0.824, loss_scale=64, train_wall=50, gb_free=9.7, wall=9550
2022-03-22 22:44:01 | INFO | train_inner | epoch 042:    278 / 411 loss=5.139, ppl=35.25, wps=30844.7, ups=1.88, wpb=16384, bsz=32, num_updates=17100, lr=0.000241825, gnorm=0.824, loss_scale=64, train_wall=49, gb_free=9.7, wall=9603
2022-03-22 22:44:54 | INFO | train_inner | epoch 042:    378 / 411 loss=5.181, ppl=36.28, wps=30864.4, ups=1.88, wpb=16378.9, bsz=32, num_updates=17200, lr=0.000241121, gnorm=0.83, loss_scale=64, train_wall=49, gb_free=9.7, wall=9656
2022-03-22 22:45:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:45:16 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.403 | ppl 84.61 | wps 51400.8 | wpb 511.2 | bsz 1 | num_updates 17233 | best_loss 6.392
2022-03-22 22:45:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 17233 updates
2022-03-22 22:45:16 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-22 22:45:16 | INFO | train | epoch 042 | loss 5.125 | ppl 34.9 | wps 30127.7 | ups 1.84 | wpb 16367.8 | bsz 32 | num_updates 17233 | lr 0.00024089 | gnorm 0.825 | loss_scale 64 | train_wall 202 | gb_free 9.7 | wall 9678
2022-03-22 22:45:16 | INFO | fairseq.trainer | begin training epoch 43
2022-03-22 22:45:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:45:53 | INFO | train_inner | epoch 043:     67 / 411 loss=5.086, ppl=33.97, wps=27917.6, ups=1.71, wpb=16317.5, bsz=31.9, num_updates=17300, lr=0.000240424, gnorm=0.829, loss_scale=64, train_wall=50, gb_free=9.7, wall=9714
2022-03-22 22:46:46 | INFO | train_inner | epoch 043:    167 / 411 loss=5.086, ppl=33.96, wps=30900.6, ups=1.89, wpb=16384, bsz=32, num_updates=17400, lr=0.000239732, gnorm=0.829, loss_scale=64, train_wall=49, gb_free=9.7, wall=9767
2022-03-22 22:47:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-22 22:47:39 | INFO | train_inner | epoch 043:    268 / 411 loss=5.11, ppl=34.55, wps=30615.1, ups=1.87, wpb=16384, bsz=32, num_updates=17500, lr=0.000239046, gnorm=0.83, loss_scale=64, train_wall=50, gb_free=9.7, wall=9821
2022-03-22 22:48:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-22 22:48:33 | INFO | train_inner | epoch 043:    369 / 411 loss=5.149, ppl=35.49, wps=30572.4, ups=1.87, wpb=16384, bsz=32, num_updates=17600, lr=0.000238366, gnorm=0.837, loss_scale=32, train_wall=50, gb_free=9.7, wall=9874
2022-03-22 22:48:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:48:59 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.402 | ppl 84.58 | wps 53969.9 | wpb 511.2 | bsz 1 | num_updates 17642 | best_loss 6.392
2022-03-22 22:48:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 17642 updates
2022-03-22 22:48:59 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-22 22:48:59 | INFO | train | epoch 043 | loss 5.108 | ppl 34.49 | wps 30026.4 | ups 1.83 | wpb 16367.7 | bsz 32 | num_updates 17642 | lr 0.000238082 | gnorm 0.831 | loss_scale 32 | train_wall 202 | gb_free 9.7 | wall 9901
2022-03-22 22:48:59 | INFO | fairseq.trainer | begin training epoch 44
2022-03-22 22:48:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-22 22:49:30 | INFO | train_inner | epoch 044:     58 / 411 loss=5.064, ppl=33.44, wps=28527.2, ups=1.75, wpb=16322.6, bsz=31.9, num_updates=17700, lr=0.000237691, gnorm=0.83, loss_scale=32, train_wall=49, gb_free=9.7, wall=9932
2022-03-22 22:50:23 | INFO | train_inner | epoch 044:    158 / 411 loss=5.057, ppl=33.28, wps=30867.7, ups=1.88, wpb=16384, bsz=32, num_updates=17800, lr=0.000237023, gnorm=0.834, loss_scale=32, train_wall=49, gb_free=9.7, wall=9985
2022-03-22 22:51:16 | INFO | train_inner | epoch 044:    258 / 411 loss=5.096, ppl=34.19, wps=30860.9, ups=1.88, wpb=16384, bsz=32, num_updates=17900, lr=0.00023636, gnorm=0.841, loss_scale=32, train_wall=49, gb_free=9.7, wall=10038
2022-03-22 22:52:09 | INFO | train_inner | epoch 044:    358 / 411 loss=5.125, ppl=34.9, wps=30908.5, ups=1.89, wpb=16378.9, bsz=32, num_updates=18000, lr=0.000235702, gnorm=0.839, loss_scale=32, train_wall=49, gb_free=9.7, wall=10091
2022-03-22 22:52:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-22 22:52:41 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.403 | ppl 84.61 | wps 53746.3 | wpb 511.2 | bsz 1 | num_updates 18053 | best_loss 6.392
2022-03-22 22:52:41 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-22 22:52:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 18053 updates
2022-03-22 22:52:41 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-22 22:52:41 | INFO | train | epoch 044 | loss 5.089 | ppl 34.04 | wps 30251.6 | ups 1.85 | wpb 16367.8 | bsz 32 | num_updates 18053 | lr 0.000235356 | gnorm 0.837 | loss_scale 32 | train_wall 202 | gb_free 9.7 | wall 10123
2022-03-22 22:52:41 | INFO | fairseq_cli.train | done training in 10123.1 seconds
Sender: LSF System <lsfadmin@eu-g3-054>
Subject: Job 210566203: <wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3> in cluster <euler> Done

Job <wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3> was submitted from host <eu-login-06> by user <andriusb> in cluster <euler> at Wed Mar 23 06:36:59 2022
Job was executed on host(s) <eu-g3-054>, in queue <gpuhe.4h>, as user <andriusb> in cluster <euler> at Wed Mar 23 06:37:30 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Wed Mar 23 06:37:30 2022
Terminated at Wed Mar 23 08:01:49 2022
Results reported at Wed Mar 23 08:01:49 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-cleaned-bpe-size0.0625 --save-dir /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.45 --criterion cross_entropy --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --seed 66575611 --fp16 --no-epoch-checkpoints --no-last-checkpoints --patience 3 --seed 66575613 --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   5046.38 sec.
    Max Memory :                                 4374 MB
    Average Memory :                             3242.99 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               15626.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   5058 sec.
    Turnaround time :                            5090 sec.

The output (if any) follows:

2022-03-23 06:37:39 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575613, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 3, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.45, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-cleaned-bpe-size0.0625', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575613, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-23 06:37:40 | INFO | fairseq.tasks.language_modeling | dictionary: 39136 types
2022-03-23 06:37:40 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39136, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=39136, bias=False)
  )
)
2022-03-23 06:37:40 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-23 06:37:40 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-23 06:37:40 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion
2022-03-23 06:37:40 | INFO | fairseq_cli.train | num. shared model params: 38,951,936 (num. trained: 38,951,936)
2022-03-23 06:37:40 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-23 06:37:40 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/valid
2022-03-23 06:37:43 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-23 06:37:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 06:37:43 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-23 06:37:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-23 06:37:43 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-23 06:37:43 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-23 06:37:43 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_last.pt
2022-03-23 06:37:43 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_last.pt
2022-03-23 06:37:43 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-23 06:37:43 | INFO | fairseq.data.data_utils | loaded 112,584 examples from: data-bin/wikitext-103-cleaned-bpe-size0.0625/train
2022-03-23 06:37:43 | INFO | fairseq.trainer | begin training epoch 1
2022-03-23 06:37:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:37:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-23 06:37:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 06:37:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 06:37:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-23 06:39:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:39:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 12.968 | ppl 8013.69 | wps 170980 | wpb 2040.3 | bsz 4 | num_updates 99
2022-03-23 06:39:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 99 updates
2022-03-23 06:39:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:39:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:39:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 1 @ 99 updates, score 12.968) (writing took 0.9283741399995051 seconds)
2022-03-23 06:39:14 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-23 06:39:14 | INFO | train | epoch 001 | loss 14.502 | ppl 23195.2 | wps 80315.1 | ups 1.23 | wpb 65303.3 | bsz 127.6 | num_updates 99 | lr 1.24725e-05 | gnorm 2.776 | loss_scale 8 | train_wall 83 | gb_free 21.6 | wall 90
2022-03-23 06:39:14 | INFO | fairseq.trainer | begin training epoch 2
2022-03-23 06:39:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:39:14 | INFO | train_inner | epoch 002:      1 / 103 loss=14.489, ppl=22989.4, wps=80234.4, ups=1.23, wpb=65305.6, bsz=127.6, num_updates=100, lr=1.25975e-05, gnorm=2.762, loss_scale=8, train_wall=83, gb_free=21.6, wall=91
2022-03-23 06:40:34 | INFO | train_inner | epoch 002:    101 / 103 loss=12.565, ppl=6059.2, wps=82467.2, ups=1.26, wpb=65530.9, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.154, loss_scale=8, train_wall=75, gb_free=21.6, wall=171
2022-03-23 06:40:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:40:37 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 11.635 | ppl 3180.27 | wps 170977 | wpb 2040.3 | bsz 4 | num_updates 202 | best_loss 11.635
2022-03-23 06:40:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 202 updates
2022-03-23 06:40:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:40:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:40:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 2 @ 202 updates, score 11.635) (writing took 0.9065355519996956 seconds)
2022-03-23 06:40:38 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-23 06:40:38 | INFO | train | epoch 002 | loss 12.56 | ppl 6039.87 | wps 80098.2 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 202 | lr 2.5345e-05 | gnorm 1.152 | loss_scale 8 | train_wall 77 | gb_free 21.6 | wall 174
2022-03-23 06:40:38 | INFO | fairseq.trainer | begin training epoch 3
2022-03-23 06:40:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:41:56 | INFO | train_inner | epoch 003:     98 / 103 loss=11.254, ppl=2442.19, wps=79877, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=300, lr=3.75925e-05, gnorm=0.725, loss_scale=8, train_wall=74, gb_free=21.6, wall=252
2022-03-23 06:41:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:42:01 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.651 | ppl 1607.91 | wps 171838 | wpb 2040.3 | bsz 4 | num_updates 305 | best_loss 10.651
2022-03-23 06:42:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 305 updates
2022-03-23 06:42:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:42:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:42:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 3 @ 305 updates, score 10.651) (writing took 0.920360709016677 seconds)
2022-03-23 06:42:02 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-23 06:42:02 | INFO | train | epoch 003 | loss 11.222 | ppl 2389.47 | wps 79946.9 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 305 | lr 3.82174e-05 | gnorm 0.713 | loss_scale 8 | train_wall 77 | gb_free 21.6 | wall 258
2022-03-23 06:42:02 | INFO | fairseq.trainer | begin training epoch 4
2022-03-23 06:42:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:43:17 | INFO | train_inner | epoch 004:     95 / 103 loss=10.566, ppl=1515.61, wps=79993.6, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=400, lr=5.009e-05, gnorm=0.502, loss_scale=8, train_wall=74, gb_free=21.6, wall=334
2022-03-23 06:43:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:43:25 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 10.286 | ppl 1248.34 | wps 170010 | wpb 2040.3 | bsz 4 | num_updates 408 | best_loss 10.286
2022-03-23 06:43:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 408 updates
2022-03-23 06:43:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:43:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:43:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 4 @ 408 updates, score 10.286) (writing took 0.881598643027246 seconds)
2022-03-23 06:43:26 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-23 06:43:26 | INFO | train | epoch 004 | loss 10.543 | ppl 1492.17 | wps 80091 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 408 | lr 5.10898e-05 | gnorm 0.494 | loss_scale 8 | train_wall 77 | gb_free 21.6 | wall 342
2022-03-23 06:43:26 | INFO | fairseq.trainer | begin training epoch 5
2022-03-23 06:43:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:44:39 | INFO | train_inner | epoch 005:     92 / 103 loss=10.287, ppl=1249.52, wps=79947.1, ups=1.22, wpb=65310.7, bsz=127.6, num_updates=500, lr=6.25875e-05, gnorm=0.447, loss_scale=8, train_wall=74, gb_free=21.6, wall=416
2022-03-23 06:44:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:44:49 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.031 | ppl 1045.96 | wps 169852 | wpb 2040.3 | bsz 4 | num_updates 511 | best_loss 10.031
2022-03-23 06:44:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 511 updates
2022-03-23 06:44:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:44:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:44:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 5 @ 511 updates, score 10.031) (writing took 0.913908916991204 seconds)
2022-03-23 06:44:50 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-23 06:44:50 | INFO | train | epoch 005 | loss 10.265 | ppl 1230.79 | wps 79970.3 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 511 | lr 6.39622e-05 | gnorm 0.445 | loss_scale 8 | train_wall 77 | gb_free 21.6 | wall 427
2022-03-23 06:44:50 | INFO | fairseq.trainer | begin training epoch 6
2022-03-23 06:44:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:46:01 | INFO | train_inner | epoch 006:     89 / 103 loss=10.053, ppl=1062.27, wps=79960.6, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=600, lr=7.5085e-05, gnorm=0.474, loss_scale=16, train_wall=74, gb_free=21.6, wall=497
2022-03-23 06:46:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:46:13 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 9.766 | ppl 870.77 | wps 170269 | wpb 2040.3 | bsz 4 | num_updates 614 | best_loss 9.766
2022-03-23 06:46:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 614 updates
2022-03-23 06:46:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:46:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:46:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 6 @ 614 updates, score 9.766) (writing took 0.8768142109620385 seconds)
2022-03-23 06:46:14 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-23 06:46:14 | INFO | train | epoch 006 | loss 10.026 | ppl 1042.78 | wps 80060.3 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 614 | lr 7.68347e-05 | gnorm 0.482 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 511
2022-03-23 06:46:14 | INFO | fairseq.trainer | begin training epoch 7
2022-03-23 06:46:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:47:22 | INFO | train_inner | epoch 007:     86 / 103 loss=9.822, ppl=905.4, wps=79980.9, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=700, lr=8.75825e-05, gnorm=0.489, loss_scale=16, train_wall=74, gb_free=21.6, wall=579
2022-03-23 06:47:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:47:37 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 9.533 | ppl 740.95 | wps 171028 | wpb 2040.3 | bsz 4 | num_updates 717 | best_loss 9.533
2022-03-23 06:47:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 717 updates
2022-03-23 06:47:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:47:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:47:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 7 @ 717 updates, score 9.533) (writing took 0.9008756559924223 seconds)
2022-03-23 06:47:38 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-23 06:47:38 | INFO | train | epoch 007 | loss 9.789 | ppl 884.78 | wps 80047 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 717 | lr 8.97071e-05 | gnorm 0.49 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 595
2022-03-23 06:47:38 | INFO | fairseq.trainer | begin training epoch 8
2022-03-23 06:47:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:48:44 | INFO | train_inner | epoch 008:     83 / 103 loss=9.602, ppl=776.9, wps=79935, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=800, lr=0.00010008, gnorm=0.522, loss_scale=16, train_wall=74, gb_free=21.6, wall=661
2022-03-23 06:49:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:49:01 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.282 | ppl 622.69 | wps 171076 | wpb 2040.3 | bsz 4 | num_updates 820 | best_loss 9.282
2022-03-23 06:49:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 820 updates
2022-03-23 06:49:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:49:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:49:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 8 @ 820 updates, score 9.282) (writing took 0.8929847039980814 seconds)
2022-03-23 06:49:02 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-23 06:49:02 | INFO | train | epoch 008 | loss 9.559 | ppl 754.13 | wps 79988.9 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 820 | lr 0.00010258 | gnorm 0.536 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 679
2022-03-23 06:49:02 | INFO | fairseq.trainer | begin training epoch 9
2022-03-23 06:49:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:50:06 | INFO | train_inner | epoch 009:     80 / 103 loss=9.376, ppl=664.27, wps=79852.9, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=900, lr=0.000112578, gnorm=0.526, loss_scale=16, train_wall=74, gb_free=21.6, wall=743
2022-03-23 06:50:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:50:25 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.041 | ppl 526.77 | wps 172686 | wpb 2040.3 | bsz 4 | num_updates 923 | best_loss 9.041
2022-03-23 06:50:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 923 updates
2022-03-23 06:50:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:50:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:50:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 9 @ 923 updates, score 9.041) (writing took 0.9066518819890916 seconds)
2022-03-23 06:50:26 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-23 06:50:26 | INFO | train | epoch 009 | loss 9.323 | ppl 640.61 | wps 79936.8 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 923 | lr 0.000115452 | gnorm 0.546 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 763
2022-03-23 06:50:26 | INFO | fairseq.trainer | begin training epoch 10
2022-03-23 06:50:26 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:51:27 | INFO | train_inner | epoch 010:     77 / 103 loss=9.143, ppl=565.31, wps=79974.3, ups=1.22, wpb=65310.7, bsz=127.6, num_updates=1000, lr=0.000125075, gnorm=0.586, loss_scale=16, train_wall=74, gb_free=21.6, wall=824
2022-03-23 06:51:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:51:49 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.83 | ppl 455.16 | wps 171189 | wpb 2040.3 | bsz 4 | num_updates 1026 | best_loss 8.83
2022-03-23 06:51:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1026 updates
2022-03-23 06:51:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:51:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:51:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 10 @ 1026 updates, score 8.83) (writing took 0.8774064189638011 seconds)
2022-03-23 06:51:50 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-23 06:51:50 | INFO | train | epoch 010 | loss 9.094 | ppl 546.65 | wps 80049.9 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 1026 | lr 0.000128324 | gnorm 0.562 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 847
2022-03-23 06:51:50 | INFO | fairseq.trainer | begin training epoch 11
2022-03-23 06:51:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:52:49 | INFO | train_inner | epoch 011:     74 / 103 loss=8.937, ppl=490.13, wps=79886.7, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=1100, lr=0.000137573, gnorm=0.56, loss_scale=32, train_wall=74, gb_free=21.6, wall=906
2022-03-23 06:53:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:53:13 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.657 | ppl 403.55 | wps 171403 | wpb 2040.3 | bsz 4 | num_updates 1129 | best_loss 8.657
2022-03-23 06:53:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1129 updates
2022-03-23 06:53:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:53:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:53:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 11 @ 1129 updates, score 8.657) (writing took 0.9120290089631453 seconds)
2022-03-23 06:53:14 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-23 06:53:14 | INFO | train | epoch 011 | loss 8.889 | ppl 474.15 | wps 79886.9 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 1129 | lr 0.000141197 | gnorm 0.567 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 931
2022-03-23 06:53:14 | INFO | fairseq.trainer | begin training epoch 12
2022-03-23 06:53:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:54:11 | INFO | train_inner | epoch 012:     71 / 103 loss=8.759, ppl=433.16, wps=79700.5, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=1200, lr=0.00015007, gnorm=0.576, loss_scale=32, train_wall=75, gb_free=21.6, wall=988
2022-03-23 06:54:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:54:38 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.504 | ppl 363.14 | wps 172605 | wpb 2040.3 | bsz 4 | num_updates 1232 | best_loss 8.504
2022-03-23 06:54:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1232 updates
2022-03-23 06:54:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:54:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:54:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 12 @ 1232 updates, score 8.504) (writing took 0.8686606109840795 seconds)
2022-03-23 06:54:39 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-23 06:54:39 | INFO | train | epoch 012 | loss 8.709 | ppl 418.42 | wps 79843.6 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 1232 | lr 0.000154069 | gnorm 0.575 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1015
2022-03-23 06:54:39 | INFO | fairseq.trainer | begin training epoch 13
2022-03-23 06:54:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:55:33 | INFO | train_inner | epoch 013:     68 / 103 loss=8.602, ppl=388.55, wps=79864.4, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=1300, lr=0.000162568, gnorm=0.599, loss_scale=32, train_wall=75, gb_free=21.6, wall=1070
2022-03-23 06:56:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:56:02 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.384 | ppl 334.17 | wps 171807 | wpb 2040.3 | bsz 4 | num_updates 1335 | best_loss 8.384
2022-03-23 06:56:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1335 updates
2022-03-23 06:56:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:56:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:56:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 13 @ 1335 updates, score 8.384) (writing took 0.8812830339884385 seconds)
2022-03-23 06:56:03 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-23 06:56:03 | INFO | train | epoch 013 | loss 8.552 | ppl 375.21 | wps 79911.7 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 1335 | lr 0.000166942 | gnorm 0.612 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1100
2022-03-23 06:56:03 | INFO | fairseq.trainer | begin training epoch 14
2022-03-23 06:56:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:56:55 | INFO | train_inner | epoch 014:     65 / 103 loss=8.456, ppl=351.15, wps=79926.8, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=1400, lr=0.000175065, gnorm=0.603, loss_scale=32, train_wall=74, gb_free=21.6, wall=1151
2022-03-23 06:57:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:57:26 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.266 | ppl 307.77 | wps 172480 | wpb 2040.3 | bsz 4 | num_updates 1438 | best_loss 8.266
2022-03-23 06:57:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1438 updates
2022-03-23 06:57:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:57:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:57:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 14 @ 1438 updates, score 8.266) (writing took 0.9076737649738789 seconds)
2022-03-23 06:57:27 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-23 06:57:27 | INFO | train | epoch 014 | loss 8.405 | ppl 339.01 | wps 80051.5 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 1438 | lr 0.000179814 | gnorm 0.612 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1184
2022-03-23 06:57:27 | INFO | fairseq.trainer | begin training epoch 15
2022-03-23 06:57:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:58:16 | INFO | train_inner | epoch 015:     62 / 103 loss=8.319, ppl=319.45, wps=79949.2, ups=1.22, wpb=65310.7, bsz=127.6, num_updates=1500, lr=0.000187563, gnorm=0.635, loss_scale=32, train_wall=74, gb_free=21.6, wall=1233
2022-03-23 06:58:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 06:58:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 06:58:50 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.142 | ppl 282.41 | wps 171870 | wpb 2040.3 | bsz 4 | num_updates 1540 | best_loss 8.142
2022-03-23 06:58:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1540 updates
2022-03-23 06:58:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:58:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 06:58:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 15 @ 1540 updates, score 8.142) (writing took 0.8465026949997991 seconds)
2022-03-23 06:58:51 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-23 06:58:51 | INFO | train | epoch 015 | loss 8.27 | ppl 308.61 | wps 79214.6 | ups 1.21 | wpb 65310.1 | bsz 127.6 | num_updates 1540 | lr 0.000192562 | gnorm 0.622 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1268
2022-03-23 06:58:51 | INFO | fairseq.trainer | begin training epoch 16
2022-03-23 06:58:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 06:59:39 | INFO | train_inner | epoch 016:     60 / 103 loss=8.196, ppl=293.22, wps=79210, ups=1.21, wpb=65300.5, bsz=127.6, num_updates=1600, lr=0.00020006, gnorm=0.613, loss_scale=32, train_wall=75, gb_free=21.6, wall=1316
2022-03-23 07:00:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:00:14 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.035 | ppl 262.28 | wps 171588 | wpb 2040.3 | bsz 4 | num_updates 1643 | best_loss 8.035
2022-03-23 07:00:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1643 updates
2022-03-23 07:00:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:00:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:00:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 16 @ 1643 updates, score 8.035) (writing took 0.8802355590160005 seconds)
2022-03-23 07:00:15 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-23 07:00:15 | INFO | train | epoch 016 | loss 8.143 | ppl 282.64 | wps 80035.8 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 1643 | lr 0.000205434 | gnorm 0.616 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1352
2022-03-23 07:00:15 | INFO | fairseq.trainer | begin training epoch 17
2022-03-23 07:00:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:01:00 | INFO | train_inner | epoch 017:     57 / 103 loss=8.065, ppl=267.86, wps=79982.5, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=1700, lr=0.000212558, gnorm=0.6, loss_scale=32, train_wall=74, gb_free=21.6, wall=1397
2022-03-23 07:01:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:01:38 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.93 | ppl 243.84 | wps 172901 | wpb 2040.3 | bsz 4 | num_updates 1746 | best_loss 7.93
2022-03-23 07:01:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1746 updates
2022-03-23 07:01:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:01:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:01:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 17 @ 1746 updates, score 7.93) (writing took 0.8694788619759493 seconds)
2022-03-23 07:01:39 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-23 07:01:39 | INFO | train | epoch 017 | loss 8.018 | ppl 259.13 | wps 80065.3 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 1746 | lr 0.000218306 | gnorm 0.588 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1436
2022-03-23 07:01:39 | INFO | fairseq.trainer | begin training epoch 18
2022-03-23 07:01:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:02:22 | INFO | train_inner | epoch 018:     54 / 103 loss=7.947, ppl=246.85, wps=79954.4, ups=1.22, wpb=65310.7, bsz=127.6, num_updates=1800, lr=0.000225055, gnorm=0.603, loss_scale=32, train_wall=74, gb_free=21.6, wall=1479
2022-03-23 07:03:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:03:02 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.836 | ppl 228.5 | wps 171080 | wpb 2040.3 | bsz 4 | num_updates 1849 | best_loss 7.836
2022-03-23 07:03:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1849 updates
2022-03-23 07:03:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:03:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:03:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 18 @ 1849 updates, score 7.836) (writing took 0.8902569230413064 seconds)
2022-03-23 07:03:03 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-23 07:03:03 | INFO | train | epoch 018 | loss 7.897 | ppl 238.3 | wps 79999.6 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 1849 | lr 0.000231179 | gnorm 0.622 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1520
2022-03-23 07:03:03 | INFO | fairseq.trainer | begin training epoch 19
2022-03-23 07:03:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:03:44 | INFO | train_inner | epoch 019:     51 / 103 loss=7.841, ppl=229.28, wps=79907.3, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=1900, lr=0.000237553, gnorm=0.624, loss_scale=32, train_wall=74, gb_free=21.6, wall=1561
2022-03-23 07:04:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:04:26 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.738 | ppl 213.51 | wps 172051 | wpb 2040.3 | bsz 4 | num_updates 1952 | best_loss 7.738
2022-03-23 07:04:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1952 updates
2022-03-23 07:04:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:04:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:04:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 19 @ 1952 updates, score 7.738) (writing took 0.8875402539852075 seconds)
2022-03-23 07:04:27 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-23 07:04:27 | INFO | train | epoch 019 | loss 7.779 | ppl 219.58 | wps 80016.8 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 1952 | lr 0.000244051 | gnorm 0.619 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1604
2022-03-23 07:04:27 | INFO | fairseq.trainer | begin training epoch 20
2022-03-23 07:04:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:05:05 | INFO | train_inner | epoch 020:     48 / 103 loss=7.719, ppl=210.69, wps=79978, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=2000, lr=0.00025005, gnorm=0.611, loss_scale=32, train_wall=74, gb_free=21.6, wall=1642
2022-03-23 07:05:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:05:50 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.654 | ppl 201.35 | wps 173019 | wpb 2040.3 | bsz 4 | num_updates 2055 | best_loss 7.654
2022-03-23 07:05:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 2055 updates
2022-03-23 07:05:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:05:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:05:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 20 @ 2055 updates, score 7.654) (writing took 0.8653168809833005 seconds)
2022-03-23 07:05:51 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-23 07:05:51 | INFO | train | epoch 020 | loss 7.663 | ppl 202.64 | wps 80054.8 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 2055 | lr 0.000256924 | gnorm 0.606 | loss_scale 64 | train_wall 77 | gb_free 21.6 | wall 1688
2022-03-23 07:05:51 | INFO | fairseq.trainer | begin training epoch 21
2022-03-23 07:05:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:06:27 | INFO | train_inner | epoch 021:     45 / 103 loss=7.614, ppl=195.86, wps=79960.1, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=2100, lr=0.000262548, gnorm=0.598, loss_scale=64, train_wall=74, gb_free=21.6, wall=1724
2022-03-23 07:07:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:07:14 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.565 | ppl 189.38 | wps 172206 | wpb 2040.3 | bsz 4 | num_updates 2158 | best_loss 7.565
2022-03-23 07:07:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2158 updates
2022-03-23 07:07:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:07:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:07:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 21 @ 2158 updates, score 7.565) (writing took 0.8991151690133847 seconds)
2022-03-23 07:07:15 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-23 07:07:15 | INFO | train | epoch 021 | loss 7.55 | ppl 187.36 | wps 79939.4 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 2158 | lr 0.000269796 | gnorm 0.607 | loss_scale 64 | train_wall 77 | gb_free 21.6 | wall 1772
2022-03-23 07:07:15 | INFO | fairseq.trainer | begin training epoch 22
2022-03-23 07:07:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:07:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 07:07:50 | INFO | train_inner | epoch 022:     43 / 103 loss=7.499, ppl=180.84, wps=79108.7, ups=1.21, wpb=65310.7, bsz=127.6, num_updates=2200, lr=0.000275045, gnorm=0.607, loss_scale=32, train_wall=75, gb_free=21.6, wall=1806
2022-03-23 07:08:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:08:39 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.501 | ppl 181.14 | wps 172397 | wpb 2040.3 | bsz 4 | num_updates 2260 | best_loss 7.501
2022-03-23 07:08:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2260 updates
2022-03-23 07:08:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:08:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:08:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 22 @ 2260 updates, score 7.501) (writing took 0.8624465059838258 seconds)
2022-03-23 07:08:39 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-23 07:08:39 | INFO | train | epoch 022 | loss 7.439 | ppl 173.48 | wps 79237.8 | ups 1.21 | wpb 65310.1 | bsz 127.6 | num_updates 2260 | lr 0.000282544 | gnorm 0.6 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1856
2022-03-23 07:08:39 | INFO | fairseq.trainer | begin training epoch 23
2022-03-23 07:08:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:09:11 | INFO | train_inner | epoch 023:     40 / 103 loss=7.395, ppl=168.28, wps=79969.3, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=2300, lr=0.000287543, gnorm=0.613, loss_scale=32, train_wall=74, gb_free=21.6, wall=1888
2022-03-23 07:10:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:10:03 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.396 | ppl 168.45 | wps 171923 | wpb 2040.3 | bsz 4 | num_updates 2363 | best_loss 7.396
2022-03-23 07:10:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2363 updates
2022-03-23 07:10:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:10:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:10:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 23 @ 2363 updates, score 7.396) (writing took 0.8685743110254407 seconds)
2022-03-23 07:10:03 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-23 07:10:03 | INFO | train | epoch 023 | loss 7.333 | ppl 161.22 | wps 80061.9 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 2363 | lr 0.000295416 | gnorm 0.608 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 1940
2022-03-23 07:10:03 | INFO | fairseq.trainer | begin training epoch 24
2022-03-23 07:10:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:10:33 | INFO | train_inner | epoch 024:     37 / 103 loss=7.293, ppl=156.81, wps=79958.4, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=2400, lr=0.00030004, gnorm=0.604, loss_scale=32, train_wall=74, gb_free=21.6, wall=1970
2022-03-23 07:11:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:11:27 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.335 | ppl 161.42 | wps 171545 | wpb 2040.3 | bsz 4 | num_updates 2466 | best_loss 7.335
2022-03-23 07:11:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2466 updates
2022-03-23 07:11:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:11:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:11:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 24 @ 2466 updates, score 7.335) (writing took 0.9223114519845694 seconds)
2022-03-23 07:11:28 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-23 07:11:28 | INFO | train | epoch 024 | loss 7.228 | ppl 149.95 | wps 79973.2 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 2466 | lr 0.000308288 | gnorm 0.602 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2024
2022-03-23 07:11:28 | INFO | fairseq.trainer | begin training epoch 25
2022-03-23 07:11:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:11:55 | INFO | train_inner | epoch 025:     34 / 103 loss=7.197, ppl=146.75, wps=79921.7, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=2500, lr=0.000312538, gnorm=0.598, loss_scale=32, train_wall=74, gb_free=21.6, wall=2052
2022-03-23 07:12:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:12:51 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.263 | ppl 153.63 | wps 171637 | wpb 2040.3 | bsz 4 | num_updates 2569 | best_loss 7.263
2022-03-23 07:12:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2569 updates
2022-03-23 07:12:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:12:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:12:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 25 @ 2569 updates, score 7.263) (writing took 0.8856892790063284 seconds)
2022-03-23 07:12:52 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-23 07:12:52 | INFO | train | epoch 025 | loss 7.128 | ppl 139.85 | wps 79990.2 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 2569 | lr 0.000321161 | gnorm 0.608 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2108
2022-03-23 07:12:52 | INFO | fairseq.trainer | begin training epoch 26
2022-03-23 07:12:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:13:16 | INFO | train_inner | epoch 026:     31 / 103 loss=7.095, ppl=136.73, wps=79919.7, ups=1.22, wpb=65310.7, bsz=127.6, num_updates=2600, lr=0.000325035, gnorm=0.613, loss_scale=32, train_wall=74, gb_free=21.6, wall=2133
2022-03-23 07:14:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:14:15 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.19 | ppl 145.99 | wps 170744 | wpb 2040.3 | bsz 4 | num_updates 2672 | best_loss 7.19
2022-03-23 07:14:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2672 updates
2022-03-23 07:14:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:14:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:14:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 26 @ 2672 updates, score 7.19) (writing took 0.883252587984316 seconds)
2022-03-23 07:14:16 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-23 07:14:16 | INFO | train | epoch 026 | loss 7.03 | ppl 130.7 | wps 80025 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 2672 | lr 0.000334033 | gnorm 0.608 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2193
2022-03-23 07:14:16 | INFO | fairseq.trainer | begin training epoch 27
2022-03-23 07:14:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:14:38 | INFO | train_inner | epoch 027:     28 / 103 loss=7.003, ppl=128.26, wps=79970.7, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=2700, lr=0.000337533, gnorm=0.589, loss_scale=32, train_wall=74, gb_free=21.6, wall=2215
2022-03-23 07:15:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 07:15:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:15:39 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.146 | ppl 141.67 | wps 172213 | wpb 2040.3 | bsz 4 | num_updates 2774 | best_loss 7.146
2022-03-23 07:15:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2774 updates
2022-03-23 07:15:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:15:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:15:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 27 @ 2774 updates, score 7.146) (writing took 0.8985094850067981 seconds)
2022-03-23 07:15:40 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-23 07:15:40 | INFO | train | epoch 027 | loss 6.936 | ppl 122.42 | wps 79240.2 | ups 1.21 | wpb 65310.1 | bsz 127.6 | num_updates 2774 | lr 0.000346781 | gnorm 0.576 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2277
2022-03-23 07:15:40 | INFO | fairseq.trainer | begin training epoch 28
2022-03-23 07:15:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:16:01 | INFO | train_inner | epoch 028:     26 / 103 loss=6.911, ppl=120.37, wps=79188.3, ups=1.21, wpb=65310.7, bsz=127.6, num_updates=2800, lr=0.00035003, gnorm=0.586, loss_scale=32, train_wall=75, gb_free=21.6, wall=2297
2022-03-23 07:17:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:17:03 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.092 | ppl 136.39 | wps 172011 | wpb 2040.3 | bsz 4 | num_updates 2877 | best_loss 7.092
2022-03-23 07:17:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2877 updates
2022-03-23 07:17:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:17:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:17:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 28 @ 2877 updates, score 7.092) (writing took 0.8804783719824627 seconds)
2022-03-23 07:17:04 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-23 07:17:04 | INFO | train | epoch 028 | loss 6.847 | ppl 115.09 | wps 80037.7 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 2877 | lr 0.000359653 | gnorm 0.592 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2361
2022-03-23 07:17:04 | INFO | fairseq.trainer | begin training epoch 29
2022-03-23 07:17:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:17:22 | INFO | train_inner | epoch 029:     23 / 103 loss=6.826, ppl=113.45, wps=79958.6, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=2900, lr=0.000362528, gnorm=0.589, loss_scale=32, train_wall=74, gb_free=21.6, wall=2379
2022-03-23 07:18:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:18:27 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.036 | ppl 131.22 | wps 171801 | wpb 2040.3 | bsz 4 | num_updates 2980 | best_loss 7.036
2022-03-23 07:18:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2980 updates
2022-03-23 07:18:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:18:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:18:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 29 @ 2980 updates, score 7.036) (writing took 0.8741722760023549 seconds)
2022-03-23 07:18:28 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-23 07:18:28 | INFO | train | epoch 029 | loss 6.761 | ppl 108.45 | wps 80042.8 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 2980 | lr 0.000372526 | gnorm 0.582 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2445
2022-03-23 07:18:28 | INFO | fairseq.trainer | begin training epoch 30
2022-03-23 07:18:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:18:44 | INFO | train_inner | epoch 030:     20 / 103 loss=6.743, ppl=107.13, wps=79971, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=3000, lr=0.000375025, gnorm=0.582, loss_scale=32, train_wall=74, gb_free=21.6, wall=2461
2022-03-23 07:19:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:19:51 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.011 | ppl 128.99 | wps 165923 | wpb 2040.3 | bsz 4 | num_updates 3083 | best_loss 7.011
2022-03-23 07:19:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 3083 updates
2022-03-23 07:19:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:19:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:19:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 30 @ 3083 updates, score 7.011) (writing took 0.869399118993897 seconds)
2022-03-23 07:19:52 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-23 07:19:52 | INFO | train | epoch 030 | loss 6.677 | ppl 102.35 | wps 79937.1 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 3083 | lr 0.000385398 | gnorm 0.578 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2529
2022-03-23 07:19:52 | INFO | fairseq.trainer | begin training epoch 31
2022-03-23 07:19:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:20:06 | INFO | train_inner | epoch 031:     17 / 103 loss=6.659, ppl=101.02, wps=79861.4, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=3100, lr=0.000387523, gnorm=0.579, loss_scale=32, train_wall=74, gb_free=21.6, wall=2543
2022-03-23 07:21:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:21:15 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.947 | ppl 123.36 | wps 170988 | wpb 2040.3 | bsz 4 | num_updates 3186 | best_loss 6.947
2022-03-23 07:21:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 3186 updates
2022-03-23 07:21:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:21:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:21:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 31 @ 3186 updates, score 6.947) (writing took 0.9057238720124587 seconds)
2022-03-23 07:21:16 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-23 07:21:16 | INFO | train | epoch 031 | loss 6.598 | ppl 96.88 | wps 80003.4 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 3186 | lr 0.00039827 | gnorm 0.571 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2613
2022-03-23 07:21:16 | INFO | fairseq.trainer | begin training epoch 32
2022-03-23 07:21:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:21:27 | INFO | train_inner | epoch 032:     14 / 103 loss=6.591, ppl=96.43, wps=79949.9, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=3200, lr=0.00040002, gnorm=0.572, loss_scale=32, train_wall=74, gb_free=21.6, wall=2624
2022-03-23 07:22:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:22:39 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.921 | ppl 121.17 | wps 170571 | wpb 2040.3 | bsz 4 | num_updates 3289 | best_loss 6.921
2022-03-23 07:22:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3289 updates
2022-03-23 07:22:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:22:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:22:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 32 @ 3289 updates, score 6.921) (writing took 0.8695788739714772 seconds)
2022-03-23 07:22:40 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-23 07:22:40 | INFO | train | epoch 032 | loss 6.524 | ppl 92.02 | wps 80101.5 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 3289 | lr 0.000411143 | gnorm 0.562 | loss_scale 64 | train_wall 77 | gb_free 21.6 | wall 2697
2022-03-23 07:22:40 | INFO | fairseq.trainer | begin training epoch 33
2022-03-23 07:22:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:22:49 | INFO | train_inner | epoch 033:     11 / 103 loss=6.517, ppl=91.59, wps=79994.6, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=3300, lr=0.000412518, gnorm=0.559, loss_scale=64, train_wall=74, gb_free=21.6, wall=2706
2022-03-23 07:23:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 07:24:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:24:03 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.868 | ppl 116.78 | wps 171152 | wpb 2040.3 | bsz 4 | num_updates 3391 | best_loss 6.868
2022-03-23 07:24:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3391 updates
2022-03-23 07:24:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:24:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:24:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 33 @ 3391 updates, score 6.868) (writing took 0.8777249850099906 seconds)
2022-03-23 07:24:04 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-23 07:24:04 | INFO | train | epoch 033 | loss 6.452 | ppl 87.57 | wps 79214.7 | ups 1.21 | wpb 65310.1 | bsz 127.6 | num_updates 3391 | lr 0.00042389 | gnorm 0.561 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2781
2022-03-23 07:24:04 | INFO | fairseq.trainer | begin training epoch 34
2022-03-23 07:24:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:24:11 | INFO | train_inner | epoch 034:      9 / 103 loss=6.445, ppl=87.12, wps=79203.1, ups=1.21, wpb=65305.6, bsz=127.6, num_updates=3400, lr=0.000425015, gnorm=0.562, loss_scale=32, train_wall=75, gb_free=21.6, wall=2788
2022-03-23 07:25:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:25:27 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.848 | ppl 115.16 | wps 170515 | wpb 2040.3 | bsz 4 | num_updates 3494 | best_loss 6.848
2022-03-23 07:25:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3494 updates
2022-03-23 07:25:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:25:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:25:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 34 @ 3494 updates, score 6.848) (writing took 0.8960689619998448 seconds)
2022-03-23 07:25:28 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-23 07:25:28 | INFO | train | epoch 034 | loss 6.383 | ppl 83.47 | wps 79991.7 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 3494 | lr 0.000436763 | gnorm 0.546 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2865
2022-03-23 07:25:28 | INFO | fairseq.trainer | begin training epoch 35
2022-03-23 07:25:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:25:33 | INFO | train_inner | epoch 035:      6 / 103 loss=6.381, ppl=83.37, wps=79945.9, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=3500, lr=0.000437513, gnorm=0.546, loss_scale=32, train_wall=74, gb_free=21.6, wall=2870
2022-03-23 07:26:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:26:52 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.817 | ppl 112.75 | wps 171803 | wpb 2040.3 | bsz 4 | num_updates 3597 | best_loss 6.817
2022-03-23 07:26:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3597 updates
2022-03-23 07:26:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:26:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:26:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 35 @ 3597 updates, score 6.817) (writing took 0.8886531619937159 seconds)
2022-03-23 07:26:52 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-23 07:26:52 | INFO | train | epoch 035 | loss 6.319 | ppl 79.85 | wps 79975.9 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 3597 | lr 0.000449635 | gnorm 0.563 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 2949
2022-03-23 07:26:52 | INFO | fairseq.trainer | begin training epoch 36
2022-03-23 07:26:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:26:55 | INFO | train_inner | epoch 036:      3 / 103 loss=6.32, ppl=79.87, wps=79912.3, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=3600, lr=0.00045001, gnorm=0.564, loss_scale=32, train_wall=74, gb_free=21.6, wall=2952
2022-03-23 07:28:14 | INFO | train_inner | epoch 036:    103 / 103 loss=6.255, ppl=76.39, wps=82360.4, ups=1.26, wpb=65305.6, bsz=127.6, num_updates=3700, lr=0.000462508, gnorm=0.54, loss_scale=32, train_wall=74, gb_free=21.6, wall=3031
2022-03-23 07:28:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:28:16 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.789 | ppl 110.6 | wps 170151 | wpb 2040.3 | bsz 4 | num_updates 3700 | best_loss 6.789
2022-03-23 07:28:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 3700 updates
2022-03-23 07:28:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:28:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:28:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 36 @ 3700 updates, score 6.789) (writing took 0.8920051829773001 seconds)
2022-03-23 07:28:16 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-23 07:28:16 | INFO | train | epoch 036 | loss 6.256 | ppl 76.42 | wps 80028.3 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 3700 | lr 0.000462508 | gnorm 0.542 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3033
2022-03-23 07:28:16 | INFO | fairseq.trainer | begin training epoch 37
2022-03-23 07:28:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:29:36 | INFO | train_inner | epoch 037:    100 / 103 loss=6.196, ppl=73.31, wps=80046.7, ups=1.22, wpb=65530.9, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.551, loss_scale=32, train_wall=75, gb_free=21.6, wall=3113
2022-03-23 07:29:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:29:40 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.757 | ppl 108.18 | wps 172624 | wpb 2040.3 | bsz 4 | num_updates 3803 | best_loss 6.757
2022-03-23 07:29:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 3803 updates
2022-03-23 07:29:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:29:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:29:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 37 @ 3803 updates, score 6.757) (writing took 0.9251157179824077 seconds)
2022-03-23 07:29:40 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-23 07:29:40 | INFO | train | epoch 037 | loss 6.197 | ppl 73.36 | wps 80081.8 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 3803 | lr 0.00047538 | gnorm 0.551 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3117
2022-03-23 07:29:40 | INFO | fairseq.trainer | begin training epoch 38
2022-03-23 07:29:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:30:58 | INFO | train_inner | epoch 038:     97 / 103 loss=6.141, ppl=70.58, wps=79927.2, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=3900, lr=0.000487503, gnorm=0.541, loss_scale=64, train_wall=74, gb_free=21.6, wall=3195
2022-03-23 07:31:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:31:04 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.743 | ppl 107.08 | wps 171030 | wpb 2040.3 | bsz 4 | num_updates 3906 | best_loss 6.743
2022-03-23 07:31:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3906 updates
2022-03-23 07:31:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:31:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:31:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 38 @ 3906 updates, score 6.743) (writing took 0.8942378649953753 seconds)
2022-03-23 07:31:05 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-23 07:31:05 | INFO | train | epoch 038 | loss 6.139 | ppl 70.45 | wps 80016 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 3906 | lr 0.000488252 | gnorm 0.541 | loss_scale 64 | train_wall 77 | gb_free 21.6 | wall 3201
2022-03-23 07:31:05 | INFO | fairseq.trainer | begin training epoch 39
2022-03-23 07:31:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:32:19 | INFO | train_inner | epoch 039:     94 / 103 loss=6.086, ppl=67.92, wps=79957.3, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=4000, lr=0.0005, gnorm=0.544, loss_scale=64, train_wall=74, gb_free=21.6, wall=3276
2022-03-23 07:32:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 07:32:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:32:28 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.705 | ppl 104.35 | wps 171782 | wpb 2040.3 | bsz 4 | num_updates 4008 | best_loss 6.705
2022-03-23 07:32:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 4008 updates
2022-03-23 07:32:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:32:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:32:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 39 @ 4008 updates, score 6.705) (writing took 0.8864041970227845 seconds)
2022-03-23 07:32:29 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-23 07:32:29 | INFO | train | epoch 039 | loss 6.085 | ppl 67.87 | wps 79282.4 | ups 1.21 | wpb 65310.1 | bsz 127.6 | num_updates 4008 | lr 0.000499501 | gnorm 0.542 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3285
2022-03-23 07:32:29 | INFO | fairseq.trainer | begin training epoch 40
2022-03-23 07:32:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:33:42 | INFO | train_inner | epoch 040:     92 / 103 loss=6.033, ppl=65.46, wps=79184.6, ups=1.21, wpb=65305.6, bsz=127.6, num_updates=4100, lr=0.000493865, gnorm=0.535, loss_scale=32, train_wall=75, gb_free=21.6, wall=3359
2022-03-23 07:33:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:33:52 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.69 | ppl 103.25 | wps 170748 | wpb 2040.3 | bsz 4 | num_updates 4111 | best_loss 6.69
2022-03-23 07:33:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 4111 updates
2022-03-23 07:33:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:33:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:33:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 40 @ 4111 updates, score 6.69) (writing took 0.9168672779924236 seconds)
2022-03-23 07:33:53 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-23 07:33:53 | INFO | train | epoch 040 | loss 6.03 | ppl 65.34 | wps 79989.4 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 4111 | lr 0.000493204 | gnorm 0.535 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3369
2022-03-23 07:33:53 | INFO | fairseq.trainer | begin training epoch 41
2022-03-23 07:33:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:35:04 | INFO | train_inner | epoch 041:     89 / 103 loss=5.978, ppl=63.05, wps=79960.6, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=4200, lr=0.00048795, gnorm=0.522, loss_scale=32, train_wall=74, gb_free=21.6, wall=3440
2022-03-23 07:35:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:35:16 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.661 | ppl 101.23 | wps 171121 | wpb 2040.3 | bsz 4 | num_updates 4214 | best_loss 6.661
2022-03-23 07:35:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 4214 updates
2022-03-23 07:35:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:35:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:35:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 41 @ 4214 updates, score 6.661) (writing took 0.8990380039904267 seconds)
2022-03-23 07:35:17 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-23 07:35:17 | INFO | train | epoch 041 | loss 5.973 | ppl 62.83 | wps 80038.4 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 4214 | lr 0.000487139 | gnorm 0.522 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3454
2022-03-23 07:35:17 | INFO | fairseq.trainer | begin training epoch 42
2022-03-23 07:35:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:36:25 | INFO | train_inner | epoch 042:     86 / 103 loss=5.928, ppl=60.88, wps=80002.5, ups=1.23, wpb=65305.6, bsz=127.6, num_updates=4300, lr=0.000482243, gnorm=0.531, loss_scale=32, train_wall=74, gb_free=21.6, wall=3522
2022-03-23 07:36:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:36:40 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.644 | ppl 99.98 | wps 170968 | wpb 2040.3 | bsz 4 | num_updates 4317 | best_loss 6.644
2022-03-23 07:36:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 4317 updates
2022-03-23 07:36:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:36:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:36:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 42 @ 4317 updates, score 6.644) (writing took 0.8633967619971372 seconds)
2022-03-23 07:36:41 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-23 07:36:41 | INFO | train | epoch 042 | loss 5.923 | ppl 60.68 | wps 80122.6 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 4317 | lr 0.000481292 | gnorm 0.529 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3538
2022-03-23 07:36:41 | INFO | fairseq.trainer | begin training epoch 43
2022-03-23 07:36:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:37:47 | INFO | train_inner | epoch 043:     83 / 103 loss=5.878, ppl=58.8, wps=80016.9, ups=1.23, wpb=65310.7, bsz=127.6, num_updates=4400, lr=0.000476731, gnorm=0.517, loss_scale=32, train_wall=74, gb_free=21.6, wall=3604
2022-03-23 07:38:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:38:04 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.622 | ppl 98.52 | wps 171811 | wpb 2040.3 | bsz 4 | num_updates 4420 | best_loss 6.622
2022-03-23 07:38:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 4420 updates
2022-03-23 07:38:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:38:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:38:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 43 @ 4420 updates, score 6.622) (writing took 0.867424163967371 seconds)
2022-03-23 07:38:05 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-23 07:38:05 | INFO | train | epoch 043 | loss 5.872 | ppl 58.55 | wps 80075.3 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 4420 | lr 0.000475651 | gnorm 0.519 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3622
2022-03-23 07:38:05 | INFO | fairseq.trainer | begin training epoch 44
2022-03-23 07:38:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:39:09 | INFO | train_inner | epoch 044:     80 / 103 loss=5.833, ppl=57.02, wps=79939.5, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=4500, lr=0.000471405, gnorm=0.515, loss_scale=32, train_wall=74, gb_free=21.6, wall=3685
2022-03-23 07:39:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:39:28 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.625 | ppl 98.68 | wps 172375 | wpb 2040.3 | bsz 4 | num_updates 4523 | best_loss 6.622
2022-03-23 07:39:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 4523 updates
2022-03-23 07:39:28 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-23 07:39:28 | INFO | train | epoch 044 | loss 5.826 | ppl 56.73 | wps 80807.3 | ups 1.24 | wpb 65312.3 | bsz 127.6 | num_updates 4523 | lr 0.000470204 | gnorm 0.518 | loss_scale 64 | train_wall 77 | gb_free 21.6 | wall 3705
2022-03-23 07:39:28 | INFO | fairseq.trainer | begin training epoch 45
2022-03-23 07:39:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:40:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 07:40:30 | INFO | train_inner | epoch 045:     78 / 103 loss=5.794, ppl=55.5, wps=80142.1, ups=1.23, wpb=65305.6, bsz=127.6, num_updates=4600, lr=0.000466252, gnorm=0.515, loss_scale=32, train_wall=75, gb_free=21.6, wall=3767
2022-03-23 07:40:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:40:51 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.604 | ppl 97.28 | wps 171884 | wpb 2040.3 | bsz 4 | num_updates 4625 | best_loss 6.604
2022-03-23 07:40:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 4625 updates
2022-03-23 07:40:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:40:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:40:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 45 @ 4625 updates, score 6.604) (writing took 0.8699112239992246 seconds)
2022-03-23 07:40:52 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-23 07:40:52 | INFO | train | epoch 045 | loss 5.78 | ppl 54.96 | wps 79378.2 | ups 1.22 | wpb 65310.1 | bsz 127.6 | num_updates 4625 | lr 0.000464991 | gnorm 0.51 | loss_scale 32 | train_wall 76 | gb_free 21.6 | wall 3789
2022-03-23 07:40:52 | INFO | fairseq.trainer | begin training epoch 46
2022-03-23 07:40:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:41:52 | INFO | train_inner | epoch 046:     75 / 103 loss=5.744, ppl=53.59, wps=80014.8, ups=1.23, wpb=65305.6, bsz=127.6, num_updates=4700, lr=0.000461266, gnorm=0.512, loss_scale=32, train_wall=74, gb_free=21.6, wall=3848
2022-03-23 07:42:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:42:15 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.593 | ppl 96.51 | wps 171569 | wpb 2040.3 | bsz 4 | num_updates 4728 | best_loss 6.593
2022-03-23 07:42:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 4728 updates
2022-03-23 07:42:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:42:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:42:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 46 @ 4728 updates, score 6.593) (writing took 0.8988048229948618 seconds)
2022-03-23 07:42:16 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-23 07:42:16 | INFO | train | epoch 046 | loss 5.74 | ppl 53.44 | wps 80038.9 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 4728 | lr 0.000459898 | gnorm 0.513 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3873
2022-03-23 07:42:16 | INFO | fairseq.trainer | begin training epoch 47
2022-03-23 07:42:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:43:13 | INFO | train_inner | epoch 047:     72 / 103 loss=5.714, ppl=52.5, wps=79963.8, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=4800, lr=0.000456435, gnorm=0.513, loss_scale=32, train_wall=74, gb_free=21.6, wall=3930
2022-03-23 07:43:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:43:39 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 6.577 | ppl 95.45 | wps 172013 | wpb 2040.3 | bsz 4 | num_updates 4831 | best_loss 6.577
2022-03-23 07:43:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4831 updates
2022-03-23 07:43:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:43:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:43:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 47 @ 4831 updates, score 6.577) (writing took 0.8716755780042149 seconds)
2022-03-23 07:43:40 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-23 07:43:40 | INFO | train | epoch 047 | loss 5.698 | ppl 51.91 | wps 80086.3 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 4831 | lr 0.000454969 | gnorm 0.509 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 3957
2022-03-23 07:43:40 | INFO | fairseq.trainer | begin training epoch 48
2022-03-23 07:43:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:44:35 | INFO | train_inner | epoch 048:     69 / 103 loss=5.672, ppl=50.99, wps=80006.3, ups=1.23, wpb=65305.6, bsz=127.6, num_updates=4900, lr=0.000451754, gnorm=0.507, loss_scale=32, train_wall=74, gb_free=21.6, wall=4012
2022-03-23 07:45:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:45:03 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 6.57 | ppl 95.02 | wps 172236 | wpb 2040.3 | bsz 4 | num_updates 4934 | best_loss 6.57
2022-03-23 07:45:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 4934 updates
2022-03-23 07:45:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:45:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:45:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 48 @ 4934 updates, score 6.57) (writing took 0.8473274639691226 seconds)
2022-03-23 07:45:04 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-23 07:45:04 | INFO | train | epoch 048 | loss 5.661 | ppl 50.58 | wps 80083.2 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 4934 | lr 0.000450195 | gnorm 0.509 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 4041
2022-03-23 07:45:04 | INFO | fairseq.trainer | begin training epoch 49
2022-03-23 07:45:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:45:57 | INFO | train_inner | epoch 049:     66 / 103 loss=5.633, ppl=49.63, wps=79952.2, ups=1.22, wpb=65310.7, bsz=127.6, num_updates=5000, lr=0.000447214, gnorm=0.513, loss_scale=32, train_wall=74, gb_free=21.6, wall=4093
2022-03-23 07:46:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:46:27 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 6.554 | ppl 93.95 | wps 170929 | wpb 2040.3 | bsz 4 | num_updates 5037 | best_loss 6.554
2022-03-23 07:46:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 5037 updates
2022-03-23 07:46:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:46:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:46:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 49 @ 5037 updates, score 6.554) (writing took 0.8978132650372572 seconds)
2022-03-23 07:46:28 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-23 07:46:28 | INFO | train | epoch 049 | loss 5.623 | ppl 49.29 | wps 79966.1 | ups 1.22 | wpb 65312.3 | bsz 127.6 | num_updates 5037 | lr 0.000445568 | gnorm 0.515 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 4125
2022-03-23 07:46:28 | INFO | fairseq.trainer | begin training epoch 50
2022-03-23 07:46:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:47:18 | INFO | train_inner | epoch 050:     63 / 103 loss=5.597, ppl=48.4, wps=79951.3, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=5100, lr=0.000442807, gnorm=0.505, loss_scale=32, train_wall=74, gb_free=21.6, wall=4175
2022-03-23 07:47:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:47:51 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 6.549 | ppl 93.62 | wps 172236 | wpb 2040.3 | bsz 4 | num_updates 5140 | best_loss 6.549
2022-03-23 07:47:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 5140 updates
2022-03-23 07:47:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:47:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:47:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 50 @ 5140 updates, score 6.549) (writing took 0.8804844320402481 seconds)
2022-03-23 07:47:52 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-23 07:47:52 | INFO | train | epoch 050 | loss 5.588 | ppl 48.11 | wps 80055.2 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 5140 | lr 0.000441081 | gnorm 0.504 | loss_scale 64 | train_wall 77 | gb_free 21.6 | wall 4209
2022-03-23 07:47:52 | INFO | fairseq.trainer | begin training epoch 51
2022-03-23 07:47:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:48:40 | INFO | train_inner | epoch 051:     60 / 103 loss=5.57, ppl=47.51, wps=80011.3, ups=1.23, wpb=65305.6, bsz=127.6, num_updates=5200, lr=0.000438529, gnorm=0.507, loss_scale=64, train_wall=74, gb_free=21.6, wall=4257
2022-03-23 07:49:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:49:15 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 6.551 | ppl 93.78 | wps 171768 | wpb 2040.3 | bsz 4 | num_updates 5243 | best_loss 6.549
2022-03-23 07:49:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 5243 updates
2022-03-23 07:49:15 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-23 07:49:15 | INFO | train | epoch 051 | loss 5.555 | ppl 47.01 | wps 80906.9 | ups 1.24 | wpb 65312.3 | bsz 127.6 | num_updates 5243 | lr 0.000436727 | gnorm 0.506 | loss_scale 64 | train_wall 77 | gb_free 21.6 | wall 4292
2022-03-23 07:49:15 | INFO | fairseq.trainer | begin training epoch 52
2022-03-23 07:49:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:50:01 | INFO | train_inner | epoch 052:     57 / 103 loss=5.531, ppl=46.25, wps=80777, ups=1.24, wpb=65310.7, bsz=127.6, num_updates=5300, lr=0.000434372, gnorm=0.504, loss_scale=64, train_wall=74, gb_free=21.6, wall=4338
2022-03-23 07:50:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 07:50:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:50:38 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 6.541 | ppl 93.1 | wps 171187 | wpb 2040.3 | bsz 4 | num_updates 5345 | best_loss 6.541
2022-03-23 07:50:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 5345 updates
2022-03-23 07:50:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:50:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:50:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 52 @ 5345 updates, score 6.541) (writing took 0.8766661040135659 seconds)
2022-03-23 07:50:39 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-23 07:50:39 | INFO | train | epoch 052 | loss 5.521 | ppl 45.91 | wps 79183.7 | ups 1.21 | wpb 65310.1 | bsz 127.6 | num_updates 5345 | lr 0.00043254 | gnorm 0.505 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 4376
2022-03-23 07:50:39 | INFO | fairseq.trainer | begin training epoch 53
2022-03-23 07:50:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:51:23 | INFO | train_inner | epoch 053:     55 / 103 loss=5.51, ppl=45.56, wps=79204.8, ups=1.21, wpb=65305.6, bsz=127.6, num_updates=5400, lr=0.000430331, gnorm=0.508, loss_scale=32, train_wall=75, gb_free=21.6, wall=4420
2022-03-23 07:52:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:52:03 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 6.535 | ppl 92.75 | wps 169137 | wpb 2040.3 | bsz 4 | num_updates 5448 | best_loss 6.535
2022-03-23 07:52:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 5448 updates
2022-03-23 07:52:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:52:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:52:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 53 @ 5448 updates, score 6.535) (writing took 0.8749104870366864 seconds)
2022-03-23 07:52:03 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-23 07:52:03 | INFO | train | epoch 053 | loss 5.491 | ppl 44.96 | wps 80014.8 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 5448 | lr 0.000428432 | gnorm 0.506 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 4460
2022-03-23 07:52:03 | INFO | fairseq.trainer | begin training epoch 54
2022-03-23 07:52:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:52:45 | INFO | train_inner | epoch 054:     52 / 103 loss=5.468, ppl=44.26, wps=79914.8, ups=1.22, wpb=65300.5, bsz=127.6, num_updates=5500, lr=0.000426401, gnorm=0.507, loss_scale=32, train_wall=74, gb_free=21.6, wall=4502
2022-03-23 07:53:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:53:27 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 6.522 | ppl 91.91 | wps 172472 | wpb 2040.3 | bsz 4 | num_updates 5551 | best_loss 6.522
2022-03-23 07:53:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 5551 updates
2022-03-23 07:53:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:53:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:53:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 54 @ 5551 updates, score 6.522) (writing took 0.8580471649765968 seconds)
2022-03-23 07:53:27 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-23 07:53:27 | INFO | train | epoch 054 | loss 5.461 | ppl 44.04 | wps 80055.8 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 5551 | lr 0.000424438 | gnorm 0.508 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 4544
2022-03-23 07:53:27 | INFO | fairseq.trainer | begin training epoch 55
2022-03-23 07:53:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:54:07 | INFO | train_inner | epoch 055:     49 / 103 loss=5.451, ppl=43.74, wps=79977.7, ups=1.22, wpb=65305.6, bsz=127.6, num_updates=5600, lr=0.000422577, gnorm=0.511, loss_scale=32, train_wall=74, gb_free=21.6, wall=4583
2022-03-23 07:54:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:54:51 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 6.528 | ppl 92.26 | wps 171694 | wpb 2040.3 | bsz 4 | num_updates 5654 | best_loss 6.522
2022-03-23 07:54:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 5654 updates
2022-03-23 07:54:51 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-23 07:54:51 | INFO | train | epoch 055 | loss 5.433 | ppl 43.19 | wps 80860.1 | ups 1.24 | wpb 65312.3 | bsz 127.6 | num_updates 5654 | lr 0.000420554 | gnorm 0.515 | loss_scale 32 | train_wall 77 | gb_free 21.6 | wall 4627
2022-03-23 07:54:51 | INFO | fairseq.trainer | begin training epoch 56
2022-03-23 07:54:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:55:27 | INFO | train_inner | epoch 056:     46 / 103 loss=5.42, ppl=42.8, wps=80897.3, ups=1.24, wpb=65310.7, bsz=127.6, num_updates=5700, lr=0.000418854, gnorm=0.514, loss_scale=32, train_wall=74, gb_free=21.6, wall=4664
2022-03-23 07:56:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:56:14 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 6.518 | ppl 91.66 | wps 171237 | wpb 2040.3 | bsz 4 | num_updates 5757 | best_loss 6.518
2022-03-23 07:56:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 5757 updates
2022-03-23 07:56:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:56:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:56:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 56 @ 5757 updates, score 6.518) (writing took 0.8860925080371089 seconds)
2022-03-23 07:56:15 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-23 07:56:15 | INFO | train | epoch 056 | loss 5.403 | ppl 42.31 | wps 80132.9 | ups 1.23 | wpb 65312.3 | bsz 127.6 | num_updates 5757 | lr 0.000416775 | gnorm 0.517 | loss_scale 32 | train_wall 76 | gb_free 21.6 | wall 4711
2022-03-23 07:56:15 | INFO | fairseq.trainer | begin training epoch 57
2022-03-23 07:56:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:56:49 | INFO | train_inner | epoch 057:     43 / 103 loss=5.39, ppl=41.93, wps=80044.1, ups=1.23, wpb=65305.6, bsz=127.6, num_updates=5800, lr=0.000415227, gnorm=0.516, loss_scale=32, train_wall=74, gb_free=21.6, wall=4746
2022-03-23 07:57:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-23 07:57:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:57:38 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 6.508 | ppl 91.03 | wps 171589 | wpb 2040.3 | bsz 4 | num_updates 5859 | best_loss 6.508
2022-03-23 07:57:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 5859 updates
2022-03-23 07:57:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:57:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt
2022-03-23 07:57:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/wikitext-103-cleaned-bpe-size0.0625_cross_entropy_dropout_0.45_#3/checkpoint_best.pt (epoch 57 @ 5859 updates, score 6.508) (writing took 0.8688591069658287 seconds)
2022-03-23 07:57:39 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-23 07:57:39 | INFO | train | epoch 057 | loss 5.375 | ppl 41.51 | wps 79380.1 | ups 1.22 | wpb 65310.1 | bsz 127.6 | num_updates 5859 | lr 0.000413131 | gnorm 0.512 | loss_scale 32 | train_wall 76 | gb_free 21.6 | wall 4795
2022-03-23 07:57:39 | INFO | fairseq.trainer | begin training epoch 58
2022-03-23 07:57:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:57:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-23 07:58:12 | INFO | train_inner | epoch 058:     42 / 103 loss=5.367, ppl=41.28, wps=78579.1, ups=1.2, wpb=65305.6, bsz=127.6, num_updates=5900, lr=0.000411693, gnorm=0.517, loss_scale=16, train_wall=76, gb_free=21.6, wall=4829
2022-03-23 07:59:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 07:59:02 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 6.521 | ppl 91.85 | wps 171525 | wpb 2040.3 | bsz 4 | num_updates 5961 | best_loss 6.508
2022-03-23 07:59:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 5961 updates
2022-03-23 07:59:02 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-23 07:59:02 | INFO | train | epoch 058 | loss 5.351 | ppl 40.81 | wps 80205.8 | ups 1.23 | wpb 65310.1 | bsz 127.6 | num_updates 5961 | lr 0.000409582 | gnorm 0.518 | loss_scale 16 | train_wall 76 | gb_free 21.6 | wall 4878
2022-03-23 07:59:02 | INFO | fairseq.trainer | begin training epoch 59
2022-03-23 07:59:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 07:59:33 | INFO | train_inner | epoch 059:     39 / 103 loss=5.344, ppl=40.62, wps=80935.5, ups=1.24, wpb=65305.6, bsz=127.6, num_updates=6000, lr=0.000408248, gnorm=0.522, loss_scale=16, train_wall=74, gb_free=21.6, wall=4910
2022-03-23 08:00:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 08:00:25 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 6.512 | ppl 91.28 | wps 171472 | wpb 2040.3 | bsz 4 | num_updates 6064 | best_loss 6.508
2022-03-23 08:00:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 6064 updates
2022-03-23 08:00:25 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-23 08:00:25 | INFO | train | epoch 059 | loss 5.326 | ppl 40.12 | wps 80997.9 | ups 1.24 | wpb 65312.3 | bsz 127.6 | num_updates 6064 | lr 0.000406088 | gnorm 0.519 | loss_scale 16 | train_wall 77 | gb_free 21.6 | wall 4961
2022-03-23 08:00:25 | INFO | fairseq.trainer | begin training epoch 60
2022-03-23 08:00:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-23 08:00:53 | INFO | train_inner | epoch 060:     36 / 103 loss=5.313, ppl=39.75, wps=81034.4, ups=1.24, wpb=65305.6, bsz=127.6, num_updates=6100, lr=0.000404888, gnorm=0.513, loss_scale=16, train_wall=74, gb_free=21.6, wall=4990
2022-03-23 08:01:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-23 08:01:48 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 6.517 | ppl 91.57 | wps 171953 | wpb 2040.3 | bsz 4 | num_updates 6167 | best_loss 6.508
2022-03-23 08:01:48 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 3 runs
2022-03-23 08:01:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 6167 updates
2022-03-23 08:01:48 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-23 08:01:48 | INFO | train | epoch 060 | loss 5.301 | ppl 39.42 | wps 81035.3 | ups 1.24 | wpb 65312.3 | bsz 127.6 | num_updates 6167 | lr 0.000402683 | gnorm 0.515 | loss_scale 16 | train_wall 76 | gb_free 21.6 | wall 5044
2022-03-23 08:01:48 | INFO | fairseq_cli.train | done training in 5044.3 seconds
