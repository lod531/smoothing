Sender: LSF System <lsfadmin@eu-g3-065>
Subject: Job 207014394: <w103_size_0.25_fp16_label_smoothing_0.1_#3> in cluster <euler> Done

Job <w103_size_0.25_fp16_label_smoothing_0.1_#3> was submitted from host <eu-login-19> by user <andriusb> in cluster <euler> at Thu Mar  3 10:52:49 2022
Job was executed on host(s) <eu-g3-065>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Thu Mar  3 10:53:19 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Mar  3 10:53:19 2022
Terminated at Fri Mar  4 23:16:20 2022
Results reported at Fri Mar  4 23:16:20 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --seed 1321613 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   130901.98 sec.
    Max Memory :                                 8406 MB
    Average Memory :                             3783.50 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11594.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                14
    Run time :                                   130981 sec.
    Turnaround time :                            131011 sec.

The output (if any) follows:

2022-03-03 10:53:35 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321613, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321613, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-03 10:53:36 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-03 10:53:40 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-03 10:53:40 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-03 10:53:40 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-03 10:53:40 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-03 10:53:40 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-03 10:53:40 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-03 10:53:40 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-03 10:53:43 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-03 10:53:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 10:53:43 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-03 10:53:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 10:53:43 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-03 10:53:43 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-03 10:53:43 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 10:53:43 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 10:53:43 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-03 10:53:43 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-03 10:53:43 | INFO | fairseq.trainer | begin training epoch 1
2022-03-03 10:53:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 10:53:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-03 10:53:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 10:53:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 10:54:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 10:58:21 | INFO | train_inner | epoch 001:    104 / 393 loss=17.12, nll_loss=16.922, ppl=124196, wps=25295.8, ups=0.39, wpb=65530.9, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.236, loss_scale=8, train_wall=273, gb_free=12.3, wall=279
2022-03-03 11:02:41 | INFO | train_inner | epoch 001:    204 / 393 loss=14.924, nll_loss=14.487, ppl=22960, wps=25296.4, ups=0.39, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.473, loss_scale=8, train_wall=254, gb_free=12.3, wall=538
2022-03-03 11:07:00 | INFO | train_inner | epoch 001:    304 / 393 loss=13.016, nll_loss=12.339, ppl=5181.73, wps=25297.1, ups=0.39, wpb=65535.4, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.994, loss_scale=8, train_wall=254, gb_free=12.3, wall=797
2022-03-03 11:10:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 11:10:52 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 11.369 | nll_loss 10.386 | ppl 1338.5 | wps 66008.7 | wpb 2034.1 | bsz 4 | num_updates 389
2022-03-03 11:10:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 389 updates
2022-03-03 11:10:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 11:10:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 11:11:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 1 @ 389 updates, score 11.369) (writing took 9.049037036951631 seconds)
2022-03-03 11:11:01 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-03 11:11:01 | INFO | train | epoch 001 | loss 14.277 | nll_loss 13.735 | ppl 13639.3 | wps 24992.1 | ups 0.38 | wpb 65460.8 | bsz 127.9 | num_updates 389 | lr 4.87153e-05 | gnorm 1.597 | loss_scale 8 | train_wall 1006 | gb_free 12.3 | wall 1038
2022-03-03 11:11:01 | INFO | fairseq.trainer | begin training epoch 2
2022-03-03 11:11:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 11:11:30 | INFO | train_inner | epoch 002:     11 / 393 loss=11.718, nll_loss=10.816, ppl=1802.56, wps=24148.5, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.558, loss_scale=8, train_wall=253, gb_free=12.3, wall=1067
2022-03-03 11:15:48 | INFO | train_inner | epoch 002:    111 / 393 loss=11.214, nll_loss=10.18, ppl=1160.19, wps=25334.9, ups=0.39, wpb=65530.2, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.468, loss_scale=8, train_wall=254, gb_free=12.3, wall=1326
2022-03-03 11:20:07 | INFO | train_inner | epoch 002:    211 / 393 loss=10.918, nll_loss=9.825, ppl=907.32, wps=25308.5, ups=0.39, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.515, loss_scale=16, train_wall=254, gb_free=12.3, wall=1585
2022-03-03 11:24:26 | INFO | train_inner | epoch 002:    311 / 393 loss=10.663, nll_loss=9.534, ppl=741.16, wps=25298.8, ups=0.39, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.579, loss_scale=16, train_wall=254, gb_free=12.3, wall=1844
2022-03-03 11:27:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 11:28:01 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.311 | nll_loss 9.13 | ppl 560.41 | wps 65682.8 | wpb 2034.1 | bsz 4 | num_updates 782 | best_loss 10.311
2022-03-03 11:28:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 782 updates
2022-03-03 11:28:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 11:28:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 11:28:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 2 @ 782 updates, score 10.311) (writing took 8.989682774059474 seconds)
2022-03-03 11:28:10 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-03 11:28:10 | INFO | train | epoch 002 | loss 10.847 | nll_loss 9.75 | ppl 860.8 | wps 25006.2 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 782 | lr 9.78305e-05 | gnorm 0.541 | loss_scale 16 | train_wall 997 | gb_free 12.3 | wall 2067
2022-03-03 11:28:10 | INFO | fairseq.trainer | begin training epoch 3
2022-03-03 11:28:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 11:28:57 | INFO | train_inner | epoch 003:     18 / 393 loss=10.439, nll_loss=9.279, ppl=621.26, wps=24140.2, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.653, loss_scale=16, train_wall=253, gb_free=12.3, wall=2114
2022-03-03 11:33:16 | INFO | train_inner | epoch 003:    118 / 393 loss=10.237, nll_loss=9.048, ppl=529.5, wps=25278.5, ups=0.39, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.714, loss_scale=16, train_wall=254, gb_free=12.3, wall=2373
2022-03-03 11:37:35 | INFO | train_inner | epoch 003:    218 / 393 loss=10.056, nll_loss=8.841, ppl=458.68, wps=25271.5, ups=0.39, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.788, loss_scale=16, train_wall=254, gb_free=12.3, wall=2633
2022-03-03 11:41:55 | INFO | train_inner | epoch 003:    318 / 393 loss=9.902, nll_loss=8.666, ppl=406.31, wps=25284.8, ups=0.39, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.824, loss_scale=32, train_wall=254, gb_free=12.3, wall=2892
2022-03-03 11:45:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 11:45:11 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 9.639 | nll_loss 8.364 | ppl 329.51 | wps 66034.6 | wpb 2034.1 | bsz 4 | num_updates 1175 | best_loss 9.639
2022-03-03 11:45:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1175 updates
2022-03-03 11:45:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 11:45:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 11:45:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 3 @ 1175 updates, score 9.639) (writing took 8.867932552006096 seconds)
2022-03-03 11:45:20 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-03 11:45:20 | INFO | train | epoch 003 | loss 10.024 | nll_loss 8.806 | ppl 447.45 | wps 24976.6 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 1175 | lr 0.000146946 | gnorm 0.78 | loss_scale 32 | train_wall 999 | gb_free 12.3 | wall 3097
2022-03-03 11:45:20 | INFO | fairseq.trainer | begin training epoch 4
2022-03-03 11:45:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 11:46:25 | INFO | train_inner | epoch 004:     25 / 393 loss=9.759, nll_loss=8.503, ppl=362.83, wps=24122.6, ups=0.37, wpb=65243.5, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.821, loss_scale=32, train_wall=253, gb_free=12.3, wall=3162
2022-03-03 11:50:44 | INFO | train_inner | epoch 004:    125 / 393 loss=9.607, nll_loss=8.33, ppl=321.82, wps=25261, ups=0.39, wpb=65530.9, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.816, loss_scale=32, train_wall=255, gb_free=12.3, wall=3422
2022-03-03 11:55:04 | INFO | train_inner | epoch 004:    225 / 393 loss=9.491, nll_loss=8.198, ppl=293.6, wps=25269.4, ups=0.39, wpb=65536, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.849, loss_scale=32, train_wall=254, gb_free=12.3, wall=3681
2022-03-03 11:59:23 | INFO | train_inner | epoch 004:    325 / 393 loss=9.38, nll_loss=8.071, ppl=268.99, wps=25282, ups=0.39, wpb=65535.4, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.858, loss_scale=32, train_wall=254, gb_free=12.3, wall=3940
2022-03-03 12:01:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 12:02:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 12:02:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:02:21 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.17 | nll_loss 7.82 | ppl 225.98 | wps 66287 | wpb 2034.1 | bsz 4 | num_updates 1566 | best_loss 9.17
2022-03-03 12:02:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1566 updates
2022-03-03 12:02:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 12:02:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 12:02:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 4 @ 1566 updates, score 9.17) (writing took 8.916465151123703 seconds)
2022-03-03 12:02:30 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-03 12:02:30 | INFO | train | epoch 004 | loss 9.47 | nll_loss 8.173 | ppl 288.65 | wps 24845 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 1566 | lr 0.000195811 | gnorm 0.848 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 4127
2022-03-03 12:02:30 | INFO | fairseq.trainer | begin training epoch 5
2022-03-03 12:02:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:03:58 | INFO | train_inner | epoch 005:     34 / 393 loss=9.248, nll_loss=7.92, ppl=242.23, wps=23680.5, ups=0.36, wpb=65243.5, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.846, loss_scale=16, train_wall=258, gb_free=12.3, wall=4216
2022-03-03 12:08:18 | INFO | train_inner | epoch 005:    134 / 393 loss=9.13, nll_loss=7.786, ppl=220.66, wps=25278.1, ups=0.39, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.852, loss_scale=16, train_wall=254, gb_free=12.3, wall=4475
2022-03-03 12:12:37 | INFO | train_inner | epoch 005:    234 / 393 loss=9.052, nll_loss=7.696, ppl=207.31, wps=25274.6, ups=0.39, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.832, loss_scale=16, train_wall=254, gb_free=12.3, wall=4734
2022-03-03 12:16:56 | INFO | train_inner | epoch 005:    334 / 393 loss=8.962, nll_loss=7.593, ppl=193.1, wps=25266.4, ups=0.39, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.806, loss_scale=16, train_wall=254, gb_free=12.3, wall=4994
2022-03-03 12:19:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:19:32 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.816 | nll_loss 7.406 | ppl 169.56 | wps 66112.2 | wpb 2034.1 | bsz 4 | num_updates 1959 | best_loss 8.816
2022-03-03 12:19:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1959 updates
2022-03-03 12:19:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 12:19:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 12:19:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 5 @ 1959 updates, score 8.816) (writing took 8.926674993010238 seconds)
2022-03-03 12:19:41 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-03 12:19:41 | INFO | train | epoch 005 | loss 9.038 | nll_loss 7.681 | ppl 205.16 | wps 24971.1 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 1959 | lr 0.000244926 | gnorm 0.824 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 5158
2022-03-03 12:19:41 | INFO | fairseq.trainer | begin training epoch 6
2022-03-03 12:19:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:21:27 | INFO | train_inner | epoch 006:     41 / 393 loss=8.858, nll_loss=7.475, ppl=177.95, wps=24122.8, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.813, loss_scale=16, train_wall=253, gb_free=12.3, wall=5264
2022-03-03 12:25:46 | INFO | train_inner | epoch 006:    141 / 393 loss=8.744, nll_loss=7.345, ppl=162.55, wps=25272.3, ups=0.39, wpb=65535.4, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.808, loss_scale=32, train_wall=254, gb_free=12.3, wall=5523
2022-03-03 12:30:06 | INFO | train_inner | epoch 006:    241 / 393 loss=8.68, nll_loss=7.272, ppl=154.5, wps=25273.4, ups=0.39, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.76, loss_scale=32, train_wall=254, gb_free=12.3, wall=5783
2022-03-03 12:34:25 | INFO | train_inner | epoch 006:    341 / 393 loss=8.617, nll_loss=7.199, ppl=146.91, wps=25275.2, ups=0.39, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.784, loss_scale=32, train_wall=254, gb_free=12.3, wall=6042
2022-03-03 12:36:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:36:42 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.552 | nll_loss 7.082 | ppl 135.51 | wps 65849.9 | wpb 2034.1 | bsz 4 | num_updates 2352 | best_loss 8.552
2022-03-03 12:36:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2352 updates
2022-03-03 12:36:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 12:36:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 12:36:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 6 @ 2352 updates, score 8.552) (writing took 8.900489398976788 seconds)
2022-03-03 12:36:51 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-03 12:36:51 | INFO | train | epoch 006 | loss 8.676 | nll_loss 7.267 | ppl 154.03 | wps 24971.1 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 2352 | lr 0.000294041 | gnorm 0.781 | loss_scale 32 | train_wall 999 | gb_free 12.3 | wall 6188
2022-03-03 12:36:51 | INFO | fairseq.trainer | begin training epoch 7
2022-03-03 12:36:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:38:55 | INFO | train_inner | epoch 007:     48 / 393 loss=8.503, nll_loss=7.07, ppl=134.38, wps=24115.9, ups=0.37, wpb=65243.5, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.748, loss_scale=32, train_wall=253, gb_free=12.3, wall=6313
2022-03-03 12:43:15 | INFO | train_inner | epoch 007:    148 / 393 loss=8.428, nll_loss=6.983, ppl=126.53, wps=25283.2, ups=0.39, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.749, loss_scale=32, train_wall=254, gb_free=12.3, wall=6572
2022-03-03 12:47:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 12:47:37 | INFO | train_inner | epoch 007:    249 / 393 loss=8.371, nll_loss=6.918, ppl=120.96, wps=25014.8, ups=0.38, wpb=65536, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.733, loss_scale=32, train_wall=257, gb_free=12.3, wall=6834
2022-03-03 12:51:56 | INFO | train_inner | epoch 007:    349 / 393 loss=8.323, nll_loss=6.863, ppl=116.43, wps=25266.4, ups=0.39, wpb=65530.9, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.73, loss_scale=32, train_wall=254, gb_free=12.3, wall=7093
2022-03-03 12:53:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 12:53:52 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.317 | nll_loss 6.831 | ppl 113.85 | wps 65855.9 | wpb 2034.1 | bsz 4 | num_updates 2744 | best_loss 8.317
2022-03-03 12:53:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2744 updates
2022-03-03 12:53:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 12:53:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 12:54:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 7 @ 2744 updates, score 8.317) (writing took 9.032666854094714 seconds)
2022-03-03 12:54:01 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-03 12:54:01 | INFO | train | epoch 007 | loss 8.372 | nll_loss 6.919 | ppl 121.03 | wps 24902.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 2744 | lr 0.000343031 | gnorm 0.739 | loss_scale 32 | train_wall 999 | gb_free 12.3 | wall 7218
2022-03-03 12:54:01 | INFO | fairseq.trainer | begin training epoch 8
2022-03-03 12:54:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 12:56:27 | INFO | train_inner | epoch 008:     56 / 393 loss=8.21, nll_loss=6.735, ppl=106.52, wps=24113.9, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.723, loss_scale=32, train_wall=253, gb_free=12.3, wall=7364
2022-03-03 13:00:46 | INFO | train_inner | epoch 008:    156 / 393 loss=8.142, nll_loss=6.656, ppl=100.86, wps=25255.6, ups=0.39, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.715, loss_scale=32, train_wall=255, gb_free=12.3, wall=7623
2022-03-03 13:05:05 | INFO | train_inner | epoch 008:    256 / 393 loss=8.124, nll_loss=6.636, ppl=99.46, wps=25267.4, ups=0.39, wpb=65536, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.706, loss_scale=32, train_wall=254, gb_free=12.3, wall=7883
2022-03-03 13:09:25 | INFO | train_inner | epoch 008:    356 / 393 loss=8.085, nll_loss=6.591, ppl=96.39, wps=25261.5, ups=0.39, wpb=65530.2, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.679, loss_scale=32, train_wall=254, gb_free=12.3, wall=8142
2022-03-03 13:09:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 13:11:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 13:11:03 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.144 | nll_loss 6.637 | ppl 99.51 | wps 65669.9 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 8.144
2022-03-03 13:11:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3136 updates
2022-03-03 13:11:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 13:11:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 13:11:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 8 @ 3136 updates, score 8.144) (writing took 9.034276833990589 seconds)
2022-03-03 13:11:12 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-03 13:11:12 | INFO | train | epoch 008 | loss 8.118 | nll_loss 6.629 | ppl 98.99 | wps 24892.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 3136 | lr 0.000392022 | gnorm 0.701 | loss_scale 32 | train_wall 999 | gb_free 12.3 | wall 8249
2022-03-03 13:11:12 | INFO | fairseq.trainer | begin training epoch 9
2022-03-03 13:11:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 13:13:58 | INFO | train_inner | epoch 009:     64 / 393 loss=7.97, nll_loss=6.461, ppl=88.09, wps=23867.4, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.691, loss_scale=32, train_wall=256, gb_free=12.3, wall=8415
2022-03-03 13:16:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 13:18:20 | INFO | train_inner | epoch 009:    165 / 393 loss=7.916, nll_loss=6.399, ppl=84.37, wps=25016.8, ups=0.38, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.663, loss_scale=16, train_wall=257, gb_free=12.3, wall=8677
2022-03-03 13:22:39 | INFO | train_inner | epoch 009:    265 / 393 loss=7.912, nll_loss=6.394, ppl=84.07, wps=25282.2, ups=0.39, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.656, loss_scale=16, train_wall=254, gb_free=12.3, wall=8937
2022-03-03 13:26:59 | INFO | train_inner | epoch 009:    365 / 393 loss=7.894, nll_loss=6.373, ppl=82.86, wps=25284.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.648, loss_scale=16, train_wall=254, gb_free=12.3, wall=9196
2022-03-03 13:28:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 13:28:13 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.018 | nll_loss 6.475 | ppl 88.97 | wps 65694.7 | wpb 2034.1 | bsz 4 | num_updates 3528 | best_loss 8.018
2022-03-03 13:28:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3528 updates
2022-03-03 13:28:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 13:28:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 13:28:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 9 @ 3528 updates, score 8.018) (writing took 8.954162708949298 seconds)
2022-03-03 13:28:22 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-03 13:28:22 | INFO | train | epoch 009 | loss 7.906 | nll_loss 6.388 | ppl 83.73 | wps 24909.9 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 3528 | lr 0.000441012 | gnorm 0.664 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 9279
2022-03-03 13:28:22 | INFO | fairseq.trainer | begin training epoch 10
2022-03-03 13:28:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 13:31:29 | INFO | train_inner | epoch 010:     72 / 393 loss=7.766, nll_loss=6.228, ppl=74.96, wps=24125.4, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.653, loss_scale=16, train_wall=253, gb_free=12.3, wall=9466
2022-03-03 13:35:48 | INFO | train_inner | epoch 010:    172 / 393 loss=7.731, nll_loss=6.187, ppl=72.85, wps=25277.2, ups=0.39, wpb=65530.9, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.639, loss_scale=16, train_wall=254, gb_free=12.3, wall=9725
2022-03-03 13:40:08 | INFO | train_inner | epoch 010:    272 / 393 loss=7.728, nll_loss=6.184, ppl=72.71, wps=25275.7, ups=0.39, wpb=65535.4, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.633, loss_scale=32, train_wall=254, gb_free=12.3, wall=9985
2022-03-03 13:42:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 13:44:29 | INFO | train_inner | epoch 010:    373 / 393 loss=7.721, nll_loss=6.176, ppl=72.31, wps=25037.1, ups=0.38, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.615, loss_scale=16, train_wall=257, gb_free=12.3, wall=10246
2022-03-03 13:45:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 13:45:23 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.91 | nll_loss 6.38 | ppl 83.27 | wps 65710.3 | wpb 2034.1 | bsz 4 | num_updates 3920 | best_loss 7.91
2022-03-03 13:45:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3920 updates
2022-03-03 13:45:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 13:45:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 13:45:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 10 @ 3920 updates, score 7.91) (writing took 9.021453924942762 seconds)
2022-03-03 13:45:32 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-03 13:45:32 | INFO | train | epoch 010 | loss 7.726 | nll_loss 6.182 | ppl 72.61 | wps 24909.5 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 3920 | lr 0.000490002 | gnorm 0.632 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 10310
2022-03-03 13:45:32 | INFO | fairseq.trainer | begin training epoch 11
2022-03-03 13:45:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 13:49:00 | INFO | train_inner | epoch 011:     80 / 393 loss=7.596, nll_loss=6.035, ppl=65.55, wps=24107.7, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.622, loss_scale=16, train_wall=253, gb_free=12.3, wall=10517
2022-03-03 13:53:19 | INFO | train_inner | epoch 011:    180 / 393 loss=7.562, nll_loss=5.994, ppl=63.75, wps=25275.3, ups=0.39, wpb=65535.4, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.611, loss_scale=16, train_wall=254, gb_free=12.3, wall=10776
2022-03-03 13:57:39 | INFO | train_inner | epoch 011:    280 / 393 loss=7.575, nll_loss=6.01, ppl=64.43, wps=25263.8, ups=0.39, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.597, loss_scale=16, train_wall=254, gb_free=12.3, wall=11036
2022-03-03 14:01:58 | INFO | train_inner | epoch 011:    380 / 393 loss=7.569, nll_loss=6.004, ppl=64.16, wps=25269.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.575, loss_scale=16, train_wall=254, gb_free=12.3, wall=11295
2022-03-03 14:02:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:02:34 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.824 | nll_loss 6.264 | ppl 76.87 | wps 65611.4 | wpb 2034.1 | bsz 4 | num_updates 4313 | best_loss 7.824
2022-03-03 14:02:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4313 updates
2022-03-03 14:02:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 14:02:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 14:02:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 11 @ 4313 updates, score 7.824) (writing took 8.986780171049759 seconds)
2022-03-03 14:02:43 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-03 14:02:43 | INFO | train | epoch 011 | loss 7.567 | nll_loss 6.001 | ppl 64.06 | wps 24965.1 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 4313 | lr 0.000481515 | gnorm 0.598 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 11340
2022-03-03 14:02:43 | INFO | fairseq.trainer | begin training epoch 12
2022-03-03 14:02:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:06:29 | INFO | train_inner | epoch 012:     87 / 393 loss=7.412, nll_loss=5.825, ppl=56.7, wps=24100.3, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.57, loss_scale=32, train_wall=253, gb_free=12.3, wall=11566
2022-03-03 14:07:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 14:10:51 | INFO | train_inner | epoch 012:    188 / 393 loss=7.418, nll_loss=5.831, ppl=56.93, wps=25020.2, ups=0.38, wpb=65536, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.551, loss_scale=16, train_wall=257, gb_free=12.3, wall=11828
2022-03-03 14:15:10 | INFO | train_inner | epoch 012:    288 / 393 loss=7.428, nll_loss=5.843, ppl=57.41, wps=25276.9, ups=0.39, wpb=65536, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.563, loss_scale=16, train_wall=254, gb_free=12.3, wall=12087
2022-03-03 14:19:29 | INFO | train_inner | epoch 012:    388 / 393 loss=7.427, nll_loss=5.842, ppl=57.36, wps=25275.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.544, loss_scale=16, train_wall=254, gb_free=12.3, wall=12346
2022-03-03 14:19:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:19:44 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.767 | nll_loss 6.186 | ppl 72.82 | wps 65306.3 | wpb 2034.1 | bsz 4 | num_updates 4705 | best_loss 7.767
2022-03-03 14:19:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4705 updates
2022-03-03 14:19:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 14:19:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 14:19:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 12 @ 4705 updates, score 7.767) (writing took 9.041966430144385 seconds)
2022-03-03 14:19:53 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-03 14:19:53 | INFO | train | epoch 012 | loss 7.417 | nll_loss 5.83 | ppl 56.9 | wps 24901.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 4705 | lr 0.00046102 | gnorm 0.558 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 12371
2022-03-03 14:19:53 | INFO | fairseq.trainer | begin training epoch 13
2022-03-03 14:19:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:24:00 | INFO | train_inner | epoch 013:     95 / 393 loss=7.259, nll_loss=5.651, ppl=50.27, wps=24095.5, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.538, loss_scale=16, train_wall=253, gb_free=12.3, wall=12617
2022-03-03 14:28:19 | INFO | train_inner | epoch 013:    195 / 393 loss=7.285, nll_loss=5.68, ppl=51.27, wps=25272.1, ups=0.39, wpb=65530.2, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.556, loss_scale=16, train_wall=254, gb_free=12.3, wall=12876
2022-03-03 14:30:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 14:32:41 | INFO | train_inner | epoch 013:    296 / 393 loss=7.307, nll_loss=5.705, ppl=52.18, wps=25027.5, ups=0.38, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.527, loss_scale=16, train_wall=257, gb_free=12.3, wall=13138
2022-03-03 14:36:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:36:55 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.709 | nll_loss 6.132 | ppl 70.15 | wps 65660.3 | wpb 2034.1 | bsz 4 | num_updates 5097 | best_loss 7.709
2022-03-03 14:36:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5097 updates
2022-03-03 14:36:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 14:36:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 14:37:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 13 @ 5097 updates, score 7.709) (writing took 8.993256027111784 seconds)
2022-03-03 14:37:04 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-03 14:37:04 | INFO | train | epoch 013 | loss 7.292 | nll_loss 5.688 | ppl 51.57 | wps 24902.7 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 5097 | lr 0.000442938 | gnorm 0.543 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 13401
2022-03-03 14:37:04 | INFO | fairseq.trainer | begin training epoch 14
2022-03-03 14:37:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:37:12 | INFO | train_inner | epoch 014:      3 / 393 loss=7.319, nll_loss=5.719, ppl=52.67, wps=24110.7, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.553, loss_scale=16, train_wall=253, gb_free=12.3, wall=13409
2022-03-03 14:41:31 | INFO | train_inner | epoch 014:    103 / 393 loss=7.144, nll_loss=5.521, ppl=45.92, wps=25290.6, ups=0.39, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.534, loss_scale=16, train_wall=254, gb_free=12.3, wall=13668
2022-03-03 14:45:50 | INFO | train_inner | epoch 014:    203 / 393 loss=7.176, nll_loss=5.557, ppl=47.07, wps=25277, ups=0.39, wpb=65536, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.532, loss_scale=16, train_wall=254, gb_free=12.3, wall=13927
2022-03-03 14:50:09 | INFO | train_inner | epoch 014:    303 / 393 loss=7.207, nll_loss=5.592, ppl=48.22, wps=25276.5, ups=0.39, wpb=65535.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.515, loss_scale=16, train_wall=254, gb_free=12.3, wall=14187
2022-03-03 14:53:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 14:54:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 14:54:05 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.69 | nll_loss 6.103 | ppl 68.73 | wps 65534.8 | wpb 2034.1 | bsz 4 | num_updates 5489 | best_loss 7.69
2022-03-03 14:54:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5489 updates
2022-03-03 14:54:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 14:54:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 14:54:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 14 @ 5489 updates, score 7.69) (writing took 9.001134921098128 seconds)
2022-03-03 14:54:14 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-03 14:54:14 | INFO | train | epoch 014 | loss 7.185 | nll_loss 5.567 | ppl 47.39 | wps 24909.1 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 5489 | lr 0.000426828 | gnorm 0.529 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 14431
2022-03-03 14:54:14 | INFO | fairseq.trainer | begin training epoch 15
2022-03-03 14:54:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 14:54:43 | INFO | train_inner | epoch 015:     11 / 393 loss=7.193, nll_loss=5.577, ppl=47.72, wps=23881.6, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.532, loss_scale=16, train_wall=256, gb_free=12.3, wall=14460
2022-03-03 14:59:02 | INFO | train_inner | epoch 015:    111 / 393 loss=7.044, nll_loss=5.407, ppl=42.43, wps=25274.5, ups=0.39, wpb=65536, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.534, loss_scale=16, train_wall=254, gb_free=12.3, wall=14719
2022-03-03 15:03:21 | INFO | train_inner | epoch 015:    211 / 393 loss=7.087, nll_loss=5.454, ppl=43.85, wps=25264.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.521, loss_scale=16, train_wall=254, gb_free=12.3, wall=14979
2022-03-03 15:07:41 | INFO | train_inner | epoch 015:    311 / 393 loss=7.115, nll_loss=5.487, ppl=44.84, wps=25281.1, ups=0.39, wpb=65530.9, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.516, loss_scale=16, train_wall=254, gb_free=12.3, wall=15238
2022-03-03 15:11:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 15:11:15 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.661 | nll_loss 6.065 | ppl 66.96 | wps 65615.7 | wpb 2034.1 | bsz 4 | num_updates 5882 | best_loss 7.661
2022-03-03 15:11:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5882 updates
2022-03-03 15:11:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 15:11:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 15:11:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 15 @ 5882 updates, score 7.661) (writing took 9.274999685119838 seconds)
2022-03-03 15:11:25 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-03 15:11:25 | INFO | train | epoch 015 | loss 7.092 | nll_loss 5.461 | ppl 44.04 | wps 24962.6 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 5882 | lr 0.000412323 | gnorm 0.522 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 15462
2022-03-03 15:11:25 | INFO | fairseq.trainer | begin training epoch 16
2022-03-03 15:11:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 15:12:11 | INFO | train_inner | epoch 016:     18 / 393 loss=7.102, nll_loss=5.473, ppl=44.41, wps=24089.4, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.526, loss_scale=16, train_wall=253, gb_free=12.3, wall=15509
2022-03-03 15:16:31 | INFO | train_inner | epoch 016:    118 / 393 loss=6.966, nll_loss=5.318, ppl=39.88, wps=25282, ups=0.39, wpb=65536, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.522, loss_scale=32, train_wall=254, gb_free=12.3, wall=15768
2022-03-03 15:17:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 15:20:52 | INFO | train_inner | epoch 016:    219 / 393 loss=7.005, nll_loss=5.361, ppl=41.1, wps=25030.5, ups=0.38, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.528, loss_scale=16, train_wall=257, gb_free=12.3, wall=16030
2022-03-03 15:25:12 | INFO | train_inner | epoch 016:    319 / 393 loss=7.035, nll_loss=5.396, ppl=42.1, wps=25270, ups=0.39, wpb=65530.2, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.514, loss_scale=16, train_wall=254, gb_free=12.3, wall=16289
2022-03-03 15:28:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 15:28:26 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.661 | nll_loss 6.065 | ppl 66.96 | wps 66349 | wpb 2034.1 | bsz 4 | num_updates 6274 | best_loss 7.661
2022-03-03 15:28:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6274 updates
2022-03-03 15:28:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 15:28:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 15:28:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 16 @ 6274 updates, score 7.661) (writing took 9.00847671693191 seconds)
2022-03-03 15:28:35 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-03 15:28:35 | INFO | train | epoch 016 | loss 7.008 | nll_loss 5.366 | ppl 41.23 | wps 24909 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 6274 | lr 0.000399234 | gnorm 0.523 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 16492
2022-03-03 15:28:35 | INFO | fairseq.trainer | begin training epoch 17
2022-03-03 15:28:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 15:29:42 | INFO | train_inner | epoch 017:     26 / 393 loss=7.004, nll_loss=5.361, ppl=41.08, wps=24114.4, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.517, loss_scale=16, train_wall=253, gb_free=12.3, wall=16560
2022-03-03 15:34:02 | INFO | train_inner | epoch 017:    126 / 393 loss=6.887, nll_loss=5.227, ppl=37.46, wps=25273.3, ups=0.39, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.541, loss_scale=16, train_wall=254, gb_free=12.3, wall=16819
2022-03-03 15:38:21 | INFO | train_inner | epoch 017:    226 / 393 loss=6.934, nll_loss=5.281, ppl=38.88, wps=25276.2, ups=0.39, wpb=65535.4, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.519, loss_scale=16, train_wall=254, gb_free=12.3, wall=17078
2022-03-03 15:41:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 15:42:43 | INFO | train_inner | epoch 017:    327 / 393 loss=6.964, nll_loss=5.314, ppl=39.79, wps=25021.2, ups=0.38, wpb=65530.9, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.52, loss_scale=16, train_wall=257, gb_free=12.3, wall=17340
2022-03-03 15:45:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 15:45:36 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.647 | nll_loss 6.06 | ppl 66.71 | wps 65456.6 | wpb 2034.1 | bsz 4 | num_updates 6666 | best_loss 7.647
2022-03-03 15:45:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6666 updates
2022-03-03 15:45:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 15:45:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 15:45:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 17 @ 6666 updates, score 7.647) (writing took 9.038225417025387 seconds)
2022-03-03 15:45:45 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-03 15:45:45 | INFO | train | epoch 017 | loss 6.935 | nll_loss 5.281 | ppl 38.89 | wps 24901.7 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 6666 | lr 0.000387318 | gnorm 0.522 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 17523
2022-03-03 15:45:45 | INFO | fairseq.trainer | begin training epoch 18
2022-03-03 15:45:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 15:47:14 | INFO | train_inner | epoch 018:     34 / 393 loss=6.925, nll_loss=5.27, ppl=38.6, wps=24104.2, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.512, loss_scale=16, train_wall=253, gb_free=12.3, wall=17611
2022-03-03 15:51:33 | INFO | train_inner | epoch 018:    134 / 393 loss=6.833, nll_loss=5.166, ppl=35.9, wps=25284.5, ups=0.39, wpb=65535.4, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.54, loss_scale=16, train_wall=254, gb_free=12.3, wall=17870
2022-03-03 15:55:52 | INFO | train_inner | epoch 018:    234 / 393 loss=6.867, nll_loss=5.203, ppl=36.84, wps=25249.8, ups=0.39, wpb=65530.9, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.528, loss_scale=16, train_wall=255, gb_free=12.3, wall=18129
2022-03-03 16:00:12 | INFO | train_inner | epoch 018:    334 / 393 loss=6.893, nll_loss=5.233, ppl=37.62, wps=25273.1, ups=0.39, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.534, loss_scale=16, train_wall=254, gb_free=12.3, wall=18389
2022-03-03 16:02:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:02:47 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.644 | nll_loss 6.036 | ppl 65.64 | wps 65860.1 | wpb 2034.1 | bsz 4 | num_updates 7059 | best_loss 7.644
2022-03-03 16:02:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7059 updates
2022-03-03 16:02:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 16:02:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 16:02:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 18 @ 7059 updates, score 7.644) (writing took 8.98183420789428 seconds)
2022-03-03 16:02:56 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-03 16:02:56 | INFO | train | epoch 018 | loss 6.868 | nll_loss 5.205 | ppl 36.89 | wps 24967.2 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 7059 | lr 0.000376382 | gnorm 0.532 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 18553
2022-03-03 16:02:56 | INFO | fairseq.trainer | begin training epoch 19
2022-03-03 16:02:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:04:42 | INFO | train_inner | epoch 019:     41 / 393 loss=6.847, nll_loss=5.181, ppl=36.28, wps=24116.8, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.526, loss_scale=32, train_wall=253, gb_free=12.3, wall=18659
2022-03-03 16:05:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 16:09:04 | INFO | train_inner | epoch 019:    142 / 393 loss=6.766, nll_loss=5.088, ppl=34.01, wps=25024.7, ups=0.38, wpb=65535.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.526, loss_scale=16, train_wall=257, gb_free=12.3, wall=18921
2022-03-03 16:13:23 | INFO | train_inner | epoch 019:    242 / 393 loss=6.809, nll_loss=5.137, ppl=35.19, wps=25274.8, ups=0.39, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.545, loss_scale=16, train_wall=254, gb_free=12.3, wall=19181
2022-03-03 16:17:43 | INFO | train_inner | epoch 019:    342 / 393 loss=6.845, nll_loss=5.178, ppl=36.21, wps=25280.6, ups=0.39, wpb=65536, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.533, loss_scale=16, train_wall=254, gb_free=12.3, wall=19440
2022-03-03 16:19:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:19:57 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.638 | nll_loss 6.045 | ppl 66.02 | wps 66048.6 | wpb 2034.1 | bsz 4 | num_updates 7451 | best_loss 7.638
2022-03-03 16:19:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7451 updates
2022-03-03 16:19:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 16:20:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt
2022-03-03 16:20:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_best.pt (epoch 19 @ 7451 updates, score 7.638) (writing took 8.98074238281697 seconds)
2022-03-03 16:20:06 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-03 16:20:06 | INFO | train | epoch 019 | loss 6.806 | nll_loss 5.134 | ppl 35.12 | wps 24908.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 7451 | lr 0.000366347 | gnorm 0.533 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 19583
2022-03-03 16:20:06 | INFO | fairseq.trainer | begin training epoch 20
2022-03-03 16:20:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:22:13 | INFO | train_inner | epoch 020:     49 / 393 loss=6.773, nll_loss=5.097, ppl=34.23, wps=24119.7, ups=0.37, wpb=65239, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.533, loss_scale=16, train_wall=253, gb_free=12.3, wall=19710
2022-03-03 16:26:32 | INFO | train_inner | epoch 020:    149 / 393 loss=6.713, nll_loss=5.027, ppl=32.61, wps=25274.1, ups=0.39, wpb=65535.4, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.552, loss_scale=16, train_wall=254, gb_free=12.3, wall=19970
2022-03-03 16:28:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 16:30:54 | INFO | train_inner | epoch 020:    250 / 393 loss=6.753, nll_loss=5.073, ppl=33.66, wps=25032.6, ups=0.38, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.551, loss_scale=16, train_wall=257, gb_free=12.3, wall=20231
2022-03-03 16:35:13 | INFO | train_inner | epoch 020:    350 / 393 loss=6.791, nll_loss=5.117, ppl=34.69, wps=25281, ups=0.39, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.527, loss_scale=16, train_wall=254, gb_free=12.3, wall=20491
2022-03-03 16:37:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:37:07 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.643 | nll_loss 6.035 | ppl 65.55 | wps 65318 | wpb 2034.1 | bsz 4 | num_updates 7843 | best_loss 7.638
2022-03-03 16:37:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7843 updates
2022-03-03 16:37:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 16:37:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 16:37:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 20 @ 7843 updates, score 7.643) (writing took 4.231094607850537 seconds)
2022-03-03 16:37:11 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-03 16:37:11 | INFO | train | epoch 020 | loss 6.75 | nll_loss 5.07 | ppl 33.59 | wps 25025.2 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 7843 | lr 0.000357075 | gnorm 0.54 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 20609
2022-03-03 16:37:11 | INFO | fairseq.trainer | begin training epoch 21
2022-03-03 16:37:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:39:39 | INFO | train_inner | epoch 021:     57 / 393 loss=6.703, nll_loss=5.016, ppl=32.37, wps=24541.4, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.535, loss_scale=16, train_wall=253, gb_free=12.3, wall=20756
2022-03-03 16:43:59 | INFO | train_inner | epoch 021:    157 / 393 loss=6.663, nll_loss=4.97, ppl=31.34, wps=25275, ups=0.39, wpb=65535.4, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.535, loss_scale=16, train_wall=254, gb_free=12.3, wall=21016
2022-03-03 16:48:18 | INFO | train_inner | epoch 021:    257 / 393 loss=6.716, nll_loss=5.031, ppl=32.68, wps=25289.8, ups=0.39, wpb=65530.9, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.534, loss_scale=16, train_wall=254, gb_free=12.3, wall=21275
2022-03-03 16:50:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 16:52:39 | INFO | train_inner | epoch 021:    358 / 393 loss=6.741, nll_loss=5.059, ppl=33.33, wps=25042.6, ups=0.38, wpb=65536, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.541, loss_scale=16, train_wall=257, gb_free=12.3, wall=21537
2022-03-03 16:54:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 16:54:12 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.65 | nll_loss 6.058 | ppl 66.61 | wps 65928.3 | wpb 2034.1 | bsz 4 | num_updates 8235 | best_loss 7.638
2022-03-03 16:54:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8235 updates
2022-03-03 16:54:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 16:54:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 16:54:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 21 @ 8235 updates, score 7.65) (writing took 4.218145177001134 seconds)
2022-03-03 16:54:17 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-03 16:54:17 | INFO | train | epoch 021 | loss 6.699 | nll_loss 5.011 | ppl 32.24 | wps 25030.4 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 8235 | lr 0.000348472 | gnorm 0.543 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 21634
2022-03-03 16:54:17 | INFO | fairseq.trainer | begin training epoch 22
2022-03-03 16:54:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 16:57:05 | INFO | train_inner | epoch 022:     65 / 393 loss=6.64, nll_loss=4.943, ppl=30.77, wps=24553.7, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.563, loss_scale=16, train_wall=253, gb_free=12.3, wall=21802
2022-03-03 17:01:24 | INFO | train_inner | epoch 022:    165 / 393 loss=6.623, nll_loss=4.923, ppl=30.34, wps=25283.1, ups=0.39, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.551, loss_scale=16, train_wall=254, gb_free=12.3, wall=22061
2022-03-03 17:05:44 | INFO | train_inner | epoch 022:    265 / 393 loss=6.655, nll_loss=4.96, ppl=31.12, wps=25271.3, ups=0.39, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.552, loss_scale=16, train_wall=254, gb_free=12.3, wall=22321
2022-03-03 17:10:03 | INFO | train_inner | epoch 022:    365 / 393 loss=6.697, nll_loss=5.008, ppl=32.18, wps=25268.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.548, loss_scale=16, train_wall=254, gb_free=12.3, wall=22580
2022-03-03 17:11:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:11:18 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.659 | nll_loss 6.074 | ppl 67.36 | wps 65865.7 | wpb 2034.1 | bsz 4 | num_updates 8628 | best_loss 7.638
2022-03-03 17:11:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8628 updates
2022-03-03 17:11:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 17:11:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 17:11:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 22 @ 8628 updates, score 7.659) (writing took 4.301766821881756 seconds)
2022-03-03 17:11:22 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-03 17:11:22 | INFO | train | epoch 022 | loss 6.651 | nll_loss 4.955 | ppl 31.02 | wps 25086.7 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 8628 | lr 0.000340443 | gnorm 0.55 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 22659
2022-03-03 17:11:22 | INFO | fairseq.trainer | begin training epoch 23
2022-03-03 17:11:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 17:13:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 17:14:31 | INFO | train_inner | epoch 023:     73 / 393 loss=6.593, nll_loss=4.89, ppl=29.65, wps=24311, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.548, loss_scale=16, train_wall=256, gb_free=12.3, wall=22849
2022-03-03 17:18:51 | INFO | train_inner | epoch 023:    173 / 393 loss=6.568, nll_loss=4.86, ppl=29.04, wps=25274.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.557, loss_scale=16, train_wall=254, gb_free=12.3, wall=23108
2022-03-03 17:23:10 | INFO | train_inner | epoch 023:    273 / 393 loss=6.624, nll_loss=4.923, ppl=30.35, wps=25269, ups=0.39, wpb=65535.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.559, loss_scale=16, train_wall=254, gb_free=12.3, wall=23367
2022-03-03 17:27:29 | INFO | train_inner | epoch 023:    373 / 393 loss=6.659, nll_loss=4.964, ppl=31.22, wps=25283.2, ups=0.39, wpb=65536, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.553, loss_scale=16, train_wall=254, gb_free=12.3, wall=23626
2022-03-03 17:28:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:28:23 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.669 | nll_loss 6.068 | ppl 67.07 | wps 65763.1 | wpb 2034.1 | bsz 4 | num_updates 9020 | best_loss 7.638
2022-03-03 17:28:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9020 updates
2022-03-03 17:28:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 17:28:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 17:28:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 23 @ 9020 updates, score 7.669) (writing took 4.226869320962578 seconds)
2022-03-03 17:28:28 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-03 17:28:28 | INFO | train | epoch 023 | loss 6.605 | nll_loss 4.903 | ppl 29.92 | wps 25022.4 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 9020 | lr 0.000332964 | gnorm 0.553 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 23685
2022-03-03 17:28:28 | INFO | fairseq.trainer | begin training epoch 24
2022-03-03 17:28:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 17:31:55 | INFO | train_inner | epoch 024:     80 / 393 loss=6.527, nll_loss=4.813, ppl=28.11, wps=24546.8, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.552, loss_scale=16, train_wall=253, gb_free=12.3, wall=23892
2022-03-03 17:36:14 | INFO | train_inner | epoch 024:    180 / 393 loss=6.549, nll_loss=4.838, ppl=28.6, wps=25271.4, ups=0.39, wpb=65530.9, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.57, loss_scale=32, train_wall=254, gb_free=12.3, wall=24152
2022-03-03 17:36:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 17:40:36 | INFO | train_inner | epoch 024:    281 / 393 loss=6.581, nll_loss=4.874, ppl=29.33, wps=25042.7, ups=0.38, wpb=65535.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.564, loss_scale=16, train_wall=257, gb_free=12.3, wall=24413
2022-03-03 17:44:55 | INFO | train_inner | epoch 024:    381 / 393 loss=6.611, nll_loss=4.909, ppl=30.04, wps=25285.2, ups=0.39, wpb=65536, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.565, loss_scale=16, train_wall=254, gb_free=12.3, wall=24672
2022-03-03 17:45:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 17:45:29 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.687 | nll_loss 6.09 | ppl 68.1 | wps 65665.4 | wpb 2034.1 | bsz 4 | num_updates 9412 | best_loss 7.638
2022-03-03 17:45:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9412 updates
2022-03-03 17:45:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 17:45:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 17:45:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 24 @ 9412 updates, score 7.687) (writing took 4.179939826950431 seconds)
2022-03-03 17:45:33 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-03 17:45:33 | INFO | train | epoch 024 | loss 6.564 | nll_loss 4.855 | ppl 28.94 | wps 25030.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 9412 | lr 0.000325956 | gnorm 0.563 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 24710
2022-03-03 17:45:33 | INFO | fairseq.trainer | begin training epoch 25
2022-03-03 17:45:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 17:49:21 | INFO | train_inner | epoch 025:     88 / 393 loss=6.48, nll_loss=4.76, ppl=27.09, wps=24546.1, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.556, loss_scale=16, train_wall=253, gb_free=12.3, wall=24938
2022-03-03 17:53:40 | INFO | train_inner | epoch 025:    188 / 393 loss=6.504, nll_loss=4.786, ppl=27.59, wps=25289.4, ups=0.39, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.588, loss_scale=16, train_wall=254, gb_free=12.3, wall=25197
2022-03-03 17:57:59 | INFO | train_inner | epoch 025:    288 / 393 loss=6.542, nll_loss=4.829, ppl=28.43, wps=25274.4, ups=0.39, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.558, loss_scale=16, train_wall=254, gb_free=12.3, wall=25457
2022-03-03 17:59:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 18:02:21 | INFO | train_inner | epoch 025:    389 / 393 loss=6.578, nll_loss=4.871, ppl=29.25, wps=25032.2, ups=0.38, wpb=65535.4, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.566, loss_scale=16, train_wall=257, gb_free=12.3, wall=25718
2022-03-03 18:02:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:02:34 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.701 | nll_loss 6.108 | ppl 68.98 | wps 65577.5 | wpb 2034.1 | bsz 4 | num_updates 9804 | best_loss 7.638
2022-03-03 18:02:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9804 updates
2022-03-03 18:02:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 18:02:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 18:02:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 25 @ 9804 updates, score 7.701) (writing took 4.237360735889524 seconds)
2022-03-03 18:02:38 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-03 18:02:38 | INFO | train | epoch 025 | loss 6.523 | nll_loss 4.809 | ppl 28.02 | wps 25025.8 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 9804 | lr 0.000319373 | gnorm 0.568 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 25735
2022-03-03 18:02:38 | INFO | fairseq.trainer | begin training epoch 26
2022-03-03 18:02:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:06:47 | INFO | train_inner | epoch 026:     96 / 393 loss=6.423, nll_loss=4.693, ppl=25.87, wps=24536.8, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.56, loss_scale=16, train_wall=253, gb_free=12.3, wall=25984
2022-03-03 18:10:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 18:11:09 | INFO | train_inner | epoch 026:    197 / 393 loss=6.459, nll_loss=4.734, ppl=26.62, wps=25029.1, ups=0.38, wpb=65530.9, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.585, loss_scale=8, train_wall=257, gb_free=12.3, wall=26246
2022-03-03 18:15:28 | INFO | train_inner | epoch 026:    297 / 393 loss=6.521, nll_loss=4.805, ppl=27.96, wps=25289.2, ups=0.39, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.581, loss_scale=8, train_wall=254, gb_free=12.3, wall=26505
2022-03-03 18:19:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:19:39 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.721 | nll_loss 6.128 | ppl 69.91 | wps 66226.2 | wpb 2034.1 | bsz 4 | num_updates 10196 | best_loss 7.638
2022-03-03 18:19:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10196 updates
2022-03-03 18:19:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 18:19:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 18:19:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 26 @ 10196 updates, score 7.721) (writing took 4.150033202022314 seconds)
2022-03-03 18:19:43 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-03 18:19:43 | INFO | train | epoch 026 | loss 6.486 | nll_loss 4.765 | ppl 27.2 | wps 25032.5 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 10196 | lr 0.000313174 | gnorm 0.575 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 26760
2022-03-03 18:19:43 | INFO | fairseq.trainer | begin training epoch 27
2022-03-03 18:19:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:19:54 | INFO | train_inner | epoch 027:      4 / 393 loss=6.543, nll_loss=4.83, ppl=28.45, wps=24572.1, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.574, loss_scale=8, train_wall=253, gb_free=12.3, wall=26771
2022-03-03 18:24:13 | INFO | train_inner | epoch 027:    104 / 393 loss=6.382, nll_loss=4.646, ppl=25.04, wps=25316.2, ups=0.39, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.581, loss_scale=8, train_wall=254, gb_free=12.3, wall=27030
2022-03-03 18:28:32 | INFO | train_inner | epoch 027:    204 / 393 loss=6.441, nll_loss=4.713, ppl=26.22, wps=25307.2, ups=0.39, wpb=65536, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.575, loss_scale=8, train_wall=254, gb_free=12.3, wall=27289
2022-03-03 18:32:50 | INFO | train_inner | epoch 027:    304 / 393 loss=6.485, nll_loss=4.764, ppl=27.16, wps=25307.4, ups=0.39, wpb=65536, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.587, loss_scale=16, train_wall=254, gb_free=12.3, wall=27548
2022-03-03 18:36:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:36:43 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.726 | nll_loss 6.128 | ppl 69.92 | wps 65866 | wpb 2034.1 | bsz 4 | num_updates 10589 | best_loss 7.638
2022-03-03 18:36:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10589 updates
2022-03-03 18:36:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 18:36:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 18:36:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 27 @ 10589 updates, score 7.726) (writing took 4.175527534913272 seconds)
2022-03-03 18:36:48 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-03 18:36:48 | INFO | train | epoch 027 | loss 6.452 | nll_loss 4.726 | ppl 26.46 | wps 25115.1 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 10589 | lr 0.000307307 | gnorm 0.584 | loss_scale 16 | train_wall 997 | gb_free 12.3 | wall 27785
2022-03-03 18:36:48 | INFO | fairseq.trainer | begin training epoch 28
2022-03-03 18:36:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:37:16 | INFO | train_inner | epoch 028:     11 / 393 loss=6.49, nll_loss=4.77, ppl=27.28, wps=24554.2, ups=0.38, wpb=65243.5, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.595, loss_scale=16, train_wall=253, gb_free=12.3, wall=27813
2022-03-03 18:41:35 | INFO | train_inner | epoch 028:    111 / 393 loss=6.361, nll_loss=4.621, ppl=24.61, wps=25275.1, ups=0.39, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.588, loss_scale=16, train_wall=254, gb_free=12.3, wall=28073
2022-03-03 18:45:55 | INFO | train_inner | epoch 028:    211 / 393 loss=6.402, nll_loss=4.668, ppl=25.42, wps=25290.7, ups=0.39, wpb=65536, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.582, loss_scale=16, train_wall=254, gb_free=12.3, wall=28332
2022-03-03 18:50:14 | INFO | train_inner | epoch 028:    311 / 393 loss=6.45, nll_loss=4.723, ppl=26.42, wps=25295.3, ups=0.39, wpb=65530.2, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.604, loss_scale=16, train_wall=254, gb_free=12.3, wall=28591
2022-03-03 18:53:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 18:53:48 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.738 | nll_loss 6.146 | ppl 70.82 | wps 65906.6 | wpb 2034.1 | bsz 4 | num_updates 10982 | best_loss 7.638
2022-03-03 18:53:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10982 updates
2022-03-03 18:53:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 18:53:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 18:53:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 28 @ 10982 updates, score 7.738) (writing took 4.233402814948931 seconds)
2022-03-03 18:53:53 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-03 18:53:53 | INFO | train | epoch 028 | loss 6.419 | nll_loss 4.687 | ppl 25.76 | wps 25097.1 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 10982 | lr 0.000301758 | gnorm 0.589 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 28810
2022-03-03 18:53:53 | INFO | fairseq.trainer | begin training epoch 29
2022-03-03 18:53:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 18:54:39 | INFO | train_inner | epoch 029:     18 / 393 loss=6.451, nll_loss=4.724, ppl=26.43, wps=24556.8, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.583, loss_scale=16, train_wall=253, gb_free=12.3, wall=28857
2022-03-03 18:55:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 18:59:01 | INFO | train_inner | epoch 029:    119 / 393 loss=6.337, nll_loss=4.593, ppl=24.13, wps=25043.1, ups=0.38, wpb=65536, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.586, loss_scale=16, train_wall=257, gb_free=12.3, wall=29118
2022-03-03 19:03:20 | INFO | train_inner | epoch 029:    219 / 393 loss=6.378, nll_loss=4.64, ppl=24.93, wps=25290.7, ups=0.39, wpb=65530.9, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.59, loss_scale=16, train_wall=254, gb_free=12.3, wall=29377
2022-03-03 19:05:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 19:07:42 | INFO | train_inner | epoch 029:    320 / 393 loss=6.413, nll_loss=4.68, ppl=25.63, wps=25052.1, ups=0.38, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.612, loss_scale=8, train_wall=257, gb_free=12.3, wall=29639
2022-03-03 19:10:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:10:53 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.75 | nll_loss 6.14 | ppl 70.52 | wps 66039.3 | wpb 2034.1 | bsz 4 | num_updates 11373 | best_loss 7.638
2022-03-03 19:10:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11373 updates
2022-03-03 19:10:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 19:10:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 19:10:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 29 @ 11373 updates, score 7.75) (writing took 4.222468343097717 seconds)
2022-03-03 19:10:57 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-03 19:10:57 | INFO | train | epoch 029 | loss 6.387 | nll_loss 4.651 | ppl 25.12 | wps 24979.6 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 11373 | lr 0.000296526 | gnorm 0.596 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 29834
2022-03-03 19:10:57 | INFO | fairseq.trainer | begin training epoch 30
2022-03-03 19:10:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 19:12:07 | INFO | train_inner | epoch 030:     27 / 393 loss=6.402, nll_loss=4.668, ppl=25.42, wps=24572, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.598, loss_scale=8, train_wall=253, gb_free=12.3, wall=29905
2022-03-03 19:16:26 | INFO | train_inner | epoch 030:    127 / 393 loss=6.31, nll_loss=4.562, ppl=23.63, wps=25293.5, ups=0.39, wpb=65530.2, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.583, loss_scale=8, train_wall=254, gb_free=12.3, wall=30164
2022-03-03 19:20:45 | INFO | train_inner | epoch 030:    227 / 393 loss=6.349, nll_loss=4.607, ppl=24.36, wps=25299.7, ups=0.39, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.609, loss_scale=8, train_wall=254, gb_free=12.3, wall=30423
2022-03-03 19:25:04 | INFO | train_inner | epoch 030:    327 / 393 loss=6.395, nll_loss=4.66, ppl=25.27, wps=25307.3, ups=0.39, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.6, loss_scale=8, train_wall=254, gb_free=12.3, wall=30682
2022-03-03 19:27:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:27:58 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.775 | nll_loss 6.189 | ppl 72.95 | wps 65808.4 | wpb 2034.1 | bsz 4 | num_updates 11766 | best_loss 7.638
2022-03-03 19:27:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11766 updates
2022-03-03 19:27:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 19:28:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 19:28:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 30 @ 11766 updates, score 7.775) (writing took 4.2729919189587235 seconds)
2022-03-03 19:28:02 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-03 19:28:02 | INFO | train | epoch 030 | loss 6.358 | nll_loss 4.617 | ppl 24.54 | wps 25110.8 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 11766 | lr 0.000291532 | gnorm 0.6 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 30859
2022-03-03 19:28:02 | INFO | fairseq.trainer | begin training epoch 31
2022-03-03 19:28:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 19:29:30 | INFO | train_inner | epoch 031:     34 / 393 loss=6.366, nll_loss=4.626, ppl=24.7, wps=24569.3, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.61, loss_scale=16, train_wall=253, gb_free=12.3, wall=30947
2022-03-03 19:33:49 | INFO | train_inner | epoch 031:    134 / 393 loss=6.278, nll_loss=4.524, ppl=23.01, wps=25274.2, ups=0.39, wpb=65535.4, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.601, loss_scale=16, train_wall=254, gb_free=12.3, wall=31206
2022-03-03 19:38:09 | INFO | train_inner | epoch 031:    234 / 393 loss=6.326, nll_loss=4.579, ppl=23.9, wps=25276.1, ups=0.39, wpb=65530.9, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.616, loss_scale=16, train_wall=254, gb_free=12.3, wall=31466
2022-03-03 19:42:28 | INFO | train_inner | epoch 031:    334 / 393 loss=6.374, nll_loss=4.635, ppl=24.85, wps=25283, ups=0.39, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.602, loss_scale=16, train_wall=254, gb_free=12.3, wall=31725
2022-03-03 19:44:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 19:45:03 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.785 | nll_loss 6.19 | ppl 73.03 | wps 65775.5 | wpb 2034.1 | bsz 4 | num_updates 12159 | best_loss 7.638
2022-03-03 19:45:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12159 updates
2022-03-03 19:45:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 19:45:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 19:45:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 31 @ 12159 updates, score 7.785) (writing took 4.2488117618486285 seconds)
2022-03-03 19:45:07 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-03 19:45:07 | INFO | train | epoch 031 | loss 6.33 | nll_loss 4.585 | ppl 23.99 | wps 25091.4 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 12159 | lr 0.000286781 | gnorm 0.606 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 31884
2022-03-03 19:45:07 | INFO | fairseq.trainer | begin training epoch 32
2022-03-03 19:45:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 19:46:54 | INFO | train_inner | epoch 032:     41 / 393 loss=6.326, nll_loss=4.58, ppl=23.91, wps=24542.2, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.604, loss_scale=16, train_wall=253, gb_free=12.3, wall=31991
2022-03-03 19:50:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 19:51:15 | INFO | train_inner | epoch 032:    142 / 393 loss=6.258, nll_loss=4.501, ppl=22.65, wps=25024.1, ups=0.38, wpb=65530.9, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.611, loss_scale=16, train_wall=257, gb_free=12.3, wall=32253
2022-03-03 19:55:35 | INFO | train_inner | epoch 032:    242 / 393 loss=6.305, nll_loss=4.555, ppl=23.51, wps=25273.4, ups=0.39, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.617, loss_scale=16, train_wall=254, gb_free=12.3, wall=32512
2022-03-03 19:59:54 | INFO | train_inner | epoch 032:    342 / 393 loss=6.345, nll_loss=4.601, ppl=24.27, wps=25281.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.618, loss_scale=16, train_wall=254, gb_free=12.3, wall=32771
2022-03-03 20:01:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:02:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:02:08 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.788 | nll_loss 6.187 | ppl 72.84 | wps 66000.8 | wpb 2034.1 | bsz 4 | num_updates 12550 | best_loss 7.638
2022-03-03 20:02:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12550 updates
2022-03-03 20:02:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 20:02:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 20:02:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 32 @ 12550 updates, score 7.788) (writing took 4.5411483771167696 seconds)
2022-03-03 20:02:13 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-03 20:02:13 | INFO | train | epoch 032 | loss 6.303 | nll_loss 4.553 | ppl 23.48 | wps 24954.2 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 12550 | lr 0.000282279 | gnorm 0.616 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 32910
2022-03-03 20:02:13 | INFO | fairseq.trainer | begin training epoch 33
2022-03-03 20:02:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:04:22 | INFO | train_inner | epoch 033:     50 / 393 loss=6.286, nll_loss=4.534, ppl=23.17, wps=24316.5, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.622, loss_scale=8, train_wall=255, gb_free=12.3, wall=33040
2022-03-03 20:08:41 | INFO | train_inner | epoch 033:    150 / 393 loss=6.233, nll_loss=4.472, ppl=22.2, wps=25296.2, ups=0.39, wpb=65536, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.629, loss_scale=8, train_wall=254, gb_free=12.3, wall=33299
2022-03-03 20:13:00 | INFO | train_inner | epoch 033:    250 / 393 loss=6.288, nll_loss=4.535, ppl=23.18, wps=25298.5, ups=0.39, wpb=65530.9, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.624, loss_scale=8, train_wall=254, gb_free=12.3, wall=33558
2022-03-03 20:17:19 | INFO | train_inner | epoch 033:    350 / 393 loss=6.322, nll_loss=4.575, ppl=23.83, wps=25299.1, ups=0.39, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.635, loss_scale=8, train_wall=254, gb_free=12.3, wall=33817
2022-03-03 20:19:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:19:13 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.797 | nll_loss 6.204 | ppl 73.7 | wps 65648 | wpb 2034.1 | bsz 4 | num_updates 12943 | best_loss 7.638
2022-03-03 20:19:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12943 updates
2022-03-03 20:19:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 20:19:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 20:19:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 33 @ 12943 updates, score 7.797) (writing took 4.253479102160782 seconds)
2022-03-03 20:19:17 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-03 20:19:17 | INFO | train | epoch 033 | loss 6.278 | nll_loss 4.524 | ppl 23.01 | wps 25108.5 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 12943 | lr 0.00027796 | gnorm 0.626 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 33935
2022-03-03 20:19:17 | INFO | fairseq.trainer | begin training epoch 34
2022-03-03 20:19:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:21:45 | INFO | train_inner | epoch 034:     57 / 393 loss=6.253, nll_loss=4.496, ppl=22.56, wps=24560, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.62, loss_scale=8, train_wall=253, gb_free=12.3, wall=34082
2022-03-03 20:26:04 | INFO | train_inner | epoch 034:    157 / 393 loss=6.214, nll_loss=4.45, ppl=21.86, wps=25288, ups=0.39, wpb=65536, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.62, loss_scale=16, train_wall=254, gb_free=12.3, wall=34342
2022-03-03 20:30:24 | INFO | train_inner | epoch 034:    257 / 393 loss=6.261, nll_loss=4.504, ppl=22.7, wps=25271, ups=0.39, wpb=65535.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.626, loss_scale=16, train_wall=254, gb_free=12.3, wall=34601
2022-03-03 20:34:43 | INFO | train_inner | epoch 034:    357 / 393 loss=6.305, nll_loss=4.554, ppl=23.49, wps=25283.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.638, loss_scale=16, train_wall=254, gb_free=12.3, wall=34860
2022-03-03 20:36:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:36:18 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.835 | nll_loss 6.236 | ppl 75.36 | wps 65944 | wpb 2034.1 | bsz 4 | num_updates 13336 | best_loss 7.638
2022-03-03 20:36:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13336 updates
2022-03-03 20:36:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 20:36:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 20:36:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 34 @ 13336 updates, score 7.835) (writing took 4.26329140807502 seconds)
2022-03-03 20:36:23 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-03 20:36:23 | INFO | train | epoch 034 | loss 6.255 | nll_loss 4.497 | ppl 22.58 | wps 25093.6 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 13336 | lr 0.000273834 | gnorm 0.627 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 34960
2022-03-03 20:36:23 | INFO | fairseq.trainer | begin training epoch 35
2022-03-03 20:36:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:39:09 | INFO | train_inner | epoch 035:     64 / 393 loss=6.213, nll_loss=4.449, ppl=21.84, wps=24552.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.64, loss_scale=16, train_wall=253, gb_free=12.3, wall=35126
2022-03-03 20:43:28 | INFO | train_inner | epoch 035:    164 / 393 loss=6.195, nll_loss=4.427, ppl=21.52, wps=25273.5, ups=0.39, wpb=65530.9, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.629, loss_scale=16, train_wall=254, gb_free=12.3, wall=35385
2022-03-03 20:46:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 20:47:50 | INFO | train_inner | epoch 035:    265 / 393 loss=6.241, nll_loss=4.48, ppl=22.32, wps=25034.7, ups=0.38, wpb=65535.4, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.637, loss_scale=16, train_wall=257, gb_free=12.3, wall=35647
2022-03-03 20:51:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:52:11 | INFO | train_inner | epoch 035:    366 / 393 loss=6.29, nll_loss=4.537, ppl=23.22, wps=25034.4, ups=0.38, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.64, loss_scale=8, train_wall=257, gb_free=12.3, wall=35909
2022-03-03 20:53:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 20:53:24 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.841 | nll_loss 6.258 | ppl 76.55 | wps 65720.5 | wpb 2034.1 | bsz 4 | num_updates 13727 | best_loss 7.638
2022-03-03 20:53:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13727 updates
2022-03-03 20:53:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 20:53:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 20:53:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 35 @ 13727 updates, score 7.841) (writing took 4.207405238877982 seconds)
2022-03-03 20:53:28 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-03 20:53:28 | INFO | train | epoch 035 | loss 6.232 | nll_loss 4.47 | ppl 22.16 | wps 24965.2 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 13727 | lr 0.000269906 | gnorm 0.638 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 35985
2022-03-03 20:53:28 | INFO | fairseq.trainer | begin training epoch 36
2022-03-03 20:53:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 20:56:37 | INFO | train_inner | epoch 036:     73 / 393 loss=6.178, nll_loss=4.408, ppl=21.24, wps=24564, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.642, loss_scale=8, train_wall=253, gb_free=12.3, wall=36174
2022-03-03 21:00:56 | INFO | train_inner | epoch 036:    173 / 393 loss=6.175, nll_loss=4.405, ppl=21.18, wps=25290.1, ups=0.39, wpb=65536, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.638, loss_scale=8, train_wall=254, gb_free=12.3, wall=36433
2022-03-03 21:05:15 | INFO | train_inner | epoch 036:    273 / 393 loss=6.226, nll_loss=4.463, ppl=22.06, wps=25285.7, ups=0.39, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.638, loss_scale=8, train_wall=254, gb_free=12.3, wall=36693
2022-03-03 21:09:34 | INFO | train_inner | epoch 036:    373 / 393 loss=6.275, nll_loss=4.519, ppl=22.93, wps=25291.7, ups=0.39, wpb=65530.2, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.646, loss_scale=8, train_wall=254, gb_free=12.3, wall=36952
2022-03-03 21:10:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:10:29 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.852 | nll_loss 6.271 | ppl 77.25 | wps 66092.1 | wpb 2034.1 | bsz 4 | num_updates 14120 | best_loss 7.638
2022-03-03 21:10:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14120 updates
2022-03-03 21:10:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 21:10:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 21:10:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 36 @ 14120 updates, score 7.852) (writing took 4.229155093198642 seconds)
2022-03-03 21:10:33 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-03 21:10:33 | INFO | train | epoch 036 | loss 6.21 | nll_loss 4.445 | ppl 21.78 | wps 25101.6 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 14120 | lr 0.000266123 | gnorm 0.641 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 37010
2022-03-03 21:10:33 | INFO | fairseq.trainer | begin training epoch 37
2022-03-03 21:10:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:14:00 | INFO | train_inner | epoch 037:     80 / 393 loss=6.146, nll_loss=4.371, ppl=20.69, wps=24568.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.645, loss_scale=16, train_wall=253, gb_free=12.3, wall=37217
2022-03-03 21:18:19 | INFO | train_inner | epoch 037:    180 / 393 loss=6.156, nll_loss=4.382, ppl=20.85, wps=25273.9, ups=0.39, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.644, loss_scale=16, train_wall=254, gb_free=12.3, wall=37477
2022-03-03 21:22:39 | INFO | train_inner | epoch 037:    280 / 393 loss=6.209, nll_loss=4.443, ppl=21.74, wps=25273.3, ups=0.39, wpb=65530.2, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.662, loss_scale=16, train_wall=254, gb_free=12.3, wall=37736
2022-03-03 21:24:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 21:27:00 | INFO | train_inner | epoch 037:    381 / 393 loss=6.255, nll_loss=4.496, ppl=22.56, wps=25032.5, ups=0.38, wpb=65536, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.647, loss_scale=8, train_wall=257, gb_free=12.3, wall=37998
2022-03-03 21:27:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:27:34 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.866 | nll_loss 6.267 | ppl 77.02 | wps 65685.1 | wpb 2034.1 | bsz 4 | num_updates 14512 | best_loss 7.638
2022-03-03 21:27:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14512 updates
2022-03-03 21:27:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 21:27:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 21:27:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 37 @ 14512 updates, score 7.866) (writing took 4.2333368309773505 seconds)
2022-03-03 21:27:38 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-03 21:27:38 | INFO | train | epoch 037 | loss 6.189 | nll_loss 4.42 | ppl 21.41 | wps 25028.9 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 14512 | lr 0.000262504 | gnorm 0.65 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 38035
2022-03-03 21:27:38 | INFO | fairseq.trainer | begin training epoch 38
2022-03-03 21:27:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:31:26 | INFO | train_inner | epoch 038:     88 / 393 loss=6.116, nll_loss=4.336, ppl=20.19, wps=24556.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.652, loss_scale=8, train_wall=253, gb_free=12.3, wall=38263
2022-03-03 21:35:45 | INFO | train_inner | epoch 038:    188 / 393 loss=6.145, nll_loss=4.37, ppl=20.67, wps=25291.2, ups=0.39, wpb=65535.4, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.664, loss_scale=8, train_wall=254, gb_free=12.3, wall=38522
2022-03-03 21:40:04 | INFO | train_inner | epoch 038:    288 / 393 loss=6.193, nll_loss=4.424, ppl=21.47, wps=25302.1, ups=0.39, wpb=65530.9, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.652, loss_scale=8, train_wall=254, gb_free=12.3, wall=38781
2022-03-03 21:44:23 | INFO | train_inner | epoch 038:    388 / 393 loss=6.233, nll_loss=4.47, ppl=22.17, wps=25294.8, ups=0.39, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.663, loss_scale=8, train_wall=254, gb_free=12.3, wall=39041
2022-03-03 21:44:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:44:39 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.875 | nll_loss 6.289 | ppl 78.18 | wps 65621.2 | wpb 2034.1 | bsz 4 | num_updates 14905 | best_loss 7.638
2022-03-03 21:44:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14905 updates
2022-03-03 21:44:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 21:44:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 21:44:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 38 @ 14905 updates, score 7.875) (writing took 4.23540657106787 seconds)
2022-03-03 21:44:43 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-03 21:44:43 | INFO | train | epoch 038 | loss 6.17 | nll_loss 4.398 | ppl 21.08 | wps 25103.5 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 14905 | lr 0.00025902 | gnorm 0.659 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 39060
2022-03-03 21:44:43 | INFO | fairseq.trainer | begin training epoch 39
2022-03-03 21:44:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:48:49 | INFO | train_inner | epoch 039:     95 / 393 loss=6.087, nll_loss=4.302, ppl=19.73, wps=24558.2, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.643, loss_scale=16, train_wall=253, gb_free=12.3, wall=39306
2022-03-03 21:53:09 | INFO | train_inner | epoch 039:    195 / 393 loss=6.132, nll_loss=4.354, ppl=20.44, wps=25257.1, ups=0.39, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.652, loss_scale=16, train_wall=255, gb_free=12.3, wall=39566
2022-03-03 21:57:28 | INFO | train_inner | epoch 039:    295 / 393 loss=6.171, nll_loss=4.399, ppl=21.09, wps=25270.4, ups=0.39, wpb=65530.2, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.661, loss_scale=16, train_wall=254, gb_free=12.3, wall=39825
2022-03-03 22:01:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:01:44 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.892 | nll_loss 6.313 | ppl 79.48 | wps 65696 | wpb 2034.1 | bsz 4 | num_updates 15298 | best_loss 7.638
2022-03-03 22:01:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15298 updates
2022-03-03 22:01:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 22:01:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 22:01:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 39 @ 15298 updates, score 7.892) (writing took 4.2169814130757 seconds)
2022-03-03 22:01:48 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-03 22:01:48 | INFO | train | epoch 039 | loss 6.15 | nll_loss 4.375 | ppl 20.75 | wps 25086.2 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 15298 | lr 0.000255672 | gnorm 0.655 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 40086
2022-03-03 22:01:48 | INFO | fairseq.trainer | begin training epoch 40
2022-03-03 22:01:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:01:54 | INFO | train_inner | epoch 040:      2 / 393 loss=6.216, nll_loss=4.451, ppl=21.87, wps=24551.1, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.664, loss_scale=16, train_wall=253, gb_free=12.3, wall=40091
2022-03-03 22:06:13 | INFO | train_inner | epoch 040:    102 / 393 loss=6.066, nll_loss=4.278, ppl=19.4, wps=25285.5, ups=0.39, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.665, loss_scale=16, train_wall=254, gb_free=12.3, wall=40350
2022-03-03 22:09:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 22:10:35 | INFO | train_inner | epoch 040:    203 / 393 loss=6.116, nll_loss=4.335, ppl=20.18, wps=25032.4, ups=0.38, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.666, loss_scale=16, train_wall=257, gb_free=12.3, wall=40612
2022-03-03 22:14:54 | INFO | train_inner | epoch 040:    303 / 393 loss=6.153, nll_loss=4.378, ppl=20.8, wps=25278.4, ups=0.39, wpb=65530.2, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.665, loss_scale=16, train_wall=254, gb_free=12.3, wall=40871
2022-03-03 22:18:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:18:49 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.912 | nll_loss 6.327 | ppl 80.27 | wps 65945.2 | wpb 2034.1 | bsz 4 | num_updates 15690 | best_loss 7.638
2022-03-03 22:18:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15690 updates
2022-03-03 22:18:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 22:18:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 22:18:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 40 @ 15690 updates, score 7.912) (writing took 4.3143652870785445 seconds)
2022-03-03 22:18:54 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-03 22:18:54 | INFO | train | epoch 040 | loss 6.132 | nll_loss 4.354 | ppl 20.44 | wps 25023.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 15690 | lr 0.000252458 | gnorm 0.667 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 41111
2022-03-03 22:18:54 | INFO | fairseq.trainer | begin training epoch 41
2022-03-03 22:18:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:19:20 | INFO | train_inner | epoch 041:     10 / 393 loss=6.186, nll_loss=4.416, ppl=21.35, wps=24530.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.672, loss_scale=16, train_wall=253, gb_free=12.3, wall=41137
2022-03-03 22:23:39 | INFO | train_inner | epoch 041:    110 / 393 loss=6.048, nll_loss=4.257, ppl=19.12, wps=25278.9, ups=0.39, wpb=65535.4, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.668, loss_scale=16, train_wall=254, gb_free=12.3, wall=41396
2022-03-03 22:27:58 | INFO | train_inner | epoch 041:    210 / 393 loss=6.104, nll_loss=4.321, ppl=19.99, wps=25279.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.674, loss_scale=16, train_wall=254, gb_free=12.3, wall=41656
2022-03-03 22:31:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 22:32:20 | INFO | train_inner | epoch 041:    311 / 393 loss=6.135, nll_loss=4.357, ppl=20.49, wps=25040.7, ups=0.38, wpb=65536, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.67, loss_scale=16, train_wall=257, gb_free=12.3, wall=41917
2022-03-03 22:35:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:35:55 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.924 | nll_loss 6.339 | ppl 80.94 | wps 65747.9 | wpb 2034.1 | bsz 4 | num_updates 16082 | best_loss 7.638
2022-03-03 22:35:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16082 updates
2022-03-03 22:35:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 22:35:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 22:35:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 41 @ 16082 updates, score 7.924) (writing took 4.2557576729450375 seconds)
2022-03-03 22:35:59 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-03 22:35:59 | INFO | train | epoch 041 | loss 6.115 | nll_loss 4.334 | ppl 20.16 | wps 25026.2 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 16082 | lr 0.000249362 | gnorm 0.671 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 42136
2022-03-03 22:35:59 | INFO | fairseq.trainer | begin training epoch 42
2022-03-03 22:35:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:36:46 | INFO | train_inner | epoch 042:     18 / 393 loss=6.158, nll_loss=4.384, ppl=20.88, wps=24537.2, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.67, loss_scale=16, train_wall=253, gb_free=12.3, wall=42183
2022-03-03 22:41:05 | INFO | train_inner | epoch 042:    118 / 393 loss=6.039, nll_loss=4.246, ppl=18.98, wps=25277.7, ups=0.39, wpb=65535.4, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.68, loss_scale=16, train_wall=254, gb_free=12.3, wall=42442
2022-03-03 22:45:24 | INFO | train_inner | epoch 042:    218 / 393 loss=6.083, nll_loss=4.297, ppl=19.65, wps=25282.5, ups=0.39, wpb=65530.9, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.672, loss_scale=16, train_wall=254, gb_free=12.3, wall=42702
2022-03-03 22:49:44 | INFO | train_inner | epoch 042:    318 / 393 loss=6.135, nll_loss=4.356, ppl=20.48, wps=25275.9, ups=0.39, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.673, loss_scale=16, train_wall=254, gb_free=12.3, wall=42961
2022-03-03 22:50:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 22:52:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:53:00 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.928 | nll_loss 6.327 | ppl 80.28 | wps 66143.6 | wpb 2034.1 | bsz 4 | num_updates 16474 | best_loss 7.638
2022-03-03 22:53:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16474 updates
2022-03-03 22:53:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 22:53:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 22:53:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 42 @ 16474 updates, score 7.928) (writing took 4.284717618022114 seconds)
2022-03-03 22:53:05 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-03 22:53:05 | INFO | train | epoch 042 | loss 6.098 | nll_loss 4.315 | ppl 19.9 | wps 25025.8 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 16474 | lr 0.000246377 | gnorm 0.679 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 43162
2022-03-03 22:53:05 | INFO | fairseq.trainer | begin training epoch 43
2022-03-03 22:53:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:54:12 | INFO | train_inner | epoch 043:     26 / 393 loss=6.126, nll_loss=4.347, ppl=20.34, wps=24320.3, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.686, loss_scale=8, train_wall=256, gb_free=12.3, wall=43229
2022-03-03 22:58:31 | INFO | train_inner | epoch 043:    126 / 393 loss=6.024, nll_loss=4.229, ppl=18.75, wps=25304.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.672, loss_scale=8, train_wall=254, gb_free=12.3, wall=43488
2022-03-03 23:02:50 | INFO | train_inner | epoch 043:    226 / 393 loss=6.076, nll_loss=4.288, ppl=19.53, wps=25295.5, ups=0.39, wpb=65536, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.681, loss_scale=8, train_wall=254, gb_free=12.3, wall=43747
2022-03-03 23:07:09 | INFO | train_inner | epoch 043:    326 / 393 loss=6.117, nll_loss=4.335, ppl=20.19, wps=25297, ups=0.39, wpb=65530.9, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.682, loss_scale=8, train_wall=254, gb_free=12.3, wall=44006
2022-03-03 23:10:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:10:05 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.929 | nll_loss 6.346 | ppl 81.36 | wps 66039.6 | wpb 2034.1 | bsz 4 | num_updates 16867 | best_loss 7.638
2022-03-03 23:10:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16867 updates
2022-03-03 23:10:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 23:10:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 23:10:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 43 @ 16867 updates, score 7.929) (writing took 4.279058263869956 seconds)
2022-03-03 23:10:09 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-03 23:10:09 | INFO | train | epoch 043 | loss 6.083 | nll_loss 4.297 | ppl 19.65 | wps 25108.1 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 16867 | lr 0.00024349 | gnorm 0.679 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 44186
2022-03-03 23:10:09 | INFO | fairseq.trainer | begin training epoch 44
2022-03-03 23:10:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:11:35 | INFO | train_inner | epoch 044:     33 / 393 loss=6.106, nll_loss=4.323, ppl=20.02, wps=24564.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.698, loss_scale=8, train_wall=253, gb_free=12.3, wall=44272
2022-03-03 23:15:54 | INFO | train_inner | epoch 044:    133 / 393 loss=6.012, nll_loss=4.214, ppl=18.56, wps=25278.7, ups=0.39, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.692, loss_scale=16, train_wall=254, gb_free=12.3, wall=44531
2022-03-03 23:20:13 | INFO | train_inner | epoch 044:    233 / 393 loss=6.068, nll_loss=4.279, ppl=19.42, wps=25289.2, ups=0.39, wpb=65530.9, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.695, loss_scale=16, train_wall=254, gb_free=12.3, wall=44790
2022-03-03 23:24:33 | INFO | train_inner | epoch 044:    333 / 393 loss=6.107, nll_loss=4.324, ppl=20.03, wps=25260.1, ups=0.39, wpb=65535.4, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.698, loss_scale=16, train_wall=255, gb_free=12.3, wall=45050
2022-03-03 23:27:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:27:10 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.955 | nll_loss 6.361 | ppl 82.21 | wps 65523.2 | wpb 2034.1 | bsz 4 | num_updates 17260 | best_loss 7.638
2022-03-03 23:27:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17260 updates
2022-03-03 23:27:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 23:27:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 23:27:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 44 @ 17260 updates, score 7.955) (writing took 4.189906127052382 seconds)
2022-03-03 23:27:15 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-03 23:27:15 | INFO | train | epoch 044 | loss 6.067 | nll_loss 4.278 | ppl 19.41 | wps 25091 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 17260 | lr 0.000240702 | gnorm 0.698 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 45212
2022-03-03 23:27:15 | INFO | fairseq.trainer | begin training epoch 45
2022-03-03 23:27:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:28:58 | INFO | train_inner | epoch 045:     40 / 393 loss=6.074, nll_loss=4.286, ppl=19.51, wps=24554.2, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.689, loss_scale=16, train_wall=253, gb_free=12.3, wall=45315
2022-03-03 23:33:18 | INFO | train_inner | epoch 045:    140 / 393 loss=6.004, nll_loss=4.205, ppl=18.45, wps=25282.4, ups=0.39, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.693, loss_scale=16, train_wall=254, gb_free=12.3, wall=45575
2022-03-03 23:35:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 23:37:39 | INFO | train_inner | epoch 045:    241 / 393 loss=6.054, nll_loss=4.263, ppl=19.2, wps=25036.3, ups=0.38, wpb=65530.9, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.683, loss_scale=16, train_wall=257, gb_free=12.3, wall=45836
2022-03-03 23:41:59 | INFO | train_inner | epoch 045:    341 / 393 loss=6.088, nll_loss=4.302, ppl=19.73, wps=25270.9, ups=0.39, wpb=65535.4, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.699, loss_scale=16, train_wall=254, gb_free=12.3, wall=46096
2022-03-03 23:44:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:44:16 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.962 | nll_loss 6.38 | ppl 83.3 | wps 65858.9 | wpb 2034.1 | bsz 4 | num_updates 17652 | best_loss 7.638
2022-03-03 23:44:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17652 updates
2022-03-03 23:44:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 23:44:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-03 23:44:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 45 @ 17652 updates, score 7.962) (writing took 4.213740511098877 seconds)
2022-03-03 23:44:20 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-03 23:44:20 | INFO | train | epoch 045 | loss 6.053 | nll_loss 4.261 | ppl 19.18 | wps 25026.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 17652 | lr 0.000238014 | gnorm 0.69 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 46237
2022-03-03 23:44:20 | INFO | fairseq.trainer | begin training epoch 46
2022-03-03 23:44:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:46:24 | INFO | train_inner | epoch 046:     48 / 393 loss=6.055, nll_loss=4.264, ppl=19.22, wps=24548.4, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.697, loss_scale=16, train_wall=253, gb_free=12.3, wall=46362
2022-03-03 23:50:44 | INFO | train_inner | epoch 046:    148 / 393 loss=5.996, nll_loss=4.196, ppl=18.33, wps=25277.1, ups=0.39, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.697, loss_scale=16, train_wall=254, gb_free=12.3, wall=46621
2022-03-03 23:55:03 | INFO | train_inner | epoch 046:    248 / 393 loss=6.038, nll_loss=4.244, ppl=18.95, wps=25268.3, ups=0.39, wpb=65535.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.697, loss_scale=16, train_wall=254, gb_free=12.3, wall=46880
2022-03-03 23:57:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 23:59:25 | INFO | train_inner | epoch 046:    349 / 393 loss=6.082, nll_loss=4.295, ppl=19.63, wps=25026.5, ups=0.38, wpb=65536, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.716, loss_scale=16, train_wall=257, gb_free=12.3, wall=47142
2022-03-04 00:01:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:01:21 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.965 | nll_loss 6.38 | ppl 83.3 | wps 65687.1 | wpb 2034.1 | bsz 4 | num_updates 18044 | best_loss 7.638
2022-03-04 00:01:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18044 updates
2022-03-04 00:01:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 00:01:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 00:01:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 46 @ 18044 updates, score 7.965) (writing took 4.2182253720238805 seconds)
2022-03-04 00:01:25 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 00:01:25 | INFO | train | epoch 046 | loss 6.039 | nll_loss 4.245 | ppl 18.96 | wps 25021.9 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 18044 | lr 0.000235415 | gnorm 0.703 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 47263
2022-03-04 00:01:25 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 00:01:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:03:51 | INFO | train_inner | epoch 047:     56 / 393 loss=6.018, nll_loss=4.221, ppl=18.65, wps=24542.2, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.704, loss_scale=16, train_wall=253, gb_free=12.3, wall=47408
2022-03-04 00:08:10 | INFO | train_inner | epoch 047:    156 / 393 loss=5.98, nll_loss=4.177, ppl=18.09, wps=25274.1, ups=0.39, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.704, loss_scale=16, train_wall=254, gb_free=12.3, wall=47667
2022-03-04 00:12:29 | INFO | train_inner | epoch 047:    256 / 393 loss=6.026, nll_loss=4.231, ppl=18.77, wps=25282.9, ups=0.39, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.703, loss_scale=16, train_wall=254, gb_free=12.3, wall=47926
2022-03-04 00:16:48 | INFO | train_inner | epoch 047:    356 / 393 loss=6.08, nll_loss=4.292, ppl=19.59, wps=25285.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.71, loss_scale=16, train_wall=254, gb_free=12.3, wall=48186
2022-03-04 00:18:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:18:26 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.977 | nll_loss 6.38 | ppl 83.28 | wps 65865.4 | wpb 2034.1 | bsz 4 | num_updates 18437 | best_loss 7.638
2022-03-04 00:18:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18437 updates
2022-03-04 00:18:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 00:18:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 00:18:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 47 @ 18437 updates, score 7.977) (writing took 4.205698843114078 seconds)
2022-03-04 00:18:31 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 00:18:31 | INFO | train | epoch 047 | loss 6.026 | nll_loss 4.23 | ppl 18.76 | wps 25090.9 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 18437 | lr 0.000232892 | gnorm 0.705 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 48288
2022-03-04 00:18:31 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 00:18:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:20:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 00:21:17 | INFO | train_inner | epoch 048:     64 / 393 loss=6.005, nll_loss=4.205, ppl=18.45, wps=24317.4, ups=0.37, wpb=65248.6, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.703, loss_scale=16, train_wall=256, gb_free=12.3, wall=48454
2022-03-04 00:25:36 | INFO | train_inner | epoch 048:    164 / 393 loss=5.973, nll_loss=4.169, ppl=17.99, wps=25285.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.705, loss_scale=16, train_wall=254, gb_free=12.3, wall=48713
2022-03-04 00:29:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 00:29:58 | INFO | train_inner | epoch 048:    265 / 393 loss=6.019, nll_loss=4.222, ppl=18.66, wps=25028.7, ups=0.38, wpb=65530.9, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.718, loss_scale=8, train_wall=257, gb_free=12.3, wall=48975
2022-03-04 00:34:17 | INFO | train_inner | epoch 048:    365 / 393 loss=6.07, nll_loss=4.281, ppl=19.44, wps=25281.1, ups=0.39, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.713, loss_scale=8, train_wall=254, gb_free=12.3, wall=49234
2022-03-04 00:35:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:35:32 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.995 | nll_loss 6.407 | ppl 84.85 | wps 65869.7 | wpb 2034.1 | bsz 4 | num_updates 18828 | best_loss 7.638
2022-03-04 00:35:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18828 updates
2022-03-04 00:35:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 00:35:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 00:35:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 48 @ 18828 updates, score 7.995) (writing took 4.21817048988305 seconds)
2022-03-04 00:35:36 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 00:35:36 | INFO | train | epoch 048 | loss 6.013 | nll_loss 4.214 | ppl 18.56 | wps 24964.4 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 18828 | lr 0.000230461 | gnorm 0.709 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 49313
2022-03-04 00:35:36 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 00:35:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:38:43 | INFO | train_inner | epoch 049:     72 / 393 loss=5.963, nll_loss=4.157, ppl=17.84, wps=24561.1, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.703, loss_scale=8, train_wall=253, gb_free=12.3, wall=49500
2022-03-04 00:43:02 | INFO | train_inner | epoch 049:    172 / 393 loss=5.971, nll_loss=4.166, ppl=17.95, wps=25305.2, ups=0.39, wpb=65536, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.714, loss_scale=8, train_wall=254, gb_free=12.3, wall=49759
2022-03-04 00:47:21 | INFO | train_inner | epoch 049:    272 / 393 loss=6.02, nll_loss=4.223, ppl=18.67, wps=25294.5, ups=0.39, wpb=65530.9, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.712, loss_scale=8, train_wall=254, gb_free=12.3, wall=50018
2022-03-04 00:51:40 | INFO | train_inner | epoch 049:    372 / 393 loss=6.06, nll_loss=4.268, ppl=19.27, wps=25297.7, ups=0.39, wpb=65535.4, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.719, loss_scale=8, train_wall=254, gb_free=12.3, wall=50277
2022-03-04 00:52:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:52:36 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.996 | nll_loss 6.407 | ppl 84.84 | wps 65443.3 | wpb 2034.1 | bsz 4 | num_updates 19221 | best_loss 7.638
2022-03-04 00:52:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19221 updates
2022-03-04 00:52:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 00:52:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 00:52:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 49 @ 19221 updates, score 7.996) (writing took 4.2443464528769255 seconds)
2022-03-04 00:52:41 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 00:52:41 | INFO | train | epoch 049 | loss 6.001 | nll_loss 4.201 | ppl 18.39 | wps 25105.9 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 19221 | lr 0.000228093 | gnorm 0.714 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 50338
2022-03-04 00:52:41 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 00:52:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:56:06 | INFO | train_inner | epoch 050:     79 / 393 loss=5.946, nll_loss=4.137, ppl=17.6, wps=24542.7, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=19300, lr=0.000227626, gnorm=0.707, loss_scale=16, train_wall=253, gb_free=12.3, wall=50543
2022-03-04 01:00:25 | INFO | train_inner | epoch 050:    179 / 393 loss=5.962, nll_loss=4.155, ppl=17.81, wps=25274.9, ups=0.39, wpb=65530.2, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.723, loss_scale=16, train_wall=254, gb_free=12.3, wall=50802
2022-03-04 01:04:44 | INFO | train_inner | epoch 050:    279 / 393 loss=6.002, nll_loss=4.201, ppl=18.39, wps=25289.7, ups=0.39, wpb=65536, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.724, loss_scale=16, train_wall=254, gb_free=12.3, wall=51061
2022-03-04 01:09:03 | INFO | train_inner | epoch 050:    379 / 393 loss=6.054, nll_loss=4.262, ppl=19.18, wps=25289.7, ups=0.39, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.725, loss_scale=16, train_wall=254, gb_free=12.3, wall=51320
2022-03-04 01:09:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:09:42 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.008 | nll_loss 6.418 | ppl 85.49 | wps 65428.8 | wpb 2034.1 | bsz 4 | num_updates 19614 | best_loss 7.638
2022-03-04 01:09:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19614 updates
2022-03-04 01:09:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 01:09:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 01:09:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 50 @ 19614 updates, score 8.008) (writing took 4.256225512130186 seconds)
2022-03-04 01:09:46 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 01:09:46 | INFO | train | epoch 050 | loss 5.989 | nll_loss 4.187 | ppl 18.21 | wps 25092.4 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 19614 | lr 0.000225796 | gnorm 0.72 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 51363
2022-03-04 01:09:46 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 01:09:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:13:29 | INFO | train_inner | epoch 051:     86 / 393 loss=5.924, nll_loss=4.112, ppl=17.29, wps=24543.3, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.715, loss_scale=16, train_wall=253, gb_free=12.3, wall=51586
2022-03-04 01:14:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 01:17:51 | INFO | train_inner | epoch 051:    187 / 393 loss=5.95, nll_loss=4.141, ppl=17.65, wps=25032.4, ups=0.38, wpb=65535.4, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.715, loss_scale=16, train_wall=257, gb_free=12.3, wall=51848
2022-03-04 01:22:10 | INFO | train_inner | epoch 051:    287 / 393 loss=6, nll_loss=4.199, ppl=18.37, wps=25271.2, ups=0.39, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.724, loss_scale=16, train_wall=254, gb_free=12.3, wall=52107
2022-03-04 01:26:29 | INFO | train_inner | epoch 051:    387 / 393 loss=6.043, nll_loss=4.249, ppl=19.01, wps=25283.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.732, loss_scale=16, train_wall=254, gb_free=12.3, wall=52367
2022-03-04 01:26:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:26:47 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 8.018 | nll_loss 6.427 | ppl 86.04 | wps 65640.9 | wpb 2034.1 | bsz 4 | num_updates 20006 | best_loss 7.638
2022-03-04 01:26:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20006 updates
2022-03-04 01:26:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 01:26:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 01:26:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 51 @ 20006 updates, score 8.018) (writing took 4.217659997055307 seconds)
2022-03-04 01:26:51 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 01:26:51 | INFO | train | epoch 051 | loss 5.977 | nll_loss 4.173 | ppl 18.04 | wps 25026.4 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 20006 | lr 0.000223573 | gnorm 0.722 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 52389
2022-03-04 01:26:51 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 01:26:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:30:55 | INFO | train_inner | epoch 052:     94 / 393 loss=5.906, nll_loss=4.091, ppl=17.04, wps=24547.4, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=20100, lr=0.00022305, gnorm=0.709, loss_scale=16, train_wall=253, gb_free=12.3, wall=52632
2022-03-04 01:35:14 | INFO | train_inner | epoch 052:    194 / 393 loss=5.936, nll_loss=4.125, ppl=17.45, wps=25282.9, ups=0.39, wpb=65530.2, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.719, loss_scale=16, train_wall=254, gb_free=12.3, wall=52892
2022-03-04 01:36:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 01:39:36 | INFO | train_inner | epoch 052:    295 / 393 loss=5.99, nll_loss=4.187, ppl=18.21, wps=25023.5, ups=0.38, wpb=65536, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.722, loss_scale=16, train_wall=257, gb_free=12.3, wall=53153
2022-03-04 01:43:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:43:53 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 8.023 | nll_loss 6.44 | ppl 86.84 | wps 65407.3 | wpb 2034.1 | bsz 4 | num_updates 20398 | best_loss 7.638
2022-03-04 01:43:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20398 updates
2022-03-04 01:43:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 01:43:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 01:43:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 52 @ 20398 updates, score 8.023) (writing took 4.26359783904627 seconds)
2022-03-04 01:43:57 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 01:43:57 | INFO | train | epoch 052 | loss 5.966 | nll_loss 4.16 | ppl 17.87 | wps 25022.8 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 20398 | lr 0.000221415 | gnorm 0.718 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 53414
2022-03-04 01:43:57 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 01:43:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:44:02 | INFO | train_inner | epoch 053:      2 / 393 loss=6.036, nll_loss=4.241, ppl=18.91, wps=24541, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.724, loss_scale=16, train_wall=253, gb_free=12.3, wall=53419
2022-03-04 01:48:21 | INFO | train_inner | epoch 053:    102 / 393 loss=5.893, nll_loss=4.075, ppl=16.86, wps=25276.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.724, loss_scale=16, train_wall=254, gb_free=12.3, wall=53679
2022-03-04 01:52:41 | INFO | train_inner | epoch 053:    202 / 393 loss=5.937, nll_loss=4.126, ppl=17.46, wps=25270.9, ups=0.39, wpb=65536, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.733, loss_scale=16, train_wall=254, gb_free=12.3, wall=53938
2022-03-04 01:57:00 | INFO | train_inner | epoch 053:    302 / 393 loss=5.978, nll_loss=4.173, ppl=18.04, wps=25263.8, ups=0.39, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.725, loss_scale=16, train_wall=255, gb_free=12.3, wall=54197
2022-03-04 01:59:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 02:00:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:00:58 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 8.023 | nll_loss 6.443 | ppl 87 | wps 65783.8 | wpb 2034.1 | bsz 4 | num_updates 20790 | best_loss 7.638
2022-03-04 02:00:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20790 updates
2022-03-04 02:00:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 02:01:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 02:01:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 53 @ 20790 updates, score 8.023) (writing took 4.195502909831703 seconds)
2022-03-04 02:01:02 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 02:01:02 | INFO | train | epoch 053 | loss 5.956 | nll_loss 4.148 | ppl 17.72 | wps 25021.5 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 20790 | lr 0.000219317 | gnorm 0.727 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 54440
2022-03-04 02:01:02 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 02:01:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:01:28 | INFO | train_inner | epoch 054:     10 / 393 loss=6.006, nll_loss=4.206, ppl=18.46, wps=24319.8, ups=0.37, wpb=65248.6, bsz=127.4, num_updates=20800, lr=0.000219265, gnorm=0.728, loss_scale=16, train_wall=256, gb_free=12.3, wall=54466
2022-03-04 02:05:48 | INFO | train_inner | epoch 054:    110 / 393 loss=5.886, nll_loss=4.067, ppl=16.76, wps=25283.4, ups=0.39, wpb=65530.2, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.731, loss_scale=16, train_wall=254, gb_free=12.3, wall=54725
2022-03-04 02:10:07 | INFO | train_inner | epoch 054:    210 / 393 loss=5.925, nll_loss=4.112, ppl=17.3, wps=25287.7, ups=0.39, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.728, loss_scale=16, train_wall=254, gb_free=12.3, wall=54984
2022-03-04 02:14:26 | INFO | train_inner | epoch 054:    310 / 393 loss=5.976, nll_loss=4.171, ppl=18.02, wps=25271.3, ups=0.39, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.744, loss_scale=16, train_wall=254, gb_free=12.3, wall=55243
2022-03-04 02:18:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:18:04 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 8.031 | nll_loss 6.46 | ppl 88.05 | wps 65512.3 | wpb 2034.1 | bsz 4 | num_updates 21183 | best_loss 7.638
2022-03-04 02:18:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21183 updates
2022-03-04 02:18:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 02:18:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 02:18:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 54 @ 21183 updates, score 8.031) (writing took 4.2480114051140845 seconds)
2022-03-04 02:18:08 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 02:18:08 | INFO | train | epoch 054 | loss 5.946 | nll_loss 4.136 | ppl 17.58 | wps 25086.6 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 21183 | lr 0.000217273 | gnorm 0.736 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 55465
2022-03-04 02:18:08 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 02:18:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:18:52 | INFO | train_inner | epoch 055:     17 / 393 loss=5.988, nll_loss=4.185, ppl=18.19, wps=24533.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.748, loss_scale=16, train_wall=253, gb_free=12.3, wall=55509
2022-03-04 02:21:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 02:23:14 | INFO | train_inner | epoch 055:    118 / 393 loss=5.869, nll_loss=4.047, ppl=16.53, wps=25027.4, ups=0.38, wpb=65530.9, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.727, loss_scale=16, train_wall=257, gb_free=12.3, wall=55771
2022-03-04 02:27:33 | INFO | train_inner | epoch 055:    218 / 393 loss=5.922, nll_loss=4.108, ppl=17.25, wps=25268.2, ups=0.39, wpb=65535.4, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.737, loss_scale=16, train_wall=254, gb_free=12.3, wall=56030
2022-03-04 02:31:53 | INFO | train_inner | epoch 055:    318 / 393 loss=5.977, nll_loss=4.172, ppl=18.02, wps=25262.4, ups=0.39, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.74, loss_scale=16, train_wall=255, gb_free=12.3, wall=56290
2022-03-04 02:35:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:35:09 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 8.056 | nll_loss 6.476 | ppl 89.02 | wps 65567.2 | wpb 2034.1 | bsz 4 | num_updates 21575 | best_loss 7.638
2022-03-04 02:35:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 21575 updates
2022-03-04 02:35:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 02:35:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 02:35:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 55 @ 21575 updates, score 8.056) (writing took 4.246564984088764 seconds)
2022-03-04 02:35:14 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 02:35:14 | INFO | train | epoch 055 | loss 5.936 | nll_loss 4.124 | ppl 17.44 | wps 25016.5 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 21575 | lr 0.00021529 | gnorm 0.738 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 56491
2022-03-04 02:35:14 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 02:35:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:36:18 | INFO | train_inner | epoch 056:     25 / 393 loss=5.964, nll_loss=4.157, ppl=17.84, wps=24544.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=21600, lr=0.000215166, gnorm=0.742, loss_scale=16, train_wall=253, gb_free=12.3, wall=56556
2022-03-04 02:40:38 | INFO | train_inner | epoch 056:    125 / 393 loss=5.872, nll_loss=4.051, ppl=16.57, wps=25286.8, ups=0.39, wpb=65530.9, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.738, loss_scale=16, train_wall=254, gb_free=12.3, wall=56815
2022-03-04 02:41:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 02:44:59 | INFO | train_inner | epoch 056:    226 / 393 loss=5.91, nll_loss=4.094, ppl=17.08, wps=25047.3, ups=0.38, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.741, loss_scale=8, train_wall=257, gb_free=12.3, wall=57077
2022-03-04 02:49:18 | INFO | train_inner | epoch 056:    326 / 393 loss=5.967, nll_loss=4.16, ppl=17.88, wps=25304.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.745, loss_scale=8, train_wall=254, gb_free=12.3, wall=57335
2022-03-04 02:52:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:52:14 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 8.04 | nll_loss 6.459 | ppl 87.99 | wps 65878.2 | wpb 2034.1 | bsz 4 | num_updates 21967 | best_loss 7.638
2022-03-04 02:52:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 21967 updates
2022-03-04 02:52:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 02:52:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 02:52:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 56 @ 21967 updates, score 8.04) (writing took 4.316916072042659 seconds)
2022-03-04 02:52:18 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-04 02:52:18 | INFO | train | epoch 056 | loss 5.926 | nll_loss 4.113 | ppl 17.31 | wps 25039.8 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 21967 | lr 0.000213361 | gnorm 0.743 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 57516
2022-03-04 02:52:18 | INFO | fairseq.trainer | begin training epoch 57
2022-03-04 02:52:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:53:44 | INFO | train_inner | epoch 057:     33 / 393 loss=5.948, nll_loss=4.138, ppl=17.61, wps=24555.9, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=22000, lr=0.000213201, gnorm=0.749, loss_scale=8, train_wall=253, gb_free=12.3, wall=57601
2022-03-04 02:58:03 | INFO | train_inner | epoch 057:    133 / 393 loss=5.87, nll_loss=4.048, ppl=16.54, wps=25297.8, ups=0.39, wpb=65530.2, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.736, loss_scale=8, train_wall=254, gb_free=12.3, wall=57860
2022-03-04 03:02:22 | INFO | train_inner | epoch 057:    233 / 393 loss=5.912, nll_loss=4.096, ppl=17.11, wps=25289, ups=0.39, wpb=65536, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.741, loss_scale=8, train_wall=254, gb_free=12.3, wall=58119
2022-03-04 03:06:41 | INFO | train_inner | epoch 057:    333 / 393 loss=5.951, nll_loss=4.141, ppl=17.65, wps=25275.5, ups=0.39, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.744, loss_scale=16, train_wall=254, gb_free=12.3, wall=58379
2022-03-04 03:09:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:09:19 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 8.06 | nll_loss 6.484 | ppl 89.5 | wps 65596.6 | wpb 2034.1 | bsz 4 | num_updates 22360 | best_loss 7.638
2022-03-04 03:09:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 22360 updates
2022-03-04 03:09:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 03:09:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 03:09:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 57 @ 22360 updates, score 8.06) (writing took 4.232286016922444 seconds)
2022-03-04 03:09:23 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-04 03:09:23 | INFO | train | epoch 057 | loss 5.917 | nll_loss 4.103 | ppl 17.18 | wps 25098.3 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 22360 | lr 0.000211477 | gnorm 0.744 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 58541
2022-03-04 03:09:23 | INFO | fairseq.trainer | begin training epoch 58
2022-03-04 03:09:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:11:07 | INFO | train_inner | epoch 058:     40 / 393 loss=5.923, nll_loss=4.109, ppl=17.26, wps=24545.9, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=22400, lr=0.000211289, gnorm=0.756, loss_scale=16, train_wall=253, gb_free=12.3, wall=58645
2022-03-04 03:15:27 | INFO | train_inner | epoch 058:    140 / 393 loss=5.862, nll_loss=4.039, ppl=16.43, wps=25272.7, ups=0.39, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.749, loss_scale=16, train_wall=254, gb_free=12.3, wall=58904
2022-03-04 03:19:46 | INFO | train_inner | epoch 058:    240 / 393 loss=5.909, nll_loss=4.093, ppl=17.06, wps=25276.5, ups=0.39, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.744, loss_scale=16, train_wall=254, gb_free=12.3, wall=59163
2022-03-04 03:24:05 | INFO | train_inner | epoch 058:    340 / 393 loss=5.953, nll_loss=4.144, ppl=17.68, wps=25271.9, ups=0.39, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.758, loss_scale=16, train_wall=254, gb_free=12.3, wall=59422
2022-03-04 03:26:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:26:25 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 8.064 | nll_loss 6.484 | ppl 89.54 | wps 65352.8 | wpb 2034.1 | bsz 4 | num_updates 22753 | best_loss 7.638
2022-03-04 03:26:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 22753 updates
2022-03-04 03:26:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 03:26:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 03:26:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 58 @ 22753 updates, score 8.064) (writing took 4.2729866781737655 seconds)
2022-03-04 03:26:29 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-04 03:26:29 | INFO | train | epoch 058 | loss 5.908 | nll_loss 4.092 | ppl 17.06 | wps 25082.5 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 22753 | lr 0.000209643 | gnorm 0.75 | loss_scale 32 | train_wall 999 | gb_free 12.3 | wall 59566
2022-03-04 03:26:29 | INFO | fairseq.trainer | begin training epoch 59
2022-03-04 03:26:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:26:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 03:28:34 | INFO | train_inner | epoch 059:     48 / 393 loss=5.903, nll_loss=4.086, ppl=16.98, wps=24302.2, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=22800, lr=0.000209427, gnorm=0.743, loss_scale=16, train_wall=256, gb_free=12.3, wall=59691
2022-03-04 03:32:53 | INFO | train_inner | epoch 059:    148 / 393 loss=5.854, nll_loss=4.03, ppl=16.33, wps=25286.7, ups=0.39, wpb=65535.4, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.758, loss_scale=16, train_wall=254, gb_free=12.3, wall=59950
2022-03-04 03:37:12 | INFO | train_inner | epoch 059:    248 / 393 loss=5.904, nll_loss=4.087, ppl=17, wps=25279.2, ups=0.39, wpb=65536, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.757, loss_scale=16, train_wall=254, gb_free=12.3, wall=60209
2022-03-04 03:41:31 | INFO | train_inner | epoch 059:    348 / 393 loss=5.945, nll_loss=4.134, ppl=17.56, wps=25276.6, ups=0.39, wpb=65536, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.757, loss_scale=16, train_wall=254, gb_free=12.3, wall=60469
2022-03-04 03:43:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:43:30 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 8.075 | nll_loss 6.5 | ppl 90.49 | wps 65805.3 | wpb 2034.1 | bsz 4 | num_updates 23145 | best_loss 7.638
2022-03-04 03:43:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 23145 updates
2022-03-04 03:43:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 03:43:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 03:43:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 59 @ 23145 updates, score 8.075) (writing took 4.2708230160642415 seconds)
2022-03-04 03:43:35 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-04 03:43:35 | INFO | train | epoch 059 | loss 5.9 | nll_loss 4.082 | ppl 16.94 | wps 25024.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 23145 | lr 0.00020786 | gnorm 0.755 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 60592
2022-03-04 03:43:35 | INFO | fairseq.trainer | begin training epoch 60
2022-03-04 03:43:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:45:57 | INFO | train_inner | epoch 060:     55 / 393 loss=5.886, nll_loss=4.066, ppl=16.75, wps=24536.7, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=23200, lr=0.000207614, gnorm=0.756, loss_scale=16, train_wall=253, gb_free=12.3, wall=60734
2022-03-04 03:49:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 03:50:19 | INFO | train_inner | epoch 060:    156 / 393 loss=5.849, nll_loss=4.023, ppl=16.26, wps=25029.2, ups=0.38, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.749, loss_scale=16, train_wall=257, gb_free=12.3, wall=60996
2022-03-04 03:54:38 | INFO | train_inner | epoch 060:    256 / 393 loss=5.898, nll_loss=4.08, ppl=16.91, wps=25286.3, ups=0.39, wpb=65536, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.749, loss_scale=16, train_wall=254, gb_free=12.3, wall=61256
2022-03-04 03:58:57 | INFO | train_inner | epoch 060:    356 / 393 loss=5.945, nll_loss=4.134, ppl=17.56, wps=25283.6, ups=0.39, wpb=65530.2, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.761, loss_scale=16, train_wall=254, gb_free=12.3, wall=61515
2022-03-04 04:00:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:00:36 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 8.065 | nll_loss 6.49 | ppl 89.9 | wps 65704.7 | wpb 2034.1 | bsz 4 | num_updates 23537 | best_loss 7.638
2022-03-04 04:00:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 23537 updates
2022-03-04 04:00:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 04:00:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 04:00:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 60 @ 23537 updates, score 8.065) (writing took 4.282805237919092 seconds)
2022-03-04 04:00:40 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-04 04:00:40 | INFO | train | epoch 060 | loss 5.892 | nll_loss 4.073 | ppl 16.83 | wps 25026.7 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 23537 | lr 0.000206122 | gnorm 0.755 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 61617
2022-03-04 04:00:40 | INFO | fairseq.trainer | begin training epoch 61
2022-03-04 04:00:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:03:23 | INFO | train_inner | epoch 061:     63 / 393 loss=5.868, nll_loss=4.046, ppl=16.52, wps=24544.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=23600, lr=0.000205847, gnorm=0.766, loss_scale=16, train_wall=253, gb_free=12.3, wall=61781
2022-03-04 04:07:42 | INFO | train_inner | epoch 061:    163 / 393 loss=5.842, nll_loss=4.015, ppl=16.17, wps=25291.7, ups=0.39, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.747, loss_scale=16, train_wall=254, gb_free=12.3, wall=62040
2022-03-04 04:12:02 | INFO | train_inner | epoch 061:    263 / 393 loss=5.9, nll_loss=4.082, ppl=16.93, wps=25288.3, ups=0.39, wpb=65535.4, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.769, loss_scale=32, train_wall=254, gb_free=12.3, wall=62299
2022-03-04 04:12:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 04:16:24 | INFO | train_inner | epoch 061:    364 / 393 loss=5.936, nll_loss=4.124, ppl=17.43, wps=25018, ups=0.38, wpb=65530.9, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.764, loss_scale=16, train_wall=257, gb_free=12.3, wall=62561
2022-03-04 04:17:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:17:41 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 8.094 | nll_loss 6.519 | ppl 91.69 | wps 65635.5 | wpb 2034.1 | bsz 4 | num_updates 23929 | best_loss 7.638
2022-03-04 04:17:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 23929 updates
2022-03-04 04:17:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 04:17:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 04:17:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 61 @ 23929 updates, score 8.094) (writing took 4.242003894876689 seconds)
2022-03-04 04:17:45 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-04 04:17:45 | INFO | train | epoch 061 | loss 5.884 | nll_loss 4.064 | ppl 16.72 | wps 25026.2 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 23929 | lr 0.000204427 | gnorm 0.761 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 62642
2022-03-04 04:17:45 | INFO | fairseq.trainer | begin training epoch 62
2022-03-04 04:17:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:20:49 | INFO | train_inner | epoch 062:     71 / 393 loss=5.849, nll_loss=4.024, ppl=16.27, wps=24540.8, ups=0.38, wpb=65243.5, bsz=127.4, num_updates=24000, lr=0.000204124, gnorm=0.768, loss_scale=16, train_wall=253, gb_free=12.3, wall=62827
2022-03-04 04:25:09 | INFO | train_inner | epoch 062:    171 / 393 loss=5.84, nll_loss=4.012, ppl=16.14, wps=25283.2, ups=0.39, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.749, loss_scale=16, train_wall=254, gb_free=12.3, wall=63086
2022-03-04 04:29:28 | INFO | train_inner | epoch 062:    271 / 393 loss=5.888, nll_loss=4.068, ppl=16.78, wps=25273.9, ups=0.39, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.75, loss_scale=16, train_wall=254, gb_free=12.3, wall=63345
2022-03-04 04:33:47 | INFO | train_inner | epoch 062:    371 / 393 loss=5.936, nll_loss=4.123, ppl=17.43, wps=25276.4, ups=0.39, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.774, loss_scale=16, train_wall=254, gb_free=12.3, wall=63604
2022-03-04 04:34:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:34:46 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 8.072 | nll_loss 6.492 | ppl 90.04 | wps 65633.6 | wpb 2034.1 | bsz 4 | num_updates 24322 | best_loss 7.638
2022-03-04 04:34:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 24322 updates
2022-03-04 04:34:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 04:34:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 04:34:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 62 @ 24322 updates, score 8.072) (writing took 4.25085241580382 seconds)
2022-03-04 04:34:51 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-04 04:34:51 | INFO | train | epoch 062 | loss 5.876 | nll_loss 4.055 | ppl 16.62 | wps 25087.6 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 24322 | lr 0.000202768 | gnorm 0.76 | loss_scale 32 | train_wall 999 | gb_free 12.3 | wall 63668
2022-03-04 04:34:51 | INFO | fairseq.trainer | begin training epoch 63
2022-03-04 04:34:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:34:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 04:38:16 | INFO | train_inner | epoch 063:     79 / 393 loss=5.833, nll_loss=4.005, ppl=16.06, wps=24307.1, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=24400, lr=0.000202444, gnorm=0.768, loss_scale=16, train_wall=256, gb_free=12.3, wall=63873
2022-03-04 04:42:35 | INFO | train_inner | epoch 063:    179 / 393 loss=5.837, nll_loss=4.01, ppl=16.11, wps=25270.6, ups=0.39, wpb=65536, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.759, loss_scale=16, train_wall=254, gb_free=12.3, wall=64132
2022-03-04 04:46:54 | INFO | train_inner | epoch 063:    279 / 393 loss=5.891, nll_loss=4.071, ppl=16.81, wps=25284.7, ups=0.39, wpb=65535.4, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.771, loss_scale=16, train_wall=254, gb_free=12.3, wall=64391
2022-03-04 04:51:13 | INFO | train_inner | epoch 063:    379 / 393 loss=5.927, nll_loss=4.113, ppl=17.3, wps=25277.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.778, loss_scale=16, train_wall=254, gb_free=12.3, wall=64651
2022-03-04 04:51:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:51:52 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 8.125 | nll_loss 6.563 | ppl 94.55 | wps 65718.8 | wpb 2034.1 | bsz 4 | num_updates 24714 | best_loss 7.638
2022-03-04 04:51:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 24714 updates
2022-03-04 04:51:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 04:51:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 04:51:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 63 @ 24714 updates, score 8.125) (writing took 4.244560474064201 seconds)
2022-03-04 04:51:56 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-04 04:51:56 | INFO | train | epoch 063 | loss 5.869 | nll_loss 4.046 | ppl 16.52 | wps 25025 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 24714 | lr 0.000201154 | gnorm 0.769 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 64693
2022-03-04 04:51:56 | INFO | fairseq.trainer | begin training epoch 64
2022-03-04 04:51:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:55:39 | INFO | train_inner | epoch 064:     86 / 393 loss=5.814, nll_loss=3.982, ppl=15.8, wps=24543.4, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=24800, lr=0.000200805, gnorm=0.776, loss_scale=16, train_wall=253, gb_free=12.3, wall=64916
2022-03-04 04:57:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 05:00:01 | INFO | train_inner | epoch 064:    187 / 393 loss=5.827, nll_loss=3.997, ppl=15.97, wps=25024.4, ups=0.38, wpb=65535.4, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.761, loss_scale=16, train_wall=257, gb_free=12.3, wall=65178
2022-03-04 05:04:20 | INFO | train_inner | epoch 064:    287 / 393 loss=5.885, nll_loss=4.064, ppl=16.73, wps=25287.1, ups=0.39, wpb=65536, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.778, loss_scale=16, train_wall=254, gb_free=12.3, wall=65437
2022-03-04 05:08:39 | INFO | train_inner | epoch 064:    387 / 393 loss=5.928, nll_loss=4.115, ppl=17.32, wps=25286, ups=0.39, wpb=65536, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.784, loss_scale=16, train_wall=254, gb_free=12.3, wall=65697
2022-03-04 05:08:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:08:57 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 8.12 | nll_loss 6.542 | ppl 93.21 | wps 65922.6 | wpb 2034.1 | bsz 4 | num_updates 25106 | best_loss 7.638
2022-03-04 05:08:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 25106 updates
2022-03-04 05:08:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 05:09:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 05:09:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 64 @ 25106 updates, score 8.12) (writing took 4.190624383976683 seconds)
2022-03-04 05:09:01 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-04 05:09:01 | INFO | train | epoch 064 | loss 5.862 | nll_loss 4.038 | ppl 16.43 | wps 25026.7 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 25106 | lr 0.000199577 | gnorm 0.774 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 65719
2022-03-04 05:09:02 | INFO | fairseq.trainer | begin training epoch 65
2022-03-04 05:09:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:13:05 | INFO | train_inner | epoch 065:     94 / 393 loss=5.791, nll_loss=3.955, ppl=15.51, wps=24559.1, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=25200, lr=0.000199205, gnorm=0.764, loss_scale=16, train_wall=253, gb_free=12.3, wall=65962
2022-03-04 05:17:24 | INFO | train_inner | epoch 065:    194 / 393 loss=5.841, nll_loss=4.013, ppl=16.15, wps=25285, ups=0.39, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.772, loss_scale=16, train_wall=254, gb_free=12.3, wall=66222
2022-03-04 05:19:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 05:21:46 | INFO | train_inner | epoch 065:    295 / 393 loss=5.875, nll_loss=4.053, ppl=16.6, wps=25036.1, ups=0.38, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.782, loss_scale=16, train_wall=257, gb_free=12.3, wall=66483
2022-03-04 05:25:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:26:02 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 8.096 | nll_loss 6.515 | ppl 91.48 | wps 65881.5 | wpb 2034.1 | bsz 4 | num_updates 25498 | best_loss 7.638
2022-03-04 05:26:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 25498 updates
2022-03-04 05:26:02 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 05:26:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 05:26:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 65 @ 25498 updates, score 8.096) (writing took 4.158451901981607 seconds)
2022-03-04 05:26:07 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-04 05:26:07 | INFO | train | epoch 065 | loss 5.855 | nll_loss 4.03 | ppl 16.34 | wps 25031.1 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 25498 | lr 0.000198037 | gnorm 0.775 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 66744
2022-03-04 05:26:07 | INFO | fairseq.trainer | begin training epoch 66
2022-03-04 05:26:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:26:12 | INFO | train_inner | epoch 066:      2 / 393 loss=5.917, nll_loss=4.102, ppl=17.17, wps=24543.1, ups=0.38, wpb=65243.5, bsz=127.4, num_updates=25500, lr=0.00019803, gnorm=0.782, loss_scale=16, train_wall=253, gb_free=12.3, wall=66749
2022-03-04 05:30:31 | INFO | train_inner | epoch 066:    102 / 393 loss=5.783, nll_loss=3.946, ppl=15.41, wps=25271.6, ups=0.39, wpb=65530.9, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.776, loss_scale=16, train_wall=254, gb_free=12.3, wall=67008
2022-03-04 05:34:50 | INFO | train_inner | epoch 066:    202 / 393 loss=5.827, nll_loss=3.997, ppl=15.97, wps=25281.5, ups=0.39, wpb=65536, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.775, loss_scale=16, train_wall=254, gb_free=12.3, wall=67268
2022-03-04 05:39:10 | INFO | train_inner | epoch 066:    302 / 393 loss=5.879, nll_loss=4.057, ppl=16.64, wps=25275, ups=0.39, wpb=65535.4, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.783, loss_scale=16, train_wall=254, gb_free=12.3, wall=67527
2022-03-04 05:42:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 05:43:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:43:08 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 8.117 | nll_loss 6.55 | ppl 93.71 | wps 65838.1 | wpb 2034.1 | bsz 4 | num_updates 25890 | best_loss 7.638
2022-03-04 05:43:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 25890 updates
2022-03-04 05:43:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 05:43:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 05:43:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 66 @ 25890 updates, score 8.117) (writing took 4.168092367006466 seconds)
2022-03-04 05:43:12 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-04 05:43:12 | INFO | train | epoch 066 | loss 5.849 | nll_loss 4.023 | ppl 16.25 | wps 25022.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 25890 | lr 0.000196532 | gnorm 0.779 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 67769
2022-03-04 05:43:12 | INFO | fairseq.trainer | begin training epoch 67
2022-03-04 05:43:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:43:38 | INFO | train_inner | epoch 067:     10 / 393 loss=5.903, nll_loss=4.085, ppl=16.97, wps=24309, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=25900, lr=0.000196494, gnorm=0.783, loss_scale=16, train_wall=256, gb_free=12.3, wall=67795
2022-03-04 05:47:57 | INFO | train_inner | epoch 067:    110 / 393 loss=5.783, nll_loss=3.946, ppl=15.42, wps=25283.8, ups=0.39, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.775, loss_scale=16, train_wall=254, gb_free=12.3, wall=68055
2022-03-04 05:52:17 | INFO | train_inner | epoch 067:    210 / 393 loss=5.819, nll_loss=3.988, ppl=15.87, wps=25280.5, ups=0.39, wpb=65530.2, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.777, loss_scale=16, train_wall=254, gb_free=12.3, wall=68314
2022-03-04 05:56:36 | INFO | train_inner | epoch 067:    310 / 393 loss=5.873, nll_loss=4.051, ppl=16.57, wps=25272.8, ups=0.39, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.778, loss_scale=16, train_wall=254, gb_free=12.3, wall=68573
2022-03-04 06:00:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:00:13 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 8.116 | nll_loss 6.537 | ppl 92.83 | wps 66078.3 | wpb 2034.1 | bsz 4 | num_updates 26283 | best_loss 7.638
2022-03-04 06:00:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 26283 updates
2022-03-04 06:00:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 06:00:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 06:00:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 67 @ 26283 updates, score 8.116) (writing took 4.194769531022757 seconds)
2022-03-04 06:00:18 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-04 06:00:18 | INFO | train | epoch 067 | loss 5.842 | nll_loss 4.015 | ppl 16.17 | wps 25090.5 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 26283 | lr 0.000195057 | gnorm 0.78 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 68795
2022-03-04 06:00:18 | INFO | fairseq.trainer | begin training epoch 68
2022-03-04 06:00:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:01:02 | INFO | train_inner | epoch 068:     17 / 393 loss=5.884, nll_loss=4.063, ppl=16.71, wps=24552, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=26300, lr=0.000194994, gnorm=0.794, loss_scale=16, train_wall=253, gb_free=12.3, wall=68839
2022-03-04 06:04:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 06:05:24 | INFO | train_inner | epoch 068:    118 / 393 loss=5.781, nll_loss=3.943, ppl=15.38, wps=25025.4, ups=0.38, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.769, loss_scale=16, train_wall=257, gb_free=12.3, wall=69101
2022-03-04 06:09:43 | INFO | train_inner | epoch 068:    218 / 393 loss=5.826, nll_loss=3.996, ppl=15.96, wps=25283.1, ups=0.39, wpb=65536, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.771, loss_scale=16, train_wall=254, gb_free=12.3, wall=69360
2022-03-04 06:14:02 | INFO | train_inner | epoch 068:    318 / 393 loss=5.868, nll_loss=4.044, ppl=16.5, wps=25283.4, ups=0.39, wpb=65530.9, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.793, loss_scale=16, train_wall=254, gb_free=12.3, wall=69619
2022-03-04 06:17:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:17:19 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 8.14 | nll_loss 6.564 | ppl 94.65 | wps 65670.6 | wpb 2034.1 | bsz 4 | num_updates 26675 | best_loss 7.638
2022-03-04 06:17:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 26675 updates
2022-03-04 06:17:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 06:17:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 06:17:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 68 @ 26675 updates, score 8.14) (writing took 4.182542114984244 seconds)
2022-03-04 06:17:23 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-04 06:17:23 | INFO | train | epoch 068 | loss 5.836 | nll_loss 4.007 | ppl 16.08 | wps 25027.4 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 26675 | lr 0.000193619 | gnorm 0.779 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 69820
2022-03-04 06:17:23 | INFO | fairseq.trainer | begin training epoch 69
2022-03-04 06:17:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:18:28 | INFO | train_inner | epoch 069:     25 / 393 loss=5.862, nll_loss=4.037, ppl=16.42, wps=24553.7, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=26700, lr=0.000193528, gnorm=0.778, loss_scale=16, train_wall=253, gb_free=12.3, wall=69885
2022-03-04 06:22:47 | INFO | train_inner | epoch 069:    125 / 393 loss=5.772, nll_loss=3.933, ppl=15.28, wps=25276.2, ups=0.39, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.785, loss_scale=16, train_wall=254, gb_free=12.3, wall=70144
2022-03-04 06:27:06 | INFO | train_inner | epoch 069:    225 / 393 loss=5.822, nll_loss=3.991, ppl=15.9, wps=25274.5, ups=0.39, wpb=65536, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.781, loss_scale=32, train_wall=254, gb_free=12.3, wall=70403
2022-03-04 06:27:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 06:31:28 | INFO | train_inner | epoch 069:    326 / 393 loss=5.865, nll_loss=4.041, ppl=16.46, wps=25037.6, ups=0.38, wpb=65530.9, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.794, loss_scale=16, train_wall=257, gb_free=12.3, wall=70665
2022-03-04 06:34:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:34:24 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 8.129 | nll_loss 6.554 | ppl 93.98 | wps 66024.5 | wpb 2034.1 | bsz 4 | num_updates 27067 | best_loss 7.638
2022-03-04 06:34:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 27067 updates
2022-03-04 06:34:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 06:34:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 06:34:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 69 @ 27067 updates, score 8.129) (writing took 4.25930185103789 seconds)
2022-03-04 06:34:28 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-04 06:34:28 | INFO | train | epoch 069 | loss 5.83 | nll_loss 4 | ppl 16 | wps 25028.1 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 27067 | lr 0.000192212 | gnorm 0.786 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 70845
2022-03-04 06:34:28 | INFO | fairseq.trainer | begin training epoch 70
2022-03-04 06:34:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:35:54 | INFO | train_inner | epoch 070:     33 / 393 loss=5.853, nll_loss=4.027, ppl=16.3, wps=24554, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=27100, lr=0.000192095, gnorm=0.791, loss_scale=16, train_wall=253, gb_free=12.3, wall=70931
2022-03-04 06:40:13 | INFO | train_inner | epoch 070:    133 / 393 loss=5.774, nll_loss=3.935, ppl=15.3, wps=25285.5, ups=0.39, wpb=65536, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.769, loss_scale=16, train_wall=254, gb_free=12.3, wall=71190
2022-03-04 06:44:32 | INFO | train_inner | epoch 070:    233 / 393 loss=5.817, nll_loss=3.986, ppl=15.84, wps=25285.4, ups=0.39, wpb=65530.2, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.778, loss_scale=16, train_wall=254, gb_free=12.3, wall=71449
2022-03-04 06:48:51 | INFO | train_inner | epoch 070:    333 / 393 loss=5.864, nll_loss=4.04, ppl=16.45, wps=25278.5, ups=0.39, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.783, loss_scale=16, train_wall=254, gb_free=12.3, wall=71709
2022-03-04 06:49:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 06:51:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:51:29 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 8.148 | nll_loss 6.574 | ppl 95.26 | wps 65486.7 | wpb 2034.1 | bsz 4 | num_updates 27459 | best_loss 7.638
2022-03-04 06:51:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 27459 updates
2022-03-04 06:51:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 06:51:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 06:51:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 70 @ 27459 updates, score 8.148) (writing took 4.192231717985123 seconds)
2022-03-04 06:51:33 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-04 06:51:33 | INFO | train | epoch 070 | loss 5.825 | nll_loss 3.994 | ppl 15.94 | wps 25030.2 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 27459 | lr 0.000190835 | gnorm 0.783 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 71871
2022-03-04 06:51:33 | INFO | fairseq.trainer | begin training epoch 71
2022-03-04 06:51:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:53:20 | INFO | train_inner | epoch 071:     41 / 393 loss=5.837, nll_loss=4.008, ppl=16.09, wps=24310.4, ups=0.37, wpb=65243.5, bsz=127.4, num_updates=27500, lr=0.000190693, gnorm=0.8, loss_scale=16, train_wall=256, gb_free=12.3, wall=71977
2022-03-04 06:57:39 | INFO | train_inner | epoch 071:    141 / 393 loss=5.769, nll_loss=3.929, ppl=15.23, wps=25281.9, ups=0.39, wpb=65536, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.792, loss_scale=16, train_wall=254, gb_free=12.3, wall=72236
2022-03-04 07:01:58 | INFO | train_inner | epoch 071:    241 / 393 loss=5.817, nll_loss=3.985, ppl=15.83, wps=25278.4, ups=0.39, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.786, loss_scale=16, train_wall=254, gb_free=12.3, wall=72495
2022-03-04 07:06:17 | INFO | train_inner | epoch 071:    341 / 393 loss=5.863, nll_loss=4.038, ppl=16.43, wps=25276.1, ups=0.39, wpb=65536, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.791, loss_scale=16, train_wall=254, gb_free=12.3, wall=72755
2022-03-04 07:08:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:08:34 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 8.156 | nll_loss 6.581 | ppl 95.72 | wps 65469.6 | wpb 2034.1 | bsz 4 | num_updates 27852 | best_loss 7.638
2022-03-04 07:08:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 27852 updates
2022-03-04 07:08:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 07:08:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 07:08:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 71 @ 27852 updates, score 8.156) (writing took 4.176568441092968 seconds)
2022-03-04 07:08:39 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-04 07:08:39 | INFO | train | epoch 071 | loss 5.819 | nll_loss 3.987 | ppl 15.86 | wps 25089.2 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 27852 | lr 0.000189484 | gnorm 0.791 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 72896
2022-03-04 07:08:39 | INFO | fairseq.trainer | begin training epoch 72
2022-03-04 07:08:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:10:43 | INFO | train_inner | epoch 072:     48 / 393 loss=5.816, nll_loss=3.984, ppl=15.82, wps=24550.8, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=27900, lr=0.000189321, gnorm=0.8, loss_scale=16, train_wall=253, gb_free=12.3, wall=73020
2022-03-04 07:12:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 07:15:05 | INFO | train_inner | epoch 072:    149 / 393 loss=5.775, nll_loss=3.936, ppl=15.3, wps=25040, ups=0.38, wpb=65536, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.788, loss_scale=16, train_wall=257, gb_free=12.3, wall=73282
2022-03-04 07:19:24 | INFO | train_inner | epoch 072:    249 / 393 loss=5.819, nll_loss=3.987, ppl=15.85, wps=25292.4, ups=0.39, wpb=65530.2, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.785, loss_scale=16, train_wall=254, gb_free=12.3, wall=73541
2022-03-04 07:23:43 | INFO | train_inner | epoch 072:    349 / 393 loss=5.856, nll_loss=4.031, ppl=16.34, wps=25296.7, ups=0.39, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.798, loss_scale=16, train_wall=254, gb_free=12.3, wall=73800
2022-03-04 07:25:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:25:39 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 8.151 | nll_loss 6.579 | ppl 95.6 | wps 66176.4 | wpb 2034.1 | bsz 4 | num_updates 28244 | best_loss 7.638
2022-03-04 07:25:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 28244 updates
2022-03-04 07:25:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 07:25:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 07:25:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 72 @ 28244 updates, score 8.151) (writing took 4.274513341020793 seconds)
2022-03-04 07:25:44 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-04 07:25:44 | INFO | train | epoch 072 | loss 5.813 | nll_loss 3.981 | ppl 15.79 | wps 25036.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 28244 | lr 0.000188164 | gnorm 0.792 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 73921
2022-03-04 07:25:44 | INFO | fairseq.trainer | begin training epoch 73
2022-03-04 07:25:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:28:09 | INFO | train_inner | epoch 073:     56 / 393 loss=5.795, nll_loss=3.96, ppl=15.56, wps=24556.1, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=28300, lr=0.000187978, gnorm=0.796, loss_scale=16, train_wall=253, gb_free=12.3, wall=74066
2022-03-04 07:32:28 | INFO | train_inner | epoch 073:    156 / 393 loss=5.772, nll_loss=3.932, ppl=15.27, wps=25264.8, ups=0.39, wpb=65535.4, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.802, loss_scale=16, train_wall=254, gb_free=12.3, wall=74325
2022-03-04 07:35:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 07:36:50 | INFO | train_inner | epoch 073:    257 / 393 loss=5.812, nll_loss=3.979, ppl=15.77, wps=25030.2, ups=0.38, wpb=65530.9, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.793, loss_scale=16, train_wall=257, gb_free=12.3, wall=74587
2022-03-04 07:41:09 | INFO | train_inner | epoch 073:    357 / 393 loss=5.861, nll_loss=4.036, ppl=16.4, wps=25291.6, ups=0.39, wpb=65536, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.82, loss_scale=16, train_wall=254, gb_free=12.3, wall=74846
2022-03-04 07:42:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:42:45 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 8.144 | nll_loss 6.562 | ppl 94.5 | wps 65709.7 | wpb 2034.1 | bsz 4 | num_updates 28636 | best_loss 7.638
2022-03-04 07:42:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 28636 updates
2022-03-04 07:42:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 07:42:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 07:42:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 73 @ 28636 updates, score 8.144) (writing took 4.257251180009916 seconds)
2022-03-04 07:42:49 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-04 07:42:49 | INFO | train | epoch 073 | loss 5.809 | nll_loss 3.975 | ppl 15.73 | wps 25027.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 28636 | lr 0.000186872 | gnorm 0.805 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 74946
2022-03-04 07:42:49 | INFO | fairseq.trainer | begin training epoch 74
2022-03-04 07:42:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:45:35 | INFO | train_inner | epoch 074:     64 / 393 loss=5.788, nll_loss=3.951, ppl=15.46, wps=24561.4, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=28700, lr=0.000186663, gnorm=0.803, loss_scale=16, train_wall=253, gb_free=12.3, wall=75112
2022-03-04 07:49:54 | INFO | train_inner | epoch 074:    164 / 393 loss=5.757, nll_loss=3.915, ppl=15.08, wps=25289.1, ups=0.39, wpb=65530.9, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.813, loss_scale=16, train_wall=254, gb_free=12.3, wall=75371
2022-03-04 07:54:13 | INFO | train_inner | epoch 074:    264 / 393 loss=5.81, nll_loss=3.976, ppl=15.74, wps=25281, ups=0.39, wpb=65535.4, bsz=128, num_updates=28900, lr=0.000186016, gnorm=0.787, loss_scale=16, train_wall=254, gb_free=12.3, wall=75630
2022-03-04 07:57:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 07:58:35 | INFO | train_inner | epoch 074:    365 / 393 loss=5.86, nll_loss=4.035, ppl=16.39, wps=25050.8, ups=0.38, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=0.809, loss_scale=16, train_wall=257, gb_free=12.3, wall=75892
2022-03-04 07:59:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:59:50 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 8.171 | nll_loss 6.6 | ppl 97.02 | wps 66094.5 | wpb 2034.1 | bsz 4 | num_updates 29028 | best_loss 7.638
2022-03-04 07:59:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 29028 updates
2022-03-04 07:59:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 07:59:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 07:59:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 74 @ 29028 updates, score 8.171) (writing took 4.258108680834994 seconds)
2022-03-04 07:59:54 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-04 07:59:54 | INFO | train | epoch 074 | loss 5.803 | nll_loss 3.968 | ppl 15.65 | wps 25038.8 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 29028 | lr 0.000185606 | gnorm 0.802 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 75971
2022-03-04 07:59:54 | INFO | fairseq.trainer | begin training epoch 75
2022-03-04 07:59:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:03:01 | INFO | train_inner | epoch 075:     72 / 393 loss=5.775, nll_loss=3.936, ppl=15.31, wps=24556.4, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=29100, lr=0.000185376, gnorm=0.8, loss_scale=16, train_wall=253, gb_free=12.3, wall=76158
2022-03-04 08:07:20 | INFO | train_inner | epoch 075:    172 / 393 loss=5.766, nll_loss=3.925, ppl=15.19, wps=25288.6, ups=0.39, wpb=65530.2, bsz=128, num_updates=29200, lr=0.000185058, gnorm=0.798, loss_scale=16, train_wall=254, gb_free=12.3, wall=76417
2022-03-04 08:11:39 | INFO | train_inner | epoch 075:    272 / 393 loss=5.812, nll_loss=3.979, ppl=15.77, wps=25283.8, ups=0.39, wpb=65536, bsz=128, num_updates=29300, lr=0.000184742, gnorm=0.802, loss_scale=16, train_wall=254, gb_free=12.3, wall=76676
2022-03-04 08:15:58 | INFO | train_inner | epoch 075:    372 / 393 loss=5.855, nll_loss=4.028, ppl=16.32, wps=25298.8, ups=0.39, wpb=65536, bsz=128, num_updates=29400, lr=0.000184428, gnorm=0.81, loss_scale=16, train_wall=254, gb_free=12.3, wall=76935
2022-03-04 08:16:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:16:55 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 8.173 | nll_loss 6.605 | ppl 97.33 | wps 65567.9 | wpb 2034.1 | bsz 4 | num_updates 29421 | best_loss 7.638
2022-03-04 08:16:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 29421 updates
2022-03-04 08:16:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 08:16:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 08:16:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 75 @ 29421 updates, score 8.173) (writing took 4.2868558978661895 seconds)
2022-03-04 08:16:59 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-04 08:16:59 | INFO | train | epoch 075 | loss 5.798 | nll_loss 3.963 | ppl 15.59 | wps 25097.1 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 29421 | lr 0.000184362 | gnorm 0.803 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 76996
2022-03-04 08:16:59 | INFO | fairseq.trainer | begin training epoch 76
2022-03-04 08:16:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:20:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 08:20:26 | INFO | train_inner | epoch 076:     80 / 393 loss=5.752, nll_loss=3.91, ppl=15.03, wps=24320, ups=0.37, wpb=65243.5, bsz=127.4, num_updates=29500, lr=0.000184115, gnorm=0.809, loss_scale=16, train_wall=256, gb_free=12.3, wall=77203
2022-03-04 08:24:45 | INFO | train_inner | epoch 076:    180 / 393 loss=5.767, nll_loss=3.926, ppl=15.2, wps=25285.1, ups=0.39, wpb=65536, bsz=128, num_updates=29600, lr=0.000183804, gnorm=0.797, loss_scale=16, train_wall=254, gb_free=12.3, wall=77463
2022-03-04 08:29:04 | INFO | train_inner | epoch 076:    280 / 393 loss=5.807, nll_loss=3.973, ppl=15.7, wps=25306.7, ups=0.39, wpb=65536, bsz=128, num_updates=29700, lr=0.000183494, gnorm=0.814, loss_scale=16, train_wall=254, gb_free=12.3, wall=77722
2022-03-04 08:31:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 08:33:26 | INFO | train_inner | epoch 076:    381 / 393 loss=5.849, nll_loss=4.022, ppl=16.24, wps=25052.3, ups=0.38, wpb=65536, bsz=128, num_updates=29800, lr=0.000183186, gnorm=0.819, loss_scale=8, train_wall=257, gb_free=12.3, wall=77983
2022-03-04 08:33:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:33:59 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 8.163 | nll_loss 6.597 | ppl 96.84 | wps 65755.4 | wpb 2034.1 | bsz 4 | num_updates 29812 | best_loss 7.638
2022-03-04 08:33:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 29812 updates
2022-03-04 08:33:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 08:34:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 08:34:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 76 @ 29812 updates, score 8.163) (writing took 4.293486987007782 seconds)
2022-03-04 08:34:04 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-04 08:34:04 | INFO | train | epoch 076 | loss 5.793 | nll_loss 3.956 | ppl 15.52 | wps 24979.8 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 29812 | lr 0.000183149 | gnorm 0.81 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 78021
2022-03-04 08:34:04 | INFO | fairseq.trainer | begin training epoch 77
2022-03-04 08:34:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:37:52 | INFO | train_inner | epoch 077:     88 / 393 loss=5.732, nll_loss=3.887, ppl=14.79, wps=24565.3, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=29900, lr=0.000182879, gnorm=0.807, loss_scale=8, train_wall=253, gb_free=12.3, wall=78249
2022-03-04 08:42:11 | INFO | train_inner | epoch 077:    188 / 393 loss=5.767, nll_loss=3.926, ppl=15.2, wps=25302.7, ups=0.39, wpb=65536, bsz=128, num_updates=30000, lr=0.000182574, gnorm=0.824, loss_scale=8, train_wall=254, gb_free=12.3, wall=78508
2022-03-04 08:46:30 | INFO | train_inner | epoch 077:    288 / 393 loss=5.802, nll_loss=3.968, ppl=15.65, wps=25264.1, ups=0.39, wpb=65530.9, bsz=128, num_updates=30100, lr=0.000182271, gnorm=0.81, loss_scale=8, train_wall=254, gb_free=12.3, wall=78767
2022-03-04 08:50:49 | INFO | train_inner | epoch 077:    388 / 393 loss=5.859, nll_loss=4.033, ppl=16.37, wps=25263.9, ups=0.39, wpb=65535.4, bsz=128, num_updates=30200, lr=0.000181969, gnorm=0.83, loss_scale=8, train_wall=254, gb_free=12.3, wall=79027
2022-03-04 08:51:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:51:05 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 8.186 | nll_loss 6.609 | ppl 97.62 | wps 65383.7 | wpb 2034.1 | bsz 4 | num_updates 30205 | best_loss 7.638
2022-03-04 08:51:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 30205 updates
2022-03-04 08:51:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 08:51:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 08:51:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 77 @ 30205 updates, score 8.186) (writing took 4.413663073908538 seconds)
2022-03-04 08:51:09 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-04 08:51:09 | INFO | train | epoch 077 | loss 5.788 | nll_loss 3.951 | ppl 15.47 | wps 25087.5 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 30205 | lr 0.000181954 | gnorm 0.818 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 79046
2022-03-04 08:51:09 | INFO | fairseq.trainer | begin training epoch 78
2022-03-04 08:51:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:55:16 | INFO | train_inner | epoch 078:     95 / 393 loss=5.725, nll_loss=3.878, ppl=14.7, wps=24440.3, ups=0.37, wpb=65248.6, bsz=127.4, num_updates=30300, lr=0.000181668, gnorm=0.809, loss_scale=16, train_wall=254, gb_free=12.3, wall=79294
2022-03-04 08:59:37 | INFO | train_inner | epoch 078:    195 / 393 loss=5.765, nll_loss=3.924, ppl=15.18, wps=25141.9, ups=0.38, wpb=65536, bsz=128, num_updates=30400, lr=0.000181369, gnorm=0.802, loss_scale=16, train_wall=256, gb_free=12.3, wall=79554
2022-03-04 09:03:58 | INFO | train_inner | epoch 078:    295 / 393 loss=5.808, nll_loss=3.974, ppl=15.71, wps=25118.8, ups=0.38, wpb=65530.9, bsz=128, num_updates=30500, lr=0.000181071, gnorm=0.806, loss_scale=16, train_wall=256, gb_free=12.3, wall=79815
2022-03-04 09:08:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:08:16 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 8.178 | nll_loss 6.604 | ppl 97.27 | wps 64366.5 | wpb 2034.1 | bsz 4 | num_updates 30598 | best_loss 7.638
2022-03-04 09:08:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 30598 updates
2022-03-04 09:08:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 09:08:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 09:08:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 78 @ 30598 updates, score 8.178) (writing took 4.884649993851781 seconds)
2022-03-04 09:08:20 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-04 09:08:20 | INFO | train | epoch 078 | loss 5.784 | nll_loss 3.946 | ppl 15.41 | wps 24941.6 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 30598 | lr 0.000180781 | gnorm 0.811 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 80078
2022-03-04 09:08:20 | INFO | fairseq.trainer | begin training epoch 79
2022-03-04 09:08:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:08:26 | INFO | train_inner | epoch 079:      2 / 393 loss=5.84, nll_loss=4.011, ppl=16.12, wps=24355.9, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=30600, lr=0.000180775, gnorm=0.828, loss_scale=16, train_wall=254, gb_free=12.3, wall=80083
2022-03-04 09:12:46 | INFO | train_inner | epoch 079:    102 / 393 loss=5.717, nll_loss=3.869, ppl=14.61, wps=25176.2, ups=0.38, wpb=65536, bsz=128, num_updates=30700, lr=0.000180481, gnorm=0.807, loss_scale=16, train_wall=255, gb_free=12.3, wall=80343
2022-03-04 09:16:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 09:17:09 | INFO | train_inner | epoch 079:    203 / 393 loss=5.76, nll_loss=3.919, ppl=15.12, wps=24912.2, ups=0.38, wpb=65536, bsz=128, num_updates=30800, lr=0.000180187, gnorm=0.807, loss_scale=16, train_wall=258, gb_free=12.3, wall=80606
2022-03-04 09:21:30 | INFO | train_inner | epoch 079:    303 / 393 loss=5.796, nll_loss=3.96, ppl=15.57, wps=25155.3, ups=0.38, wpb=65536, bsz=128, num_updates=30900, lr=0.000179896, gnorm=0.813, loss_scale=16, train_wall=255, gb_free=12.3, wall=80867
2022-03-04 09:25:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:25:26 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 8.198 | nll_loss 6.63 | ppl 99.05 | wps 64122.8 | wpb 2034.1 | bsz 4 | num_updates 30990 | best_loss 7.638
2022-03-04 09:25:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 30990 updates
2022-03-04 09:25:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 09:25:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 09:25:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 79 @ 30990 updates, score 8.198) (writing took 4.659689150052145 seconds)
2022-03-04 09:25:31 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-04 09:25:31 | INFO | train | epoch 079 | loss 5.78 | nll_loss 3.941 | ppl 15.36 | wps 24897.9 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 30990 | lr 0.000179634 | gnorm 0.813 | loss_scale 16 | train_wall 1002 | gb_free 12.3 | wall 81108
2022-03-04 09:25:31 | INFO | fairseq.trainer | begin training epoch 80
2022-03-04 09:25:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:25:57 | INFO | train_inner | epoch 080:     10 / 393 loss=5.836, nll_loss=4.007, ppl=16.07, wps=24383.1, ups=0.37, wpb=65243.5, bsz=127.4, num_updates=31000, lr=0.000179605, gnorm=0.824, loss_scale=16, train_wall=254, gb_free=12.3, wall=81134
2022-03-04 09:30:18 | INFO | train_inner | epoch 080:    110 / 393 loss=5.712, nll_loss=3.863, ppl=14.55, wps=25164, ups=0.38, wpb=65536, bsz=128, num_updates=31100, lr=0.000179316, gnorm=0.805, loss_scale=16, train_wall=255, gb_free=12.3, wall=81395
2022-03-04 09:31:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:34:41 | INFO | train_inner | epoch 080:    211 / 393 loss=5.752, nll_loss=3.909, ppl=15.03, wps=24926.8, ups=0.38, wpb=65535.4, bsz=128, num_updates=31200, lr=0.000179029, gnorm=0.833, loss_scale=8, train_wall=258, gb_free=12.3, wall=81658
2022-03-04 09:39:01 | INFO | train_inner | epoch 080:    311 / 393 loss=5.81, nll_loss=3.976, ppl=15.74, wps=25173.4, ups=0.38, wpb=65536, bsz=128, num_updates=31300, lr=0.000178743, gnorm=0.818, loss_scale=8, train_wall=255, gb_free=12.3, wall=81918
2022-03-04 09:42:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:42:37 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 8.19 | nll_loss 6.618 | ppl 98.24 | wps 64463.5 | wpb 2034.1 | bsz 4 | num_updates 31382 | best_loss 7.638
2022-03-04 09:42:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 31382 updates
2022-03-04 09:42:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 09:42:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 09:42:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 80 @ 31382 updates, score 8.19) (writing took 4.441654478898272 seconds)
2022-03-04 09:42:41 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-04 09:42:41 | INFO | train | epoch 080 | loss 5.775 | nll_loss 3.936 | ppl 15.3 | wps 24912.1 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 31382 | lr 0.000178509 | gnorm 0.82 | loss_scale 8 | train_wall 1002 | gb_free 12.3 | wall 82138
2022-03-04 09:42:41 | INFO | fairseq.trainer | begin training epoch 81
2022-03-04 09:42:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:43:28 | INFO | train_inner | epoch 081:     18 / 393 loss=5.82, nll_loss=3.988, ppl=15.87, wps=24416.5, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=31400, lr=0.000178458, gnorm=0.824, loss_scale=8, train_wall=254, gb_free=12.3, wall=82185
2022-03-04 09:47:48 | INFO | train_inner | epoch 081:    118 / 393 loss=5.711, nll_loss=3.862, ppl=14.54, wps=25173.3, ups=0.38, wpb=65536, bsz=128, num_updates=31500, lr=0.000178174, gnorm=0.808, loss_scale=8, train_wall=255, gb_free=12.3, wall=82446
2022-03-04 09:52:09 | INFO | train_inner | epoch 081:    218 / 393 loss=5.762, nll_loss=3.92, ppl=15.14, wps=25174.5, ups=0.38, wpb=65536, bsz=128, num_updates=31600, lr=0.000177892, gnorm=0.807, loss_scale=8, train_wall=255, gb_free=12.3, wall=82706
2022-03-04 09:56:29 | INFO | train_inner | epoch 081:    318 / 393 loss=5.796, nll_loss=3.961, ppl=15.57, wps=25158.5, ups=0.38, wpb=65536, bsz=128, num_updates=31700, lr=0.000177611, gnorm=0.819, loss_scale=16, train_wall=255, gb_free=12.3, wall=82966
2022-03-04 09:59:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:59:47 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 8.204 | nll_loss 6.633 | ppl 99.24 | wps 64912.6 | wpb 2034.1 | bsz 4 | num_updates 31775 | best_loss 7.638
2022-03-04 09:59:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 31775 updates
2022-03-04 09:59:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 09:59:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 09:59:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 81 @ 31775 updates, score 8.204) (writing took 4.347960814135149 seconds)
2022-03-04 09:59:51 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-04 09:59:51 | INFO | train | epoch 081 | loss 5.771 | nll_loss 3.931 | ppl 15.25 | wps 24974.2 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 31775 | lr 0.000177401 | gnorm 0.816 | loss_scale 16 | train_wall 1002 | gb_free 12.3 | wall 83168
2022-03-04 09:59:51 | INFO | fairseq.trainer | begin training epoch 82
2022-03-04 09:59:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:00:56 | INFO | train_inner | epoch 082:     25 / 393 loss=5.809, nll_loss=3.976, ppl=15.73, wps=24419.7, ups=0.37, wpb=65243.5, bsz=127.4, num_updates=31800, lr=0.000177332, gnorm=0.826, loss_scale=16, train_wall=254, gb_free=12.3, wall=83234
2022-03-04 10:05:17 | INFO | train_inner | epoch 082:    125 / 393 loss=5.713, nll_loss=3.864, ppl=14.56, wps=25162.9, ups=0.38, wpb=65530.9, bsz=128, num_updates=31900, lr=0.000177054, gnorm=0.816, loss_scale=16, train_wall=255, gb_free=12.3, wall=83494
2022-03-04 10:09:37 | INFO | train_inner | epoch 082:    225 / 393 loss=5.754, nll_loss=3.911, ppl=15.04, wps=25165.1, ups=0.38, wpb=65535.4, bsz=128, num_updates=32000, lr=0.000176777, gnorm=0.819, loss_scale=16, train_wall=255, gb_free=12.3, wall=83755
2022-03-04 10:13:58 | INFO | train_inner | epoch 082:    325 / 393 loss=5.802, nll_loss=3.967, ppl=15.64, wps=25152.6, ups=0.38, wpb=65536, bsz=128, num_updates=32100, lr=0.000176501, gnorm=0.836, loss_scale=16, train_wall=255, gb_free=12.3, wall=84015
2022-03-04 10:16:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 10:16:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:16:57 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 8.215 | nll_loss 6.645 | ppl 100.08 | wps 63751 | wpb 2034.1 | bsz 4 | num_updates 32167 | best_loss 7.638
2022-03-04 10:16:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 32167 updates
2022-03-04 10:16:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 10:17:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 10:17:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 82 @ 32167 updates, score 8.215) (writing took 4.296026492025703 seconds)
2022-03-04 10:17:02 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-04 10:17:02 | INFO | train | epoch 082 | loss 5.767 | nll_loss 3.926 | ppl 15.2 | wps 24907.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 32167 | lr 0.000176317 | gnorm 0.824 | loss_scale 16 | train_wall 1002 | gb_free 12.3 | wall 84199
2022-03-04 10:17:02 | INFO | fairseq.trainer | begin training epoch 83
2022-03-04 10:17:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:18:28 | INFO | train_inner | epoch 083:     33 / 393 loss=5.787, nll_loss=3.949, ppl=15.45, wps=24193.9, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=32200, lr=0.000176227, gnorm=0.827, loss_scale=16, train_wall=257, gb_free=12.3, wall=84285
2022-03-04 10:22:48 | INFO | train_inner | epoch 083:    133 / 393 loss=5.718, nll_loss=3.869, ppl=14.61, wps=25157.3, ups=0.38, wpb=65536, bsz=128, num_updates=32300, lr=0.000175954, gnorm=0.815, loss_scale=16, train_wall=255, gb_free=12.3, wall=84545
2022-03-04 10:27:09 | INFO | train_inner | epoch 083:    233 / 393 loss=5.755, nll_loss=3.913, ppl=15.06, wps=25152.3, ups=0.38, wpb=65535.4, bsz=128, num_updates=32400, lr=0.000175682, gnorm=0.826, loss_scale=16, train_wall=255, gb_free=12.3, wall=84806
2022-03-04 10:31:29 | INFO | train_inner | epoch 083:    333 / 393 loss=5.8, nll_loss=3.965, ppl=15.62, wps=25157.8, ups=0.38, wpb=65536, bsz=128, num_updates=32500, lr=0.000175412, gnorm=0.83, loss_scale=16, train_wall=255, gb_free=12.3, wall=85066
2022-03-04 10:34:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:34:08 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 8.198 | nll_loss 6.632 | ppl 99.15 | wps 65084.5 | wpb 2034.1 | bsz 4 | num_updates 32560 | best_loss 7.638
2022-03-04 10:34:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 32560 updates
2022-03-04 10:34:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 10:34:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 10:34:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 83 @ 32560 updates, score 8.198) (writing took 4.204659227980301 seconds)
2022-03-04 10:34:12 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-04 10:34:12 | INFO | train | epoch 083 | loss 5.763 | nll_loss 3.922 | ppl 15.15 | wps 24967.3 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 32560 | lr 0.00017525 | gnorm 0.823 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 85229
2022-03-04 10:34:12 | INFO | fairseq.trainer | begin training epoch 84
2022-03-04 10:34:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:35:56 | INFO | train_inner | epoch 084:     40 / 393 loss=5.771, nll_loss=3.931, ppl=15.25, wps=24433.4, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=32600, lr=0.000175142, gnorm=0.822, loss_scale=16, train_wall=254, gb_free=12.3, wall=85333
2022-03-04 10:39:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 10:40:19 | INFO | train_inner | epoch 084:    141 / 393 loss=5.709, nll_loss=3.859, ppl=14.51, wps=24922.4, ups=0.38, wpb=65536, bsz=128, num_updates=32700, lr=0.000174874, gnorm=0.818, loss_scale=16, train_wall=258, gb_free=12.3, wall=85596
2022-03-04 10:44:40 | INFO | train_inner | epoch 084:    241 / 393 loss=5.76, nll_loss=3.918, ppl=15.11, wps=25166.7, ups=0.38, wpb=65536, bsz=128, num_updates=32800, lr=0.000174608, gnorm=0.824, loss_scale=16, train_wall=255, gb_free=12.3, wall=85857
2022-03-04 10:47:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 10:49:03 | INFO | train_inner | epoch 084:    342 / 393 loss=5.799, nll_loss=3.963, ppl=15.6, wps=24911.4, ups=0.38, wpb=65536, bsz=128, num_updates=32900, lr=0.000174342, gnorm=0.836, loss_scale=8, train_wall=258, gb_free=12.3, wall=86120
2022-03-04 10:51:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:51:18 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 8.241 | nll_loss 6.669 | ppl 101.75 | wps 64507.3 | wpb 2034.1 | bsz 4 | num_updates 32951 | best_loss 7.638
2022-03-04 10:51:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 32951 updates
2022-03-04 10:51:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 10:51:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 10:51:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 84 @ 32951 updates, score 8.241) (writing took 4.86041763285175 seconds)
2022-03-04 10:51:23 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-04 10:51:23 | INFO | train | epoch 084 | loss 5.758 | nll_loss 3.916 | ppl 15.09 | wps 24834.3 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 32951 | lr 0.000174207 | gnorm 0.827 | loss_scale 8 | train_wall 1002 | gb_free 12.3 | wall 86260
2022-03-04 10:51:23 | INFO | fairseq.trainer | begin training epoch 85
2022-03-04 10:51:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:53:30 | INFO | train_inner | epoch 085:     49 / 393 loss=5.754, nll_loss=3.911, ppl=15.04, wps=24379.4, ups=0.37, wpb=65238.4, bsz=127.4, num_updates=33000, lr=0.000174078, gnorm=0.829, loss_scale=8, train_wall=254, gb_free=12.3, wall=86387
2022-03-04 10:57:51 | INFO | train_inner | epoch 085:    149 / 393 loss=5.712, nll_loss=3.862, ppl=14.54, wps=25166, ups=0.38, wpb=65536, bsz=128, num_updates=33100, lr=0.000173814, gnorm=0.817, loss_scale=8, train_wall=255, gb_free=12.3, wall=86648
2022-03-04 11:02:11 | INFO | train_inner | epoch 085:    249 / 393 loss=5.751, nll_loss=3.907, ppl=15.01, wps=25122, ups=0.38, wpb=65536, bsz=128, num_updates=33200, lr=0.000173553, gnorm=0.831, loss_scale=8, train_wall=256, gb_free=12.3, wall=86909
2022-03-04 11:06:33 | INFO | train_inner | epoch 085:    349 / 393 loss=5.8, nll_loss=3.964, ppl=15.61, wps=25103.2, ups=0.38, wpb=65535.4, bsz=128, num_updates=33300, lr=0.000173292, gnorm=0.827, loss_scale=8, train_wall=256, gb_free=12.3, wall=87170
2022-03-04 11:08:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:08:29 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 8.219 | nll_loss 6.653 | ppl 100.66 | wps 64456.2 | wpb 2034.1 | bsz 4 | num_updates 33344 | best_loss 7.638
2022-03-04 11:08:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 33344 updates
2022-03-04 11:08:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 11:08:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 11:08:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 85 @ 33344 updates, score 8.219) (writing took 4.776046854909509 seconds)
2022-03-04 11:08:34 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-04 11:08:34 | INFO | train | epoch 085 | loss 5.755 | nll_loss 3.912 | ppl 15.05 | wps 24937.1 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 33344 | lr 0.000173177 | gnorm 0.825 | loss_scale 8 | train_wall 1003 | gb_free 12.3 | wall 87291
2022-03-04 11:08:34 | INFO | fairseq.trainer | begin training epoch 86
2022-03-04 11:08:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:11:00 | INFO | train_inner | epoch 086:     56 / 393 loss=5.753, nll_loss=3.909, ppl=15.03, wps=24387.3, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=33400, lr=0.000173032, gnorm=0.839, loss_scale=16, train_wall=254, gb_free=12.3, wall=87437
2022-03-04 11:15:21 | INFO | train_inner | epoch 086:    156 / 393 loss=5.707, nll_loss=3.857, ppl=14.49, wps=25158.3, ups=0.38, wpb=65535.4, bsz=128, num_updates=33500, lr=0.000172774, gnorm=0.82, loss_scale=16, train_wall=255, gb_free=12.3, wall=87698
2022-03-04 11:19:41 | INFO | train_inner | epoch 086:    256 / 393 loss=5.763, nll_loss=3.921, ppl=15.15, wps=25173.8, ups=0.38, wpb=65530.9, bsz=128, num_updates=33600, lr=0.000172516, gnorm=0.822, loss_scale=16, train_wall=255, gb_free=12.3, wall=87958
2022-03-04 11:24:01 | INFO | train_inner | epoch 086:    356 / 393 loss=5.798, nll_loss=3.962, ppl=15.58, wps=25158.7, ups=0.38, wpb=65536, bsz=128, num_updates=33700, lr=0.00017226, gnorm=0.835, loss_scale=16, train_wall=255, gb_free=12.3, wall=88219
2022-03-04 11:25:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:25:40 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 8.225 | nll_loss 6.659 | ppl 101.05 | wps 64415.7 | wpb 2034.1 | bsz 4 | num_updates 33737 | best_loss 7.638
2022-03-04 11:25:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 33737 updates
2022-03-04 11:25:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 11:25:45 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 11:25:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 86 @ 33737 updates, score 8.225) (writing took 5.3546143071725965 seconds)
2022-03-04 11:25:45 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-04 11:25:45 | INFO | train | epoch 086 | loss 5.751 | nll_loss 3.907 | ppl 15 | wps 24946.9 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 33737 | lr 0.000172166 | gnorm 0.83 | loss_scale 16 | train_wall 1002 | gb_free 12.3 | wall 88323
2022-03-04 11:25:45 | INFO | fairseq.trainer | begin training epoch 87
2022-03-04 11:25:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:28:30 | INFO | train_inner | epoch 087:     63 / 393 loss=5.73, nll_loss=3.883, ppl=14.75, wps=24324.1, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=33800, lr=0.000172005, gnorm=0.825, loss_scale=16, train_wall=254, gb_free=12.3, wall=88487
2022-03-04 11:32:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:32:53 | INFO | train_inner | epoch 087:    164 / 393 loss=5.71, nll_loss=3.86, ppl=14.52, wps=24919.8, ups=0.38, wpb=65536, bsz=128, num_updates=33900, lr=0.000171751, gnorm=0.825, loss_scale=16, train_wall=258, gb_free=12.3, wall=88750
2022-03-04 11:37:13 | INFO | train_inner | epoch 087:    264 / 393 loss=5.752, nll_loss=3.908, ppl=15.01, wps=25150.5, ups=0.38, wpb=65530.2, bsz=128, num_updates=34000, lr=0.000171499, gnorm=0.833, loss_scale=16, train_wall=256, gb_free=12.3, wall=89010
2022-03-04 11:41:34 | INFO | train_inner | epoch 087:    364 / 393 loss=5.802, nll_loss=3.966, ppl=15.63, wps=25167.2, ups=0.38, wpb=65536, bsz=128, num_updates=34100, lr=0.000171247, gnorm=0.841, loss_scale=16, train_wall=255, gb_free=12.3, wall=89271
2022-03-04 11:42:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:42:51 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 8.221 | nll_loss 6.65 | ppl 100.44 | wps 64162.5 | wpb 2034.1 | bsz 4 | num_updates 34129 | best_loss 7.638
2022-03-04 11:42:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 34129 updates
2022-03-04 11:42:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 11:42:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 11:42:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 87 @ 34129 updates, score 8.221) (writing took 5.0986858329270035 seconds)
2022-03-04 11:42:57 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-04 11:42:57 | INFO | train | epoch 087 | loss 5.747 | nll_loss 3.903 | ppl 14.96 | wps 24885.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 34129 | lr 0.000171174 | gnorm 0.831 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 89354
2022-03-04 11:42:57 | INFO | fairseq.trainer | begin training epoch 88
2022-03-04 11:42:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:46:02 | INFO | train_inner | epoch 088:     71 / 393 loss=5.711, nll_loss=3.861, ppl=14.53, wps=24351.4, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=34200, lr=0.000170996, gnorm=0.827, loss_scale=16, train_wall=254, gb_free=12.3, wall=89539
2022-03-04 11:50:22 | INFO | train_inner | epoch 088:    171 / 393 loss=5.712, nll_loss=3.862, ppl=14.54, wps=25147.5, ups=0.38, wpb=65530.9, bsz=128, num_updates=34300, lr=0.000170747, gnorm=0.829, loss_scale=16, train_wall=256, gb_free=12.3, wall=89799
2022-03-04 11:54:43 | INFO | train_inner | epoch 088:    271 / 393 loss=5.762, nll_loss=3.92, ppl=15.14, wps=25160.7, ups=0.38, wpb=65535.4, bsz=128, num_updates=34400, lr=0.000170499, gnorm=0.832, loss_scale=16, train_wall=255, gb_free=12.3, wall=90060
2022-03-04 11:55:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 11:59:06 | INFO | train_inner | epoch 088:    372 / 393 loss=5.797, nll_loss=3.96, ppl=15.57, wps=24908, ups=0.38, wpb=65536, bsz=128, num_updates=34500, lr=0.000170251, gnorm=0.845, loss_scale=16, train_wall=258, gb_free=12.3, wall=90323
2022-03-04 11:59:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:00:03 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 8.251 | nll_loss 6.675 | ppl 102.19 | wps 64512.4 | wpb 2034.1 | bsz 4 | num_updates 34521 | best_loss 7.638
2022-03-04 12:00:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 34521 updates
2022-03-04 12:00:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 12:00:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 12:00:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 88 @ 34521 updates, score 8.251) (writing took 4.904580912087113 seconds)
2022-03-04 12:00:08 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-04 12:00:08 | INFO | train | epoch 088 | loss 5.743 | nll_loss 3.899 | ppl 14.92 | wps 24888.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 34521 | lr 0.0001702 | gnorm 0.834 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 90385
2022-03-04 12:00:08 | INFO | fairseq.trainer | begin training epoch 89
2022-03-04 12:00:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:03:33 | INFO | train_inner | epoch 089:     79 / 393 loss=5.707, nll_loss=3.856, ppl=14.48, wps=24371.2, ups=0.37, wpb=65243.5, bsz=127.4, num_updates=34600, lr=0.000170005, gnorm=0.826, loss_scale=16, train_wall=254, gb_free=12.3, wall=90591
2022-03-04 12:07:54 | INFO | train_inner | epoch 089:    179 / 393 loss=5.719, nll_loss=3.871, ppl=14.63, wps=25161.3, ups=0.38, wpb=65536, bsz=128, num_updates=34700, lr=0.00016976, gnorm=0.84, loss_scale=16, train_wall=255, gb_free=12.3, wall=90851
2022-03-04 12:12:14 | INFO | train_inner | epoch 089:    279 / 393 loss=5.752, nll_loss=3.908, ppl=15.01, wps=25172.4, ups=0.38, wpb=65536, bsz=128, num_updates=34800, lr=0.000169516, gnorm=0.838, loss_scale=16, train_wall=255, gb_free=12.3, wall=91111
2022-03-04 12:16:35 | INFO | train_inner | epoch 089:    379 / 393 loss=5.791, nll_loss=3.954, ppl=15.5, wps=25160.4, ups=0.38, wpb=65536, bsz=128, num_updates=34900, lr=0.000169273, gnorm=0.837, loss_scale=16, train_wall=255, gb_free=12.3, wall=91372
2022-03-04 12:17:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:17:13 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 8.259 | nll_loss 6.693 | ppl 103.44 | wps 63931.4 | wpb 2034.1 | bsz 4 | num_updates 34914 | best_loss 7.638
2022-03-04 12:17:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 34914 updates
2022-03-04 12:17:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 12:17:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 12:17:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 89 @ 34914 updates, score 8.259) (writing took 4.934719173936173 seconds)
2022-03-04 12:17:18 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-04 12:17:18 | INFO | train | epoch 089 | loss 5.74 | nll_loss 3.895 | ppl 14.88 | wps 24957.4 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 34914 | lr 0.000169239 | gnorm 0.835 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 91416
2022-03-04 12:17:18 | INFO | fairseq.trainer | begin training epoch 90
2022-03-04 12:17:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:17:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:21:05 | INFO | train_inner | epoch 090:     87 / 393 loss=5.691, nll_loss=3.837, ppl=14.3, wps=24120.8, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=35000, lr=0.000169031, gnorm=0.824, loss_scale=16, train_wall=257, gb_free=12.3, wall=91642
2022-03-04 12:25:26 | INFO | train_inner | epoch 090:    187 / 393 loss=5.711, nll_loss=3.861, ppl=14.53, wps=25155.6, ups=0.38, wpb=65530.2, bsz=128, num_updates=35100, lr=0.00016879, gnorm=0.829, loss_scale=16, train_wall=255, gb_free=12.3, wall=91903
2022-03-04 12:29:46 | INFO | train_inner | epoch 090:    287 / 393 loss=5.745, nll_loss=3.901, ppl=14.94, wps=25155.2, ups=0.38, wpb=65536, bsz=128, num_updates=35200, lr=0.00016855, gnorm=0.84, loss_scale=16, train_wall=255, gb_free=12.3, wall=92163
2022-03-04 12:34:07 | INFO | train_inner | epoch 090:    387 / 393 loss=5.804, nll_loss=3.969, ppl=15.66, wps=25161.8, ups=0.38, wpb=65536, bsz=128, num_updates=35300, lr=0.000168311, gnorm=0.839, loss_scale=16, train_wall=255, gb_free=12.3, wall=92424
2022-03-04 12:34:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:34:25 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 8.237 | nll_loss 6.672 | ppl 101.94 | wps 63995.2 | wpb 2034.1 | bsz 4 | num_updates 35306 | best_loss 7.638
2022-03-04 12:34:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 35306 updates
2022-03-04 12:34:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 12:34:29 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 12:34:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 90 @ 35306 updates, score 8.237) (writing took 4.905623814091086 seconds)
2022-03-04 12:34:30 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-04 12:34:30 | INFO | train | epoch 090 | loss 5.737 | nll_loss 3.891 | ppl 14.83 | wps 24886.8 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 35306 | lr 0.000168297 | gnorm 0.832 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 92447
2022-03-04 12:34:30 | INFO | fairseq.trainer | begin training epoch 91
2022-03-04 12:34:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:38:35 | INFO | train_inner | epoch 091:     94 / 393 loss=5.671, nll_loss=3.814, ppl=14.07, wps=24331.6, ups=0.37, wpb=65248.6, bsz=127.4, num_updates=35400, lr=0.000168073, gnorm=0.833, loss_scale=16, train_wall=254, gb_free=12.3, wall=92692
2022-03-04 12:40:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 12:42:58 | INFO | train_inner | epoch 091:    195 / 393 loss=5.713, nll_loss=3.864, ppl=14.56, wps=24866.8, ups=0.38, wpb=65536, bsz=128, num_updates=35500, lr=0.000167836, gnorm=0.833, loss_scale=16, train_wall=258, gb_free=12.3, wall=92956
2022-03-04 12:47:19 | INFO | train_inner | epoch 091:    295 / 393 loss=5.761, nll_loss=3.918, ppl=15.12, wps=25122.7, ups=0.38, wpb=65530.9, bsz=128, num_updates=35600, lr=0.0001676, gnorm=0.844, loss_scale=16, train_wall=255, gb_free=12.3, wall=93216
2022-03-04 12:51:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:51:37 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 8.225 | nll_loss 6.652 | ppl 100.58 | wps 64714.9 | wpb 2034.1 | bsz 4 | num_updates 35698 | best_loss 7.638
2022-03-04 12:51:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 35698 updates
2022-03-04 12:51:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 12:51:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 12:51:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 91 @ 35698 updates, score 8.225) (writing took 4.831865241052583 seconds)
2022-03-04 12:51:42 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-04 12:51:42 | INFO | train | epoch 091 | loss 5.733 | nll_loss 3.887 | ppl 14.79 | wps 24853.6 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 35698 | lr 0.00016737 | gnorm 0.835 | loss_scale 16 | train_wall 1002 | gb_free 12.3 | wall 93479
2022-03-04 12:51:42 | INFO | fairseq.trainer | begin training epoch 92
2022-03-04 12:51:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:51:47 | INFO | train_inner | epoch 092:      2 / 393 loss=5.791, nll_loss=3.954, ppl=15.5, wps=24340.5, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=35700, lr=0.000167365, gnorm=0.831, loss_scale=16, train_wall=254, gb_free=12.3, wall=93485
2022-03-04 12:56:08 | INFO | train_inner | epoch 092:    102 / 393 loss=5.664, nll_loss=3.806, ppl=13.99, wps=25157.1, ups=0.38, wpb=65530.9, bsz=128, num_updates=35800, lr=0.000167132, gnorm=0.828, loss_scale=16, train_wall=255, gb_free=12.3, wall=93745
2022-03-04 13:00:29 | INFO | train_inner | epoch 092:    202 / 393 loss=5.708, nll_loss=3.857, ppl=14.49, wps=25102, ups=0.38, wpb=65536, bsz=128, num_updates=35900, lr=0.000166899, gnorm=0.839, loss_scale=16, train_wall=256, gb_free=12.3, wall=94006
2022-03-04 13:03:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:04:53 | INFO | train_inner | epoch 092:    303 / 393 loss=5.757, nll_loss=3.914, ppl=15.07, wps=24829.7, ups=0.38, wpb=65535.4, bsz=128, num_updates=36000, lr=0.000166667, gnorm=0.837, loss_scale=16, train_wall=259, gb_free=12.3, wall=94270
2022-03-04 13:08:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:08:50 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 8.221 | nll_loss 6.651 | ppl 100.48 | wps 64110.3 | wpb 2034.1 | bsz 4 | num_updates 36090 | best_loss 7.638
2022-03-04 13:08:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 36090 updates
2022-03-04 13:08:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 13:08:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 13:08:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 92 @ 36090 updates, score 8.221) (writing took 4.806519473902881 seconds)
2022-03-04 13:08:54 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-04 13:08:54 | INFO | train | epoch 092 | loss 5.73 | nll_loss 3.883 | ppl 14.76 | wps 24854.9 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 36090 | lr 0.000166459 | gnorm 0.836 | loss_scale 16 | train_wall 1004 | gb_free 12.3 | wall 94512
2022-03-04 13:08:54 | INFO | fairseq.trainer | begin training epoch 93
2022-03-04 13:08:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:09:21 | INFO | train_inner | epoch 093:     10 / 393 loss=5.788, nll_loss=3.95, ppl=15.46, wps=24364.2, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=36100, lr=0.000166436, gnorm=0.841, loss_scale=16, train_wall=254, gb_free=12.3, wall=94538
2022-03-04 13:13:41 | INFO | train_inner | epoch 093:    110 / 393 loss=5.662, nll_loss=3.804, ppl=13.96, wps=25159.4, ups=0.38, wpb=65536, bsz=128, num_updates=36200, lr=0.000166206, gnorm=0.833, loss_scale=16, train_wall=255, gb_free=12.3, wall=94798
2022-03-04 13:18:02 | INFO | train_inner | epoch 093:    210 / 393 loss=5.712, nll_loss=3.862, ppl=14.54, wps=25156.3, ups=0.38, wpb=65535.4, bsz=128, num_updates=36300, lr=0.000165977, gnorm=0.838, loss_scale=16, train_wall=255, gb_free=12.3, wall=95059
2022-03-04 13:22:22 | INFO | train_inner | epoch 093:    310 / 393 loss=5.765, nll_loss=3.923, ppl=15.17, wps=25166.8, ups=0.38, wpb=65536, bsz=128, num_updates=36400, lr=0.000165748, gnorm=0.834, loss_scale=16, train_wall=255, gb_free=12.3, wall=95319
2022-03-04 13:25:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:25:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:26:01 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 8.252 | nll_loss 6.683 | ppl 102.77 | wps 65144.4 | wpb 2034.1 | bsz 4 | num_updates 36482 | best_loss 7.638
2022-03-04 13:26:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 36482 updates
2022-03-04 13:26:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 13:26:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 13:26:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 93 @ 36482 updates, score 8.252) (writing took 4.827671409118921 seconds)
2022-03-04 13:26:05 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-04 13:26:05 | INFO | train | epoch 093 | loss 5.727 | nll_loss 3.88 | ppl 14.72 | wps 24891.7 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 36482 | lr 0.000165562 | gnorm 0.837 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 95543
2022-03-04 13:26:05 | INFO | fairseq.trainer | begin training epoch 94
2022-03-04 13:26:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:26:52 | INFO | train_inner | epoch 094:     18 / 393 loss=5.762, nll_loss=3.92, ppl=15.14, wps=24141.2, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=36500, lr=0.000165521, gnorm=0.845, loss_scale=16, train_wall=257, gb_free=12.3, wall=95590
2022-03-04 13:31:13 | INFO | train_inner | epoch 094:    118 / 393 loss=5.671, nll_loss=3.814, ppl=14.07, wps=25153.2, ups=0.38, wpb=65536, bsz=128, num_updates=36600, lr=0.000165295, gnorm=0.84, loss_scale=16, train_wall=255, gb_free=12.3, wall=95850
2022-03-04 13:35:33 | INFO | train_inner | epoch 094:    218 / 393 loss=5.713, nll_loss=3.863, ppl=14.55, wps=25153, ups=0.38, wpb=65530.9, bsz=128, num_updates=36700, lr=0.00016507, gnorm=0.84, loss_scale=16, train_wall=255, gb_free=12.3, wall=96111
2022-03-04 13:39:54 | INFO | train_inner | epoch 094:    318 / 393 loss=5.755, nll_loss=3.911, ppl=15.05, wps=25172.3, ups=0.38, wpb=65536, bsz=128, num_updates=36800, lr=0.000164845, gnorm=0.849, loss_scale=16, train_wall=255, gb_free=12.3, wall=96371
2022-03-04 13:43:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:43:11 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 8.253 | nll_loss 6.695 | ppl 103.62 | wps 64204.6 | wpb 2034.1 | bsz 4 | num_updates 36875 | best_loss 7.638
2022-03-04 13:43:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 36875 updates
2022-03-04 13:43:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 13:43:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 13:43:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 94 @ 36875 updates, score 8.253) (writing took 4.726657771971077 seconds)
2022-03-04 13:43:16 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-04 13:43:16 | INFO | train | epoch 094 | loss 5.724 | nll_loss 3.876 | ppl 14.68 | wps 24957.3 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 36875 | lr 0.000164677 | gnorm 0.845 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 96573
2022-03-04 13:43:16 | INFO | fairseq.trainer | begin training epoch 95
2022-03-04 13:43:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:44:21 | INFO | train_inner | epoch 095:     25 / 393 loss=5.749, nll_loss=3.905, ppl=14.98, wps=24375.7, ups=0.37, wpb=65248.6, bsz=127.4, num_updates=36900, lr=0.000164622, gnorm=0.849, loss_scale=16, train_wall=254, gb_free=12.3, wall=96639
2022-03-04 13:48:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 13:48:44 | INFO | train_inner | epoch 095:    126 / 393 loss=5.665, nll_loss=3.808, ppl=14, wps=24914.7, ups=0.38, wpb=65530.9, bsz=128, num_updates=37000, lr=0.000164399, gnorm=0.841, loss_scale=16, train_wall=258, gb_free=12.3, wall=96902
2022-03-04 13:53:05 | INFO | train_inner | epoch 095:    226 / 393 loss=5.715, nll_loss=3.865, ppl=14.57, wps=25163, ups=0.38, wpb=65535.4, bsz=128, num_updates=37100, lr=0.000164177, gnorm=0.835, loss_scale=16, train_wall=255, gb_free=12.3, wall=97162
2022-03-04 13:57:25 | INFO | train_inner | epoch 095:    326 / 393 loss=5.756, nll_loss=3.913, ppl=15.06, wps=25156.8, ups=0.38, wpb=65536, bsz=128, num_updates=37200, lr=0.000163956, gnorm=0.846, loss_scale=16, train_wall=255, gb_free=12.3, wall=97423
2022-03-04 14:00:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:00:22 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 8.254 | nll_loss 6.685 | ppl 102.88 | wps 64557.2 | wpb 2034.1 | bsz 4 | num_updates 37267 | best_loss 7.638
2022-03-04 14:00:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 37267 updates
2022-03-04 14:00:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 14:00:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 14:00:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 95 @ 37267 updates, score 8.254) (writing took 4.693105906015262 seconds)
2022-03-04 14:00:27 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-04 14:00:27 | INFO | train | epoch 095 | loss 5.72 | nll_loss 3.872 | ppl 14.64 | wps 24895.5 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 37267 | lr 0.000163809 | gnorm 0.844 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 97604
2022-03-04 14:00:27 | INFO | fairseq.trainer | begin training epoch 96
2022-03-04 14:00:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:01:53 | INFO | train_inner | epoch 096:     33 / 393 loss=5.741, nll_loss=3.896, ppl=14.88, wps=24386.2, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=37300, lr=0.000163737, gnorm=0.85, loss_scale=16, train_wall=254, gb_free=12.3, wall=97690
2022-03-04 14:06:13 | INFO | train_inner | epoch 096:    133 / 393 loss=5.662, nll_loss=3.804, ppl=13.97, wps=25154.9, ups=0.38, wpb=65530.2, bsz=128, num_updates=37400, lr=0.000163517, gnorm=0.845, loss_scale=16, train_wall=255, gb_free=12.3, wall=97951
2022-03-04 14:10:34 | INFO | train_inner | epoch 096:    233 / 393 loss=5.715, nll_loss=3.865, ppl=14.57, wps=25146.4, ups=0.38, wpb=65536, bsz=128, num_updates=37500, lr=0.000163299, gnorm=0.832, loss_scale=16, train_wall=256, gb_free=12.3, wall=98211
2022-03-04 14:11:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:14:57 | INFO | train_inner | epoch 096:    334 / 393 loss=5.761, nll_loss=3.919, ppl=15.13, wps=24903.3, ups=0.38, wpb=65536, bsz=128, num_updates=37600, lr=0.000163082, gnorm=0.841, loss_scale=16, train_wall=258, gb_free=12.3, wall=98474
2022-03-04 14:17:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:17:33 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 8.248 | nll_loss 6.681 | ppl 102.61 | wps 64418 | wpb 2034.1 | bsz 4 | num_updates 37659 | best_loss 7.638
2022-03-04 14:17:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 37659 updates
2022-03-04 14:17:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 14:17:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 14:17:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 96 @ 37659 updates, score 8.248) (writing took 4.77911057183519 seconds)
2022-03-04 14:17:38 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-04 14:17:38 | INFO | train | epoch 096 | loss 5.718 | nll_loss 3.868 | ppl 14.6 | wps 24888 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 37659 | lr 0.000162954 | gnorm 0.84 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 98635
2022-03-04 14:17:38 | INFO | fairseq.trainer | begin training epoch 97
2022-03-04 14:17:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:19:25 | INFO | train_inner | epoch 097:     41 / 393 loss=5.727, nll_loss=3.879, ppl=14.72, wps=24373.4, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=37700, lr=0.000162866, gnorm=0.848, loss_scale=16, train_wall=254, gb_free=12.3, wall=98742
2022-03-04 14:23:45 | INFO | train_inner | epoch 097:    141 / 393 loss=5.675, nll_loss=3.819, ppl=14.11, wps=25157.6, ups=0.38, wpb=65536, bsz=128, num_updates=37800, lr=0.00016265, gnorm=0.849, loss_scale=16, train_wall=255, gb_free=12.3, wall=99003
2022-03-04 14:28:06 | INFO | train_inner | epoch 097:    241 / 393 loss=5.716, nll_loss=3.866, ppl=14.58, wps=25152.6, ups=0.38, wpb=65536, bsz=128, num_updates=37900, lr=0.000162435, gnorm=0.852, loss_scale=16, train_wall=255, gb_free=12.3, wall=99263
2022-03-04 14:32:26 | INFO | train_inner | epoch 097:    341 / 393 loss=5.748, nll_loss=3.904, ppl=14.97, wps=25164.6, ups=0.38, wpb=65535.4, bsz=128, num_updates=38000, lr=0.000162221, gnorm=0.856, loss_scale=16, train_wall=255, gb_free=12.3, wall=99524
2022-03-04 14:33:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:34:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:34:44 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 8.25 | nll_loss 6.68 | ppl 102.55 | wps 64448.8 | wpb 2034.1 | bsz 4 | num_updates 38051 | best_loss 7.638
2022-03-04 14:34:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 38051 updates
2022-03-04 14:34:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 14:34:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 14:34:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 97 @ 38051 updates, score 8.25) (writing took 4.724427466047928 seconds)
2022-03-04 14:34:49 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-04 14:34:49 | INFO | train | epoch 097 | loss 5.715 | nll_loss 3.866 | ppl 14.58 | wps 24893.8 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 38051 | lr 0.000162113 | gnorm 0.854 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 99666
2022-03-04 14:34:49 | INFO | fairseq.trainer | begin training epoch 98
2022-03-04 14:34:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:36:57 | INFO | train_inner | epoch 098:     49 / 393 loss=5.709, nll_loss=3.859, ppl=14.51, wps=24146.2, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=38100, lr=0.000162008, gnorm=0.849, loss_scale=16, train_wall=257, gb_free=12.3, wall=99794
2022-03-04 14:41:18 | INFO | train_inner | epoch 098:    149 / 393 loss=5.674, nll_loss=3.817, ppl=14.09, wps=25117.6, ups=0.38, wpb=65535.4, bsz=128, num_updates=38200, lr=0.000161796, gnorm=0.841, loss_scale=16, train_wall=255, gb_free=12.3, wall=100055
2022-03-04 14:45:38 | INFO | train_inner | epoch 098:    249 / 393 loss=5.716, nll_loss=3.867, ppl=14.59, wps=25123.5, ups=0.38, wpb=65536, bsz=128, num_updates=38300, lr=0.000161585, gnorm=0.852, loss_scale=16, train_wall=255, gb_free=12.3, wall=100316
2022-03-04 14:49:59 | INFO | train_inner | epoch 098:    349 / 393 loss=5.756, nll_loss=3.913, ppl=15.06, wps=25120.8, ups=0.38, wpb=65536, bsz=128, num_updates=38400, lr=0.000161374, gnorm=0.854, loss_scale=16, train_wall=255, gb_free=12.3, wall=100576
2022-03-04 14:51:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:51:56 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 8.245 | nll_loss 6.671 | ppl 101.92 | wps 64223.1 | wpb 2034.1 | bsz 4 | num_updates 38444 | best_loss 7.638
2022-03-04 14:51:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 38444 updates
2022-03-04 14:51:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 14:52:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 14:52:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 98 @ 38444 updates, score 8.245) (writing took 4.635620581917465 seconds)
2022-03-04 14:52:01 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-04 14:52:01 | INFO | train | epoch 098 | loss 5.712 | nll_loss 3.862 | ppl 14.54 | wps 24923.6 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 38444 | lr 0.000161282 | gnorm 0.849 | loss_scale 16 | train_wall 1002 | gb_free 12.3 | wall 100698
2022-03-04 14:52:01 | INFO | fairseq.trainer | begin training epoch 99
2022-03-04 14:52:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:54:27 | INFO | train_inner | epoch 099:     56 / 393 loss=5.702, nll_loss=3.85, ppl=14.42, wps=24377.4, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=38500, lr=0.000161165, gnorm=0.86, loss_scale=16, train_wall=254, gb_free=12.3, wall=100844
2022-03-04 14:56:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 14:58:49 | INFO | train_inner | epoch 099:    157 / 393 loss=5.674, nll_loss=3.817, ppl=14.1, wps=24983.3, ups=0.38, wpb=65536, bsz=128, num_updates=38600, lr=0.000160956, gnorm=0.858, loss_scale=16, train_wall=257, gb_free=12.3, wall=101106
2022-03-04 15:03:09 | INFO | train_inner | epoch 099:    257 / 393 loss=5.714, nll_loss=3.864, ppl=14.57, wps=25243.5, ups=0.39, wpb=65536, bsz=128, num_updates=38700, lr=0.000160748, gnorm=0.847, loss_scale=16, train_wall=255, gb_free=12.3, wall=101366
2022-03-04 15:07:28 | INFO | train_inner | epoch 099:    357 / 393 loss=5.756, nll_loss=3.912, ppl=15.06, wps=25270.8, ups=0.39, wpb=65530.2, bsz=128, num_updates=38800, lr=0.00016054, gnorm=0.866, loss_scale=16, train_wall=254, gb_free=12.3, wall=101625
2022-03-04 15:09:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:09:04 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 8.267 | nll_loss 6.702 | ppl 104.15 | wps 65469.9 | wpb 2034.1 | bsz 4 | num_updates 38836 | best_loss 7.638
2022-03-04 15:09:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 38836 updates
2022-03-04 15:09:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 15:09:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 15:09:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 99 @ 38836 updates, score 8.267) (writing took 4.638633418828249 seconds)
2022-03-04 15:09:08 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-04 15:09:08 | INFO | train | epoch 099 | loss 5.709 | nll_loss 3.858 | ppl 14.5 | wps 24975 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 38836 | lr 0.000160466 | gnorm 0.857 | loss_scale 16 | train_wall 1000 | gb_free 12.3 | wall 101726
2022-03-04 15:09:08 | INFO | fairseq.trainer | begin training epoch 100
2022-03-04 15:09:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:11:55 | INFO | train_inner | epoch 100:     64 / 393 loss=5.683, nll_loss=3.829, ppl=14.21, wps=24491.5, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=38900, lr=0.000160334, gnorm=0.847, loss_scale=16, train_wall=253, gb_free=12.3, wall=101892
2022-03-04 15:16:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 15:16:17 | INFO | train_inner | epoch 100:    165 / 393 loss=5.674, nll_loss=3.817, ppl=14.09, wps=25017.6, ups=0.38, wpb=65536, bsz=128, num_updates=39000, lr=0.000160128, gnorm=0.861, loss_scale=8, train_wall=257, gb_free=12.3, wall=102154
2022-03-04 15:20:36 | INFO | train_inner | epoch 100:    265 / 393 loss=5.718, nll_loss=3.869, ppl=14.61, wps=25267.1, ups=0.39, wpb=65530.9, bsz=128, num_updates=39100, lr=0.000159923, gnorm=0.852, loss_scale=8, train_wall=254, gb_free=12.3, wall=102413
2022-03-04 15:24:55 | INFO | train_inner | epoch 100:    365 / 393 loss=5.753, nll_loss=3.909, ppl=15.02, wps=25278.9, ups=0.39, wpb=65536, bsz=128, num_updates=39200, lr=0.000159719, gnorm=0.85, loss_scale=8, train_wall=254, gb_free=12.3, wall=102672
2022-03-04 15:26:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:26:10 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 8.277 | nll_loss 6.704 | ppl 104.26 | wps 65564.6 | wpb 2034.1 | bsz 4 | num_updates 39228 | best_loss 7.638
2022-03-04 15:26:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 39228 updates
2022-03-04 15:26:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 15:26:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 15:26:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 100 @ 39228 updates, score 8.277) (writing took 4.6189344639424235 seconds)
2022-03-04 15:26:15 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-04 15:26:15 | INFO | train | epoch 100 | loss 5.706 | nll_loss 3.855 | ppl 14.47 | wps 25005.3 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 39228 | lr 0.000159662 | gnorm 0.854 | loss_scale 8 | train_wall 999 | gb_free 12.3 | wall 102752
2022-03-04 15:26:15 | INFO | fairseq.trainer | begin training epoch 101
2022-03-04 15:26:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:29:21 | INFO | train_inner | epoch 101:     72 / 393 loss=5.67, nll_loss=3.813, ppl=14.06, wps=24504.1, ups=0.38, wpb=65248.6, bsz=127.4, num_updates=39300, lr=0.000159516, gnorm=0.857, loss_scale=8, train_wall=253, gb_free=12.3, wall=102939
2022-03-04 15:33:41 | INFO | train_inner | epoch 101:    172 / 393 loss=5.676, nll_loss=3.819, ppl=14.11, wps=25260, ups=0.39, wpb=65536, bsz=128, num_updates=39400, lr=0.000159313, gnorm=0.852, loss_scale=8, train_wall=254, gb_free=12.3, wall=103198
2022-03-04 15:38:00 | INFO | train_inner | epoch 101:    272 / 393 loss=5.711, nll_loss=3.861, ppl=14.53, wps=25279.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=39500, lr=0.000159111, gnorm=0.857, loss_scale=8, train_wall=254, gb_free=12.3, wall=103457
2022-03-04 15:42:20 | INFO | train_inner | epoch 101:    372 / 393 loss=5.763, nll_loss=3.922, ppl=15.15, wps=25203.8, ups=0.38, wpb=65530.9, bsz=128, num_updates=39600, lr=0.00015891, gnorm=0.863, loss_scale=16, train_wall=255, gb_free=12.3, wall=103717
2022-03-04 15:43:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:43:17 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 8.298 | nll_loss 6.735 | ppl 106.5 | wps 64752.2 | wpb 2034.1 | bsz 4 | num_updates 39621 | best_loss 7.638
2022-03-04 15:43:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 39621 updates
2022-03-04 15:43:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 15:43:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 15:43:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 101 @ 39621 updates, score 8.298) (writing took 4.647929137805477 seconds)
2022-03-04 15:43:22 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-04 15:43:22 | INFO | train | epoch 101 | loss 5.704 | nll_loss 3.852 | ppl 14.44 | wps 25046 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 39621 | lr 0.000158868 | gnorm 0.858 | loss_scale 16 | train_wall 1000 | gb_free 12.3 | wall 103779
2022-03-04 15:43:22 | INFO | fairseq.trainer | begin training epoch 102
2022-03-04 15:43:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:46:48 | INFO | train_inner | epoch 102:     79 / 393 loss=5.664, nll_loss=3.806, ppl=13.99, wps=24385, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=39700, lr=0.00015871, gnorm=0.854, loss_scale=16, train_wall=254, gb_free=12.3, wall=103985
2022-03-04 15:51:08 | INFO | train_inner | epoch 102:    179 / 393 loss=5.673, nll_loss=3.816, ppl=14.09, wps=25154.3, ups=0.38, wpb=65530.2, bsz=128, num_updates=39800, lr=0.000158511, gnorm=0.85, loss_scale=16, train_wall=255, gb_free=12.3, wall=104245
2022-03-04 15:55:29 | INFO | train_inner | epoch 102:    279 / 393 loss=5.715, nll_loss=3.865, ppl=14.57, wps=25173.9, ups=0.38, wpb=65536, bsz=128, num_updates=39900, lr=0.000158312, gnorm=0.853, loss_scale=16, train_wall=255, gb_free=12.3, wall=104506
2022-03-04 15:59:49 | INFO | train_inner | epoch 102:    379 / 393 loss=5.761, nll_loss=3.918, ppl=15.12, wps=25159.6, ups=0.38, wpb=65536, bsz=128, num_updates=40000, lr=0.000158114, gnorm=0.869, loss_scale=16, train_wall=255, gb_free=12.3, wall=104766
2022-03-04 16:00:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:00:28 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 8.266 | nll_loss 6.7 | ppl 103.95 | wps 65018.8 | wpb 2034.1 | bsz 4 | num_updates 40014 | best_loss 7.638
2022-03-04 16:00:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 40014 updates
2022-03-04 16:00:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 16:00:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 16:00:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 102 @ 40014 updates, score 8.266) (writing took 4.707895337138325 seconds)
2022-03-04 16:00:33 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-04 16:00:33 | INFO | train | epoch 102 | loss 5.701 | nll_loss 3.849 | ppl 14.41 | wps 24961.2 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 40014 | lr 0.000158086 | gnorm 0.854 | loss_scale 16 | train_wall 1002 | gb_free 12.3 | wall 104810
2022-03-04 16:00:33 | INFO | fairseq.trainer | begin training epoch 103
2022-03-04 16:00:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:00:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:04:19 | INFO | train_inner | epoch 103:     87 / 393 loss=5.655, nll_loss=3.795, ppl=13.88, wps=24150.9, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=40100, lr=0.000157917, gnorm=0.853, loss_scale=16, train_wall=257, gb_free=12.3, wall=105036
2022-03-04 16:08:40 | INFO | train_inner | epoch 103:    187 / 393 loss=5.673, nll_loss=3.816, ppl=14.09, wps=25151.3, ups=0.38, wpb=65535.4, bsz=128, num_updates=40200, lr=0.00015772, gnorm=0.863, loss_scale=16, train_wall=255, gb_free=12.3, wall=105297
2022-03-04 16:13:00 | INFO | train_inner | epoch 103:    287 / 393 loss=5.709, nll_loss=3.858, ppl=14.5, wps=25163.3, ups=0.38, wpb=65530.9, bsz=128, num_updates=40300, lr=0.000157524, gnorm=0.864, loss_scale=16, train_wall=255, gb_free=12.3, wall=105557
2022-03-04 16:17:21 | INFO | train_inner | epoch 103:    387 / 393 loss=5.765, nll_loss=3.923, ppl=15.17, wps=25165.5, ups=0.38, wpb=65536, bsz=128, num_updates=40400, lr=0.000157329, gnorm=0.864, loss_scale=16, train_wall=255, gb_free=12.3, wall=105818
2022-03-04 16:17:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:17:39 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 8.284 | nll_loss 6.715 | ppl 105.03 | wps 64319.3 | wpb 2034.1 | bsz 4 | num_updates 40406 | best_loss 7.638
2022-03-04 16:17:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 40406 updates
2022-03-04 16:17:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 16:17:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 16:17:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 103 @ 40406 updates, score 8.284) (writing took 4.718725712969899 seconds)
2022-03-04 16:17:43 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-04 16:17:43 | INFO | train | epoch 103 | loss 5.699 | nll_loss 3.847 | ppl 14.39 | wps 24894.2 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 40406 | lr 0.000157318 | gnorm 0.863 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 105841
2022-03-04 16:17:43 | INFO | fairseq.trainer | begin training epoch 104
2022-03-04 16:17:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:21:48 | INFO | train_inner | epoch 104:     94 / 393 loss=5.638, nll_loss=3.775, ppl=13.69, wps=24383.1, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=40500, lr=0.000157135, gnorm=0.863, loss_scale=16, train_wall=254, gb_free=12.3, wall=106085
2022-03-04 16:23:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:26:11 | INFO | train_inner | epoch 104:    195 / 393 loss=5.677, nll_loss=3.821, ppl=14.13, wps=24911.2, ups=0.38, wpb=65535.4, bsz=128, num_updates=40600, lr=0.000156941, gnorm=0.86, loss_scale=16, train_wall=258, gb_free=12.3, wall=106349
2022-03-04 16:30:32 | INFO | train_inner | epoch 104:    295 / 393 loss=5.711, nll_loss=3.86, ppl=14.52, wps=25159, ups=0.38, wpb=65536, bsz=128, num_updates=40700, lr=0.000156748, gnorm=0.877, loss_scale=16, train_wall=255, gb_free=12.3, wall=106609
2022-03-04 16:34:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:34:49 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 8.282 | nll_loss 6.721 | ppl 105.46 | wps 64367.1 | wpb 2034.1 | bsz 4 | num_updates 40798 | best_loss 7.638
2022-03-04 16:34:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 40798 updates
2022-03-04 16:34:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 16:34:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 16:34:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 104 @ 40798 updates, score 8.282) (writing took 4.687486074864864 seconds)
2022-03-04 16:34:54 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-04 16:34:54 | INFO | train | epoch 104 | loss 5.696 | nll_loss 3.843 | ppl 14.35 | wps 24896.8 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 40798 | lr 0.00015656 | gnorm 0.867 | loss_scale 16 | train_wall 1002 | gb_free 12.3 | wall 106871
2022-03-04 16:34:54 | INFO | fairseq.trainer | begin training epoch 105
2022-03-04 16:34:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:34:59 | INFO | train_inner | epoch 105:      2 / 393 loss=5.76, nll_loss=3.918, ppl=15.11, wps=24393.7, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=40800, lr=0.000156556, gnorm=0.872, loss_scale=16, train_wall=254, gb_free=12.3, wall=106876
2022-03-04 16:39:20 | INFO | train_inner | epoch 105:    102 / 393 loss=5.632, nll_loss=3.769, ppl=13.63, wps=25159.7, ups=0.38, wpb=65536, bsz=128, num_updates=40900, lr=0.000156365, gnorm=0.859, loss_scale=16, train_wall=255, gb_free=12.3, wall=107137
2022-03-04 16:43:40 | INFO | train_inner | epoch 105:    202 / 393 loss=5.681, nll_loss=3.825, ppl=14.17, wps=25163.3, ups=0.38, wpb=65536, bsz=128, num_updates=41000, lr=0.000156174, gnorm=0.861, loss_scale=16, train_wall=255, gb_free=12.3, wall=107397
2022-03-04 16:45:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 16:48:03 | INFO | train_inner | epoch 105:    303 / 393 loss=5.716, nll_loss=3.866, ppl=14.58, wps=24920.7, ups=0.38, wpb=65530.2, bsz=128, num_updates=41100, lr=0.000155984, gnorm=0.866, loss_scale=16, train_wall=258, gb_free=12.3, wall=107660
2022-03-04 16:51:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:52:00 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 8.302 | nll_loss 6.74 | ppl 106.92 | wps 64190.3 | wpb 2034.1 | bsz 4 | num_updates 41190 | best_loss 7.638
2022-03-04 16:52:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 41190 updates
2022-03-04 16:52:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 16:52:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 16:52:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 105 @ 41190 updates, score 8.302) (writing took 4.65201600198634 seconds)
2022-03-04 16:52:05 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-04 16:52:05 | INFO | train | epoch 105 | loss 5.693 | nll_loss 3.84 | ppl 14.32 | wps 24896.5 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 41190 | lr 0.000155813 | gnorm 0.864 | loss_scale 16 | train_wall 1002 | gb_free 12.3 | wall 107902
2022-03-04 16:52:05 | INFO | fairseq.trainer | begin training epoch 106
2022-03-04 16:52:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:52:31 | INFO | train_inner | epoch 106:     10 / 393 loss=5.739, nll_loss=3.893, ppl=14.86, wps=24371.3, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=41200, lr=0.000155794, gnorm=0.868, loss_scale=16, train_wall=254, gb_free=12.3, wall=107928
2022-03-04 16:56:52 | INFO | train_inner | epoch 106:    110 / 393 loss=5.631, nll_loss=3.767, ppl=13.62, wps=25110.6, ups=0.38, wpb=65536, bsz=128, num_updates=41300, lr=0.000155606, gnorm=0.853, loss_scale=16, train_wall=256, gb_free=12.3, wall=108189
2022-03-04 17:01:13 | INFO | train_inner | epoch 106:    210 / 393 loss=5.674, nll_loss=3.818, ppl=14.1, wps=25106.9, ups=0.38, wpb=65535.4, bsz=128, num_updates=41400, lr=0.000155417, gnorm=0.864, loss_scale=16, train_wall=256, gb_free=12.3, wall=108450
2022-03-04 17:05:34 | INFO | train_inner | epoch 106:    310 / 393 loss=5.718, nll_loss=3.869, ppl=14.61, wps=25098.3, ups=0.38, wpb=65530.9, bsz=128, num_updates=41500, lr=0.00015523, gnorm=0.869, loss_scale=16, train_wall=256, gb_free=12.3, wall=108711
2022-03-04 17:08:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:09:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:09:12 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 8.288 | nll_loss 6.726 | ppl 105.82 | wps 63869.6 | wpb 2034.1 | bsz 4 | num_updates 41582 | best_loss 7.638
2022-03-04 17:09:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 41582 updates
2022-03-04 17:09:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 17:09:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 17:09:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 106 @ 41582 updates, score 8.288) (writing took 4.6564134990330786 seconds)
2022-03-04 17:09:17 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-04 17:09:17 | INFO | train | epoch 106 | loss 5.691 | nll_loss 3.837 | ppl 14.29 | wps 24854 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 41582 | lr 0.000155077 | gnorm 0.864 | loss_scale 16 | train_wall 1004 | gb_free 12.3 | wall 108934
2022-03-04 17:09:17 | INFO | fairseq.trainer | begin training epoch 107
2022-03-04 17:09:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:10:04 | INFO | train_inner | epoch 107:     18 / 393 loss=5.731, nll_loss=3.884, ppl=14.76, wps=24147.7, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=41600, lr=0.000155043, gnorm=0.874, loss_scale=16, train_wall=257, gb_free=12.3, wall=108981
2022-03-04 17:14:25 | INFO | train_inner | epoch 107:    118 / 393 loss=5.632, nll_loss=3.769, ppl=13.64, wps=25161.4, ups=0.38, wpb=65536, bsz=128, num_updates=41700, lr=0.000154857, gnorm=0.863, loss_scale=16, train_wall=255, gb_free=12.3, wall=109242
2022-03-04 17:18:45 | INFO | train_inner | epoch 107:    218 / 393 loss=5.676, nll_loss=3.82, ppl=14.13, wps=25161.7, ups=0.38, wpb=65536, bsz=128, num_updates=41800, lr=0.000154672, gnorm=0.872, loss_scale=16, train_wall=255, gb_free=12.3, wall=109502
2022-03-04 17:23:06 | INFO | train_inner | epoch 107:    318 / 393 loss=5.722, nll_loss=3.873, ppl=14.65, wps=25160.8, ups=0.38, wpb=65536, bsz=128, num_updates=41900, lr=0.000154487, gnorm=0.874, loss_scale=16, train_wall=255, gb_free=12.3, wall=109763
2022-03-04 17:26:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:26:23 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 8.304 | nll_loss 6.744 | ppl 107.16 | wps 64005.9 | wpb 2034.1 | bsz 4 | num_updates 41975 | best_loss 7.638
2022-03-04 17:26:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 41975 updates
2022-03-04 17:26:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 17:26:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 17:26:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 107 @ 41975 updates, score 8.304) (writing took 4.694906831951812 seconds)
2022-03-04 17:26:28 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-04 17:26:28 | INFO | train | epoch 107 | loss 5.689 | nll_loss 3.835 | ppl 14.27 | wps 24956.9 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 41975 | lr 0.000154349 | gnorm 0.871 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 109965
2022-03-04 17:26:28 | INFO | fairseq.trainer | begin training epoch 108
2022-03-04 17:26:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:27:33 | INFO | train_inner | epoch 108:     25 / 393 loss=5.722, nll_loss=3.873, ppl=14.66, wps=24388, ups=0.37, wpb=65243.5, bsz=127.4, num_updates=42000, lr=0.000154303, gnorm=0.868, loss_scale=16, train_wall=254, gb_free=12.3, wall=110030
2022-03-04 17:31:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 17:31:56 | INFO | train_inner | epoch 108:    126 / 393 loss=5.632, nll_loss=3.769, ppl=13.63, wps=24919.2, ups=0.38, wpb=65535.4, bsz=128, num_updates=42100, lr=0.00015412, gnorm=0.862, loss_scale=16, train_wall=258, gb_free=12.3, wall=110293
2022-03-04 17:31:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:36:19 | INFO | train_inner | epoch 108:    227 / 393 loss=5.674, nll_loss=3.817, ppl=14.1, wps=24932.8, ups=0.38, wpb=65530.9, bsz=128, num_updates=42200, lr=0.000153937, gnorm=0.863, loss_scale=8, train_wall=258, gb_free=12.3, wall=110556
2022-03-04 17:40:39 | INFO | train_inner | epoch 108:    327 / 393 loss=5.724, nll_loss=3.876, ppl=14.68, wps=25174.2, ups=0.38, wpb=65536, bsz=128, num_updates=42300, lr=0.000153755, gnorm=0.87, loss_scale=8, train_wall=255, gb_free=12.3, wall=110816
2022-03-04 17:43:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:43:33 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 8.281 | nll_loss 6.715 | ppl 105.04 | wps 64559.3 | wpb 2034.1 | bsz 4 | num_updates 42366 | best_loss 7.638
2022-03-04 17:43:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 42366 updates
2022-03-04 17:43:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 17:43:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 17:43:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 108 @ 42366 updates, score 8.281) (writing took 4.663356194971129 seconds)
2022-03-04 17:43:38 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-04 17:43:38 | INFO | train | epoch 108 | loss 5.687 | nll_loss 3.832 | ppl 14.24 | wps 24848.5 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 42366 | lr 0.000153635 | gnorm 0.866 | loss_scale 8 | train_wall 1002 | gb_free 12.3 | wall 110995
2022-03-04 17:43:38 | INFO | fairseq.trainer | begin training epoch 109
2022-03-04 17:43:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:45:07 | INFO | train_inner | epoch 109:     34 / 393 loss=5.711, nll_loss=3.86, ppl=14.52, wps=24398.2, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=42400, lr=0.000153574, gnorm=0.864, loss_scale=8, train_wall=254, gb_free=12.3, wall=111084
2022-03-04 17:49:27 | INFO | train_inner | epoch 109:    134 / 393 loss=5.639, nll_loss=3.776, ppl=13.7, wps=25159, ups=0.38, wpb=65536, bsz=128, num_updates=42500, lr=0.000153393, gnorm=0.856, loss_scale=8, train_wall=255, gb_free=12.3, wall=111344
2022-03-04 17:53:48 | INFO | train_inner | epoch 109:    234 / 393 loss=5.677, nll_loss=3.821, ppl=14.13, wps=25170.7, ups=0.38, wpb=65530.2, bsz=128, num_updates=42600, lr=0.000153213, gnorm=0.861, loss_scale=8, train_wall=255, gb_free=12.3, wall=111605
2022-03-04 17:58:08 | INFO | train_inner | epoch 109:    334 / 393 loss=5.72, nll_loss=3.871, ppl=14.63, wps=25157, ups=0.38, wpb=65536, bsz=128, num_updates=42700, lr=0.000153033, gnorm=0.865, loss_scale=16, train_wall=255, gb_free=12.3, wall=111865
2022-03-04 18:00:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:00:44 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 8.306 | nll_loss 6.747 | ppl 107.37 | wps 64969.6 | wpb 2034.1 | bsz 4 | num_updates 42759 | best_loss 7.638
2022-03-04 18:00:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 42759 updates
2022-03-04 18:00:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 18:00:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 18:00:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 109 @ 42759 updates, score 8.306) (writing took 4.685716612963006 seconds)
2022-03-04 18:00:49 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-04 18:00:49 | INFO | train | epoch 109 | loss 5.685 | nll_loss 3.83 | ppl 14.22 | wps 24959.9 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 42759 | lr 0.000152928 | gnorm 0.862 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 112026
2022-03-04 18:00:49 | INFO | fairseq.trainer | begin training epoch 110
2022-03-04 18:00:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:02:36 | INFO | train_inner | epoch 110:     41 / 393 loss=5.695, nll_loss=3.841, ppl=14.33, wps=24395, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=42800, lr=0.000152854, gnorm=0.867, loss_scale=16, train_wall=254, gb_free=12.3, wall=112133
2022-03-04 18:06:56 | INFO | train_inner | epoch 110:    141 / 393 loss=5.638, nll_loss=3.776, ppl=13.7, wps=25145.1, ups=0.38, wpb=65535.4, bsz=128, num_updates=42900, lr=0.000152676, gnorm=0.863, loss_scale=16, train_wall=256, gb_free=12.3, wall=112393
2022-03-04 18:11:17 | INFO | train_inner | epoch 110:    241 / 393 loss=5.676, nll_loss=3.82, ppl=14.13, wps=25157.6, ups=0.38, wpb=65530.9, bsz=128, num_updates=43000, lr=0.000152499, gnorm=0.866, loss_scale=16, train_wall=255, gb_free=12.3, wall=112654
2022-03-04 18:15:37 | INFO | train_inner | epoch 110:    341 / 393 loss=5.725, nll_loss=3.877, ppl=14.69, wps=25155.7, ups=0.38, wpb=65536, bsz=128, num_updates=43100, lr=0.000152322, gnorm=0.882, loss_scale=16, train_wall=255, gb_free=12.3, wall=112914
2022-03-04 18:16:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 18:17:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:17:55 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 8.303 | nll_loss 6.732 | ppl 106.31 | wps 64762.3 | wpb 2034.1 | bsz 4 | num_updates 43151 | best_loss 7.638
2022-03-04 18:17:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 43151 updates
2022-03-04 18:17:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 18:17:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 18:18:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 110 @ 43151 updates, score 8.303) (writing took 4.753200773848221 seconds)
2022-03-04 18:18:00 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-04 18:18:00 | INFO | train | epoch 110 | loss 5.683 | nll_loss 3.828 | ppl 14.2 | wps 24893.2 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 43151 | lr 0.000152232 | gnorm 0.871 | loss_scale 16 | train_wall 1003 | gb_free 12.3 | wall 113057
2022-03-04 18:18:00 | INFO | fairseq.trainer | begin training epoch 111
2022-03-04 18:18:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:19:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:20:10 | INFO | train_inner | epoch 111:     50 / 393 loss=5.685, nll_loss=3.83, ppl=14.22, wps=23927.6, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=43200, lr=0.000152145, gnorm=0.875, loss_scale=8, train_wall=259, gb_free=12.3, wall=113187
2022-03-04 18:24:30 | INFO | train_inner | epoch 111:    150 / 393 loss=5.648, nll_loss=3.787, ppl=13.8, wps=25181.1, ups=0.38, wpb=65530.2, bsz=128, num_updates=43300, lr=0.000151969, gnorm=0.866, loss_scale=8, train_wall=255, gb_free=12.3, wall=113447
2022-03-04 18:28:50 | INFO | train_inner | epoch 111:    250 / 393 loss=5.676, nll_loss=3.819, ppl=14.12, wps=25186.1, ups=0.38, wpb=65536, bsz=128, num_updates=43400, lr=0.000151794, gnorm=0.871, loss_scale=8, train_wall=255, gb_free=12.3, wall=113708
2022-03-04 18:33:10 | INFO | train_inner | epoch 111:    350 / 393 loss=5.722, nll_loss=3.873, ppl=14.65, wps=25239.1, ups=0.39, wpb=65536, bsz=128, num_updates=43500, lr=0.00015162, gnorm=0.88, loss_scale=8, train_wall=255, gb_free=12.3, wall=113967
2022-03-04 18:35:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:35:04 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 8.31 | nll_loss 6.749 | ppl 107.58 | wps 65605.3 | wpb 2034.1 | bsz 4 | num_updates 43543 | best_loss 7.638
2022-03-04 18:35:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 43543 updates
2022-03-04 18:35:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 18:35:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 18:35:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 111 @ 43543 updates, score 8.31) (writing took 4.760819399030879 seconds)
2022-03-04 18:35:08 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-04 18:35:08 | INFO | train | epoch 111 | loss 5.681 | nll_loss 3.825 | ppl 14.17 | wps 24944.9 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 43543 | lr 0.000151545 | gnorm 0.872 | loss_scale 8 | train_wall 1001 | gb_free 12.3 | wall 114086
2022-03-04 18:35:08 | INFO | fairseq.trainer | begin training epoch 112
2022-03-04 18:35:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:37:36 | INFO | train_inner | epoch 112:     57 / 393 loss=5.671, nll_loss=3.814, ppl=14.06, wps=24529.3, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=43600, lr=0.000151446, gnorm=0.863, loss_scale=8, train_wall=253, gb_free=12.3, wall=114233
2022-03-04 18:41:55 | INFO | train_inner | epoch 112:    157 / 393 loss=5.642, nll_loss=3.781, ppl=13.74, wps=25303.4, ups=0.39, wpb=65530.9, bsz=128, num_updates=43700, lr=0.000151272, gnorm=0.874, loss_scale=16, train_wall=254, gb_free=12.3, wall=114492
2022-03-04 18:46:14 | INFO | train_inner | epoch 112:    257 / 393 loss=5.683, nll_loss=3.828, ppl=14.2, wps=25295.5, ups=0.39, wpb=65535.4, bsz=128, num_updates=43800, lr=0.000151099, gnorm=0.879, loss_scale=16, train_wall=254, gb_free=12.3, wall=114751
2022-03-04 18:50:33 | INFO | train_inner | epoch 112:    357 / 393 loss=5.727, nll_loss=3.879, ppl=14.72, wps=25298.7, ups=0.39, wpb=65536, bsz=128, num_updates=43900, lr=0.000150927, gnorm=0.881, loss_scale=16, train_wall=254, gb_free=12.3, wall=115010
2022-03-04 18:52:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:52:09 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 8.314 | nll_loss 6.756 | ppl 108.08 | wps 65712 | wpb 2034.1 | bsz 4 | num_updates 43936 | best_loss 7.638
2022-03-04 18:52:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 43936 updates
2022-03-04 18:52:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 18:52:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 18:52:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 112 @ 43936 updates, score 8.314) (writing took 4.523121457081288 seconds)
2022-03-04 18:52:13 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-04 18:52:13 | INFO | train | epoch 112 | loss 5.679 | nll_loss 3.823 | ppl 14.16 | wps 25103.2 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 43936 | lr 0.000150865 | gnorm 0.875 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 115110
2022-03-04 18:52:13 | INFO | fairseq.trainer | begin training epoch 113
2022-03-04 18:52:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:54:59 | INFO | train_inner | epoch 113:     64 / 393 loss=5.664, nll_loss=3.805, ppl=13.98, wps=24536.5, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=44000, lr=0.000150756, gnorm=0.875, loss_scale=16, train_wall=253, gb_free=12.3, wall=115276
2022-03-04 18:59:18 | INFO | train_inner | epoch 113:    164 / 393 loss=5.645, nll_loss=3.783, ppl=13.77, wps=25282.9, ups=0.39, wpb=65535.4, bsz=128, num_updates=44100, lr=0.000150585, gnorm=0.877, loss_scale=16, train_wall=254, gb_free=12.3, wall=115535
2022-03-04 19:03:37 | INFO | train_inner | epoch 113:    264 / 393 loss=5.683, nll_loss=3.828, ppl=14.2, wps=25291.5, ups=0.39, wpb=65536, bsz=128, num_updates=44200, lr=0.000150414, gnorm=0.884, loss_scale=16, train_wall=254, gb_free=12.3, wall=115795
2022-03-04 19:04:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:07:59 | INFO | train_inner | epoch 113:    365 / 393 loss=5.726, nll_loss=3.877, ppl=14.69, wps=25037.6, ups=0.38, wpb=65536, bsz=128, num_updates=44300, lr=0.000150244, gnorm=0.878, loss_scale=16, train_wall=257, gb_free=12.3, wall=116056
2022-03-04 19:09:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:09:14 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 8.344 | nll_loss 6.794 | ppl 110.96 | wps 65716.3 | wpb 2034.1 | bsz 4 | num_updates 44328 | best_loss 7.638
2022-03-04 19:09:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 44328 updates
2022-03-04 19:09:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 19:09:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 19:09:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 113 @ 44328 updates, score 8.344) (writing took 4.37585839885287 seconds)
2022-03-04 19:09:18 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-04 19:09:18 | INFO | train | epoch 113 | loss 5.677 | nll_loss 3.82 | ppl 14.13 | wps 25032.5 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 44328 | lr 0.000150197 | gnorm 0.878 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 116135
2022-03-04 19:09:18 | INFO | fairseq.trainer | begin training epoch 114
2022-03-04 19:09:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:12:25 | INFO | train_inner | epoch 114:     72 / 393 loss=5.645, nll_loss=3.784, ppl=13.77, wps=24554.5, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=44400, lr=0.000150075, gnorm=0.886, loss_scale=16, train_wall=253, gb_free=12.3, wall=116322
2022-03-04 19:16:44 | INFO | train_inner | epoch 114:    172 / 393 loss=5.638, nll_loss=3.775, ppl=13.69, wps=25284.1, ups=0.39, wpb=65536, bsz=128, num_updates=44500, lr=0.000149906, gnorm=0.878, loss_scale=16, train_wall=254, gb_free=12.3, wall=116581
2022-03-04 19:21:03 | INFO | train_inner | epoch 114:    272 / 393 loss=5.686, nll_loss=3.831, ppl=14.23, wps=25288.8, ups=0.39, wpb=65536, bsz=128, num_updates=44600, lr=0.000149738, gnorm=0.883, loss_scale=16, train_wall=254, gb_free=12.3, wall=116840
2022-03-04 19:25:22 | INFO | train_inner | epoch 114:    372 / 393 loss=5.731, nll_loss=3.884, ppl=14.76, wps=25278.2, ups=0.39, wpb=65535.4, bsz=128, num_updates=44700, lr=0.000149571, gnorm=0.877, loss_scale=16, train_wall=254, gb_free=12.3, wall=117100
2022-03-04 19:26:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:26:19 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 8.326 | nll_loss 6.768 | ppl 108.99 | wps 65818.4 | wpb 2034.1 | bsz 4 | num_updates 44721 | best_loss 7.638
2022-03-04 19:26:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 44721 updates
2022-03-04 19:26:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 19:26:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 19:26:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 114 @ 44721 updates, score 8.326) (writing took 4.340381562011316 seconds)
2022-03-04 19:26:23 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-04 19:26:23 | INFO | train | epoch 114 | loss 5.675 | nll_loss 3.818 | ppl 14.1 | wps 25095 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 44721 | lr 0.000149535 | gnorm 0.881 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 117161
2022-03-04 19:26:23 | INFO | fairseq.trainer | begin training epoch 115
2022-03-04 19:26:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:26:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:29:51 | INFO | train_inner | epoch 115:     80 / 393 loss=5.639, nll_loss=3.777, ppl=13.71, wps=24324.7, ups=0.37, wpb=65244.2, bsz=127.4, num_updates=44800, lr=0.000149404, gnorm=0.874, loss_scale=16, train_wall=255, gb_free=12.3, wall=117368
2022-03-04 19:34:10 | INFO | train_inner | epoch 115:    180 / 393 loss=5.64, nll_loss=3.778, ppl=13.72, wps=25288.6, ups=0.39, wpb=65536, bsz=128, num_updates=44900, lr=0.000149237, gnorm=0.88, loss_scale=16, train_wall=254, gb_free=12.3, wall=117627
2022-03-04 19:38:29 | INFO | train_inner | epoch 115:    280 / 393 loss=5.686, nll_loss=3.831, ppl=14.23, wps=25301.6, ups=0.39, wpb=65535.4, bsz=128, num_updates=45000, lr=0.000149071, gnorm=0.881, loss_scale=16, train_wall=254, gb_free=12.3, wall=117886
2022-03-04 19:42:48 | INFO | train_inner | epoch 115:    380 / 393 loss=5.731, nll_loss=3.883, ppl=14.76, wps=25278.9, ups=0.39, wpb=65536, bsz=128, num_updates=45100, lr=0.000148906, gnorm=0.889, loss_scale=16, train_wall=254, gb_free=12.3, wall=118145
2022-03-04 19:43:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:43:24 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 8.313 | nll_loss 6.753 | ppl 107.88 | wps 65395.2 | wpb 2034.1 | bsz 4 | num_updates 45113 | best_loss 7.638
2022-03-04 19:43:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 45113 updates
2022-03-04 19:43:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 19:43:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 19:43:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 115 @ 45113 updates, score 8.313) (writing took 4.396145164966583 seconds)
2022-03-04 19:43:28 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-04 19:43:28 | INFO | train | epoch 115 | loss 5.672 | nll_loss 3.815 | ppl 14.08 | wps 25034.7 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 45113 | lr 0.000148884 | gnorm 0.882 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 118186
2022-03-04 19:43:28 | INFO | fairseq.trainer | begin training epoch 116
2022-03-04 19:43:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:47:14 | INFO | train_inner | epoch 116:     87 / 393 loss=5.629, nll_loss=3.765, ppl=13.6, wps=24541.2, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=45200, lr=0.000148741, gnorm=0.876, loss_scale=16, train_wall=253, gb_free=12.3, wall=118411
2022-03-04 19:49:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 19:51:36 | INFO | train_inner | epoch 116:    188 / 393 loss=5.648, nll_loss=3.787, ppl=13.8, wps=25047.6, ups=0.38, wpb=65536, bsz=128, num_updates=45300, lr=0.000148577, gnorm=0.875, loss_scale=16, train_wall=257, gb_free=12.3, wall=118673
2022-03-04 19:55:55 | INFO | train_inner | epoch 116:    288 / 393 loss=5.684, nll_loss=3.829, ppl=14.21, wps=25289.6, ups=0.39, wpb=65530.9, bsz=128, num_updates=45400, lr=0.000148413, gnorm=0.881, loss_scale=16, train_wall=254, gb_free=12.3, wall=118932
2022-03-04 19:55:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:00:16 | INFO | train_inner | epoch 116:    389 / 393 loss=5.729, nll_loss=3.882, ppl=14.74, wps=25075, ups=0.38, wpb=65535.4, bsz=128, num_updates=45500, lr=0.00014825, gnorm=0.891, loss_scale=8, train_wall=256, gb_free=12.3, wall=119193
2022-03-04 20:00:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:00:29 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 8.317 | nll_loss 6.759 | ppl 108.3 | wps 65586.3 | wpb 2034.1 | bsz 4 | num_updates 45504 | best_loss 7.638
2022-03-04 20:00:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 45504 updates
2022-03-04 20:00:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 20:00:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 20:00:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 116 @ 45504 updates, score 8.317) (writing took 4.319218687014654 seconds)
2022-03-04 20:00:33 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-04 20:00:33 | INFO | train | epoch 116 | loss 5.67 | nll_loss 3.813 | ppl 14.06 | wps 24981.3 | ups 0.38 | wpb 65461.2 | bsz 127.9 | num_updates 45504 | lr 0.000148243 | gnorm 0.881 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 119210
2022-03-04 20:00:33 | INFO | fairseq.trainer | begin training epoch 117
2022-03-04 20:00:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:04:42 | INFO | train_inner | epoch 117:     96 / 393 loss=5.613, nll_loss=3.747, ppl=13.42, wps=24575.1, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=45600, lr=0.000148087, gnorm=0.871, loss_scale=8, train_wall=253, gb_free=12.3, wall=119459
2022-03-04 20:09:00 | INFO | train_inner | epoch 117:    196 / 393 loss=5.646, nll_loss=3.785, ppl=13.78, wps=25319, ups=0.39, wpb=65530.2, bsz=128, num_updates=45700, lr=0.000147925, gnorm=0.876, loss_scale=8, train_wall=254, gb_free=12.3, wall=119718
2022-03-04 20:13:19 | INFO | train_inner | epoch 117:    296 / 393 loss=5.688, nll_loss=3.833, ppl=14.26, wps=25309.2, ups=0.39, wpb=65536, bsz=128, num_updates=45800, lr=0.000147764, gnorm=0.891, loss_scale=8, train_wall=254, gb_free=12.3, wall=119977
2022-03-04 20:17:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:17:33 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 8.319 | nll_loss 6.757 | ppl 108.18 | wps 65827.9 | wpb 2034.1 | bsz 4 | num_updates 45897 | best_loss 7.638
2022-03-04 20:17:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 45897 updates
2022-03-04 20:17:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 20:17:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 20:17:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 117 @ 45897 updates, score 8.319) (writing took 4.297247146023437 seconds)
2022-03-04 20:17:37 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-04 20:17:37 | INFO | train | epoch 117 | loss 5.669 | nll_loss 3.812 | ppl 14.05 | wps 25124.5 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 45897 | lr 0.000147607 | gnorm 0.881 | loss_scale 8 | train_wall 997 | gb_free 12.3 | wall 120234
2022-03-04 20:17:37 | INFO | fairseq.trainer | begin training epoch 118
2022-03-04 20:17:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:17:45 | INFO | train_inner | epoch 118:      3 / 393 loss=5.732, nll_loss=3.885, ppl=14.77, wps=24576.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=45900, lr=0.000147602, gnorm=0.888, loss_scale=8, train_wall=253, gb_free=12.3, wall=120242
2022-03-04 20:22:04 | INFO | train_inner | epoch 118:    103 / 393 loss=5.607, nll_loss=3.74, ppl=13.36, wps=25315.4, ups=0.39, wpb=65535.4, bsz=128, num_updates=46000, lr=0.000147442, gnorm=0.876, loss_scale=16, train_wall=254, gb_free=12.3, wall=120501
2022-03-04 20:26:23 | INFO | train_inner | epoch 118:    203 / 393 loss=5.649, nll_loss=3.788, ppl=13.81, wps=25296.1, ups=0.39, wpb=65536, bsz=128, num_updates=46100, lr=0.000147282, gnorm=0.879, loss_scale=16, train_wall=254, gb_free=12.3, wall=120760
2022-03-04 20:30:42 | INFO | train_inner | epoch 118:    303 / 393 loss=5.687, nll_loss=3.833, ppl=14.25, wps=25304.1, ups=0.39, wpb=65536, bsz=128, num_updates=46200, lr=0.000147122, gnorm=0.879, loss_scale=16, train_wall=254, gb_free=12.3, wall=121019
2022-03-04 20:34:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:34:37 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 8.317 | nll_loss 6.76 | ppl 108.36 | wps 66025.6 | wpb 2034.1 | bsz 4 | num_updates 46290 | best_loss 7.638
2022-03-04 20:34:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 46290 updates
2022-03-04 20:34:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 20:34:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 20:34:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 118 @ 46290 updates, score 8.317) (writing took 4.19960849895142 seconds)
2022-03-04 20:34:41 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-04 20:34:41 | INFO | train | epoch 118 | loss 5.667 | nll_loss 3.809 | ppl 14.02 | wps 25114.2 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 46290 | lr 0.000146979 | gnorm 0.881 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 121259
2022-03-04 20:34:41 | INFO | fairseq.trainer | begin training epoch 119
2022-03-04 20:34:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:35:07 | INFO | train_inner | epoch 119:     10 / 393 loss=5.72, nll_loss=3.87, ppl=14.62, wps=24568.8, ups=0.38, wpb=65244.2, bsz=127.4, num_updates=46300, lr=0.000146964, gnorm=0.891, loss_scale=16, train_wall=253, gb_free=12.3, wall=121285
2022-03-04 20:39:26 | INFO | train_inner | epoch 119:    110 / 393 loss=5.603, nll_loss=3.735, ppl=13.31, wps=25296.9, ups=0.39, wpb=65536, bsz=128, num_updates=46400, lr=0.000146805, gnorm=0.878, loss_scale=16, train_wall=254, gb_free=12.3, wall=121544
2022-03-04 20:40:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 20:43:48 | INFO | train_inner | epoch 119:    211 / 393 loss=5.654, nll_loss=3.794, ppl=13.87, wps=25036.9, ups=0.38, wpb=65535.4, bsz=128, num_updates=46500, lr=0.000146647, gnorm=0.878, loss_scale=16, train_wall=257, gb_free=12.3, wall=121805
2022-03-04 20:48:07 | INFO | train_inner | epoch 119:    311 / 393 loss=5.687, nll_loss=3.833, ppl=14.25, wps=25286.3, ups=0.39, wpb=65530.9, bsz=128, num_updates=46600, lr=0.00014649, gnorm=0.886, loss_scale=16, train_wall=254, gb_free=12.3, wall=122064
2022-03-04 20:51:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:51:42 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 8.334 | nll_loss 6.778 | ppl 109.73 | wps 65611.3 | wpb 2034.1 | bsz 4 | num_updates 46682 | best_loss 7.638
2022-03-04 20:51:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 46682 updates
2022-03-04 20:51:42 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 20:51:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 20:51:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 119 @ 46682 updates, score 8.334) (writing took 4.162593387067318 seconds)
2022-03-04 20:51:46 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-04 20:51:46 | INFO | train | epoch 119 | loss 5.665 | nll_loss 3.807 | ppl 13.99 | wps 25032.4 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 46682 | lr 0.000146361 | gnorm 0.882 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 122284
2022-03-04 20:51:46 | INFO | fairseq.trainer | begin training epoch 120
2022-03-04 20:51:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:52:33 | INFO | train_inner | epoch 120:     18 / 393 loss=5.704, nll_loss=3.853, ppl=14.45, wps=24546.1, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=46700, lr=0.000146333, gnorm=0.883, loss_scale=16, train_wall=253, gb_free=12.3, wall=122330
2022-03-04 20:56:52 | INFO | train_inner | epoch 120:    118 / 393 loss=5.607, nll_loss=3.74, ppl=13.36, wps=25289.2, ups=0.39, wpb=65536, bsz=128, num_updates=46800, lr=0.000146176, gnorm=0.879, loss_scale=16, train_wall=254, gb_free=12.3, wall=122589
2022-03-04 21:01:11 | INFO | train_inner | epoch 120:    218 / 393 loss=5.65, nll_loss=3.79, ppl=13.83, wps=25304.3, ups=0.39, wpb=65535.4, bsz=128, num_updates=46900, lr=0.00014602, gnorm=0.877, loss_scale=16, train_wall=254, gb_free=12.3, wall=122848
2022-03-04 21:03:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:05:33 | INFO | train_inner | epoch 120:    319 / 393 loss=5.696, nll_loss=3.843, ppl=14.35, wps=25048.5, ups=0.38, wpb=65530.9, bsz=128, num_updates=47000, lr=0.000145865, gnorm=0.902, loss_scale=16, train_wall=257, gb_free=12.3, wall=123110
2022-03-04 21:08:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:08:47 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 8.315 | nll_loss 6.758 | ppl 108.21 | wps 65424.7 | wpb 2034.1 | bsz 4 | num_updates 47074 | best_loss 7.638
2022-03-04 21:08:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 47074 updates
2022-03-04 21:08:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 21:08:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 21:08:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 120 @ 47074 updates, score 8.315) (writing took 4.183717443840578 seconds)
2022-03-04 21:08:51 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-04 21:08:51 | INFO | train | epoch 120 | loss 5.663 | nll_loss 3.805 | ppl 13.97 | wps 25040.4 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 47074 | lr 0.00014575 | gnorm 0.887 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 123308
2022-03-04 21:08:51 | INFO | fairseq.trainer | begin training epoch 121
2022-03-04 21:08:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:09:59 | INFO | train_inner | epoch 121:     26 / 393 loss=5.696, nll_loss=3.844, ppl=14.36, wps=24552.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=47100, lr=0.00014571, gnorm=0.889, loss_scale=16, train_wall=253, gb_free=12.3, wall=123376
2022-03-04 21:14:18 | INFO | train_inner | epoch 121:    126 / 393 loss=5.611, nll_loss=3.744, ppl=13.4, wps=25298.9, ups=0.39, wpb=65536, bsz=128, num_updates=47200, lr=0.000145556, gnorm=0.884, loss_scale=16, train_wall=254, gb_free=12.3, wall=123635
2022-03-04 21:18:37 | INFO | train_inner | epoch 121:    226 / 393 loss=5.649, nll_loss=3.788, ppl=13.81, wps=25301.9, ups=0.39, wpb=65535.4, bsz=128, num_updates=47300, lr=0.000145402, gnorm=0.892, loss_scale=16, train_wall=254, gb_free=12.3, wall=123894
2022-03-04 21:22:56 | INFO | train_inner | epoch 121:    326 / 393 loss=5.698, nll_loss=3.845, ppl=14.37, wps=25285.9, ups=0.39, wpb=65530.9, bsz=128, num_updates=47400, lr=0.000145248, gnorm=0.881, loss_scale=16, train_wall=254, gb_free=12.3, wall=124153
2022-03-04 21:25:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 21:25:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:25:52 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 8.319 | nll_loss 6.757 | ppl 108.14 | wps 65831.8 | wpb 2034.1 | bsz 4 | num_updates 47466 | best_loss 7.638
2022-03-04 21:25:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 47466 updates
2022-03-04 21:25:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 21:25:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 21:25:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 121 @ 47466 updates, score 8.319) (writing took 4.206587475026026 seconds)
2022-03-04 21:25:56 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-04 21:25:56 | INFO | train | epoch 121 | loss 5.661 | nll_loss 3.802 | ppl 13.95 | wps 25045 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 47466 | lr 0.000145147 | gnorm 0.888 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 124333
2022-03-04 21:25:56 | INFO | fairseq.trainer | begin training epoch 122
2022-03-04 21:25:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:27:24 | INFO | train_inner | epoch 122:     34 / 393 loss=5.68, nll_loss=3.825, ppl=14.17, wps=24331.6, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=47500, lr=0.000145095, gnorm=0.891, loss_scale=16, train_wall=256, gb_free=12.3, wall=124421
2022-03-04 21:31:43 | INFO | train_inner | epoch 122:    134 / 393 loss=5.612, nll_loss=3.745, ppl=13.41, wps=25292.7, ups=0.39, wpb=65530.2, bsz=128, num_updates=47600, lr=0.000144943, gnorm=0.878, loss_scale=16, train_wall=254, gb_free=12.3, wall=124680
2022-03-04 21:36:02 | INFO | train_inner | epoch 122:    234 / 393 loss=5.656, nll_loss=3.797, ppl=13.9, wps=25312.8, ups=0.39, wpb=65536, bsz=128, num_updates=47700, lr=0.000144791, gnorm=0.896, loss_scale=16, train_wall=254, gb_free=12.3, wall=124939
2022-03-04 21:40:21 | INFO | train_inner | epoch 122:    334 / 393 loss=5.697, nll_loss=3.844, ppl=14.36, wps=25285, ups=0.39, wpb=65536, bsz=128, num_updates=47800, lr=0.000144639, gnorm=0.876, loss_scale=16, train_wall=254, gb_free=12.3, wall=125198
2022-03-04 21:41:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:42:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:42:56 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 8.311 | nll_loss 6.751 | ppl 107.74 | wps 65844.4 | wpb 2034.1 | bsz 4 | num_updates 47858 | best_loss 7.638
2022-03-04 21:42:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 47858 updates
2022-03-04 21:42:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 21:43:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 21:43:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 122 @ 47858 updates, score 8.311) (writing took 4.167639413150027 seconds)
2022-03-04 21:43:00 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-04 21:43:00 | INFO | train | epoch 122 | loss 5.66 | nll_loss 3.801 | ppl 13.94 | wps 25045 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 47858 | lr 0.000144552 | gnorm 0.887 | loss_scale 8 | train_wall 998 | gb_free 12.3 | wall 125358
2022-03-04 21:43:00 | INFO | fairseq.trainer | begin training epoch 123
2022-03-04 21:43:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:44:49 | INFO | train_inner | epoch 123:     42 / 393 loss=5.666, nll_loss=3.808, ppl=14.01, wps=24351.4, ups=0.37, wpb=65249.3, bsz=127.4, num_updates=47900, lr=0.000144488, gnorm=0.896, loss_scale=8, train_wall=255, gb_free=12.3, wall=125466
2022-03-04 21:49:08 | INFO | train_inner | epoch 123:    142 / 393 loss=5.621, nll_loss=3.756, ppl=13.51, wps=25327, ups=0.39, wpb=65536, bsz=128, num_updates=48000, lr=0.000144338, gnorm=0.886, loss_scale=8, train_wall=254, gb_free=12.3, wall=125725
2022-03-04 21:53:27 | INFO | train_inner | epoch 123:    242 / 393 loss=5.657, nll_loss=3.797, ppl=13.9, wps=25300.7, ups=0.39, wpb=65530.2, bsz=128, num_updates=48100, lr=0.000144187, gnorm=0.887, loss_scale=8, train_wall=254, gb_free=12.3, wall=125984
2022-03-04 21:57:46 | INFO | train_inner | epoch 123:    342 / 393 loss=5.694, nll_loss=3.841, ppl=14.33, wps=25302.8, ups=0.39, wpb=65536, bsz=128, num_updates=48200, lr=0.000144038, gnorm=0.891, loss_scale=8, train_wall=254, gb_free=12.3, wall=126243
2022-03-04 21:59:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:00:00 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 8.314 | nll_loss 6.759 | ppl 108.34 | wps 66004.8 | wpb 2034.1 | bsz 4 | num_updates 48251 | best_loss 7.638
2022-03-04 22:00:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 48251 updates
2022-03-04 22:00:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 22:00:04 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 22:00:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 123 @ 48251 updates, score 8.314) (writing took 4.2327691270038486 seconds)
2022-03-04 22:00:04 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-04 22:00:04 | INFO | train | epoch 123 | loss 5.658 | nll_loss 3.799 | ppl 13.92 | wps 25120.9 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 48251 | lr 0.000143962 | gnorm 0.886 | loss_scale 8 | train_wall 997 | gb_free 12.3 | wall 126382
2022-03-04 22:00:05 | INFO | fairseq.trainer | begin training epoch 124
2022-03-04 22:00:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:02:11 | INFO | train_inner | epoch 124:     49 / 393 loss=5.658, nll_loss=3.799, ppl=13.92, wps=24574.3, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=48300, lr=0.000143889, gnorm=0.881, loss_scale=8, train_wall=253, gb_free=12.3, wall=126509
2022-03-04 22:06:30 | INFO | train_inner | epoch 124:    149 / 393 loss=5.619, nll_loss=3.754, ppl=13.49, wps=25298.6, ups=0.39, wpb=65536, bsz=128, num_updates=48400, lr=0.00014374, gnorm=0.88, loss_scale=16, train_wall=254, gb_free=12.3, wall=126768
2022-03-04 22:10:50 | INFO | train_inner | epoch 124:    249 / 393 loss=5.653, nll_loss=3.793, ppl=13.86, wps=25296.4, ups=0.39, wpb=65536, bsz=128, num_updates=48500, lr=0.000143592, gnorm=0.902, loss_scale=16, train_wall=254, gb_free=12.3, wall=127027
2022-03-04 22:15:09 | INFO | train_inner | epoch 124:    349 / 393 loss=5.697, nll_loss=3.843, ppl=14.35, wps=25294.9, ups=0.39, wpb=65530.2, bsz=128, num_updates=48600, lr=0.000143444, gnorm=0.896, loss_scale=16, train_wall=254, gb_free=12.3, wall=127286
2022-03-04 22:17:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:17:05 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 8.344 | nll_loss 6.787 | ppl 110.39 | wps 65387.6 | wpb 2034.1 | bsz 4 | num_updates 48644 | best_loss 7.638
2022-03-04 22:17:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 48644 updates
2022-03-04 22:17:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 22:17:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 22:17:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 124 @ 48644 updates, score 8.344) (writing took 4.217215128941461 seconds)
2022-03-04 22:17:09 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-04 22:17:09 | INFO | train | epoch 124 | loss 5.657 | nll_loss 3.797 | ppl 13.9 | wps 25108.3 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 48644 | lr 0.000143379 | gnorm 0.892 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 127406
2022-03-04 22:17:09 | INFO | fairseq.trainer | begin training epoch 125
2022-03-04 22:17:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:19:34 | INFO | train_inner | epoch 125:     56 / 393 loss=5.651, nll_loss=3.79, ppl=13.83, wps=24558.6, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=48700, lr=0.000143296, gnorm=0.885, loss_scale=16, train_wall=253, gb_free=12.3, wall=127552
2022-03-04 22:23:54 | INFO | train_inner | epoch 125:    156 / 393 loss=5.618, nll_loss=3.752, ppl=13.47, wps=25283.1, ups=0.39, wpb=65530.2, bsz=128, num_updates=48800, lr=0.00014315, gnorm=0.883, loss_scale=16, train_wall=254, gb_free=12.3, wall=127811
2022-03-04 22:26:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:28:15 | INFO | train_inner | epoch 125:    257 / 393 loss=5.652, nll_loss=3.792, ppl=13.85, wps=25047, ups=0.38, wpb=65536, bsz=128, num_updates=48900, lr=0.000143003, gnorm=0.889, loss_scale=16, train_wall=257, gb_free=12.3, wall=128072
2022-03-04 22:32:34 | INFO | train_inner | epoch 125:    357 / 393 loss=5.7, nll_loss=3.848, ppl=14.4, wps=25289.5, ups=0.39, wpb=65536, bsz=128, num_updates=49000, lr=0.000142857, gnorm=0.902, loss_scale=16, train_wall=254, gb_free=12.3, wall=128332
2022-03-04 22:34:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:34:10 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 8.335 | nll_loss 6.774 | ppl 109.42 | wps 65976.3 | wpb 2034.1 | bsz 4 | num_updates 49036 | best_loss 7.638
2022-03-04 22:34:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 49036 updates
2022-03-04 22:34:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 22:34:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 22:34:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 125 @ 49036 updates, score 8.335) (writing took 4.191357041010633 seconds)
2022-03-04 22:34:14 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-04 22:34:14 | INFO | train | epoch 125 | loss 5.654 | nll_loss 3.794 | ppl 13.87 | wps 25036.4 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 49036 | lr 0.000142805 | gnorm 0.892 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 128431
2022-03-04 22:34:14 | INFO | fairseq.trainer | begin training epoch 126
2022-03-04 22:34:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:37:00 | INFO | train_inner | epoch 126:     64 / 393 loss=5.635, nll_loss=3.772, ppl=13.66, wps=24559.1, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=49100, lr=0.000142712, gnorm=0.896, loss_scale=16, train_wall=253, gb_free=12.3, wall=128597
2022-03-04 22:41:19 | INFO | train_inner | epoch 126:    164 / 393 loss=5.619, nll_loss=3.753, ppl=13.49, wps=25300.6, ups=0.39, wpb=65530.2, bsz=128, num_updates=49200, lr=0.000142566, gnorm=0.893, loss_scale=16, train_wall=254, gb_free=12.3, wall=128856
2022-03-04 22:45:38 | INFO | train_inner | epoch 126:    264 / 393 loss=5.663, nll_loss=3.804, ppl=13.97, wps=25267.1, ups=0.39, wpb=65536, bsz=128, num_updates=49300, lr=0.000142422, gnorm=0.9, loss_scale=16, train_wall=254, gb_free=12.3, wall=129116
2022-03-04 22:49:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 22:50:01 | INFO | train_inner | epoch 126:    365 / 393 loss=5.701, nll_loss=3.849, ppl=14.41, wps=24996.7, ups=0.38, wpb=65536, bsz=128, num_updates=49400, lr=0.000142278, gnorm=0.896, loss_scale=16, train_wall=257, gb_free=12.3, wall=129378
2022-03-04 22:51:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:51:15 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 8.331 | nll_loss 6.776 | ppl 109.56 | wps 65855.8 | wpb 2034.1 | bsz 4 | num_updates 49428 | best_loss 7.638
2022-03-04 22:51:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 49428 updates
2022-03-04 22:51:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 22:51:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 22:51:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 126 @ 49428 updates, score 8.331) (writing took 4.201456486014649 seconds)
2022-03-04 22:51:20 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-04 22:51:20 | INFO | train | epoch 126 | loss 5.652 | nll_loss 3.792 | ppl 13.85 | wps 25019.2 | ups 0.38 | wpb 65461.4 | bsz 127.9 | num_updates 49428 | lr 0.000142237 | gnorm 0.895 | loss_scale 16 | train_wall 999 | gb_free 12.3 | wall 129457
2022-03-04 22:51:20 | INFO | fairseq.trainer | begin training epoch 127
2022-03-04 22:51:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:54:26 | INFO | train_inner | epoch 127:     72 / 393 loss=5.629, nll_loss=3.765, ppl=13.6, wps=24551.3, ups=0.38, wpb=65249.3, bsz=127.4, num_updates=49500, lr=0.000142134, gnorm=0.892, loss_scale=16, train_wall=253, gb_free=12.3, wall=129644
2022-03-04 22:58:46 | INFO | train_inner | epoch 127:    172 / 393 loss=5.617, nll_loss=3.75, ppl=13.46, wps=25277.4, ups=0.39, wpb=65536, bsz=128, num_updates=49600, lr=0.00014199, gnorm=0.875, loss_scale=16, train_wall=254, gb_free=12.3, wall=129903
2022-03-04 23:03:05 | INFO | train_inner | epoch 127:    272 / 393 loss=5.664, nll_loss=3.805, ppl=13.98, wps=25287.5, ups=0.39, wpb=65530.9, bsz=128, num_updates=49700, lr=0.000141848, gnorm=0.888, loss_scale=16, train_wall=254, gb_free=12.3, wall=130162
2022-03-04 23:07:24 | INFO | train_inner | epoch 127:    372 / 393 loss=5.701, nll_loss=3.848, ppl=14.4, wps=25272, ups=0.39, wpb=65536, bsz=128, num_updates=49800, lr=0.000141705, gnorm=0.887, loss_scale=16, train_wall=254, gb_free=12.3, wall=130421
2022-03-04 23:08:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:08:21 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 8.36 | nll_loss 6.802 | ppl 111.6 | wps 65713.8 | wpb 2034.1 | bsz 4 | num_updates 49821 | best_loss 7.638
2022-03-04 23:08:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 49821 updates
2022-03-04 23:08:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 23:08:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 23:08:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 127 @ 49821 updates, score 8.36) (writing took 4.213805630104616 seconds)
2022-03-04 23:08:25 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-04 23:08:25 | INFO | train | epoch 127 | loss 5.65 | nll_loss 3.79 | ppl 13.83 | wps 25093.2 | ups 0.38 | wpb 65461.6 | bsz 127.9 | num_updates 49821 | lr 0.000141675 | gnorm 0.884 | loss_scale 16 | train_wall 998 | gb_free 12.3 | wall 130482
2022-03-04 23:08:25 | INFO | fairseq.trainer | begin training epoch 128
2022-03-04 23:08:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:11:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 23:11:52 | INFO | train_inner | epoch 128:     80 / 393 loss=5.611, nll_loss=3.744, ppl=13.4, wps=24327, ups=0.37, wpb=65248.6, bsz=127.4, num_updates=49900, lr=0.000141563, gnorm=0.897, loss_scale=16, train_wall=256, gb_free=12.3, wall=130689
2022-03-04 23:16:11 | INFO | train_inner | epoch 128:    180 / 393 loss=5.62, nll_loss=3.754, ppl=13.49, wps=25297.8, ups=0.39, wpb=65530.9, bsz=128, num_updates=50000, lr=0.000141421, gnorm=0.88, loss_scale=16, train_wall=254, gb_free=12.3, wall=130949
2022-03-04 23:16:11 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2022-03-04 23:16:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:16:15 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 8.42 | nll_loss 6.856 | ppl 115.88 | wps 65972.7 | wpb 2034.1 | bsz 4 | num_updates 50000 | best_loss 7.638
2022-03-04 23:16:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 50000 updates
2022-03-04 23:16:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 23:16:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt
2022-03-04 23:16:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.25_fp16-label_smoothing_0.1_#3/checkpoint_last.pt (epoch 128 @ 50000 updates, score 8.42) (writing took 4.193654532078654 seconds)
2022-03-04 23:16:19 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-04 23:16:19 | INFO | train | epoch 128 | loss 5.603 | nll_loss 3.735 | ppl 13.32 | wps 24750.8 | ups 0.38 | wpb 65533.1 | bsz 128 | num_updates 50000 | lr 0.000141421 | gnorm 0.889 | loss_scale 16 | train_wall 457 | gb_free 12.3 | wall 130956
2022-03-04 23:16:19 | INFO | fairseq_cli.train | done training in 130955.4 seconds
