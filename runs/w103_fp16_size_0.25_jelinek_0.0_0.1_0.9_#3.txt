Sender: LSF System <lsfadmin@eu-g3-071>
Subject: Job 207019779: <w103_fp16_size_0.25_jelinek_0.0_0.1_0.9_#3> in cluster <euler> Exited

Job <w103_fp16_size_0.25_jelinek_0.0_0.1_0.9_#3> was submitted from host <eu-login-19> by user <andriusb> in cluster <euler> at Thu Mar  3 11:53:39 2022
Job was executed on host(s) <eu-g3-071>, in queue <gpuhe.120h>, as user <andriusb> in cluster <euler> at Thu Mar  3 20:37:41 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Thu Mar  3 20:37:41 2022
Terminated at Sat Mar  5 08:35:25 2022
Results reported at Sat Mar  5 08:35:25 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.25 --save-dir /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion jelinek_mercer_smoothing --jelinek-n 2 --alphas "(0.0, 0.1, 0.9)" --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 2048 --update-freq 32 --no-epoch-checkpoints --no-last-checkpoints --seed 1321673 --no-epoch-checkpoints --fp16 --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   129410.87 sec.
    Max Memory :                                 8323 MB
    Average Memory :                             2876.67 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               11677.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   129466 sec.
    Turnaround time :                            160906 sec.

The output (if any) follows:

2022-03-03 20:37:48 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1321673, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [32], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': True, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.25', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1321673, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'jelinek_mercer_smoothing', 'alphas': '(0.0, 0.1, 0.9)', 'jelinek_n': 2, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-03 20:37:48 | INFO | fairseq.tasks.language_modeling | dictionary: 291888 types
2022-03-03 20:37:53 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
Calculating frequency stats:
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 674/450337 [00:00<01:06, 6729.37it/s]  0%|          | 1347/450337 [00:00<01:15, 5969.90it/s]  0%|          | 1950/450337 [00:00<01:17, 5762.77it/s]  1%|          | 2543/450337 [00:00<01:16, 5823.90it/s]  1%|          | 3243/450337 [00:00<01:11, 6228.25it/s]  1%|          | 3884/450337 [00:00<01:11, 6285.48it/s]  1%|          | 4602/450337 [00:00<01:07, 6571.86it/s]  1%|          | 5330/450337 [00:00<01:05, 6790.75it/s]  1%|▏         | 6045/450337 [00:00<01:04, 6897.84it/s]  1%|▏         | 6737/450337 [00:01<01:10, 6251.36it/s]  2%|▏         | 7375/450337 [00:01<01:10, 6243.88it/s]  2%|▏         | 8008/450337 [00:01<01:12, 6089.46it/s]  2%|▏         | 8623/450337 [00:01<01:13, 6004.59it/s]  2%|▏         | 9292/450337 [00:01<01:11, 6194.68it/s]  2%|▏         | 9951/450337 [00:01<01:09, 6302.73it/s]  2%|▏         | 10585/450337 [00:01<01:12, 6090.86it/s]  2%|▏         | 11213/450337 [00:01<01:11, 6136.92it/s]  3%|▎         | 11830/450337 [00:01<01:13, 5993.79it/s]  3%|▎         | 12504/450337 [00:02<01:10, 6199.39it/s]  3%|▎         | 13129/450337 [00:02<01:10, 6212.51it/s]  3%|▎         | 13856/450337 [00:02<01:06, 6515.76it/s]  3%|▎         | 14510/450337 [00:02<01:09, 6265.58it/s]  3%|▎         | 15237/450337 [00:02<01:06, 6554.94it/s]  4%|▎         | 15896/450337 [00:02<01:11, 6100.84it/s]  4%|▎         | 16514/450337 [00:02<01:12, 5991.42it/s]  4%|▍         | 17146/450337 [00:02<01:11, 6082.05it/s]  4%|▍         | 17774/450337 [00:02<01:10, 6133.95it/s]  4%|▍         | 18391/450337 [00:02<01:10, 6112.24it/s]  4%|▍         | 19096/450337 [00:03<01:07, 6384.72it/s]  4%|▍         | 19787/450337 [00:03<01:05, 6535.11it/s]  5%|▍         | 20443/450337 [00:03<01:08, 6242.82it/s]  5%|▍         | 21076/450337 [00:03<01:08, 6259.11it/s]  5%|▍         | 21705/450337 [00:03<01:10, 6047.97it/s]  5%|▍         | 22366/450337 [00:03<01:08, 6206.00it/s]  5%|▌         | 23034/450337 [00:03<01:07, 6339.74it/s]  5%|▌         | 23731/450337 [00:03<01:05, 6521.32it/s]  5%|▌         | 24510/450337 [00:03<01:01, 6889.49it/s]  6%|▌         | 25226/450337 [00:03<01:01, 6968.44it/s]  6%|▌         | 25925/450337 [00:04<01:03, 6645.67it/s]  6%|▌         | 26594/450337 [00:04<01:07, 6319.67it/s]  6%|▌         | 27232/450337 [00:04<01:10, 6013.12it/s]  6%|▌         | 27839/450337 [00:04<01:11, 5940.98it/s]  6%|▋         | 28592/450337 [00:04<01:06, 6382.89it/s]  6%|▋         | 29236/450337 [00:04<01:07, 6241.86it/s]  7%|▋         | 29865/450337 [00:04<01:07, 6244.37it/s]  7%|▋         | 30493/450337 [00:04<01:08, 6125.34it/s]  7%|▋         | 31108/450337 [00:04<01:12, 5744.68it/s]  7%|▋         | 31795/450337 [00:05<01:09, 6051.13it/s]  7%|▋         | 32407/450337 [00:05<01:11, 5838.76it/s]  7%|▋         | 32998/450337 [00:05<01:11, 5853.32it/s]  7%|▋         | 33587/450337 [00:05<01:11, 5860.45it/s]  8%|▊         | 34180/450337 [00:05<01:10, 5879.87it/s]  8%|▊         | 34914/450337 [00:05<01:05, 6299.99it/s]  8%|▊         | 35547/450337 [00:05<01:08, 6083.86it/s]  8%|▊         | 36170/450337 [00:05<01:07, 6125.29it/s]  8%|▊         | 36785/450337 [00:05<01:07, 6119.30it/s]  8%|▊         | 37399/450337 [00:06<01:11, 5801.81it/s]  8%|▊         | 37984/450337 [00:06<01:11, 5777.78it/s]  9%|▊         | 38606/450337 [00:06<01:09, 5902.81it/s]  9%|▊         | 39244/450337 [00:06<01:08, 6038.47it/s]  9%|▉         | 39850/450337 [00:06<01:08, 5953.98it/s]  9%|▉         | 40541/450337 [00:06<01:05, 6231.60it/s]  9%|▉         | 41166/450337 [00:06<01:05, 6211.53it/s]  9%|▉         | 41789/450337 [00:06<01:08, 5959.67it/s]  9%|▉         | 42388/450337 [00:06<01:10, 5762.82it/s] 10%|▉         | 42983/450337 [00:06<01:10, 5815.36it/s] 10%|▉         | 43601/450337 [00:07<01:08, 5918.92it/s] 10%|▉         | 44226/450337 [00:07<01:07, 6013.13it/s] 10%|▉         | 44905/450337 [00:07<01:05, 6235.81it/s] 10%|█         | 45557/450337 [00:07<01:04, 6319.47it/s] 10%|█         | 46276/450337 [00:07<01:01, 6575.49it/s] 10%|█         | 46935/450337 [00:07<01:03, 6318.13it/s] 11%|█         | 47933/450337 [00:07<00:54, 7378.27it/s] 11%|█         | 48677/450337 [00:07<00:55, 7212.30it/s] 11%|█         | 49403/450337 [00:07<00:55, 7197.27it/s] 11%|█         | 50126/450337 [00:08<00:58, 6826.18it/s] 11%|█▏        | 50815/450337 [00:08<01:02, 6436.07it/s] 11%|█▏        | 51466/450337 [00:08<01:02, 6385.10it/s] 12%|█▏        | 52233/450337 [00:08<00:59, 6744.32it/s] 12%|█▏        | 52975/450337 [00:08<00:57, 6937.40it/s] 12%|█▏        | 53674/450337 [00:08<01:00, 6601.95it/s] 12%|█▏        | 54341/450337 [00:08<01:03, 6223.88it/s] 12%|█▏        | 54971/450337 [00:08<01:04, 6149.82it/s] 12%|█▏        | 55677/450337 [00:08<01:01, 6403.29it/s] 13%|█▎        | 56323/450337 [00:08<01:02, 6329.91it/s] 13%|█▎        | 56960/450337 [00:09<01:05, 6034.51it/s] 13%|█▎        | 57568/450337 [00:09<01:05, 5992.02it/s] 13%|█▎        | 58204/450337 [00:09<01:04, 6095.99it/s] 13%|█▎        | 58930/450337 [00:09<01:00, 6431.07it/s] 13%|█▎        | 59577/450337 [00:09<01:04, 6061.77it/s] 13%|█▎        | 60190/450337 [00:09<01:04, 6009.48it/s] 14%|█▎        | 60931/450337 [00:09<01:00, 6405.61it/s] 14%|█▎        | 61604/450337 [00:09<00:59, 6497.09it/s] 14%|█▍        | 62258/450337 [00:09<01:01, 6345.35it/s] 14%|█▍        | 62914/450337 [00:10<01:00, 6405.35it/s] 14%|█▍        | 63557/450337 [00:10<01:02, 6215.78it/s] 14%|█▍        | 64182/450337 [00:10<01:02, 6171.59it/s] 14%|█▍        | 64847/450337 [00:10<01:01, 6304.61it/s] 15%|█▍        | 65480/450337 [00:10<01:03, 6105.65it/s] 15%|█▍        | 66116/450337 [00:10<01:02, 6168.42it/s] 15%|█▍        | 66876/450337 [00:10<00:58, 6574.69it/s] 15%|█▍        | 67536/450337 [00:10<01:03, 6049.03it/s] 15%|█▌        | 68151/450337 [00:10<01:04, 5893.54it/s] 15%|█▌        | 68926/450337 [00:11<00:59, 6408.23it/s] 15%|█▌        | 69576/450337 [00:11<00:59, 6356.17it/s] 16%|█▌        | 70239/450337 [00:11<00:59, 6428.60it/s] 16%|█▌        | 70887/450337 [00:11<01:01, 6181.95it/s] 16%|█▌        | 71532/450337 [00:11<01:00, 6256.63it/s] 16%|█▌        | 72171/450337 [00:11<01:00, 6294.38it/s] 16%|█▌        | 72910/450337 [00:11<00:57, 6614.20it/s] 16%|█▋        | 73620/450337 [00:11<00:55, 6750.75it/s] 17%|█▋        | 74469/450337 [00:11<00:51, 7262.23it/s] 17%|█▋        | 75198/450337 [00:11<00:54, 6840.87it/s] 17%|█▋        | 75889/450337 [00:12<00:56, 6655.05it/s] 17%|█▋        | 76566/450337 [00:12<00:55, 6682.03it/s] 17%|█▋        | 77238/450337 [00:12<00:58, 6406.48it/s] 17%|█▋        | 77954/450337 [00:12<00:56, 6615.86it/s] 17%|█▋        | 78620/450337 [00:12<01:00, 6161.01it/s] 18%|█▊        | 79245/450337 [00:12<01:04, 5762.25it/s] 18%|█▊        | 79842/450337 [00:12<01:03, 5812.79it/s] 18%|█▊        | 80457/450337 [00:12<01:02, 5896.78it/s] 18%|█▊        | 81140/450337 [00:12<00:59, 6159.96it/s] 18%|█▊        | 81787/450337 [00:13<00:59, 6244.95it/s] 18%|█▊        | 82416/450337 [00:13<00:58, 6257.58it/s] 18%|█▊        | 83045/450337 [00:13<00:58, 6242.67it/s] 19%|█▊        | 83747/450337 [00:13<00:56, 6463.01it/s] 19%|█▊        | 84395/450337 [00:13<00:58, 6228.19it/s] 19%|█▉        | 85067/450337 [00:13<00:57, 6367.17it/s] 19%|█▉        | 85707/450337 [00:13<00:57, 6374.23it/s] 19%|█▉        | 86347/450337 [00:13<00:57, 6313.35it/s] 19%|█▉        | 87060/450337 [00:13<00:55, 6548.17it/s] 19%|█▉        | 87717/450337 [00:13<00:55, 6544.46it/s] 20%|█▉        | 88373/450337 [00:14<00:55, 6497.04it/s] 20%|█▉        | 89024/450337 [00:14<00:59, 6033.24it/s] 20%|█▉        | 89649/450337 [00:14<00:59, 6093.75it/s] 20%|██        | 90264/450337 [00:14<01:00, 5910.15it/s] 20%|██        | 90860/450337 [00:14<01:01, 5846.20it/s] 20%|██        | 91591/450337 [00:14<00:57, 6264.20it/s] 20%|██        | 92222/450337 [00:14<00:57, 6240.34it/s] 21%|██        | 92886/450337 [00:14<00:56, 6348.61it/s] 21%|██        | 93524/450337 [00:14<00:56, 6300.50it/s] 21%|██        | 94156/450337 [00:15<00:57, 6154.77it/s] 21%|██        | 94895/450337 [00:15<00:54, 6508.25it/s] 21%|██        | 95549/450337 [00:15<00:56, 6291.38it/s] 21%|██▏       | 96208/450337 [00:15<00:55, 6376.37it/s] 22%|██▏       | 97045/450337 [00:15<00:50, 6956.92it/s] 22%|██▏       | 97744/450337 [00:15<00:54, 6452.73it/s] 22%|██▏       | 98399/450337 [00:15<00:54, 6470.51it/s] 22%|██▏       | 99053/450337 [00:15<00:55, 6365.12it/s] 22%|██▏       | 99694/450337 [00:15<00:57, 6125.27it/s] 22%|██▏       | 100311/450337 [00:15<00:58, 6009.20it/s] 22%|██▏       | 100942/450337 [00:16<00:57, 6089.13it/s] 23%|██▎       | 101554/450337 [00:16<00:57, 6066.08it/s] 23%|██▎       | 102226/450337 [00:16<00:55, 6255.11it/s] 23%|██▎       | 102854/450337 [00:16<00:55, 6244.01it/s] 23%|██▎       | 103480/450337 [00:16<00:56, 6124.70it/s] 23%|██▎       | 104094/450337 [00:16<00:58, 5887.28it/s] 23%|██▎       | 104779/450337 [00:16<00:56, 6153.63it/s] 23%|██▎       | 105398/450337 [00:16<00:56, 6114.19it/s] 24%|██▎       | 106033/450337 [00:16<00:55, 6181.57it/s] 24%|██▎       | 106657/450337 [00:17<00:55, 6194.50it/s] 24%|██▍       | 107278/450337 [00:17<00:56, 6045.56it/s] 24%|██▍       | 107921/450337 [00:17<00:55, 6154.68it/s] 24%|██▍       | 108538/450337 [00:17<00:56, 6035.96it/s] 24%|██▍       | 109143/450337 [00:17<00:57, 5905.81it/s] 24%|██▍       | 109735/450337 [00:17<00:58, 5844.97it/s] 25%|██▍       | 110381/450337 [00:17<00:56, 6023.27it/s] 25%|██▍       | 111118/450337 [00:17<00:52, 6416.18it/s] 25%|██▍       | 111762/450337 [00:17<00:54, 6228.53it/s] 25%|██▍       | 112388/450337 [00:17<00:54, 6149.45it/s] 25%|██▌       | 113031/450337 [00:18<00:54, 6225.70it/s] 25%|██▌       | 113655/450337 [00:18<00:56, 5989.08it/s] 25%|██▌       | 114328/450337 [00:18<00:54, 6199.72it/s] 26%|██▌       | 114959/450337 [00:18<00:53, 6230.24it/s] 26%|██▌       | 115584/450337 [00:18<00:57, 5798.65it/s] 26%|██▌       | 116195/450337 [00:18<00:56, 5884.34it/s] 26%|██▌       | 116789/450337 [00:18<00:57, 5798.52it/s] 26%|██▌       | 117387/450337 [00:18<00:56, 5847.81it/s] 26%|██▌       | 118004/450337 [00:18<00:55, 5935.41it/s] 26%|██▋       | 118661/450337 [00:19<00:54, 6120.44it/s] 26%|██▋       | 119276/450337 [00:19<00:54, 6036.60it/s] 27%|██▋       | 119882/450337 [00:19<00:55, 5904.95it/s] 27%|██▋       | 120707/450337 [00:19<00:50, 6582.61it/s] 27%|██▋       | 121369/450337 [00:19<00:51, 6388.12it/s] 27%|██▋       | 122011/450337 [00:19<00:53, 6092.66it/s] 27%|██▋       | 122686/450337 [00:19<00:52, 6276.78it/s] 27%|██▋       | 123318/450337 [00:19<00:54, 6041.98it/s] 28%|██▊       | 123939/450337 [00:19<00:53, 6089.36it/s] 28%|██▊       | 124552/450337 [00:19<00:53, 6098.98it/s] 28%|██▊       | 125171/450337 [00:20<00:53, 6122.90it/s] 28%|██▊       | 125834/450337 [00:20<00:51, 6269.63it/s] 28%|██▊       | 126477/450337 [00:20<00:51, 6316.77it/s] 28%|██▊       | 127152/450337 [00:20<00:50, 6444.31it/s] 28%|██▊       | 127798/450337 [00:20<00:51, 6302.02it/s] 29%|██▊       | 128430/450337 [00:20<00:52, 6152.79it/s] 29%|██▊       | 129122/450337 [00:20<00:50, 6368.81it/s] 29%|██▉       | 129761/450337 [00:20<00:52, 6099.20it/s] 29%|██▉       | 130380/450337 [00:20<00:52, 6124.27it/s] 29%|██▉       | 130995/450337 [00:20<00:52, 6068.96it/s] 29%|██▉       | 131604/450337 [00:21<00:55, 5785.28it/s] 29%|██▉       | 132186/450337 [00:21<00:55, 5713.70it/s] 30%|██▉       | 132891/450337 [00:21<00:52, 6094.41it/s] 30%|██▉       | 133504/450337 [00:21<00:51, 6099.03it/s] 30%|██▉       | 134205/450337 [00:21<00:49, 6364.48it/s] 30%|██▉       | 134935/450337 [00:21<00:47, 6639.85it/s] 30%|███       | 135640/450337 [00:21<00:46, 6761.15it/s] 30%|███       | 136318/450337 [00:21<00:47, 6601.06it/s] 30%|███       | 137145/450337 [00:21<00:44, 7077.69it/s] 31%|███       | 137856/450337 [00:22<00:46, 6722.23it/s] 31%|███       | 138534/450337 [00:22<00:46, 6702.11it/s] 31%|███       | 139208/450337 [00:22<00:47, 6485.27it/s] 31%|███       | 139860/450337 [00:22<00:48, 6447.37it/s] 31%|███       | 140507/450337 [00:22<00:50, 6159.40it/s] 31%|███▏      | 141132/450337 [00:22<00:49, 6184.53it/s] 31%|███▏      | 141753/450337 [00:22<00:50, 6121.83it/s] 32%|███▏      | 142367/450337 [00:22<00:50, 6046.65it/s] 32%|███▏      | 142973/450337 [00:22<00:51, 5963.13it/s] 32%|███▏      | 143743/450337 [00:22<00:47, 6465.21it/s] 32%|███▏      | 144392/450337 [00:23<00:47, 6383.53it/s] 32%|███▏      | 145153/450337 [00:23<00:45, 6736.70it/s] 32%|███▏      | 145829/450337 [00:23<00:45, 6665.79it/s] 33%|███▎      | 146498/450337 [00:23<00:47, 6414.76it/s] 33%|███▎      | 147143/450337 [00:23<00:48, 6273.74it/s] 33%|███▎      | 147773/450337 [00:23<00:51, 5932.27it/s] 33%|███▎      | 148385/450337 [00:23<00:50, 5978.83it/s] 33%|███▎      | 149079/450337 [00:23<00:48, 6247.67it/s] 33%|███▎      | 149708/450337 [00:23<00:49, 6031.42it/s] 33%|███▎      | 150315/450337 [00:24<00:51, 5844.15it/s] 34%|███▎      | 150921/450337 [00:24<00:50, 5903.60it/s] 34%|███▎      | 151571/450337 [00:24<00:49, 6073.23it/s] 34%|███▍      | 152181/450337 [00:24<00:51, 5736.89it/s] 34%|███▍      | 152870/450337 [00:24<00:49, 6058.06it/s] 34%|███▍      | 153567/450337 [00:24<00:47, 6310.82it/s] 34%|███▍      | 154203/450337 [00:24<00:48, 6050.70it/s] 34%|███▍      | 154814/450337 [00:24<00:48, 6061.54it/s] 35%|███▍      | 155488/450337 [00:24<00:47, 6249.97it/s] 35%|███▍      | 156142/450337 [00:25<00:46, 6334.26it/s] 35%|███▍      | 156778/450337 [00:25<00:46, 6316.84it/s] 35%|███▍      | 157495/450337 [00:25<00:44, 6568.28it/s] 35%|███▌      | 158180/450337 [00:25<00:43, 6651.07it/s] 35%|███▌      | 158847/450337 [00:25<00:45, 6459.30it/s] 35%|███▌      | 159495/450337 [00:25<00:47, 6127.40it/s] 36%|███▌      | 160241/450337 [00:25<00:44, 6501.65it/s] 36%|███▌      | 160897/450337 [00:25<00:45, 6402.37it/s] 36%|███▌      | 161541/450337 [00:25<00:45, 6351.53it/s] 36%|███▌      | 162179/450337 [00:25<00:45, 6302.67it/s] 36%|███▌      | 162841/450337 [00:26<00:44, 6391.19it/s] 36%|███▋      | 163482/450337 [00:26<00:45, 6310.65it/s] 36%|███▋      | 164115/450337 [00:26<00:47, 6069.75it/s] 37%|███▋      | 164794/450337 [00:26<00:45, 6274.48it/s] 37%|███▋      | 165490/450337 [00:26<00:44, 6473.23it/s] 37%|███▋      | 166140/450337 [00:26<00:44, 6424.57it/s] 37%|███▋      | 166869/450337 [00:26<00:42, 6673.23it/s] 37%|███▋      | 167539/450337 [00:26<00:43, 6574.78it/s] 37%|███▋      | 168198/450337 [00:26<00:45, 6144.81it/s] 38%|███▊      | 168893/450337 [00:27<00:44, 6366.32it/s] 38%|███▊      | 169536/450337 [00:27<00:45, 6186.67it/s] 38%|███▊      | 170160/450337 [00:27<00:46, 6009.55it/s] 38%|███▊      | 170765/450337 [00:27<00:46, 5963.34it/s] 38%|███▊      | 171390/450337 [00:27<00:46, 6039.41it/s] 38%|███▊      | 171998/450337 [00:27<00:46, 6050.34it/s] 38%|███▊      | 172616/450337 [00:27<00:45, 6087.09it/s] 38%|███▊      | 173226/450337 [00:27<00:46, 6017.30it/s] 39%|███▊      | 173882/450337 [00:27<00:44, 6170.64it/s] 39%|███▊      | 174500/450337 [00:27<00:45, 6077.24it/s] 39%|███▉      | 175166/450337 [00:28<00:44, 6247.20it/s] 39%|███▉      | 175792/450337 [00:28<00:45, 6036.29it/s] 39%|███▉      | 176413/450337 [00:28<00:45, 6084.43it/s] 39%|███▉      | 177023/450337 [00:28<00:46, 5921.09it/s] 39%|███▉      | 177617/450337 [00:28<00:49, 5490.58it/s] 40%|███▉      | 178253/450337 [00:28<00:47, 5728.54it/s] 40%|███▉      | 178922/450337 [00:28<00:45, 5995.20it/s] 40%|███▉      | 179564/450337 [00:28<00:44, 6112.15it/s] 40%|████      | 180180/450337 [00:28<00:45, 5911.75it/s] 40%|████      | 180831/450337 [00:29<00:44, 6080.24it/s] 40%|████      | 181443/450337 [00:29<00:45, 5943.17it/s] 40%|████      | 182041/450337 [00:29<00:45, 5885.79it/s] 41%|████      | 182648/450337 [00:29<00:45, 5930.03it/s] 41%|████      | 183243/450337 [00:29<00:45, 5883.05it/s] 41%|████      | 183833/450337 [00:29<00:47, 5599.34it/s] 41%|████      | 184423/450337 [00:29<00:46, 5682.38it/s] 41%|████      | 185056/450337 [00:29<00:45, 5867.92it/s] 41%|████      | 185646/450337 [00:29<00:46, 5654.87it/s] 41%|████▏     | 186250/450337 [00:29<00:45, 5761.39it/s] 41%|████▏     | 186838/450337 [00:30<00:45, 5792.61it/s] 42%|████▏     | 187512/450337 [00:30<00:43, 6070.13it/s] 42%|████▏     | 188122/450337 [00:30<00:44, 5880.51it/s] 42%|████▏     | 188821/450337 [00:30<00:42, 6201.45it/s] 42%|████▏     | 189445/450337 [00:30<00:42, 6091.19it/s] 42%|████▏     | 190071/450337 [00:30<00:42, 6140.19it/s] 42%|████▏     | 190723/450337 [00:30<00:41, 6251.21it/s] 42%|████▏     | 191350/450337 [00:30<00:41, 6222.42it/s] 43%|████▎     | 191974/450337 [00:30<00:41, 6192.16it/s] 43%|████▎     | 192661/450337 [00:30<00:40, 6391.23it/s] 43%|████▎     | 193301/450337 [00:31<00:40, 6274.57it/s] 43%|████▎     | 193930/450337 [00:31<00:40, 6260.41it/s] 43%|████▎     | 194557/450337 [00:31<00:40, 6252.18it/s] 43%|████▎     | 195183/450337 [00:31<00:40, 6237.31it/s] 43%|████▎     | 195872/450337 [00:31<00:39, 6429.26it/s] 44%|████▎     | 196516/450337 [00:31<00:41, 6159.88it/s] 44%|████▍     | 197379/450337 [00:31<00:36, 6873.21it/s] 44%|████▍     | 198071/450337 [00:31<00:37, 6643.99it/s] 44%|████▍     | 198740/450337 [00:31<00:39, 6428.40it/s] 44%|████▍     | 199396/450337 [00:32<00:38, 6461.79it/s] 44%|████▍     | 200046/450337 [00:32<00:40, 6182.94it/s] 45%|████▍     | 200669/450337 [00:32<00:42, 5852.25it/s] 45%|████▍     | 201323/450337 [00:32<00:41, 6037.74it/s] 45%|████▍     | 201948/450337 [00:32<00:40, 6089.43it/s] 45%|████▍     | 202561/450337 [00:32<00:40, 6084.00it/s] 45%|████▌     | 203226/450337 [00:32<00:39, 6242.45it/s] 45%|████▌     | 203949/450337 [00:32<00:37, 6528.98it/s] 45%|████▌     | 204605/450337 [00:32<00:39, 6201.53it/s] 46%|████▌     | 205230/450337 [00:32<00:40, 6086.45it/s] 46%|████▌     | 205842/450337 [00:33<00:40, 6007.22it/s] 46%|████▌     | 206519/450337 [00:33<00:39, 6224.78it/s] 46%|████▌     | 207145/450337 [00:33<00:40, 5970.83it/s] 46%|████▌     | 207753/450337 [00:33<00:40, 6001.38it/s] 46%|████▋     | 208356/450337 [00:33<00:40, 5933.68it/s] 46%|████▋     | 208952/450337 [00:33<00:42, 5668.43it/s] 47%|████▋     | 209568/450337 [00:33<00:41, 5799.33it/s] 47%|████▋     | 210248/450337 [00:33<00:39, 6086.17it/s] 47%|████▋     | 210920/450337 [00:33<00:38, 6268.59it/s] 47%|████▋     | 211550/450337 [00:34<00:38, 6231.84it/s] 47%|████▋     | 212176/450337 [00:34<00:38, 6144.96it/s] 47%|████▋     | 212910/450337 [00:34<00:36, 6489.45it/s] 47%|████▋     | 213561/450337 [00:34<00:38, 6130.09it/s] 48%|████▊     | 214179/450337 [00:34<00:38, 6072.38it/s] 48%|████▊     | 214790/450337 [00:34<00:39, 5934.42it/s] 48%|████▊     | 215386/450337 [00:34<00:41, 5689.87it/s] 48%|████▊     | 215977/450337 [00:34<00:40, 5741.09it/s] 48%|████▊     | 216554/450337 [00:34<00:41, 5645.78it/s] 48%|████▊     | 217160/450337 [00:34<00:40, 5762.05it/s] 48%|████▊     | 217831/450337 [00:35<00:38, 6034.03it/s] 49%|████▊     | 218494/450337 [00:35<00:37, 6208.55it/s] 49%|████▊     | 219139/450337 [00:35<00:36, 6272.92it/s] 49%|████▉     | 219768/450337 [00:35<00:37, 6074.85it/s] 49%|████▉     | 220391/450337 [00:35<00:37, 6114.89it/s] 49%|████▉     | 221059/450337 [00:35<00:36, 6273.14it/s] 49%|████▉     | 221688/450337 [00:35<00:37, 6066.92it/s] 49%|████▉     | 222297/450337 [00:35<00:39, 5763.05it/s] 50%|████▉     | 222945/450337 [00:35<00:38, 5960.73it/s] 50%|████▉     | 223546/450337 [00:36<00:40, 5609.57it/s] 50%|████▉     | 224246/450337 [00:36<00:37, 5992.82it/s] 50%|████▉     | 224931/450337 [00:36<00:36, 6235.52it/s] 50%|█████     | 225595/450337 [00:36<00:35, 6351.60it/s] 50%|█████     | 226354/450337 [00:36<00:33, 6712.79it/s] 50%|█████     | 227030/450337 [00:36<00:35, 6316.15it/s] 51%|█████     | 227670/450337 [00:36<00:35, 6195.42it/s] 51%|█████     | 228295/450337 [00:36<00:36, 6124.18it/s] 51%|█████     | 228911/450337 [00:36<00:37, 5911.75it/s] 51%|█████     | 229506/450337 [00:37<00:37, 5850.48it/s] 51%|█████     | 230094/450337 [00:37<00:38, 5733.94it/s] 51%|█████     | 230752/450337 [00:37<00:36, 5971.31it/s] 51%|█████▏    | 231352/450337 [00:37<00:36, 5958.12it/s] 52%|█████▏    | 231950/450337 [00:37<00:37, 5821.52it/s] 52%|█████▏    | 232607/450337 [00:37<00:36, 6037.38it/s] 52%|█████▏    | 233339/450337 [00:37<00:33, 6410.27it/s] 52%|█████▏    | 233991/450337 [00:37<00:33, 6431.47it/s] 52%|█████▏    | 234712/450337 [00:37<00:32, 6658.05it/s] 52%|█████▏    | 235394/450337 [00:37<00:32, 6704.51it/s] 52%|█████▏    | 236066/450337 [00:38<00:33, 6330.88it/s] 53%|█████▎    | 236704/450337 [00:38<00:35, 6040.53it/s] 53%|█████▎    | 237314/450337 [00:38<00:35, 6044.70it/s] 53%|█████▎    | 237923/450337 [00:38<00:36, 5882.82it/s] 53%|█████▎    | 238708/450337 [00:38<00:32, 6437.01it/s] 53%|█████▎    | 239357/450337 [00:38<00:32, 6435.24it/s] 53%|█████▎    | 240005/450337 [00:38<00:34, 6060.98it/s] 53%|█████▎    | 240809/450337 [00:38<00:31, 6610.35it/s] 54%|█████▎    | 241478/450337 [00:38<00:31, 6565.96it/s] 54%|█████▍    | 242149/450337 [00:39<00:31, 6606.17it/s] 54%|█████▍    | 242814/450337 [00:39<00:32, 6409.90it/s] 54%|█████▍    | 243459/450337 [00:39<00:32, 6388.84it/s] 54%|█████▍    | 244101/450337 [00:39<00:32, 6285.53it/s] 54%|█████▍    | 244732/450337 [00:39<00:32, 6273.10it/s] 54%|█████▍    | 245362/450337 [00:39<00:32, 6273.22it/s] 55%|█████▍    | 246081/450337 [00:39<00:31, 6542.56it/s] 55%|█████▍    | 246850/450337 [00:39<00:29, 6879.43it/s] 55%|█████▍    | 247540/450337 [00:39<00:30, 6643.02it/s] 55%|█████▌    | 248207/450337 [00:39<00:31, 6375.12it/s] 55%|█████▌    | 248935/450337 [00:40<00:30, 6632.52it/s] 55%|█████▌    | 249669/450337 [00:40<00:29, 6832.69it/s] 56%|█████▌    | 250356/450337 [00:40<00:31, 6440.74it/s] 56%|█████▌    | 251020/450337 [00:40<00:30, 6494.19it/s] 56%|█████▌    | 251678/450337 [00:40<00:30, 6514.25it/s] 56%|█████▌    | 252333/450337 [00:40<00:30, 6434.75it/s] 56%|█████▌    | 252979/450337 [00:40<00:31, 6266.66it/s] 56%|█████▋    | 253637/450337 [00:40<00:30, 6351.52it/s] 56%|█████▋    | 254288/450337 [00:40<00:30, 6397.44it/s] 57%|█████▋    | 254930/450337 [00:41<00:31, 6189.93it/s] 57%|█████▋    | 255552/450337 [00:41<00:31, 6143.50it/s] 57%|█████▋    | 256272/450337 [00:41<00:30, 6441.45it/s] 57%|█████▋    | 256960/450337 [00:41<00:29, 6567.81it/s] 57%|█████▋    | 257651/450337 [00:41<00:28, 6665.54it/s] 57%|█████▋    | 258319/450337 [00:41<00:31, 6011.03it/s] 57%|█████▋    | 258933/450337 [00:41<00:32, 5920.84it/s] 58%|█████▊    | 259534/450337 [00:41<00:32, 5852.36it/s] 58%|█████▊    | 260204/450337 [00:41<00:31, 6090.10it/s] 58%|█████▊    | 260954/450337 [00:41<00:29, 6489.18it/s] 58%|█████▊    | 261609/450337 [00:42<00:29, 6441.80it/s] 58%|█████▊    | 262258/450337 [00:42<00:30, 6132.79it/s] 58%|█████▊    | 262877/450337 [00:42<00:31, 6026.15it/s] 59%|█████▊    | 263484/450337 [00:42<00:31, 5911.00it/s] 59%|█████▊    | 264087/450337 [00:42<00:31, 5938.91it/s] 59%|█████▉    | 264705/450337 [00:42<00:30, 5999.36it/s] 59%|█████▉    | 265307/450337 [00:42<00:31, 5886.18it/s] 59%|█████▉    | 265899/450337 [00:42<00:31, 5893.94it/s] 59%|█████▉    | 266513/450337 [00:42<00:30, 5960.56it/s] 59%|█████▉    | 267128/450337 [00:43<00:30, 6009.23it/s] 59%|█████▉    | 267730/450337 [00:43<00:30, 5902.40it/s] 60%|█████▉    | 268341/450337 [00:43<00:30, 5958.88it/s] 60%|█████▉    | 269036/450337 [00:43<00:29, 6251.29it/s] 60%|█████▉    | 269663/450337 [00:43<00:30, 6018.30it/s] 60%|██████    | 270351/450337 [00:43<00:28, 6262.88it/s] 60%|██████    | 270980/450337 [00:43<00:29, 6182.28it/s] 60%|██████    | 271601/450337 [00:43<00:29, 6050.12it/s] 60%|██████    | 272423/450337 [00:43<00:26, 6677.23it/s] 61%|██████    | 273159/450337 [00:43<00:25, 6875.99it/s] 61%|██████    | 273850/450337 [00:44<00:26, 6766.46it/s] 61%|██████    | 274529/450337 [00:44<00:27, 6459.98it/s] 61%|██████    | 275179/450337 [00:44<00:28, 6127.87it/s] 61%|██████    | 275797/450337 [00:44<00:28, 6103.81it/s] 61%|██████▏   | 276411/450337 [00:44<00:29, 5808.52it/s] 62%|██████▏   | 277039/450337 [00:44<00:29, 5936.56it/s] 62%|██████▏   | 277637/450337 [00:44<00:29, 5899.47it/s] 62%|██████▏   | 278305/450337 [00:44<00:28, 6121.02it/s] 62%|██████▏   | 279009/450337 [00:44<00:26, 6382.32it/s] 62%|██████▏   | 279651/450337 [00:45<00:27, 6161.41it/s] 62%|██████▏   | 280356/450337 [00:45<00:26, 6416.33it/s] 62%|██████▏   | 281002/450337 [00:45<00:28, 5921.33it/s] 63%|██████▎   | 281650/450337 [00:45<00:27, 6075.67it/s] 63%|██████▎   | 282266/450337 [00:45<00:27, 6021.80it/s] 63%|██████▎   | 282874/450337 [00:45<00:27, 5996.28it/s] 63%|██████▎   | 283573/450337 [00:45<00:26, 6282.59it/s] 63%|██████▎   | 284218/450337 [00:45<00:26, 6329.36it/s] 63%|██████▎   | 284854/450337 [00:45<00:26, 6197.10it/s] 63%|██████▎   | 285477/450337 [00:45<00:27, 6100.20it/s] 64%|██████▎   | 286140/450337 [00:46<00:26, 6251.03it/s] 64%|██████▎   | 286767/450337 [00:46<00:26, 6075.35it/s] 64%|██████▍   | 287377/450337 [00:46<00:27, 5999.13it/s] 64%|██████▍   | 288097/450337 [00:46<00:25, 6343.27it/s] 64%|██████▍   | 288734/450337 [00:46<00:25, 6278.97it/s] 64%|██████▍   | 289364/450337 [00:46<00:25, 6263.45it/s] 64%|██████▍   | 289992/450337 [00:46<00:27, 5824.95it/s] 65%|██████▍   | 290607/450337 [00:46<00:27, 5912.06it/s] 65%|██████▍   | 291213/450337 [00:46<00:26, 5947.02it/s] 65%|██████▍   | 291812/450337 [00:47<00:26, 5953.95it/s] 65%|██████▍   | 292498/450337 [00:47<00:25, 6212.22it/s] 65%|██████▌   | 293122/450337 [00:47<00:26, 5993.41it/s] 65%|██████▌   | 293743/450337 [00:47<00:25, 6050.75it/s] 65%|██████▌   | 294351/450337 [00:47<00:25, 6058.78it/s] 66%|██████▌   | 295058/450337 [00:47<00:24, 6349.59it/s] 66%|██████▌   | 295695/450337 [00:47<00:24, 6300.56it/s] 66%|██████▌   | 296327/450337 [00:47<00:24, 6216.60it/s] 66%|██████▌   | 296983/450337 [00:47<00:24, 6310.12it/s] 66%|██████▌   | 297615/450337 [00:47<00:24, 6223.14it/s] 66%|██████▌   | 298271/450337 [00:48<00:24, 6318.08it/s] 66%|██████▋   | 298904/450337 [00:48<00:24, 6208.96it/s] 67%|██████▋   | 299590/450337 [00:48<00:23, 6397.69it/s] 67%|██████▋   | 300231/450337 [00:48<00:23, 6289.28it/s] 67%|██████▋   | 300861/450337 [00:48<00:23, 6285.99it/s] 67%|██████▋   | 301491/450337 [00:48<00:24, 6067.52it/s] 67%|██████▋   | 302100/450337 [00:48<00:25, 5831.63it/s] 67%|██████▋   | 302814/450337 [00:48<00:23, 6202.82it/s] 67%|██████▋   | 303463/450337 [00:48<00:23, 6285.55it/s] 68%|██████▊   | 304113/450337 [00:48<00:23, 6347.87it/s] 68%|██████▊   | 304751/450337 [00:49<00:23, 6214.95it/s] 68%|██████▊   | 305375/450337 [00:49<00:23, 6178.42it/s] 68%|██████▊   | 305995/450337 [00:49<00:23, 6159.09it/s] 68%|██████▊   | 306612/450337 [00:49<00:23, 6062.14it/s] 68%|██████▊   | 307236/450337 [00:49<00:23, 6109.55it/s] 68%|██████▊   | 307880/450337 [00:49<00:22, 6206.52it/s] 69%|██████▊   | 308502/450337 [00:49<00:25, 5667.32it/s] 69%|██████▊   | 309325/450337 [00:49<00:22, 6371.53it/s] 69%|██████▉   | 310058/450337 [00:49<00:21, 6641.09it/s] 69%|██████▉   | 310732/450337 [00:50<00:21, 6416.69it/s] 69%|██████▉   | 311382/450337 [00:50<00:22, 6314.56it/s] 69%|██████▉   | 312092/450337 [00:50<00:21, 6537.23it/s] 69%|██████▉   | 312751/450337 [00:50<00:21, 6408.82it/s] 70%|██████▉   | 313419/450337 [00:50<00:21, 6482.55it/s] 70%|██████▉   | 314135/450337 [00:50<00:20, 6677.09it/s] 70%|██████▉   | 314806/450337 [00:50<00:21, 6408.27it/s] 70%|███████   | 315451/450337 [00:50<00:22, 6107.63it/s] 70%|███████   | 316068/450337 [00:50<00:21, 6118.22it/s] 70%|███████   | 316684/450337 [00:51<00:22, 6008.72it/s] 70%|███████   | 317288/450337 [00:51<00:22, 5969.89it/s] 71%|███████   | 317887/450337 [00:51<00:23, 5758.22it/s] 71%|███████   | 318518/450337 [00:51<00:22, 5914.28it/s] 71%|███████   | 319174/450337 [00:51<00:21, 6093.92it/s] 71%|███████   | 319797/450337 [00:51<00:21, 6133.35it/s] 71%|███████   | 320413/450337 [00:51<00:21, 5910.55it/s] 71%|███████▏  | 321150/450337 [00:51<00:20, 6326.47it/s] 71%|███████▏  | 321787/450337 [00:51<00:20, 6133.46it/s] 72%|███████▏  | 322404/450337 [00:51<00:21, 5882.49it/s] 72%|███████▏  | 322998/450337 [00:52<00:21, 5897.51it/s] 72%|███████▏  | 323684/450337 [00:52<00:20, 6173.47it/s] 72%|███████▏  | 324310/450337 [00:52<00:20, 6193.40it/s] 72%|███████▏  | 324936/450337 [00:52<00:20, 6205.66it/s] 72%|███████▏  | 325559/450337 [00:52<00:21, 5742.40it/s] 72%|███████▏  | 326192/450337 [00:52<00:21, 5906.71it/s] 73%|███████▎  | 326922/450337 [00:52<00:19, 6297.60it/s] 73%|███████▎  | 327612/450337 [00:52<00:18, 6470.51it/s] 73%|███████▎  | 328381/450337 [00:52<00:17, 6821.48it/s] 73%|███████▎  | 329068/450337 [00:52<00:17, 6749.81it/s] 73%|███████▎  | 329747/450337 [00:53<00:19, 6310.32it/s] 73%|███████▎  | 330386/450337 [00:53<00:20, 5989.23it/s] 74%|███████▎  | 331033/450337 [00:53<00:19, 6109.09it/s] 74%|███████▎  | 331650/450337 [00:53<00:19, 6010.96it/s] 74%|███████▍  | 332256/450337 [00:53<00:19, 5966.15it/s] 74%|███████▍  | 332906/450337 [00:53<00:19, 6112.88it/s] 74%|███████▍  | 333520/450337 [00:53<00:19, 6070.74it/s] 74%|███████▍  | 334129/450337 [00:53<00:19, 6053.34it/s] 74%|███████▍  | 334775/450337 [00:53<00:18, 6171.05it/s] 74%|███████▍  | 335413/450337 [00:54<00:18, 6231.84it/s] 75%|███████▍  | 336076/450337 [00:54<00:18, 6344.78it/s] 75%|███████▍  | 336712/450337 [00:54<00:18, 6268.08it/s] 75%|███████▍  | 337394/450337 [00:54<00:17, 6424.59it/s] 75%|███████▌  | 338049/450337 [00:54<00:17, 6460.66it/s] 75%|███████▌  | 338708/450337 [00:54<00:17, 6492.14it/s] 75%|███████▌  | 339372/450337 [00:54<00:16, 6530.33it/s] 76%|███████▌  | 340026/450337 [00:54<00:18, 5827.75it/s] 76%|███████▌  | 340642/450337 [00:54<00:18, 5916.45it/s] 76%|███████▌  | 341312/450337 [00:55<00:17, 6133.78it/s] 76%|███████▌  | 341935/450337 [00:55<00:17, 6147.28it/s] 76%|███████▌  | 342595/450337 [00:55<00:17, 6272.40it/s] 76%|███████▌  | 343227/450337 [00:55<00:17, 6215.56it/s] 76%|███████▋  | 343931/450337 [00:55<00:16, 6448.76it/s] 77%|███████▋  | 344579/450337 [00:55<00:17, 6006.41it/s] 77%|███████▋  | 345274/450337 [00:55<00:16, 6270.66it/s] 77%|███████▋  | 345909/450337 [00:55<00:18, 5754.09it/s] 77%|███████▋  | 346635/450337 [00:55<00:16, 6159.54it/s] 77%|███████▋  | 347264/450337 [00:55<00:16, 6108.72it/s] 77%|███████▋  | 347896/450337 [00:56<00:16, 6157.29it/s] 77%|███████▋  | 348518/450337 [00:56<00:16, 6064.60it/s] 78%|███████▊  | 349148/450337 [00:56<00:16, 6122.76it/s] 78%|███████▊  | 349764/450337 [00:56<00:16, 6050.49it/s] 78%|███████▊  | 350653/450337 [00:56<00:14, 6871.72it/s] 78%|███████▊  | 351345/450337 [00:56<00:15, 6542.12it/s] 78%|███████▊  | 352005/450337 [00:56<00:15, 6236.33it/s] 78%|███████▊  | 352635/450337 [00:56<00:17, 5747.02it/s] 78%|███████▊  | 353253/450337 [00:56<00:16, 5856.73it/s] 79%|███████▊  | 353847/450337 [00:57<00:16, 5814.43it/s] 79%|███████▊  | 354604/450337 [00:57<00:15, 6307.65it/s] 79%|███████▉  | 355242/450337 [00:57<00:16, 5921.62it/s] 79%|███████▉  | 355936/450337 [00:57<00:15, 6201.88it/s] 79%|███████▉  | 356565/450337 [00:57<00:15, 6105.13it/s] 79%|███████▉  | 357182/450337 [00:57<00:15, 5835.69it/s] 79%|███████▉  | 357818/450337 [00:57<00:15, 5968.83it/s] 80%|███████▉  | 358420/450337 [00:57<00:15, 5873.16it/s] 80%|███████▉  | 359061/450337 [00:57<00:15, 6022.52it/s] 80%|███████▉  | 359684/450337 [00:58<00:14, 6080.47it/s] 80%|████████  | 360351/450337 [00:58<00:14, 6250.89it/s] 80%|████████  | 361021/450337 [00:58<00:13, 6382.58it/s] 80%|████████  | 361705/450337 [00:58<00:13, 6516.37it/s] 80%|████████  | 362393/450337 [00:58<00:13, 6620.34it/s] 81%|████████  | 363065/450337 [00:58<00:13, 6649.91it/s] 81%|████████  | 363731/450337 [00:58<00:13, 6610.97it/s] 81%|████████  | 364436/450337 [00:58<00:12, 6741.17it/s] 81%|████████  | 365112/450337 [00:58<00:12, 6741.37it/s] 81%|████████  | 365787/450337 [00:58<00:12, 6595.34it/s] 81%|████████▏ | 366448/450337 [00:59<00:13, 6435.80it/s] 82%|████████▏ | 367123/450337 [00:59<00:12, 6525.54it/s] 82%|████████▏ | 367777/450337 [00:59<00:13, 6024.70it/s] 82%|████████▏ | 368388/450337 [00:59<00:13, 6043.87it/s] 82%|████████▏ | 369052/450337 [00:59<00:13, 6210.72it/s] 82%|████████▏ | 369684/450337 [00:59<00:12, 6238.78it/s] 82%|████████▏ | 370312/450337 [00:59<00:12, 6207.64it/s] 82%|████████▏ | 370936/450337 [00:59<00:12, 6117.78it/s] 83%|████████▎ | 371550/450337 [00:59<00:12, 6121.47it/s] 83%|████████▎ | 372164/450337 [00:59<00:13, 5954.00it/s] 83%|████████▎ | 372798/450337 [01:00<00:12, 6061.05it/s] 83%|████████▎ | 373433/450337 [01:00<00:12, 6145.52it/s] 83%|████████▎ | 374049/450337 [01:00<00:12, 6117.19it/s] 83%|████████▎ | 374767/450337 [01:00<00:11, 6417.87it/s] 83%|████████▎ | 375410/450337 [01:00<00:12, 6210.26it/s] 84%|████████▎ | 376033/450337 [01:00<00:12, 5956.12it/s] 84%|████████▎ | 376632/450337 [01:00<00:12, 5855.99it/s] 84%|████████▍ | 377220/450337 [01:00<00:12, 5851.72it/s] 84%|████████▍ | 377873/450337 [01:00<00:11, 6046.53it/s] 84%|████████▍ | 378483/450337 [01:01<00:11, 6058.88it/s] 84%|████████▍ | 379152/450337 [01:01<00:11, 6241.07it/s] 84%|████████▍ | 379827/450337 [01:01<00:11, 6385.00it/s] 84%|████████▍ | 380467/450337 [01:01<00:11, 6278.13it/s] 85%|████████▍ | 381229/450337 [01:01<00:10, 6671.31it/s] 85%|████████▍ | 381898/450337 [01:01<00:10, 6468.20it/s] 85%|████████▍ | 382547/450337 [01:01<00:11, 6084.24it/s] 85%|████████▌ | 383161/450337 [01:01<00:11, 6030.15it/s] 85%|████████▌ | 383768/450337 [01:01<00:11, 5947.44it/s] 85%|████████▌ | 384501/450337 [01:01<00:10, 6339.62it/s] 86%|████████▌ | 385139/450337 [01:02<00:10, 6268.06it/s] 86%|████████▌ | 385788/450337 [01:02<00:10, 6323.13it/s] 86%|████████▌ | 386423/450337 [01:02<00:10, 5960.99it/s] 86%|████████▌ | 387154/450337 [01:02<00:09, 6341.47it/s] 86%|████████▌ | 387900/450337 [01:02<00:09, 6662.18it/s] 86%|████████▋ | 388572/450337 [01:02<00:09, 6279.33it/s] 86%|████████▋ | 389240/450337 [01:02<00:09, 6386.70it/s] 87%|████████▋ | 389892/450337 [01:02<00:09, 6424.57it/s] 87%|████████▋ | 390539/450337 [01:02<00:09, 6334.76it/s] 87%|████████▋ | 391176/450337 [01:03<00:09, 6307.07it/s] 87%|████████▋ | 391809/450337 [01:03<00:09, 5968.57it/s] 87%|████████▋ | 392411/450337 [01:03<00:09, 5966.20it/s] 87%|████████▋ | 393011/450337 [01:03<00:09, 5884.62it/s] 87%|████████▋ | 393602/450337 [01:03<00:09, 5729.70it/s] 88%|████████▊ | 394310/450337 [01:03<00:09, 6114.58it/s] 88%|████████▊ | 394925/450337 [01:03<00:09, 6042.20it/s] 88%|████████▊ | 395576/450337 [01:03<00:08, 6175.94it/s] 88%|████████▊ | 396196/450337 [01:03<00:08, 6154.65it/s] 88%|████████▊ | 396813/450337 [01:03<00:08, 6019.56it/s] 88%|████████▊ | 397417/450337 [01:04<00:09, 5770.20it/s] 88%|████████▊ | 398091/450337 [01:04<00:08, 6044.19it/s] 89%|████████▊ | 398756/450337 [01:04<00:08, 6218.69it/s] 89%|████████▊ | 399422/450337 [01:04<00:08, 6347.32it/s] 89%|████████▉ | 400107/450337 [01:04<00:07, 6487.53it/s] 89%|████████▉ | 400758/450337 [01:04<00:07, 6477.58it/s] 89%|████████▉ | 401408/450337 [01:04<00:07, 6265.06it/s] 89%|████████▉ | 402037/450337 [01:04<00:08, 5794.29it/s] 89%|████████▉ | 402625/450337 [01:04<00:08, 5658.04it/s] 90%|████████▉ | 403239/450337 [01:05<00:08, 5790.43it/s] 90%|████████▉ | 403858/450337 [01:05<00:07, 5897.87it/s] 90%|████████▉ | 404489/450337 [01:05<00:07, 6010.87it/s] 90%|████████▉ | 405094/450337 [01:05<00:07, 5879.14it/s] 90%|█████████ | 405686/450337 [01:05<00:07, 5890.18it/s] 90%|█████████ | 406325/450337 [01:05<00:07, 6031.82it/s] 90%|█████████ | 406930/450337 [01:05<00:07, 5925.64it/s] 90%|█████████ | 407524/450337 [01:05<00:07, 5869.99it/s] 91%|█████████ | 408112/450337 [01:05<00:07, 5788.16it/s] 91%|█████████ | 408794/450337 [01:05<00:06, 6088.97it/s] 91%|█████████ | 409430/450337 [01:06<00:06, 6163.57it/s] 91%|█████████ | 410048/450337 [01:06<00:06, 6004.18it/s] 91%|█████████ | 410691/450337 [01:06<00:06, 6122.50it/s] 91%|█████████▏| 411305/450337 [01:06<00:06, 5994.64it/s] 91%|█████████▏| 411921/450337 [01:06<00:06, 6040.57it/s] 92%|█████████▏| 412527/450337 [01:06<00:06, 5503.68it/s] 92%|█████████▏| 413087/450337 [01:06<00:06, 5388.01it/s] 92%|█████████▏| 413786/450337 [01:06<00:06, 5830.93it/s] 92%|█████████▏| 414378/450337 [01:06<00:06, 5575.83it/s] 92%|█████████▏| 414943/450337 [01:07<00:06, 5446.38it/s] 92%|█████████▏| 415584/450337 [01:07<00:06, 5711.46it/s] 92%|█████████▏| 416161/450337 [01:07<00:06, 5690.49it/s] 93%|█████████▎| 416764/450337 [01:07<00:05, 5783.41it/s] 93%|█████████▎| 417356/450337 [01:07<00:05, 5823.08it/s] 93%|█████████▎| 418079/450337 [01:07<00:05, 6235.68it/s] 93%|█████████▎| 418705/450337 [01:07<00:05, 5868.94it/s] 93%|█████████▎| 419378/450337 [01:07<00:05, 6107.29it/s] 93%|█████████▎| 419995/450337 [01:07<00:05, 6063.04it/s] 93%|█████████▎| 420632/450337 [01:07<00:04, 6151.34it/s] 94%|█████████▎| 421250/450337 [01:08<00:05, 5755.97it/s] 94%|█████████▎| 421862/450337 [01:08<00:04, 5850.88it/s] 94%|█████████▍| 422512/450337 [01:08<00:04, 6032.35it/s] 94%|█████████▍| 423158/450337 [01:08<00:04, 6151.03it/s] 94%|█████████▍| 423777/450337 [01:08<00:04, 6115.12it/s] 94%|█████████▍| 424391/450337 [01:08<00:04, 6002.50it/s] 94%|█████████▍| 424994/450337 [01:08<00:04, 5913.99it/s] 95%|█████████▍| 425587/450337 [01:08<00:04, 5876.94it/s] 95%|█████████▍| 426365/450337 [01:08<00:03, 6431.75it/s] 95%|█████████▍| 427019/450337 [01:09<00:03, 6456.05it/s] 95%|█████████▍| 427667/450337 [01:09<00:03, 6375.98it/s] 95%|█████████▌| 428357/450337 [01:09<00:03, 6527.47it/s] 95%|█████████▌| 429011/450337 [01:09<00:03, 6227.51it/s] 95%|█████████▌| 429716/450337 [01:09<00:03, 6461.72it/s] 96%|█████████▌| 430366/450337 [01:09<00:03, 6445.77it/s] 96%|█████████▌| 431013/450337 [01:09<00:03, 6088.87it/s] 96%|█████████▌| 431628/450337 [01:09<00:03, 5949.99it/s] 96%|█████████▌| 432227/450337 [01:09<00:03, 5934.52it/s] 96%|█████████▌| 433126/450337 [01:09<00:02, 6809.54it/s] 96%|█████████▋| 433813/450337 [01:10<00:02, 6471.68it/s] 96%|█████████▋| 434468/450337 [01:10<00:02, 6084.47it/s] 97%|█████████▋| 435085/450337 [01:10<00:02, 5988.00it/s] 97%|█████████▋| 435762/450337 [01:10<00:02, 6204.92it/s] 97%|█████████▋| 436388/450337 [01:10<00:02, 5941.86it/s] 97%|█████████▋| 437024/450337 [01:10<00:02, 6057.74it/s] 97%|█████████▋| 437661/450337 [01:10<00:02, 6146.16it/s] 97%|█████████▋| 438280/450337 [01:10<00:01, 6093.93it/s] 97%|█████████▋| 438892/450337 [01:10<00:01, 5977.97it/s] 98%|█████████▊| 439492/450337 [01:11<00:01, 5920.96it/s] 98%|█████████▊| 440086/450337 [01:11<00:01, 5871.98it/s] 98%|█████████▊| 440675/450337 [01:11<00:01, 5740.32it/s] 98%|█████████▊| 441392/450337 [01:11<00:01, 6141.42it/s] 98%|█████████▊| 442017/450337 [01:11<00:01, 6171.63it/s] 98%|█████████▊| 442636/450337 [01:11<00:01, 6057.41it/s] 98%|█████████▊| 443310/450337 [01:11<00:01, 6251.46it/s] 99%|█████████▊| 443960/450337 [01:11<00:01, 6324.40it/s] 99%|█████████▊| 444594/450337 [01:11<00:00, 5906.07it/s] 99%|█████████▉| 445191/450337 [01:12<00:00, 5710.66it/s] 99%|█████████▉| 445781/450337 [01:12<00:00, 5761.89it/s] 99%|█████████▉| 446361/450337 [01:12<00:00, 5758.83it/s] 99%|█████████▉| 446985/450337 [01:12<00:00, 5892.92it/s] 99%|█████████▉| 447639/450337 [01:12<00:00, 6076.26it/s]100%|█████████▉| 448249/450337 [01:12<00:00, 6075.04it/s]100%|█████████▉| 448902/450337 [01:12<00:00, 6208.85it/s]100%|█████████▉| 449525/450337 [01:12<00:00, 5989.82it/s]100%|█████████▉| 450165/450337 [01:12<00:00, 6106.23it/s]100%|██████████| 450337/450337 [01:12<00:00, 6181.77it/s]

gathering stats for n=1
  0%|          | 0/450337 [00:00<?, ?it/s]  0%|          | 1877/450337 [00:00<00:23, 18755.26it/s]  1%|          | 3863/450337 [00:00<00:23, 19397.10it/s]  1%|▏         | 6084/450337 [00:00<00:21, 20672.44it/s]  2%|▏         | 8152/450337 [00:00<00:22, 19748.39it/s]  2%|▏         | 10156/450337 [00:00<00:22, 19850.11it/s]  3%|▎         | 12147/450337 [00:00<00:22, 19860.34it/s]  3%|▎         | 14156/450337 [00:00<00:21, 19929.53it/s]  4%|▎         | 16151/450337 [00:00<00:21, 19829.09it/s]  4%|▍         | 18136/450337 [00:00<00:21, 19772.99it/s]  4%|▍         | 20241/450337 [00:01<00:21, 20152.91it/s]  5%|▍         | 22258/450337 [00:01<00:21, 20002.48it/s]  5%|▌         | 24516/450337 [00:01<00:20, 20776.62it/s]  6%|▌         | 26595/450337 [00:01<00:20, 20511.48it/s]  6%|▋         | 28648/450337 [00:01<00:20, 20312.61it/s]  7%|▋         | 30681/450337 [00:01<00:21, 19829.86it/s]  7%|▋         | 32667/450337 [00:01<00:21, 19575.44it/s]  8%|▊         | 34662/450337 [00:01<00:21, 19673.34it/s]  8%|▊         | 36638/450337 [00:01<00:21, 19695.46it/s]  9%|▊         | 38609/450337 [00:01<00:21, 19259.67it/s]  9%|▉         | 40655/450337 [00:02<00:20, 19610.30it/s]  9%|▉         | 42619/450337 [00:02<00:21, 19272.07it/s] 10%|▉         | 44677/450337 [00:02<00:20, 19654.30it/s] 10%|█         | 46745/450337 [00:02<00:20, 19894.84it/s] 11%|█         | 49224/450337 [00:02<00:18, 21341.70it/s] 11%|█▏        | 51362/450337 [00:02<00:19, 20657.06it/s] 12%|█▏        | 53572/450337 [00:02<00:18, 21072.36it/s] 12%|█▏        | 55686/450337 [00:02<00:19, 20739.78it/s] 13%|█▎        | 57765/450337 [00:02<00:19, 20158.40it/s] 13%|█▎        | 59819/450337 [00:02<00:19, 20265.11it/s] 14%|█▍        | 61928/450337 [00:03<00:18, 20504.26it/s] 14%|█▍        | 63983/450337 [00:03<00:18, 20505.96it/s] 15%|█▍        | 66037/450337 [00:03<00:18, 20287.06it/s] 15%|█▌        | 68068/450337 [00:03<00:19, 19938.08it/s] 16%|█▌        | 70242/450337 [00:03<00:18, 20459.94it/s] 16%|█▌        | 72291/450337 [00:03<00:18, 20280.54it/s] 17%|█▋        | 74686/450337 [00:03<00:17, 21359.34it/s] 17%|█▋        | 76826/450337 [00:03<00:17, 21019.61it/s] 18%|█▊        | 78932/450337 [00:03<00:18, 20301.16it/s] 18%|█▊        | 80969/450337 [00:04<00:18, 20124.68it/s] 18%|█▊        | 83042/450337 [00:04<00:18, 20288.51it/s] 19%|█▉        | 85115/450337 [00:04<00:17, 20406.39it/s] 19%|█▉        | 87247/450337 [00:04<00:17, 20674.77it/s] 20%|█▉        | 89317/450337 [00:04<00:17, 20225.72it/s] 20%|██        | 91343/450337 [00:04<00:17, 20030.76it/s] 21%|██        | 93375/450337 [00:04<00:17, 20114.96it/s] 21%|██        | 95446/450337 [00:04<00:17, 20288.41it/s] 22%|██▏       | 97584/450337 [00:04<00:17, 20606.26it/s] 22%|██▏       | 99647/450337 [00:04<00:17, 20365.67it/s] 23%|██▎       | 101686/450337 [00:05<00:17, 20178.23it/s] 23%|██▎       | 103706/450337 [00:05<00:17, 20078.90it/s] 23%|██▎       | 105715/450337 [00:05<00:17, 20061.67it/s] 24%|██▍       | 107722/450337 [00:05<00:17, 19930.06it/s] 24%|██▍       | 109716/450337 [00:05<00:17, 19633.94it/s] 25%|██▍       | 111813/450337 [00:05<00:16, 20026.09it/s] 25%|██▌       | 113818/450337 [00:05<00:16, 19999.86it/s] 26%|██▌       | 115819/450337 [00:05<00:16, 19741.09it/s] 26%|██▌       | 117795/450337 [00:05<00:17, 19534.18it/s] 27%|██▋       | 119799/450337 [00:05<00:16, 19674.45it/s] 27%|██▋       | 121869/450337 [00:06<00:16, 19977.29it/s] 28%|██▊       | 123868/450337 [00:06<00:16, 19892.05it/s] 28%|██▊       | 125904/450337 [00:06<00:16, 20025.96it/s] 28%|██▊       | 127985/450337 [00:06<00:15, 20258.99it/s] 29%|██▉       | 130012/450337 [00:06<00:16, 19949.90it/s] 29%|██▉       | 132009/450337 [00:06<00:16, 19362.20it/s] 30%|██▉       | 134206/450337 [00:06<00:15, 20115.14it/s] 30%|███       | 136437/450337 [00:06<00:15, 20757.55it/s] 31%|███       | 138617/450337 [00:06<00:14, 21058.01it/s] 31%|███       | 140727/450337 [00:06<00:14, 20743.47it/s] 32%|███▏      | 142805/450337 [00:07<00:15, 20169.74it/s] 32%|███▏      | 145030/450337 [00:07<00:14, 20772.96it/s] 33%|███▎      | 147113/450337 [00:07<00:14, 20411.68it/s] 33%|███▎      | 149159/450337 [00:07<00:15, 20075.25it/s] 34%|███▎      | 151171/450337 [00:07<00:15, 19484.56it/s] 34%|███▍      | 153247/450337 [00:07<00:14, 19850.25it/s] 34%|███▍      | 155238/450337 [00:07<00:14, 19694.23it/s] 35%|███▍      | 157341/450337 [00:07<00:14, 20067.53it/s] 35%|███▌      | 159407/450337 [00:07<00:14, 20237.30it/s] 36%|███▌      | 161467/450337 [00:08<00:14, 20343.06it/s] 36%|███▋      | 163504/450337 [00:08<00:14, 20316.78it/s] 37%|███▋      | 165639/450337 [00:08<00:13, 20623.57it/s] 37%|███▋      | 167703/450337 [00:08<00:13, 20573.21it/s] 38%|███▊      | 169762/450337 [00:08<00:13, 20265.19it/s] 38%|███▊      | 171791/450337 [00:08<00:13, 19939.00it/s] 39%|███▊      | 173787/450337 [00:08<00:13, 19887.43it/s] 39%|███▉      | 175777/450337 [00:08<00:13, 19650.54it/s] 39%|███▉      | 177744/450337 [00:08<00:14, 19031.10it/s] 40%|███▉      | 179799/450337 [00:08<00:13, 19465.12it/s] 40%|████      | 181750/450337 [00:09<00:13, 19326.13it/s] 41%|████      | 183686/450337 [00:09<00:13, 19228.07it/s] 41%|████      | 185611/450337 [00:09<00:14, 18895.70it/s] 42%|████▏     | 187626/450337 [00:09<00:13, 19254.44it/s] 42%|████▏     | 189664/450337 [00:09<00:13, 19576.01it/s] 43%|████▎     | 191624/450337 [00:09<00:13, 19555.53it/s] 43%|████▎     | 193670/450337 [00:09<00:12, 19822.50it/s] 43%|████▎     | 195703/450337 [00:09<00:12, 19967.57it/s] 44%|████▍     | 197930/450337 [00:09<00:12, 20652.46it/s] 44%|████▍     | 199997/450337 [00:09<00:12, 20187.68it/s] 45%|████▍     | 202019/450337 [00:10<00:12, 19802.40it/s] 45%|████▌     | 204136/450337 [00:10<00:12, 20200.99it/s] 46%|████▌     | 206160/450337 [00:10<00:12, 19865.81it/s] 46%|████▌     | 208150/450337 [00:10<00:12, 19725.27it/s] 47%|████▋     | 210125/450337 [00:10<00:12, 19552.57it/s] 47%|████▋     | 212132/450337 [00:10<00:12, 19703.35it/s] 48%|████▊     | 214146/450337 [00:10<00:11, 19829.31it/s] 48%|████▊     | 216131/450337 [00:10<00:11, 19522.40it/s] 48%|████▊     | 218118/450337 [00:10<00:11, 19623.67it/s] 49%|████▉     | 220139/450337 [00:10<00:11, 19792.83it/s] 49%|████▉     | 222120/450337 [00:11<00:11, 19588.43it/s] 50%|████▉     | 224080/450337 [00:11<00:11, 19293.42it/s] 50%|█████     | 226343/450337 [00:11<00:11, 20269.15it/s] 51%|█████     | 228373/450337 [00:11<00:11, 19730.97it/s] 51%|█████     | 230351/450337 [00:11<00:11, 19298.52it/s] 52%|█████▏    | 232286/450337 [00:11<00:11, 19285.24it/s] 52%|█████▏    | 234550/450337 [00:11<00:10, 20261.75it/s] 53%|█████▎    | 236581/450337 [00:11<00:10, 20044.59it/s] 53%|█████▎    | 238666/450337 [00:11<00:10, 20279.86it/s] 53%|█████▎    | 240751/450337 [00:12<00:10, 20447.12it/s] 54%|█████▍    | 242816/450337 [00:12<00:10, 20504.64it/s] 54%|█████▍    | 244869/450337 [00:12<00:10, 20392.86it/s] 55%|█████▍    | 247123/450337 [00:12<00:09, 21030.00it/s] 55%|█████▌    | 249228/450337 [00:12<00:09, 20818.20it/s] 56%|█████▌    | 251329/450337 [00:12<00:09, 20874.56it/s] 56%|█████▋    | 253418/450337 [00:12<00:09, 20564.67it/s] 57%|█████▋    | 255477/450337 [00:12<00:09, 20517.36it/s] 57%|█████▋    | 257682/450337 [00:12<00:09, 20961.01it/s] 58%|█████▊    | 259780/450337 [00:12<00:09, 19869.35it/s] 58%|█████▊    | 261936/450337 [00:13<00:09, 20351.41it/s] 59%|█████▊    | 263982/450337 [00:13<00:09, 19806.79it/s] 59%|█████▉    | 265972/450337 [00:13<00:09, 19450.49it/s] 59%|█████▉    | 267929/450337 [00:13<00:09, 19477.87it/s] 60%|█████▉    | 269915/450337 [00:13<00:09, 19586.77it/s] 60%|██████    | 272010/450337 [00:13<00:08, 19983.12it/s] 61%|██████    | 274220/450337 [00:13<00:08, 20600.07it/s] 61%|██████▏   | 276284/450337 [00:13<00:08, 19880.08it/s] 62%|██████▏   | 278280/450337 [00:13<00:08, 19811.18it/s] 62%|██████▏   | 280388/450337 [00:13<00:08, 20177.86it/s] 63%|██████▎   | 282411/450337 [00:14<00:08, 19603.59it/s] 63%|██████▎   | 284488/450337 [00:14<00:08, 19939.07it/s] 64%|██████▎   | 286488/450337 [00:14<00:08, 19798.51it/s] 64%|██████▍   | 288562/450337 [00:14<00:08, 20074.41it/s] 65%|██████▍   | 290573/450337 [00:14<00:08, 19750.27it/s] 65%|██████▍   | 292583/450337 [00:14<00:07, 19848.69it/s] 65%|██████▌   | 294571/450337 [00:14<00:07, 19818.50it/s] 66%|██████▌   | 296612/450337 [00:14<00:07, 19992.18it/s] 66%|██████▋   | 298623/450337 [00:14<00:07, 20025.18it/s] 67%|██████▋   | 300702/450337 [00:14<00:07, 20246.60it/s] 67%|██████▋   | 302728/450337 [00:15<00:07, 20026.13it/s] 68%|██████▊   | 304767/450337 [00:15<00:07, 20130.71it/s] 68%|██████▊   | 306781/450337 [00:15<00:07, 19796.12it/s] 69%|██████▊   | 308763/450337 [00:15<00:07, 19602.69it/s] 69%|██████▉   | 311024/450337 [00:15<00:06, 20481.34it/s] 70%|██████▉   | 313089/450337 [00:15<00:06, 20529.17it/s] 70%|██████▉   | 315144/450337 [00:15<00:06, 20357.25it/s] 70%|███████   | 317182/450337 [00:15<00:06, 20127.72it/s] 71%|███████   | 319197/450337 [00:15<00:06, 19854.57it/s] 71%|███████▏  | 321243/450337 [00:16<00:06, 20027.54it/s] 72%|███████▏  | 323248/450337 [00:16<00:06, 19708.62it/s] 72%|███████▏  | 325221/450337 [00:16<00:06, 19449.28it/s] 73%|███████▎  | 327318/450337 [00:16<00:06, 19889.23it/s] 73%|███████▎  | 329477/450337 [00:16<00:05, 20388.13it/s] 74%|███████▎  | 331519/450337 [00:16<00:06, 19762.06it/s] 74%|███████▍  | 333543/450337 [00:16<00:05, 19888.60it/s] 75%|███████▍  | 335577/450337 [00:16<00:05, 20019.68it/s] 75%|███████▍  | 337696/450337 [00:16<00:05, 20364.13it/s] 75%|███████▌  | 339736/450337 [00:16<00:05, 20298.06it/s] 76%|███████▌  | 341768/450337 [00:17<00:05, 19849.93it/s] 76%|███████▋  | 343924/450337 [00:17<00:05, 20345.62it/s] 77%|███████▋  | 345962/450337 [00:17<00:05, 19655.23it/s] 77%|███████▋  | 348001/450337 [00:17<00:05, 19862.13it/s] 78%|███████▊  | 350164/450337 [00:17<00:04, 20378.19it/s] 78%|███████▊  | 352207/450337 [00:17<00:04, 19865.62it/s] 79%|███████▊  | 354200/450337 [00:17<00:04, 19620.27it/s] 79%|███████▉  | 356242/450337 [00:17<00:04, 19851.83it/s] 80%|███████▉  | 358231/450337 [00:17<00:04, 19503.43it/s] 80%|████████  | 360275/450337 [00:17<00:04, 19776.37it/s] 80%|████████  | 362474/450337 [00:18<00:04, 20421.16it/s] 81%|████████  | 364563/450337 [00:18<00:04, 20559.46it/s] 81%|████████▏ | 366622/450337 [00:18<00:04, 20474.82it/s] 82%|████████▏ | 368672/450337 [00:18<00:04, 20097.23it/s] 82%|████████▏ | 370685/450337 [00:18<00:03, 20075.72it/s] 83%|████████▎ | 372695/450337 [00:18<00:03, 19985.51it/s] 83%|████████▎ | 374778/450337 [00:18<00:03, 20234.95it/s] 84%|████████▎ | 376803/450337 [00:18<00:03, 19520.55it/s] 84%|████████▍ | 378826/450337 [00:18<00:03, 19720.72it/s] 85%|████████▍ | 381035/450337 [00:19<00:03, 20413.89it/s] 85%|████████▌ | 383082/450337 [00:19<00:03, 19728.40it/s] 86%|████████▌ | 385172/450337 [00:19<00:03, 20064.33it/s] 86%|████████▌ | 387185/450337 [00:19<00:03, 20082.74it/s] 86%|████████▋ | 389264/450337 [00:19<00:03, 20290.38it/s] 87%|████████▋ | 391297/450337 [00:19<00:02, 20094.74it/s] 87%|████████▋ | 393310/450337 [00:19<00:02, 19515.72it/s] 88%|████████▊ | 395341/450337 [00:19<00:02, 19738.75it/s] 88%|████████▊ | 397319/450337 [00:19<00:02, 19428.53it/s] 89%|████████▊ | 399517/450337 [00:19<00:02, 20169.27it/s] 89%|████████▉ | 401539/450337 [00:20<00:02, 20111.74it/s] 90%|████████▉ | 403554/450337 [00:20<00:02, 19618.54it/s] 90%|█████████ | 405520/450337 [00:20<00:02, 19448.81it/s] 90%|█████████ | 407469/450337 [00:20<00:02, 19456.84it/s] 91%|█████████ | 409447/450337 [00:20<00:02, 19547.42it/s] 91%|█████████▏| 411404/450337 [00:20<00:01, 19470.56it/s] 92%|█████████▏| 413353/450337 [00:20<00:01, 18930.61it/s] 92%|█████████▏| 415250/450337 [00:20<00:01, 18668.55it/s] 93%|█████████▎| 417156/450337 [00:20<00:01, 18780.53it/s] 93%|█████████▎| 419175/450337 [00:20<00:01, 19190.81it/s] 94%|█████████▎| 421100/450337 [00:21<00:01, 19203.34it/s] 94%|█████████▍| 423073/450337 [00:21<00:01, 19359.35it/s] 94%|█████████▍| 425011/450337 [00:21<00:01, 19187.91it/s] 95%|█████████▍| 427135/450337 [00:21<00:01, 19795.92it/s] 95%|█████████▌| 429173/450337 [00:21<00:01, 19969.37it/s] 96%|█████████▌| 431172/450337 [00:21<00:00, 19747.51it/s] 96%|█████████▌| 433308/450337 [00:21<00:00, 20224.97it/s] 97%|█████████▋| 435332/450337 [00:21<00:00, 19651.82it/s] 97%|█████████▋| 437372/450337 [00:21<00:00, 19870.21it/s] 98%|█████████▊| 439363/450337 [00:21<00:00, 19597.62it/s] 98%|█████████▊| 441326/450337 [00:22<00:00, 19517.14it/s] 98%|█████████▊| 443344/450337 [00:22<00:00, 19699.18it/s] 99%|█████████▉| 445316/450337 [00:22<00:00, 19238.67it/s] 99%|█████████▉| 447329/450337 [00:22<00:00, 19498.81it/s]100%|█████████▉| 449338/450337 [00:22<00:00, 19668.01it/s]100%|██████████| 450337/450337 [00:22<00:00, 19971.34it/s]

transferring to GPU memory
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 12.04it/s]2022-03-03 20:39:36 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(291888, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=291888, bias=False)
  )
)
2022-03-03 20:39:36 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-03 20:39:36 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-03 20:39:36 | INFO | fairseq_cli.train | criterion: JelinekMercerSmoothingCriterion
2022-03-03 20:39:36 | INFO | fairseq_cli.train | num. shared model params: 168,360,960 (num. trained: 168,360,960)
2022-03-03 20:39:36 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-03 20:39:36 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.25/valid
2022-03-03 20:39:37 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-03 20:39:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 20:39:37 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = NVIDIA TITAN RTX                        
2022-03-03 20:39:37 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-03 20:39:37 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-03 20:39:37 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None
2022-03-03 20:39:37 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-03 20:39:37 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_last.pt
2022-03-03 20:39:37 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-03 20:39:37 | INFO | fairseq.data.data_utils | loaded 450,337 examples from: data-bin/wikitext-103-raw-size-0.25/train
2022-03-03 20:39:37 | INFO | fairseq.trainer | begin training epoch 1
2022-03-03 20:39:37 | INFO | fairseq_cli.train | Start iterating over samples

2022-03-03 20:39:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-03 20:39:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 20:39:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 20:40:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-03 20:41:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-03 20:47:39 | INFO | train_inner | epoch 001:    105 / 393 loss=16.919, ppl=123901, wps=14525.2, ups=0.22, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.684, loss_scale=4, train_wall=477, gb_free=10.1, wall=482
2022-03-03 20:55:05 | INFO | train_inner | epoch 001:    205 / 393 loss=14.392, ppl=21493, wps=14664.2, ups=0.22, wpb=65536, bsz=128, num_updates=200, lr=2.5095e-05, gnorm=1.594, loss_scale=4, train_wall=442, gb_free=10.1, wall=929
2022-03-03 21:02:32 | INFO | train_inner | epoch 001:    305 / 393 loss=12.32, ppl=5113.86, wps=14663.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=1.029, loss_scale=4, train_wall=442, gb_free=10.1, wall=1376
2022-03-03 21:09:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:09:10 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 10.478 | ppl 1425.93 | wps 34223 | wpb 2034.1 | bsz 4 | num_updates 388
2022-03-03 21:09:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 388 updates
2022-03-03 21:09:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-03 21:09:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-03 21:09:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 1 @ 388 updates, score 10.478) (writing took 4.839434223249555 seconds)
2022-03-03 21:09:15 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-03 21:09:15 | INFO | train | epoch 001 | loss 13.718 | ppl 13475.3 | wps 14536.3 | ups 0.22 | wpb 65460.6 | bsz 127.9 | num_updates 388 | lr 4.85903e-05 | gnorm 1.76 | loss_scale 4 | train_wall 1747 | gb_free 10.1 | wall 1778
2022-03-03 21:09:15 | INFO | fairseq.trainer | begin training epoch 2
2022-03-03 21:09:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:10:08 | INFO | train_inner | epoch 002:     12 / 393 loss=10.839, ppl=1832.32, wps=14310.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=400, lr=5.009e-05, gnorm=0.586, loss_scale=4, train_wall=440, gb_free=10.1, wall=1832
2022-03-03 21:17:34 | INFO | train_inner | epoch 002:    112 / 393 loss=10.26, ppl=1226.13, wps=14688.1, ups=0.22, wpb=65530.2, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.46, loss_scale=4, train_wall=441, gb_free=10.1, wall=2278
2022-03-03 21:25:01 | INFO | train_inner | epoch 002:    212 / 393 loss=9.982, ppl=1011.41, wps=14685.5, ups=0.22, wpb=65536, bsz=128, num_updates=600, lr=7.5085e-05, gnorm=0.513, loss_scale=8, train_wall=441, gb_free=10.1, wall=2724
2022-03-03 21:32:27 | INFO | train_inner | epoch 002:    312 / 393 loss=9.753, ppl=863.13, wps=14684.3, ups=0.22, wpb=65536, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.557, loss_scale=8, train_wall=441, gb_free=10.1, wall=3170
2022-03-03 21:38:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 21:38:33 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.464 | ppl 706.39 | wps 33900.1 | wpb 2034.1 | bsz 4 | num_updates 781 | best_loss 9.464
2022-03-03 21:38:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 781 updates
2022-03-03 21:38:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-03 21:38:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-03 21:38:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 2 @ 781 updates, score 9.464) (writing took 4.518610150553286 seconds)
2022-03-03 21:38:37 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-03 21:38:37 | INFO | train | epoch 002 | loss 9.925 | ppl 972.36 | wps 14593.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 781 | lr 9.77055e-05 | gnorm 0.528 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 3541
2022-03-03 21:38:37 | INFO | fairseq.trainer | begin training epoch 3
2022-03-03 21:38:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 21:40:02 | INFO | train_inner | epoch 003:     19 / 393 loss=9.547, ppl=747.99, wps=14332.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=800, lr=0.00010008, gnorm=0.621, loss_scale=8, train_wall=439, gb_free=10.1, wall=3626
2022-03-03 21:47:28 | INFO | train_inner | epoch 003:    119 / 393 loss=9.338, ppl=646.99, wps=14687.6, ups=0.22, wpb=65536, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.683, loss_scale=8, train_wall=441, gb_free=10.1, wall=4072
2022-03-03 21:54:55 | INFO | train_inner | epoch 003:    219 / 393 loss=9.173, ppl=577.25, wps=14682, ups=0.22, wpb=65536, bsz=128, num_updates=1000, lr=0.000125075, gnorm=0.817, loss_scale=8, train_wall=441, gb_free=10.1, wall=4518
2022-03-03 22:02:21 | INFO | train_inner | epoch 003:    319 / 393 loss=9.03, ppl=522.77, wps=14678.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.758, loss_scale=16, train_wall=442, gb_free=10.1, wall=4965
2022-03-03 22:07:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:07:56 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.832 | ppl 455.82 | wps 34012.2 | wpb 2034.1 | bsz 4 | num_updates 1174 | best_loss 8.832
2022-03-03 22:07:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 1174 updates
2022-03-03 22:07:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-03 22:08:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-03 22:08:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 3 @ 1174 updates, score 8.832) (writing took 4.458670625463128 seconds)
2022-03-03 22:08:01 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-03 22:08:01 | INFO | train | epoch 003 | loss 9.141 | ppl 564.5 | wps 14591.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1174 | lr 0.000146821 | gnorm 0.765 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 5304
2022-03-03 22:08:01 | INFO | fairseq.trainer | begin training epoch 4
2022-03-03 22:08:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:09:57 | INFO | train_inner | epoch 004:     26 / 393 loss=8.871, ppl=468.15, wps=14327.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1200, lr=0.00015007, gnorm=0.83, loss_scale=16, train_wall=440, gb_free=10.1, wall=5420
2022-03-03 22:17:23 | INFO | train_inner | epoch 004:    126 / 393 loss=8.722, ppl=422.32, wps=14677.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.8, loss_scale=16, train_wall=442, gb_free=10.1, wall=5867
2022-03-03 22:24:50 | INFO | train_inner | epoch 004:    226 / 393 loss=8.607, ppl=389.95, wps=14671.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=1400, lr=0.000175065, gnorm=0.789, loss_scale=16, train_wall=442, gb_free=10.1, wall=6313
2022-03-03 22:32:17 | INFO | train_inner | epoch 004:    326 / 393 loss=8.509, ppl=364.33, wps=14666.3, ups=0.22, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.825, loss_scale=16, train_wall=442, gb_free=10.1, wall=6760
2022-03-03 22:37:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 22:37:20 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.371 | ppl 331.17 | wps 33980.8 | wpb 2034.1 | bsz 4 | num_updates 1567 | best_loss 8.371
2022-03-03 22:37:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 1567 updates
2022-03-03 22:37:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-03 22:37:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-03 22:37:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 4 @ 1567 updates, score 8.371) (writing took 4.528162096627057 seconds)
2022-03-03 22:37:25 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-03 22:37:25 | INFO | train | epoch 004 | loss 8.593 | ppl 386.23 | wps 14580.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1567 | lr 0.000195936 | gnorm 0.813 | loss_scale 32 | train_wall 1734 | gb_free 10.1 | wall 7068
2022-03-03 22:37:25 | INFO | fairseq.trainer | begin training epoch 5
2022-03-03 22:37:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 22:39:52 | INFO | train_inner | epoch 005:     33 / 393 loss=8.39, ppl=335.47, wps=14318.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=1600, lr=0.00020006, gnorm=0.834, loss_scale=32, train_wall=440, gb_free=10.1, wall=7216
2022-03-03 22:47:19 | INFO | train_inner | epoch 005:    133 / 393 loss=8.258, ppl=306.18, wps=14669.4, ups=0.22, wpb=65536, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.829, loss_scale=32, train_wall=442, gb_free=10.1, wall=7663
2022-03-03 22:54:46 | INFO | train_inner | epoch 005:    233 / 393 loss=8.173, ppl=288.66, wps=14667.3, ups=0.22, wpb=65536, bsz=128, num_updates=1800, lr=0.000225055, gnorm=0.792, loss_scale=32, train_wall=442, gb_free=10.1, wall=8109
2022-03-03 23:02:13 | INFO | train_inner | epoch 005:    333 / 393 loss=8.083, ppl=271.17, wps=14668.3, ups=0.22, wpb=65536, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.797, loss_scale=32, train_wall=442, gb_free=10.1, wall=8556
2022-03-03 23:06:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:06:45 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.012 | ppl 258.06 | wps 33981.4 | wpb 2034.1 | bsz 4 | num_updates 1960 | best_loss 8.012
2022-03-03 23:06:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 1960 updates
2022-03-03 23:06:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-03 23:06:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-03 23:06:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 5 @ 1960 updates, score 8.012) (writing took 4.509620551019907 seconds)
2022-03-03 23:06:50 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-03 23:06:50 | INFO | train | epoch 005 | loss 8.16 | ppl 286.05 | wps 14578 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 1960 | lr 0.000245051 | gnorm 0.802 | loss_scale 32 | train_wall 1735 | gb_free 10.1 | wall 8833
2022-03-03 23:06:50 | INFO | fairseq.trainer | begin training epoch 6
2022-03-03 23:06:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:09:48 | INFO | train_inner | epoch 006:     40 / 393 loss=7.974, ppl=251.35, wps=14318.3, ups=0.22, wpb=65238.4, bsz=127.4, num_updates=2000, lr=0.00025005, gnorm=0.773, loss_scale=32, train_wall=440, gb_free=10.1, wall=9012
2022-03-03 23:14:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-03 23:17:20 | INFO | train_inner | epoch 006:    141 / 393 loss=7.865, ppl=233.1, wps=14524.7, ups=0.22, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.773, loss_scale=32, train_wall=446, gb_free=10.1, wall=9463
2022-03-03 23:18:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-03 23:24:51 | INFO | train_inner | epoch 006:    242 / 393 loss=7.802, ppl=223.18, wps=14526.8, ups=0.22, wpb=65536, bsz=128, num_updates=2200, lr=0.000275045, gnorm=0.759, loss_scale=16, train_wall=446, gb_free=10.1, wall=9914
2022-03-03 23:32:17 | INFO | train_inner | epoch 006:    342 / 393 loss=7.735, ppl=213.06, wps=14670.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.754, loss_scale=16, train_wall=442, gb_free=10.1, wall=10361
2022-03-03 23:36:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-03 23:36:10 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 7.743 | ppl 214.24 | wps 33999.8 | wpb 2034.1 | bsz 4 | num_updates 2351 | best_loss 7.743
2022-03-03 23:36:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 2351 updates
2022-03-03 23:36:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-03 23:36:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-03 23:36:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 6 @ 2351 updates, score 7.743) (writing took 4.519367036409676 seconds)
2022-03-03 23:36:14 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-03 23:36:14 | INFO | train | epoch 006 | loss 7.797 | ppl 222.34 | wps 14505.1 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 2351 | lr 0.000293916 | gnorm 0.764 | loss_scale 16 | train_wall 1734 | gb_free 10.1 | wall 10598
2022-03-03 23:36:14 | INFO | fairseq.trainer | begin training epoch 7
2022-03-03 23:36:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-03 23:39:53 | INFO | train_inner | epoch 007:     49 / 393 loss=7.627, ppl=197.67, wps=14316.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2400, lr=0.00030004, gnorm=0.744, loss_scale=16, train_wall=440, gb_free=10.1, wall=10817
2022-03-03 23:47:20 | INFO | train_inner | epoch 007:    149 / 393 loss=7.534, ppl=185.34, wps=14669.3, ups=0.22, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.733, loss_scale=16, train_wall=442, gb_free=10.1, wall=11263
2022-03-03 23:54:47 | INFO | train_inner | epoch 007:    249 / 393 loss=7.489, ppl=179.68, wps=14672.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=2600, lr=0.000325035, gnorm=0.728, loss_scale=16, train_wall=442, gb_free=10.1, wall=11710
2022-03-03 23:59:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 00:02:18 | INFO | train_inner | epoch 007:    350 / 393 loss=7.449, ppl=174.72, wps=14525.5, ups=0.22, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.726, loss_scale=16, train_wall=446, gb_free=10.1, wall=12161
2022-03-04 00:05:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:05:34 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.523 | ppl 183.89 | wps 34135.2 | wpb 2034.1 | bsz 4 | num_updates 2743 | best_loss 7.523
2022-03-04 00:05:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 2743 updates
2022-03-04 00:05:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 00:05:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 00:05:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 7 @ 2743 updates, score 7.523) (writing took 4.446388114243746 seconds)
2022-03-04 00:05:39 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-04 00:05:39 | INFO | train | epoch 007 | loss 7.492 | ppl 180.04 | wps 14543.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 2743 | lr 0.000342906 | gnorm 0.725 | loss_scale 16 | train_wall 1734 | gb_free 10.1 | wall 12362
2022-03-04 00:05:39 | INFO | fairseq.trainer | begin training epoch 8
2022-03-04 00:05:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:09:53 | INFO | train_inner | epoch 008:     57 / 393 loss=7.337, ppl=161.7, wps=14323.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=2800, lr=0.00035003, gnorm=0.712, loss_scale=16, train_wall=440, gb_free=10.1, wall=12617
2022-03-04 00:17:20 | INFO | train_inner | epoch 008:    157 / 393 loss=7.263, ppl=153.64, wps=14670.5, ups=0.22, wpb=65536, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.699, loss_scale=16, train_wall=442, gb_free=10.1, wall=13063
2022-03-04 00:24:47 | INFO | train_inner | epoch 008:    257 / 393 loss=7.239, ppl=151.02, wps=14672.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=3000, lr=0.000375025, gnorm=0.725, loss_scale=16, train_wall=442, gb_free=10.1, wall=13510
2022-03-04 00:32:13 | INFO | train_inner | epoch 008:    357 / 393 loss=7.204, ppl=147.48, wps=14674.7, ups=0.22, wpb=65536, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.683, loss_scale=16, train_wall=442, gb_free=10.1, wall=13957
2022-03-04 00:34:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 00:34:58 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.359 | ppl 164.14 | wps 33991.8 | wpb 2034.1 | bsz 4 | num_updates 3136 | best_loss 7.359
2022-03-04 00:34:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 3136 updates
2022-03-04 00:34:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 00:35:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 00:35:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 8 @ 3136 updates, score 7.359) (writing took 4.423092160373926 seconds)
2022-03-04 00:35:03 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-04 00:35:03 | INFO | train | epoch 008 | loss 7.236 | ppl 150.75 | wps 14582.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 3136 | lr 0.000392022 | gnorm 0.707 | loss_scale 16 | train_wall 1734 | gb_free 10.1 | wall 14126
2022-03-04 00:35:03 | INFO | fairseq.trainer | begin training epoch 9
2022-03-04 00:35:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 00:39:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 00:39:53 | INFO | train_inner | epoch 009:     65 / 393 loss=7.086, ppl=135.9, wps=14184.4, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=3200, lr=0.00040002, gnorm=0.702, loss_scale=16, train_wall=444, gb_free=10.1, wall=14417
2022-03-04 00:47:20 | INFO | train_inner | epoch 009:    165 / 393 loss=7.039, ppl=131.49, wps=14676.6, ups=0.22, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.684, loss_scale=16, train_wall=442, gb_free=10.1, wall=14863
2022-03-04 00:54:46 | INFO | train_inner | epoch 009:    265 / 393 loss=7.025, ppl=130.24, wps=14675.6, ups=0.22, wpb=65536, bsz=128, num_updates=3400, lr=0.000425015, gnorm=0.681, loss_scale=16, train_wall=442, gb_free=10.1, wall=15310
2022-03-04 01:02:13 | INFO | train_inner | epoch 009:    365 / 393 loss=6.999, ppl=127.94, wps=14666.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.672, loss_scale=16, train_wall=442, gb_free=10.1, wall=15757
2022-03-04 01:04:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:04:23 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.225 | ppl 149.56 | wps 34128.5 | wpb 2034.1 | bsz 4 | num_updates 3528 | best_loss 7.225
2022-03-04 01:04:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 3528 updates
2022-03-04 01:04:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 01:04:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 01:04:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 9 @ 3528 updates, score 7.225) (writing took 4.393116503953934 seconds)
2022-03-04 01:04:27 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-04 01:04:27 | INFO | train | epoch 009 | loss 7.022 | ppl 129.95 | wps 14545.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3528 | lr 0.000441012 | gnorm 0.683 | loss_scale 16 | train_wall 1734 | gb_free 10.1 | wall 15890
2022-03-04 01:04:27 | INFO | fairseq.trainer | begin training epoch 10
2022-03-04 01:04:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:09:49 | INFO | train_inner | epoch 010:     72 / 393 loss=6.879, ppl=117.71, wps=14326.2, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=3600, lr=0.00045001, gnorm=0.68, loss_scale=16, train_wall=440, gb_free=10.1, wall=16212
2022-03-04 01:17:15 | INFO | train_inner | epoch 010:    172 / 393 loss=6.842, ppl=114.7, wps=14670.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=3700, lr=0.000462508, gnorm=0.674, loss_scale=16, train_wall=442, gb_free=10.1, wall=16659
2022-03-04 01:18:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 01:24:46 | INFO | train_inner | epoch 010:    273 / 393 loss=6.839, ppl=114.47, wps=14531.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.657, loss_scale=16, train_wall=446, gb_free=10.1, wall=17110
2022-03-04 01:32:13 | INFO | train_inner | epoch 010:    373 / 393 loss=6.842, ppl=114.72, wps=14671.4, ups=0.22, wpb=65536, bsz=128, num_updates=3900, lr=0.000487503, gnorm=0.661, loss_scale=16, train_wall=442, gb_free=10.1, wall=17556
2022-03-04 01:33:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 01:33:47 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.134 | ppl 140.47 | wps 34015.2 | wpb 2034.1 | bsz 4 | num_updates 3920 | best_loss 7.134
2022-03-04 01:33:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 3920 updates
2022-03-04 01:33:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 01:33:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 01:33:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 10 @ 3920 updates, score 7.134) (writing took 4.270411070436239 seconds)
2022-03-04 01:33:51 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-04 01:33:51 | INFO | train | epoch 010 | loss 6.841 | ppl 114.66 | wps 14546.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 3920 | lr 0.000490002 | gnorm 0.668 | loss_scale 16 | train_wall 1734 | gb_free 10.1 | wall 17655
2022-03-04 01:33:51 | INFO | fairseq.trainer | begin training epoch 11
2022-03-04 01:33:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 01:39:48 | INFO | train_inner | epoch 011:     80 / 393 loss=6.712, ppl=104.84, wps=14329.3, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=4000, lr=0.0005, gnorm=0.653, loss_scale=16, train_wall=440, gb_free=10.1, wall=18012
2022-03-04 01:47:15 | INFO | train_inner | epoch 011:    180 / 393 loss=6.68, ppl=102.55, wps=14674.9, ups=0.22, wpb=65536, bsz=128, num_updates=4100, lr=0.000493865, gnorm=0.656, loss_scale=16, train_wall=442, gb_free=10.1, wall=18458
2022-03-04 01:54:41 | INFO | train_inner | epoch 011:    280 / 393 loss=6.683, ppl=102.74, wps=14673.7, ups=0.22, wpb=65536, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.624, loss_scale=16, train_wall=442, gb_free=10.1, wall=18905
2022-03-04 01:57:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 02:02:13 | INFO | train_inner | epoch 011:    381 / 393 loss=6.676, ppl=102.24, wps=14527.6, ups=0.22, wpb=65536, bsz=128, num_updates=4300, lr=0.000482243, gnorm=0.628, loss_scale=16, train_wall=446, gb_free=10.1, wall=19356
2022-03-04 02:03:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:03:11 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.049 | ppl 132.42 | wps 34083.9 | wpb 2034.1 | bsz 4 | num_updates 4312 | best_loss 7.049
2022-03-04 02:03:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 4312 updates
2022-03-04 02:03:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 02:03:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 02:03:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 11 @ 4312 updates, score 7.049) (writing took 4.26704516261816 seconds)
2022-03-04 02:03:15 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-04 02:03:15 | INFO | train | epoch 011 | loss 6.678 | ppl 102.42 | wps 14548.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 4312 | lr 0.000481571 | gnorm 0.635 | loss_scale 16 | train_wall 1734 | gb_free 10.1 | wall 19418
2022-03-04 02:03:15 | INFO | fairseq.trainer | begin training epoch 12
2022-03-04 02:03:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:09:48 | INFO | train_inner | epoch 012:     88 / 393 loss=6.526, ppl=92.17, wps=14335.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=4400, lr=0.000476731, gnorm=0.602, loss_scale=16, train_wall=440, gb_free=10.1, wall=19811
2022-03-04 02:17:14 | INFO | train_inner | epoch 012:    188 / 393 loss=6.52, ppl=91.76, wps=14674.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=4500, lr=0.000471405, gnorm=0.611, loss_scale=16, train_wall=442, gb_free=10.1, wall=20258
2022-03-04 02:24:41 | INFO | train_inner | epoch 012:    288 / 393 loss=6.536, ppl=92.77, wps=14670.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.605, loss_scale=16, train_wall=442, gb_free=10.1, wall=20704
2022-03-04 02:32:08 | INFO | train_inner | epoch 012:    388 / 393 loss=6.537, ppl=92.89, wps=14669.5, ups=0.22, wpb=65536, bsz=128, num_updates=4700, lr=0.000461266, gnorm=0.599, loss_scale=16, train_wall=442, gb_free=10.1, wall=21151
2022-03-04 02:32:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 02:32:35 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.012 | ppl 129.06 | wps 34055.4 | wpb 2034.1 | bsz 4 | num_updates 4705 | best_loss 7.012
2022-03-04 02:32:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 4705 updates
2022-03-04 02:32:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 02:32:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 02:32:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 12 @ 4705 updates, score 7.012) (writing took 4.302465082146227 seconds)
2022-03-04 02:32:39 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-04 02:32:39 | INFO | train | epoch 012 | loss 6.525 | ppl 92.1 | wps 14584.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 4705 | lr 0.00046102 | gnorm 0.605 | loss_scale 16 | train_wall 1734 | gb_free 10.1 | wall 21182
2022-03-04 02:32:39 | INFO | fairseq.trainer | begin training epoch 13
2022-03-04 02:32:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 02:36:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 02:39:47 | INFO | train_inner | epoch 013:     96 / 393 loss=6.365, ppl=82.44, wps=14194.4, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=4800, lr=0.000456435, gnorm=0.585, loss_scale=16, train_wall=444, gb_free=10.1, wall=21611
2022-03-04 02:43:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 02:47:18 | INFO | train_inner | epoch 013:    197 / 393 loss=6.393, ppl=84.06, wps=14537.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=4900, lr=0.000451754, gnorm=0.581, loss_scale=8, train_wall=446, gb_free=10.1, wall=22062
2022-03-04 02:54:44 | INFO | train_inner | epoch 013:    297 / 393 loss=6.402, ppl=84.57, wps=14685.4, ups=0.22, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.599, loss_scale=8, train_wall=441, gb_free=10.1, wall=22508
2022-03-04 03:01:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:01:57 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.963 | ppl 124.77 | wps 34020.4 | wpb 2034.1 | bsz 4 | num_updates 5096 | best_loss 6.963
2022-03-04 03:01:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 5096 updates
2022-03-04 03:01:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 03:02:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 03:02:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 13 @ 5096 updates, score 6.963) (writing took 4.210314610041678 seconds)
2022-03-04 03:02:02 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-04 03:02:02 | INFO | train | epoch 013 | loss 6.396 | ppl 84.19 | wps 14519.7 | ups 0.22 | wpb 65461.2 | bsz 127.9 | num_updates 5096 | lr 0.000442981 | gnorm 0.589 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 22945
2022-03-04 03:02:02 | INFO | fairseq.trainer | begin training epoch 14
2022-03-04 03:02:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:02:20 | INFO | train_inner | epoch 014:      4 / 393 loss=6.422, ppl=85.73, wps=14338, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5100, lr=0.000442807, gnorm=0.588, loss_scale=8, train_wall=440, gb_free=10.1, wall=22963
2022-03-04 03:09:46 | INFO | train_inner | epoch 014:    104 / 393 loss=6.254, ppl=76.31, wps=14688.7, ups=0.22, wpb=65536, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.566, loss_scale=8, train_wall=441, gb_free=10.1, wall=23409
2022-03-04 03:17:12 | INFO | train_inner | epoch 014:    204 / 393 loss=6.277, ppl=77.55, wps=14678.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=5300, lr=0.000434372, gnorm=0.586, loss_scale=8, train_wall=442, gb_free=10.1, wall=23856
2022-03-04 03:24:38 | INFO | train_inner | epoch 014:    304 / 393 loss=6.307, ppl=79.16, wps=14685.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.579, loss_scale=16, train_wall=441, gb_free=10.1, wall=24302
2022-03-04 03:31:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 03:31:20 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.929 | ppl 121.85 | wps 34010.7 | wpb 2034.1 | bsz 4 | num_updates 5489 | best_loss 6.929
2022-03-04 03:31:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 5489 updates
2022-03-04 03:31:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 03:31:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 03:31:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 14 @ 5489 updates, score 6.929) (writing took 4.069636786356568 seconds)
2022-03-04 03:31:24 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-04 03:31:24 | INFO | train | epoch 014 | loss 6.285 | ppl 77.98 | wps 14595.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 5489 | lr 0.000426828 | gnorm 0.576 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 24708
2022-03-04 03:31:24 | INFO | fairseq.trainer | begin training epoch 15
2022-03-04 03:31:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 03:32:13 | INFO | train_inner | epoch 015:     11 / 393 loss=6.288, ppl=78.14, wps=14340.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5500, lr=0.000426401, gnorm=0.564, loss_scale=16, train_wall=440, gb_free=10.1, wall=24757
2022-03-04 03:39:40 | INFO | train_inner | epoch 015:    111 / 393 loss=6.151, ppl=71.08, wps=14684.2, ups=0.22, wpb=65530.2, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.581, loss_scale=16, train_wall=441, gb_free=10.1, wall=25203
2022-03-04 03:47:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 03:47:10 | INFO | train_inner | epoch 015:    212 / 393 loss=6.182, ppl=72.59, wps=14542.8, ups=0.22, wpb=65536, bsz=128, num_updates=5700, lr=0.000418854, gnorm=0.572, loss_scale=8, train_wall=446, gb_free=10.1, wall=25654
2022-03-04 03:54:37 | INFO | train_inner | epoch 015:    312 / 393 loss=6.203, ppl=73.69, wps=14684.8, ups=0.22, wpb=65536, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.58, loss_scale=8, train_wall=441, gb_free=10.1, wall=26100
2022-03-04 04:00:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:00:43 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 6.924 | ppl 121.47 | wps 34095.2 | wpb 2034.1 | bsz 4 | num_updates 5881 | best_loss 6.924
2022-03-04 04:00:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 5881 updates
2022-03-04 04:00:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 04:00:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 04:00:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 15 @ 5881 updates, score 6.924) (writing took 4.190457979217172 seconds)
2022-03-04 04:00:47 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-04 04:00:47 | INFO | train | epoch 015 | loss 6.189 | ppl 72.94 | wps 14559.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 5881 | lr 0.000412358 | gnorm 0.578 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 26470
2022-03-04 04:00:47 | INFO | fairseq.trainer | begin training epoch 16
2022-03-04 04:00:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:02:12 | INFO | train_inner | epoch 016:     19 / 393 loss=6.197, ppl=73.37, wps=14340, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=5900, lr=0.000411693, gnorm=0.588, loss_scale=8, train_wall=439, gb_free=10.1, wall=26555
2022-03-04 04:09:38 | INFO | train_inner | epoch 016:    119 / 393 loss=6.06, ppl=66.72, wps=14686.2, ups=0.22, wpb=65530.2, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.589, loss_scale=8, train_wall=441, gb_free=10.1, wall=27001
2022-03-04 04:17:04 | INFO | train_inner | epoch 016:    219 / 393 loss=6.1, ppl=68.59, wps=14678.8, ups=0.22, wpb=65536, bsz=128, num_updates=6100, lr=0.000404888, gnorm=0.571, loss_scale=8, train_wall=442, gb_free=10.1, wall=27448
2022-03-04 04:24:31 | INFO | train_inner | epoch 016:    319 / 393 loss=6.13, ppl=70.05, wps=14688.2, ups=0.22, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.561, loss_scale=8, train_wall=441, gb_free=10.1, wall=27894
2022-03-04 04:29:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:30:05 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 6.919 | ppl 121 | wps 34076.8 | wpb 2034.1 | bsz 4 | num_updates 6274 | best_loss 6.919
2022-03-04 04:30:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 6274 updates
2022-03-04 04:30:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 04:30:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt
2022-03-04 04:30:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_fp16_size_0.25-jelinek_0.0_0.1_0.9_#3/checkpoint_best.pt (epoch 16 @ 6274 updates, score 6.919) (writing took 4.1839598855003715 seconds)
2022-03-04 04:30:10 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-04 04:30:10 | INFO | train | epoch 016 | loss 6.104 | ppl 68.77 | wps 14594.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 6274 | lr 0.000399234 | gnorm 0.574 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 28233
2022-03-04 04:30:10 | INFO | fairseq.trainer | begin training epoch 17
2022-03-04 04:30:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 04:32:06 | INFO | train_inner | epoch 017:     26 / 393 loss=6.098, ppl=68.51, wps=14334.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=6300, lr=0.00039841, gnorm=0.584, loss_scale=16, train_wall=440, gb_free=10.1, wall=28349
2022-03-04 04:39:32 | INFO | train_inner | epoch 017:    126 / 393 loss=5.986, ppl=63.37, wps=14678.1, ups=0.22, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.586, loss_scale=16, train_wall=442, gb_free=10.1, wall=28796
2022-03-04 04:46:59 | INFO | train_inner | epoch 017:    226 / 393 loss=6.022, ppl=64.99, wps=14679.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=6500, lr=0.000392232, gnorm=0.578, loss_scale=16, train_wall=442, gb_free=10.1, wall=29242
2022-03-04 04:54:25 | INFO | train_inner | epoch 017:    326 / 393 loss=6.058, ppl=66.61, wps=14677.7, ups=0.22, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.564, loss_scale=16, train_wall=442, gb_free=10.1, wall=29689
2022-03-04 04:59:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 04:59:29 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 6.931 | ppl 121.99 | wps 34099.3 | wpb 2034.1 | bsz 4 | num_updates 6667 | best_loss 6.919
2022-03-04 04:59:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 6667 updates
2022-03-04 04:59:29 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-04 04:59:29 | INFO | train | epoch 017 | loss 6.028 | ppl 65.25 | wps 14625 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 6667 | lr 0.000387289 | gnorm 0.581 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 29992
2022-03-04 04:59:29 | INFO | fairseq.trainer | begin training epoch 18
2022-03-04 04:59:29 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:01:56 | INFO | train_inner | epoch 018:     33 / 393 loss=6.025, ppl=65.12, wps=14470.8, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=6700, lr=0.000386334, gnorm=0.592, loss_scale=16, train_wall=440, gb_free=10.1, wall=30139
2022-03-04 05:03:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-04 05:09:27 | INFO | train_inner | epoch 018:    134 / 393 loss=5.915, ppl=60.36, wps=14535.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.584, loss_scale=16, train_wall=446, gb_free=10.1, wall=30590
2022-03-04 05:16:53 | INFO | train_inner | epoch 018:    234 / 393 loss=5.956, ppl=62.08, wps=14683.5, ups=0.22, wpb=65536, bsz=128, num_updates=6900, lr=0.000380693, gnorm=0.581, loss_scale=16, train_wall=441, gb_free=10.1, wall=31037
2022-03-04 05:24:20 | INFO | train_inner | epoch 018:    334 / 393 loss=5.994, ppl=63.74, wps=14683, ups=0.22, wpb=65530.9, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.58, loss_scale=16, train_wall=441, gb_free=10.1, wall=31483
2022-03-04 05:28:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:28:47 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.931 | ppl 122.04 | wps 34070.8 | wpb 2034.1 | bsz 4 | num_updates 7059 | best_loss 6.919
2022-03-04 05:28:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 7059 updates
2022-03-04 05:28:47 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-04 05:28:47 | INFO | train | epoch 018 | loss 5.959 | ppl 62.2 | wps 14590.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7059 | lr 0.000376382 | gnorm 0.58 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 31751
2022-03-04 05:28:47 | INFO | fairseq.trainer | begin training epoch 19
2022-03-04 05:28:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 05:29:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 05:31:55 | INFO | train_inner | epoch 019:     42 / 393 loss=5.929, ppl=60.95, wps=14331.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=7100, lr=0.000375293, gnorm=0.586, loss_scale=8, train_wall=444, gb_free=10.1, wall=31938
2022-03-04 05:39:21 | INFO | train_inner | epoch 019:    142 / 393 loss=5.864, ppl=58.25, wps=14680.5, ups=0.22, wpb=65536, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.563, loss_scale=8, train_wall=441, gb_free=10.1, wall=32385
2022-03-04 05:46:48 | INFO | train_inner | epoch 019:    242 / 393 loss=5.895, ppl=59.52, wps=14684.1, ups=0.22, wpb=65536, bsz=128, num_updates=7300, lr=0.000370117, gnorm=0.595, loss_scale=8, train_wall=441, gb_free=10.1, wall=32831
2022-03-04 05:54:14 | INFO | train_inner | epoch 019:    342 / 393 loss=5.93, ppl=60.96, wps=14681.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=7400, lr=0.000367607, gnorm=0.604, loss_scale=8, train_wall=441, gb_free=10.1, wall=33277
2022-03-04 05:58:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 05:58:06 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.952 | ppl 123.82 | wps 34053 | wpb 2034.1 | bsz 4 | num_updates 7451 | best_loss 6.919
2022-03-04 05:58:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 7451 updates
2022-03-04 05:58:06 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-04 05:58:06 | INFO | train | epoch 019 | loss 5.897 | ppl 59.57 | wps 14591.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7451 | lr 0.000366347 | gnorm 0.593 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 33509
2022-03-04 05:58:06 | INFO | fairseq.trainer | begin training epoch 20
2022-03-04 05:58:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:01:45 | INFO | train_inner | epoch 020:     49 / 393 loss=5.867, ppl=58.36, wps=14474.6, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=7500, lr=0.000365148, gnorm=0.576, loss_scale=8, train_wall=439, gb_free=10.1, wall=33728
2022-03-04 06:09:11 | INFO | train_inner | epoch 020:    149 / 393 loss=5.803, ppl=55.83, wps=14684.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=7600, lr=0.000362738, gnorm=0.577, loss_scale=16, train_wall=441, gb_free=10.1, wall=34174
2022-03-04 06:12:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 06:16:42 | INFO | train_inner | epoch 020:    250 / 393 loss=5.842, ppl=57.34, wps=14541.2, ups=0.22, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.583, loss_scale=8, train_wall=446, gb_free=10.1, wall=34625
2022-03-04 06:24:08 | INFO | train_inner | epoch 020:    350 / 393 loss=5.88, ppl=58.88, wps=14683.4, ups=0.22, wpb=65536, bsz=128, num_updates=7800, lr=0.000358057, gnorm=0.598, loss_scale=8, train_wall=441, gb_free=10.1, wall=35071
2022-03-04 06:27:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:27:24 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.943 | ppl 123.03 | wps 33997.3 | wpb 2034.1 | bsz 4 | num_updates 7843 | best_loss 6.919
2022-03-04 06:27:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 7843 updates
2022-03-04 06:27:24 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-04 06:27:24 | INFO | train | epoch 020 | loss 5.838 | ppl 57.2 | wps 14592.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 7843 | lr 0.000357075 | gnorm 0.576 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 35268
2022-03-04 06:27:24 | INFO | fairseq.trainer | begin training epoch 21
2022-03-04 06:27:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 06:31:39 | INFO | train_inner | epoch 021:     57 / 393 loss=5.792, ppl=55.43, wps=14469.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=7900, lr=0.000355784, gnorm=0.585, loss_scale=8, train_wall=440, gb_free=10.1, wall=35522
2022-03-04 06:39:05 | INFO | train_inner | epoch 021:    157 / 393 loss=5.759, ppl=54.15, wps=14687.4, ups=0.22, wpb=65536, bsz=128, num_updates=8000, lr=0.000353553, gnorm=0.591, loss_scale=8, train_wall=441, gb_free=10.1, wall=35969
2022-03-04 06:46:31 | INFO | train_inner | epoch 021:    257 / 393 loss=5.795, ppl=55.54, wps=14686.7, ups=0.22, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.608, loss_scale=8, train_wall=441, gb_free=10.1, wall=36415
2022-03-04 06:53:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 06:54:02 | INFO | train_inner | epoch 021:    358 / 393 loss=5.826, ppl=56.73, wps=14543.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=8200, lr=0.000349215, gnorm=0.577, loss_scale=8, train_wall=446, gb_free=10.1, wall=36865
2022-03-04 06:56:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 06:56:43 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.944 | ppl 123.11 | wps 34081 | wpb 2034.1 | bsz 4 | num_updates 8235 | best_loss 6.919
2022-03-04 06:56:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 8235 updates
2022-03-04 06:56:43 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-04 06:56:43 | INFO | train | epoch 021 | loss 5.786 | ppl 55.16 | wps 14595.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 8235 | lr 0.000348472 | gnorm 0.599 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 37026
2022-03-04 06:56:43 | INFO | fairseq.trainer | begin training epoch 22
2022-03-04 06:56:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:01:33 | INFO | train_inner | epoch 022:     65 / 393 loss=5.728, ppl=52.99, wps=14471.9, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=8300, lr=0.000347105, gnorm=0.611, loss_scale=8, train_wall=439, gb_free=10.1, wall=37316
2022-03-04 07:08:59 | INFO | train_inner | epoch 022:    165 / 393 loss=5.713, ppl=52.46, wps=14683.8, ups=0.22, wpb=65536, bsz=128, num_updates=8400, lr=0.000345033, gnorm=0.599, loss_scale=8, train_wall=441, gb_free=10.1, wall=37762
2022-03-04 07:16:25 | INFO | train_inner | epoch 022:    265 / 393 loss=5.74, ppl=53.46, wps=14685.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.608, loss_scale=8, train_wall=441, gb_free=10.1, wall=38209
2022-03-04 07:23:52 | INFO | train_inner | epoch 022:    365 / 393 loss=5.782, ppl=55.02, wps=14682, ups=0.22, wpb=65536, bsz=128, num_updates=8600, lr=0.000340997, gnorm=0.607, loss_scale=8, train_wall=441, gb_free=10.1, wall=38655
2022-03-04 07:25:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:26:01 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.983 | ppl 126.52 | wps 34120.1 | wpb 2034.1 | bsz 4 | num_updates 8628 | best_loss 6.919
2022-03-04 07:26:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 8628 updates
2022-03-04 07:26:01 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-04 07:26:01 | INFO | train | epoch 022 | loss 5.736 | ppl 53.3 | wps 14629.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 8628 | lr 0.000340443 | gnorm 0.603 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 38785
2022-03-04 07:26:01 | INFO | fairseq.trainer | begin training epoch 23
2022-03-04 07:26:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 07:31:23 | INFO | train_inner | epoch 023:     72 / 393 loss=5.671, ppl=50.94, wps=14471.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=8700, lr=0.000339032, gnorm=0.583, loss_scale=8, train_wall=440, gb_free=10.1, wall=39106
2022-03-04 07:38:49 | INFO | train_inner | epoch 023:    172 / 393 loss=5.666, ppl=50.76, wps=14686.2, ups=0.22, wpb=65536, bsz=128, num_updates=8800, lr=0.0003371, gnorm=0.615, loss_scale=16, train_wall=441, gb_free=10.1, wall=39552
2022-03-04 07:45:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 07:46:20 | INFO | train_inner | epoch 023:    273 / 393 loss=5.705, ppl=52.15, wps=14540, ups=0.22, wpb=65535.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.595, loss_scale=8, train_wall=446, gb_free=10.1, wall=40003
2022-03-04 07:53:46 | INFO | train_inner | epoch 023:    373 / 393 loss=5.73, ppl=53.06, wps=14679.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=9000, lr=0.000333333, gnorm=0.607, loss_scale=8, train_wall=442, gb_free=10.1, wall=40449
2022-03-04 07:55:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 07:55:20 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.01 | ppl 128.88 | wps 34086.3 | wpb 2034.1 | bsz 4 | num_updates 9020 | best_loss 6.919
2022-03-04 07:55:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 9020 updates
2022-03-04 07:55:20 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-04 07:55:20 | INFO | train | epoch 023 | loss 5.689 | ppl 51.58 | wps 14591.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 9020 | lr 0.000332964 | gnorm 0.601 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 40543
2022-03-04 07:55:20 | INFO | fairseq.trainer | begin training epoch 24
2022-03-04 07:55:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:01:17 | INFO | train_inner | epoch 024:     80 / 393 loss=5.612, ppl=48.89, wps=14474.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9100, lr=0.000331497, gnorm=0.602, loss_scale=8, train_wall=439, gb_free=10.1, wall=40900
2022-03-04 08:08:43 | INFO | train_inner | epoch 024:    180 / 393 loss=5.627, ppl=49.42, wps=14688.9, ups=0.22, wpb=65536, bsz=128, num_updates=9200, lr=0.00032969, gnorm=0.621, loss_scale=8, train_wall=441, gb_free=10.1, wall=41346
2022-03-04 08:16:09 | INFO | train_inner | epoch 024:    280 / 393 loss=5.664, ppl=50.7, wps=14688.2, ups=0.22, wpb=65536, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.602, loss_scale=8, train_wall=441, gb_free=10.1, wall=41792
2022-03-04 08:23:35 | INFO | train_inner | epoch 024:    380 / 393 loss=5.691, ppl=51.68, wps=14687.7, ups=0.22, wpb=65530.2, bsz=128, num_updates=9400, lr=0.000326164, gnorm=0.602, loss_scale=16, train_wall=441, gb_free=10.1, wall=42239
2022-03-04 08:24:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:24:38 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.008 | ppl 128.69 | wps 34008.4 | wpb 2034.1 | bsz 4 | num_updates 9413 | best_loss 6.919
2022-03-04 08:24:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 9413 updates
2022-03-04 08:24:38 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-04 08:24:38 | INFO | train | epoch 024 | loss 5.646 | ppl 50.07 | wps 14633.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 9413 | lr 0.000325939 | gnorm 0.606 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 42301
2022-03-04 08:24:38 | INFO | fairseq.trainer | begin training epoch 25
2022-03-04 08:24:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 08:31:06 | INFO | train_inner | epoch 025:     87 / 393 loss=5.564, ppl=47.3, wps=14474.3, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=9500, lr=0.000324443, gnorm=0.606, loss_scale=16, train_wall=439, gb_free=10.1, wall=42689
2022-03-04 08:36:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 08:38:37 | INFO | train_inner | epoch 025:    188 / 393 loss=5.582, ppl=47.89, wps=14537.5, ups=0.22, wpb=65536, bsz=128, num_updates=9600, lr=0.000322749, gnorm=0.593, loss_scale=8, train_wall=446, gb_free=10.1, wall=43140
2022-03-04 08:46:03 | INFO | train_inner | epoch 025:    288 / 393 loss=5.619, ppl=49.13, wps=14689.2, ups=0.22, wpb=65536, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.6, loss_scale=8, train_wall=441, gb_free=10.1, wall=43586
2022-03-04 08:53:29 | INFO | train_inner | epoch 025:    388 / 393 loss=5.664, ppl=50.7, wps=14687.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=9800, lr=0.000319438, gnorm=0.619, loss_scale=8, train_wall=441, gb_free=10.1, wall=44033
2022-03-04 08:53:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 08:53:56 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.025 | ppl 130.27 | wps 34140.6 | wpb 2034.1 | bsz 4 | num_updates 9805 | best_loss 6.919
2022-03-04 08:53:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 9805 updates
2022-03-04 08:53:56 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-04 08:53:56 | INFO | train | epoch 025 | loss 5.605 | ppl 48.67 | wps 14594.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 9805 | lr 0.000319357 | gnorm 0.603 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 44059
2022-03-04 08:53:56 | INFO | fairseq.trainer | begin training epoch 26
2022-03-04 08:53:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:01:00 | INFO | train_inner | epoch 026:     95 / 393 loss=5.513, ppl=45.65, wps=14478.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=9900, lr=0.000317821, gnorm=0.602, loss_scale=8, train_wall=439, gb_free=10.1, wall=44483
2022-03-04 09:08:26 | INFO | train_inner | epoch 026:    195 / 393 loss=5.552, ppl=46.92, wps=14681.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=10000, lr=0.000316228, gnorm=0.62, loss_scale=8, train_wall=441, gb_free=10.1, wall=44930
2022-03-04 09:15:52 | INFO | train_inner | epoch 026:    295 / 393 loss=5.584, ppl=47.97, wps=14684.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.642, loss_scale=16, train_wall=441, gb_free=10.1, wall=45376
2022-03-04 09:21:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 09:23:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:23:14 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.071 | ppl 134.44 | wps 34047 | wpb 2034.1 | bsz 4 | num_updates 10197 | best_loss 6.919
2022-03-04 09:23:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 10197 updates
2022-03-04 09:23:14 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-04 09:23:14 | INFO | train | epoch 026 | loss 5.566 | ppl 47.36 | wps 14592.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10197 | lr 0.000313158 | gnorm 0.614 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 45818
2022-03-04 09:23:14 | INFO | fairseq.trainer | begin training epoch 27
2022-03-04 09:23:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:23:28 | INFO | train_inner | epoch 027:      3 / 393 loss=5.615, ppl=49.01, wps=14326.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10200, lr=0.000313112, gnorm=0.591, loss_scale=8, train_wall=444, gb_free=10.1, wall=45831
2022-03-04 09:30:54 | INFO | train_inner | epoch 027:    103 / 393 loss=5.471, ppl=44.35, wps=14688.9, ups=0.22, wpb=65536, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.613, loss_scale=8, train_wall=441, gb_free=10.1, wall=46277
2022-03-04 09:38:20 | INFO | train_inner | epoch 027:    203 / 393 loss=5.515, ppl=45.72, wps=14684.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=10400, lr=0.000310087, gnorm=0.618, loss_scale=8, train_wall=441, gb_free=10.1, wall=46724
2022-03-04 09:45:46 | INFO | train_inner | epoch 027:    303 / 393 loss=5.556, ppl=47.03, wps=14689.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.624, loss_scale=8, train_wall=441, gb_free=10.1, wall=47170
2022-03-04 09:52:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 09:52:32 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.085 | ppl 135.78 | wps 34087.4 | wpb 2034.1 | bsz 4 | num_updates 10590 | best_loss 6.919
2022-03-04 09:52:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 10590 updates
2022-03-04 09:52:32 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-04 09:52:32 | INFO | train | epoch 027 | loss 5.531 | ppl 46.22 | wps 14633.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 10590 | lr 0.000307293 | gnorm 0.625 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 47576
2022-03-04 09:52:33 | INFO | fairseq.trainer | begin training epoch 28
2022-03-04 09:52:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 09:53:17 | INFO | train_inner | epoch 028:     10 / 393 loss=5.573, ppl=47.6, wps=14476.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=10600, lr=0.000307148, gnorm=0.651, loss_scale=8, train_wall=439, gb_free=10.1, wall=47621
2022-03-04 10:00:44 | INFO | train_inner | epoch 028:    110 / 393 loss=5.437, ppl=43.32, wps=14684.9, ups=0.22, wpb=65536, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.605, loss_scale=16, train_wall=441, gb_free=10.1, wall=48067
2022-03-04 10:06:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 10:08:14 | INFO | train_inner | epoch 028:    211 / 393 loss=5.485, ppl=44.77, wps=14544.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=10800, lr=0.00030429, gnorm=0.626, loss_scale=8, train_wall=446, gb_free=10.1, wall=48517
2022-03-04 10:15:40 | INFO | train_inner | epoch 028:    311 / 393 loss=5.524, ppl=46.01, wps=14684.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.597, loss_scale=8, train_wall=441, gb_free=10.1, wall=48964
2022-03-04 10:21:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:21:51 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.119 | ppl 139.05 | wps 34054.6 | wpb 2034.1 | bsz 4 | num_updates 10982 | best_loss 6.919
2022-03-04 10:21:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 10982 updates
2022-03-04 10:21:51 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-04 10:21:51 | INFO | train | epoch 028 | loss 5.495 | ppl 45.09 | wps 14595.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 10982 | lr 0.000301758 | gnorm 0.617 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 49334
2022-03-04 10:21:51 | INFO | fairseq.trainer | begin training epoch 29
2022-03-04 10:21:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:23:11 | INFO | train_inner | epoch 029:     18 / 393 loss=5.518, ppl=45.83, wps=14476.2, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11000, lr=0.000301511, gnorm=0.633, loss_scale=8, train_wall=439, gb_free=10.1, wall=49415
2022-03-04 10:30:37 | INFO | train_inner | epoch 029:    118 / 393 loss=5.415, ppl=42.66, wps=14684.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=11100, lr=0.00030015, gnorm=0.607, loss_scale=8, train_wall=441, gb_free=10.1, wall=49861
2022-03-04 10:38:04 | INFO | train_inner | epoch 029:    218 / 393 loss=5.455, ppl=43.86, wps=14684.6, ups=0.22, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.652, loss_scale=8, train_wall=441, gb_free=10.1, wall=50307
2022-03-04 10:45:30 | INFO | train_inner | epoch 029:    318 / 393 loss=5.495, ppl=45.08, wps=14683.3, ups=0.22, wpb=65536, bsz=128, num_updates=11300, lr=0.000297482, gnorm=0.621, loss_scale=16, train_wall=441, gb_free=10.1, wall=50753
2022-03-04 10:46:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 10:51:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 10:51:09 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.134 | ppl 140.41 | wps 34090.6 | wpb 2034.1 | bsz 4 | num_updates 11374 | best_loss 6.919
2022-03-04 10:51:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 11374 updates
2022-03-04 10:51:09 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-04 10:51:09 | INFO | train | epoch 029 | loss 5.462 | ppl 44.09 | wps 14593.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 11374 | lr 0.000296513 | gnorm 0.632 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 51092
2022-03-04 10:51:09 | INFO | fairseq.trainer | begin training epoch 30
2022-03-04 10:51:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 10:53:05 | INFO | train_inner | epoch 030:     26 / 393 loss=5.473, ppl=44.4, wps=14332.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11400, lr=0.000296174, gnorm=0.655, loss_scale=8, train_wall=444, gb_free=10.1, wall=51209
2022-03-04 11:00:32 | INFO | train_inner | epoch 030:    126 / 393 loss=5.384, ppl=41.76, wps=14683.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=11500, lr=0.000294884, gnorm=0.621, loss_scale=8, train_wall=441, gb_free=10.1, wall=51655
2022-03-04 11:07:58 | INFO | train_inner | epoch 030:    226 / 393 loss=5.428, ppl=43.06, wps=14682.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.638, loss_scale=8, train_wall=441, gb_free=10.1, wall=52101
2022-03-04 11:15:24 | INFO | train_inner | epoch 030:    326 / 393 loss=5.461, ppl=44.03, wps=14686.1, ups=0.22, wpb=65536, bsz=128, num_updates=11700, lr=0.000292353, gnorm=0.622, loss_scale=8, train_wall=441, gb_free=10.1, wall=52548
2022-03-04 11:20:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:20:28 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.133 | ppl 140.32 | wps 34090.8 | wpb 2034.1 | bsz 4 | num_updates 11767 | best_loss 6.919
2022-03-04 11:20:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 11767 updates
2022-03-04 11:20:28 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-04 11:20:28 | INFO | train | epoch 030 | loss 5.432 | ppl 43.16 | wps 14629.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 11767 | lr 0.000291519 | gnorm 0.639 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 52851
2022-03-04 11:20:28 | INFO | fairseq.trainer | begin training epoch 31
2022-03-04 11:20:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:22:55 | INFO | train_inner | epoch 031:     33 / 393 loss=5.441, ppl=43.44, wps=14473.7, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=11800, lr=0.000291111, gnorm=0.665, loss_scale=8, train_wall=439, gb_free=10.1, wall=52998
2022-03-04 11:28:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 11:30:26 | INFO | train_inner | epoch 031:    134 / 393 loss=5.354, ppl=40.9, wps=14535, ups=0.22, wpb=65535.4, bsz=128, num_updates=11900, lr=0.000289886, gnorm=0.621, loss_scale=8, train_wall=446, gb_free=10.1, wall=53449
2022-03-04 11:37:52 | INFO | train_inner | epoch 031:    234 / 393 loss=5.398, ppl=42.16, wps=14687.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.63, loss_scale=8, train_wall=441, gb_free=10.1, wall=53895
2022-03-04 11:45:18 | INFO | train_inner | epoch 031:    334 / 393 loss=5.438, ppl=43.35, wps=14685.7, ups=0.22, wpb=65536, bsz=128, num_updates=12100, lr=0.00028748, gnorm=0.639, loss_scale=8, train_wall=441, gb_free=10.1, wall=54342
2022-03-04 11:49:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 11:49:46 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.133 | ppl 140.39 | wps 34064.4 | wpb 2034.1 | bsz 4 | num_updates 12159 | best_loss 6.919
2022-03-04 11:49:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 12159 updates
2022-03-04 11:49:46 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-04 11:49:46 | INFO | train | epoch 031 | loss 5.402 | ppl 42.27 | wps 14593.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12159 | lr 0.000286781 | gnorm 0.629 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 54609
2022-03-04 11:49:46 | INFO | fairseq.trainer | begin training epoch 32
2022-03-04 11:49:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 11:52:49 | INFO | train_inner | epoch 032:     41 / 393 loss=5.4, ppl=42.24, wps=14474.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12200, lr=0.000286299, gnorm=0.647, loss_scale=8, train_wall=439, gb_free=10.1, wall=54792
2022-03-04 12:00:15 | INFO | train_inner | epoch 032:    141 / 393 loss=5.327, ppl=40.15, wps=14681.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=12300, lr=0.000285133, gnorm=0.649, loss_scale=8, train_wall=441, gb_free=10.1, wall=55239
2022-03-04 12:07:42 | INFO | train_inner | epoch 032:    241 / 393 loss=5.377, ppl=41.56, wps=14685, ups=0.22, wpb=65536, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.631, loss_scale=16, train_wall=441, gb_free=10.1, wall=55685
2022-03-04 12:09:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 12:15:12 | INFO | train_inner | epoch 032:    342 / 393 loss=5.419, ppl=42.8, wps=14542.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=12500, lr=0.000282843, gnorm=0.632, loss_scale=8, train_wall=446, gb_free=10.1, wall=56136
2022-03-04 12:18:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:19:04 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.181 | ppl 145.12 | wps 34139.3 | wpb 2034.1 | bsz 4 | num_updates 12551 | best_loss 6.919
2022-03-04 12:19:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 12551 updates
2022-03-04 12:19:04 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-04 12:19:04 | INFO | train | epoch 032 | loss 5.373 | ppl 41.45 | wps 14593.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12551 | lr 0.000282267 | gnorm 0.646 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 56368
2022-03-04 12:19:04 | INFO | fairseq.trainer | begin training epoch 33
2022-03-04 12:19:04 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:22:43 | INFO | train_inner | epoch 033:     49 / 393 loss=5.349, ppl=40.75, wps=14470.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=12600, lr=0.000281718, gnorm=0.648, loss_scale=8, train_wall=440, gb_free=10.1, wall=56587
2022-03-04 12:30:09 | INFO | train_inner | epoch 033:    149 / 393 loss=5.31, ppl=39.68, wps=14683.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=12700, lr=0.000280607, gnorm=0.653, loss_scale=8, train_wall=441, gb_free=10.1, wall=57033
2022-03-04 12:37:36 | INFO | train_inner | epoch 033:    249 / 393 loss=5.354, ppl=40.89, wps=14679.4, ups=0.22, wpb=65536, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.677, loss_scale=8, train_wall=442, gb_free=10.1, wall=57479
2022-03-04 12:45:02 | INFO | train_inner | epoch 033:    349 / 393 loss=5.387, ppl=41.85, wps=14684.9, ups=0.22, wpb=65536, bsz=128, num_updates=12900, lr=0.000278423, gnorm=0.65, loss_scale=8, train_wall=441, gb_free=10.1, wall=57926
2022-03-04 12:47:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 12:48:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 12:48:23 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.221 | ppl 149.2 | wps 34075.7 | wpb 2034.1 | bsz 4 | num_updates 12943 | best_loss 6.919
2022-03-04 12:48:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 12943 updates
2022-03-04 12:48:23 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-04 12:48:23 | INFO | train | epoch 033 | loss 5.347 | ppl 40.69 | wps 14591.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 12943 | lr 0.00027796 | gnorm 0.658 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 58126
2022-03-04 12:48:23 | INFO | fairseq.trainer | begin training epoch 34
2022-03-04 12:48:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 12:52:37 | INFO | train_inner | epoch 034:     57 / 393 loss=5.313, ppl=39.76, wps=14333.2, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=13000, lr=0.00027735, gnorm=0.665, loss_scale=8, train_wall=444, gb_free=10.1, wall=58381
2022-03-04 13:00:04 | INFO | train_inner | epoch 034:    157 / 393 loss=5.289, ppl=39.1, wps=14684.1, ups=0.22, wpb=65530.9, bsz=128, num_updates=13100, lr=0.000276289, gnorm=0.683, loss_scale=8, train_wall=441, gb_free=10.1, wall=58827
2022-03-04 13:07:30 | INFO | train_inner | epoch 034:    257 / 393 loss=5.327, ppl=40.15, wps=14685.4, ups=0.22, wpb=65536, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.652, loss_scale=8, train_wall=441, gb_free=10.1, wall=59273
2022-03-04 13:14:56 | INFO | train_inner | epoch 034:    357 / 393 loss=5.367, ppl=41.27, wps=14685.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=13300, lr=0.000274204, gnorm=0.647, loss_scale=8, train_wall=441, gb_free=10.1, wall=59720
2022-03-04 13:17:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:17:41 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.222 | ppl 149.3 | wps 34022.3 | wpb 2034.1 | bsz 4 | num_updates 13336 | best_loss 6.919
2022-03-04 13:17:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 13336 updates
2022-03-04 13:17:41 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-04 13:17:41 | INFO | train | epoch 034 | loss 5.321 | ppl 39.97 | wps 14631.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 13336 | lr 0.000273834 | gnorm 0.657 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 59885
2022-03-04 13:17:41 | INFO | fairseq.trainer | begin training epoch 35
2022-03-04 13:17:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:22:27 | INFO | train_inner | epoch 035:     64 / 393 loss=5.284, ppl=38.96, wps=14473.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=13400, lr=0.000273179, gnorm=0.637, loss_scale=8, train_wall=439, gb_free=10.1, wall=60170
2022-03-04 13:29:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 13:29:58 | INFO | train_inner | epoch 035:    165 / 393 loss=5.265, ppl=38.46, wps=14540.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=13500, lr=0.000272166, gnorm=0.652, loss_scale=8, train_wall=446, gb_free=10.1, wall=60621
2022-03-04 13:37:24 | INFO | train_inner | epoch 035:    265 / 393 loss=5.309, ppl=39.64, wps=14686.9, ups=0.22, wpb=65535.4, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.648, loss_scale=8, train_wall=441, gb_free=10.1, wall=61067
2022-03-04 13:44:50 | INFO | train_inner | epoch 035:    365 / 393 loss=5.342, ppl=40.55, wps=14687.6, ups=0.22, wpb=65536, bsz=128, num_updates=13700, lr=0.000270172, gnorm=0.64, loss_scale=8, train_wall=441, gb_free=10.1, wall=61514
2022-03-04 13:46:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 13:46:59 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.219 | ppl 148.94 | wps 34130.6 | wpb 2034.1 | bsz 4 | num_updates 13728 | best_loss 6.919
2022-03-04 13:46:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 13728 updates
2022-03-04 13:46:59 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-04 13:46:59 | INFO | train | epoch 035 | loss 5.296 | ppl 39.29 | wps 14594.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 13728 | lr 0.000269896 | gnorm 0.646 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 61643
2022-03-04 13:47:00 | INFO | fairseq.trainer | begin training epoch 36
2022-03-04 13:47:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 13:52:21 | INFO | train_inner | epoch 036:     72 / 393 loss=5.251, ppl=38.09, wps=14471.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=13800, lr=0.000269191, gnorm=0.672, loss_scale=8, train_wall=440, gb_free=10.1, wall=61964
2022-03-04 13:59:47 | INFO | train_inner | epoch 036:    172 / 393 loss=5.24, ppl=37.79, wps=14687.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=13900, lr=0.000268221, gnorm=0.637, loss_scale=8, train_wall=441, gb_free=10.1, wall=62411
2022-03-04 14:07:13 | INFO | train_inner | epoch 036:    272 / 393 loss=5.289, ppl=39.09, wps=14688.9, ups=0.22, wpb=65536, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.654, loss_scale=8, train_wall=441, gb_free=10.1, wall=62857
2022-03-04 14:14:40 | INFO | train_inner | epoch 036:    372 / 393 loss=5.319, ppl=39.91, wps=14685.9, ups=0.22, wpb=65536, bsz=128, num_updates=14100, lr=0.000266312, gnorm=0.644, loss_scale=16, train_wall=441, gb_free=10.1, wall=63303
2022-03-04 14:15:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 14:16:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:16:18 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.247 | ppl 151.87 | wps 33910.6 | wpb 2034.1 | bsz 4 | num_updates 14120 | best_loss 6.919
2022-03-04 14:16:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 14120 updates
2022-03-04 14:16:18 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-04 14:16:18 | INFO | train | epoch 036 | loss 5.272 | ppl 38.64 | wps 14594.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14120 | lr 0.000266123 | gnorm 0.657 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 63401
2022-03-04 14:16:18 | INFO | fairseq.trainer | begin training epoch 37
2022-03-04 14:16:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:22:15 | INFO | train_inner | epoch 037:     80 / 393 loss=5.218, ppl=37.21, wps=14334, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=14200, lr=0.000265372, gnorm=0.665, loss_scale=8, train_wall=444, gb_free=10.1, wall=63758
2022-03-04 14:29:41 | INFO | train_inner | epoch 037:    180 / 393 loss=5.224, ppl=37.37, wps=14683.6, ups=0.22, wpb=65536, bsz=128, num_updates=14300, lr=0.000264443, gnorm=0.676, loss_scale=8, train_wall=441, gb_free=10.1, wall=64205
2022-03-04 14:37:08 | INFO | train_inner | epoch 037:    280 / 393 loss=5.265, ppl=38.46, wps=14678.4, ups=0.22, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.651, loss_scale=8, train_wall=442, gb_free=10.1, wall=64651
2022-03-04 14:44:34 | INFO | train_inner | epoch 037:    380 / 393 loss=5.299, ppl=39.36, wps=14676, ups=0.22, wpb=65535.4, bsz=128, num_updates=14500, lr=0.000262613, gnorm=0.668, loss_scale=8, train_wall=442, gb_free=10.1, wall=65098
2022-03-04 14:45:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 14:45:37 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.275 | ppl 154.87 | wps 33950.2 | wpb 2034.1 | bsz 4 | num_updates 14513 | best_loss 6.919
2022-03-04 14:45:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 14513 updates
2022-03-04 14:45:37 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-04 14:45:37 | INFO | train | epoch 037 | loss 5.25 | ppl 38.04 | wps 14625.9 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 14513 | lr 0.000262495 | gnorm 0.657 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 65160
2022-03-04 14:45:37 | INFO | fairseq.trainer | begin training epoch 38
2022-03-04 14:45:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 14:52:06 | INFO | train_inner | epoch 038:     87 / 393 loss=5.183, ppl=36.34, wps=14452.2, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=14600, lr=0.000261712, gnorm=0.658, loss_scale=8, train_wall=440, gb_free=10.1, wall=65549
2022-03-04 14:59:32 | INFO | train_inner | epoch 038:    187 / 393 loss=5.207, ppl=36.94, wps=14669.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=14700, lr=0.00026082, gnorm=0.65, loss_scale=16, train_wall=442, gb_free=10.1, wall=65996
2022-03-04 15:00:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 15:07:03 | INFO | train_inner | epoch 038:    288 / 393 loss=5.246, ppl=37.94, wps=14540, ups=0.22, wpb=65536, bsz=128, num_updates=14800, lr=0.000259938, gnorm=0.695, loss_scale=8, train_wall=446, gb_free=10.1, wall=66446
2022-03-04 15:14:29 | INFO | train_inner | epoch 038:    388 / 393 loss=5.284, ppl=38.97, wps=14682.4, ups=0.22, wpb=65536, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.664, loss_scale=8, train_wall=441, gb_free=10.1, wall=66893
2022-03-04 15:14:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:14:56 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.294 | ppl 156.88 | wps 34041.9 | wpb 2034.1 | bsz 4 | num_updates 14905 | best_loss 6.919
2022-03-04 15:14:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 14905 updates
2022-03-04 15:14:56 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-04 15:14:56 | INFO | train | epoch 038 | loss 5.228 | ppl 37.47 | wps 14584.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 14905 | lr 0.00025902 | gnorm 0.669 | loss_scale 8 | train_wall 1734 | gb_free 10.1 | wall 66920
2022-03-04 15:14:56 | INFO | fairseq.trainer | begin training epoch 39
2022-03-04 15:14:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:22:00 | INFO | train_inner | epoch 039:     95 / 393 loss=5.148, ppl=35.45, wps=14474.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=15000, lr=0.000258199, gnorm=0.677, loss_scale=8, train_wall=439, gb_free=10.1, wall=67344
2022-03-04 15:29:26 | INFO | train_inner | epoch 039:    195 / 393 loss=5.189, ppl=36.49, wps=14684.1, ups=0.22, wpb=65536, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.68, loss_scale=8, train_wall=441, gb_free=10.1, wall=67790
2022-03-04 15:36:53 | INFO | train_inner | epoch 039:    295 / 393 loss=5.229, ppl=37.5, wps=14685, ups=0.22, wpb=65536, bsz=128, num_updates=15200, lr=0.000256495, gnorm=0.642, loss_scale=8, train_wall=441, gb_free=10.1, wall=68236
2022-03-04 15:40:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 15:44:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 15:44:14 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.296 | ppl 157.2 | wps 34011.3 | wpb 2034.1 | bsz 4 | num_updates 15297 | best_loss 6.919
2022-03-04 15:44:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 15297 updates
2022-03-04 15:44:14 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-04 15:44:14 | INFO | train | epoch 039 | loss 5.207 | ppl 36.93 | wps 14594.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 15297 | lr 0.00025568 | gnorm 0.669 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 68678
2022-03-04 15:44:14 | INFO | fairseq.trainer | begin training epoch 40
2022-03-04 15:44:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 15:44:28 | INFO | train_inner | epoch 040:      3 / 393 loss=5.261, ppl=38.33, wps=14333.6, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=15300, lr=0.000255655, gnorm=0.679, loss_scale=8, train_wall=444, gb_free=10.1, wall=68691
2022-03-04 15:51:54 | INFO | train_inner | epoch 040:    103 / 393 loss=5.131, ppl=35.04, wps=14685.3, ups=0.22, wpb=65536, bsz=128, num_updates=15400, lr=0.000254824, gnorm=0.67, loss_scale=8, train_wall=441, gb_free=10.1, wall=69138
2022-03-04 15:59:21 | INFO | train_inner | epoch 040:    203 / 393 loss=5.166, ppl=35.91, wps=14663.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.668, loss_scale=8, train_wall=442, gb_free=10.1, wall=69585
2022-03-04 16:06:48 | INFO | train_inner | epoch 040:    303 / 393 loss=5.212, ppl=37.06, wps=14652.1, ups=0.22, wpb=65536, bsz=128, num_updates=15600, lr=0.000253185, gnorm=0.664, loss_scale=8, train_wall=442, gb_free=10.1, wall=70032
2022-03-04 16:13:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:13:35 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.332 | ppl 161.16 | wps 34086.2 | wpb 2034.1 | bsz 4 | num_updates 15690 | best_loss 6.919
2022-03-04 16:13:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 15690 updates
2022-03-04 16:13:35 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-04 16:13:35 | INFO | train | epoch 040 | loss 5.187 | ppl 36.43 | wps 14615 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 15690 | lr 0.000252458 | gnorm 0.676 | loss_scale 8 | train_wall 1734 | gb_free 10.1 | wall 70438
2022-03-04 16:13:35 | INFO | fairseq.trainer | begin training epoch 41
2022-03-04 16:13:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:14:19 | INFO | train_inner | epoch 041:     10 / 393 loss=5.232, ppl=37.58, wps=14465.2, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=15700, lr=0.000252377, gnorm=0.691, loss_scale=8, train_wall=440, gb_free=10.1, wall=70483
2022-03-04 16:19:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 16:21:50 | INFO | train_inner | epoch 041:    111 / 393 loss=5.109, ppl=34.51, wps=14540, ups=0.22, wpb=65536, bsz=128, num_updates=15800, lr=0.000251577, gnorm=0.664, loss_scale=8, train_wall=446, gb_free=10.1, wall=70934
2022-03-04 16:29:17 | INFO | train_inner | epoch 041:    211 / 393 loss=5.158, ppl=35.71, wps=14683.6, ups=0.22, wpb=65536, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.674, loss_scale=8, train_wall=441, gb_free=10.1, wall=71380
2022-03-04 16:36:43 | INFO | train_inner | epoch 041:    311 / 393 loss=5.198, ppl=36.71, wps=14686.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=16000, lr=0.00025, gnorm=0.668, loss_scale=8, train_wall=441, gb_free=10.1, wall=71826
2022-03-04 16:42:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 16:42:53 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.364 | ppl 164.76 | wps 34012.7 | wpb 2034.1 | bsz 4 | num_updates 16082 | best_loss 6.919
2022-03-04 16:42:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 16082 updates
2022-03-04 16:42:53 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-04 16:42:53 | INFO | train | epoch 041 | loss 5.167 | ppl 35.92 | wps 14594.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16082 | lr 0.000249362 | gnorm 0.666 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 72196
2022-03-04 16:42:53 | INFO | fairseq.trainer | begin training epoch 42
2022-03-04 16:42:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 16:44:13 | INFO | train_inner | epoch 042:     18 / 393 loss=5.19, ppl=36.51, wps=14475.7, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=16100, lr=0.000249222, gnorm=0.673, loss_scale=8, train_wall=439, gb_free=10.1, wall=72277
2022-03-04 16:51:40 | INFO | train_inner | epoch 042:    118 / 393 loss=5.102, ppl=34.33, wps=14683, ups=0.22, wpb=65536, bsz=128, num_updates=16200, lr=0.000248452, gnorm=0.684, loss_scale=8, train_wall=441, gb_free=10.1, wall=72723
2022-03-04 16:59:06 | INFO | train_inner | epoch 042:    218 / 393 loss=5.139, ppl=35.24, wps=14687.4, ups=0.22, wpb=65530.2, bsz=128, num_updates=16300, lr=0.000247689, gnorm=0.678, loss_scale=16, train_wall=441, gb_free=10.1, wall=73169
2022-03-04 17:06:32 | INFO | train_inner | epoch 042:    318 / 393 loss=5.175, ppl=36.13, wps=14676.7, ups=0.22, wpb=65536, bsz=128, num_updates=16400, lr=0.000246932, gnorm=0.674, loss_scale=16, train_wall=442, gb_free=10.1, wall=73616
2022-03-04 17:12:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:12:12 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.361 | ppl 164.4 | wps 34070.8 | wpb 2034.1 | bsz 4 | num_updates 16475 | best_loss 6.919
2022-03-04 17:12:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 16475 updates
2022-03-04 17:12:12 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-04 17:12:12 | INFO | train | epoch 042 | loss 5.148 | ppl 35.47 | wps 14627.6 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 16475 | lr 0.00024637 | gnorm 0.679 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 73955
2022-03-04 17:12:12 | INFO | fairseq.trainer | begin training epoch 43
2022-03-04 17:12:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:14:03 | INFO | train_inner | epoch 043:     25 / 393 loss=5.17, ppl=36.01, wps=14468.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16500, lr=0.000246183, gnorm=0.687, loss_scale=16, train_wall=440, gb_free=10.1, wall=74067
2022-03-04 17:14:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:21:34 | INFO | train_inner | epoch 043:    126 / 393 loss=5.079, ppl=33.8, wps=14544.3, ups=0.22, wpb=65536, bsz=128, num_updates=16600, lr=0.00024544, gnorm=0.7, loss_scale=8, train_wall=446, gb_free=10.1, wall=74518
2022-03-04 17:29:00 | INFO | train_inner | epoch 043:    226 / 393 loss=5.126, ppl=34.93, wps=14684.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=16700, lr=0.000244704, gnorm=0.687, loss_scale=8, train_wall=441, gb_free=10.1, wall=74964
2022-03-04 17:36:27 | INFO | train_inner | epoch 043:    326 / 393 loss=5.159, ppl=35.72, wps=14681.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=16800, lr=0.000243975, gnorm=0.68, loss_scale=8, train_wall=441, gb_free=10.1, wall=75410
2022-03-04 17:41:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 17:41:30 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.38 | ppl 166.57 | wps 34090.5 | wpb 2034.1 | bsz 4 | num_updates 16867 | best_loss 6.919
2022-03-04 17:41:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 16867 updates
2022-03-04 17:41:30 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-04 17:41:30 | INFO | train | epoch 043 | loss 5.13 | ppl 35.02 | wps 14593.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 16867 | lr 0.00024349 | gnorm 0.694 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 75714
2022-03-04 17:41:30 | INFO | fairseq.trainer | begin training epoch 44
2022-03-04 17:41:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 17:43:57 | INFO | train_inner | epoch 044:     33 / 393 loss=5.145, ppl=35.38, wps=14475, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=16900, lr=0.000243252, gnorm=0.699, loss_scale=8, train_wall=439, gb_free=10.1, wall=75861
2022-03-04 17:51:24 | INFO | train_inner | epoch 044:    133 / 393 loss=5.078, ppl=33.79, wps=14681.8, ups=0.22, wpb=65536, bsz=128, num_updates=17000, lr=0.000242536, gnorm=0.705, loss_scale=8, train_wall=441, gb_free=10.1, wall=76307
2022-03-04 17:53:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 17:58:55 | INFO | train_inner | epoch 044:    234 / 393 loss=5.1, ppl=34.3, wps=14540.7, ups=0.22, wpb=65535.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=0.658, loss_scale=8, train_wall=446, gb_free=10.1, wall=76758
2022-03-04 18:06:21 | INFO | train_inner | epoch 044:    334 / 393 loss=5.144, ppl=35.36, wps=14681.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=17200, lr=0.000241121, gnorm=0.681, loss_scale=8, train_wall=441, gb_free=10.1, wall=77204
2022-03-04 18:10:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:10:49 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.374 | ppl 165.91 | wps 34057.3 | wpb 2034.1 | bsz 4 | num_updates 17259 | best_loss 6.919
2022-03-04 18:10:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 17259 updates
2022-03-04 18:10:49 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-04 18:10:49 | INFO | train | epoch 044 | loss 5.113 | ppl 34.61 | wps 14592.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 17259 | lr 0.000240709 | gnorm 0.685 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 77472
2022-03-04 18:10:49 | INFO | fairseq.trainer | begin training epoch 45
2022-03-04 18:10:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:13:52 | INFO | train_inner | epoch 045:     41 / 393 loss=5.114, ppl=34.64, wps=14473.8, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17300, lr=0.000240424, gnorm=0.683, loss_scale=8, train_wall=439, gb_free=10.1, wall=77655
2022-03-04 18:21:18 | INFO | train_inner | epoch 045:    141 / 393 loss=5.062, ppl=33.4, wps=14684.3, ups=0.22, wpb=65536, bsz=128, num_updates=17400, lr=0.000239732, gnorm=0.686, loss_scale=8, train_wall=441, gb_free=10.1, wall=78101
2022-03-04 18:28:44 | INFO | train_inner | epoch 045:    241 / 393 loss=5.09, ppl=34.07, wps=14685.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=17500, lr=0.000239046, gnorm=0.689, loss_scale=8, train_wall=441, gb_free=10.1, wall=78548
2022-03-04 18:36:11 | INFO | train_inner | epoch 045:    341 / 393 loss=5.132, ppl=35.06, wps=14684.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=17600, lr=0.000238366, gnorm=0.685, loss_scale=16, train_wall=441, gb_free=10.1, wall=78994
2022-03-04 18:36:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 18:40:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 18:40:07 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.413 | ppl 170.46 | wps 34125.6 | wpb 2034.1 | bsz 4 | num_updates 17651 | best_loss 6.919
2022-03-04 18:40:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 17651 updates
2022-03-04 18:40:07 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-04 18:40:07 | INFO | train | epoch 045 | loss 5.096 | ppl 34.19 | wps 14594.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 17651 | lr 0.000238021 | gnorm 0.684 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 79230
2022-03-04 18:40:07 | INFO | fairseq.trainer | begin training epoch 46
2022-03-04 18:40:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 18:43:46 | INFO | train_inner | epoch 046:     49 / 393 loss=5.085, ppl=33.95, wps=14334.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=17700, lr=0.000237691, gnorm=0.696, loss_scale=8, train_wall=444, gb_free=10.1, wall=79449
2022-03-04 18:51:12 | INFO | train_inner | epoch 046:    149 / 393 loss=5.043, ppl=32.97, wps=14680.4, ups=0.22, wpb=65536, bsz=128, num_updates=17800, lr=0.000237023, gnorm=0.7, loss_scale=8, train_wall=441, gb_free=10.1, wall=79896
2022-03-04 18:58:38 | INFO | train_inner | epoch 046:    249 / 393 loss=5.084, ppl=33.92, wps=14685.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=0.72, loss_scale=8, train_wall=441, gb_free=10.1, wall=80342
2022-03-04 19:06:05 | INFO | train_inner | epoch 046:    349 / 393 loss=5.122, ppl=34.83, wps=14685.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=18000, lr=0.000235702, gnorm=0.697, loss_scale=8, train_wall=441, gb_free=10.1, wall=80788
2022-03-04 19:09:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:09:25 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.42 | ppl 171.23 | wps 34275.1 | wpb 2034.1 | bsz 4 | num_updates 18044 | best_loss 6.919
2022-03-04 19:09:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 18044 updates
2022-03-04 19:09:25 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-04 19:09:25 | INFO | train | epoch 046 | loss 5.08 | ppl 33.82 | wps 14630.4 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 18044 | lr 0.000235415 | gnorm 0.702 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 80989
2022-03-04 19:09:25 | INFO | fairseq.trainer | begin training epoch 47
2022-03-04 19:09:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:13:35 | INFO | train_inner | epoch 047:     56 / 393 loss=5.058, ppl=33.32, wps=14479.3, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18100, lr=0.00023505, gnorm=0.682, loss_scale=8, train_wall=439, gb_free=10.1, wall=81239
2022-03-04 19:21:02 | INFO | train_inner | epoch 047:    156 / 393 loss=5.026, ppl=32.59, wps=14687.5, ups=0.22, wpb=65536, bsz=128, num_updates=18200, lr=0.000234404, gnorm=0.678, loss_scale=16, train_wall=441, gb_free=10.1, wall=81685
2022-03-04 19:21:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 19:28:32 | INFO | train_inner | epoch 047:    257 / 393 loss=5.076, ppl=33.72, wps=14541.9, ups=0.22, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=0.728, loss_scale=8, train_wall=446, gb_free=10.1, wall=82136
2022-03-04 19:35:58 | INFO | train_inner | epoch 047:    357 / 393 loss=5.103, ppl=34.37, wps=14684.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=18400, lr=0.000233126, gnorm=0.693, loss_scale=8, train_wall=441, gb_free=10.1, wall=82582
2022-03-04 19:38:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 19:38:44 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.45 | ppl 174.81 | wps 34046.5 | wpb 2034.1 | bsz 4 | num_updates 18436 | best_loss 6.919
2022-03-04 19:38:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 18436 updates
2022-03-04 19:38:44 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-04 19:38:44 | INFO | train | epoch 047 | loss 5.064 | ppl 33.46 | wps 14595.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18436 | lr 0.000232898 | gnorm 0.699 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 82747
2022-03-04 19:38:44 | INFO | fairseq.trainer | begin training epoch 48
2022-03-04 19:38:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 19:43:29 | INFO | train_inner | epoch 048:     64 / 393 loss=5.037, ppl=32.82, wps=14475.7, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=18500, lr=0.000232495, gnorm=0.691, loss_scale=8, train_wall=439, gb_free=10.1, wall=83033
2022-03-04 19:50:55 | INFO | train_inner | epoch 048:    164 / 393 loss=5.022, ppl=32.49, wps=14687.7, ups=0.22, wpb=65530.2, bsz=128, num_updates=18600, lr=0.000231869, gnorm=0.685, loss_scale=8, train_wall=441, gb_free=10.1, wall=83479
2022-03-04 19:58:22 | INFO | train_inner | epoch 048:    264 / 393 loss=5.061, ppl=33.37, wps=14684.6, ups=0.22, wpb=65536, bsz=128, num_updates=18700, lr=0.000231249, gnorm=0.692, loss_scale=8, train_wall=441, gb_free=10.1, wall=83925
2022-03-04 20:05:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:05:52 | INFO | train_inner | epoch 048:    365 / 393 loss=5.091, ppl=34.09, wps=14543.9, ups=0.22, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=0.715, loss_scale=8, train_wall=446, gb_free=10.1, wall=84376
2022-03-04 20:07:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:08:02 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.46 | ppl 176.09 | wps 34210 | wpb 2034.1 | bsz 4 | num_updates 18828 | best_loss 6.919
2022-03-04 20:08:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 18828 updates
2022-03-04 20:08:02 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-04 20:08:02 | INFO | train | epoch 048 | loss 5.049 | ppl 33.1 | wps 14595.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 18828 | lr 0.000230461 | gnorm 0.695 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 84505
2022-03-04 20:08:02 | INFO | fairseq.trainer | begin training epoch 49
2022-03-04 20:08:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:13:23 | INFO | train_inner | epoch 049:     72 / 393 loss=5.006, ppl=32.14, wps=14472.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=18900, lr=0.000230022, gnorm=0.711, loss_scale=8, train_wall=440, gb_free=10.1, wall=84827
2022-03-04 20:20:49 | INFO | train_inner | epoch 049:    172 / 393 loss=5.004, ppl=32.09, wps=14684, ups=0.22, wpb=65530.9, bsz=128, num_updates=19000, lr=0.000229416, gnorm=0.721, loss_scale=8, train_wall=441, gb_free=10.1, wall=85273
2022-03-04 20:28:16 | INFO | train_inner | epoch 049:    272 / 393 loss=5.046, ppl=33.04, wps=14685, ups=0.22, wpb=65535.4, bsz=128, num_updates=19100, lr=0.000228814, gnorm=0.684, loss_scale=8, train_wall=441, gb_free=10.1, wall=85719
2022-03-04 20:35:42 | INFO | train_inner | epoch 049:    372 / 393 loss=5.085, ppl=33.95, wps=14689.3, ups=0.22, wpb=65536, bsz=128, num_updates=19200, lr=0.000228218, gnorm=0.709, loss_scale=8, train_wall=441, gb_free=10.1, wall=86165
2022-03-04 20:37:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 20:37:20 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.481 | ppl 178.71 | wps 34057.6 | wpb 2034.1 | bsz 4 | num_updates 19221 | best_loss 6.919
2022-03-04 20:37:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 19221 updates
2022-03-04 20:37:20 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-04 20:37:20 | INFO | train | epoch 049 | loss 5.034 | ppl 32.76 | wps 14631.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 19221 | lr 0.000228093 | gnorm 0.707 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 86263
2022-03-04 20:37:20 | INFO | fairseq.trainer | begin training epoch 50
2022-03-04 20:37:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 20:43:12 | INFO | train_inner | epoch 050:     79 / 393 loss=4.987, ppl=31.71, wps=14477.8, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=19300, lr=0.000227626, gnorm=0.678, loss_scale=8, train_wall=439, gb_free=10.1, wall=86616
2022-03-04 20:50:39 | INFO | train_inner | epoch 050:    179 / 393 loss=4.995, ppl=31.89, wps=14683.7, ups=0.22, wpb=65536, bsz=128, num_updates=19400, lr=0.000227038, gnorm=0.689, loss_scale=16, train_wall=441, gb_free=10.1, wall=87062
2022-03-04 20:53:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 20:58:09 | INFO | train_inner | epoch 050:    280 / 393 loss=5.036, ppl=32.81, wps=14540.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=19500, lr=0.000226455, gnorm=0.724, loss_scale=8, train_wall=446, gb_free=10.1, wall=87513
2022-03-04 21:05:36 | INFO | train_inner | epoch 050:    380 / 393 loss=5.073, ppl=33.67, wps=14684.8, ups=0.22, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=0.761, loss_scale=8, train_wall=441, gb_free=10.1, wall=87959
2022-03-04 21:06:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:06:38 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.494 | ppl 180.25 | wps 33978 | wpb 2034.1 | bsz 4 | num_updates 19613 | best_loss 6.919
2022-03-04 21:06:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 19613 updates
2022-03-04 21:06:38 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-04 21:06:38 | INFO | train | epoch 050 | loss 5.02 | ppl 32.45 | wps 14594.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 19613 | lr 0.000225802 | gnorm 0.713 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 88022
2022-03-04 21:06:38 | INFO | fairseq.trainer | begin training epoch 51
2022-03-04 21:06:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:13:07 | INFO | train_inner | epoch 051:     87 / 393 loss=4.956, ppl=31.03, wps=14475.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=19700, lr=0.000225303, gnorm=0.7, loss_scale=8, train_wall=439, gb_free=10.1, wall=88410
2022-03-04 21:20:33 | INFO | train_inner | epoch 051:    187 / 393 loss=4.99, ppl=31.77, wps=14685, ups=0.22, wpb=65530.2, bsz=128, num_updates=19800, lr=0.000224733, gnorm=0.722, loss_scale=8, train_wall=441, gb_free=10.1, wall=88856
2022-03-04 21:27:59 | INFO | train_inner | epoch 051:    287 / 393 loss=5.024, ppl=32.54, wps=14686.8, ups=0.22, wpb=65536, bsz=128, num_updates=19900, lr=0.000224168, gnorm=0.709, loss_scale=8, train_wall=441, gb_free=10.1, wall=89302
2022-03-04 21:33:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 21:35:30 | INFO | train_inner | epoch 051:    388 / 393 loss=5.061, ppl=33.39, wps=14539.5, ups=0.22, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=0.751, loss_scale=8, train_wall=446, gb_free=10.1, wall=89753
2022-03-04 21:35:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 21:35:56 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 7.529 | ppl 184.75 | wps 34106.4 | wpb 2034.1 | bsz 4 | num_updates 20005 | best_loss 6.919
2022-03-04 21:35:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 20005 updates
2022-03-04 21:35:56 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-04 21:35:56 | INFO | train | epoch 051 | loss 5.006 | ppl 32.14 | wps 14594.8 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20005 | lr 0.000223579 | gnorm 0.721 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 89780
2022-03-04 21:35:56 | INFO | fairseq.trainer | begin training epoch 52
2022-03-04 21:35:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 21:43:00 | INFO | train_inner | epoch 052:     95 / 393 loss=4.939, ppl=30.68, wps=14474.9, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=20100, lr=0.00022305, gnorm=0.705, loss_scale=8, train_wall=439, gb_free=10.1, wall=90204
2022-03-04 21:50:27 | INFO | train_inner | epoch 052:    195 / 393 loss=4.975, ppl=31.45, wps=14686.3, ups=0.22, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=0.717, loss_scale=8, train_wall=441, gb_free=10.1, wall=90650
2022-03-04 21:57:53 | INFO | train_inner | epoch 052:    295 / 393 loss=5.015, ppl=32.34, wps=14687.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=20300, lr=0.000221948, gnorm=0.715, loss_scale=8, train_wall=441, gb_free=10.1, wall=91096
2022-03-04 22:05:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:05:15 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 7.518 | ppl 183.27 | wps 34053.4 | wpb 2034.1 | bsz 4 | num_updates 20398 | best_loss 6.919
2022-03-04 22:05:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 20398 updates
2022-03-04 22:05:15 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-04 22:05:15 | INFO | train | epoch 052 | loss 4.993 | ppl 31.84 | wps 14632.3 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 20398 | lr 0.000221415 | gnorm 0.718 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 91538
2022-03-04 22:05:15 | INFO | fairseq.trainer | begin training epoch 53
2022-03-04 22:05:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:05:24 | INFO | train_inner | epoch 053:      2 / 393 loss=5.043, ppl=32.98, wps=14475.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=20400, lr=0.000221404, gnorm=0.735, loss_scale=8, train_wall=439, gb_free=10.1, wall=91547
2022-03-04 22:12:50 | INFO | train_inner | epoch 053:    102 / 393 loss=4.923, ppl=30.34, wps=14685.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=20500, lr=0.000220863, gnorm=0.709, loss_scale=16, train_wall=441, gb_free=10.1, wall=91993
2022-03-04 22:13:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:20:21 | INFO | train_inner | epoch 053:    203 / 393 loss=4.967, ppl=31.27, wps=14538.2, ups=0.22, wpb=65535.4, bsz=128, num_updates=20600, lr=0.000220326, gnorm=0.731, loss_scale=8, train_wall=446, gb_free=10.1, wall=92444
2022-03-04 22:27:47 | INFO | train_inner | epoch 053:    303 / 393 loss=5.002, ppl=32.04, wps=14686.8, ups=0.22, wpb=65536, bsz=128, num_updates=20700, lr=0.000219793, gnorm=0.749, loss_scale=8, train_wall=441, gb_free=10.1, wall=92890
2022-03-04 22:34:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 22:34:33 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 7.527 | ppl 184.41 | wps 34075.3 | wpb 2034.1 | bsz 4 | num_updates 20790 | best_loss 6.919
2022-03-04 22:34:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 20790 updates
2022-03-04 22:34:33 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-04 22:34:33 | INFO | train | epoch 053 | loss 4.979 | ppl 31.54 | wps 14594.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 20790 | lr 0.000219317 | gnorm 0.728 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 93296
2022-03-04 22:34:33 | INFO | fairseq.trainer | begin training epoch 54
2022-03-04 22:34:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 22:35:18 | INFO | train_inner | epoch 054:     10 / 393 loss=5.016, ppl=32.35, wps=14477.1, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=20800, lr=0.000219265, gnorm=0.723, loss_scale=8, train_wall=439, gb_free=10.1, wall=93341
2022-03-04 22:42:44 | INFO | train_inner | epoch 054:    110 / 393 loss=4.918, ppl=30.24, wps=14689.7, ups=0.22, wpb=65536, bsz=128, num_updates=20900, lr=0.000218739, gnorm=0.706, loss_scale=8, train_wall=441, gb_free=10.1, wall=93787
2022-03-04 22:50:10 | INFO | train_inner | epoch 054:    210 / 393 loss=4.955, ppl=31.02, wps=14682.8, ups=0.22, wpb=65536, bsz=128, num_updates=21000, lr=0.000218218, gnorm=0.709, loss_scale=8, train_wall=441, gb_free=10.1, wall=94233
2022-03-04 22:51:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 22:57:41 | INFO | train_inner | epoch 054:    311 / 393 loss=4.99, ppl=31.77, wps=14538.5, ups=0.22, wpb=65536, bsz=128, num_updates=21100, lr=0.0002177, gnorm=0.749, loss_scale=8, train_wall=446, gb_free=10.1, wall=94684
2022-03-04 23:03:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:03:51 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 7.547 | ppl 187.04 | wps 34154.8 | wpb 2034.1 | bsz 4 | num_updates 21182 | best_loss 6.919
2022-03-04 23:03:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 21182 updates
2022-03-04 23:03:51 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-04 23:03:51 | INFO | train | epoch 054 | loss 4.967 | ppl 31.29 | wps 14594.4 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 21182 | lr 0.000217278 | gnorm 0.721 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 95055
2022-03-04 23:03:51 | INFO | fairseq.trainer | begin training epoch 55
2022-03-04 23:03:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:05:12 | INFO | train_inner | epoch 055:     18 / 393 loss=5.001, ppl=32.01, wps=14476, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=21200, lr=0.000217186, gnorm=0.732, loss_scale=8, train_wall=439, gb_free=10.1, wall=95135
2022-03-04 23:12:38 | INFO | train_inner | epoch 055:    118 / 393 loss=4.905, ppl=29.97, wps=14681.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=21300, lr=0.000216676, gnorm=0.701, loss_scale=8, train_wall=441, gb_free=10.1, wall=95581
2022-03-04 23:20:04 | INFO | train_inner | epoch 055:    218 / 393 loss=4.947, ppl=30.85, wps=14680.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=21400, lr=0.000216169, gnorm=0.722, loss_scale=8, train_wall=442, gb_free=10.1, wall=96028
2022-03-04 23:27:31 | INFO | train_inner | epoch 055:    318 / 393 loss=4.982, ppl=31.61, wps=14685.2, ups=0.22, wpb=65536, bsz=128, num_updates=21500, lr=0.000215666, gnorm=0.722, loss_scale=8, train_wall=441, gb_free=10.1, wall=96474
2022-03-04 23:33:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-04 23:33:10 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 7.537 | ppl 185.78 | wps 34018.2 | wpb 2034.1 | bsz 4 | num_updates 21575 | best_loss 6.919
2022-03-04 23:33:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 21575 updates
2022-03-04 23:33:10 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-04 23:33:10 | INFO | train | epoch 055 | loss 4.956 | ppl 31.03 | wps 14628.2 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 21575 | lr 0.00021529 | gnorm 0.717 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 96813
2022-03-04 23:33:10 | INFO | fairseq.trainer | begin training epoch 56
2022-03-04 23:33:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-04 23:35:02 | INFO | train_inner | epoch 056:     25 / 393 loss=4.978, ppl=31.53, wps=14470.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=21600, lr=0.000215166, gnorm=0.706, loss_scale=16, train_wall=440, gb_free=10.1, wall=96925
2022-03-04 23:41:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-04 23:42:32 | INFO | train_inner | epoch 056:    126 / 393 loss=4.891, ppl=29.67, wps=14534.5, ups=0.22, wpb=65536, bsz=128, num_updates=21700, lr=0.000214669, gnorm=0.739, loss_scale=8, train_wall=446, gb_free=10.1, wall=97376
2022-03-04 23:49:59 | INFO | train_inner | epoch 056:    226 / 393 loss=4.941, ppl=30.71, wps=14688.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=21800, lr=0.000214176, gnorm=0.714, loss_scale=8, train_wall=441, gb_free=10.1, wall=97822
2022-03-04 23:57:25 | INFO | train_inner | epoch 056:    326 / 393 loss=4.976, ppl=31.46, wps=14685, ups=0.22, wpb=65536, bsz=128, num_updates=21900, lr=0.000213687, gnorm=0.734, loss_scale=8, train_wall=441, gb_free=10.1, wall=98268
2022-03-05 00:02:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:02:28 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 7.565 | ppl 189.36 | wps 34086.3 | wpb 2034.1 | bsz 4 | num_updates 21967 | best_loss 6.919
2022-03-05 00:02:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 21967 updates
2022-03-05 00:02:28 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-05 00:02:28 | INFO | train | epoch 056 | loss 4.943 | ppl 30.76 | wps 14593.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 21967 | lr 0.000213361 | gnorm 0.733 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 98572
2022-03-05 00:02:28 | INFO | fairseq.trainer | begin training epoch 57
2022-03-05 00:02:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:04:56 | INFO | train_inner | epoch 057:     33 / 393 loss=4.954, ppl=31.01, wps=14475, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=22000, lr=0.000213201, gnorm=0.746, loss_scale=8, train_wall=439, gb_free=10.1, wall=98719
2022-03-05 00:12:22 | INFO | train_inner | epoch 057:    133 / 393 loss=4.893, ppl=29.71, wps=14683.4, ups=0.22, wpb=65536, bsz=128, num_updates=22100, lr=0.000212718, gnorm=0.717, loss_scale=8, train_wall=441, gb_free=10.1, wall=99165
2022-03-05 00:19:48 | INFO | train_inner | epoch 057:    233 / 393 loss=4.927, ppl=30.42, wps=14680.3, ups=0.22, wpb=65530.2, bsz=128, num_updates=22200, lr=0.000212238, gnorm=0.744, loss_scale=8, train_wall=441, gb_free=10.1, wall=99612
2022-03-05 00:27:15 | INFO | train_inner | epoch 057:    333 / 393 loss=4.963, ppl=31.18, wps=14686.5, ups=0.22, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=0.737, loss_scale=16, train_wall=441, gb_free=10.1, wall=100058
2022-03-05 00:31:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 00:31:47 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 7.59 | ppl 192.7 | wps 34180 | wpb 2034.1 | bsz 4 | num_updates 22360 | best_loss 6.919
2022-03-05 00:31:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 22360 updates
2022-03-05 00:31:47 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-05 00:31:47 | INFO | train | epoch 057 | loss 4.932 | ppl 30.52 | wps 14630.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 22360 | lr 0.000211477 | gnorm 0.733 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 100330
2022-03-05 00:31:47 | INFO | fairseq.trainer | begin training epoch 58
2022-03-05 00:31:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 00:34:45 | INFO | train_inner | epoch 058:     40 / 393 loss=4.935, ppl=30.6, wps=14477.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=22400, lr=0.000211289, gnorm=0.737, loss_scale=16, train_wall=439, gb_free=10.1, wall=100509
2022-03-05 00:41:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 00:42:16 | INFO | train_inner | epoch 058:    141 / 393 loss=4.881, ppl=29.46, wps=14539.3, ups=0.22, wpb=65535.4, bsz=128, num_updates=22500, lr=0.000210819, gnorm=0.732, loss_scale=8, train_wall=446, gb_free=10.1, wall=100959
2022-03-05 00:49:42 | INFO | train_inner | epoch 058:    241 / 393 loss=4.925, ppl=30.38, wps=14686.9, ups=0.22, wpb=65536, bsz=128, num_updates=22600, lr=0.000210352, gnorm=0.759, loss_scale=8, train_wall=441, gb_free=10.1, wall=101406
2022-03-05 00:57:09 | INFO | train_inner | epoch 058:    341 / 393 loss=4.956, ppl=31.04, wps=14682.2, ups=0.22, wpb=65536, bsz=128, num_updates=22700, lr=0.000209888, gnorm=0.731, loss_scale=8, train_wall=441, gb_free=10.1, wall=101852
2022-03-05 01:00:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:01:05 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 7.583 | ppl 191.73 | wps 34020.4 | wpb 2034.1 | bsz 4 | num_updates 22752 | best_loss 6.919
2022-03-05 01:01:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 22752 updates
2022-03-05 01:01:05 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-05 01:01:05 | INFO | train | epoch 058 | loss 4.92 | ppl 30.28 | wps 14593.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 22752 | lr 0.000209648 | gnorm 0.739 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 102088
2022-03-05 01:01:05 | INFO | fairseq.trainer | begin training epoch 59
2022-03-05 01:01:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:04:39 | INFO | train_inner | epoch 059:     48 / 393 loss=4.909, ppl=30.05, wps=14475.6, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=22800, lr=0.000209427, gnorm=0.746, loss_scale=8, train_wall=439, gb_free=10.1, wall=102303
2022-03-05 01:12:06 | INFO | train_inner | epoch 059:    148 / 393 loss=4.873, ppl=29.31, wps=14682, ups=0.22, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=0.717, loss_scale=8, train_wall=441, gb_free=10.1, wall=102749
2022-03-05 01:19:32 | INFO | train_inner | epoch 059:    248 / 393 loss=4.915, ppl=30.16, wps=14686.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=23000, lr=0.000208514, gnorm=0.757, loss_scale=8, train_wall=441, gb_free=10.1, wall=103195
2022-03-05 01:20:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 01:27:03 | INFO | train_inner | epoch 059:    349 / 393 loss=4.951, ppl=30.93, wps=14542.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=23100, lr=0.000208063, gnorm=0.758, loss_scale=8, train_wall=446, gb_free=10.1, wall=103646
2022-03-05 01:30:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:30:23 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 7.587 | ppl 192.31 | wps 34122.5 | wpb 2034.1 | bsz 4 | num_updates 23144 | best_loss 6.919
2022-03-05 01:30:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 23144 updates
2022-03-05 01:30:23 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-05 01:30:23 | INFO | train | epoch 059 | loss 4.91 | ppl 30.06 | wps 14594 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 23144 | lr 0.000207865 | gnorm 0.746 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 103847
2022-03-05 01:30:23 | INFO | fairseq.trainer | begin training epoch 60
2022-03-05 01:30:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 01:34:33 | INFO | train_inner | epoch 060:     56 / 393 loss=4.892, ppl=29.69, wps=14473.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=23200, lr=0.000207614, gnorm=0.732, loss_scale=8, train_wall=439, gb_free=10.1, wall=104097
2022-03-05 01:42:00 | INFO | train_inner | epoch 060:    156 / 393 loss=4.868, ppl=29.21, wps=14685.2, ups=0.22, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=0.746, loss_scale=8, train_wall=441, gb_free=10.1, wall=104543
2022-03-05 01:49:26 | INFO | train_inner | epoch 060:    256 / 393 loss=4.906, ppl=29.99, wps=14683.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=23400, lr=0.000206725, gnorm=0.727, loss_scale=8, train_wall=441, gb_free=10.1, wall=104989
2022-03-05 01:56:52 | INFO | train_inner | epoch 060:    356 / 393 loss=4.939, ppl=30.67, wps=14688.7, ups=0.22, wpb=65530.9, bsz=128, num_updates=23500, lr=0.000206284, gnorm=0.751, loss_scale=8, train_wall=441, gb_free=10.1, wall=105436
2022-03-05 01:59:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 01:59:42 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 7.629 | ppl 197.95 | wps 34191.2 | wpb 2034.1 | bsz 4 | num_updates 23537 | best_loss 6.919
2022-03-05 01:59:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 23537 updates
2022-03-05 01:59:42 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-05 01:59:42 | INFO | train | epoch 060 | loss 4.899 | ppl 29.84 | wps 14631.8 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 23537 | lr 0.000206122 | gnorm 0.739 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 105605
2022-03-05 01:59:42 | INFO | fairseq.trainer | begin training epoch 61
2022-03-05 01:59:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:04:23 | INFO | train_inner | epoch 061:     63 / 393 loss=4.866, ppl=29.16, wps=14475.5, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=23600, lr=0.000205847, gnorm=0.725, loss_scale=16, train_wall=439, gb_free=10.1, wall=105886
2022-03-05 02:07:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 02:11:54 | INFO | train_inner | epoch 061:    164 / 393 loss=4.855, ppl=28.94, wps=14539.5, ups=0.22, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=0.746, loss_scale=8, train_wall=446, gb_free=10.1, wall=106337
2022-03-05 02:19:20 | INFO | train_inner | epoch 061:    264 / 393 loss=4.905, ppl=29.97, wps=14687.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=23800, lr=0.00020498, gnorm=0.75, loss_scale=8, train_wall=441, gb_free=10.1, wall=106783
2022-03-05 02:26:46 | INFO | train_inner | epoch 061:    364 / 393 loss=4.936, ppl=30.61, wps=14687.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=23900, lr=0.000204551, gnorm=0.753, loss_scale=8, train_wall=441, gb_free=10.1, wall=107229
2022-03-05 02:28:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:29:00 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 7.619 | ppl 196.65 | wps 34292.5 | wpb 2034.1 | bsz 4 | num_updates 23929 | best_loss 6.919
2022-03-05 02:29:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 23929 updates
2022-03-05 02:29:00 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-05 02:29:00 | INFO | train | epoch 061 | loss 4.889 | ppl 29.62 | wps 14595.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 23929 | lr 0.000204427 | gnorm 0.742 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 107363
2022-03-05 02:29:00 | INFO | fairseq.trainer | begin training epoch 62
2022-03-05 02:29:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 02:34:17 | INFO | train_inner | epoch 062:     71 / 393 loss=4.858, ppl=29, wps=14478.4, ups=0.22, wpb=65243.5, bsz=127.4, num_updates=24000, lr=0.000204124, gnorm=0.748, loss_scale=8, train_wall=439, gb_free=10.1, wall=107680
2022-03-05 02:41:43 | INFO | train_inner | epoch 062:    171 / 393 loss=4.846, ppl=28.77, wps=14687.8, ups=0.22, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=0.738, loss_scale=8, train_wall=441, gb_free=10.1, wall=108126
2022-03-05 02:49:09 | INFO | train_inner | epoch 062:    271 / 393 loss=4.893, ppl=29.72, wps=14682.9, ups=0.22, wpb=65536, bsz=128, num_updates=24200, lr=0.000203279, gnorm=0.763, loss_scale=16, train_wall=441, gb_free=10.1, wall=108573
2022-03-05 02:56:35 | INFO | train_inner | epoch 062:    371 / 393 loss=4.926, ppl=30.4, wps=14686.3, ups=0.22, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=0.743, loss_scale=16, train_wall=441, gb_free=10.1, wall=109019
2022-03-05 02:58:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 02:58:18 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 7.65 | ppl 200.8 | wps 34011.1 | wpb 2034.1 | bsz 4 | num_updates 24322 | best_loss 6.919
2022-03-05 02:58:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 24322 updates
2022-03-05 02:58:18 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-05 02:58:18 | INFO | train | epoch 062 | loss 4.879 | ppl 29.43 | wps 14632.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 24322 | lr 0.000202768 | gnorm 0.749 | loss_scale 16 | train_wall 1732 | gb_free 10.1 | wall 109121
2022-03-05 02:58:18 | INFO | fairseq.trainer | begin training epoch 63
2022-03-05 02:58:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:04:06 | INFO | train_inner | epoch 063:     78 / 393 loss=4.834, ppl=28.53, wps=14473.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=24400, lr=0.000202444, gnorm=0.744, loss_scale=16, train_wall=439, gb_free=10.1, wall=109470
2022-03-05 03:05:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:11:37 | INFO | train_inner | epoch 063:    179 / 393 loss=4.84, ppl=28.64, wps=14540.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=24500, lr=0.000202031, gnorm=0.75, loss_scale=8, train_wall=446, gb_free=10.1, wall=109920
2022-03-05 03:19:03 | INFO | train_inner | epoch 063:    279 / 393 loss=4.886, ppl=29.57, wps=14684.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=24600, lr=0.000201619, gnorm=0.746, loss_scale=8, train_wall=441, gb_free=10.1, wall=110367
2022-03-05 03:26:29 | INFO | train_inner | epoch 063:    379 / 393 loss=4.921, ppl=30.29, wps=14687.7, ups=0.22, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=0.735, loss_scale=8, train_wall=441, gb_free=10.1, wall=110813
2022-03-05 03:27:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:27:36 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 7.634 | ppl 198.58 | wps 34195.2 | wpb 2034.1 | bsz 4 | num_updates 24714 | best_loss 6.919
2022-03-05 03:27:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 24714 updates
2022-03-05 03:27:36 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-05 03:27:36 | INFO | train | epoch 063 | loss 4.868 | ppl 29.21 | wps 14594.6 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 24714 | lr 0.000201154 | gnorm 0.743 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 110880
2022-03-05 03:27:36 | INFO | fairseq.trainer | begin training epoch 64
2022-03-05 03:27:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 03:34:00 | INFO | train_inner | epoch 064:     86 / 393 loss=4.817, ppl=28.18, wps=14477.6, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=24800, lr=0.000200805, gnorm=0.751, loss_scale=8, train_wall=439, gb_free=10.1, wall=111264
2022-03-05 03:41:26 | INFO | train_inner | epoch 064:    186 / 393 loss=4.836, ppl=28.56, wps=14688.9, ups=0.22, wpb=65536, bsz=128, num_updates=24900, lr=0.000200401, gnorm=0.743, loss_scale=8, train_wall=441, gb_free=10.1, wall=111710
2022-03-05 03:44:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 03:48:57 | INFO | train_inner | epoch 064:    287 / 393 loss=4.877, ppl=29.38, wps=14541.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=25000, lr=0.0002, gnorm=0.764, loss_scale=8, train_wall=446, gb_free=10.1, wall=112160
2022-03-05 03:56:23 | INFO | train_inner | epoch 064:    387 / 393 loss=4.913, ppl=30.13, wps=14683.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=25100, lr=0.000199601, gnorm=0.755, loss_scale=8, train_wall=441, gb_free=10.1, wall=112607
2022-03-05 03:56:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 03:56:54 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 7.667 | ppl 203.29 | wps 34015.2 | wpb 2034.1 | bsz 4 | num_updates 25106 | best_loss 6.919
2022-03-05 03:56:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 25106 updates
2022-03-05 03:56:54 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-05 03:56:54 | INFO | train | epoch 064 | loss 4.859 | ppl 29.03 | wps 14595.1 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 25106 | lr 0.000199577 | gnorm 0.755 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 112638
2022-03-05 03:56:54 | INFO | fairseq.trainer | begin training epoch 65
2022-03-05 03:56:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:03:54 | INFO | train_inner | epoch 065:     94 / 393 loss=4.797, ppl=27.79, wps=14472.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=25200, lr=0.000199205, gnorm=0.757, loss_scale=8, train_wall=439, gb_free=10.1, wall=113057
2022-03-05 04:11:20 | INFO | train_inner | epoch 065:    194 / 393 loss=4.835, ppl=28.53, wps=14683.1, ups=0.22, wpb=65530.2, bsz=128, num_updates=25300, lr=0.000198811, gnorm=0.742, loss_scale=8, train_wall=441, gb_free=10.1, wall=113504
2022-03-05 04:18:47 | INFO | train_inner | epoch 065:    294 / 393 loss=4.866, ppl=29.16, wps=14685.6, ups=0.22, wpb=65536, bsz=128, num_updates=25400, lr=0.000198419, gnorm=0.77, loss_scale=8, train_wall=441, gb_free=10.1, wall=113950
2022-03-05 04:26:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:26:13 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 7.672 | ppl 203.94 | wps 34058.6 | wpb 2034.1 | bsz 4 | num_updates 25499 | best_loss 6.919
2022-03-05 04:26:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 25499 updates
2022-03-05 04:26:13 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-05 04:26:13 | INFO | train | epoch 065 | loss 4.85 | ppl 28.84 | wps 14629.5 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 25499 | lr 0.000198033 | gnorm 0.757 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 114396
2022-03-05 04:26:13 | INFO | fairseq.trainer | begin training epoch 66
2022-03-05 04:26:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:26:18 | INFO | train_inner | epoch 066:      1 / 393 loss=4.906, ppl=29.99, wps=14470.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=25500, lr=0.00019803, gnorm=0.759, loss_scale=16, train_wall=440, gb_free=10.1, wall=114401
2022-03-05 04:33:44 | INFO | train_inner | epoch 066:    101 / 393 loss=4.781, ppl=27.5, wps=14683.9, ups=0.22, wpb=65530.9, bsz=128, num_updates=25600, lr=0.000197642, gnorm=0.75, loss_scale=16, train_wall=441, gb_free=10.1, wall=114847
2022-03-05 04:34:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 04:41:15 | INFO | train_inner | epoch 066:    202 / 393 loss=4.824, ppl=28.33, wps=14539.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=25700, lr=0.000197257, gnorm=0.749, loss_scale=8, train_wall=446, gb_free=10.1, wall=115298
2022-03-05 04:48:41 | INFO | train_inner | epoch 066:    302 / 393 loss=4.869, ppl=29.21, wps=14684.7, ups=0.22, wpb=65536, bsz=128, num_updates=25800, lr=0.000196875, gnorm=0.744, loss_scale=8, train_wall=441, gb_free=10.1, wall=115744
2022-03-05 04:55:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 04:55:31 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 7.668 | ppl 203.44 | wps 34037.1 | wpb 2034.1 | bsz 4 | num_updates 25891 | best_loss 6.919
2022-03-05 04:55:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 25891 updates
2022-03-05 04:55:31 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-05 04:55:31 | INFO | train | epoch 066 | loss 4.841 | ppl 28.65 | wps 14592.7 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 25891 | lr 0.000196529 | gnorm 0.753 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 116155
2022-03-05 04:55:31 | INFO | fairseq.trainer | begin training epoch 67
2022-03-05 04:55:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 04:56:12 | INFO | train_inner | epoch 067:      9 / 393 loss=4.882, ppl=29.48, wps=14472.4, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=25900, lr=0.000196494, gnorm=0.768, loss_scale=8, train_wall=440, gb_free=10.1, wall=116195
2022-03-05 05:03:38 | INFO | train_inner | epoch 067:    109 / 393 loss=4.777, ppl=27.42, wps=14686.8, ups=0.22, wpb=65536, bsz=128, num_updates=26000, lr=0.000196116, gnorm=0.751, loss_scale=8, train_wall=441, gb_free=10.1, wall=116641
2022-03-05 05:11:04 | INFO | train_inner | epoch 067:    209 / 393 loss=4.817, ppl=28.2, wps=14684.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=26100, lr=0.00019574, gnorm=0.74, loss_scale=8, train_wall=441, gb_free=10.1, wall=117088
2022-03-05 05:18:30 | INFO | train_inner | epoch 067:    309 / 393 loss=4.858, ppl=29, wps=14684.9, ups=0.22, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=0.761, loss_scale=16, train_wall=441, gb_free=10.1, wall=117534
2022-03-05 05:19:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 05:24:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:24:50 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 7.7 | ppl 207.91 | wps 34048.5 | wpb 2034.1 | bsz 4 | num_updates 26283 | best_loss 6.919
2022-03-05 05:24:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 26283 updates
2022-03-05 05:24:50 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-05 05:24:50 | INFO | train | epoch 067 | loss 4.832 | ppl 28.49 | wps 14594.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 26283 | lr 0.000195057 | gnorm 0.755 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 117913
2022-03-05 05:24:50 | INFO | fairseq.trainer | begin training epoch 68
2022-03-05 05:24:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:26:06 | INFO | train_inner | epoch 068:     17 / 393 loss=4.867, ppl=29.19, wps=14331.4, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=26300, lr=0.000194994, gnorm=0.763, loss_scale=8, train_wall=444, gb_free=10.1, wall=117989
2022-03-05 05:33:32 | INFO | train_inner | epoch 068:    117 / 393 loss=4.775, ppl=27.37, wps=14689, ups=0.22, wpb=65536, bsz=128, num_updates=26400, lr=0.000194625, gnorm=0.775, loss_scale=8, train_wall=441, gb_free=10.1, wall=118435
2022-03-05 05:40:58 | INFO | train_inner | epoch 068:    217 / 393 loss=4.812, ppl=28.1, wps=14684.5, ups=0.22, wpb=65530.9, bsz=128, num_updates=26500, lr=0.000194257, gnorm=0.766, loss_scale=8, train_wall=441, gb_free=10.1, wall=118882
2022-03-05 05:48:24 | INFO | train_inner | epoch 068:    317 / 393 loss=4.851, ppl=28.85, wps=14686.5, ups=0.22, wpb=65535.4, bsz=128, num_updates=26600, lr=0.000193892, gnorm=0.766, loss_scale=8, train_wall=441, gb_free=10.1, wall=119328
2022-03-05 05:54:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 05:54:08 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 7.681 | ppl 205.24 | wps 34008.2 | wpb 2034.1 | bsz 4 | num_updates 26676 | best_loss 6.919
2022-03-05 05:54:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 26676 updates
2022-03-05 05:54:08 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-05 05:54:08 | INFO | train | epoch 068 | loss 4.823 | ppl 28.31 | wps 14631.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 26676 | lr 0.000193615 | gnorm 0.771 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 119672
2022-03-05 05:54:08 | INFO | fairseq.trainer | begin training epoch 69
2022-03-05 05:54:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 05:55:55 | INFO | train_inner | epoch 069:     24 / 393 loss=4.85, ppl=28.84, wps=14470, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=26700, lr=0.000193528, gnorm=0.776, loss_scale=8, train_wall=440, gb_free=10.1, wall=119779
2022-03-05 05:57:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:03:26 | INFO | train_inner | epoch 069:    125 / 393 loss=4.768, ppl=27.24, wps=14540.5, ups=0.22, wpb=65536, bsz=128, num_updates=26800, lr=0.000193167, gnorm=0.78, loss_scale=8, train_wall=446, gb_free=10.1, wall=120229
2022-03-05 06:10:52 | INFO | train_inner | epoch 069:    225 / 393 loss=4.805, ppl=27.95, wps=14685.2, ups=0.22, wpb=65536, bsz=128, num_updates=26900, lr=0.000192807, gnorm=0.759, loss_scale=8, train_wall=441, gb_free=10.1, wall=120676
2022-03-05 06:18:19 | INFO | train_inner | epoch 069:    325 / 393 loss=4.847, ppl=28.77, wps=14685.6, ups=0.22, wpb=65535.4, bsz=128, num_updates=27000, lr=0.00019245, gnorm=0.768, loss_scale=8, train_wall=441, gb_free=10.1, wall=121122
2022-03-05 06:23:20 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:23:27 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 7.706 | ppl 208.73 | wps 34026.2 | wpb 2034.1 | bsz 4 | num_updates 27068 | best_loss 6.919
2022-03-05 06:23:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 27068 updates
2022-03-05 06:23:27 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-05 06:23:27 | INFO | train | epoch 069 | loss 4.815 | ppl 28.15 | wps 14592.9 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 27068 | lr 0.000192208 | gnorm 0.77 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 121430
2022-03-05 06:23:27 | INFO | fairseq.trainer | begin training epoch 70
2022-03-05 06:23:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:25:49 | INFO | train_inner | epoch 070:     32 / 393 loss=4.83, ppl=28.44, wps=14470.9, ups=0.22, wpb=65244.2, bsz=127.4, num_updates=27100, lr=0.000192095, gnorm=0.765, loss_scale=8, train_wall=440, gb_free=10.1, wall=121573
2022-03-05 06:33:16 | INFO | train_inner | epoch 070:    132 / 393 loss=4.767, ppl=27.22, wps=14687.4, ups=0.22, wpb=65530.9, bsz=128, num_updates=27200, lr=0.000191741, gnorm=0.756, loss_scale=8, train_wall=441, gb_free=10.1, wall=122019
2022-03-05 06:39:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 06:40:46 | INFO | train_inner | epoch 070:    233 / 393 loss=4.8, ppl=27.85, wps=14545.1, ups=0.22, wpb=65535.4, bsz=128, num_updates=27300, lr=0.00019139, gnorm=0.762, loss_scale=8, train_wall=446, gb_free=10.1, wall=122470
2022-03-05 06:48:12 | INFO | train_inner | epoch 070:    333 / 393 loss=4.841, ppl=28.66, wps=14686.3, ups=0.22, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=0.791, loss_scale=8, train_wall=441, gb_free=10.1, wall=122916
2022-03-05 06:52:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 06:52:45 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 7.713 | ppl 209.76 | wps 33993.8 | wpb 2034.1 | bsz 4 | num_updates 27460 | best_loss 6.919
2022-03-05 06:52:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 27460 updates
2022-03-05 06:52:45 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-05 06:52:45 | INFO | train | epoch 070 | loss 4.807 | ppl 27.99 | wps 14596.5 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 27460 | lr 0.000190831 | gnorm 0.766 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 123188
2022-03-05 06:52:45 | INFO | fairseq.trainer | begin training epoch 71
2022-03-05 06:52:45 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 06:55:43 | INFO | train_inner | epoch 071:     40 / 393 loss=4.813, ppl=28.1, wps=14476.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=27500, lr=0.000190693, gnorm=0.779, loss_scale=8, train_wall=439, gb_free=10.1, wall=123367
2022-03-05 07:03:09 | INFO | train_inner | epoch 071:    140 / 393 loss=4.756, ppl=27.03, wps=14683.4, ups=0.22, wpb=65535.4, bsz=128, num_updates=27600, lr=0.000190347, gnorm=0.757, loss_scale=8, train_wall=441, gb_free=10.1, wall=123813
2022-03-05 07:10:36 | INFO | train_inner | epoch 071:    240 / 393 loss=4.803, ppl=27.92, wps=14681.7, ups=0.22, wpb=65536, bsz=128, num_updates=27700, lr=0.000190003, gnorm=0.748, loss_scale=8, train_wall=441, gb_free=10.1, wall=124259
2022-03-05 07:18:02 | INFO | train_inner | epoch 071:    340 / 393 loss=4.832, ppl=28.48, wps=14685.8, ups=0.22, wpb=65530.9, bsz=128, num_updates=27800, lr=0.000189661, gnorm=0.786, loss_scale=16, train_wall=441, gb_free=10.1, wall=124705
2022-03-05 07:21:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:22:03 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 7.711 | ppl 209.5 | wps 34040.9 | wpb 2034.1 | bsz 4 | num_updates 27853 | best_loss 6.919
2022-03-05 07:22:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 27853 updates
2022-03-05 07:22:03 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-05 07:22:03 | INFO | train | epoch 071 | loss 4.798 | ppl 27.83 | wps 14630.1 | ups 0.22 | wpb 65461.6 | bsz 127.9 | num_updates 27853 | lr 0.00018948 | gnorm 0.767 | loss_scale 16 | train_wall 1733 | gb_free 10.1 | wall 124946
2022-03-05 07:22:03 | INFO | fairseq.trainer | begin training epoch 72
2022-03-05 07:22:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:22:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 07:25:37 | INFO | train_inner | epoch 072:     48 / 393 loss=4.793, ppl=27.73, wps=14336.1, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=27900, lr=0.000189321, gnorm=0.773, loss_scale=8, train_wall=444, gb_free=10.1, wall=125161
2022-03-05 07:33:03 | INFO | train_inner | epoch 072:    148 / 393 loss=4.757, ppl=27.03, wps=14686.8, ups=0.22, wpb=65535.4, bsz=128, num_updates=28000, lr=0.000188982, gnorm=0.769, loss_scale=8, train_wall=441, gb_free=10.1, wall=125607
2022-03-05 07:40:30 | INFO | train_inner | epoch 072:    248 / 393 loss=4.796, ppl=27.79, wps=14686.6, ups=0.22, wpb=65536, bsz=128, num_updates=28100, lr=0.000188646, gnorm=0.77, loss_scale=8, train_wall=441, gb_free=10.1, wall=126053
2022-03-05 07:47:56 | INFO | train_inner | epoch 072:    348 / 393 loss=4.826, ppl=28.36, wps=14686.3, ups=0.22, wpb=65530.9, bsz=128, num_updates=28200, lr=0.000188311, gnorm=0.774, loss_scale=8, train_wall=441, gb_free=10.1, wall=126499
2022-03-05 07:51:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 07:51:21 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 7.734 | ppl 212.97 | wps 34154.8 | wpb 2034.1 | bsz 4 | num_updates 28245 | best_loss 6.919
2022-03-05 07:51:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 28245 updates
2022-03-05 07:51:21 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-05 07:51:21 | INFO | train | epoch 072 | loss 4.791 | ppl 27.68 | wps 14596.3 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 28245 | lr 0.000188161 | gnorm 0.775 | loss_scale 8 | train_wall 1732 | gb_free 10.1 | wall 126704
2022-03-05 07:51:21 | INFO | fairseq.trainer | begin training epoch 73
2022-03-05 07:51:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 07:55:26 | INFO | train_inner | epoch 073:     55 / 393 loss=4.772, ppl=27.31, wps=14478.6, ups=0.22, wpb=65248.6, bsz=127.4, num_updates=28300, lr=0.000187978, gnorm=0.786, loss_scale=8, train_wall=439, gb_free=10.1, wall=126950
2022-03-05 08:01:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-05 08:02:57 | INFO | train_inner | epoch 073:    156 / 393 loss=4.755, ppl=27, wps=14541.2, ups=0.22, wpb=65530.9, bsz=128, num_updates=28400, lr=0.000187647, gnorm=0.774, loss_scale=8, train_wall=446, gb_free=10.1, wall=127401
2022-03-05 08:10:23 | INFO | train_inner | epoch 073:    256 / 393 loss=4.792, ppl=27.7, wps=14684.1, ups=0.22, wpb=65536, bsz=128, num_updates=28500, lr=0.000187317, gnorm=0.788, loss_scale=8, train_wall=441, gb_free=10.1, wall=127847
2022-03-05 08:17:50 | INFO | train_inner | epoch 073:    356 / 393 loss=4.816, ppl=28.17, wps=14685.9, ups=0.22, wpb=65536, bsz=128, num_updates=28600, lr=0.000186989, gnorm=0.776, loss_scale=8, train_wall=441, gb_free=10.1, wall=128293
2022-03-05 08:20:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-05 08:20:39 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 7.735 | ppl 213.03 | wps 34071.9 | wpb 2034.1 | bsz 4 | num_updates 28637 | best_loss 6.919
2022-03-05 08:20:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 28637 updates
2022-03-05 08:20:39 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-05 08:20:39 | INFO | train | epoch 073 | loss 4.783 | ppl 27.52 | wps 14594.2 | ups 0.22 | wpb 65461.4 | bsz 127.9 | num_updates 28637 | lr 0.000186869 | gnorm 0.785 | loss_scale 8 | train_wall 1733 | gb_free 10.1 | wall 128463
2022-03-05 08:20:39 | INFO | fairseq.trainer | begin training epoch 74
2022-03-05 08:20:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-05 08:25:20 | INFO | train_inner | epoch 074:     63 / 393 loss=4.763, ppl=27.15, wps=14475.9, ups=0.22, wpb=65249.3, bsz=127.4, num_updates=28700, lr=0.000186663, gnorm=0.79, loss_scale=8, train_wall=439, gb_free=10.1, wall=128744
2022-03-05 08:32:47 | INFO | train_inner | epoch 074:    163 / 393 loss=4.746, ppl=26.83, wps=14684.5, ups=0.22, wpb=65536, bsz=128, num_updates=28800, lr=0.000186339, gnorm=0.797, loss_scale=8, train_wall=441, gb_free=10.1, wall=129190
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/jelinek_mercer.py", line 98, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 328, in extract_features_scriptable
    if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():
KeyboardInterrupt
