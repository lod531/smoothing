Sender: LSF System <lsfadmin@eu-g3-059>
Subject: Job 207345372: <w103_size_0.125_fp16_label_smoothing_0.04_#2> in cluster <euler> Exited

Job <w103_size_0.125_fp16_label_smoothing_0.04_#2> was submitted from host <eu-login-10> by user <andriusb> in cluster <euler> at Sun Mar  6 12:41:22 2022
Job was executed on host(s) <eu-g3-059>, in queue <gpuhe.24h>, as user <andriusb> in cluster <euler> at Sun Mar  6 12:41:47 2022
</cluster/home/andriusb> was used as the home directory.
</cluster/home/andriusb/fq/fairseq> was used as the working directory.
Started at Sun Mar  6 12:41:47 2022
Terminated at Tue Mar  8 06:20:45 2022
Results reported at Tue Mar  8 06:20:45 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
CUDA_VISIBLE_DEVICES=0 fairseq-train --task language_modeling data-bin/wikitext-103-raw-size-0.125 --save-dir /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2 --arch transformer_lm --share-decoder-input-output-embed --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.04 --optimizer adam --adam-betas "(0.9, 0.98)" --weight-decay 0.01 --clip-norm 0.0 --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 --tokens-per-sample 512 --sample-break-mode none --max-tokens 512 --update-freq 128 --seed 66575612 --fp16 --no-epoch-checkpoints --max-update 50000
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   149858.59 sec.
    Max Memory :                                 6893 MB
    Average Memory :                             3649.53 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               13107.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                15
    Run time :                                   149938 sec.
    Turnaround time :                            149963 sec.

The output (if any) follows:

2022-03-06 12:41:52 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 66575612, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 512, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 512, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [128], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': 'relu', 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103-raw-size-0.125', 'sample_break_mode': 'none', 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': 'none', 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 66575612, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.04, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2022-03-06 12:41:52 | INFO | fairseq.tasks.language_modeling | dictionary: 201328 types
2022-03-06 12:41:55 | INFO | fairseq_cli.train | TransformerLanguageModel(
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(201328, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=201328, bias=False)
  )
)
2022-03-06 12:41:55 | INFO | fairseq_cli.train | task: LanguageModelingTask
2022-03-06 12:41:55 | INFO | fairseq_cli.train | model: TransformerLanguageModel
2022-03-06 12:41:55 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2022-03-06 12:41:55 | INFO | fairseq_cli.train | num. shared model params: 121,994,240 (num. trained: 121,994,240)
2022-03-06 12:41:55 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2022-03-06 12:41:55 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103-raw-size-0.125/valid
2022-03-06 12:41:57 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2022-03-06 12:41:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 12:41:57 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 23.653 GB ; name = Quadro RTX 6000                         
2022-03-06 12:41:57 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2022-03-06 12:41:57 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2022-03-06 12:41:57 | INFO | fairseq_cli.train | max tokens per device = 512 and max sentences per device = None
2022-03-06 12:41:57 | INFO | fairseq.trainer | Preparing to load checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 12:41:57 | INFO | fairseq.trainer | No existing checkpoint found /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 12:41:57 | INFO | fairseq.trainer | loading train data for epoch 1
2022-03-06 12:41:57 | INFO | fairseq.data.data_utils | loaded 225,169 examples from: data-bin/wikitext-103-raw-size-0.125/train
2022-03-06 12:41:57 | INFO | fairseq.trainer | begin training epoch 1
2022-03-06 12:41:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 12:42:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2022-03-06 12:42:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 12:42:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 12:42:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 12:42:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2022-03-06 12:47:47 | INFO | train_inner | epoch 001:    105 / 196 loss=16.487, nll_loss=16.408, ppl=86944.4, wps=21018.4, ups=0.32, wpb=65536, bsz=128, num_updates=100, lr=1.25975e-05, gnorm=3.476, loss_scale=4, train_wall=327, gb_free=19.9, wall=350
2022-03-06 12:52:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 12:52:36 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 13.225 | nll_loss 13.007 | ppl 8232.37 | wps 40889.4 | wpb 510.9 | bsz 1 | num_updates 191
2022-03-06 12:52:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 191 updates
2022-03-06 12:52:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 12:52:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 12:52:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 1 @ 191 updates, score 13.225) (writing took 6.561657180078328 seconds)
2022-03-06 12:52:42 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2022-03-06 12:52:42 | INFO | train | epoch 001 | loss 15.418 | nll_loss 15.295 | ppl 40206.9 | wps 20596.2 | ups 0.31 | wpb 65445.7 | bsz 127.8 | num_updates 191 | lr 2.39702e-05 | gnorm 2.518 | loss_scale 8 | train_wall 590 | gb_free 19.9 | wall 645
2022-03-06 12:52:42 | INFO | fairseq.trainer | begin training epoch 2
2022-03-06 12:52:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 12:53:10 | INFO | train_inner | epoch 002:      9 / 196 loss=14.154, nll_loss=13.978, ppl=16134.2, wps=20224.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=200, lr=2.5095e-05, gnorm=1.441, loss_scale=8, train_wall=289, gb_free=19.9, wall=673
2022-03-06 12:58:22 | INFO | train_inner | epoch 002:    109 / 196 loss=12.279, nll_loss=12.013, ppl=4133.37, wps=21010.2, ups=0.32, wpb=65536, bsz=128, num_updates=300, lr=3.75925e-05, gnorm=0.941, loss_scale=16, train_wall=290, gb_free=19.9, wall=985
2022-03-06 13:02:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:02:58 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 10.67 | nll_loss 10.286 | ppl 1248.47 | wps 41211.4 | wpb 510.9 | bsz 1 | num_updates 387 | best_loss 10.67
2022-03-06 13:02:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 387 updates
2022-03-06 13:02:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 13:03:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 13:03:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 2 @ 387 updates, score 10.67) (writing took 6.688911632169038 seconds)
2022-03-06 13:03:05 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2022-03-06 13:03:05 | INFO | train | epoch 002 | loss 11.782 | nll_loss 11.482 | ppl 2860.43 | wps 20599 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 387 | lr 4.84653e-05 | gnorm 0.791 | loss_scale 32 | train_wall 567 | gb_free 19.9 | wall 1268
2022-03-06 13:03:05 | INFO | fairseq.trainer | begin training epoch 3
2022-03-06 13:03:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:03:46 | INFO | train_inner | epoch 003:     13 / 196 loss=11.007, nll_loss=10.655, ppl=1612.01, wps=20213.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=400, lr=5.009e-05, gnorm=0.559, loss_scale=32, train_wall=289, gb_free=19.9, wall=1309
2022-03-06 13:08:58 | INFO | train_inner | epoch 003:    113 / 196 loss=10.469, nll_loss=10.06, ppl=1067.65, wps=21013.6, ups=0.32, wpb=65536, bsz=128, num_updates=500, lr=6.25875e-05, gnorm=0.508, loss_scale=32, train_wall=290, gb_free=19.9, wall=1620
2022-03-06 13:09:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:13:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:13:21 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 10.011 | nll_loss 9.562 | ppl 755.74 | wps 40704.9 | wpb 510.9 | bsz 1 | num_updates 582 | best_loss 10.011
2022-03-06 13:13:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 582 updates
2022-03-06 13:13:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 13:13:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 13:13:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 3 @ 582 updates, score 10.011) (writing took 6.597338502062485 seconds)
2022-03-06 13:13:28 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2022-03-06 13:13:28 | INFO | train | epoch 003 | loss 10.367 | nll_loss 9.948 | ppl 988.01 | wps 20495.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 582 | lr 7.28355e-05 | gnorm 0.521 | loss_scale 32 | train_wall 567 | gb_free 19.9 | wall 1891
2022-03-06 13:13:28 | INFO | fairseq.trainer | begin training epoch 4
2022-03-06 13:13:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:14:24 | INFO | train_inner | epoch 004:     18 / 196 loss=10.158, nll_loss=9.72, ppl=843.6, wps=20025.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=600, lr=7.5085e-05, gnorm=0.565, loss_scale=32, train_wall=292, gb_free=19.9, wall=1947
2022-03-06 13:15:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 13:19:39 | INFO | train_inner | epoch 004:    119 / 196 loss=9.87, nll_loss=9.414, ppl=681.95, wps=20806.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=700, lr=8.75825e-05, gnorm=0.679, loss_scale=16, train_wall=293, gb_free=19.9, wall=2262
2022-03-06 13:23:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:23:44 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 9.553 | nll_loss 9.071 | ppl 537.86 | wps 40845.6 | wpb 510.9 | bsz 1 | num_updates 777 | best_loss 9.553
2022-03-06 13:23:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 777 updates
2022-03-06 13:23:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 13:23:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 13:23:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 4 @ 777 updates, score 9.553) (writing took 6.6103842640295625 seconds)
2022-03-06 13:23:50 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2022-03-06 13:23:50 | INFO | train | epoch 004 | loss 9.805 | nll_loss 9.344 | ppl 649.68 | wps 20495.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 777 | lr 9.72056e-05 | gnorm 0.7 | loss_scale 32 | train_wall 567 | gb_free 19.9 | wall 2513
2022-03-06 13:23:50 | INFO | fairseq.trainer | begin training epoch 5
2022-03-06 13:23:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:25:02 | INFO | train_inner | epoch 005:     23 / 196 loss=9.634, nll_loss=9.161, ppl=572.61, wps=20220, ups=0.31, wpb=65367, bsz=127.7, num_updates=800, lr=0.00010008, gnorm=0.713, loss_scale=32, train_wall=289, gb_free=19.9, wall=2585
2022-03-06 13:28:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:30:17 | INFO | train_inner | epoch 005:    124 / 196 loss=9.411, nll_loss=8.924, ppl=485.59, wps=20789.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=900, lr=0.000112578, gnorm=0.817, loss_scale=32, train_wall=293, gb_free=19.9, wall=2900
2022-03-06 13:34:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:34:07 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 9.174 | nll_loss 8.673 | ppl 408.18 | wps 40741.9 | wpb 510.9 | bsz 1 | num_updates 972 | best_loss 9.174
2022-03-06 13:34:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 972 updates
2022-03-06 13:34:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 13:34:10 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 13:34:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 5 @ 972 updates, score 9.174) (writing took 6.617863426916301 seconds)
2022-03-06 13:34:14 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2022-03-06 13:34:14 | INFO | train | epoch 005 | loss 9.361 | nll_loss 8.871 | ppl 468.09 | wps 20482.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 972 | lr 0.000121576 | gnorm 0.806 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 3136
2022-03-06 13:34:14 | INFO | fairseq.trainer | begin training epoch 6
2022-03-06 13:34:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:35:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:35:44 | INFO | train_inner | epoch 006:     29 / 196 loss=9.21, nll_loss=8.71, ppl=418.76, wps=20017.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=1000, lr=0.000125075, gnorm=0.847, loss_scale=32, train_wall=292, gb_free=19.9, wall=3227
2022-03-06 13:40:56 | INFO | train_inner | epoch 006:    129 / 196 loss=9.02, nll_loss=8.507, ppl=363.92, wps=20995, ups=0.32, wpb=65536, bsz=128, num_updates=1100, lr=0.000137573, gnorm=0.879, loss_scale=32, train_wall=290, gb_free=19.9, wall=3539
2022-03-06 13:41:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 13:44:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:44:30 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.848 | nll_loss 8.321 | ppl 319.76 | wps 40916.9 | wpb 510.9 | bsz 1 | num_updates 1166 | best_loss 8.848
2022-03-06 13:44:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1166 updates
2022-03-06 13:44:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 13:44:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 13:44:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 6 @ 1166 updates, score 8.848) (writing took 6.711347765056416 seconds)
2022-03-06 13:44:37 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2022-03-06 13:44:37 | INFO | train | epoch 006 | loss 8.993 | nll_loss 8.479 | ppl 356.85 | wps 20376.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1166 | lr 0.000145821 | gnorm 0.882 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 3759
2022-03-06 13:44:37 | INFO | fairseq.trainer | begin training epoch 7
2022-03-06 13:44:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:46:23 | INFO | train_inner | epoch 007:     34 / 196 loss=8.858, nll_loss=8.335, ppl=322.97, wps=20016.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=1200, lr=0.00015007, gnorm=0.897, loss_scale=16, train_wall=292, gb_free=19.9, wall=3866
2022-03-06 13:51:35 | INFO | train_inner | epoch 007:    134 / 196 loss=8.701, nll_loss=8.169, ppl=287.81, wps=20997.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=1300, lr=0.000162568, gnorm=0.896, loss_scale=32, train_wall=290, gb_free=19.9, wall=4178
2022-03-06 13:54:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 13:54:53 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 8.597 | nll_loss 8.053 | ppl 265.61 | wps 40763.8 | wpb 510.9 | bsz 1 | num_updates 1362 | best_loss 8.597
2022-03-06 13:54:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1362 updates
2022-03-06 13:54:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 13:54:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 13:55:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 7 @ 1362 updates, score 8.597) (writing took 6.661169999046251 seconds)
2022-03-06 13:55:00 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2022-03-06 13:55:00 | INFO | train | epoch 007 | loss 8.686 | nll_loss 8.152 | ppl 284.53 | wps 20587.3 | ups 0.31 | wpb 65448 | bsz 127.8 | num_updates 1362 | lr 0.000170316 | gnorm 0.9 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 4383
2022-03-06 13:55:00 | INFO | fairseq.trainer | begin training epoch 8
2022-03-06 13:55:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 13:56:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 13:57:01 | INFO | train_inner | epoch 008:     39 / 196 loss=8.557, nll_loss=8.016, ppl=258.9, wps=20012.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=1400, lr=0.000175065, gnorm=0.899, loss_scale=32, train_wall=292, gb_free=19.9, wall=4504
2022-03-06 14:02:14 | INFO | train_inner | epoch 008:    139 / 196 loss=8.418, nll_loss=7.868, ppl=233.6, wps=20993.7, ups=0.32, wpb=65536, bsz=128, num_updates=1500, lr=0.000187563, gnorm=0.917, loss_scale=32, train_wall=290, gb_free=19.9, wall=4816
2022-03-06 14:03:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:05:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:05:16 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 8.365 | nll_loss 7.802 | ppl 223.22 | wps 40061.9 | wpb 510.9 | bsz 1 | num_updates 1556 | best_loss 8.365
2022-03-06 14:05:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1556 updates
2022-03-06 14:05:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 14:05:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 14:05:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 8 @ 1556 updates, score 8.365) (writing took 6.6908015590161085 seconds)
2022-03-06 14:05:23 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2022-03-06 14:05:23 | INFO | train | epoch 008 | loss 8.409 | nll_loss 7.858 | ppl 232.05 | wps 20372.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1556 | lr 0.000194561 | gnorm 0.915 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 5006
2022-03-06 14:05:23 | INFO | fairseq.trainer | begin training epoch 9
2022-03-06 14:05:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:07:40 | INFO | train_inner | epoch 009:     44 / 196 loss=8.288, nll_loss=7.73, ppl=212.37, wps=20004.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=1600, lr=0.00020006, gnorm=0.946, loss_scale=32, train_wall=292, gb_free=19.9, wall=5143
2022-03-06 14:10:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:12:56 | INFO | train_inner | epoch 009:    145 / 196 loss=8.152, nll_loss=7.586, ppl=192.11, wps=20781.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=1700, lr=0.000212558, gnorm=0.893, loss_scale=32, train_wall=293, gb_free=19.9, wall=5459
2022-03-06 14:15:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:15:40 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 8.169 | nll_loss 7.596 | ppl 193.49 | wps 40942.1 | wpb 510.9 | bsz 1 | num_updates 1751 | best_loss 8.169
2022-03-06 14:15:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1751 updates
2022-03-06 14:15:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 14:15:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 14:15:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 9 @ 1751 updates, score 8.169) (writing took 6.722687558969483 seconds)
2022-03-06 14:15:46 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2022-03-06 14:15:46 | INFO | train | epoch 009 | loss 8.15 | nll_loss 7.583 | ppl 191.79 | wps 20473.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 1751 | lr 0.000218931 | gnorm 0.921 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 5629
2022-03-06 14:15:46 | INFO | fairseq.trainer | begin training epoch 10
2022-03-06 14:15:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:17:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:18:23 | INFO | train_inner | epoch 010:     50 / 196 loss=8.026, nll_loss=7.452, ppl=175.12, wps=20003.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=1800, lr=0.000225055, gnorm=0.925, loss_scale=32, train_wall=292, gb_free=19.9, wall=5785
2022-03-06 14:23:35 | INFO | train_inner | epoch 010:    150 / 196 loss=7.903, nll_loss=7.321, ppl=159.92, wps=20984.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=1900, lr=0.000237553, gnorm=0.895, loss_scale=32, train_wall=290, gb_free=19.9, wall=6098
2022-03-06 14:23:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:25:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:26:03 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.988 | nll_loss 7.398 | ppl 168.65 | wps 40671.7 | wpb 510.9 | bsz 1 | num_updates 1945 | best_loss 7.988
2022-03-06 14:26:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1945 updates
2022-03-06 14:26:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 14:26:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 14:26:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 10 @ 1945 updates, score 7.988) (writing took 6.652435516938567 seconds)
2022-03-06 14:26:10 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2022-03-06 14:26:10 | INFO | train | epoch 010 | loss 7.907 | nll_loss 7.325 | ppl 160.37 | wps 20366.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 1945 | lr 0.000243176 | gnorm 0.899 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 6253
2022-03-06 14:26:10 | INFO | fairseq.trainer | begin training epoch 11
2022-03-06 14:26:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:29:02 | INFO | train_inner | epoch 011:     55 / 196 loss=7.783, nll_loss=7.194, ppl=146.38, wps=20006, ups=0.31, wpb=65367, bsz=127.7, num_updates=2000, lr=0.00025005, gnorm=0.903, loss_scale=32, train_wall=292, gb_free=19.9, wall=6424
2022-03-06 14:31:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:34:17 | INFO | train_inner | epoch 011:    156 / 196 loss=7.68, nll_loss=7.083, ppl=135.58, wps=20780.4, ups=0.32, wpb=65536, bsz=128, num_updates=2100, lr=0.000262548, gnorm=0.865, loss_scale=32, train_wall=293, gb_free=19.9, wall=6740
2022-03-06 14:36:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:36:27 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 7.859 | nll_loss 7.261 | ppl 153.33 | wps 40462.7 | wpb 510.9 | bsz 1 | num_updates 2140 | best_loss 7.859
2022-03-06 14:36:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 2140 updates
2022-03-06 14:36:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 14:36:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 14:36:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 11 @ 2140 updates, score 7.859) (writing took 6.60700044198893 seconds)
2022-03-06 14:36:33 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2022-03-06 14:36:33 | INFO | train | epoch 011 | loss 7.682 | nll_loss 7.086 | ppl 135.82 | wps 20470.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2140 | lr 0.000267547 | gnorm 0.876 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 6876
2022-03-06 14:36:33 | INFO | fairseq.trainer | begin training epoch 12
2022-03-06 14:36:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:38:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:39:44 | INFO | train_inner | epoch 012:     61 / 196 loss=7.555, nll_loss=6.95, ppl=123.66, wps=20008.8, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=2200, lr=0.000275045, gnorm=0.868, loss_scale=32, train_wall=292, gb_free=19.9, wall=7066
2022-03-06 14:44:56 | INFO | train_inner | epoch 012:    161 / 196 loss=7.465, nll_loss=6.854, ppl=115.66, wps=20998.5, ups=0.32, wpb=65536, bsz=128, num_updates=2300, lr=0.000287543, gnorm=0.867, loss_scale=32, train_wall=290, gb_free=19.9, wall=7378
2022-03-06 14:45:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:46:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:46:50 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 7.732 | nll_loss 7.134 | ppl 140.45 | wps 40311.4 | wpb 510.9 | bsz 1 | num_updates 2334 | best_loss 7.732
2022-03-06 14:46:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2334 updates
2022-03-06 14:46:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 14:46:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 14:46:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 12 @ 2334 updates, score 7.732) (writing took 6.579607235034928 seconds)
2022-03-06 14:46:56 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2022-03-06 14:46:56 | INFO | train | epoch 012 | loss 7.472 | nll_loss 6.862 | ppl 116.33 | wps 20379.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 2334 | lr 0.000291792 | gnorm 0.86 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 7499
2022-03-06 14:46:56 | INFO | fairseq.trainer | begin training epoch 13
2022-03-06 14:46:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 14:50:22 | INFO | train_inner | epoch 013:     66 / 196 loss=7.344, nll_loss=6.726, ppl=105.85, wps=20010.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=2400, lr=0.00030004, gnorm=0.854, loss_scale=32, train_wall=292, gb_free=19.9, wall=7705
2022-03-06 14:53:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 14:55:38 | INFO | train_inner | epoch 013:    167 / 196 loss=7.278, nll_loss=6.654, ppl=100.73, wps=20792.2, ups=0.32, wpb=65536, bsz=128, num_updates=2500, lr=0.000312538, gnorm=0.854, loss_scale=32, train_wall=293, gb_free=19.9, wall=8020
2022-03-06 14:57:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 14:57:13 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 7.624 | nll_loss 7.005 | ppl 128.44 | wps 39194.1 | wpb 510.9 | bsz 1 | num_updates 2529 | best_loss 7.624
2022-03-06 14:57:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2529 updates
2022-03-06 14:57:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 14:57:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 14:57:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 13 @ 2529 updates, score 7.624) (writing took 6.595395464915782 seconds)
2022-03-06 14:57:19 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2022-03-06 14:57:19 | INFO | train | epoch 013 | loss 7.28 | nll_loss 6.657 | ppl 100.93 | wps 20475.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2529 | lr 0.000316162 | gnorm 0.852 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 8122
2022-03-06 14:57:20 | INFO | fairseq.trainer | begin training epoch 14
2022-03-06 14:57:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:00:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:01:04 | INFO | train_inner | epoch 014:     72 / 196 loss=7.153, nll_loss=6.522, ppl=91.87, wps=20000.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=2600, lr=0.000325035, gnorm=0.835, loss_scale=32, train_wall=292, gb_free=19.9, wall=8347
2022-03-06 15:06:16 | INFO | train_inner | epoch 014:    172 / 196 loss=7.093, nll_loss=6.457, ppl=87.85, wps=20997.5, ups=0.32, wpb=65536, bsz=128, num_updates=2700, lr=0.000337533, gnorm=0.817, loss_scale=32, train_wall=290, gb_free=19.9, wall=8659
2022-03-06 15:07:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:07:36 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.53 | nll_loss 6.9 | ppl 119.42 | wps 39214.4 | wpb 510.9 | bsz 1 | num_updates 2724 | best_loss 7.53
2022-03-06 15:07:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2724 updates
2022-03-06 15:07:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 15:07:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 15:07:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 14 @ 2724 updates, score 7.53) (writing took 6.835762071888894 seconds)
2022-03-06 15:07:43 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2022-03-06 15:07:43 | INFO | train | epoch 014 | loss 7.102 | nll_loss 6.468 | ppl 88.49 | wps 20467.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 2724 | lr 0.000340532 | gnorm 0.831 | loss_scale 64 | train_wall 568 | gb_free 19.9 | wall 8746
2022-03-06 15:07:43 | INFO | fairseq.trainer | begin training epoch 15
2022-03-06 15:07:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:08:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:11:43 | INFO | train_inner | epoch 015:     77 / 196 loss=6.983, nll_loss=6.34, ppl=81.02, wps=19986.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=2800, lr=0.00035003, gnorm=0.841, loss_scale=32, train_wall=292, gb_free=19.9, wall=8986
2022-03-06 15:15:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:16:59 | INFO | train_inner | epoch 015:    178 / 196 loss=6.935, nll_loss=6.288, ppl=78.13, wps=20791, ups=0.32, wpb=65532.4, bsz=128, num_updates=2900, lr=0.000362528, gnorm=0.819, loss_scale=32, train_wall=293, gb_free=19.9, wall=9302
2022-03-06 15:17:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:18:00 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.483 | nll_loss 6.853 | ppl 115.62 | wps 39698.8 | wpb 510.9 | bsz 1 | num_updates 2918 | best_loss 7.483
2022-03-06 15:18:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2918 updates
2022-03-06 15:18:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 15:18:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 15:18:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 15 @ 2918 updates, score 7.483) (writing took 6.846097016008571 seconds)
2022-03-06 15:18:07 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2022-03-06 15:18:07 | INFO | train | epoch 015 | loss 6.938 | nll_loss 6.292 | ppl 78.38 | wps 20364.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 2918 | lr 0.000364777 | gnorm 0.824 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 9369
2022-03-06 15:18:07 | INFO | fairseq.trainer | begin training epoch 16
2022-03-06 15:18:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:19:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:22:26 | INFO | train_inner | epoch 016:     83 / 196 loss=6.805, nll_loss=6.15, ppl=71.02, wps=19997.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=3000, lr=0.000375025, gnorm=0.798, loss_scale=16, train_wall=292, gb_free=19.9, wall=9628
2022-03-06 15:27:38 | INFO | train_inner | epoch 016:    183 / 196 loss=6.793, nll_loss=6.137, ppl=70.36, wps=21006.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=3100, lr=0.000387523, gnorm=0.819, loss_scale=32, train_wall=290, gb_free=19.9, wall=9940
2022-03-06 15:28:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:28:23 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.44 | nll_loss 6.798 | ppl 111.27 | wps 40599.1 | wpb 510.9 | bsz 1 | num_updates 3113 | best_loss 7.44
2022-03-06 15:28:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 3113 updates
2022-03-06 15:28:23 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 15:28:26 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 15:28:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 16 @ 3113 updates, score 7.44) (writing took 6.991965966997668 seconds)
2022-03-06 15:28:30 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2022-03-06 15:28:30 | INFO | train | epoch 016 | loss 6.785 | nll_loss 6.129 | ppl 69.99 | wps 20478.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3113 | lr 0.000389147 | gnorm 0.813 | loss_scale 32 | train_wall 567 | gb_free 19.9 | wall 9993
2022-03-06 15:28:30 | INFO | fairseq.trainer | begin training epoch 17
2022-03-06 15:28:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:32:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:33:05 | INFO | train_inner | epoch 017:     88 / 196 loss=6.651, nll_loss=5.986, ppl=63.37, wps=19988.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3200, lr=0.00040002, gnorm=0.819, loss_scale=32, train_wall=292, gb_free=19.9, wall=10267
2022-03-06 15:38:17 | INFO | train_inner | epoch 017:    188 / 196 loss=6.651, nll_loss=5.984, ppl=63.31, wps=21001.9, ups=0.32, wpb=65536, bsz=128, num_updates=3300, lr=0.000412518, gnorm=0.813, loss_scale=32, train_wall=290, gb_free=19.9, wall=10579
2022-03-06 15:38:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:38:46 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.405 | nll_loss 6.763 | ppl 108.58 | wps 40556.4 | wpb 510.9 | bsz 1 | num_updates 3308 | best_loss 7.405
2022-03-06 15:38:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 3308 updates
2022-03-06 15:38:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 15:38:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 15:38:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 17 @ 3308 updates, score 7.405) (writing took 7.057555231032893 seconds)
2022-03-06 15:38:53 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2022-03-06 15:38:53 | INFO | train | epoch 017 | loss 6.643 | nll_loss 5.977 | ppl 62.99 | wps 20465.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3308 | lr 0.000413517 | gnorm 0.812 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 10616
2022-03-06 15:38:53 | INFO | fairseq.trainer | begin training epoch 18
2022-03-06 15:38:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:39:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2022-03-06 15:39:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:43:47 | INFO | train_inner | epoch 018:     94 / 196 loss=6.508, nll_loss=5.833, ppl=56.99, wps=19800.5, ups=0.3, wpb=65363.4, bsz=127.7, num_updates=3400, lr=0.000425015, gnorm=0.814, loss_scale=16, train_wall=295, gb_free=19.9, wall=10910
2022-03-06 15:48:59 | INFO | train_inner | epoch 018:    194 / 196 loss=6.525, nll_loss=5.85, ppl=57.66, wps=21001.7, ups=0.32, wpb=65536, bsz=128, num_updates=3500, lr=0.000437513, gnorm=0.796, loss_scale=32, train_wall=290, gb_free=19.9, wall=11222
2022-03-06 15:49:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:49:10 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.404 | nll_loss 6.762 | ppl 108.53 | wps 40610.8 | wpb 510.9 | bsz 1 | num_updates 3502 | best_loss 7.404
2022-03-06 15:49:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3502 updates
2022-03-06 15:49:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 15:49:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 15:49:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 18 @ 3502 updates, score 7.404) (writing took 7.008296397980303 seconds)
2022-03-06 15:49:17 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2022-03-06 15:49:17 | INFO | train | epoch 018 | loss 6.509 | nll_loss 5.834 | ppl 57.03 | wps 20369.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 3502 | lr 0.000437762 | gnorm 0.805 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 11239
2022-03-06 15:49:17 | INFO | fairseq.trainer | begin training epoch 19
2022-03-06 15:49:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:50:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 15:54:26 | INFO | train_inner | epoch 019:     99 / 196 loss=6.365, nll_loss=5.68, ppl=51.27, wps=20003.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3600, lr=0.00045001, gnorm=0.796, loss_scale=16, train_wall=292, gb_free=19.9, wall=11548
2022-03-06 15:59:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 15:59:33 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.384 | nll_loss 6.744 | ppl 107.21 | wps 40557.4 | wpb 510.9 | bsz 1 | num_updates 3697 | best_loss 7.384
2022-03-06 15:59:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3697 updates
2022-03-06 15:59:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 15:59:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt
2022-03-06 15:59:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_best.pt (epoch 19 @ 3697 updates, score 7.384) (writing took 7.007212332915515 seconds)
2022-03-06 15:59:40 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2022-03-06 15:59:40 | INFO | train | epoch 019 | loss 6.383 | nll_loss 5.698 | ppl 51.92 | wps 20474.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 3697 | lr 0.000462133 | gnorm 0.796 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 11863
2022-03-06 15:59:40 | INFO | fairseq.trainer | begin training epoch 20
2022-03-06 15:59:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 15:59:49 | INFO | train_inner | epoch 020:      3 / 196 loss=6.397, nll_loss=5.712, ppl=52.43, wps=20180, ups=0.31, wpb=65367, bsz=127.7, num_updates=3700, lr=0.000462508, gnorm=0.794, loss_scale=32, train_wall=289, gb_free=19.9, wall=11872
2022-03-06 16:01:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:05:05 | INFO | train_inner | epoch 020:    104 / 196 loss=6.236, nll_loss=5.542, ppl=46.59, wps=20764.5, ups=0.32, wpb=65536, bsz=128, num_updates=3800, lr=0.000475005, gnorm=0.812, loss_scale=16, train_wall=293, gb_free=19.9, wall=12188
2022-03-06 16:09:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:09:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:09:57 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.436 | nll_loss 6.799 | ppl 111.39 | wps 40562.2 | wpb 510.9 | bsz 1 | num_updates 3891 | best_loss 7.384
2022-03-06 16:09:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3891 updates
2022-03-06 16:09:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 16:10:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 16:10:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 20 @ 3891 updates, score 7.436) (writing took 3.4792768170591444 seconds)
2022-03-06 16:10:00 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2022-03-06 16:10:00 | INFO | train | epoch 020 | loss 6.264 | nll_loss 5.57 | ppl 47.52 | wps 20465.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 3891 | lr 0.000486378 | gnorm 0.801 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 12483
2022-03-06 16:10:00 | INFO | fairseq.trainer | begin training epoch 21
2022-03-06 16:10:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:10:29 | INFO | train_inner | epoch 021:      9 / 196 loss=6.279, nll_loss=5.586, ppl=48.03, wps=20202.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=3900, lr=0.000487503, gnorm=0.802, loss_scale=16, train_wall=292, gb_free=19.9, wall=12511
2022-03-06 16:15:40 | INFO | train_inner | epoch 021:    109 / 196 loss=6.123, nll_loss=5.42, ppl=42.81, wps=21016.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=4000, lr=0.0005, gnorm=0.81, loss_scale=16, train_wall=290, gb_free=19.9, wall=12823
2022-03-06 16:17:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:20:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:20:16 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.418 | nll_loss 6.763 | ppl 108.61 | wps 40609.4 | wpb 510.9 | bsz 1 | num_updates 4086 | best_loss 7.384
2022-03-06 16:20:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 4086 updates
2022-03-06 16:20:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 16:20:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 16:20:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 21 @ 4086 updates, score 7.418) (writing took 3.459617992863059 seconds)
2022-03-06 16:20:20 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2022-03-06 16:20:20 | INFO | train | epoch 021 | loss 6.15 | nll_loss 5.448 | ppl 43.67 | wps 20597.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4086 | lr 0.00049471 | gnorm 0.801 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 13103
2022-03-06 16:20:20 | INFO | fairseq.trainer | begin training epoch 22
2022-03-06 16:20:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:21:04 | INFO | train_inner | epoch 022:     14 / 196 loss=6.151, nll_loss=5.45, ppl=43.7, wps=20213.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=4100, lr=0.000493865, gnorm=0.769, loss_scale=16, train_wall=292, gb_free=19.9, wall=13147
2022-03-06 16:24:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:26:19 | INFO | train_inner | epoch 022:    115 / 196 loss=6.01, nll_loss=5.299, ppl=39.36, wps=20800.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=4200, lr=0.00048795, gnorm=0.771, loss_scale=16, train_wall=293, gb_free=19.9, wall=13462
2022-03-06 16:30:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:30:36 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.449 | nll_loss 6.782 | ppl 110.06 | wps 40922.7 | wpb 510.9 | bsz 1 | num_updates 4281 | best_loss 7.384
2022-03-06 16:30:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 4281 updates
2022-03-06 16:30:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 16:30:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 16:30:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 22 @ 4281 updates, score 7.449) (writing took 3.55223463405855 seconds)
2022-03-06 16:30:40 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2022-03-06 16:30:40 | INFO | train | epoch 022 | loss 6.026 | nll_loss 5.316 | ppl 39.84 | wps 20593.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4281 | lr 0.000483312 | gnorm 0.768 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 13723
2022-03-06 16:30:40 | INFO | fairseq.trainer | begin training epoch 23
2022-03-06 16:30:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:31:39 | INFO | train_inner | epoch 023:     19 / 196 loss=6.022, nll_loss=5.311, ppl=39.7, wps=20409.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=4300, lr=0.000482243, gnorm=0.768, loss_scale=32, train_wall=289, gb_free=19.9, wall=13782
2022-03-06 16:32:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:36:54 | INFO | train_inner | epoch 023:    120 / 196 loss=5.89, nll_loss=5.171, ppl=36.02, wps=20798.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=4400, lr=0.000476731, gnorm=0.756, loss_scale=16, train_wall=293, gb_free=19.9, wall=14097
2022-03-06 16:39:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:40:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:40:56 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.465 | nll_loss 6.806 | ppl 111.89 | wps 40686.3 | wpb 510.9 | bsz 1 | num_updates 4475 | best_loss 7.384
2022-03-06 16:40:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 4475 updates
2022-03-06 16:40:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 16:40:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 16:40:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 23 @ 4475 updates, score 7.465) (writing took 3.339120116084814 seconds)
2022-03-06 16:40:59 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2022-03-06 16:40:59 | INFO | train | epoch 023 | loss 5.91 | nll_loss 5.191 | ppl 36.53 | wps 20488.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 4475 | lr 0.000472719 | gnorm 0.763 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 14342
2022-03-06 16:40:59 | INFO | fairseq.trainer | begin training epoch 24
2022-03-06 16:40:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:42:18 | INFO | train_inner | epoch 024:     25 / 196 loss=5.89, nll_loss=5.169, ppl=35.98, wps=20210.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=4500, lr=0.000471405, gnorm=0.747, loss_scale=16, train_wall=292, gb_free=19.9, wall=14420
2022-03-06 16:47:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:47:33 | INFO | train_inner | epoch 024:    126 / 196 loss=5.787, nll_loss=5.06, ppl=33.36, wps=20788.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=4600, lr=0.000466252, gnorm=0.759, loss_scale=16, train_wall=293, gb_free=19.9, wall=14736
2022-03-06 16:51:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 16:51:16 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.454 | nll_loss 6.788 | ppl 110.5 | wps 40675.2 | wpb 510.9 | bsz 1 | num_updates 4670 | best_loss 7.384
2022-03-06 16:51:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4670 updates
2022-03-06 16:51:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 16:51:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 16:51:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 24 @ 4670 updates, score 7.454) (writing took 3.471615252783522 seconds)
2022-03-06 16:51:19 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2022-03-06 16:51:19 | INFO | train | epoch 024 | loss 5.801 | nll_loss 5.074 | ppl 33.7 | wps 20580.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4670 | lr 0.000462745 | gnorm 0.748 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 14962
2022-03-06 16:51:20 | INFO | fairseq.trainer | begin training epoch 25
2022-03-06 16:51:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 16:52:53 | INFO | train_inner | epoch 025:     30 / 196 loss=5.786, nll_loss=5.058, ppl=33.31, wps=20402.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=4700, lr=0.000461266, gnorm=0.741, loss_scale=16, train_wall=289, gb_free=19.9, wall=15056
2022-03-06 16:55:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 16:58:08 | INFO | train_inner | epoch 025:    131 / 196 loss=5.684, nll_loss=4.949, ppl=30.89, wps=20802.9, ups=0.32, wpb=65536, bsz=128, num_updates=4800, lr=0.000456435, gnorm=0.747, loss_scale=16, train_wall=293, gb_free=19.9, wall=15371
2022-03-06 17:01:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:01:36 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.504 | nll_loss 6.838 | ppl 114.37 | wps 40695.5 | wpb 510.9 | bsz 1 | num_updates 4865 | best_loss 7.384
2022-03-06 17:01:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4865 updates
2022-03-06 17:01:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 17:01:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 17:01:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 25 @ 4865 updates, score 7.504) (writing took 3.767308478942141 seconds)
2022-03-06 17:01:39 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2022-03-06 17:01:39 | INFO | train | epoch 025 | loss 5.699 | nll_loss 4.965 | ppl 31.24 | wps 20587.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 4865 | lr 0.000453376 | gnorm 0.735 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 15582
2022-03-06 17:01:39 | INFO | fairseq.trainer | begin training epoch 26
2022-03-06 17:01:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:02:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:03:32 | INFO | train_inner | epoch 026:     36 / 196 loss=5.677, nll_loss=4.941, ppl=30.73, wps=20201.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=4900, lr=0.000451754, gnorm=0.735, loss_scale=16, train_wall=292, gb_free=19.9, wall=15695
2022-03-06 17:08:44 | INFO | train_inner | epoch 026:    136 / 196 loss=5.596, nll_loss=4.854, ppl=28.93, wps=20978.8, ups=0.32, wpb=65536, bsz=128, num_updates=5000, lr=0.000447214, gnorm=0.754, loss_scale=16, train_wall=290, gb_free=19.9, wall=16007
2022-03-06 17:11:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:11:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:11:56 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.558 | nll_loss 6.892 | ppl 118.79 | wps 40695.1 | wpb 510.9 | bsz 1 | num_updates 5059 | best_loss 7.384
2022-03-06 17:11:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 5059 updates
2022-03-06 17:11:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 17:12:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 17:12:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 26 @ 5059 updates, score 7.558) (writing took 3.37923133908771 seconds)
2022-03-06 17:12:00 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2022-03-06 17:12:00 | INFO | train | epoch 026 | loss 5.601 | nll_loss 4.859 | ppl 29.03 | wps 20464.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 5059 | lr 0.000444598 | gnorm 0.748 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 16203
2022-03-06 17:12:00 | INFO | fairseq.trainer | begin training epoch 27
2022-03-06 17:12:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:14:08 | INFO | train_inner | epoch 027:     41 / 196 loss=5.562, nll_loss=4.818, ppl=28.2, wps=20194.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=5100, lr=0.000442807, gnorm=0.733, loss_scale=16, train_wall=292, gb_free=19.9, wall=16331
2022-03-06 17:18:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:19:23 | INFO | train_inner | epoch 027:    142 / 196 loss=5.513, nll_loss=4.764, ppl=27.18, wps=20804.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=5200, lr=0.000438529, gnorm=0.749, loss_scale=16, train_wall=293, gb_free=19.9, wall=16646
2022-03-06 17:22:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:22:16 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.587 | nll_loss 6.923 | ppl 121.34 | wps 40914.4 | wpb 510.9 | bsz 1 | num_updates 5254 | best_loss 7.384
2022-03-06 17:22:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 5254 updates
2022-03-06 17:22:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 17:22:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 17:22:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 27 @ 5254 updates, score 7.587) (writing took 3.479432824999094 seconds)
2022-03-06 17:22:20 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2022-03-06 17:22:20 | INFO | train | epoch 027 | loss 5.513 | nll_loss 4.764 | ppl 27.17 | wps 20594.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5254 | lr 0.00043627 | gnorm 0.741 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 16822
2022-03-06 17:22:20 | INFO | fairseq.trainer | begin training epoch 28
2022-03-06 17:22:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:24:43 | INFO | train_inner | epoch 028:     46 / 196 loss=5.471, nll_loss=4.719, ppl=26.34, wps=20404.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=5300, lr=0.000434372, gnorm=0.749, loss_scale=16, train_wall=289, gb_free=19.9, wall=16966
2022-03-06 17:25:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:29:58 | INFO | train_inner | epoch 028:    147 / 196 loss=5.421, nll_loss=4.666, ppl=25.38, wps=20799.8, ups=0.32, wpb=65536, bsz=128, num_updates=5400, lr=0.000430331, gnorm=0.749, loss_scale=16, train_wall=293, gb_free=19.9, wall=17281
2022-03-06 17:32:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:32:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:32:36 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.603 | nll_loss 6.932 | ppl 122.1 | wps 40970.8 | wpb 510.9 | bsz 1 | num_updates 5448 | best_loss 7.384
2022-03-06 17:32:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 5448 updates
2022-03-06 17:32:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 17:32:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 17:32:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 28 @ 5448 updates, score 7.603) (writing took 3.528859158977866 seconds)
2022-03-06 17:32:39 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2022-03-06 17:32:39 | INFO | train | epoch 028 | loss 5.425 | nll_loss 4.669 | ppl 25.45 | wps 20513.6 | ups 0.31 | wpb 65534.1 | bsz 128 | num_updates 5448 | lr 0.000428432 | gnorm 0.744 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 17442
2022-03-06 17:32:39 | INFO | fairseq.trainer | begin training epoch 29
2022-03-06 17:32:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:35:22 | INFO | train_inner | epoch 029:     52 / 196 loss=5.383, nll_loss=4.625, ppl=24.67, wps=20257.5, ups=0.31, wpb=65536, bsz=128, num_updates=5500, lr=0.000426401, gnorm=0.736, loss_scale=16, train_wall=292, gb_free=19.9, wall=17605
2022-03-06 17:39:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:40:37 | INFO | train_inner | epoch 029:    153 / 196 loss=5.35, nll_loss=4.588, ppl=24.05, wps=20790, ups=0.32, wpb=65532.4, bsz=128, num_updates=5600, lr=0.000422577, gnorm=0.77, loss_scale=16, train_wall=293, gb_free=19.9, wall=17920
2022-03-06 17:42:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:42:56 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.689 | nll_loss 7.014 | ppl 129.27 | wps 40739 | wpb 510.9 | bsz 1 | num_updates 5643 | best_loss 7.384
2022-03-06 17:42:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 5643 updates
2022-03-06 17:42:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 17:42:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 17:42:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 29 @ 5643 updates, score 7.689) (writing took 3.483122098026797 seconds)
2022-03-06 17:42:59 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2022-03-06 17:42:59 | INFO | train | epoch 029 | loss 5.345 | nll_loss 4.583 | ppl 23.97 | wps 20579.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5643 | lr 0.000420964 | gnorm 0.757 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 18062
2022-03-06 17:42:59 | INFO | fairseq.trainer | begin training epoch 30
2022-03-06 17:42:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:45:57 | INFO | train_inner | epoch 030:     57 / 196 loss=5.295, nll_loss=4.53, ppl=23.1, wps=20404.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=5700, lr=0.000418854, gnorm=0.735, loss_scale=16, train_wall=289, gb_free=19.9, wall=18240
2022-03-06 17:47:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:51:13 | INFO | train_inner | epoch 030:    158 / 196 loss=5.278, nll_loss=4.511, ppl=22.8, wps=20794.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=5800, lr=0.000415227, gnorm=0.762, loss_scale=16, train_wall=293, gb_free=19.9, wall=18555
2022-03-06 17:53:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 17:53:16 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.74 | nll_loss 7.079 | ppl 135.24 | wps 40876.9 | wpb 510.9 | bsz 1 | num_updates 5838 | best_loss 7.384
2022-03-06 17:53:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5838 updates
2022-03-06 17:53:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 17:53:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 17:53:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 30 @ 5838 updates, score 7.74) (writing took 3.4378401280846447 seconds)
2022-03-06 17:53:19 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2022-03-06 17:53:19 | INFO | train | epoch 030 | loss 5.268 | nll_loss 4.5 | ppl 22.63 | wps 20591.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 5838 | lr 0.000413874 | gnorm 0.75 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 18682
2022-03-06 17:53:19 | INFO | fairseq.trainer | begin training epoch 31
2022-03-06 17:53:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 17:54:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 17:56:36 | INFO | train_inner | epoch 031:     63 / 196 loss=5.204, nll_loss=4.432, ppl=21.59, wps=20218.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=5900, lr=0.000411693, gnorm=0.752, loss_scale=16, train_wall=292, gb_free=19.9, wall=18879
2022-03-06 18:00:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:01:51 | INFO | train_inner | epoch 031:    164 / 196 loss=5.214, nll_loss=4.441, ppl=21.73, wps=20789.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=6000, lr=0.000408248, gnorm=0.762, loss_scale=16, train_wall=293, gb_free=19.9, wall=19194
2022-03-06 18:03:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:03:36 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.781 | nll_loss 7.118 | ppl 138.87 | wps 40522.6 | wpb 510.9 | bsz 1 | num_updates 6032 | best_loss 7.384
2022-03-06 18:03:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 6032 updates
2022-03-06 18:03:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 18:03:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 18:03:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 31 @ 6032 updates, score 7.781) (writing took 3.767747909994796 seconds)
2022-03-06 18:03:40 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2022-03-06 18:03:40 | INFO | train | epoch 031 | loss 5.194 | nll_loss 4.42 | ppl 21.41 | wps 20468.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 6032 | lr 0.000407164 | gnorm 0.757 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 19302
2022-03-06 18:03:40 | INFO | fairseq.trainer | begin training epoch 32
2022-03-06 18:03:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:07:12 | INFO | train_inner | epoch 032:     68 / 196 loss=5.124, nll_loss=4.345, ppl=20.32, wps=20355.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6100, lr=0.000404888, gnorm=0.756, loss_scale=16, train_wall=290, gb_free=19.9, wall=19515
2022-03-06 18:08:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:12:28 | INFO | train_inner | epoch 032:    169 / 196 loss=5.146, nll_loss=4.368, ppl=20.64, wps=20761.8, ups=0.32, wpb=65536, bsz=128, num_updates=6200, lr=0.00040161, gnorm=0.762, loss_scale=16, train_wall=293, gb_free=19.9, wall=19831
2022-03-06 18:13:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:13:57 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.844 | nll_loss 7.17 | ppl 143.98 | wps 40821.9 | wpb 510.9 | bsz 1 | num_updates 6227 | best_loss 7.384
2022-03-06 18:13:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 6227 updates
2022-03-06 18:13:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 18:14:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 18:14:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 32 @ 6227 updates, score 7.844) (writing took 3.358156551141292 seconds)
2022-03-06 18:14:00 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2022-03-06 18:14:00 | INFO | train | epoch 032 | loss 5.123 | nll_loss 4.344 | ppl 20.31 | wps 20560.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6227 | lr 0.000400738 | gnorm 0.763 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 19923
2022-03-06 18:14:00 | INFO | fairseq.trainer | begin training epoch 33
2022-03-06 18:14:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:14:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:17:51 | INFO | train_inner | epoch 033:     74 / 196 loss=5.053, nll_loss=4.269, ppl=19.28, wps=20206.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6300, lr=0.00039841, gnorm=0.762, loss_scale=16, train_wall=292, gb_free=19.9, wall=20154
2022-03-06 18:19:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:23:07 | INFO | train_inner | epoch 033:    175 / 196 loss=5.083, nll_loss=4.3, ppl=19.7, wps=20791.1, ups=0.32, wpb=65536, bsz=128, num_updates=6400, lr=0.000395285, gnorm=0.763, loss_scale=8, train_wall=293, gb_free=19.9, wall=20469
2022-03-06 18:24:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:24:17 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.914 | nll_loss 7.244 | ppl 151.55 | wps 40519.1 | wpb 510.9 | bsz 1 | num_updates 6421 | best_loss 7.384
2022-03-06 18:24:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 6421 updates
2022-03-06 18:24:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 18:24:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 18:24:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 33 @ 6421 updates, score 7.914) (writing took 3.6421584500931203 seconds)
2022-03-06 18:24:20 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2022-03-06 18:24:20 | INFO | train | epoch 033 | loss 5.055 | nll_loss 4.271 | ppl 19.3 | wps 20471.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 6421 | lr 0.000394638 | gnorm 0.758 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 20543
2022-03-06 18:24:20 | INFO | fairseq.trainer | begin training epoch 34
2022-03-06 18:24:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:28:27 | INFO | train_inner | epoch 034:     79 / 196 loss=4.975, nll_loss=4.184, ppl=18.18, wps=20390.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6500, lr=0.000392232, gnorm=0.777, loss_scale=16, train_wall=289, gb_free=19.9, wall=20790
2022-03-06 18:30:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:33:42 | INFO | train_inner | epoch 034:    180 / 196 loss=5.025, nll_loss=4.236, ppl=18.85, wps=20795.6, ups=0.32, wpb=65536, bsz=128, num_updates=6600, lr=0.000389249, gnorm=0.788, loss_scale=8, train_wall=293, gb_free=19.9, wall=21105
2022-03-06 18:34:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:34:37 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.938 | nll_loss 7.266 | ppl 153.95 | wps 40652.7 | wpb 510.9 | bsz 1 | num_updates 6616 | best_loss 7.384
2022-03-06 18:34:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 6616 updates
2022-03-06 18:34:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 18:34:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 18:34:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 34 @ 6616 updates, score 7.938) (writing took 3.5158356549218297 seconds)
2022-03-06 18:34:40 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2022-03-06 18:34:40 | INFO | train | epoch 034 | loss 4.993 | nll_loss 4.203 | ppl 18.42 | wps 20587.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6616 | lr 0.000388779 | gnorm 0.787 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 21163
2022-03-06 18:34:40 | INFO | fairseq.trainer | begin training epoch 35
2022-03-06 18:34:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:39:03 | INFO | train_inner | epoch 035:     84 / 196 loss=4.906, nll_loss=4.11, ppl=17.26, wps=20390.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6700, lr=0.000386334, gnorm=0.773, loss_scale=16, train_wall=289, gb_free=19.9, wall=21426
2022-03-06 18:44:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 18:44:19 | INFO | train_inner | epoch 035:    185 / 196 loss=4.973, nll_loss=4.18, ppl=18.13, wps=20755.1, ups=0.32, wpb=65536, bsz=128, num_updates=6800, lr=0.000383482, gnorm=0.782, loss_scale=16, train_wall=293, gb_free=19.9, wall=21741
2022-03-06 18:44:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:44:58 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.987 | nll_loss 7.315 | ppl 159.2 | wps 40764.4 | wpb 510.9 | bsz 1 | num_updates 6811 | best_loss 7.384
2022-03-06 18:44:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 6811 updates
2022-03-06 18:44:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 18:45:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 18:45:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 35 @ 6811 updates, score 7.987) (writing took 3.8950387449003756 seconds)
2022-03-06 18:45:02 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2022-03-06 18:45:02 | INFO | train | epoch 035 | loss 4.932 | nll_loss 4.137 | ppl 17.59 | wps 20545.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 6811 | lr 0.000383173 | gnorm 0.778 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 21784
2022-03-06 18:45:02 | INFO | fairseq.trainer | begin training epoch 36
2022-03-06 18:45:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:48:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 18:49:43 | INFO | train_inner | epoch 036:     90 / 196 loss=4.839, nll_loss=4.038, ppl=16.42, wps=20144.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=6900, lr=0.000380693, gnorm=0.79, loss_scale=8, train_wall=293, gb_free=19.9, wall=22066
2022-03-06 18:54:55 | INFO | train_inner | epoch 036:    190 / 196 loss=4.915, nll_loss=4.118, ppl=17.36, wps=21009.5, ups=0.32, wpb=65536, bsz=128, num_updates=7000, lr=0.000377964, gnorm=0.767, loss_scale=8, train_wall=290, gb_free=19.9, wall=22378
2022-03-06 18:55:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 18:55:18 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.041 | nll_loss 7.359 | ppl 164.21 | wps 40713.2 | wpb 510.9 | bsz 1 | num_updates 7006 | best_loss 7.384
2022-03-06 18:55:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 7006 updates
2022-03-06 18:55:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 18:55:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 18:55:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 36 @ 7006 updates, score 8.041) (writing took 3.4502405289094895 seconds)
2022-03-06 18:55:22 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2022-03-06 18:55:22 | INFO | train | epoch 036 | loss 4.872 | nll_loss 4.072 | ppl 16.82 | wps 20575.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7006 | lr 0.000377803 | gnorm 0.779 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 22405
2022-03-06 18:55:22 | INFO | fairseq.trainer | begin training epoch 37
2022-03-06 18:55:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 18:55:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:00:18 | INFO | train_inner | epoch 037:     95 / 196 loss=4.781, nll_loss=3.974, ppl=15.72, wps=20221.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=7100, lr=0.000375293, gnorm=0.804, loss_scale=8, train_wall=292, gb_free=19.9, wall=22701
2022-03-06 19:05:30 | INFO | train_inner | epoch 037:    195 / 196 loss=4.861, nll_loss=4.059, ppl=16.67, wps=21007.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=7200, lr=0.000372678, gnorm=0.784, loss_scale=16, train_wall=290, gb_free=19.9, wall=23013
2022-03-06 19:05:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:05:38 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.105 | nll_loss 7.437 | ppl 173.32 | wps 40647.2 | wpb 510.9 | bsz 1 | num_updates 7201 | best_loss 7.384
2022-03-06 19:05:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 7201 updates
2022-03-06 19:05:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 19:05:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 19:05:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 37 @ 7201 updates, score 8.105) (writing took 3.434680872131139 seconds)
2022-03-06 19:05:41 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2022-03-06 19:05:41 | INFO | train | epoch 037 | loss 4.817 | nll_loss 4.013 | ppl 16.14 | wps 20596.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7201 | lr 0.000372652 | gnorm 0.794 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 23024
2022-03-06 19:05:41 | INFO | fairseq.trainer | begin training epoch 38
2022-03-06 19:05:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:06:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:10:53 | INFO | train_inner | epoch 038:    100 / 196 loss=4.719, nll_loss=3.907, ppl=15.01, wps=20232, ups=0.31, wpb=65367, bsz=127.7, num_updates=7300, lr=0.000370117, gnorm=0.793, loss_scale=8, train_wall=292, gb_free=19.9, wall=23336
2022-03-06 19:15:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:15:57 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.122 | nll_loss 7.454 | ppl 175.3 | wps 40913.1 | wpb 510.9 | bsz 1 | num_updates 7396 | best_loss 7.384
2022-03-06 19:15:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 7396 updates
2022-03-06 19:15:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 19:16:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 19:16:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 38 @ 7396 updates, score 8.122) (writing took 3.665650837821886 seconds)
2022-03-06 19:16:01 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2022-03-06 19:16:01 | INFO | train | epoch 038 | loss 4.764 | nll_loss 3.955 | ppl 15.51 | wps 20596.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7396 | lr 0.000367707 | gnorm 0.8 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 23644
2022-03-06 19:16:01 | INFO | fairseq.trainer | begin training epoch 39
2022-03-06 19:16:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:16:14 | INFO | train_inner | epoch 039:      4 / 196 loss=4.803, nll_loss=3.997, ppl=15.96, wps=20403.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7400, lr=0.000367607, gnorm=0.806, loss_scale=16, train_wall=289, gb_free=19.9, wall=23656
2022-03-06 19:20:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 19:21:29 | INFO | train_inner | epoch 039:    105 / 196 loss=4.663, nll_loss=3.847, ppl=14.39, wps=20789.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=7500, lr=0.000365148, gnorm=0.786, loss_scale=16, train_wall=293, gb_free=19.9, wall=23972
2022-03-06 19:21:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:26:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:26:17 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.222 | nll_loss 7.56 | ppl 188.66 | wps 40755.4 | wpb 510.9 | bsz 1 | num_updates 7590 | best_loss 7.384
2022-03-06 19:26:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 7590 updates
2022-03-06 19:26:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 19:26:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 19:26:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 39 @ 7590 updates, score 8.222) (writing took 3.479850559029728 seconds)
2022-03-06 19:26:21 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2022-03-06 19:26:21 | INFO | train | epoch 039 | loss 4.711 | nll_loss 3.898 | ppl 14.91 | wps 20485 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 7590 | lr 0.000362977 | gnorm 0.794 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 24264
2022-03-06 19:26:21 | INFO | fairseq.trainer | begin training epoch 40
2022-03-06 19:26:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:26:52 | INFO | train_inner | epoch 040:     10 / 196 loss=4.748, nll_loss=3.937, ppl=15.32, wps=20214.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=7600, lr=0.000362738, gnorm=0.801, loss_scale=8, train_wall=292, gb_free=19.9, wall=24295
2022-03-06 19:32:04 | INFO | train_inner | epoch 040:    110 / 196 loss=4.628, nll_loss=3.808, ppl=14.01, wps=20999, ups=0.32, wpb=65536, bsz=128, num_updates=7700, lr=0.000360375, gnorm=0.799, loss_scale=16, train_wall=290, gb_free=19.9, wall=24607
2022-03-06 19:34:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:36:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:36:38 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.25 | nll_loss 7.58 | ppl 191.36 | wps 40601.4 | wpb 510.9 | bsz 1 | num_updates 7785 | best_loss 7.384
2022-03-06 19:36:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 7785 updates
2022-03-06 19:36:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 19:36:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 19:36:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 40 @ 7785 updates, score 8.25) (writing took 3.6180801780428737 seconds)
2022-03-06 19:36:41 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2022-03-06 19:36:41 | INFO | train | epoch 040 | loss 4.664 | nll_loss 3.846 | ppl 14.38 | wps 20565.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7785 | lr 0.000358402 | gnorm 0.802 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 24884
2022-03-06 19:36:41 | INFO | fairseq.trainer | begin training epoch 41
2022-03-06 19:36:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:37:28 | INFO | train_inner | epoch 041:     15 / 196 loss=4.691, nll_loss=3.875, ppl=14.67, wps=20164.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=7800, lr=0.000358057, gnorm=0.812, loss_scale=8, train_wall=293, gb_free=19.9, wall=24931
2022-03-06 19:42:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:42:44 | INFO | train_inner | epoch 041:    116 / 196 loss=4.581, nll_loss=3.758, ppl=13.52, wps=20747.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=7900, lr=0.000355784, gnorm=0.806, loss_scale=8, train_wall=293, gb_free=19.9, wall=25247
2022-03-06 19:46:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:46:59 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.332 | nll_loss 7.661 | ppl 202.36 | wps 40698.3 | wpb 510.9 | bsz 1 | num_updates 7980 | best_loss 7.384
2022-03-06 19:46:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 7980 updates
2022-03-06 19:46:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 19:47:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 19:47:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 41 @ 7980 updates, score 8.332) (writing took 3.4211712600663304 seconds)
2022-03-06 19:47:02 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2022-03-06 19:47:02 | INFO | train | epoch 041 | loss 4.615 | nll_loss 3.794 | ppl 13.87 | wps 20560.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 7980 | lr 0.000353996 | gnorm 0.816 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 25505
2022-03-06 19:47:02 | INFO | fairseq.trainer | begin training epoch 42
2022-03-06 19:47:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:48:05 | INFO | train_inner | epoch 042:     20 / 196 loss=4.635, nll_loss=3.815, ppl=14.08, wps=20399.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=8000, lr=0.000353553, gnorm=0.825, loss_scale=8, train_wall=289, gb_free=19.9, wall=25568
2022-03-06 19:53:17 | INFO | train_inner | epoch 042:    120 / 196 loss=4.543, nll_loss=3.716, ppl=13.14, wps=20997.8, ups=0.32, wpb=65536, bsz=128, num_updates=8100, lr=0.000351364, gnorm=0.806, loss_scale=16, train_wall=290, gb_free=19.9, wall=25880
2022-03-06 19:55:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 19:57:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 19:57:19 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.342 | nll_loss 7.676 | ppl 204.5 | wps 40716.4 | wpb 510.9 | bsz 1 | num_updates 8175 | best_loss 7.384
2022-03-06 19:57:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 8175 updates
2022-03-06 19:57:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 19:57:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 19:57:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 42 @ 8175 updates, score 8.342) (writing took 3.544510189909488 seconds)
2022-03-06 19:57:22 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2022-03-06 19:57:22 | INFO | train | epoch 042 | loss 4.57 | nll_loss 3.744 | ppl 13.4 | wps 20584.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 8175 | lr 0.000349749 | gnorm 0.818 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 26125
2022-03-06 19:57:22 | INFO | fairseq.trainer | begin training epoch 43
2022-03-06 19:57:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 19:58:40 | INFO | train_inner | epoch 043:     25 / 196 loss=4.582, nll_loss=3.758, ppl=13.53, wps=20207.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=8200, lr=0.000349215, gnorm=0.831, loss_scale=8, train_wall=292, gb_free=19.9, wall=26203
2022-03-06 20:03:52 | INFO | train_inner | epoch 043:    125 / 196 loss=4.508, nll_loss=3.678, ppl=12.8, wps=21007.6, ups=0.32, wpb=65536, bsz=128, num_updates=8300, lr=0.000347105, gnorm=0.825, loss_scale=16, train_wall=290, gb_free=19.9, wall=26515
2022-03-06 20:05:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:07:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:07:38 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.435 | nll_loss 7.771 | ppl 218.41 | wps 40764.6 | wpb 510.9 | bsz 1 | num_updates 8370 | best_loss 7.384
2022-03-06 20:07:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 8370 updates
2022-03-06 20:07:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 20:07:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 20:07:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 43 @ 8370 updates, score 8.435) (writing took 3.402610832126811 seconds)
2022-03-06 20:07:42 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2022-03-06 20:07:42 | INFO | train | epoch 043 | loss 4.527 | nll_loss 3.698 | ppl 12.98 | wps 20595.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 8370 | lr 0.000345651 | gnorm 0.827 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 26745
2022-03-06 20:07:42 | INFO | fairseq.trainer | begin training epoch 44
2022-03-06 20:07:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:09:15 | INFO | train_inner | epoch 044:     30 / 196 loss=4.524, nll_loss=3.695, ppl=12.95, wps=20223.8, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=8400, lr=0.000345033, gnorm=0.825, loss_scale=8, train_wall=292, gb_free=19.9, wall=26838
2022-03-06 20:14:27 | INFO | train_inner | epoch 044:    130 / 196 loss=4.47, nll_loss=3.636, ppl=12.43, wps=21004.9, ups=0.32, wpb=65536, bsz=128, num_updates=8500, lr=0.000342997, gnorm=0.83, loss_scale=16, train_wall=290, gb_free=19.9, wall=27150
2022-03-06 20:14:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:17:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:17:58 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.454 | nll_loss 7.791 | ppl 221.45 | wps 40780.4 | wpb 510.9 | bsz 1 | num_updates 8565 | best_loss 7.384
2022-03-06 20:17:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 8565 updates
2022-03-06 20:17:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 20:18:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 20:18:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 44 @ 8565 updates, score 8.454) (writing took 3.3557077900040895 seconds)
2022-03-06 20:18:01 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2022-03-06 20:18:01 | INFO | train | epoch 044 | loss 4.484 | nll_loss 3.651 | ppl 12.56 | wps 20602.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 8565 | lr 0.000341693 | gnorm 0.831 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 27364
2022-03-06 20:18:01 | INFO | fairseq.trainer | begin training epoch 45
2022-03-06 20:18:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:19:51 | INFO | train_inner | epoch 045:     35 / 196 loss=4.481, nll_loss=3.649, ppl=12.54, wps=20221.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=8600, lr=0.000340997, gnorm=0.844, loss_scale=8, train_wall=292, gb_free=19.9, wall=27474
2022-03-06 20:25:03 | INFO | train_inner | epoch 045:    135 / 196 loss=4.432, nll_loss=3.596, ppl=12.09, wps=20999.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=8700, lr=0.000339032, gnorm=0.849, loss_scale=16, train_wall=290, gb_free=19.9, wall=27786
2022-03-06 20:28:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:28:18 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.517 | nll_loss 7.858 | ppl 232.04 | wps 40900.2 | wpb 510.9 | bsz 1 | num_updates 8761 | best_loss 7.384
2022-03-06 20:28:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 8761 updates
2022-03-06 20:28:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 20:28:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 20:28:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 45 @ 8761 updates, score 8.517) (writing took 3.4373051039874554 seconds)
2022-03-06 20:28:21 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2022-03-06 20:28:21 | INFO | train | epoch 045 | loss 4.443 | nll_loss 3.607 | ppl 12.18 | wps 20690.6 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 8761 | lr 0.000337849 | gnorm 0.848 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 27984
2022-03-06 20:28:21 | INFO | fairseq.trainer | begin training epoch 46
2022-03-06 20:28:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:28:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:30:26 | INFO | train_inner | epoch 046:     40 / 196 loss=4.439, nll_loss=3.603, ppl=12.15, wps=20219.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=8800, lr=0.0003371, gnorm=0.842, loss_scale=16, train_wall=292, gb_free=19.9, wall=28109
2022-03-06 20:35:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 20:35:41 | INFO | train_inner | epoch 046:    141 / 196 loss=4.396, nll_loss=3.556, ppl=11.76, wps=20796.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=8900, lr=0.000335201, gnorm=0.843, loss_scale=16, train_wall=293, gb_free=19.9, wall=28424
2022-03-06 20:36:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:38:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:38:37 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.593 | nll_loss 7.928 | ppl 243.62 | wps 40785.3 | wpb 510.9 | bsz 1 | num_updates 8954 | best_loss 7.384
2022-03-06 20:38:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 8954 updates
2022-03-06 20:38:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 20:38:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 20:38:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 46 @ 8954 updates, score 8.593) (writing took 3.5886000799946487 seconds)
2022-03-06 20:38:41 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2022-03-06 20:38:41 | INFO | train | epoch 046 | loss 4.403 | nll_loss 3.564 | ppl 11.82 | wps 20383.8 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 8954 | lr 0.000334188 | gnorm 0.843 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 28604
2022-03-06 20:38:41 | INFO | fairseq.trainer | begin training epoch 47
2022-03-06 20:38:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:41:05 | INFO | train_inner | epoch 047:     46 / 196 loss=4.383, nll_loss=3.542, ppl=11.65, wps=20189.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=9000, lr=0.000333333, gnorm=0.842, loss_scale=8, train_wall=292, gb_free=19.9, wall=28748
2022-03-06 20:44:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:46:21 | INFO | train_inner | epoch 047:    147 / 196 loss=4.371, nll_loss=3.528, ppl=11.54, wps=20750.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=9100, lr=0.000331497, gnorm=0.858, loss_scale=8, train_wall=293, gb_free=19.9, wall=29064
2022-03-06 20:48:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:48:59 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.614 | nll_loss 7.957 | ppl 248.49 | wps 40652.5 | wpb 510.9 | bsz 1 | num_updates 9149 | best_loss 7.384
2022-03-06 20:48:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 9149 updates
2022-03-06 20:48:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 20:49:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 20:49:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 47 @ 9149 updates, score 8.614) (writing took 3.8314534339588135 seconds)
2022-03-06 20:49:03 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2022-03-06 20:49:03 | INFO | train | epoch 047 | loss 4.368 | nll_loss 3.525 | ppl 11.51 | wps 20530.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 9149 | lr 0.000330608 | gnorm 0.857 | loss_scale 8 | train_wall 569 | gb_free 19.9 | wall 29225
2022-03-06 20:49:03 | INFO | fairseq.trainer | begin training epoch 48
2022-03-06 20:49:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 20:51:42 | INFO | train_inner | epoch 048:     51 / 196 loss=4.34, nll_loss=3.495, ppl=11.28, wps=20358.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=9200, lr=0.00032969, gnorm=0.867, loss_scale=16, train_wall=290, gb_free=19.9, wall=29385
2022-03-06 20:54:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 20:56:57 | INFO | train_inner | epoch 048:    152 / 196 loss=4.344, nll_loss=3.499, ppl=11.3, wps=20793.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=9300, lr=0.000327913, gnorm=0.848, loss_scale=8, train_wall=293, gb_free=19.9, wall=29700
2022-03-06 20:59:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 20:59:19 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.679 | nll_loss 8.013 | ppl 258.39 | wps 40724.6 | wpb 510.9 | bsz 1 | num_updates 9344 | best_loss 7.384
2022-03-06 20:59:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 9344 updates
2022-03-06 20:59:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 20:59:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 20:59:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 48 @ 9344 updates, score 8.679) (writing took 3.5956268589943647 seconds)
2022-03-06 20:59:22 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2022-03-06 20:59:22 | INFO | train | epoch 048 | loss 4.331 | nll_loss 3.485 | ppl 11.2 | wps 20586.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 9344 | lr 0.00032714 | gnorm 0.851 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 29845
2022-03-06 20:59:22 | INFO | fairseq.trainer | begin training epoch 49
2022-03-06 20:59:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:02:17 | INFO | train_inner | epoch 049:     56 / 196 loss=4.299, nll_loss=3.451, ppl=10.94, wps=20401.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=9400, lr=0.000326164, gnorm=0.847, loss_scale=16, train_wall=289, gb_free=19.9, wall=30020
2022-03-06 21:02:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:07:33 | INFO | train_inner | epoch 049:    157 / 196 loss=4.312, nll_loss=3.464, ppl=11.04, wps=20786.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=9500, lr=0.000324443, gnorm=0.874, loss_scale=8, train_wall=293, gb_free=19.9, wall=30336
2022-03-06 21:09:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:09:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:09:39 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.765 | nll_loss 8.095 | ppl 273.36 | wps 40918.9 | wpb 510.9 | bsz 1 | num_updates 9538 | best_loss 7.384
2022-03-06 21:09:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 9538 updates
2022-03-06 21:09:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 21:09:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 21:09:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 49 @ 9538 updates, score 8.765) (writing took 3.388052854919806 seconds)
2022-03-06 21:09:42 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2022-03-06 21:09:42 | INFO | train | epoch 049 | loss 4.296 | nll_loss 3.447 | ppl 10.91 | wps 20484 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 9538 | lr 0.000323796 | gnorm 0.864 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 30465
2022-03-06 21:09:42 | INFO | fairseq.trainer | begin training epoch 50
2022-03-06 21:09:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:12:56 | INFO | train_inner | epoch 050:     62 / 196 loss=4.262, nll_loss=3.41, ppl=10.63, wps=20224.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=9600, lr=0.000322749, gnorm=0.866, loss_scale=8, train_wall=292, gb_free=19.9, wall=30659
2022-03-06 21:18:08 | INFO | train_inner | epoch 050:    162 / 196 loss=4.278, nll_loss=3.428, ppl=10.76, wps=21005.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=9700, lr=0.000321081, gnorm=0.855, loss_scale=16, train_wall=290, gb_free=19.9, wall=30971
2022-03-06 21:19:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:19:59 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.732 | nll_loss 8.065 | ppl 267.84 | wps 40819.7 | wpb 510.9 | bsz 1 | num_updates 9734 | best_loss 7.384
2022-03-06 21:19:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 9734 updates
2022-03-06 21:19:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 21:20:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 21:20:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 50 @ 9734 updates, score 8.732) (writing took 3.3501879051327705 seconds)
2022-03-06 21:20:02 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2022-03-06 21:20:02 | INFO | train | epoch 050 | loss 4.263 | nll_loss 3.411 | ppl 10.64 | wps 20701.3 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 9734 | lr 0.000320519 | gnorm 0.855 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 31085
2022-03-06 21:20:02 | INFO | fairseq.trainer | begin training epoch 51
2022-03-06 21:20:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:20:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:23:31 | INFO | train_inner | epoch 051:     67 / 196 loss=4.214, nll_loss=3.359, ppl=10.26, wps=20223.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=9800, lr=0.000319438, gnorm=0.858, loss_scale=8, train_wall=292, gb_free=19.9, wall=31294
2022-03-06 21:27:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:28:46 | INFO | train_inner | epoch 051:    168 / 196 loss=4.256, nll_loss=3.403, ppl=10.58, wps=20814, ups=0.32, wpb=65536, bsz=128, num_updates=9900, lr=0.000317821, gnorm=0.887, loss_scale=8, train_wall=293, gb_free=19.9, wall=31609
2022-03-06 21:30:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:30:18 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 8.82 | nll_loss 8.164 | ppl 286.74 | wps 40685 | wpb 510.9 | bsz 1 | num_updates 9928 | best_loss 7.384
2022-03-06 21:30:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 9928 updates
2022-03-06 21:30:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 21:30:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 21:30:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 51 @ 9928 updates, score 8.82) (writing took 3.3970085300970823 seconds)
2022-03-06 21:30:21 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2022-03-06 21:30:21 | INFO | train | epoch 051 | loss 4.229 | nll_loss 3.375 | ppl 10.37 | wps 20500.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 9928 | lr 0.000317372 | gnorm 0.877 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 31704
2022-03-06 21:30:21 | INFO | fairseq.trainer | begin training epoch 52
2022-03-06 21:30:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:34:06 | INFO | train_inner | epoch 052:     72 / 196 loss=4.188, nll_loss=3.33, ppl=10.06, wps=20420.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10000, lr=0.000316228, gnorm=0.884, loss_scale=8, train_wall=289, gb_free=19.9, wall=31929
2022-03-06 21:39:18 | INFO | train_inner | epoch 052:    172 / 196 loss=4.229, nll_loss=3.375, ppl=10.37, wps=21004.3, ups=0.32, wpb=65536, bsz=128, num_updates=10100, lr=0.000314658, gnorm=0.876, loss_scale=16, train_wall=290, gb_free=19.9, wall=32241
2022-03-06 21:39:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:40:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:40:37 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 8.872 | nll_loss 8.211 | ppl 296.32 | wps 40719.6 | wpb 510.9 | bsz 1 | num_updates 10123 | best_loss 7.384
2022-03-06 21:40:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 10123 updates
2022-03-06 21:40:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 21:40:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 21:40:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 52 @ 10123 updates, score 8.872) (writing took 3.406420738203451 seconds)
2022-03-06 21:40:41 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2022-03-06 21:40:41 | INFO | train | epoch 052 | loss 4.199 | nll_loss 3.342 | ppl 10.14 | wps 20597 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10123 | lr 0.000314301 | gnorm 0.881 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 32324
2022-03-06 21:40:41 | INFO | fairseq.trainer | begin training epoch 53
2022-03-06 21:40:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:44:41 | INFO | train_inner | epoch 053:     77 / 196 loss=4.146, nll_loss=3.285, ppl=9.75, wps=20218.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=10200, lr=0.000313112, gnorm=0.875, loss_scale=8, train_wall=292, gb_free=19.9, wall=32564
2022-03-06 21:49:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 21:49:57 | INFO | train_inner | epoch 053:    178 / 196 loss=4.205, nll_loss=3.348, ppl=10.18, wps=20767.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=10300, lr=0.000311588, gnorm=0.893, loss_scale=8, train_wall=293, gb_free=19.9, wall=32880
2022-03-06 21:50:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 21:50:58 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 8.883 | nll_loss 8.226 | ppl 299.38 | wps 40773.2 | wpb 510.9 | bsz 1 | num_updates 10318 | best_loss 7.384
2022-03-06 21:50:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 10318 updates
2022-03-06 21:50:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 21:51:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 21:51:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 53 @ 10318 updates, score 8.883) (writing took 3.8808017959818244 seconds)
2022-03-06 21:51:02 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2022-03-06 21:51:02 | INFO | train | epoch 053 | loss 4.169 | nll_loss 3.31 | ppl 9.91 | wps 20560 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10318 | lr 0.000311317 | gnorm 0.887 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 32944
2022-03-06 21:51:02 | INFO | fairseq.trainer | begin training epoch 54
2022-03-06 21:51:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 21:55:18 | INFO | train_inner | epoch 054:     82 / 196 loss=4.101, nll_loss=3.237, ppl=9.43, wps=20363.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=10400, lr=0.000310087, gnorm=0.877, loss_scale=8, train_wall=289, gb_free=19.9, wall=33201
2022-03-06 21:59:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:00:33 | INFO | train_inner | epoch 054:    183 / 196 loss=4.18, nll_loss=3.32, ppl=9.99, wps=20801.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=10500, lr=0.000308607, gnorm=0.89, loss_scale=8, train_wall=293, gb_free=19.9, wall=33516
2022-03-06 22:01:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:01:18 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 8.919 | nll_loss 8.261 | ppl 306.77 | wps 40759.2 | wpb 510.9 | bsz 1 | num_updates 10513 | best_loss 7.384
2022-03-06 22:01:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 10513 updates
2022-03-06 22:01:18 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 22:01:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 22:01:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 54 @ 10513 updates, score 8.919) (writing took 3.5340086398646235 seconds)
2022-03-06 22:01:22 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2022-03-06 22:01:22 | INFO | train | epoch 054 | loss 4.138 | nll_loss 3.276 | ppl 9.69 | wps 20584.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10513 | lr 0.000308416 | gnorm 0.88 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 33564
2022-03-06 22:01:22 | INFO | fairseq.trainer | begin training epoch 55
2022-03-06 22:01:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:05:53 | INFO | train_inner | epoch 055:     87 / 196 loss=4.074, nll_loss=3.207, ppl=9.24, wps=20420, ups=0.31, wpb=65367, bsz=127.7, num_updates=10600, lr=0.000307148, gnorm=0.888, loss_scale=8, train_wall=289, gb_free=19.9, wall=33836
2022-03-06 22:08:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:11:08 | INFO | train_inner | epoch 055:    188 / 196 loss=4.156, nll_loss=3.294, ppl=9.81, wps=20814.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=10700, lr=0.000305709, gnorm=0.884, loss_scale=8, train_wall=293, gb_free=19.9, wall=34151
2022-03-06 22:11:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:11:37 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 8.991 | nll_loss 8.336 | ppl 323.14 | wps 40913.9 | wpb 510.9 | bsz 1 | num_updates 10708 | best_loss 7.384
2022-03-06 22:11:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 10708 updates
2022-03-06 22:11:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 22:11:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 22:11:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 55 @ 10708 updates, score 8.991) (writing took 3.6133348850999027 seconds)
2022-03-06 22:11:41 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2022-03-06 22:11:41 | INFO | train | epoch 055 | loss 4.11 | nll_loss 3.246 | ppl 9.49 | wps 20602.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10708 | lr 0.000305595 | gnorm 0.89 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 34184
2022-03-06 22:11:41 | INFO | fairseq.trainer | begin training epoch 56
2022-03-06 22:11:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:15:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:16:31 | INFO | train_inner | epoch 056:     93 / 196 loss=4.042, nll_loss=3.172, ppl=9.01, wps=20208.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=10800, lr=0.00030429, gnorm=0.886, loss_scale=8, train_wall=292, gb_free=19.9, wall=34474
2022-03-06 22:21:43 | INFO | train_inner | epoch 056:    193 / 196 loss=4.128, nll_loss=3.264, ppl=9.61, wps=21017.6, ups=0.32, wpb=65536, bsz=128, num_updates=10900, lr=0.000302891, gnorm=0.898, loss_scale=8, train_wall=290, gb_free=19.9, wall=34786
2022-03-06 22:21:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:21:57 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 9.006 | nll_loss 8.343 | ppl 324.81 | wps 40895.2 | wpb 510.9 | bsz 1 | num_updates 10903 | best_loss 7.384
2022-03-06 22:21:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 10903 updates
2022-03-06 22:21:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 22:22:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 22:22:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 56 @ 10903 updates, score 9.006) (writing took 3.5089567420072854 seconds)
2022-03-06 22:22:01 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2022-03-06 22:22:01 | INFO | train | epoch 056 | loss 4.083 | nll_loss 3.216 | ppl 9.29 | wps 20599 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 10903 | lr 0.00030285 | gnorm 0.891 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 34803
2022-03-06 22:22:01 | INFO | fairseq.trainer | begin training epoch 57
2022-03-06 22:22:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:27:03 | INFO | train_inner | epoch 057:     97 / 196 loss=4.014, nll_loss=3.142, ppl=8.83, wps=20411.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=11000, lr=0.000301511, gnorm=0.896, loss_scale=16, train_wall=289, gb_free=19.9, wall=35106
2022-03-06 22:27:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:32:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:32:17 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 9.073 | nll_loss 8.425 | ppl 343.74 | wps 40626.5 | wpb 510.9 | bsz 1 | num_updates 11098 | best_loss 7.384
2022-03-06 22:32:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 11098 updates
2022-03-06 22:32:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 22:32:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 22:32:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 57 @ 11098 updates, score 9.073) (writing took 3.599120987113565 seconds)
2022-03-06 22:32:20 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2022-03-06 22:32:20 | INFO | train | epoch 057 | loss 4.056 | nll_loss 3.187 | ppl 9.11 | wps 20593.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11098 | lr 0.000300177 | gnorm 0.905 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 35423
2022-03-06 22:32:20 | INFO | fairseq.trainer | begin training epoch 58
2022-03-06 22:32:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:32:27 | INFO | train_inner | epoch 058:      2 / 196 loss=4.101, nll_loss=3.235, ppl=9.41, wps=20217.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=11100, lr=0.00030015, gnorm=0.917, loss_scale=8, train_wall=292, gb_free=19.9, wall=35430
2022-03-06 22:36:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:37:42 | INFO | train_inner | epoch 058:    103 / 196 loss=3.984, nll_loss=3.11, ppl=8.63, wps=20803.5, ups=0.32, wpb=65536, bsz=128, num_updates=11200, lr=0.000298807, gnorm=0.887, loss_scale=8, train_wall=293, gb_free=19.9, wall=35745
2022-03-06 22:42:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:42:36 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 9.123 | nll_loss 8.459 | ppl 351.99 | wps 40705.1 | wpb 510.9 | bsz 1 | num_updates 11293 | best_loss 7.384
2022-03-06 22:42:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 11293 updates
2022-03-06 22:42:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 22:42:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 22:42:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 58 @ 11293 updates, score 9.123) (writing took 3.5199539731256664 seconds)
2022-03-06 22:42:40 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2022-03-06 22:42:40 | INFO | train | epoch 058 | loss 4.031 | nll_loss 3.159 | ppl 8.93 | wps 20596.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11293 | lr 0.000297574 | gnorm 0.905 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 36043
2022-03-06 22:42:40 | INFO | fairseq.trainer | begin training epoch 59
2022-03-06 22:42:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:43:02 | INFO | train_inner | epoch 059:      7 / 196 loss=4.069, nll_loss=3.201, ppl=9.19, wps=20414.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=11300, lr=0.000297482, gnorm=0.922, loss_scale=8, train_wall=289, gb_free=19.9, wall=36065
2022-03-06 22:43:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:48:17 | INFO | train_inner | epoch 059:    108 / 196 loss=3.963, nll_loss=3.086, ppl=8.49, wps=20789.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=11400, lr=0.000296174, gnorm=0.923, loss_scale=8, train_wall=293, gb_free=19.9, wall=36380
2022-03-06 22:51:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 22:52:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 22:52:56 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 9.137 | nll_loss 8.483 | ppl 357.83 | wps 40551.6 | wpb 510.9 | bsz 1 | num_updates 11487 | best_loss 7.384
2022-03-06 22:52:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 11487 updates
2022-03-06 22:52:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 22:53:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 22:53:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 59 @ 11487 updates, score 9.137) (writing took 3.5925349488388747 seconds)
2022-03-06 22:53:00 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2022-03-06 22:53:00 | INFO | train | epoch 059 | loss 4.004 | nll_loss 3.131 | ppl 8.76 | wps 20480.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 11487 | lr 0.000295051 | gnorm 0.92 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 36663
2022-03-06 22:53:00 | INFO | fairseq.trainer | begin training epoch 60
2022-03-06 22:53:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 22:53:41 | INFO | train_inner | epoch 060:     13 / 196 loss=4.039, nll_loss=3.168, ppl=8.99, wps=20213, ups=0.31, wpb=65367, bsz=127.7, num_updates=11500, lr=0.000294884, gnorm=0.915, loss_scale=8, train_wall=292, gb_free=19.9, wall=36703
2022-03-06 22:58:53 | INFO | train_inner | epoch 060:    113 / 196 loss=3.949, nll_loss=3.07, ppl=8.4, wps=21006, ups=0.32, wpb=65536, bsz=128, num_updates=11600, lr=0.00029361, gnorm=0.907, loss_scale=16, train_wall=290, gb_free=19.9, wall=37015
2022-03-06 23:00:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:03:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:03:16 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 9.224 | nll_loss 8.567 | ppl 379.19 | wps 40926.5 | wpb 510.9 | bsz 1 | num_updates 11682 | best_loss 7.384
2022-03-06 23:03:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 11682 updates
2022-03-06 23:03:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 23:03:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 23:03:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 60 @ 11682 updates, score 9.224) (writing took 3.554136391961947 seconds)
2022-03-06 23:03:19 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2022-03-06 23:03:19 | INFO | train | epoch 060 | loss 3.981 | nll_loss 3.105 | ppl 8.61 | wps 20601.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11682 | lr 0.000292578 | gnorm 0.912 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 37282
2022-03-06 23:03:19 | INFO | fairseq.trainer | begin training epoch 61
2022-03-06 23:03:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:04:16 | INFO | train_inner | epoch 061:     18 / 196 loss=4.005, nll_loss=3.132, ppl=8.76, wps=20229.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=11700, lr=0.000292353, gnorm=0.924, loss_scale=8, train_wall=292, gb_free=19.9, wall=37338
2022-03-06 23:09:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:09:31 | INFO | train_inner | epoch 061:    119 / 196 loss=3.928, nll_loss=3.049, ppl=8.27, wps=20796.3, ups=0.32, wpb=65536, bsz=128, num_updates=11800, lr=0.000291111, gnorm=0.921, loss_scale=8, train_wall=293, gb_free=19.9, wall=37654
2022-03-06 23:13:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:13:36 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 9.217 | nll_loss 8.569 | ppl 379.68 | wps 40823.1 | wpb 510.9 | bsz 1 | num_updates 11877 | best_loss 7.384
2022-03-06 23:13:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 11877 updates
2022-03-06 23:13:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 23:13:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 23:13:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 61 @ 11877 updates, score 9.217) (writing took 3.604390434920788 seconds)
2022-03-06 23:13:39 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2022-03-06 23:13:39 | INFO | train | epoch 061 | loss 3.958 | nll_loss 3.08 | ppl 8.46 | wps 20591.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 11877 | lr 0.000290166 | gnorm 0.923 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 37902
2022-03-06 23:13:39 | INFO | fairseq.trainer | begin training epoch 62
2022-03-06 23:13:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:14:51 | INFO | train_inner | epoch 062:     23 / 196 loss=3.977, nll_loss=3.101, ppl=8.58, wps=20402.1, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=11900, lr=0.000289886, gnorm=0.92, loss_scale=8, train_wall=289, gb_free=19.9, wall=37974
2022-03-06 23:17:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:20:06 | INFO | train_inner | epoch 062:    124 / 196 loss=3.909, nll_loss=3.027, ppl=8.15, wps=20794, ups=0.32, wpb=65536, bsz=128, num_updates=12000, lr=0.000288675, gnorm=0.922, loss_scale=8, train_wall=293, gb_free=19.9, wall=38289
2022-03-06 23:23:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:23:55 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 9.272 | nll_loss 8.618 | ppl 392.89 | wps 40894.3 | wpb 510.9 | bsz 1 | num_updates 12072 | best_loss 7.384
2022-03-06 23:23:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 12072 updates
2022-03-06 23:23:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 23:23:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 23:23:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 62 @ 12072 updates, score 9.272) (writing took 3.667424442945048 seconds)
2022-03-06 23:23:59 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2022-03-06 23:23:59 | INFO | train | epoch 062 | loss 3.935 | nll_loss 3.055 | ppl 8.31 | wps 20585.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12072 | lr 0.000287813 | gnorm 0.926 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 38522
2022-03-06 23:23:59 | INFO | fairseq.trainer | begin training epoch 63
2022-03-06 23:23:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:25:27 | INFO | train_inner | epoch 063:     28 / 196 loss=3.952, nll_loss=3.074, ppl=8.42, wps=20399.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=12100, lr=0.00028748, gnorm=0.931, loss_scale=16, train_wall=289, gb_free=19.9, wall=38610
2022-03-06 23:30:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:30:42 | INFO | train_inner | epoch 063:    129 / 196 loss=3.893, nll_loss=3.01, ppl=8.05, wps=20803.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=12200, lr=0.000286299, gnorm=0.93, loss_scale=8, train_wall=293, gb_free=19.9, wall=38925
2022-03-06 23:34:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:34:15 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 9.319 | nll_loss 8.663 | ppl 405.39 | wps 40653.4 | wpb 510.9 | bsz 1 | num_updates 12267 | best_loss 7.384
2022-03-06 23:34:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 12267 updates
2022-03-06 23:34:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 23:34:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 23:34:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 63 @ 12267 updates, score 9.319) (writing took 3.612498290836811 seconds)
2022-03-06 23:34:19 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2022-03-06 23:34:19 | INFO | train | epoch 063 | loss 3.913 | nll_loss 3.032 | ppl 8.18 | wps 20589.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12267 | lr 0.000285516 | gnorm 0.935 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 39142
2022-03-06 23:34:19 | INFO | fairseq.trainer | begin training epoch 64
2022-03-06 23:34:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:36:02 | INFO | train_inner | epoch 064:     33 / 196 loss=3.922, nll_loss=3.041, ppl=8.23, wps=20404.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=12300, lr=0.000285133, gnorm=0.947, loss_scale=8, train_wall=289, gb_free=19.9, wall=39245
2022-03-06 23:39:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:41:17 | INFO | train_inner | epoch 064:    134 / 196 loss=3.878, nll_loss=2.993, ppl=7.96, wps=20812.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=12400, lr=0.000283981, gnorm=0.935, loss_scale=8, train_wall=293, gb_free=19.9, wall=39560
2022-03-06 23:44:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:44:35 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 9.33 | nll_loss 8.674 | ppl 408.41 | wps 40752.1 | wpb 510.9 | bsz 1 | num_updates 12462 | best_loss 7.384
2022-03-06 23:44:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 12462 updates
2022-03-06 23:44:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 23:44:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 23:44:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 64 @ 12462 updates, score 9.33) (writing took 3.6342404959723353 seconds)
2022-03-06 23:44:39 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2022-03-06 23:44:39 | INFO | train | epoch 064 | loss 3.891 | nll_loss 3.008 | ppl 8.05 | wps 20598.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 12462 | lr 0.000283274 | gnorm 0.937 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 39761
2022-03-06 23:44:39 | INFO | fairseq.trainer | begin training epoch 65
2022-03-06 23:44:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:46:37 | INFO | train_inner | epoch 065:     38 / 196 loss=3.893, nll_loss=3.01, ppl=8.06, wps=20407.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=12500, lr=0.000282843, gnorm=0.929, loss_scale=16, train_wall=289, gb_free=19.9, wall=39880
2022-03-06 23:51:49 | INFO | train_inner | epoch 065:    138 / 196 loss=3.862, nll_loss=2.976, ppl=7.87, wps=21007.3, ups=0.32, wpb=65536, bsz=128, num_updates=12600, lr=0.000281718, gnorm=0.943, loss_scale=16, train_wall=290, gb_free=19.9, wall=40192
2022-03-06 23:52:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-06 23:53:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-06 23:54:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-06 23:54:55 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 9.377 | nll_loss 8.725 | ppl 423.21 | wps 40901.6 | wpb 510.9 | bsz 1 | num_updates 12656 | best_loss 7.384
2022-03-06 23:54:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 12656 updates
2022-03-06 23:54:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 23:54:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-06 23:54:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 65 @ 12656 updates, score 9.377) (writing took 3.6506583020091057 seconds)
2022-03-06 23:54:58 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2022-03-06 23:54:58 | INFO | train | epoch 065 | loss 3.871 | nll_loss 2.986 | ppl 7.92 | wps 20484.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 12656 | lr 0.000281094 | gnorm 0.933 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 40381
2022-03-06 23:54:58 | INFO | fairseq.trainer | begin training epoch 66
2022-03-06 23:54:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-06 23:57:16 | INFO | train_inner | epoch 066:     44 / 196 loss=3.867, nll_loss=2.982, ppl=7.9, wps=20018, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=12700, lr=0.000280607, gnorm=0.938, loss_scale=8, train_wall=295, gb_free=19.9, wall=40519
2022-03-07 00:02:28 | INFO | train_inner | epoch 066:    144 / 196 loss=3.851, nll_loss=2.964, ppl=7.81, wps=21005.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=12800, lr=0.000279508, gnorm=0.948, loss_scale=16, train_wall=290, gb_free=19.9, wall=40831
2022-03-07 00:05:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:05:15 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 9.443 | nll_loss 8.793 | ppl 443.71 | wps 40723.6 | wpb 510.9 | bsz 1 | num_updates 12852 | best_loss 7.384
2022-03-07 00:05:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 12852 updates
2022-03-07 00:05:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 00:05:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 00:05:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 66 @ 12852 updates, score 9.443) (writing took 3.6434858860448003 seconds)
2022-03-07 00:05:18 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2022-03-07 00:05:18 | INFO | train | epoch 066 | loss 3.851 | nll_loss 2.964 | ppl 7.8 | wps 20693.3 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 12852 | lr 0.000278942 | gnorm 0.95 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 41001
2022-03-07 00:05:18 | INFO | fairseq.trainer | begin training epoch 67
2022-03-07 00:05:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:06:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:07:51 | INFO | train_inner | epoch 067:     49 / 196 loss=3.839, nll_loss=2.951, ppl=7.73, wps=20196.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=12900, lr=0.000278423, gnorm=0.948, loss_scale=8, train_wall=292, gb_free=19.9, wall=41154
2022-03-07 00:13:03 | INFO | train_inner | epoch 067:    149 / 196 loss=3.831, nll_loss=2.943, ppl=7.69, wps=21018.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=13000, lr=0.00027735, gnorm=0.943, loss_scale=16, train_wall=290, gb_free=19.9, wall=41466
2022-03-07 00:15:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:15:34 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 9.451 | nll_loss 8.799 | ppl 445.28 | wps 40879.4 | wpb 510.9 | bsz 1 | num_updates 13047 | best_loss 7.384
2022-03-07 00:15:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 13047 updates
2022-03-07 00:15:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 00:15:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 00:15:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 67 @ 13047 updates, score 9.451) (writing took 3.6120463269762695 seconds)
2022-03-07 00:15:38 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2022-03-07 00:15:38 | INFO | train | epoch 067 | loss 3.83 | nll_loss 2.941 | ppl 7.68 | wps 20592.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13047 | lr 0.00027685 | gnorm 0.943 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 41621
2022-03-07 00:15:38 | INFO | fairseq.trainer | begin training epoch 68
2022-03-07 00:15:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:16:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:18:27 | INFO | train_inner | epoch 068:     54 / 196 loss=3.815, nll_loss=2.925, ppl=7.59, wps=20207.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=13100, lr=0.000276289, gnorm=0.953, loss_scale=8, train_wall=292, gb_free=19.9, wall=41789
2022-03-07 00:23:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:23:42 | INFO | train_inner | epoch 068:    155 / 196 loss=3.822, nll_loss=2.932, ppl=7.63, wps=20804.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=13200, lr=0.000275241, gnorm=0.963, loss_scale=8, train_wall=293, gb_free=19.9, wall=42104
2022-03-07 00:25:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:25:54 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 9.523 | nll_loss 8.879 | ppl 470.67 | wps 40810.6 | wpb 510.9 | bsz 1 | num_updates 13241 | best_loss 7.384
2022-03-07 00:25:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 13241 updates
2022-03-07 00:25:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 00:25:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 00:25:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 68 @ 13241 updates, score 9.523) (writing took 3.64895796100609 seconds)
2022-03-07 00:25:58 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2022-03-07 00:25:58 | INFO | train | epoch 068 | loss 3.811 | nll_loss 2.921 | ppl 7.57 | wps 20485.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 13241 | lr 0.000274814 | gnorm 0.965 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 42241
2022-03-07 00:25:58 | INFO | fairseq.trainer | begin training epoch 69
2022-03-07 00:25:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:29:02 | INFO | train_inner | epoch 069:     59 / 196 loss=3.79, nll_loss=2.898, ppl=7.45, wps=20406.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=13300, lr=0.000274204, gnorm=0.952, loss_scale=8, train_wall=289, gb_free=19.9, wall=42425
2022-03-07 00:30:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:34:17 | INFO | train_inner | epoch 069:    160 / 196 loss=3.806, nll_loss=2.916, ppl=7.55, wps=20808.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=13400, lr=0.000273179, gnorm=0.972, loss_scale=8, train_wall=293, gb_free=19.9, wall=42740
2022-03-07 00:36:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:36:14 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 9.542 | nll_loss 8.89 | ppl 474.55 | wps 40670.1 | wpb 510.9 | bsz 1 | num_updates 13436 | best_loss 7.384
2022-03-07 00:36:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 13436 updates
2022-03-07 00:36:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 00:36:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 00:36:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 69 @ 13436 updates, score 9.542) (writing took 3.6446157270111144 seconds)
2022-03-07 00:36:17 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2022-03-07 00:36:17 | INFO | train | epoch 069 | loss 3.793 | nll_loss 2.902 | ppl 7.47 | wps 20594.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13436 | lr 0.000272813 | gnorm 0.956 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 42860
2022-03-07 00:36:18 | INFO | fairseq.trainer | begin training epoch 70
2022-03-07 00:36:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:39:37 | INFO | train_inner | epoch 070:     64 / 196 loss=3.767, nll_loss=2.873, ppl=7.33, wps=20396.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=13500, lr=0.000272166, gnorm=0.95, loss_scale=16, train_wall=289, gb_free=19.9, wall=43060
2022-03-07 00:44:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:44:52 | INFO | train_inner | epoch 070:    165 / 196 loss=3.794, nll_loss=2.902, ppl=7.47, wps=20800.7, ups=0.32, wpb=65536, bsz=128, num_updates=13600, lr=0.000271163, gnorm=0.965, loss_scale=16, train_wall=293, gb_free=19.9, wall=43375
2022-03-07 00:46:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:46:34 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 9.495 | nll_loss 8.845 | ppl 459.96 | wps 40806.5 | wpb 510.9 | bsz 1 | num_updates 13631 | best_loss 7.384
2022-03-07 00:46:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 13631 updates
2022-03-07 00:46:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 00:46:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 00:46:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 70 @ 13631 updates, score 9.495) (writing took 3.6715719080530107 seconds)
2022-03-07 00:46:38 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2022-03-07 00:46:38 | INFO | train | epoch 070 | loss 3.775 | nll_loss 2.881 | ppl 7.37 | wps 20583.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 13631 | lr 0.000270855 | gnorm 0.965 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 43480
2022-03-07 00:46:38 | INFO | fairseq.trainer | begin training epoch 71
2022-03-07 00:46:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 00:50:13 | INFO | train_inner | epoch 071:     69 / 196 loss=3.743, nll_loss=2.847, ppl=7.19, wps=20390.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=13700, lr=0.000270172, gnorm=0.962, loss_scale=16, train_wall=289, gb_free=19.9, wall=43696
2022-03-07 00:51:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 00:52:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 00:55:31 | INFO | train_inner | epoch 071:    171 / 196 loss=3.779, nll_loss=2.886, ppl=7.39, wps=20605.6, ups=0.31, wpb=65532.4, bsz=128, num_updates=13800, lr=0.000269191, gnorm=0.968, loss_scale=8, train_wall=296, gb_free=19.9, wall=44014
2022-03-07 00:56:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 00:56:54 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 9.578 | nll_loss 8.933 | ppl 488.72 | wps 40673.3 | wpb 510.9 | bsz 1 | num_updates 13825 | best_loss 7.384
2022-03-07 00:56:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 13825 updates
2022-03-07 00:56:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 00:56:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 00:56:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 71 @ 13825 updates, score 9.578) (writing took 3.648133987095207 seconds)
2022-03-07 00:56:57 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2022-03-07 00:56:57 | INFO | train | epoch 071 | loss 3.757 | nll_loss 2.862 | ppl 7.27 | wps 20484.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 13825 | lr 0.000268947 | gnorm 0.963 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 44100
2022-03-07 00:56:57 | INFO | fairseq.trainer | begin training epoch 72
2022-03-07 00:56:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:00:51 | INFO | train_inner | epoch 072:     75 / 196 loss=3.72, nll_loss=2.822, ppl=7.07, wps=20402.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=13900, lr=0.000268221, gnorm=0.978, loss_scale=16, train_wall=289, gb_free=19.9, wall=44334
2022-03-07 01:03:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:06:06 | INFO | train_inner | epoch 072:    176 / 196 loss=3.766, nll_loss=2.871, ppl=7.32, wps=20803.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=14000, lr=0.000267261, gnorm=0.969, loss_scale=8, train_wall=293, gb_free=19.9, wall=44649
2022-03-07 01:07:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:07:13 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 9.613 | nll_loss 8.969 | ppl 501.27 | wps 40849 | wpb 510.9 | bsz 1 | num_updates 14020 | best_loss 7.384
2022-03-07 01:07:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 14020 updates
2022-03-07 01:07:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 01:07:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 01:07:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 72 @ 14020 updates, score 9.613) (writing took 3.6484328659716994 seconds)
2022-03-07 01:07:17 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2022-03-07 01:07:17 | INFO | train | epoch 072 | loss 3.74 | nll_loss 2.844 | ppl 7.18 | wps 20593.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14020 | lr 0.000267071 | gnorm 0.974 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 44720
2022-03-07 01:07:17 | INFO | fairseq.trainer | begin training epoch 73
2022-03-07 01:07:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:11:27 | INFO | train_inner | epoch 073:     80 / 196 loss=3.702, nll_loss=2.802, ppl=6.98, wps=20404, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14100, lr=0.000266312, gnorm=0.953, loss_scale=16, train_wall=289, gb_free=19.9, wall=44970
2022-03-07 01:16:39 | INFO | train_inner | epoch 073:    180 / 196 loss=3.755, nll_loss=2.86, ppl=7.26, wps=21006.1, ups=0.32, wpb=65536, bsz=128, num_updates=14200, lr=0.000265372, gnorm=0.967, loss_scale=16, train_wall=290, gb_free=19.9, wall=45282
2022-03-07 01:17:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 01:17:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:17:33 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 9.601 | nll_loss 8.947 | ppl 493.68 | wps 40852 | wpb 510.9 | bsz 1 | num_updates 14215 | best_loss 7.384
2022-03-07 01:17:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 14215 updates
2022-03-07 01:17:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 01:17:37 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 01:17:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 73 @ 14215 updates, score 9.601) (writing took 3.6563807437196374 seconds)
2022-03-07 01:17:37 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2022-03-07 01:17:37 | INFO | train | epoch 073 | loss 3.723 | nll_loss 2.825 | ppl 7.09 | wps 20587.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14215 | lr 0.000265232 | gnorm 0.959 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 45340
2022-03-07 01:17:37 | INFO | fairseq.trainer | begin training epoch 74
2022-03-07 01:17:37 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:21:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:22:05 | INFO | train_inner | epoch 074:     86 / 196 loss=3.675, nll_loss=2.773, ppl=6.84, wps=20017, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14300, lr=0.000264443, gnorm=0.97, loss_scale=8, train_wall=295, gb_free=19.9, wall=45608
2022-03-07 01:27:17 | INFO | train_inner | epoch 074:    186 / 196 loss=3.746, nll_loss=2.85, ppl=7.21, wps=21022.8, ups=0.32, wpb=65536, bsz=128, num_updates=14400, lr=0.000263523, gnorm=0.984, loss_scale=8, train_wall=290, gb_free=19.9, wall=45920
2022-03-07 01:27:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:27:53 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 9.682 | nll_loss 9.039 | ppl 525.94 | wps 40838.5 | wpb 510.9 | bsz 1 | num_updates 14410 | best_loss 7.384
2022-03-07 01:27:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 14410 updates
2022-03-07 01:27:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 01:27:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 01:27:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 74 @ 14410 updates, score 9.682) (writing took 3.6376828611828387 seconds)
2022-03-07 01:27:57 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2022-03-07 01:27:57 | INFO | train | epoch 074 | loss 3.707 | nll_loss 2.808 | ppl 7 | wps 20599.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14410 | lr 0.000263432 | gnorm 0.976 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 45959
2022-03-07 01:27:57 | INFO | fairseq.trainer | begin training epoch 75
2022-03-07 01:27:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:28:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:32:41 | INFO | train_inner | epoch 075:     91 / 196 loss=3.654, nll_loss=2.75, ppl=6.73, wps=20207.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=14500, lr=0.000262613, gnorm=0.988, loss_scale=8, train_wall=292, gb_free=19.9, wall=46243
2022-03-07 01:37:52 | INFO | train_inner | epoch 075:    191 / 196 loss=3.734, nll_loss=2.837, ppl=7.15, wps=21016.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=14600, lr=0.000261712, gnorm=0.98, loss_scale=16, train_wall=290, gb_free=19.9, wall=46555
2022-03-07 01:38:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:38:13 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 9.692 | nll_loss 9.048 | ppl 529.33 | wps 40819.4 | wpb 510.9 | bsz 1 | num_updates 14605 | best_loss 7.384
2022-03-07 01:38:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 14605 updates
2022-03-07 01:38:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 01:38:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 01:38:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 75 @ 14605 updates, score 9.692) (writing took 3.673493533860892 seconds)
2022-03-07 01:38:16 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2022-03-07 01:38:16 | INFO | train | epoch 075 | loss 3.691 | nll_loss 2.79 | ppl 6.92 | wps 20591.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14605 | lr 0.000261667 | gnorm 0.986 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 46579
2022-03-07 01:38:16 | INFO | fairseq.trainer | begin training epoch 76
2022-03-07 01:38:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:42:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 01:43:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 01:43:19 | INFO | train_inner | epoch 076:     97 / 196 loss=3.643, nll_loss=2.738, ppl=6.67, wps=20012, ups=0.31, wpb=65367, bsz=127.7, num_updates=14700, lr=0.00026082, gnorm=0.977, loss_scale=8, train_wall=295, gb_free=19.9, wall=46882
2022-03-07 01:48:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:48:32 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 9.717 | nll_loss 9.069 | ppl 536.9 | wps 40886.4 | wpb 510.9 | bsz 1 | num_updates 14799 | best_loss 7.384
2022-03-07 01:48:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 14799 updates
2022-03-07 01:48:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 01:48:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 01:48:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 76 @ 14799 updates, score 9.717) (writing took 3.6499317889101803 seconds)
2022-03-07 01:48:36 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2022-03-07 01:48:36 | INFO | train | epoch 076 | loss 3.677 | nll_loss 2.775 | ppl 6.85 | wps 20486.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 14799 | lr 0.000259946 | gnorm 0.983 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 47199
2022-03-07 01:48:36 | INFO | fairseq.trainer | begin training epoch 77
2022-03-07 01:48:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:48:39 | INFO | train_inner | epoch 077:      1 / 196 loss=3.715, nll_loss=2.816, ppl=7.04, wps=20407.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=14800, lr=0.000259938, gnorm=0.992, loss_scale=8, train_wall=289, gb_free=19.9, wall=47202
2022-03-07 01:53:52 | INFO | train_inner | epoch 077:    101 / 196 loss=3.617, nll_loss=2.711, ppl=6.55, wps=20986.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=14900, lr=0.000259064, gnorm=0.97, loss_scale=16, train_wall=290, gb_free=19.9, wall=47514
2022-03-07 01:56:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 01:58:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 01:58:53 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 9.783 | nll_loss 9.135 | ppl 562.17 | wps 40103.7 | wpb 510.9 | bsz 1 | num_updates 14994 | best_loss 7.384
2022-03-07 01:58:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 14994 updates
2022-03-07 01:58:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 01:58:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 01:58:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 77 @ 14994 updates, score 9.783) (writing took 3.653497568331659 seconds)
2022-03-07 01:58:56 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2022-03-07 01:58:56 | INFO | train | epoch 077 | loss 3.662 | nll_loss 2.759 | ppl 6.77 | wps 20571.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 14994 | lr 0.000258251 | gnorm 0.989 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 47819
2022-03-07 01:58:56 | INFO | fairseq.trainer | begin training epoch 78
2022-03-07 01:58:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 01:59:15 | INFO | train_inner | epoch 078:      6 / 196 loss=3.701, nll_loss=2.801, ppl=6.97, wps=20194, ups=0.31, wpb=65367, bsz=127.7, num_updates=15000, lr=0.000258199, gnorm=1.005, loss_scale=16, train_wall=292, gb_free=19.9, wall=47838
2022-03-07 02:01:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:04:30 | INFO | train_inner | epoch 078:    107 / 196 loss=3.604, nll_loss=2.696, ppl=6.48, wps=20794, ups=0.32, wpb=65532.4, bsz=128, num_updates=15100, lr=0.000257343, gnorm=0.983, loss_scale=8, train_wall=293, gb_free=19.9, wall=48153
2022-03-07 02:09:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:09:13 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 9.783 | nll_loss 9.137 | ppl 562.83 | wps 40496.9 | wpb 510.9 | bsz 1 | num_updates 15189 | best_loss 7.384
2022-03-07 02:09:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 15189 updates
2022-03-07 02:09:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 02:09:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 02:09:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 78 @ 15189 updates, score 9.783) (writing took 3.618367917370051 seconds)
2022-03-07 02:09:16 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2022-03-07 02:09:16 | INFO | train | epoch 078 | loss 3.647 | nll_loss 2.742 | ppl 6.69 | wps 20589.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15189 | lr 0.000256587 | gnorm 0.986 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 48439
2022-03-07 02:09:16 | INFO | fairseq.trainer | begin training epoch 79
2022-03-07 02:09:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:09:51 | INFO | train_inner | epoch 079:     11 / 196 loss=3.684, nll_loss=2.783, ppl=6.88, wps=20407.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=15200, lr=0.000256495, gnorm=0.99, loss_scale=16, train_wall=289, gb_free=19.9, wall=48474
2022-03-07 02:14:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:15:06 | INFO | train_inner | epoch 079:    112 / 196 loss=3.595, nll_loss=2.686, ppl=6.43, wps=20793, ups=0.32, wpb=65536, bsz=128, num_updates=15300, lr=0.000255655, gnorm=0.977, loss_scale=16, train_wall=293, gb_free=19.9, wall=48789
2022-03-07 02:17:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:19:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:19:33 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 9.829 | nll_loss 9.186 | ppl 582.39 | wps 40798 | wpb 510.9 | bsz 1 | num_updates 15383 | best_loss 7.384
2022-03-07 02:19:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 15383 updates
2022-03-07 02:19:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 02:19:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 02:19:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 79 @ 15383 updates, score 9.829) (writing took 3.652242199052125 seconds)
2022-03-07 02:19:36 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2022-03-07 02:19:36 | INFO | train | epoch 079 | loss 3.632 | nll_loss 2.726 | ppl 6.62 | wps 20480.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 15383 | lr 0.000254964 | gnorm 0.993 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 49059
2022-03-07 02:19:36 | INFO | fairseq.trainer | begin training epoch 80
2022-03-07 02:19:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:20:29 | INFO | train_inner | epoch 080:     17 / 196 loss=3.661, nll_loss=2.757, ppl=6.76, wps=20204.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15400, lr=0.000254824, gnorm=1.009, loss_scale=8, train_wall=292, gb_free=19.9, wall=49112
2022-03-07 02:25:42 | INFO | train_inner | epoch 080:    117 / 196 loss=3.588, nll_loss=2.679, ppl=6.4, wps=20997.3, ups=0.32, wpb=65536, bsz=128, num_updates=15500, lr=0.000254, gnorm=0.996, loss_scale=16, train_wall=290, gb_free=19.9, wall=49424
2022-03-07 02:29:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:29:53 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 9.838 | nll_loss 9.201 | ppl 588.6 | wps 40821.1 | wpb 510.9 | bsz 1 | num_updates 15579 | best_loss 7.384
2022-03-07 02:29:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 15579 updates
2022-03-07 02:29:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 02:29:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 02:29:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 80 @ 15579 updates, score 9.838) (writing took 3.6371625061146915 seconds)
2022-03-07 02:29:56 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2022-03-07 02:29:56 | INFO | train | epoch 080 | loss 3.618 | nll_loss 2.711 | ppl 6.55 | wps 20691.8 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 15579 | lr 0.000253355 | gnorm 1.001 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 49679
2022-03-07 02:29:56 | INFO | fairseq.trainer | begin training epoch 81
2022-03-07 02:29:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:30:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:31:05 | INFO | train_inner | epoch 081:     22 / 196 loss=3.65, nll_loss=2.746, ppl=6.71, wps=20211.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15600, lr=0.000253185, gnorm=1.016, loss_scale=8, train_wall=292, gb_free=19.9, wall=49748
2022-03-07 02:36:17 | INFO | train_inner | epoch 081:    122 / 196 loss=3.578, nll_loss=2.667, ppl=6.35, wps=21002.8, ups=0.32, wpb=65536, bsz=128, num_updates=15700, lr=0.000252377, gnorm=0.999, loss_scale=8, train_wall=290, gb_free=19.9, wall=50060
2022-03-07 02:40:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:40:12 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 9.915 | nll_loss 9.275 | ppl 619.38 | wps 40816 | wpb 510.9 | bsz 1 | num_updates 15774 | best_loss 7.384
2022-03-07 02:40:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 15774 updates
2022-03-07 02:40:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 02:40:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 02:40:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 81 @ 15774 updates, score 9.915) (writing took 3.672792684286833 seconds)
2022-03-07 02:40:16 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2022-03-07 02:40:16 | INFO | train | epoch 081 | loss 3.604 | nll_loss 2.696 | ppl 6.48 | wps 20583.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15774 | lr 0.000251785 | gnorm 1.005 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 50299
2022-03-07 02:40:16 | INFO | fairseq.trainer | begin training epoch 82
2022-03-07 02:40:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:41:37 | INFO | train_inner | epoch 082:     26 / 196 loss=3.619, nll_loss=2.712, ppl=6.55, wps=20395.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=15800, lr=0.000251577, gnorm=0.999, loss_scale=16, train_wall=289, gb_free=19.9, wall=50380
2022-03-07 02:41:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 02:46:52 | INFO | train_inner | epoch 082:    127 / 196 loss=3.569, nll_loss=2.657, ppl=6.31, wps=20805.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=15900, lr=0.000250785, gnorm=0.983, loss_scale=8, train_wall=293, gb_free=19.9, wall=50695
2022-03-07 02:50:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 02:50:32 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 9.851 | nll_loss 9.204 | ppl 589.61 | wps 41104.1 | wpb 510.9 | bsz 1 | num_updates 15969 | best_loss 7.384
2022-03-07 02:50:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 15969 updates
2022-03-07 02:50:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 02:50:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 02:50:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 82 @ 15969 updates, score 9.851) (writing took 3.490470173768699 seconds)
2022-03-07 02:50:36 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2022-03-07 02:50:36 | INFO | train | epoch 082 | loss 3.59 | nll_loss 2.681 | ppl 6.41 | wps 20597.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 15969 | lr 0.000250243 | gnorm 0.991 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 50919
2022-03-07 02:50:36 | INFO | fairseq.trainer | begin training epoch 83
2022-03-07 02:50:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 02:52:13 | INFO | train_inner | epoch 083:     31 / 196 loss=3.6, nll_loss=2.692, ppl=6.46, wps=20413.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=16000, lr=0.00025, gnorm=1, loss_scale=16, train_wall=289, gb_free=19.9, wall=51015
2022-03-07 02:55:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 02:57:28 | INFO | train_inner | epoch 083:    132 / 196 loss=3.565, nll_loss=2.653, ppl=6.29, wps=20797.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=16100, lr=0.000249222, gnorm=1.003, loss_scale=16, train_wall=293, gb_free=19.9, wall=51331
2022-03-07 03:00:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:00:52 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 9.907 | nll_loss 9.268 | ppl 616.36 | wps 40737.2 | wpb 510.9 | bsz 1 | num_updates 16164 | best_loss 7.384
2022-03-07 03:00:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 16164 updates
2022-03-07 03:00:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 03:00:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 03:00:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 83 @ 16164 updates, score 9.907) (writing took 3.520481341984123 seconds)
2022-03-07 03:00:56 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2022-03-07 03:00:56 | INFO | train | epoch 083 | loss 3.578 | nll_loss 2.668 | ppl 6.35 | wps 20588.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16164 | lr 0.000248729 | gnorm 1.005 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 51538
2022-03-07 03:00:56 | INFO | fairseq.trainer | begin training epoch 84
2022-03-07 03:00:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:02:48 | INFO | train_inner | epoch 084:     36 / 196 loss=3.58, nll_loss=2.67, ppl=6.36, wps=20402.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=16200, lr=0.000248452, gnorm=1.018, loss_scale=32, train_wall=289, gb_free=19.9, wall=51651
2022-03-07 03:03:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:03:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:08:06 | INFO | train_inner | epoch 084:    138 / 196 loss=3.563, nll_loss=2.651, ppl=6.28, wps=20595.4, ups=0.31, wpb=65532.4, bsz=128, num_updates=16300, lr=0.000247689, gnorm=1.015, loss_scale=8, train_wall=296, gb_free=19.9, wall=51969
2022-03-07 03:11:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:11:12 | INFO | valid | epoch 084 | valid on 'valid' subset | loss 9.93 | nll_loss 9.284 | ppl 623.44 | wps 40926.6 | wpb 510.9 | bsz 1 | num_updates 16358 | best_loss 7.384
2022-03-07 03:11:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 84 @ 16358 updates
2022-03-07 03:11:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 03:11:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 03:11:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 84 @ 16358 updates, score 9.93) (writing took 3.5237895189784467 seconds)
2022-03-07 03:11:15 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)
2022-03-07 03:11:15 | INFO | train | epoch 084 | loss 3.565 | nll_loss 2.653 | ppl 6.29 | wps 20488.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 16358 | lr 0.000247249 | gnorm 1.02 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 52158
2022-03-07 03:11:15 | INFO | fairseq.trainer | begin training epoch 85
2022-03-07 03:11:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:13:27 | INFO | train_inner | epoch 085:     42 / 196 loss=3.564, nll_loss=2.653, ppl=6.29, wps=20412.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=16400, lr=0.000246932, gnorm=1.01, loss_scale=16, train_wall=289, gb_free=19.9, wall=52289
2022-03-07 03:17:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:18:42 | INFO | train_inner | epoch 085:    143 / 196 loss=3.551, nll_loss=2.638, ppl=6.22, wps=20791.3, ups=0.32, wpb=65536, bsz=128, num_updates=16500, lr=0.000246183, gnorm=1.007, loss_scale=16, train_wall=293, gb_free=19.9, wall=52605
2022-03-07 03:21:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:21:32 | INFO | valid | epoch 085 | valid on 'valid' subset | loss 9.993 | nll_loss 9.349 | ppl 652.32 | wps 40746.5 | wpb 510.9 | bsz 1 | num_updates 16553 | best_loss 7.384
2022-03-07 03:21:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 85 @ 16553 updates
2022-03-07 03:21:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 03:21:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 03:21:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 85 @ 16553 updates, score 9.993) (writing took 3.527867831289768 seconds)
2022-03-07 03:21:35 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)
2022-03-07 03:21:35 | INFO | train | epoch 085 | loss 3.552 | nll_loss 2.639 | ppl 6.23 | wps 20586.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16553 | lr 0.000245789 | gnorm 1.006 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 52778
2022-03-07 03:21:35 | INFO | fairseq.trainer | begin training epoch 86
2022-03-07 03:21:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:24:02 | INFO | train_inner | epoch 086:     47 / 196 loss=3.542, nll_loss=2.628, ppl=6.18, wps=20400.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=16600, lr=0.00024544, gnorm=1.011, loss_scale=16, train_wall=289, gb_free=19.9, wall=52925
2022-03-07 03:24:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:26:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 03:29:20 | INFO | train_inner | epoch 086:    149 / 196 loss=3.541, nll_loss=2.628, ppl=6.18, wps=20602.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=16700, lr=0.000244704, gnorm=1.014, loss_scale=8, train_wall=296, gb_free=19.9, wall=53243
2022-03-07 03:31:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:31:52 | INFO | valid | epoch 086 | valid on 'valid' subset | loss 9.977 | nll_loss 9.338 | ppl 647.17 | wps 40698.1 | wpb 510.9 | bsz 1 | num_updates 16747 | best_loss 7.384
2022-03-07 03:31:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 86 @ 16747 updates
2022-03-07 03:31:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 03:31:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 03:31:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 86 @ 16747 updates, score 9.977) (writing took 3.5083041191101074 seconds)
2022-03-07 03:31:55 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)
2022-03-07 03:31:55 | INFO | train | epoch 086 | loss 3.539 | nll_loss 2.625 | ppl 6.17 | wps 20486.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 16747 | lr 0.000244361 | gnorm 1.01 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 53398
2022-03-07 03:31:55 | INFO | fairseq.trainer | begin training epoch 87
2022-03-07 03:31:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:34:41 | INFO | train_inner | epoch 087:     53 / 196 loss=3.532, nll_loss=2.618, ppl=6.14, wps=20408.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=16800, lr=0.000243975, gnorm=1.013, loss_scale=16, train_wall=289, gb_free=19.9, wall=53563
2022-03-07 03:39:53 | INFO | train_inner | epoch 087:    153 / 196 loss=3.53, nll_loss=2.615, ppl=6.13, wps=21003.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=16900, lr=0.000243252, gnorm=1.021, loss_scale=16, train_wall=290, gb_free=19.9, wall=53875
2022-03-07 03:39:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:42:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:42:11 | INFO | valid | epoch 087 | valid on 'valid' subset | loss 10.047 | nll_loss 9.41 | ppl 680.29 | wps 40544.6 | wpb 510.9 | bsz 1 | num_updates 16942 | best_loss 7.384
2022-03-07 03:42:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 87 @ 16942 updates
2022-03-07 03:42:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 03:42:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 03:42:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 87 @ 16942 updates, score 10.047) (writing took 3.547573385294527 seconds)
2022-03-07 03:42:15 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)
2022-03-07 03:42:15 | INFO | train | epoch 087 | loss 3.529 | nll_loss 2.614 | ppl 6.12 | wps 20590.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 16942 | lr 0.00024295 | gnorm 1.02 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 54018
2022-03-07 03:42:15 | INFO | fairseq.trainer | begin training epoch 88
2022-03-07 03:42:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:45:16 | INFO | train_inner | epoch 088:     58 / 196 loss=3.516, nll_loss=2.601, ppl=6.07, wps=20205.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=17000, lr=0.000242536, gnorm=1.021, loss_scale=16, train_wall=292, gb_free=19.9, wall=54199
2022-03-07 03:46:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:50:31 | INFO | train_inner | epoch 088:    159 / 196 loss=3.526, nll_loss=2.611, ppl=6.11, wps=20787.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=17100, lr=0.000241825, gnorm=1.022, loss_scale=16, train_wall=293, gb_free=19.9, wall=54514
2022-03-07 03:52:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 03:52:31 | INFO | valid | epoch 088 | valid on 'valid' subset | loss 10.077 | nll_loss 9.439 | ppl 694.27 | wps 40771.8 | wpb 510.9 | bsz 1 | num_updates 17137 | best_loss 7.384
2022-03-07 03:52:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 88 @ 17137 updates
2022-03-07 03:52:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 03:52:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 03:52:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 88 @ 17137 updates, score 10.077) (writing took 3.5266302661038935 seconds)
2022-03-07 03:52:35 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)
2022-03-07 03:52:35 | INFO | train | epoch 088 | loss 3.517 | nll_loss 2.601 | ppl 6.07 | wps 20581 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 17137 | lr 0.000241564 | gnorm 1.027 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 54638
2022-03-07 03:52:35 | INFO | fairseq.trainer | begin training epoch 89
2022-03-07 03:52:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 03:53:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 03:55:55 | INFO | train_inner | epoch 089:     64 / 196 loss=3.497, nll_loss=2.58, ppl=5.98, wps=20204, ups=0.31, wpb=65367, bsz=127.7, num_updates=17200, lr=0.000241121, gnorm=1.037, loss_scale=16, train_wall=292, gb_free=19.9, wall=54838
2022-03-07 04:00:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:01:10 | INFO | train_inner | epoch 089:    165 / 196 loss=3.514, nll_loss=2.598, ppl=6.06, wps=20798.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=17300, lr=0.000240424, gnorm=1.032, loss_scale=16, train_wall=293, gb_free=19.9, wall=55153
2022-03-07 04:01:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:02:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:02:51 | INFO | valid | epoch 089 | valid on 'valid' subset | loss 10.068 | nll_loss 9.431 | ppl 690.04 | wps 40580 | wpb 510.9 | bsz 1 | num_updates 17330 | best_loss 7.384
2022-03-07 04:02:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 89 @ 17330 updates
2022-03-07 04:02:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 04:02:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 04:02:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 89 @ 17330 updates, score 10.068) (writing took 3.512828948907554 seconds)
2022-03-07 04:02:55 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)
2022-03-07 04:02:55 | INFO | train | epoch 089 | loss 3.503 | nll_loss 2.586 | ppl 6 | wps 20379.6 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 17330 | lr 0.000240215 | gnorm 1.034 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 55258
2022-03-07 04:02:55 | INFO | fairseq.trainer | begin training epoch 90
2022-03-07 04:02:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:06:33 | INFO | train_inner | epoch 090:     70 / 196 loss=3.483, nll_loss=2.564, ppl=5.91, wps=20211.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=17400, lr=0.000239732, gnorm=1.038, loss_scale=8, train_wall=292, gb_free=19.9, wall=55476
2022-03-07 04:11:46 | INFO | train_inner | epoch 090:    170 / 196 loss=3.517, nll_loss=2.602, ppl=6.07, wps=20991.2, ups=0.32, wpb=65536, bsz=128, num_updates=17500, lr=0.000239046, gnorm=1.014, loss_scale=16, train_wall=290, gb_free=19.9, wall=55788
2022-03-07 04:13:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:13:11 | INFO | valid | epoch 090 | valid on 'valid' subset | loss 10.051 | nll_loss 9.416 | ppl 682.9 | wps 40881.3 | wpb 510.9 | bsz 1 | num_updates 17526 | best_loss 7.384
2022-03-07 04:13:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 90 @ 17526 updates
2022-03-07 04:13:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 04:13:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 04:13:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 90 @ 17526 updates, score 10.051) (writing took 3.6920623402111232 seconds)
2022-03-07 04:13:15 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)
2022-03-07 04:13:15 | INFO | train | epoch 090 | loss 3.494 | nll_loss 2.576 | ppl 5.96 | wps 20680.4 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 17526 | lr 0.000238868 | gnorm 1.021 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 55878
2022-03-07 04:13:15 | INFO | fairseq.trainer | begin training epoch 91
2022-03-07 04:13:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:15:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:17:10 | INFO | train_inner | epoch 091:     75 / 196 loss=3.465, nll_loss=2.544, ppl=5.83, wps=20168.8, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=17600, lr=0.000238366, gnorm=1.023, loss_scale=16, train_wall=292, gb_free=19.9, wall=56112
2022-03-07 04:18:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:22:25 | INFO | train_inner | epoch 091:    176 / 196 loss=3.507, nll_loss=2.59, ppl=6.02, wps=20781.7, ups=0.32, wpb=65536, bsz=128, num_updates=17700, lr=0.000237691, gnorm=1.023, loss_scale=8, train_wall=293, gb_free=19.9, wall=56428
2022-03-07 04:23:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:23:32 | INFO | valid | epoch 091 | valid on 'valid' subset | loss 10.134 | nll_loss 9.498 | ppl 722.88 | wps 40765.7 | wpb 510.9 | bsz 1 | num_updates 17720 | best_loss 7.384
2022-03-07 04:23:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 91 @ 17720 updates
2022-03-07 04:23:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 04:23:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 04:23:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 91 @ 17720 updates, score 10.134) (writing took 3.6936454568058252 seconds)
2022-03-07 04:23:36 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)
2022-03-07 04:23:36 | INFO | train | epoch 091 | loss 3.483 | nll_loss 2.564 | ppl 5.91 | wps 20455.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 17720 | lr 0.000237557 | gnorm 1.022 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 56499
2022-03-07 04:23:36 | INFO | fairseq.trainer | begin training epoch 92
2022-03-07 04:23:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:27:46 | INFO | train_inner | epoch 092:     80 / 196 loss=3.445, nll_loss=2.524, ppl=5.75, wps=20370.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=17800, lr=0.000237023, gnorm=1.024, loss_scale=16, train_wall=290, gb_free=19.9, wall=56749
2022-03-07 04:32:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:33:01 | INFO | train_inner | epoch 092:    181 / 196 loss=3.501, nll_loss=2.584, ppl=5.99, wps=20774.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=17900, lr=0.00023636, gnorm=1.015, loss_scale=16, train_wall=293, gb_free=19.9, wall=57064
2022-03-07 04:33:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:33:53 | INFO | valid | epoch 092 | valid on 'valid' subset | loss 10.136 | nll_loss 9.501 | ppl 724.75 | wps 40553.6 | wpb 510.9 | bsz 1 | num_updates 17915 | best_loss 7.384
2022-03-07 04:33:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 92 @ 17915 updates
2022-03-07 04:33:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 04:33:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 04:33:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 92 @ 17915 updates, score 10.136) (writing took 3.3747088089585304 seconds)
2022-03-07 04:33:56 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)
2022-03-07 04:33:56 | INFO | train | epoch 092 | loss 3.472 | nll_loss 2.552 | ppl 5.87 | wps 20570.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 17915 | lr 0.000236261 | gnorm 1.021 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 57119
2022-03-07 04:33:56 | INFO | fairseq.trainer | begin training epoch 93
2022-03-07 04:33:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:38:22 | INFO | train_inner | epoch 093:     85 / 196 loss=3.433, nll_loss=2.51, ppl=5.7, wps=20408.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18000, lr=0.000235702, gnorm=1.043, loss_scale=16, train_wall=289, gb_free=19.9, wall=57384
2022-03-07 04:39:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 04:41:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:43:40 | INFO | train_inner | epoch 093:    187 / 196 loss=3.494, nll_loss=2.576, ppl=5.96, wps=20600.3, ups=0.31, wpb=65536, bsz=128, num_updates=18100, lr=0.00023505, gnorm=1.041, loss_scale=8, train_wall=296, gb_free=19.9, wall=57703
2022-03-07 04:44:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:44:12 | INFO | valid | epoch 093 | valid on 'valid' subset | loss 10.15 | nll_loss 9.511 | ppl 729.86 | wps 40766.7 | wpb 510.9 | bsz 1 | num_updates 18109 | best_loss 7.384
2022-03-07 04:44:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 93 @ 18109 updates
2022-03-07 04:44:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 04:44:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 04:44:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 93 @ 18109 updates, score 10.15) (writing took 3.5721211340278387 seconds)
2022-03-07 04:44:16 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)
2022-03-07 04:44:16 | INFO | train | epoch 093 | loss 3.46 | nll_loss 2.54 | ppl 5.81 | wps 20483.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 18109 | lr 0.000234992 | gnorm 1.043 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 57739
2022-03-07 04:44:16 | INFO | fairseq.trainer | begin training epoch 94
2022-03-07 04:44:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:49:00 | INFO | train_inner | epoch 094:     91 / 196 loss=3.417, nll_loss=2.493, ppl=5.63, wps=20402.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18200, lr=0.000234404, gnorm=1.027, loss_scale=16, train_wall=289, gb_free=19.9, wall=58023
2022-03-07 04:50:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 04:54:15 | INFO | train_inner | epoch 094:    192 / 196 loss=3.491, nll_loss=2.573, ppl=5.95, wps=20801.4, ups=0.32, wpb=65536, bsz=128, num_updates=18300, lr=0.000233762, gnorm=1.054, loss_scale=8, train_wall=293, gb_free=19.9, wall=58338
2022-03-07 04:54:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 04:54:32 | INFO | valid | epoch 094 | valid on 'valid' subset | loss 10.187 | nll_loss 9.553 | ppl 751.43 | wps 40825.6 | wpb 510.9 | bsz 1 | num_updates 18304 | best_loss 7.384
2022-03-07 04:54:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 94 @ 18304 updates
2022-03-07 04:54:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 04:54:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 04:54:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 94 @ 18304 updates, score 10.187) (writing took 3.560656256042421 seconds)
2022-03-07 04:54:36 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)
2022-03-07 04:54:36 | INFO | train | epoch 094 | loss 3.451 | nll_loss 2.53 | ppl 5.78 | wps 20591.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18304 | lr 0.000233737 | gnorm 1.039 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 58359
2022-03-07 04:54:36 | INFO | fairseq.trainer | begin training epoch 95
2022-03-07 04:54:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 04:59:36 | INFO | train_inner | epoch 095:     96 / 196 loss=3.409, nll_loss=2.484, ppl=5.59, wps=20403, ups=0.31, wpb=65367, bsz=127.7, num_updates=18400, lr=0.000233126, gnorm=1.03, loss_scale=16, train_wall=289, gb_free=19.9, wall=58658
2022-03-07 05:04:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:04:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:04:52 | INFO | valid | epoch 095 | valid on 'valid' subset | loss 10.201 | nll_loss 9.562 | ppl 756.1 | wps 40847.1 | wpb 510.9 | bsz 1 | num_updates 18499 | best_loss 7.384
2022-03-07 05:04:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 95 @ 18499 updates
2022-03-07 05:04:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 05:04:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 05:04:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 95 @ 18499 updates, score 10.201) (writing took 3.5599286961369216 seconds)
2022-03-07 05:04:56 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)
2022-03-07 05:04:56 | INFO | train | epoch 095 | loss 3.44 | nll_loss 2.518 | ppl 5.73 | wps 20593.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18499 | lr 0.000232502 | gnorm 1.035 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 58978
2022-03-07 05:04:56 | INFO | fairseq.trainer | begin training epoch 96
2022-03-07 05:04:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:04:59 | INFO | train_inner | epoch 096:      1 / 196 loss=3.474, nll_loss=2.555, ppl=5.88, wps=20220.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18500, lr=0.000232495, gnorm=1.041, loss_scale=16, train_wall=292, gb_free=19.9, wall=58982
2022-03-07 05:10:11 | INFO | train_inner | epoch 096:    101 / 196 loss=3.395, nll_loss=2.469, ppl=5.54, wps=20999.2, ups=0.32, wpb=65536, bsz=128, num_updates=18600, lr=0.000231869, gnorm=1.037, loss_scale=16, train_wall=290, gb_free=19.9, wall=59294
2022-03-07 05:11:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:15:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:15:12 | INFO | valid | epoch 096 | valid on 'valid' subset | loss 10.195 | nll_loss 9.562 | ppl 755.82 | wps 40780.2 | wpb 510.9 | bsz 1 | num_updates 18694 | best_loss 7.384
2022-03-07 05:15:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 96 @ 18694 updates
2022-03-07 05:15:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 05:15:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 05:15:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 96 @ 18694 updates, score 10.195) (writing took 3.5361419999971986 seconds)
2022-03-07 05:15:15 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)
2022-03-07 05:15:15 | INFO | train | epoch 096 | loss 3.432 | nll_loss 2.509 | ppl 5.69 | wps 20589 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18694 | lr 0.000231286 | gnorm 1.039 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 59598
2022-03-07 05:15:15 | INFO | fairseq.trainer | begin training epoch 97
2022-03-07 05:15:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:15:34 | INFO | train_inner | epoch 097:      6 / 196 loss=3.464, nll_loss=2.545, ppl=5.83, wps=20213.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18700, lr=0.000231249, gnorm=1.042, loss_scale=16, train_wall=292, gb_free=19.9, wall=59617
2022-03-07 05:19:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:20:50 | INFO | train_inner | epoch 097:    107 / 196 loss=3.388, nll_loss=2.461, ppl=5.51, wps=20786.1, ups=0.32, wpb=65536, bsz=128, num_updates=18800, lr=0.000230633, gnorm=1.029, loss_scale=16, train_wall=293, gb_free=19.9, wall=59932
2022-03-07 05:25:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:25:32 | INFO | valid | epoch 097 | valid on 'valid' subset | loss 10.26 | nll_loss 9.624 | ppl 789.28 | wps 40781.2 | wpb 510.9 | bsz 1 | num_updates 18889 | best_loss 7.384
2022-03-07 05:25:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 97 @ 18889 updates
2022-03-07 05:25:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 05:25:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 05:25:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 97 @ 18889 updates, score 10.26) (writing took 3.553211387246847 seconds)
2022-03-07 05:25:35 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)
2022-03-07 05:25:35 | INFO | train | epoch 097 | loss 3.422 | nll_loss 2.498 | ppl 5.65 | wps 20583.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 18889 | lr 0.000230089 | gnorm 1.046 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 60218
2022-03-07 05:25:35 | INFO | fairseq.trainer | begin training epoch 98
2022-03-07 05:25:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:26:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:26:13 | INFO | train_inner | epoch 098:     12 / 196 loss=3.451, nll_loss=2.53, ppl=5.77, wps=20209.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=18900, lr=0.000230022, gnorm=1.065, loss_scale=16, train_wall=292, gb_free=19.9, wall=60256
2022-03-07 05:31:25 | INFO | train_inner | epoch 098:    112 / 196 loss=3.377, nll_loss=2.449, ppl=5.46, wps=20991.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=19000, lr=0.000229416, gnorm=1.03, loss_scale=16, train_wall=290, gb_free=19.9, wall=60568
2022-03-07 05:32:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:35:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:35:52 | INFO | valid | epoch 098 | valid on 'valid' subset | loss 10.26 | nll_loss 9.628 | ppl 791.49 | wps 40799.8 | wpb 510.9 | bsz 1 | num_updates 19083 | best_loss 7.384
2022-03-07 05:35:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 98 @ 19083 updates
2022-03-07 05:35:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 05:35:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 05:35:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 98 @ 19083 updates, score 10.26) (writing took 3.56532746180892 seconds)
2022-03-07 05:35:55 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)
2022-03-07 05:35:55 | INFO | train | epoch 098 | loss 3.411 | nll_loss 2.486 | ppl 5.6 | wps 20481.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19083 | lr 0.000228916 | gnorm 1.04 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 60838
2022-03-07 05:35:55 | INFO | fairseq.trainer | begin training epoch 99
2022-03-07 05:35:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:36:49 | INFO | train_inner | epoch 099:     17 / 196 loss=3.441, nll_loss=2.519, ppl=5.73, wps=20213.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=19100, lr=0.000228814, gnorm=1.05, loss_scale=16, train_wall=292, gb_free=19.9, wall=60891
2022-03-07 05:40:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:42:04 | INFO | train_inner | epoch 099:    118 / 196 loss=3.373, nll_loss=2.445, ppl=5.44, wps=20791.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=19200, lr=0.000228218, gnorm=1.034, loss_scale=16, train_wall=293, gb_free=19.9, wall=61207
2022-03-07 05:42:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 05:46:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:46:12 | INFO | valid | epoch 099 | valid on 'valid' subset | loss 10.273 | nll_loss 9.643 | ppl 799.34 | wps 40739.9 | wpb 510.9 | bsz 1 | num_updates 19277 | best_loss 7.384
2022-03-07 05:46:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 99 @ 19277 updates
2022-03-07 05:46:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 05:46:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 05:46:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 99 @ 19277 updates, score 10.273) (writing took 3.566167305223644 seconds)
2022-03-07 05:46:15 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)
2022-03-07 05:46:15 | INFO | train | epoch 099 | loss 3.402 | nll_loss 2.477 | ppl 5.57 | wps 20485.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19277 | lr 0.000227761 | gnorm 1.046 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 61458
2022-03-07 05:46:15 | INFO | fairseq.trainer | begin training epoch 100
2022-03-07 05:46:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:47:27 | INFO | train_inner | epoch 100:     23 / 196 loss=3.426, nll_loss=2.502, ppl=5.67, wps=20217.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=19300, lr=0.000227626, gnorm=1.059, loss_scale=8, train_wall=292, gb_free=19.9, wall=61530
2022-03-07 05:52:39 | INFO | train_inner | epoch 100:    123 / 196 loss=3.372, nll_loss=2.444, ppl=5.44, wps=21000.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=19400, lr=0.000227038, gnorm=1.034, loss_scale=16, train_wall=290, gb_free=19.9, wall=61842
2022-03-07 05:55:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 05:56:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 05:56:32 | INFO | valid | epoch 100 | valid on 'valid' subset | loss 10.294 | nll_loss 9.658 | ppl 807.73 | wps 40712.6 | wpb 510.9 | bsz 1 | num_updates 19472 | best_loss 7.384
2022-03-07 05:56:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 19472 updates
2022-03-07 05:56:32 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 05:56:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 05:56:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 100 @ 19472 updates, score 10.294) (writing took 3.6341150416992605 seconds)
2022-03-07 05:56:35 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)
2022-03-07 05:56:35 | INFO | train | epoch 100 | loss 3.393 | nll_loss 2.467 | ppl 5.53 | wps 20583.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19472 | lr 0.000226618 | gnorm 1.047 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 62078
2022-03-07 05:56:35 | INFO | fairseq.trainer | begin training epoch 101
2022-03-07 05:56:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 05:58:03 | INFO | train_inner | epoch 101:     28 / 196 loss=3.408, nll_loss=2.483, ppl=5.59, wps=20203.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=19500, lr=0.000226455, gnorm=1.052, loss_scale=16, train_wall=292, gb_free=19.9, wall=62165
2022-03-07 06:02:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:03:18 | INFO | train_inner | epoch 101:    129 / 196 loss=3.37, nll_loss=2.442, ppl=5.43, wps=20792.1, ups=0.32, wpb=65536, bsz=128, num_updates=19600, lr=0.000225877, gnorm=1.047, loss_scale=16, train_wall=293, gb_free=19.9, wall=62481
2022-03-07 06:06:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:06:51 | INFO | valid | epoch 101 | valid on 'valid' subset | loss 10.263 | nll_loss 9.625 | ppl 789.61 | wps 40573.4 | wpb 510.9 | bsz 1 | num_updates 19667 | best_loss 7.384
2022-03-07 06:06:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 101 @ 19667 updates
2022-03-07 06:06:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 06:06:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 06:06:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 101 @ 19667 updates, score 10.263) (writing took 3.5736157088540494 seconds)
2022-03-07 06:06:55 | INFO | fairseq_cli.train | end of epoch 101 (average epoch stats below)
2022-03-07 06:06:55 | INFO | train | epoch 101 | loss 3.384 | nll_loss 2.458 | ppl 5.49 | wps 20588.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 19667 | lr 0.000225492 | gnorm 1.054 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 62698
2022-03-07 06:06:55 | INFO | fairseq.trainer | begin training epoch 102
2022-03-07 06:06:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:08:38 | INFO | train_inner | epoch 102:     33 / 196 loss=3.387, nll_loss=2.461, ppl=5.51, wps=20405.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=19700, lr=0.000225303, gnorm=1.064, loss_scale=16, train_wall=289, gb_free=19.9, wall=62801
2022-03-07 06:09:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:10:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 06:13:56 | INFO | train_inner | epoch 102:    135 / 196 loss=3.366, nll_loss=2.437, ppl=5.42, wps=20594.8, ups=0.31, wpb=65532.4, bsz=128, num_updates=19800, lr=0.000224733, gnorm=1.047, loss_scale=8, train_wall=296, gb_free=19.9, wall=63119
2022-03-07 06:17:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:17:11 | INFO | valid | epoch 102 | valid on 'valid' subset | loss 10.358 | nll_loss 9.729 | ppl 848.51 | wps 40801.1 | wpb 510.9 | bsz 1 | num_updates 19861 | best_loss 7.384
2022-03-07 06:17:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 102 @ 19861 updates
2022-03-07 06:17:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 06:17:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 06:17:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 102 @ 19861 updates, score 10.358) (writing took 3.5770030920393765 seconds)
2022-03-07 06:17:15 | INFO | fairseq_cli.train | end of epoch 102 (average epoch stats below)
2022-03-07 06:17:15 | INFO | train | epoch 102 | loss 3.375 | nll_loss 2.448 | ppl 5.46 | wps 20489.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 19861 | lr 0.000224388 | gnorm 1.055 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 63318
2022-03-07 06:17:15 | INFO | fairseq.trainer | begin training epoch 103
2022-03-07 06:17:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:19:17 | INFO | train_inner | epoch 103:     39 / 196 loss=3.387, nll_loss=2.46, ppl=5.5, wps=20411.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=19900, lr=0.000224168, gnorm=1.064, loss_scale=16, train_wall=289, gb_free=19.9, wall=63439
2022-03-07 06:24:29 | INFO | train_inner | epoch 103:    139 / 196 loss=3.36, nll_loss=2.431, ppl=5.39, wps=21002.1, ups=0.32, wpb=65536, bsz=128, num_updates=20000, lr=0.000223607, gnorm=1.062, loss_scale=32, train_wall=290, gb_free=19.9, wall=63751
2022-03-07 06:24:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:27:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:27:31 | INFO | valid | epoch 103 | valid on 'valid' subset | loss 10.301 | nll_loss 9.664 | ppl 811.05 | wps 40739.1 | wpb 510.9 | bsz 1 | num_updates 20056 | best_loss 7.384
2022-03-07 06:27:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 103 @ 20056 updates
2022-03-07 06:27:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 06:27:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 06:27:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 103 @ 20056 updates, score 10.301) (writing took 3.5967041668482125 seconds)
2022-03-07 06:27:35 | INFO | fairseq_cli.train | end of epoch 103 (average epoch stats below)
2022-03-07 06:27:35 | INFO | train | epoch 103 | loss 3.367 | nll_loss 2.439 | ppl 5.42 | wps 20584 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 20056 | lr 0.000223294 | gnorm 1.06 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 63938
2022-03-07 06:27:35 | INFO | fairseq.trainer | begin training epoch 104
2022-03-07 06:27:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:29:52 | INFO | train_inner | epoch 104:     44 / 196 loss=3.361, nll_loss=2.432, ppl=5.4, wps=20203.9, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=20100, lr=0.00022305, gnorm=1.061, loss_scale=16, train_wall=292, gb_free=19.9, wall=64075
2022-03-07 06:31:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:35:07 | INFO | train_inner | epoch 104:    145 / 196 loss=3.353, nll_loss=2.423, ppl=5.36, wps=20795.9, ups=0.32, wpb=65536, bsz=128, num_updates=20200, lr=0.000222497, gnorm=1.055, loss_scale=16, train_wall=293, gb_free=19.9, wall=64390
2022-03-07 06:37:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:37:51 | INFO | valid | epoch 104 | valid on 'valid' subset | loss 10.3 | nll_loss 9.662 | ppl 810.21 | wps 40602.5 | wpb 510.9 | bsz 1 | num_updates 20251 | best_loss 7.384
2022-03-07 06:37:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 104 @ 20251 updates
2022-03-07 06:37:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 06:37:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 06:37:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 104 @ 20251 updates, score 10.3) (writing took 3.5602667620405555 seconds)
2022-03-07 06:37:55 | INFO | fairseq_cli.train | end of epoch 104 (average epoch stats below)
2022-03-07 06:37:55 | INFO | train | epoch 104 | loss 3.358 | nll_loss 2.429 | ppl 5.39 | wps 20588.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 20251 | lr 0.000222217 | gnorm 1.057 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 64557
2022-03-07 06:37:55 | INFO | fairseq.trainer | begin training epoch 105
2022-03-07 06:37:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:38:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:40:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 06:40:34 | INFO | train_inner | epoch 105:     51 / 196 loss=3.358, nll_loss=2.429, ppl=5.38, wps=20016.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=20300, lr=0.000221948, gnorm=1.059, loss_scale=8, train_wall=295, gb_free=19.9, wall=64717
2022-03-07 06:45:46 | INFO | train_inner | epoch 105:    151 / 196 loss=3.36, nll_loss=2.431, ppl=5.39, wps=21004.8, ups=0.32, wpb=65536, bsz=128, num_updates=20400, lr=0.000221404, gnorm=1.05, loss_scale=8, train_wall=290, gb_free=19.9, wall=65029
2022-03-07 06:48:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:48:11 | INFO | valid | epoch 105 | valid on 'valid' subset | loss 10.363 | nll_loss 9.725 | ppl 846.44 | wps 40860 | wpb 510.9 | bsz 1 | num_updates 20445 | best_loss 7.384
2022-03-07 06:48:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 105 @ 20445 updates
2022-03-07 06:48:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 06:48:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 06:48:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 105 @ 20445 updates, score 10.363) (writing took 3.5067272959277034 seconds)
2022-03-07 06:48:14 | INFO | fairseq_cli.train | end of epoch 105 (average epoch stats below)
2022-03-07 06:48:14 | INFO | train | epoch 105 | loss 3.349 | nll_loss 2.42 | ppl 5.35 | wps 20486.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 20445 | lr 0.00022116 | gnorm 1.053 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 65177
2022-03-07 06:48:14 | INFO | fairseq.trainer | begin training epoch 106
2022-03-07 06:48:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 06:51:06 | INFO | train_inner | epoch 106:     55 / 196 loss=3.331, nll_loss=2.4, ppl=5.28, wps=20412.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=20500, lr=0.000220863, gnorm=1.039, loss_scale=16, train_wall=289, gb_free=19.9, wall=65349
2022-03-07 06:54:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 06:56:21 | INFO | train_inner | epoch 106:    156 / 196 loss=3.351, nll_loss=2.421, ppl=5.36, wps=20793.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=20600, lr=0.000220326, gnorm=1.064, loss_scale=16, train_wall=293, gb_free=19.9, wall=65664
2022-03-07 06:58:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 06:58:31 | INFO | valid | epoch 106 | valid on 'valid' subset | loss 10.398 | nll_loss 9.77 | ppl 872.9 | wps 40765.1 | wpb 510.9 | bsz 1 | num_updates 20640 | best_loss 7.384
2022-03-07 06:58:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 106 @ 20640 updates
2022-03-07 06:58:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 06:58:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 06:58:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 106 @ 20640 updates, score 10.398) (writing took 3.52799192070961 seconds)
2022-03-07 06:58:34 | INFO | fairseq_cli.train | end of epoch 106 (average epoch stats below)
2022-03-07 06:58:34 | INFO | train | epoch 106 | loss 3.341 | nll_loss 2.411 | ppl 5.32 | wps 20592.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 20640 | lr 0.000220113 | gnorm 1.056 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 65797
2022-03-07 06:58:34 | INFO | fairseq.trainer | begin training epoch 107
2022-03-07 06:58:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:01:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:01:45 | INFO | train_inner | epoch 107:     61 / 196 loss=3.326, nll_loss=2.395, ppl=5.26, wps=20214.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=20700, lr=0.000219793, gnorm=1.055, loss_scale=16, train_wall=292, gb_free=19.9, wall=65987
2022-03-07 07:06:57 | INFO | train_inner | epoch 107:    161 / 196 loss=3.347, nll_loss=2.417, ppl=5.34, wps=21004.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=20800, lr=0.000219265, gnorm=1.071, loss_scale=16, train_wall=290, gb_free=19.9, wall=66299
2022-03-07 07:08:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:08:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:08:50 | INFO | valid | epoch 107 | valid on 'valid' subset | loss 10.444 | nll_loss 9.816 | ppl 901.25 | wps 40697 | wpb 510.9 | bsz 1 | num_updates 20834 | best_loss 7.384
2022-03-07 07:08:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 107 @ 20834 updates
2022-03-07 07:08:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 07:08:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 07:08:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 107 @ 20834 updates, score 10.444) (writing took 3.7266154098324478 seconds)
2022-03-07 07:08:54 | INFO | fairseq_cli.train | end of epoch 107 (average epoch stats below)
2022-03-07 07:08:54 | INFO | train | epoch 107 | loss 3.332 | nll_loss 2.401 | ppl 5.28 | wps 20480.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 20834 | lr 0.000219086 | gnorm 1.06 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 66417
2022-03-07 07:08:54 | INFO | fairseq.trainer | begin training epoch 108
2022-03-07 07:08:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:12:20 | INFO | train_inner | epoch 108:     66 / 196 loss=3.311, nll_loss=2.378, ppl=5.2, wps=20199.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=20900, lr=0.000218739, gnorm=1.054, loss_scale=16, train_wall=292, gb_free=19.9, wall=66623
2022-03-07 07:15:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:17:35 | INFO | train_inner | epoch 108:    167 / 196 loss=3.346, nll_loss=2.416, ppl=5.34, wps=20794, ups=0.32, wpb=65532.4, bsz=128, num_updates=21000, lr=0.000218218, gnorm=1.06, loss_scale=16, train_wall=293, gb_free=19.9, wall=66938
2022-03-07 07:19:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:19:10 | INFO | valid | epoch 108 | valid on 'valid' subset | loss 10.401 | nll_loss 9.772 | ppl 874.12 | wps 40687.3 | wpb 510.9 | bsz 1 | num_updates 21029 | best_loss 7.384
2022-03-07 07:19:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 108 @ 21029 updates
2022-03-07 07:19:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 07:19:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 07:19:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 108 @ 21029 updates, score 10.401) (writing took 3.6586646428331733 seconds)
2022-03-07 07:19:14 | INFO | fairseq_cli.train | end of epoch 108 (average epoch stats below)
2022-03-07 07:19:14 | INFO | train | epoch 108 | loss 3.325 | nll_loss 2.394 | ppl 5.25 | wps 20581.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21029 | lr 0.000218067 | gnorm 1.059 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 67037
2022-03-07 07:19:14 | INFO | fairseq.trainer | begin training epoch 109
2022-03-07 07:19:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:22:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:22:59 | INFO | train_inner | epoch 109:     72 / 196 loss=3.296, nll_loss=2.362, ppl=5.14, wps=20196.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=21100, lr=0.0002177, gnorm=1.061, loss_scale=16, train_wall=292, gb_free=19.9, wall=67262
2022-03-07 07:28:11 | INFO | train_inner | epoch 109:    172 / 196 loss=3.341, nll_loss=2.41, ppl=5.32, wps=21011.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=21200, lr=0.000217186, gnorm=1.059, loss_scale=16, train_wall=290, gb_free=19.9, wall=67574
2022-03-07 07:28:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:29:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:29:30 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 10.414 | nll_loss 9.788 | ppl 884 | wps 40897.1 | wpb 510.9 | bsz 1 | num_updates 21223 | best_loss 7.384
2022-03-07 07:29:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 109 @ 21223 updates
2022-03-07 07:29:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 07:29:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 07:29:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 109 @ 21223 updates, score 10.414) (writing took 3.4917086977511644 seconds)
2022-03-07 07:29:34 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2022-03-07 07:29:34 | INFO | train | epoch 109 | loss 3.317 | nll_loss 2.385 | ppl 5.22 | wps 20489.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 21223 | lr 0.000217068 | gnorm 1.06 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 67657
2022-03-07 07:29:34 | INFO | fairseq.trainer | begin training epoch 110
2022-03-07 07:29:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:33:34 | INFO | train_inner | epoch 110:     77 / 196 loss=3.29, nll_loss=2.355, ppl=5.12, wps=20213.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21300, lr=0.000216676, gnorm=1.062, loss_scale=16, train_wall=292, gb_free=19.9, wall=67897
2022-03-07 07:35:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:38:49 | INFO | train_inner | epoch 110:    178 / 196 loss=3.338, nll_loss=2.408, ppl=5.31, wps=20806.2, ups=0.32, wpb=65536, bsz=128, num_updates=21400, lr=0.000216169, gnorm=1.091, loss_scale=16, train_wall=293, gb_free=19.9, wall=68212
2022-03-07 07:39:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:39:50 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 10.467 | nll_loss 9.838 | ppl 915.1 | wps 40998.9 | wpb 510.9 | bsz 1 | num_updates 21418 | best_loss 7.384
2022-03-07 07:39:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 110 @ 21418 updates
2022-03-07 07:39:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 07:39:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 07:39:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 110 @ 21418 updates, score 10.467) (writing took 3.4675536770373583 seconds)
2022-03-07 07:39:53 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2022-03-07 07:39:53 | INFO | train | epoch 110 | loss 3.311 | nll_loss 2.378 | ppl 5.2 | wps 20597.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 21418 | lr 0.000216078 | gnorm 1.074 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 68276
2022-03-07 07:39:53 | INFO | fairseq.trainer | begin training epoch 111
2022-03-07 07:39:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:42:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:44:13 | INFO | train_inner | epoch 111:     83 / 196 loss=3.275, nll_loss=2.339, ppl=5.06, wps=20206.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21500, lr=0.000215666, gnorm=1.061, loss_scale=16, train_wall=292, gb_free=19.9, wall=68536
2022-03-07 07:49:25 | INFO | train_inner | epoch 111:    183 / 196 loss=3.335, nll_loss=2.404, ppl=5.29, wps=21003.1, ups=0.32, wpb=65536, bsz=128, num_updates=21600, lr=0.000215166, gnorm=1.075, loss_scale=32, train_wall=290, gb_free=19.9, wall=68848
2022-03-07 07:50:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:50:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 07:50:10 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 10.46 | nll_loss 9.833 | ppl 912.36 | wps 40986.7 | wpb 510.9 | bsz 1 | num_updates 21612 | best_loss 7.384
2022-03-07 07:50:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 111 @ 21612 updates
2022-03-07 07:50:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 07:50:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 07:50:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 111 @ 21612 updates, score 10.46) (writing took 3.430672973860055 seconds)
2022-03-07 07:50:13 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2022-03-07 07:50:13 | INFO | train | epoch 111 | loss 3.302 | nll_loss 2.368 | ppl 5.16 | wps 20481.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 21612 | lr 0.000215106 | gnorm 1.069 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 68896
2022-03-07 07:50:13 | INFO | fairseq.trainer | begin training epoch 112
2022-03-07 07:50:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 07:54:48 | INFO | train_inner | epoch 112:     88 / 196 loss=3.266, nll_loss=2.33, ppl=5.03, wps=20217.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=21700, lr=0.000214669, gnorm=1.073, loss_scale=16, train_wall=292, gb_free=19.9, wall=69171
2022-03-07 07:56:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 07:57:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 08:00:06 | INFO | train_inner | epoch 112:    190 / 196 loss=3.327, nll_loss=2.396, ppl=5.26, wps=20601.4, ups=0.31, wpb=65536, bsz=128, num_updates=21800, lr=0.000214176, gnorm=1.081, loss_scale=8, train_wall=296, gb_free=19.9, wall=69489
2022-03-07 08:00:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:00:29 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 10.434 | nll_loss 9.8 | ppl 891.67 | wps 40750.1 | wpb 510.9 | bsz 1 | num_updates 21806 | best_loss 7.384
2022-03-07 08:00:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 112 @ 21806 updates
2022-03-07 08:00:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 08:00:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 08:00:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 112 @ 21806 updates, score 10.434) (writing took 3.5152420457452536 seconds)
2022-03-07 08:00:33 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2022-03-07 08:00:33 | INFO | train | epoch 112 | loss 3.295 | nll_loss 2.361 | ppl 5.14 | wps 20488.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 21806 | lr 0.000214147 | gnorm 1.079 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 69516
2022-03-07 08:00:33 | INFO | fairseq.trainer | begin training epoch 113
2022-03-07 08:00:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:05:27 | INFO | train_inner | epoch 113:     94 / 196 loss=3.251, nll_loss=2.313, ppl=4.97, wps=20401.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=21900, lr=0.000213687, gnorm=1.056, loss_scale=16, train_wall=289, gb_free=19.9, wall=69809
2022-03-07 08:10:39 | INFO | train_inner | epoch 113:    194 / 196 loss=3.328, nll_loss=2.397, ppl=5.27, wps=20991.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=22000, lr=0.000213201, gnorm=1.102, loss_scale=32, train_wall=290, gb_free=19.9, wall=70122
2022-03-07 08:10:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:10:50 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 10.486 | nll_loss 9.854 | ppl 925.61 | wps 40876.8 | wpb 510.9 | bsz 1 | num_updates 22002 | best_loss 7.384
2022-03-07 08:10:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 113 @ 22002 updates
2022-03-07 08:10:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 08:10:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 08:10:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 113 @ 22002 updates, score 10.486) (writing took 3.5306939580477774 seconds)
2022-03-07 08:10:53 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2022-03-07 08:10:53 | INFO | train | epoch 113 | loss 3.287 | nll_loss 2.353 | ppl 5.11 | wps 20686.3 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 22002 | lr 0.000213191 | gnorm 1.078 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 70136
2022-03-07 08:10:53 | INFO | fairseq.trainer | begin training epoch 114
2022-03-07 08:10:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:10:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:16:02 | INFO | train_inner | epoch 114:     99 / 196 loss=3.236, nll_loss=2.296, ppl=4.91, wps=20215, ups=0.31, wpb=65367, bsz=127.7, num_updates=22100, lr=0.000212718, gnorm=1.073, loss_scale=16, train_wall=292, gb_free=19.9, wall=70445
2022-03-07 08:17:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:21:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:21:09 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 10.494 | nll_loss 9.865 | ppl 932.51 | wps 40620 | wpb 510.9 | bsz 1 | num_updates 22196 | best_loss 7.384
2022-03-07 08:21:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 114 @ 22196 updates
2022-03-07 08:21:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 08:21:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 08:21:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 114 @ 22196 updates, score 10.494) (writing took 3.5287151280790567 seconds)
2022-03-07 08:21:13 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2022-03-07 08:21:13 | INFO | train | epoch 114 | loss 3.28 | nll_loss 2.345 | ppl 5.08 | wps 20490.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22196 | lr 0.000212257 | gnorm 1.081 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 70756
2022-03-07 08:21:13 | INFO | fairseq.trainer | begin training epoch 115
2022-03-07 08:21:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:21:25 | INFO | train_inner | epoch 115:      4 / 196 loss=3.324, nll_loss=2.392, ppl=5.25, wps=20220.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22200, lr=0.000212238, gnorm=1.09, loss_scale=16, train_wall=292, gb_free=19.9, wall=70768
2022-03-07 08:24:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:26:41 | INFO | train_inner | epoch 115:    105 / 196 loss=3.241, nll_loss=2.302, ppl=4.93, wps=20795.2, ups=0.32, wpb=65536, bsz=128, num_updates=22300, lr=0.000211762, gnorm=1.06, loss_scale=16, train_wall=293, gb_free=19.9, wall=71083
2022-03-07 08:31:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:31:29 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 10.554 | nll_loss 9.929 | ppl 975.02 | wps 40752.3 | wpb 510.9 | bsz 1 | num_updates 22391 | best_loss 7.384
2022-03-07 08:31:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 115 @ 22391 updates
2022-03-07 08:31:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 08:31:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 08:31:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 115 @ 22391 updates, score 10.554) (writing took 3.507983314804733 seconds)
2022-03-07 08:31:33 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2022-03-07 08:31:33 | INFO | train | epoch 115 | loss 3.273 | nll_loss 2.338 | ppl 5.05 | wps 20585.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22391 | lr 0.000211331 | gnorm 1.073 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 71376
2022-03-07 08:31:33 | INFO | fairseq.trainer | begin training epoch 116
2022-03-07 08:31:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:31:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:32:04 | INFO | train_inner | epoch 116:     10 / 196 loss=3.298, nll_loss=2.365, ppl=5.15, wps=20203.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22400, lr=0.000211289, gnorm=1.084, loss_scale=16, train_wall=292, gb_free=19.9, wall=71407
2022-03-07 08:37:16 | INFO | train_inner | epoch 116:    110 / 196 loss=3.235, nll_loss=2.295, ppl=4.91, wps=20996.8, ups=0.32, wpb=65536, bsz=128, num_updates=22500, lr=0.000210819, gnorm=1.069, loss_scale=16, train_wall=290, gb_free=19.9, wall=71719
2022-03-07 08:38:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:41:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:41:49 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 10.537 | nll_loss 9.91 | ppl 962.11 | wps 40832.1 | wpb 510.9 | bsz 1 | num_updates 22585 | best_loss 7.384
2022-03-07 08:41:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 116 @ 22585 updates
2022-03-07 08:41:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 08:41:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 08:41:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 116 @ 22585 updates, score 10.537) (writing took 3.5324106058105826 seconds)
2022-03-07 08:41:53 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2022-03-07 08:41:53 | INFO | train | epoch 116 | loss 3.266 | nll_loss 2.329 | ppl 5.02 | wps 20485.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22585 | lr 0.000210421 | gnorm 1.077 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 71995
2022-03-07 08:41:53 | INFO | fairseq.trainer | begin training epoch 117
2022-03-07 08:41:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:42:39 | INFO | train_inner | epoch 117:     15 / 196 loss=3.291, nll_loss=2.357, ppl=5.12, wps=20220.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=22600, lr=0.000210352, gnorm=1.084, loss_scale=16, train_wall=292, gb_free=19.9, wall=72042
2022-03-07 08:45:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 08:45:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 08:47:58 | INFO | train_inner | epoch 117:    117 / 196 loss=3.237, nll_loss=2.298, ppl=4.92, wps=20599.9, ups=0.31, wpb=65532.4, bsz=128, num_updates=22700, lr=0.000209888, gnorm=1.074, loss_scale=8, train_wall=296, gb_free=19.9, wall=72360
2022-03-07 08:52:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 08:52:08 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 10.556 | nll_loss 9.93 | ppl 975.33 | wps 40764.4 | wpb 510.9 | bsz 1 | num_updates 22779 | best_loss 7.384
2022-03-07 08:52:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 117 @ 22779 updates
2022-03-07 08:52:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 08:52:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 08:52:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 117 @ 22779 updates, score 10.556) (writing took 3.3949286891147494 seconds)
2022-03-07 08:52:12 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2022-03-07 08:52:12 | INFO | train | epoch 117 | loss 3.258 | nll_loss 2.321 | ppl 5 | wps 20501.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 22779 | lr 0.000209523 | gnorm 1.088 | loss_scale 8 | train_wall 567 | gb_free 19.9 | wall 72615
2022-03-07 08:52:12 | INFO | fairseq.trainer | begin training epoch 118
2022-03-07 08:52:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 08:53:17 | INFO | train_inner | epoch 118:     21 / 196 loss=3.274, nll_loss=2.338, ppl=5.06, wps=20435.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=22800, lr=0.000209427, gnorm=1.103, loss_scale=16, train_wall=289, gb_free=19.9, wall=72680
2022-03-07 08:58:30 | INFO | train_inner | epoch 118:    121 / 196 loss=3.233, nll_loss=2.293, ppl=4.9, wps=20993.2, ups=0.32, wpb=65536, bsz=128, num_updates=22900, lr=0.000208969, gnorm=1.079, loss_scale=16, train_wall=290, gb_free=19.9, wall=72992
2022-03-07 08:59:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:02:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:02:28 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 10.539 | nll_loss 9.912 | ppl 963.22 | wps 40642 | wpb 510.9 | bsz 1 | num_updates 22974 | best_loss 7.384
2022-03-07 09:02:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 118 @ 22974 updates
2022-03-07 09:02:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 09:02:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 09:02:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 118 @ 22974 updates, score 10.539) (writing took 3.3559752386063337 seconds)
2022-03-07 09:02:32 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2022-03-07 09:02:32 | INFO | train | epoch 118 | loss 3.252 | nll_loss 2.314 | ppl 4.97 | wps 20593.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 22974 | lr 0.000208632 | gnorm 1.077 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 73234
2022-03-07 09:02:32 | INFO | fairseq.trainer | begin training epoch 119
2022-03-07 09:02:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:03:53 | INFO | train_inner | epoch 119:     26 / 196 loss=3.269, nll_loss=2.333, ppl=5.04, wps=20224.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23000, lr=0.000208514, gnorm=1.077, loss_scale=16, train_wall=292, gb_free=19.9, wall=73316
2022-03-07 09:06:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:09:08 | INFO | train_inner | epoch 119:    127 / 196 loss=3.231, nll_loss=2.292, ppl=4.9, wps=20792.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=23100, lr=0.000208063, gnorm=1.095, loss_scale=16, train_wall=293, gb_free=19.9, wall=73631
2022-03-07 09:12:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:12:48 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 10.614 | nll_loss 9.993 | ppl 1018.89 | wps 40934.4 | wpb 510.9 | bsz 1 | num_updates 23169 | best_loss 7.384
2022-03-07 09:12:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 119 @ 23169 updates
2022-03-07 09:12:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 09:12:51 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 09:12:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 119 @ 23169 updates, score 10.614) (writing took 3.4140415508300066 seconds)
2022-03-07 09:12:51 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2022-03-07 09:12:51 | INFO | train | epoch 119 | loss 3.246 | nll_loss 2.308 | ppl 4.95 | wps 20595.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 23169 | lr 0.000207753 | gnorm 1.096 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 73854
2022-03-07 09:12:51 | INFO | fairseq.trainer | begin training epoch 120
2022-03-07 09:12:51 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:14:28 | INFO | train_inner | epoch 120:     31 / 196 loss=3.257, nll_loss=2.32, ppl=4.99, wps=20420.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23200, lr=0.000207614, gnorm=1.092, loss_scale=32, train_wall=289, gb_free=19.9, wall=73951
2022-03-07 09:14:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:19:43 | INFO | train_inner | epoch 120:    132 / 196 loss=3.228, nll_loss=2.288, ppl=4.88, wps=20791.2, ups=0.32, wpb=65536, bsz=128, num_updates=23300, lr=0.000207168, gnorm=1.073, loss_scale=16, train_wall=293, gb_free=19.9, wall=74266
2022-03-07 09:22:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:23:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:23:07 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 10.578 | nll_loss 9.954 | ppl 992.04 | wps 40736.3 | wpb 510.9 | bsz 1 | num_updates 23363 | best_loss 7.384
2022-03-07 09:23:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 120 @ 23363 updates
2022-03-07 09:23:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 09:23:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 09:23:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 120 @ 23363 updates, score 10.578) (writing took 3.380106291733682 seconds)
2022-03-07 09:23:11 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2022-03-07 09:23:11 | INFO | train | epoch 120 | loss 3.239 | nll_loss 2.3 | ppl 4.93 | wps 20490.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23363 | lr 0.000206888 | gnorm 1.079 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 74474
2022-03-07 09:23:11 | INFO | fairseq.trainer | begin training epoch 121
2022-03-07 09:23:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:25:06 | INFO | train_inner | epoch 121:     37 / 196 loss=3.243, nll_loss=2.304, ppl=4.94, wps=20227.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=23400, lr=0.000206725, gnorm=1.085, loss_scale=16, train_wall=292, gb_free=19.9, wall=74589
2022-03-07 09:29:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:30:22 | INFO | train_inner | epoch 121:    138 / 196 loss=3.225, nll_loss=2.285, ppl=4.88, wps=20786.8, ups=0.32, wpb=65536, bsz=128, num_updates=23500, lr=0.000206284, gnorm=1.077, loss_scale=16, train_wall=293, gb_free=19.9, wall=74905
2022-03-07 09:33:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:33:27 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 10.588 | nll_loss 9.966 | ppl 1000.48 | wps 40559.3 | wpb 510.9 | bsz 1 | num_updates 23558 | best_loss 7.384
2022-03-07 09:33:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 121 @ 23558 updates
2022-03-07 09:33:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 09:33:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 09:33:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 121 @ 23558 updates, score 10.588) (writing took 3.3745140158571303 seconds)
2022-03-07 09:33:31 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2022-03-07 09:33:31 | INFO | train | epoch 121 | loss 3.233 | nll_loss 2.293 | ppl 4.9 | wps 20587.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 23558 | lr 0.00020603 | gnorm 1.082 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 75094
2022-03-07 09:33:31 | INFO | fairseq.trainer | begin training epoch 122
2022-03-07 09:33:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:35:42 | INFO | train_inner | epoch 122:     42 / 196 loss=3.234, nll_loss=2.295, ppl=4.91, wps=20408.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23600, lr=0.000205847, gnorm=1.085, loss_scale=16, train_wall=289, gb_free=19.9, wall=75225
2022-03-07 09:35:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:40:57 | INFO | train_inner | epoch 122:    143 / 196 loss=3.225, nll_loss=2.285, ppl=4.87, wps=20790.5, ups=0.32, wpb=65536, bsz=128, num_updates=23700, lr=0.000205412, gnorm=1.088, loss_scale=16, train_wall=293, gb_free=19.9, wall=75540
2022-03-07 09:42:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:43:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:43:47 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 10.607 | nll_loss 9.984 | ppl 1012.86 | wps 40816.9 | wpb 510.9 | bsz 1 | num_updates 23752 | best_loss 7.384
2022-03-07 09:43:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 122 @ 23752 updates
2022-03-07 09:43:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 09:43:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 09:43:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 122 @ 23752 updates, score 10.607) (writing took 3.3532573231495917 seconds)
2022-03-07 09:43:50 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2022-03-07 09:43:50 | INFO | train | epoch 122 | loss 3.226 | nll_loss 2.287 | ppl 4.88 | wps 20494.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 23752 | lr 0.000205187 | gnorm 1.08 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 75713
2022-03-07 09:43:50 | INFO | fairseq.trainer | begin training epoch 123
2022-03-07 09:43:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:46:20 | INFO | train_inner | epoch 123:     48 / 196 loss=3.231, nll_loss=2.292, ppl=4.9, wps=20250, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=23800, lr=0.00020498, gnorm=1.071, loss_scale=16, train_wall=292, gb_free=19.9, wall=75863
2022-03-07 09:49:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:51:35 | INFO | train_inner | epoch 123:    149 / 196 loss=3.218, nll_loss=2.277, ppl=4.85, wps=20832.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=23900, lr=0.000204551, gnorm=1.1, loss_scale=16, train_wall=292, gb_free=19.9, wall=76177
2022-03-07 09:54:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 09:54:06 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 10.664 | nll_loss 10.044 | ppl 1055.85 | wps 40702 | wpb 510.9 | bsz 1 | num_updates 23947 | best_loss 7.384
2022-03-07 09:54:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 123 @ 23947 updates
2022-03-07 09:54:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 09:54:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 09:54:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 123 @ 23947 updates, score 10.664) (writing took 3.3119747317396104 seconds)
2022-03-07 09:54:09 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2022-03-07 09:54:09 | INFO | train | epoch 123 | loss 3.221 | nll_loss 2.281 | ppl 4.86 | wps 20631.9 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 23947 | lr 0.00020435 | gnorm 1.089 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 76332
2022-03-07 09:54:09 | INFO | fairseq.trainer | begin training epoch 124
2022-03-07 09:54:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 09:56:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 09:56:57 | INFO | train_inner | epoch 124:     54 / 196 loss=3.216, nll_loss=2.276, ppl=4.84, wps=20257.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=24000, lr=0.000204124, gnorm=1.09, loss_scale=16, train_wall=292, gb_free=19.9, wall=76500
2022-03-07 10:02:09 | INFO | train_inner | epoch 124:    154 / 196 loss=3.221, nll_loss=2.281, ppl=4.86, wps=21034.9, ups=0.32, wpb=65536, bsz=128, num_updates=24100, lr=0.0002037, gnorm=1.097, loss_scale=16, train_wall=290, gb_free=19.9, wall=76812
2022-03-07 10:03:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:04:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:04:24 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 10.6 | nll_loss 9.975 | ppl 1006.49 | wps 40997.5 | wpb 510.9 | bsz 1 | num_updates 24141 | best_loss 7.384
2022-03-07 10:04:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 124 @ 24141 updates
2022-03-07 10:04:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 10:04:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 10:04:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 124 @ 24141 updates, score 10.6) (writing took 3.329672524239868 seconds)
2022-03-07 10:04:27 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2022-03-07 10:04:27 | INFO | train | epoch 124 | loss 3.215 | nll_loss 2.274 | ppl 4.84 | wps 20524.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 24141 | lr 0.000203527 | gnorm 1.093 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 76950
2022-03-07 10:04:28 | INFO | fairseq.trainer | begin training epoch 125
2022-03-07 10:04:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:07:32 | INFO | train_inner | epoch 125:     59 / 196 loss=3.204, nll_loss=2.262, ppl=4.8, wps=20249.7, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=24200, lr=0.000203279, gnorm=1.105, loss_scale=16, train_wall=292, gb_free=19.9, wall=77134
2022-03-07 10:10:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:12:46 | INFO | train_inner | epoch 125:    160 / 196 loss=3.216, nll_loss=2.275, ppl=4.84, wps=20832, ups=0.32, wpb=65536, bsz=128, num_updates=24300, lr=0.00020286, gnorm=1.104, loss_scale=16, train_wall=292, gb_free=19.9, wall=77449
2022-03-07 10:14:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:14:43 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 10.635 | nll_loss 10.017 | ppl 1036.35 | wps 40829.7 | wpb 510.9 | bsz 1 | num_updates 24336 | best_loss 7.384
2022-03-07 10:14:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 125 @ 24336 updates
2022-03-07 10:14:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 10:14:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 10:14:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 125 @ 24336 updates, score 10.635) (writing took 3.313157817814499 seconds)
2022-03-07 10:14:46 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2022-03-07 10:14:46 | INFO | train | epoch 125 | loss 3.208 | nll_loss 2.267 | ppl 4.81 | wps 20627 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 24336 | lr 0.00020271 | gnorm 1.111 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 77569
2022-03-07 10:14:46 | INFO | fairseq.trainer | begin training epoch 126
2022-03-07 10:14:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:17:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:18:09 | INFO | train_inner | epoch 126:     65 / 196 loss=3.192, nll_loss=2.249, ppl=4.75, wps=20257.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=24400, lr=0.000202444, gnorm=1.096, loss_scale=16, train_wall=292, gb_free=19.9, wall=77772
2022-03-07 10:23:20 | INFO | train_inner | epoch 126:    165 / 196 loss=3.22, nll_loss=2.28, ppl=4.86, wps=21044.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=24500, lr=0.000202031, gnorm=1.106, loss_scale=16, train_wall=290, gb_free=19.9, wall=78083
2022-03-07 10:24:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:24:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:25:01 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 10.67 | nll_loss 10.046 | ppl 1056.89 | wps 40824.4 | wpb 510.9 | bsz 1 | num_updates 24530 | best_loss 7.384
2022-03-07 10:25:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 126 @ 24530 updates
2022-03-07 10:25:01 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 10:25:05 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 10:25:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 126 @ 24530 updates, score 10.67) (writing took 3.4683781871572137 seconds)
2022-03-07 10:25:05 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2022-03-07 10:25:05 | INFO | train | epoch 126 | loss 3.202 | nll_loss 2.26 | ppl 4.79 | wps 20522.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 24530 | lr 0.000201907 | gnorm 1.1 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 78188
2022-03-07 10:25:05 | INFO | fairseq.trainer | begin training epoch 127
2022-03-07 10:25:05 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:28:43 | INFO | train_inner | epoch 127:     70 / 196 loss=3.181, nll_loss=2.237, ppl=4.71, wps=20233.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=24600, lr=0.000201619, gnorm=1.11, loss_scale=16, train_wall=292, gb_free=19.9, wall=78406
2022-03-07 10:31:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:33:58 | INFO | train_inner | epoch 127:    171 / 196 loss=3.214, nll_loss=2.273, ppl=4.83, wps=20823.3, ups=0.32, wpb=65536, bsz=128, num_updates=24700, lr=0.000201211, gnorm=1.102, loss_scale=16, train_wall=293, gb_free=19.9, wall=78721
2022-03-07 10:35:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:35:21 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 10.661 | nll_loss 10.042 | ppl 1053.88 | wps 40597.1 | wpb 510.9 | bsz 1 | num_updates 24725 | best_loss 7.384
2022-03-07 10:35:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 127 @ 24725 updates
2022-03-07 10:35:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 10:35:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 10:35:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 127 @ 24725 updates, score 10.661) (writing took 3.4901585862971842 seconds)
2022-03-07 10:35:24 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2022-03-07 10:35:24 | INFO | train | epoch 127 | loss 3.197 | nll_loss 2.255 | ppl 4.77 | wps 20611.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 24725 | lr 0.000201109 | gnorm 1.103 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 78807
2022-03-07 10:35:24 | INFO | fairseq.trainer | begin training epoch 128
2022-03-07 10:35:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:38:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:39:21 | INFO | train_inner | epoch 128:     76 / 196 loss=3.173, nll_loss=2.229, ppl=4.69, wps=20232.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=24800, lr=0.000200805, gnorm=1.108, loss_scale=16, train_wall=292, gb_free=19.9, wall=79044
2022-03-07 10:44:33 | INFO | train_inner | epoch 128:    176 / 196 loss=3.214, nll_loss=2.273, ppl=4.83, wps=21040.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=24900, lr=0.000200401, gnorm=1.086, loss_scale=16, train_wall=290, gb_free=19.9, wall=79355
2022-03-07 10:45:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:45:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:45:39 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 10.648 | nll_loss 10.024 | ppl 1041.21 | wps 40751.8 | wpb 510.9 | bsz 1 | num_updates 24919 | best_loss 7.384
2022-03-07 10:45:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 128 @ 24919 updates
2022-03-07 10:45:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 10:45:43 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 10:45:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 128 @ 24919 updates, score 10.648) (writing took 3.453794355969876 seconds)
2022-03-07 10:45:43 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2022-03-07 10:45:43 | INFO | train | epoch 128 | loss 3.19 | nll_loss 2.248 | ppl 4.75 | wps 20515.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 24919 | lr 0.000200325 | gnorm 1.099 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 79426
2022-03-07 10:45:43 | INFO | fairseq.trainer | begin training epoch 129
2022-03-07 10:45:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:49:56 | INFO | train_inner | epoch 129:     81 / 196 loss=3.16, nll_loss=2.215, ppl=4.64, wps=20238.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=25000, lr=0.0002, gnorm=1.097, loss_scale=16, train_wall=292, gb_free=19.9, wall=79678
2022-03-07 10:52:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 10:55:10 | INFO | train_inner | epoch 129:    182 / 196 loss=3.213, nll_loss=2.273, ppl=4.83, wps=20837.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=25100, lr=0.000199601, gnorm=1.116, loss_scale=16, train_wall=292, gb_free=19.9, wall=79993
2022-03-07 10:55:53 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 10:55:58 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 10.712 | nll_loss 10.094 | ppl 1092.62 | wps 40879.2 | wpb 510.9 | bsz 1 | num_updates 25114 | best_loss 7.384
2022-03-07 10:55:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 129 @ 25114 updates
2022-03-07 10:55:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 10:56:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 10:56:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 129 @ 25114 updates, score 10.712) (writing took 3.470486310776323 seconds)
2022-03-07 10:56:02 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2022-03-07 10:56:02 | INFO | train | epoch 129 | loss 3.184 | nll_loss 2.241 | ppl 4.73 | wps 20623.2 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 25114 | lr 0.000199546 | gnorm 1.105 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 80045
2022-03-07 10:56:02 | INFO | fairseq.trainer | begin training epoch 130
2022-03-07 10:56:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 10:59:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:00:33 | INFO | train_inner | epoch 130:     87 / 196 loss=3.152, nll_loss=2.206, ppl=4.61, wps=20236.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25200, lr=0.000199205, gnorm=1.1, loss_scale=16, train_wall=292, gb_free=19.9, wall=80316
2022-03-07 11:05:44 | INFO | train_inner | epoch 130:    187 / 196 loss=3.21, nll_loss=2.269, ppl=4.82, wps=21039.6, ups=0.32, wpb=65536, bsz=128, num_updates=25300, lr=0.000198811, gnorm=1.119, loss_scale=16, train_wall=290, gb_free=19.9, wall=80627
2022-03-07 11:06:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:06:17 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 10.717 | nll_loss 10.095 | ppl 1094.05 | wps 40914.3 | wpb 510.9 | bsz 1 | num_updates 25309 | best_loss 7.384
2022-03-07 11:06:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 130 @ 25309 updates
2022-03-07 11:06:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 11:06:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 11:06:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 130 @ 25309 updates, score 10.717) (writing took 3.4675887362100184 seconds)
2022-03-07 11:06:21 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2022-03-07 11:06:21 | INFO | train | epoch 130 | loss 3.179 | nll_loss 2.235 | ppl 4.71 | wps 20622.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 25309 | lr 0.000198775 | gnorm 1.108 | loss_scale 32 | train_wall 567 | gb_free 19.9 | wall 80663
2022-03-07 11:06:21 | INFO | fairseq.trainer | begin training epoch 131
2022-03-07 11:06:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:06:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:11:07 | INFO | train_inner | epoch 131:     92 / 196 loss=3.145, nll_loss=2.198, ppl=4.59, wps=20243.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25400, lr=0.000198419, gnorm=1.092, loss_scale=16, train_wall=292, gb_free=19.9, wall=80950
2022-03-07 11:13:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:16:22 | INFO | train_inner | epoch 131:    193 / 196 loss=3.207, nll_loss=2.265, ppl=4.81, wps=20826, ups=0.32, wpb=65536, bsz=128, num_updates=25500, lr=0.00019803, gnorm=1.119, loss_scale=16, train_wall=293, gb_free=19.9, wall=81265
2022-03-07 11:16:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:16:36 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 10.715 | nll_loss 10.095 | ppl 1093.78 | wps 40765.2 | wpb 510.9 | bsz 1 | num_updates 25503 | best_loss 7.384
2022-03-07 11:16:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 131 @ 25503 updates
2022-03-07 11:16:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 11:16:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 11:16:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 131 @ 25503 updates, score 10.715) (writing took 3.498843785841018 seconds)
2022-03-07 11:16:40 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2022-03-07 11:16:40 | INFO | train | epoch 131 | loss 3.173 | nll_loss 2.229 | ppl 4.69 | wps 20513.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25503 | lr 0.000198018 | gnorm 1.106 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 81282
2022-03-07 11:16:40 | INFO | fairseq.trainer | begin training epoch 132
2022-03-07 11:16:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:20:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:21:45 | INFO | train_inner | epoch 132:     98 / 196 loss=3.132, nll_loss=2.184, ppl=4.54, wps=20233.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=25600, lr=0.000197642, gnorm=1.11, loss_scale=16, train_wall=292, gb_free=19.9, wall=81588
2022-03-07 11:26:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:26:55 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 10.752 | nll_loss 10.137 | ppl 1125.82 | wps 40734.2 | wpb 510.9 | bsz 1 | num_updates 25698 | best_loss 7.384
2022-03-07 11:26:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 132 @ 25698 updates
2022-03-07 11:26:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 11:26:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 11:26:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 132 @ 25698 updates, score 10.752) (writing took 3.480443545151502 seconds)
2022-03-07 11:26:59 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2022-03-07 11:26:59 | INFO | train | epoch 132 | loss 3.168 | nll_loss 2.223 | ppl 4.67 | wps 20617 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 25698 | lr 0.000197265 | gnorm 1.105 | loss_scale 32 | train_wall 567 | gb_free 19.9 | wall 81901
2022-03-07 11:26:59 | INFO | fairseq.trainer | begin training epoch 133
2022-03-07 11:26:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:27:05 | INFO | train_inner | epoch 133:      2 / 196 loss=3.204, nll_loss=2.263, ppl=4.8, wps=20439.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=25700, lr=0.000197257, gnorm=1.101, loss_scale=32, train_wall=289, gb_free=19.9, wall=81908
2022-03-07 11:28:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:32:20 | INFO | train_inner | epoch 133:    103 / 196 loss=3.127, nll_loss=2.179, ppl=4.53, wps=20819.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=25800, lr=0.000196875, gnorm=1.105, loss_scale=16, train_wall=292, gb_free=19.9, wall=82223
2022-03-07 11:34:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:37:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:37:14 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 10.698 | nll_loss 10.077 | ppl 1079.8 | wps 40861.1 | wpb 510.9 | bsz 1 | num_updates 25892 | best_loss 7.384
2022-03-07 11:37:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 133 @ 25892 updates
2022-03-07 11:37:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 11:37:17 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 11:37:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 133 @ 25892 updates, score 10.698) (writing took 3.4642124380916357 seconds)
2022-03-07 11:37:17 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2022-03-07 11:37:17 | INFO | train | epoch 133 | loss 3.162 | nll_loss 2.217 | ppl 4.65 | wps 20519.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 25892 | lr 0.000196525 | gnorm 1.113 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 82520
2022-03-07 11:37:17 | INFO | fairseq.trainer | begin training epoch 134
2022-03-07 11:37:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:37:42 | INFO | train_inner | epoch 134:      8 / 196 loss=3.192, nll_loss=2.25, ppl=4.76, wps=20256.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=25900, lr=0.000196494, gnorm=1.124, loss_scale=16, train_wall=292, gb_free=19.9, wall=82545
2022-03-07 11:42:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:42:57 | INFO | train_inner | epoch 134:    109 / 196 loss=3.127, nll_loss=2.179, ppl=4.53, wps=20821.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=26000, lr=0.000196116, gnorm=1.114, loss_scale=16, train_wall=292, gb_free=19.9, wall=82860
2022-03-07 11:47:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:47:33 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 10.702 | nll_loss 10.085 | ppl 1085.94 | wps 40832.8 | wpb 510.9 | bsz 1 | num_updates 26087 | best_loss 7.384
2022-03-07 11:47:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 134 @ 26087 updates
2022-03-07 11:47:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 11:47:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 11:47:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 134 @ 26087 updates, score 10.702) (writing took 3.519507461693138 seconds)
2022-03-07 11:47:36 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2022-03-07 11:47:36 | INFO | train | epoch 134 | loss 3.157 | nll_loss 2.212 | ppl 4.63 | wps 20622 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 26087 | lr 0.000195789 | gnorm 1.115 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 83139
2022-03-07 11:47:36 | INFO | fairseq.trainer | begin training epoch 135
2022-03-07 11:47:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:48:17 | INFO | train_inner | epoch 135:     13 / 196 loss=3.183, nll_loss=2.241, ppl=4.73, wps=20446.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=26100, lr=0.00019574, gnorm=1.112, loss_scale=16, train_wall=289, gb_free=19.9, wall=83180
2022-03-07 11:48:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:53:32 | INFO | train_inner | epoch 135:    114 / 196 loss=3.127, nll_loss=2.179, ppl=4.53, wps=20819.3, ups=0.32, wpb=65536, bsz=128, num_updates=26200, lr=0.000195366, gnorm=1.094, loss_scale=16, train_wall=293, gb_free=19.9, wall=83494
2022-03-07 11:55:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 11:57:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 11:57:52 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 10.746 | nll_loss 10.134 | ppl 1123.62 | wps 40824 | wpb 510.9 | bsz 1 | num_updates 26281 | best_loss 7.384
2022-03-07 11:57:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 135 @ 26281 updates
2022-03-07 11:57:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 11:57:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 11:57:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 135 @ 26281 updates, score 10.746) (writing took 3.545166931115091 seconds)
2022-03-07 11:57:55 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2022-03-07 11:57:55 | INFO | train | epoch 135 | loss 3.152 | nll_loss 2.207 | ppl 4.62 | wps 20506.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26281 | lr 0.000195065 | gnorm 1.113 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 83758
2022-03-07 11:57:55 | INFO | fairseq.trainer | begin training epoch 136
2022-03-07 11:57:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 11:58:55 | INFO | train_inner | epoch 136:     19 / 196 loss=3.171, nll_loss=2.227, ppl=4.68, wps=20231.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=26300, lr=0.000194994, gnorm=1.138, loss_scale=16, train_wall=292, gb_free=19.9, wall=83818
2022-03-07 12:02:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:04:10 | INFO | train_inner | epoch 136:    120 / 196 loss=3.124, nll_loss=2.176, ppl=4.52, wps=20817.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=26400, lr=0.000194625, gnorm=1.108, loss_scale=16, train_wall=293, gb_free=19.9, wall=84132
2022-03-07 12:08:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:08:11 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 10.773 | nll_loss 10.156 | ppl 1141.16 | wps 40824.8 | wpb 510.9 | bsz 1 | num_updates 26476 | best_loss 7.384
2022-03-07 12:08:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 136 @ 26476 updates
2022-03-07 12:08:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 12:08:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 12:08:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 136 @ 26476 updates, score 10.773) (writing took 3.46770909614861 seconds)
2022-03-07 12:08:14 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2022-03-07 12:08:14 | INFO | train | epoch 136 | loss 3.147 | nll_loss 2.201 | ppl 4.6 | wps 20619.1 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 26476 | lr 0.000194345 | gnorm 1.116 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 84377
2022-03-07 12:08:14 | INFO | fairseq.trainer | begin training epoch 137
2022-03-07 12:08:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:09:29 | INFO | train_inner | epoch 137:     24 / 196 loss=3.169, nll_loss=2.225, ppl=4.67, wps=20445.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=26500, lr=0.000194257, gnorm=1.115, loss_scale=16, train_wall=289, gb_free=19.9, wall=84452
2022-03-07 12:09:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:14:44 | INFO | train_inner | epoch 137:    125 / 196 loss=3.123, nll_loss=2.174, ppl=4.51, wps=20807.7, ups=0.32, wpb=65536, bsz=128, num_updates=26600, lr=0.000193892, gnorm=1.112, loss_scale=16, train_wall=293, gb_free=19.9, wall=84767
2022-03-07 12:16:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:18:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:18:30 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 10.751 | nll_loss 10.135 | ppl 1124.14 | wps 40776 | wpb 510.9 | bsz 1 | num_updates 26670 | best_loss 7.384
2022-03-07 12:18:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 137 @ 26670 updates
2022-03-07 12:18:30 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 12:18:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 12:18:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 137 @ 26670 updates, score 10.751) (writing took 3.510288415942341 seconds)
2022-03-07 12:18:34 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2022-03-07 12:18:34 | INFO | train | epoch 137 | loss 3.142 | nll_loss 2.195 | ppl 4.58 | wps 20501.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 26670 | lr 0.000193637 | gnorm 1.111 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 84996
2022-03-07 12:18:34 | INFO | fairseq.trainer | begin training epoch 138
2022-03-07 12:18:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:20:07 | INFO | train_inner | epoch 138:     30 / 196 loss=3.155, nll_loss=2.209, ppl=4.62, wps=20238.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=26700, lr=0.000193528, gnorm=1.113, loss_scale=16, train_wall=292, gb_free=19.9, wall=85090
2022-03-07 12:23:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:25:22 | INFO | train_inner | epoch 138:    131 / 196 loss=3.122, nll_loss=2.174, ppl=4.51, wps=20826.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=26800, lr=0.000193167, gnorm=1.105, loss_scale=16, train_wall=292, gb_free=19.9, wall=85405
2022-03-07 12:28:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:28:49 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 10.765 | nll_loss 10.145 | ppl 1132.2 | wps 40610.1 | wpb 510.9 | bsz 1 | num_updates 26865 | best_loss 7.384
2022-03-07 12:28:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 138 @ 26865 updates
2022-03-07 12:28:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 12:28:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 12:28:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 138 @ 26865 updates, score 10.765) (writing took 3.4568635933101177 seconds)
2022-03-07 12:28:52 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2022-03-07 12:28:52 | INFO | train | epoch 138 | loss 3.137 | nll_loss 2.19 | ppl 4.56 | wps 20625.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 26865 | lr 0.000192933 | gnorm 1.113 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 85615
2022-03-07 12:28:52 | INFO | fairseq.trainer | begin training epoch 139
2022-03-07 12:28:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:30:42 | INFO | train_inner | epoch 139:     35 / 196 loss=3.146, nll_loss=2.2, ppl=4.6, wps=20445, ups=0.31, wpb=65367, bsz=127.7, num_updates=26900, lr=0.000192807, gnorm=1.118, loss_scale=32, train_wall=289, gb_free=19.9, wall=85724
2022-03-07 12:30:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:35:56 | INFO | train_inner | epoch 139:    136 / 196 loss=3.121, nll_loss=2.173, ppl=4.51, wps=20814.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=27000, lr=0.00019245, gnorm=1.11, loss_scale=16, train_wall=293, gb_free=19.9, wall=86039
2022-03-07 12:37:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:39:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:39:08 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 10.797 | nll_loss 10.185 | ppl 1164.01 | wps 40952.6 | wpb 510.9 | bsz 1 | num_updates 27059 | best_loss 7.384
2022-03-07 12:39:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 139 @ 27059 updates
2022-03-07 12:39:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 12:39:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 12:39:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 139 @ 27059 updates, score 10.797) (writing took 3.473690927028656 seconds)
2022-03-07 12:39:11 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2022-03-07 12:39:11 | INFO | train | epoch 139 | loss 3.131 | nll_loss 2.183 | ppl 4.54 | wps 20514.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27059 | lr 0.00019224 | gnorm 1.118 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 86234
2022-03-07 12:39:11 | INFO | fairseq.trainer | begin training epoch 140
2022-03-07 12:39:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:41:19 | INFO | train_inner | epoch 140:     41 / 196 loss=3.134, nll_loss=2.187, ppl=4.55, wps=20247, ups=0.31, wpb=65367, bsz=127.7, num_updates=27100, lr=0.000192095, gnorm=1.119, loss_scale=16, train_wall=292, gb_free=19.9, wall=86362
2022-03-07 12:44:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:46:34 | INFO | train_inner | epoch 140:    142 / 196 loss=3.126, nll_loss=2.178, ppl=4.52, wps=20818.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=27200, lr=0.000191741, gnorm=1.113, loss_scale=16, train_wall=293, gb_free=19.9, wall=86677
2022-03-07 12:49:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:49:27 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 10.808 | nll_loss 10.188 | ppl 1166.46 | wps 40728.5 | wpb 510.9 | bsz 1 | num_updates 27254 | best_loss 7.384
2022-03-07 12:49:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 140 @ 27254 updates
2022-03-07 12:49:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 12:49:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 12:49:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 140 @ 27254 updates, score 10.808) (writing took 3.492135195992887 seconds)
2022-03-07 12:49:30 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2022-03-07 12:49:30 | INFO | train | epoch 140 | loss 3.127 | nll_loss 2.179 | ppl 4.53 | wps 20617.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 27254 | lr 0.000191551 | gnorm 1.112 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 86853
2022-03-07 12:49:30 | INFO | fairseq.trainer | begin training epoch 141
2022-03-07 12:49:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 12:51:54 | INFO | train_inner | epoch 141:     46 / 196 loss=3.125, nll_loss=2.177, ppl=4.52, wps=20436.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=27300, lr=0.00019139, gnorm=1.119, loss_scale=32, train_wall=289, gb_free=19.9, wall=86997
2022-03-07 12:52:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 12:57:09 | INFO | train_inner | epoch 141:    147 / 196 loss=3.126, nll_loss=2.178, ppl=4.52, wps=20822.6, ups=0.32, wpb=65536, bsz=128, num_updates=27400, lr=0.00019104, gnorm=1.114, loss_scale=16, train_wall=293, gb_free=19.9, wall=87311
2022-03-07 12:59:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 12:59:46 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 10.769 | nll_loss 10.152 | ppl 1138.01 | wps 40586.5 | wpb 510.9 | bsz 1 | num_updates 27449 | best_loss 7.384
2022-03-07 12:59:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 141 @ 27449 updates
2022-03-07 12:59:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 12:59:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 12:59:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 141 @ 27449 updates, score 10.769) (writing took 3.4777399129234254 seconds)
2022-03-07 12:59:49 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2022-03-07 12:59:49 | INFO | train | epoch 141 | loss 3.123 | nll_loss 2.175 | ppl 4.51 | wps 20611.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27449 | lr 0.00019087 | gnorm 1.113 | loss_scale 32 | train_wall 567 | gb_free 19.9 | wall 87472
2022-03-07 12:59:50 | INFO | fairseq.trainer | begin training epoch 142
2022-03-07 12:59:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:01:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:02:32 | INFO | train_inner | epoch 142:     52 / 196 loss=3.116, nll_loss=2.168, ppl=4.49, wps=20231.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=27500, lr=0.000190693, gnorm=1.109, loss_scale=16, train_wall=292, gb_free=19.9, wall=87634
2022-03-07 13:07:43 | INFO | train_inner | epoch 142:    152 / 196 loss=3.122, nll_loss=2.174, ppl=4.51, wps=21021.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=27600, lr=0.000190347, gnorm=1.118, loss_scale=16, train_wall=290, gb_free=19.9, wall=87946
2022-03-07 13:08:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:10:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:10:05 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 10.829 | nll_loss 10.211 | ppl 1185.21 | wps 40769.8 | wpb 510.9 | bsz 1 | num_updates 27643 | best_loss 7.384
2022-03-07 13:10:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 142 @ 27643 updates
2022-03-07 13:10:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 13:10:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 13:10:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 142 @ 27643 updates, score 10.829) (writing took 3.489179353695363 seconds)
2022-03-07 13:10:09 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2022-03-07 13:10:09 | INFO | train | epoch 142 | loss 3.117 | nll_loss 2.168 | ppl 4.49 | wps 20509.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 27643 | lr 0.000190199 | gnorm 1.114 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 88091
2022-03-07 13:10:09 | INFO | fairseq.trainer | begin training epoch 143
2022-03-07 13:10:09 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:13:07 | INFO | train_inner | epoch 143:     57 / 196 loss=3.106, nll_loss=2.156, ppl=4.46, wps=20230.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=27700, lr=0.000190003, gnorm=1.117, loss_scale=16, train_wall=292, gb_free=19.9, wall=88269
2022-03-07 13:15:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:18:21 | INFO | train_inner | epoch 143:    158 / 196 loss=3.117, nll_loss=2.169, ppl=4.5, wps=20815.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=27800, lr=0.000189661, gnorm=1.124, loss_scale=16, train_wall=293, gb_free=19.9, wall=88584
2022-03-07 13:20:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:20:24 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 10.824 | nll_loss 10.209 | ppl 1183.9 | wps 40976.9 | wpb 510.9 | bsz 1 | num_updates 27838 | best_loss 7.384
2022-03-07 13:20:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 143 @ 27838 updates
2022-03-07 13:20:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 13:20:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 13:20:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 143 @ 27838 updates, score 10.824) (writing took 3.505481810774654 seconds)
2022-03-07 13:20:28 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2022-03-07 13:20:28 | INFO | train | epoch 143 | loss 3.112 | nll_loss 2.163 | ppl 4.48 | wps 20608.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 27838 | lr 0.000189531 | gnorm 1.127 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 88711
2022-03-07 13:20:28 | INFO | fairseq.trainer | begin training epoch 144
2022-03-07 13:20:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:22:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:23:44 | INFO | train_inner | epoch 144:     63 / 196 loss=3.107, nll_loss=2.158, ppl=4.46, wps=20239.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=27900, lr=0.000189321, gnorm=1.139, loss_scale=16, train_wall=292, gb_free=19.9, wall=88907
2022-03-07 13:28:56 | INFO | train_inner | epoch 144:    163 / 196 loss=3.117, nll_loss=2.168, ppl=4.49, wps=21022.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=28000, lr=0.000188982, gnorm=1.124, loss_scale=16, train_wall=290, gb_free=19.9, wall=89219
2022-03-07 13:29:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:30:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:30:43 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 10.893 | nll_loss 10.286 | ppl 1248.82 | wps 40900.9 | wpb 510.9 | bsz 1 | num_updates 28032 | best_loss 7.384
2022-03-07 13:30:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 144 @ 28032 updates
2022-03-07 13:30:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 13:30:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 13:30:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 144 @ 28032 updates, score 10.893) (writing took 3.4516083961352706 seconds)
2022-03-07 13:30:47 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2022-03-07 13:30:47 | INFO | train | epoch 144 | loss 3.108 | nll_loss 2.159 | ppl 4.46 | wps 20509.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28032 | lr 0.000188874 | gnorm 1.127 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 89330
2022-03-07 13:30:47 | INFO | fairseq.trainer | begin training epoch 145
2022-03-07 13:30:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:34:19 | INFO | train_inner | epoch 145:     68 / 196 loss=3.093, nll_loss=2.143, ppl=4.42, wps=20237, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28100, lr=0.000188646, gnorm=1.116, loss_scale=16, train_wall=292, gb_free=19.9, wall=89542
2022-03-07 13:36:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:39:34 | INFO | train_inner | epoch 145:    169 / 196 loss=3.118, nll_loss=2.17, ppl=4.5, wps=20813.8, ups=0.32, wpb=65536, bsz=128, num_updates=28200, lr=0.000188311, gnorm=1.133, loss_scale=16, train_wall=293, gb_free=19.9, wall=89857
2022-03-07 13:40:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:41:03 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 10.831 | nll_loss 10.216 | ppl 1189.2 | wps 40676.4 | wpb 510.9 | bsz 1 | num_updates 28227 | best_loss 7.384
2022-03-07 13:41:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 145 @ 28227 updates
2022-03-07 13:41:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 13:41:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 13:41:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 145 @ 28227 updates, score 10.831) (writing took 3.472929713781923 seconds)
2022-03-07 13:41:06 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2022-03-07 13:41:06 | INFO | train | epoch 145 | loss 3.104 | nll_loss 2.154 | ppl 4.45 | wps 20607.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 28227 | lr 0.000188221 | gnorm 1.128 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 89949
2022-03-07 13:41:06 | INFO | fairseq.trainer | begin training epoch 146
2022-03-07 13:41:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:43:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:44:57 | INFO | train_inner | epoch 146:     74 / 196 loss=3.084, nll_loss=2.133, ppl=4.39, wps=20232.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=28300, lr=0.000187978, gnorm=1.131, loss_scale=16, train_wall=292, gb_free=19.9, wall=90180
2022-03-07 13:50:09 | INFO | train_inner | epoch 146:    174 / 196 loss=3.118, nll_loss=2.169, ppl=4.5, wps=21034.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=28400, lr=0.000187647, gnorm=1.141, loss_scale=16, train_wall=290, gb_free=19.9, wall=90491
2022-03-07 13:50:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 13:51:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 13:51:22 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 10.833 | nll_loss 10.219 | ppl 1192.19 | wps 40667.5 | wpb 510.9 | bsz 1 | num_updates 28421 | best_loss 7.384
2022-03-07 13:51:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 146 @ 28421 updates
2022-03-07 13:51:22 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 13:51:25 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 13:51:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 146 @ 28421 updates, score 10.833) (writing took 3.4944768347777426 seconds)
2022-03-07 13:51:25 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2022-03-07 13:51:25 | INFO | train | epoch 146 | loss 3.099 | nll_loss 2.149 | ppl 4.43 | wps 20514.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28421 | lr 0.000187577 | gnorm 1.133 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 90568
2022-03-07 13:51:25 | INFO | fairseq.trainer | begin training epoch 147
2022-03-07 13:51:25 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 13:55:31 | INFO | train_inner | epoch 147:     79 / 196 loss=3.074, nll_loss=2.122, ppl=4.35, wps=20239.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=28500, lr=0.000187317, gnorm=1.097, loss_scale=16, train_wall=292, gb_free=19.9, wall=90814
2022-03-07 13:57:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:00:46 | INFO | train_inner | epoch 147:    180 / 196 loss=3.119, nll_loss=2.171, ppl=4.5, wps=20823.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=28600, lr=0.000186989, gnorm=1.138, loss_scale=16, train_wall=292, gb_free=19.9, wall=91129
2022-03-07 14:01:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:01:41 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 10.895 | nll_loss 10.283 | ppl 1245.55 | wps 40792.5 | wpb 510.9 | bsz 1 | num_updates 28616 | best_loss 7.384
2022-03-07 14:01:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 147 @ 28616 updates
2022-03-07 14:01:41 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 14:01:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 14:01:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 147 @ 28616 updates, score 10.895) (writing took 3.4824180267751217 seconds)
2022-03-07 14:01:44 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2022-03-07 14:01:44 | INFO | train | epoch 147 | loss 3.095 | nll_loss 2.144 | ppl 4.42 | wps 20617.5 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 28616 | lr 0.000186937 | gnorm 1.12 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 91187
2022-03-07 14:01:44 | INFO | fairseq.trainer | begin training epoch 148
2022-03-07 14:01:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:04:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:06:09 | INFO | train_inner | epoch 148:     85 / 196 loss=3.068, nll_loss=2.116, ppl=4.33, wps=20244.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28700, lr=0.000186663, gnorm=1.121, loss_scale=16, train_wall=292, gb_free=19.9, wall=91452
2022-03-07 14:10:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:11:24 | INFO | train_inner | epoch 148:    186 / 196 loss=3.118, nll_loss=2.17, ppl=4.5, wps=20824.9, ups=0.32, wpb=65536, bsz=128, num_updates=28800, lr=0.000186339, gnorm=1.129, loss_scale=16, train_wall=293, gb_free=19.9, wall=91767
2022-03-07 14:11:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:12:00 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 10.831 | nll_loss 10.216 | ppl 1189.75 | wps 40672.8 | wpb 510.9 | bsz 1 | num_updates 28810 | best_loss 7.384
2022-03-07 14:12:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 148 @ 28810 updates
2022-03-07 14:12:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 14:12:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 14:12:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 148 @ 28810 updates, score 10.831) (writing took 3.5028857411816716 seconds)
2022-03-07 14:12:03 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2022-03-07 14:12:03 | INFO | train | epoch 148 | loss 3.09 | nll_loss 2.139 | ppl 4.4 | wps 20512.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 28810 | lr 0.000186307 | gnorm 1.124 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 91806
2022-03-07 14:12:03 | INFO | fairseq.trainer | begin training epoch 149
2022-03-07 14:12:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:16:44 | INFO | train_inner | epoch 149:     90 / 196 loss=3.049, nll_loss=2.095, ppl=4.27, wps=20425.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=28900, lr=0.000186016, gnorm=1.118, loss_scale=16, train_wall=289, gb_free=19.9, wall=92087
2022-03-07 14:18:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:21:58 | INFO | train_inner | epoch 149:    191 / 196 loss=3.124, nll_loss=2.177, ppl=4.52, wps=20828.1, ups=0.32, wpb=65536, bsz=128, num_updates=29000, lr=0.000185695, gnorm=1.132, loss_scale=16, train_wall=292, gb_free=19.9, wall=92401
2022-03-07 14:22:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:22:19 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 10.861 | nll_loss 10.249 | ppl 1216.75 | wps 40121.7 | wpb 510.9 | bsz 1 | num_updates 29005 | best_loss 7.384
2022-03-07 14:22:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 149 @ 29005 updates
2022-03-07 14:22:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 14:22:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 14:22:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 149 @ 29005 updates, score 10.861) (writing took 3.4789263801649213 seconds)
2022-03-07 14:22:22 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2022-03-07 14:22:22 | INFO | train | epoch 149 | loss 3.086 | nll_loss 2.135 | ppl 4.39 | wps 20613.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 29005 | lr 0.000185679 | gnorm 1.125 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 92425
2022-03-07 14:22:22 | INFO | fairseq.trainer | begin training epoch 150
2022-03-07 14:22:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:25:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:27:22 | INFO | train_inner | epoch 150:     96 / 196 loss=3.054, nll_loss=2.1, ppl=4.29, wps=20227.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=29100, lr=0.000185376, gnorm=1.136, loss_scale=16, train_wall=292, gb_free=19.9, wall=92724
2022-03-07 14:31:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:32:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:32:38 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 10.851 | nll_loss 10.239 | ppl 1208.66 | wps 40789.2 | wpb 510.9 | bsz 1 | num_updates 29199 | best_loss 7.384
2022-03-07 14:32:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 150 @ 29199 updates
2022-03-07 14:32:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 14:32:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 14:32:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 150 @ 29199 updates, score 10.851) (writing took 3.5054267439991236 seconds)
2022-03-07 14:32:41 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2022-03-07 14:32:41 | INFO | train | epoch 150 | loss 3.082 | nll_loss 2.13 | ppl 4.38 | wps 20508.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 29199 | lr 0.000185061 | gnorm 1.143 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 93044
2022-03-07 14:32:41 | INFO | fairseq.trainer | begin training epoch 151
2022-03-07 14:32:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:32:45 | INFO | train_inner | epoch 151:      1 / 196 loss=3.112, nll_loss=2.164, ppl=4.48, wps=20239.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=29200, lr=0.000185058, gnorm=1.151, loss_scale=16, train_wall=292, gb_free=19.9, wall=93047
2022-03-07 14:37:56 | INFO | train_inner | epoch 151:    101 / 196 loss=3.035, nll_loss=2.08, ppl=4.23, wps=21020, ups=0.32, wpb=65532.4, bsz=128, num_updates=29300, lr=0.000184742, gnorm=1.112, loss_scale=16, train_wall=290, gb_free=19.9, wall=93359
2022-03-07 14:39:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:42:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:42:57 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 10.831 | nll_loss 10.214 | ppl 1187.78 | wps 40766.4 | wpb 510.9 | bsz 1 | num_updates 29394 | best_loss 7.384
2022-03-07 14:42:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 151 @ 29394 updates
2022-03-07 14:42:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 14:43:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 14:43:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 151 @ 29394 updates, score 10.831) (writing took 3.4658692800439894 seconds)
2022-03-07 14:43:00 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2022-03-07 14:43:00 | INFO | train | epoch 151 | loss 3.077 | nll_loss 2.125 | ppl 4.36 | wps 20614 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 29394 | lr 0.000184447 | gnorm 1.132 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 93663
2022-03-07 14:43:00 | INFO | fairseq.trainer | begin training epoch 152
2022-03-07 14:43:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:43:19 | INFO | train_inner | epoch 152:      6 / 196 loss=3.115, nll_loss=2.167, ppl=4.49, wps=20242.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=29400, lr=0.000184428, gnorm=1.149, loss_scale=16, train_wall=292, gb_free=19.9, wall=93682
2022-03-07 14:46:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:48:34 | INFO | train_inner | epoch 152:    107 / 196 loss=3.036, nll_loss=2.081, ppl=4.23, wps=20813.7, ups=0.32, wpb=65536, bsz=128, num_updates=29500, lr=0.000184115, gnorm=1.119, loss_scale=16, train_wall=293, gb_free=19.9, wall=93997
2022-03-07 14:53:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 14:53:16 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 10.911 | nll_loss 10.294 | ppl 1255.23 | wps 40808.3 | wpb 510.9 | bsz 1 | num_updates 29589 | best_loss 7.384
2022-03-07 14:53:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 152 @ 29589 updates
2022-03-07 14:53:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 14:53:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 14:53:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 152 @ 29589 updates, score 10.911) (writing took 3.514648698270321 seconds)
2022-03-07 14:53:20 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2022-03-07 14:53:20 | INFO | train | epoch 152 | loss 3.073 | nll_loss 2.121 | ppl 4.35 | wps 20610.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 29589 | lr 0.000183838 | gnorm 1.127 | loss_scale 32 | train_wall 567 | gb_free 19.9 | wall 94282
2022-03-07 14:53:20 | INFO | fairseq.trainer | begin training epoch 153
2022-03-07 14:53:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 14:53:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 14:53:57 | INFO | train_inner | epoch 153:     12 / 196 loss=3.104, nll_loss=2.154, ppl=4.45, wps=20232.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=29600, lr=0.000183804, gnorm=1.14, loss_scale=16, train_wall=292, gb_free=19.9, wall=94320
2022-03-07 14:59:09 | INFO | train_inner | epoch 153:    112 / 196 loss=3.043, nll_loss=2.089, ppl=4.25, wps=21025.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=29700, lr=0.000183494, gnorm=1.12, loss_scale=16, train_wall=290, gb_free=19.9, wall=94632
2022-03-07 15:00:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:03:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:03:35 | INFO | valid | epoch 153 | valid on 'valid' subset | loss 10.893 | nll_loss 10.281 | ppl 1243.79 | wps 40714.1 | wpb 510.9 | bsz 1 | num_updates 29783 | best_loss 7.384
2022-03-07 15:03:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 153 @ 29783 updates
2022-03-07 15:03:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 15:03:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 15:03:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 153 @ 29783 updates, score 10.893) (writing took 3.307526408229023 seconds)
2022-03-07 15:03:38 | INFO | fairseq_cli.train | end of epoch 153 (average epoch stats below)
2022-03-07 15:03:38 | INFO | train | epoch 153 | loss 3.068 | nll_loss 2.115 | ppl 4.33 | wps 20517.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 29783 | lr 0.000183238 | gnorm 1.138 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 94901
2022-03-07 15:03:38 | INFO | fairseq.trainer | begin training epoch 154
2022-03-07 15:03:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:04:31 | INFO | train_inner | epoch 154:     17 / 196 loss=3.087, nll_loss=2.136, ppl=4.4, wps=20259.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=29800, lr=0.000183186, gnorm=1.151, loss_scale=16, train_wall=292, gb_free=19.9, wall=94954
2022-03-07 15:07:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:09:46 | INFO | train_inner | epoch 154:    118 / 196 loss=3.042, nll_loss=2.087, ppl=4.25, wps=20827.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=29900, lr=0.000182879, gnorm=1.129, loss_scale=16, train_wall=292, gb_free=19.9, wall=95269
2022-03-07 15:13:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:13:54 | INFO | valid | epoch 154 | valid on 'valid' subset | loss 10.892 | nll_loss 10.279 | ppl 1242.63 | wps 40711 | wpb 510.9 | bsz 1 | num_updates 29978 | best_loss 7.384
2022-03-07 15:13:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 154 @ 29978 updates
2022-03-07 15:13:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 15:13:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 15:13:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 154 @ 29978 updates, score 10.892) (writing took 3.336169306188822 seconds)
2022-03-07 15:13:57 | INFO | fairseq_cli.train | end of epoch 154 (average epoch stats below)
2022-03-07 15:13:57 | INFO | train | epoch 154 | loss 3.064 | nll_loss 2.112 | ppl 4.32 | wps 20629.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 29978 | lr 0.000182641 | gnorm 1.133 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 95520
2022-03-07 15:13:57 | INFO | fairseq.trainer | begin training epoch 155
2022-03-07 15:13:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:15:06 | INFO | train_inner | epoch 155:     22 / 196 loss=3.085, nll_loss=2.135, ppl=4.39, wps=20450.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=30000, lr=0.000182574, gnorm=1.129, loss_scale=32, train_wall=289, gb_free=19.9, wall=95589
2022-03-07 15:15:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:20:20 | INFO | train_inner | epoch 155:    123 / 196 loss=3.046, nll_loss=2.092, ppl=4.26, wps=20824.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=30100, lr=0.000182271, gnorm=1.129, loss_scale=16, train_wall=293, gb_free=19.9, wall=95903
2022-03-07 15:22:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:24:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:24:12 | INFO | valid | epoch 155 | valid on 'valid' subset | loss 10.878 | nll_loss 10.267 | ppl 1232.21 | wps 40964.1 | wpb 510.9 | bsz 1 | num_updates 30172 | best_loss 7.384
2022-03-07 15:24:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 155 @ 30172 updates
2022-03-07 15:24:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 15:24:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 15:24:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 155 @ 30172 updates, score 10.878) (writing took 3.3191625676117837 seconds)
2022-03-07 15:24:16 | INFO | fairseq_cli.train | end of epoch 155 (average epoch stats below)
2022-03-07 15:24:16 | INFO | train | epoch 155 | loss 3.06 | nll_loss 2.108 | ppl 4.31 | wps 20523.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 30172 | lr 0.000182053 | gnorm 1.133 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 96139
2022-03-07 15:24:16 | INFO | fairseq.trainer | begin training epoch 156
2022-03-07 15:24:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:25:43 | INFO | train_inner | epoch 156:     28 / 196 loss=3.074, nll_loss=2.122, ppl=4.35, wps=20262.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=30200, lr=0.000181969, gnorm=1.143, loss_scale=16, train_wall=292, gb_free=19.9, wall=96226
2022-03-07 15:30:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:30:58 | INFO | train_inner | epoch 156:    129 / 196 loss=3.042, nll_loss=2.088, ppl=4.25, wps=20824.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=30300, lr=0.000181668, gnorm=1.121, loss_scale=16, train_wall=293, gb_free=19.9, wall=96541
2022-03-07 15:34:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:34:31 | INFO | valid | epoch 156 | valid on 'valid' subset | loss 10.888 | nll_loss 10.275 | ppl 1238.63 | wps 40920.5 | wpb 510.9 | bsz 1 | num_updates 30367 | best_loss 7.384
2022-03-07 15:34:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 156 @ 30367 updates
2022-03-07 15:34:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 15:34:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 15:34:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 156 @ 30367 updates, score 10.888) (writing took 3.320011196192354 seconds)
2022-03-07 15:34:34 | INFO | fairseq_cli.train | end of epoch 156 (average epoch stats below)
2022-03-07 15:34:34 | INFO | train | epoch 156 | loss 3.057 | nll_loss 2.104 | ppl 4.3 | wps 20632.4 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 30367 | lr 0.000181468 | gnorm 1.126 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 96757
2022-03-07 15:34:34 | INFO | fairseq.trainer | begin training epoch 157
2022-03-07 15:34:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:36:17 | INFO | train_inner | epoch 157:     33 / 196 loss=3.065, nll_loss=2.112, ppl=4.32, wps=20456.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=30400, lr=0.000181369, gnorm=1.133, loss_scale=16, train_wall=289, gb_free=19.9, wall=96860
2022-03-07 15:37:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:41:32 | INFO | train_inner | epoch 157:    134 / 196 loss=3.042, nll_loss=2.088, ppl=4.25, wps=20826.1, ups=0.32, wpb=65536, bsz=128, num_updates=30500, lr=0.000181071, gnorm=1.128, loss_scale=16, train_wall=293, gb_free=19.9, wall=97175
2022-03-07 15:44:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:44:50 | INFO | valid | epoch 157 | valid on 'valid' subset | loss 10.859 | nll_loss 10.248 | ppl 1215.84 | wps 40660 | wpb 510.9 | bsz 1 | num_updates 30562 | best_loss 7.384
2022-03-07 15:44:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 157 @ 30562 updates
2022-03-07 15:44:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 15:44:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 15:44:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 157 @ 30562 updates, score 10.859) (writing took 3.3342204610817134 seconds)
2022-03-07 15:44:53 | INFO | fairseq_cli.train | end of epoch 157 (average epoch stats below)
2022-03-07 15:44:53 | INFO | train | epoch 157 | loss 3.053 | nll_loss 2.1 | ppl 4.29 | wps 20623.7 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 30562 | lr 0.000180888 | gnorm 1.135 | loss_scale 32 | train_wall 567 | gb_free 19.9 | wall 97376
2022-03-07 15:44:53 | INFO | fairseq.trainer | begin training epoch 158
2022-03-07 15:44:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:45:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:46:55 | INFO | train_inner | epoch 158:     39 / 196 loss=3.061, nll_loss=2.108, ppl=4.31, wps=20256, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=30600, lr=0.000180775, gnorm=1.141, loss_scale=16, train_wall=292, gb_free=19.9, wall=97497
2022-03-07 15:52:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 15:52:09 | INFO | train_inner | epoch 158:    140 / 196 loss=3.044, nll_loss=2.089, ppl=4.25, wps=20822.7, ups=0.32, wpb=65536, bsz=128, num_updates=30700, lr=0.000180481, gnorm=1.144, loss_scale=16, train_wall=293, gb_free=19.9, wall=97812
2022-03-07 15:55:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 15:55:08 | INFO | valid | epoch 158 | valid on 'valid' subset | loss 10.95 | nll_loss 10.345 | ppl 1300.8 | wps 40665.5 | wpb 510.9 | bsz 1 | num_updates 30756 | best_loss 7.384
2022-03-07 15:55:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 158 @ 30756 updates
2022-03-07 15:55:08 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 15:55:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 15:55:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 158 @ 30756 updates, score 10.95) (writing took 3.313073500059545 seconds)
2022-03-07 15:55:12 | INFO | fairseq_cli.train | end of epoch 158 (average epoch stats below)
2022-03-07 15:55:12 | INFO | train | epoch 158 | loss 3.048 | nll_loss 2.094 | ppl 4.27 | wps 20523 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 30756 | lr 0.000180316 | gnorm 1.141 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 97995
2022-03-07 15:55:12 | INFO | fairseq.trainer | begin training epoch 159
2022-03-07 15:55:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 15:57:29 | INFO | train_inner | epoch 159:     44 / 196 loss=3.045, nll_loss=2.091, ppl=4.26, wps=20454.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=30800, lr=0.000180187, gnorm=1.128, loss_scale=16, train_wall=289, gb_free=19.9, wall=98132
2022-03-07 15:59:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:02:44 | INFO | train_inner | epoch 159:    145 / 196 loss=3.047, nll_loss=2.093, ppl=4.27, wps=20822, ups=0.32, wpb=65536, bsz=128, num_updates=30900, lr=0.000179896, gnorm=1.149, loss_scale=16, train_wall=293, gb_free=19.9, wall=98446
2022-03-07 16:05:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:05:27 | INFO | valid | epoch 159 | valid on 'valid' subset | loss 10.922 | nll_loss 10.307 | ppl 1267.26 | wps 40643.5 | wpb 510.9 | bsz 1 | num_updates 30951 | best_loss 7.384
2022-03-07 16:05:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 159 @ 30951 updates
2022-03-07 16:05:27 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 16:05:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 16:05:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 159 @ 30951 updates, score 10.922) (writing took 3.3198104449547827 seconds)
2022-03-07 16:05:31 | INFO | fairseq_cli.train | end of epoch 159 (average epoch stats below)
2022-03-07 16:05:31 | INFO | train | epoch 159 | loss 3.045 | nll_loss 2.091 | ppl 4.26 | wps 20616.6 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 30951 | lr 0.000179747 | gnorm 1.143 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 98614
2022-03-07 16:05:31 | INFO | fairseq.trainer | begin training epoch 160
2022-03-07 16:05:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:06:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:08:07 | INFO | train_inner | epoch 160:     50 / 196 loss=3.038, nll_loss=2.083, ppl=4.24, wps=20240, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=31000, lr=0.000179605, gnorm=1.147, loss_scale=16, train_wall=292, gb_free=19.9, wall=98769
2022-03-07 16:12:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:13:21 | INFO | train_inner | epoch 160:    151 / 196 loss=3.043, nll_loss=2.088, ppl=4.25, wps=20823.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=31100, lr=0.000179316, gnorm=1.143, loss_scale=16, train_wall=293, gb_free=19.9, wall=99084
2022-03-07 16:15:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:15:46 | INFO | valid | epoch 160 | valid on 'valid' subset | loss 10.918 | nll_loss 10.306 | ppl 1265.71 | wps 40914.2 | wpb 510.9 | bsz 1 | num_updates 31145 | best_loss 7.384
2022-03-07 16:15:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 160 @ 31145 updates
2022-03-07 16:15:46 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 16:15:49 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 16:15:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 160 @ 31145 updates, score 10.918) (writing took 3.3441638089716434 seconds)
2022-03-07 16:15:49 | INFO | fairseq_cli.train | end of epoch 160 (average epoch stats below)
2022-03-07 16:15:49 | INFO | train | epoch 160 | loss 3.04 | nll_loss 2.086 | ppl 4.25 | wps 20523.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 31145 | lr 0.000179187 | gnorm 1.148 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 99232
2022-03-07 16:15:49 | INFO | fairseq.trainer | begin training epoch 161
2022-03-07 16:15:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:18:41 | INFO | train_inner | epoch 161:     55 / 196 loss=3.036, nll_loss=2.081, ppl=4.23, wps=20454.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=31200, lr=0.000179029, gnorm=1.165, loss_scale=16, train_wall=289, gb_free=19.9, wall=99404
2022-03-07 16:19:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:23:56 | INFO | train_inner | epoch 161:    156 / 196 loss=3.042, nll_loss=2.088, ppl=4.25, wps=20825.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=31300, lr=0.000178743, gnorm=1.142, loss_scale=16, train_wall=292, gb_free=19.9, wall=99718
2022-03-07 16:25:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:26:05 | INFO | valid | epoch 161 | valid on 'valid' subset | loss 10.938 | nll_loss 10.327 | ppl 1284.18 | wps 40875.4 | wpb 510.9 | bsz 1 | num_updates 31340 | best_loss 7.384
2022-03-07 16:26:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 161 @ 31340 updates
2022-03-07 16:26:05 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 16:26:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 16:26:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 161 @ 31340 updates, score 10.938) (writing took 3.334975666832179 seconds)
2022-03-07 16:26:08 | INFO | fairseq_cli.train | end of epoch 161 (average epoch stats below)
2022-03-07 16:26:08 | INFO | train | epoch 161 | loss 3.036 | nll_loss 2.082 | ppl 4.23 | wps 20626.8 | ups 0.32 | wpb 65447.5 | bsz 127.8 | num_updates 31340 | lr 0.000178628 | gnorm 1.15 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 99851
2022-03-07 16:26:08 | INFO | fairseq.trainer | begin training epoch 162
2022-03-07 16:26:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:26:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:29:18 | INFO | train_inner | epoch 162:     61 / 196 loss=3.031, nll_loss=2.075, ppl=4.21, wps=20258.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=31400, lr=0.000178458, gnorm=1.154, loss_scale=16, train_wall=292, gb_free=19.9, wall=100041
2022-03-07 16:33:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:34:33 | INFO | train_inner | epoch 162:    162 / 196 loss=3.045, nll_loss=2.091, ppl=4.26, wps=20822.2, ups=0.32, wpb=65536, bsz=128, num_updates=31500, lr=0.000178174, gnorm=1.148, loss_scale=16, train_wall=293, gb_free=19.9, wall=100356
2022-03-07 16:36:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:36:23 | INFO | valid | epoch 162 | valid on 'valid' subset | loss 10.95 | nll_loss 10.339 | ppl 1294.88 | wps 41047.3 | wpb 510.9 | bsz 1 | num_updates 31534 | best_loss 7.384
2022-03-07 16:36:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 162 @ 31534 updates
2022-03-07 16:36:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 16:36:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 16:36:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 162 @ 31534 updates, score 10.95) (writing took 3.4500370011664927 seconds)
2022-03-07 16:36:27 | INFO | fairseq_cli.train | end of epoch 162 (average epoch stats below)
2022-03-07 16:36:27 | INFO | train | epoch 162 | loss 3.033 | nll_loss 2.077 | ppl 4.22 | wps 20517.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 31534 | lr 0.000178078 | gnorm 1.149 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 100470
2022-03-07 16:36:27 | INFO | fairseq.trainer | begin training epoch 163
2022-03-07 16:36:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:39:53 | INFO | train_inner | epoch 163:     66 / 196 loss=3.009, nll_loss=2.052, ppl=4.15, wps=20433.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=31600, lr=0.000177892, gnorm=1.132, loss_scale=16, train_wall=289, gb_free=19.9, wall=100676
2022-03-07 16:40:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:45:08 | INFO | train_inner | epoch 163:    167 / 196 loss=3.051, nll_loss=2.098, ppl=4.28, wps=20805.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=31700, lr=0.000177611, gnorm=1.14, loss_scale=16, train_wall=293, gb_free=19.9, wall=100991
2022-03-07 16:46:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:46:43 | INFO | valid | epoch 163 | valid on 'valid' subset | loss 10.955 | nll_loss 10.346 | ppl 1301.65 | wps 40816.1 | wpb 510.9 | bsz 1 | num_updates 31729 | best_loss 7.384
2022-03-07 16:46:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 163 @ 31729 updates
2022-03-07 16:46:43 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 16:46:46 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 16:46:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 163 @ 31729 updates, score 10.955) (writing took 3.4206473869271576 seconds)
2022-03-07 16:46:46 | INFO | fairseq_cli.train | end of epoch 163 (average epoch stats below)
2022-03-07 16:46:46 | INFO | train | epoch 163 | loss 3.03 | nll_loss 2.075 | ppl 4.21 | wps 20604.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 31729 | lr 0.00017753 | gnorm 1.139 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 101089
2022-03-07 16:46:46 | INFO | fairseq.trainer | begin training epoch 164
2022-03-07 16:46:46 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 16:47:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:50:31 | INFO | train_inner | epoch 164:     72 / 196 loss=3.011, nll_loss=2.054, ppl=4.15, wps=20214.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=31800, lr=0.000177332, gnorm=1.148, loss_scale=16, train_wall=292, gb_free=19.9, wall=101314
2022-03-07 16:54:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 16:55:47 | INFO | train_inner | epoch 164:    173 / 196 loss=3.041, nll_loss=2.086, ppl=4.25, wps=20783.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=31900, lr=0.000177054, gnorm=1.147, loss_scale=16, train_wall=293, gb_free=19.9, wall=101629
2022-03-07 16:56:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 16:57:03 | INFO | valid | epoch 164 | valid on 'valid' subset | loss 10.931 | nll_loss 10.321 | ppl 1279.21 | wps 40855.9 | wpb 510.9 | bsz 1 | num_updates 31923 | best_loss 7.384
2022-03-07 16:57:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 164 @ 31923 updates
2022-03-07 16:57:03 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 16:57:06 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 16:57:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 164 @ 31923 updates, score 10.931) (writing took 3.3384879985824227 seconds)
2022-03-07 16:57:06 | INFO | fairseq_cli.train | end of epoch 164 (average epoch stats below)
2022-03-07 16:57:06 | INFO | train | epoch 164 | loss 3.025 | nll_loss 2.07 | ppl 4.2 | wps 20481.1 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 31923 | lr 0.00017699 | gnorm 1.142 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 101709
2022-03-07 16:57:06 | INFO | fairseq.trainer | begin training epoch 165
2022-03-07 16:57:06 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:01:07 | INFO | train_inner | epoch 165:     77 / 196 loss=3.005, nll_loss=2.048, ppl=4.14, wps=20401.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=32000, lr=0.000176777, gnorm=1.143, loss_scale=16, train_wall=289, gb_free=19.9, wall=101950
2022-03-07 17:02:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:06:23 | INFO | train_inner | epoch 165:    178 / 196 loss=3.046, nll_loss=2.092, ppl=4.26, wps=20760.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=32100, lr=0.000176501, gnorm=1.15, loss_scale=16, train_wall=293, gb_free=19.9, wall=102265
2022-03-07 17:07:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:07:24 | INFO | valid | epoch 165 | valid on 'valid' subset | loss 10.986 | nll_loss 10.38 | ppl 1332.7 | wps 40496.6 | wpb 510.9 | bsz 1 | num_updates 32118 | best_loss 7.384
2022-03-07 17:07:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 165 @ 32118 updates
2022-03-07 17:07:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 17:07:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 17:07:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 165 @ 32118 updates, score 10.986) (writing took 3.314552792813629 seconds)
2022-03-07 17:07:27 | INFO | fairseq_cli.train | end of epoch 165 (average epoch stats below)
2022-03-07 17:07:27 | INFO | train | epoch 165 | loss 3.023 | nll_loss 2.067 | ppl 4.19 | wps 20564.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 32118 | lr 0.000176452 | gnorm 1.15 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 102330
2022-03-07 17:07:27 | INFO | fairseq.trainer | begin training epoch 166
2022-03-07 17:07:27 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:09:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:11:46 | INFO | train_inner | epoch 166:     83 / 196 loss=2.993, nll_loss=2.034, ppl=4.1, wps=20196.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=32200, lr=0.000176227, gnorm=1.139, loss_scale=16, train_wall=292, gb_free=19.9, wall=102589
2022-03-07 17:16:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:17:02 | INFO | train_inner | epoch 166:    184 / 196 loss=3.044, nll_loss=2.09, ppl=4.26, wps=20777.9, ups=0.32, wpb=65536, bsz=128, num_updates=32300, lr=0.000175954, gnorm=1.147, loss_scale=16, train_wall=293, gb_free=19.9, wall=102904
2022-03-07 17:17:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:17:44 | INFO | valid | epoch 166 | valid on 'valid' subset | loss 10.972 | nll_loss 10.367 | ppl 1320.72 | wps 40712.9 | wpb 510.9 | bsz 1 | num_updates 32312 | best_loss 7.384
2022-03-07 17:17:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 166 @ 32312 updates
2022-03-07 17:17:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 17:17:47 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 17:17:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 166 @ 32312 updates, score 10.972) (writing took 3.2748556761071086 seconds)
2022-03-07 17:17:47 | INFO | fairseq_cli.train | end of epoch 166 (average epoch stats below)
2022-03-07 17:17:47 | INFO | train | epoch 166 | loss 3.017 | nll_loss 2.061 | ppl 4.17 | wps 20472.5 | ups 0.31 | wpb 65448.9 | bsz 127.8 | num_updates 32312 | lr 0.000175921 | gnorm 1.142 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 102950
2022-03-07 17:17:47 | INFO | fairseq.trainer | begin training epoch 167
2022-03-07 17:17:47 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:22:22 | INFO | train_inner | epoch 167:     88 / 196 loss=2.987, nll_loss=2.029, ppl=4.08, wps=20409.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=32400, lr=0.000175682, gnorm=1.158, loss_scale=16, train_wall=289, gb_free=19.9, wall=103225
2022-03-07 17:22:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:27:37 | INFO | train_inner | epoch 167:    189 / 196 loss=3.046, nll_loss=2.092, ppl=4.26, wps=20778.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=32500, lr=0.000175412, gnorm=1.153, loss_scale=16, train_wall=293, gb_free=19.9, wall=103540
2022-03-07 17:27:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:28:04 | INFO | valid | epoch 167 | valid on 'valid' subset | loss 10.975 | nll_loss 10.364 | ppl 1318.13 | wps 40660.5 | wpb 510.9 | bsz 1 | num_updates 32507 | best_loss 7.384
2022-03-07 17:28:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 167 @ 32507 updates
2022-03-07 17:28:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 17:28:07 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 17:28:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 167 @ 32507 updates, score 10.975) (writing took 3.2804804369807243 seconds)
2022-03-07 17:28:07 | INFO | fairseq_cli.train | end of epoch 167 (average epoch stats below)
2022-03-07 17:28:07 | INFO | train | epoch 167 | loss 3.014 | nll_loss 2.058 | ppl 4.16 | wps 20582.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 32507 | lr 0.000175393 | gnorm 1.153 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 103570
2022-03-07 17:28:07 | INFO | fairseq.trainer | begin training epoch 168
2022-03-07 17:28:07 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:29:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:33:01 | INFO | train_inner | epoch 168:     94 / 196 loss=2.985, nll_loss=2.026, ppl=4.07, wps=20192.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=32600, lr=0.000175142, gnorm=1.131, loss_scale=16, train_wall=292, gb_free=19.9, wall=103864
2022-03-07 17:37:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:38:17 | INFO | train_inner | epoch 168:    195 / 196 loss=3.041, nll_loss=2.087, ppl=4.25, wps=20763.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=32700, lr=0.000174874, gnorm=1.151, loss_scale=16, train_wall=293, gb_free=19.9, wall=104179
2022-03-07 17:38:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:38:24 | INFO | valid | epoch 168 | valid on 'valid' subset | loss 10.979 | nll_loss 10.374 | ppl 1326.96 | wps 40919.4 | wpb 510.9 | bsz 1 | num_updates 32701 | best_loss 7.384
2022-03-07 17:38:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 168 @ 32701 updates
2022-03-07 17:38:24 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 17:38:27 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 17:38:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 168 @ 32701 updates, score 10.979) (writing took 3.1267270571552217 seconds)
2022-03-07 17:38:28 | INFO | fairseq_cli.train | end of epoch 168 (average epoch stats below)
2022-03-07 17:38:28 | INFO | train | epoch 168 | loss 3.011 | nll_loss 2.054 | ppl 4.15 | wps 20465.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 32701 | lr 0.000174872 | gnorm 1.142 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 104190
2022-03-07 17:38:28 | INFO | fairseq.trainer | begin training epoch 169
2022-03-07 17:38:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:43:37 | INFO | train_inner | epoch 169:     99 / 196 loss=2.974, nll_loss=2.014, ppl=4.04, wps=20418.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=32800, lr=0.000174608, gnorm=1.137, loss_scale=16, train_wall=289, gb_free=19.9, wall=104500
2022-03-07 17:44:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:48:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:48:44 | INFO | valid | epoch 169 | valid on 'valid' subset | loss 11.015 | nll_loss 10.411 | ppl 1361.17 | wps 41100.3 | wpb 510.9 | bsz 1 | num_updates 32896 | best_loss 7.384
2022-03-07 17:48:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 169 @ 32896 updates
2022-03-07 17:48:44 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 17:48:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 17:48:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 169 @ 32896 updates, score 11.015) (writing took 3.7044927678070962 seconds)
2022-03-07 17:48:48 | INFO | fairseq_cli.train | end of epoch 169 (average epoch stats below)
2022-03-07 17:48:48 | INFO | train | epoch 169 | loss 3.008 | nll_loss 2.051 | ppl 4.14 | wps 20574.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 32896 | lr 0.000174353 | gnorm 1.148 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 104811
2022-03-07 17:48:48 | INFO | fairseq.trainer | begin training epoch 170
2022-03-07 17:48:48 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:49:00 | INFO | train_inner | epoch 170:      4 / 196 loss=3.04, nll_loss=2.086, ppl=4.25, wps=20200.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=32900, lr=0.000174342, gnorm=1.16, loss_scale=16, train_wall=292, gb_free=19.9, wall=104823
2022-03-07 17:51:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:54:16 | INFO | train_inner | epoch 170:    105 / 196 loss=2.973, nll_loss=2.013, ppl=4.03, wps=20784.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=33000, lr=0.000174078, gnorm=1.148, loss_scale=16, train_wall=293, gb_free=19.9, wall=105138
2022-03-07 17:58:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 17:58:59 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 17:59:04 | INFO | valid | epoch 170 | valid on 'valid' subset | loss 10.993 | nll_loss 10.384 | ppl 1336.54 | wps 40647.1 | wpb 510.9 | bsz 1 | num_updates 33090 | best_loss 7.384
2022-03-07 17:59:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 170 @ 33090 updates
2022-03-07 17:59:04 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 17:59:08 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 17:59:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 170 @ 33090 updates, score 10.993) (writing took 3.6512305117212236 seconds)
2022-03-07 17:59:08 | INFO | fairseq_cli.train | end of epoch 170 (average epoch stats below)
2022-03-07 17:59:08 | INFO | train | epoch 170 | loss 3.004 | nll_loss 2.047 | ppl 4.13 | wps 20471.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 33090 | lr 0.000173841 | gnorm 1.151 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 105431
2022-03-07 17:59:08 | INFO | fairseq.trainer | begin training epoch 171
2022-03-07 17:59:08 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 17:59:39 | INFO | train_inner | epoch 171:     10 / 196 loss=3.033, nll_loss=2.079, ppl=4.22, wps=20193.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=33100, lr=0.000173814, gnorm=1.154, loss_scale=16, train_wall=292, gb_free=19.9, wall=105462
2022-03-07 18:04:52 | INFO | train_inner | epoch 171:    110 / 196 loss=2.977, nll_loss=2.018, ppl=4.05, wps=20985.5, ups=0.32, wpb=65536, bsz=128, num_updates=33200, lr=0.000173553, gnorm=1.151, loss_scale=16, train_wall=290, gb_free=19.9, wall=105774
2022-03-07 18:05:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:09:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:09:25 | INFO | valid | epoch 171 | valid on 'valid' subset | loss 10.991 | nll_loss 10.389 | ppl 1340.5 | wps 40492.1 | wpb 510.9 | bsz 1 | num_updates 33285 | best_loss 7.384
2022-03-07 18:09:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 171 @ 33285 updates
2022-03-07 18:09:25 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 18:09:28 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 18:09:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 171 @ 33285 updates, score 10.991) (writing took 3.4118836391717196 seconds)
2022-03-07 18:09:28 | INFO | fairseq_cli.train | end of epoch 171 (average epoch stats below)
2022-03-07 18:09:28 | INFO | train | epoch 171 | loss 3.001 | nll_loss 2.044 | ppl 4.12 | wps 20581.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 33285 | lr 0.000173331 | gnorm 1.154 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 106051
2022-03-07 18:09:28 | INFO | fairseq.trainer | begin training epoch 172
2022-03-07 18:09:28 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:10:15 | INFO | train_inner | epoch 172:     15 / 196 loss=3.021, nll_loss=2.065, ppl=4.19, wps=20209.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=33300, lr=0.000173292, gnorm=1.156, loss_scale=16, train_wall=292, gb_free=19.9, wall=106098
2022-03-07 18:11:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:15:31 | INFO | train_inner | epoch 172:    116 / 196 loss=2.973, nll_loss=2.013, ppl=4.03, wps=20776.3, ups=0.32, wpb=65532.4, bsz=128, num_updates=33400, lr=0.000173032, gnorm=1.144, loss_scale=16, train_wall=293, gb_free=19.9, wall=106413
2022-03-07 18:19:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:19:45 | INFO | valid | epoch 172 | valid on 'valid' subset | loss 10.985 | nll_loss 10.382 | ppl 1333.98 | wps 40421.4 | wpb 510.9 | bsz 1 | num_updates 33480 | best_loss 7.384
2022-03-07 18:19:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 172 @ 33480 updates
2022-03-07 18:19:45 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 18:19:48 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 18:19:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 172 @ 33480 updates, score 10.985) (writing took 3.4897230761125684 seconds)
2022-03-07 18:19:49 | INFO | fairseq_cli.train | end of epoch 172 (average epoch stats below)
2022-03-07 18:19:49 | INFO | train | epoch 172 | loss 2.997 | nll_loss 2.04 | ppl 4.11 | wps 20568.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 33480 | lr 0.000172825 | gnorm 1.148 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 106671
2022-03-07 18:19:49 | INFO | fairseq.trainer | begin training epoch 173
2022-03-07 18:19:49 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:19:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:20:54 | INFO | train_inner | epoch 173:     21 / 196 loss=3.018, nll_loss=2.062, ppl=4.17, wps=20189.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=33500, lr=0.000172774, gnorm=1.161, loss_scale=16, train_wall=292, gb_free=19.9, wall=106737
2022-03-07 18:26:07 | INFO | train_inner | epoch 173:    121 / 196 loss=2.978, nll_loss=2.018, ppl=4.05, wps=20980, ups=0.32, wpb=65536, bsz=128, num_updates=33600, lr=0.000172516, gnorm=1.137, loss_scale=16, train_wall=290, gb_free=19.9, wall=107049
2022-03-07 18:26:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:30:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:30:06 | INFO | valid | epoch 173 | valid on 'valid' subset | loss 11.033 | nll_loss 10.428 | ppl 1377.45 | wps 40428.9 | wpb 510.9 | bsz 1 | num_updates 33674 | best_loss 7.384
2022-03-07 18:30:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 173 @ 33674 updates
2022-03-07 18:30:06 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 18:30:09 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 18:30:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 173 @ 33674 updates, score 11.033) (writing took 3.459181581158191 seconds)
2022-03-07 18:30:10 | INFO | fairseq_cli.train | end of epoch 173 (average epoch stats below)
2022-03-07 18:30:10 | INFO | train | epoch 173 | loss 2.994 | nll_loss 2.036 | ppl 4.1 | wps 20445.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 33674 | lr 0.000172327 | gnorm 1.159 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 107292
2022-03-07 18:30:10 | INFO | fairseq.trainer | begin training epoch 174
2022-03-07 18:30:10 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:31:31 | INFO | train_inner | epoch 174:     26 / 196 loss=3.006, nll_loss=2.049, ppl=4.14, wps=20154.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=33700, lr=0.00017226, gnorm=1.174, loss_scale=16, train_wall=293, gb_free=19.9, wall=107374
2022-03-07 18:33:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:36:46 | INFO | train_inner | epoch 174:    127 / 196 loss=2.983, nll_loss=2.024, ppl=4.07, wps=20776.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=33800, lr=0.000172005, gnorm=1.15, loss_scale=16, train_wall=293, gb_free=19.9, wall=107689
2022-03-07 18:40:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:40:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:40:26 | INFO | valid | epoch 174 | valid on 'valid' subset | loss 11 | nll_loss 10.392 | ppl 1343.28 | wps 40637.3 | wpb 510.9 | bsz 1 | num_updates 33868 | best_loss 7.384
2022-03-07 18:40:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 174 @ 33868 updates
2022-03-07 18:40:26 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 18:40:30 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 18:40:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 174 @ 33868 updates, score 11.0) (writing took 3.476555972825736 seconds)
2022-03-07 18:40:30 | INFO | fairseq_cli.train | end of epoch 174 (average epoch stats below)
2022-03-07 18:40:30 | INFO | train | epoch 174 | loss 2.992 | nll_loss 2.033 | ppl 4.09 | wps 20471.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 33868 | lr 0.000171832 | gnorm 1.157 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 107913
2022-03-07 18:40:30 | INFO | fairseq.trainer | begin training epoch 175
2022-03-07 18:40:30 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:42:10 | INFO | train_inner | epoch 175:     32 / 196 loss=2.997, nll_loss=2.039, ppl=4.11, wps=20211.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=33900, lr=0.000171751, gnorm=1.166, loss_scale=16, train_wall=292, gb_free=19.9, wall=108013
2022-03-07 18:47:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:47:25 | INFO | train_inner | epoch 175:    133 / 196 loss=2.98, nll_loss=2.02, ppl=4.06, wps=20772, ups=0.32, wpb=65532.4, bsz=128, num_updates=34000, lr=0.000171499, gnorm=1.157, loss_scale=16, train_wall=293, gb_free=19.9, wall=108328
2022-03-07 18:50:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 18:50:47 | INFO | valid | epoch 175 | valid on 'valid' subset | loss 11.037 | nll_loss 10.431 | ppl 1380.22 | wps 40547 | wpb 510.9 | bsz 1 | num_updates 34063 | best_loss 7.384
2022-03-07 18:50:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 175 @ 34063 updates
2022-03-07 18:50:47 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 18:50:50 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 18:50:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 175 @ 34063 updates, score 11.037) (writing took 3.4812468318268657 seconds)
2022-03-07 18:50:50 | INFO | fairseq_cli.train | end of epoch 175 (average epoch stats below)
2022-03-07 18:50:50 | INFO | train | epoch 175 | loss 2.988 | nll_loss 2.029 | ppl 4.08 | wps 20572 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 34063 | lr 0.00017134 | gnorm 1.158 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 108533
2022-03-07 18:50:50 | INFO | fairseq.trainer | begin training epoch 176
2022-03-07 18:50:50 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 18:52:46 | INFO | train_inner | epoch 176:     37 / 196 loss=2.99, nll_loss=2.032, ppl=4.09, wps=20389.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=34100, lr=0.000171247, gnorm=1.155, loss_scale=16, train_wall=289, gb_free=19.9, wall=108649
2022-03-07 18:54:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 18:58:01 | INFO | train_inner | epoch 176:    138 / 196 loss=2.981, nll_loss=2.022, ppl=4.06, wps=20771.4, ups=0.32, wpb=65536, bsz=128, num_updates=34200, lr=0.000170996, gnorm=1.162, loss_scale=16, train_wall=293, gb_free=19.9, wall=108964
2022-03-07 19:00:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:01:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:01:07 | INFO | valid | epoch 176 | valid on 'valid' subset | loss 11.038 | nll_loss 10.431 | ppl 1380.59 | wps 40811.6 | wpb 510.9 | bsz 1 | num_updates 34257 | best_loss 7.384
2022-03-07 19:01:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 176 @ 34257 updates
2022-03-07 19:01:07 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 19:01:11 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 19:01:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 176 @ 34257 updates, score 11.038) (writing took 3.49638290097937 seconds)
2022-03-07 19:01:11 | INFO | fairseq_cli.train | end of epoch 176 (average epoch stats below)
2022-03-07 19:01:11 | INFO | train | epoch 176 | loss 2.985 | nll_loss 2.026 | ppl 4.07 | wps 20463.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 34257 | lr 0.000170854 | gnorm 1.164 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 109153
2022-03-07 19:01:11 | INFO | fairseq.trainer | begin training epoch 177
2022-03-07 19:01:11 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:03:25 | INFO | train_inner | epoch 177:     43 / 196 loss=2.99, nll_loss=2.032, ppl=4.09, wps=20190, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=34300, lr=0.000170747, gnorm=1.169, loss_scale=16, train_wall=292, gb_free=19.9, wall=109288
2022-03-07 19:07:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:08:41 | INFO | train_inner | epoch 177:    144 / 196 loss=2.976, nll_loss=2.016, ppl=4.04, wps=20769.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=34400, lr=0.000170499, gnorm=1.157, loss_scale=16, train_wall=293, gb_free=19.9, wall=109603
2022-03-07 19:11:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:11:28 | INFO | valid | epoch 177 | valid on 'valid' subset | loss 11.044 | nll_loss 10.436 | ppl 1385.76 | wps 40657.9 | wpb 510.9 | bsz 1 | num_updates 34452 | best_loss 7.384
2022-03-07 19:11:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 177 @ 34452 updates
2022-03-07 19:11:28 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 19:11:31 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 19:11:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 177 @ 34452 updates, score 11.044) (writing took 3.406171659938991 seconds)
2022-03-07 19:11:31 | INFO | fairseq_cli.train | end of epoch 177 (average epoch stats below)
2022-03-07 19:11:31 | INFO | train | epoch 177 | loss 2.982 | nll_loss 2.023 | ppl 4.06 | wps 20569.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 34452 | lr 0.00017037 | gnorm 1.163 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 109774
2022-03-07 19:11:31 | INFO | fairseq.trainer | begin training epoch 178
2022-03-07 19:11:31 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:14:01 | INFO | train_inner | epoch 178:     48 / 196 loss=2.985, nll_loss=2.027, ppl=4.08, wps=20389.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=34500, lr=0.000170251, gnorm=1.17, loss_scale=16, train_wall=290, gb_free=19.9, wall=109924
2022-03-07 19:14:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:19:17 | INFO | train_inner | epoch 178:    149 / 196 loss=2.974, nll_loss=2.014, ppl=4.04, wps=20769.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=34600, lr=0.000170005, gnorm=1.151, loss_scale=16, train_wall=293, gb_free=19.9, wall=110240
2022-03-07 19:21:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:21:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:21:48 | INFO | valid | epoch 178 | valid on 'valid' subset | loss 11.067 | nll_loss 10.465 | ppl 1413 | wps 40690.7 | wpb 510.9 | bsz 1 | num_updates 34646 | best_loss 7.384
2022-03-07 19:21:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 178 @ 34646 updates
2022-03-07 19:21:48 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 19:21:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 19:21:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 178 @ 34646 updates, score 11.067) (writing took 3.390380530618131 seconds)
2022-03-07 19:21:52 | INFO | fairseq_cli.train | end of epoch 178 (average epoch stats below)
2022-03-07 19:21:52 | INFO | train | epoch 178 | loss 2.978 | nll_loss 2.019 | ppl 4.05 | wps 20461.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 34646 | lr 0.000169892 | gnorm 1.163 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 110394
2022-03-07 19:21:52 | INFO | fairseq.trainer | begin training epoch 179
2022-03-07 19:21:52 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:24:40 | INFO | train_inner | epoch 179:     54 / 196 loss=2.978, nll_loss=2.019, ppl=4.05, wps=20206.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=34700, lr=0.00016976, gnorm=1.171, loss_scale=16, train_wall=292, gb_free=19.9, wall=110563
2022-03-07 19:28:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:29:56 | INFO | train_inner | epoch 179:    155 / 196 loss=2.983, nll_loss=2.024, ppl=4.07, wps=20765.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=34800, lr=0.000169516, gnorm=1.157, loss_scale=16, train_wall=293, gb_free=19.9, wall=110879
2022-03-07 19:32:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:32:09 | INFO | valid | epoch 179 | valid on 'valid' subset | loss 11.025 | nll_loss 10.418 | ppl 1368.35 | wps 40696.7 | wpb 510.9 | bsz 1 | num_updates 34841 | best_loss 7.384
2022-03-07 19:32:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 179 @ 34841 updates
2022-03-07 19:32:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 19:32:12 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 19:32:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 179 @ 34841 updates, score 11.025) (writing took 3.3702001301571727 seconds)
2022-03-07 19:32:12 | INFO | fairseq_cli.train | end of epoch 179 (average epoch stats below)
2022-03-07 19:32:12 | INFO | train | epoch 179 | loss 2.976 | nll_loss 2.017 | ppl 4.05 | wps 20572.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 34841 | lr 0.000169416 | gnorm 1.165 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 111015
2022-03-07 19:32:12 | INFO | fairseq.trainer | begin training epoch 180
2022-03-07 19:32:12 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:35:16 | INFO | train_inner | epoch 180:     59 / 196 loss=2.965, nll_loss=2.005, ppl=4.01, wps=20398, ups=0.31, wpb=65367, bsz=127.7, num_updates=34900, lr=0.000169273, gnorm=1.175, loss_scale=16, train_wall=289, gb_free=19.9, wall=111199
2022-03-07 19:35:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:40:32 | INFO | train_inner | epoch 180:    160 / 196 loss=2.979, nll_loss=2.02, ppl=4.06, wps=20774.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=35000, lr=0.000169031, gnorm=1.169, loss_scale=16, train_wall=293, gb_free=19.9, wall=111515
2022-03-07 19:42:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:42:29 | INFO | valid | epoch 180 | valid on 'valid' subset | loss 11.048 | nll_loss 10.443 | ppl 1391.92 | wps 40291.9 | wpb 510.9 | bsz 1 | num_updates 35036 | best_loss 7.384
2022-03-07 19:42:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 180 @ 35036 updates
2022-03-07 19:42:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 19:42:32 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 19:42:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 180 @ 35036 updates, score 11.048) (writing took 3.392561956308782 seconds)
2022-03-07 19:42:32 | INFO | fairseq_cli.train | end of epoch 180 (average epoch stats below)
2022-03-07 19:42:32 | INFO | train | epoch 180 | loss 2.972 | nll_loss 2.013 | ppl 4.04 | wps 20575.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 35036 | lr 0.000168944 | gnorm 1.167 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 111635
2022-03-07 19:42:32 | INFO | fairseq.trainer | begin training epoch 181
2022-03-07 19:42:32 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:42:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:45:55 | INFO | train_inner | epoch 181:     65 / 196 loss=2.966, nll_loss=2.006, ppl=4.02, wps=20203.4, ups=0.31, wpb=65367, bsz=127.7, num_updates=35100, lr=0.00016879, gnorm=1.166, loss_scale=16, train_wall=292, gb_free=19.9, wall=111838
2022-03-07 19:49:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:51:11 | INFO | train_inner | epoch 181:    166 / 196 loss=2.976, nll_loss=2.016, ppl=4.05, wps=20769.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=35200, lr=0.00016855, gnorm=1.167, loss_scale=16, train_wall=293, gb_free=19.9, wall=112154
2022-03-07 19:52:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 19:52:49 | INFO | valid | epoch 181 | valid on 'valid' subset | loss 11.061 | nll_loss 10.46 | ppl 1408.77 | wps 40882.1 | wpb 510.9 | bsz 1 | num_updates 35230 | best_loss 7.384
2022-03-07 19:52:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 181 @ 35230 updates
2022-03-07 19:52:49 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 19:52:52 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 19:52:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 181 @ 35230 updates, score 11.061) (writing took 3.420522244181484 seconds)
2022-03-07 19:52:53 | INFO | fairseq_cli.train | end of epoch 181 (average epoch stats below)
2022-03-07 19:52:53 | INFO | train | epoch 181 | loss 2.968 | nll_loss 2.008 | ppl 4.02 | wps 20467.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 35230 | lr 0.000168478 | gnorm 1.164 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 112255
2022-03-07 19:52:53 | INFO | fairseq.trainer | begin training epoch 182
2022-03-07 19:52:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 19:56:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 19:56:34 | INFO | train_inner | epoch 182:     71 / 196 loss=2.951, nll_loss=1.99, ppl=3.97, wps=20200.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=35300, lr=0.000168311, gnorm=1.156, loss_scale=16, train_wall=292, gb_free=19.9, wall=112477
2022-03-07 20:01:47 | INFO | train_inner | epoch 182:    171 / 196 loss=2.981, nll_loss=2.022, ppl=4.06, wps=20992.5, ups=0.32, wpb=65536, bsz=128, num_updates=35400, lr=0.000168073, gnorm=1.166, loss_scale=16, train_wall=290, gb_free=19.9, wall=112789
2022-03-07 20:03:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:03:09 | INFO | valid | epoch 182 | valid on 'valid' subset | loss 11.06 | nll_loss 10.459 | ppl 1407.1 | wps 40526.3 | wpb 510.9 | bsz 1 | num_updates 35425 | best_loss 7.384
2022-03-07 20:03:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 182 @ 35425 updates
2022-03-07 20:03:09 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 20:03:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 20:03:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 182 @ 35425 updates, score 11.06) (writing took 3.409725442994386 seconds)
2022-03-07 20:03:13 | INFO | fairseq_cli.train | end of epoch 182 (average epoch stats below)
2022-03-07 20:03:13 | INFO | train | epoch 182 | loss 2.965 | nll_loss 2.005 | ppl 4.01 | wps 20580.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 35425 | lr 0.000168014 | gnorm 1.163 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 112876
2022-03-07 20:03:13 | INFO | fairseq.trainer | begin training epoch 183
2022-03-07 20:03:13 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:04:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:07:10 | INFO | train_inner | epoch 183:     76 / 196 loss=2.949, nll_loss=1.988, ppl=3.97, wps=20207.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=35500, lr=0.000167836, gnorm=1.15, loss_scale=16, train_wall=292, gb_free=19.9, wall=113113
2022-03-07 20:11:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:12:26 | INFO | train_inner | epoch 183:    177 / 196 loss=2.984, nll_loss=2.026, ppl=4.07, wps=20773.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=35600, lr=0.0001676, gnorm=1.183, loss_scale=16, train_wall=293, gb_free=19.9, wall=113428
2022-03-07 20:13:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:13:29 | INFO | valid | epoch 183 | valid on 'valid' subset | loss 11.105 | nll_loss 10.5 | ppl 1448.14 | wps 40752 | wpb 510.9 | bsz 1 | num_updates 35619 | best_loss 7.384
2022-03-07 20:13:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 183 @ 35619 updates
2022-03-07 20:13:29 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 20:13:33 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 20:13:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 183 @ 35619 updates, score 11.105) (writing took 3.356717860326171 seconds)
2022-03-07 20:13:33 | INFO | fairseq_cli.train | end of epoch 183 (average epoch stats below)
2022-03-07 20:13:33 | INFO | train | epoch 183 | loss 2.962 | nll_loss 2.002 | ppl 4.01 | wps 20474.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 35619 | lr 0.000167556 | gnorm 1.164 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 113496
2022-03-07 20:13:33 | INFO | fairseq.trainer | begin training epoch 184
2022-03-07 20:13:33 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:17:46 | INFO | train_inner | epoch 184:     81 / 196 loss=2.937, nll_loss=1.974, ppl=3.93, wps=20400.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=35700, lr=0.000167365, gnorm=1.156, loss_scale=16, train_wall=289, gb_free=19.9, wall=113749
2022-03-07 20:18:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:23:01 | INFO | train_inner | epoch 184:    182 / 196 loss=2.984, nll_loss=2.026, ppl=4.07, wps=20786.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=35800, lr=0.000167132, gnorm=1.176, loss_scale=16, train_wall=293, gb_free=19.9, wall=114064
2022-03-07 20:23:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:23:50 | INFO | valid | epoch 184 | valid on 'valid' subset | loss 11.054 | nll_loss 10.448 | ppl 1397.22 | wps 40591.8 | wpb 510.9 | bsz 1 | num_updates 35814 | best_loss 7.384
2022-03-07 20:23:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 184 @ 35814 updates
2022-03-07 20:23:50 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 20:23:53 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 20:23:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 184 @ 35814 updates, score 11.054) (writing took 3.4120845729485154 seconds)
2022-03-07 20:23:53 | INFO | fairseq_cli.train | end of epoch 184 (average epoch stats below)
2022-03-07 20:23:53 | INFO | train | epoch 184 | loss 2.959 | nll_loss 1.999 | ppl 4 | wps 20578 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 35814 | lr 0.000167099 | gnorm 1.166 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 114116
2022-03-07 20:23:53 | INFO | fairseq.trainer | begin training epoch 185
2022-03-07 20:23:53 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:25:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:28:25 | INFO | train_inner | epoch 185:     87 / 196 loss=2.93, nll_loss=1.967, ppl=3.91, wps=20188.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=35900, lr=0.000166899, gnorm=1.157, loss_scale=16, train_wall=292, gb_free=19.9, wall=114388
2022-03-07 20:32:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:33:41 | INFO | train_inner | epoch 185:    188 / 196 loss=2.983, nll_loss=2.024, ppl=4.07, wps=20766.3, ups=0.32, wpb=65536, bsz=128, num_updates=36000, lr=0.000166667, gnorm=1.175, loss_scale=16, train_wall=293, gb_free=19.9, wall=114703
2022-03-07 20:34:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:34:10 | INFO | valid | epoch 185 | valid on 'valid' subset | loss 11.111 | nll_loss 10.509 | ppl 1457.35 | wps 40755.6 | wpb 510.9 | bsz 1 | num_updates 36008 | best_loss 7.384
2022-03-07 20:34:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 185 @ 36008 updates
2022-03-07 20:34:10 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 20:34:13 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 20:34:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 185 @ 36008 updates, score 11.111) (writing took 3.40556563064456 seconds)
2022-03-07 20:34:14 | INFO | fairseq_cli.train | end of epoch 185 (average epoch stats below)
2022-03-07 20:34:14 | INFO | train | epoch 185 | loss 2.956 | nll_loss 1.995 | ppl 3.99 | wps 20458.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 36008 | lr 0.000166648 | gnorm 1.167 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 114736
2022-03-07 20:34:14 | INFO | fairseq.trainer | begin training epoch 186
2022-03-07 20:34:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:39:01 | INFO | train_inner | epoch 186:     92 / 196 loss=2.927, nll_loss=1.964, ppl=3.9, wps=20393.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=36100, lr=0.000166436, gnorm=1.154, loss_scale=16, train_wall=289, gb_free=19.9, wall=115024
2022-03-07 20:39:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:44:16 | INFO | train_inner | epoch 186:    193 / 196 loss=2.985, nll_loss=2.027, ppl=4.07, wps=20779.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=36200, lr=0.000166206, gnorm=1.166, loss_scale=16, train_wall=293, gb_free=19.9, wall=115339
2022-03-07 20:44:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:44:31 | INFO | valid | epoch 186 | valid on 'valid' subset | loss 11.094 | nll_loss 10.488 | ppl 1435.8 | wps 40212.4 | wpb 510.9 | bsz 1 | num_updates 36203 | best_loss 7.384
2022-03-07 20:44:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 186 @ 36203 updates
2022-03-07 20:44:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 20:44:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 20:44:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 186 @ 36203 updates, score 11.094) (writing took 3.440997395198792 seconds)
2022-03-07 20:44:34 | INFO | fairseq_cli.train | end of epoch 186 (average epoch stats below)
2022-03-07 20:44:34 | INFO | train | epoch 186 | loss 2.953 | nll_loss 1.992 | ppl 3.98 | wps 20573 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 36203 | lr 0.000166199 | gnorm 1.159 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 115357
2022-03-07 20:44:34 | INFO | fairseq.trainer | begin training epoch 187
2022-03-07 20:44:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:46:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:49:40 | INFO | train_inner | epoch 187:     98 / 196 loss=2.922, nll_loss=1.958, ppl=3.88, wps=20190.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=36300, lr=0.000165977, gnorm=1.158, loss_scale=16, train_wall=292, gb_free=19.9, wall=115663
2022-03-07 20:52:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 20:54:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2022-03-07 20:54:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 20:54:51 | INFO | valid | epoch 187 | valid on 'valid' subset | loss 11.11 | nll_loss 10.506 | ppl 1454.55 | wps 40677.2 | wpb 510.9 | bsz 1 | num_updates 36396 | best_loss 7.384
2022-03-07 20:54:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 187 @ 36396 updates
2022-03-07 20:54:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 20:54:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 20:54:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 187 @ 36396 updates, score 11.11) (writing took 3.4632228491827846 seconds)
2022-03-07 20:54:54 | INFO | fairseq_cli.train | end of epoch 187 (average epoch stats below)
2022-03-07 20:54:54 | INFO | train | epoch 187 | loss 2.95 | nll_loss 1.989 | ppl 3.97 | wps 20362.8 | ups 0.31 | wpb 65446.6 | bsz 127.8 | num_updates 36396 | lr 0.000165757 | gnorm 1.172 | loss_scale 8 | train_wall 568 | gb_free 19.9 | wall 115977
2022-03-07 20:54:54 | INFO | fairseq.trainer | begin training epoch 188
2022-03-07 20:54:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 20:55:07 | INFO | train_inner | epoch 188:      4 / 196 loss=2.976, nll_loss=2.017, ppl=4.05, wps=20010.9, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=36400, lr=0.000165748, gnorm=1.186, loss_scale=8, train_wall=295, gb_free=19.9, wall=115990
2022-03-07 21:00:19 | INFO | train_inner | epoch 188:    104 / 196 loss=2.917, nll_loss=1.953, ppl=3.87, wps=20985.6, ups=0.32, wpb=65536, bsz=128, num_updates=36500, lr=0.000165521, gnorm=1.154, loss_scale=8, train_wall=290, gb_free=19.9, wall=116302
2022-03-07 21:05:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:05:11 | INFO | valid | epoch 188 | valid on 'valid' subset | loss 11.101 | nll_loss 10.498 | ppl 1445.89 | wps 40723.2 | wpb 510.9 | bsz 1 | num_updates 36592 | best_loss 7.384
2022-03-07 21:05:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 188 @ 36592 updates
2022-03-07 21:05:11 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 21:05:14 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 21:05:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 188 @ 36592 updates, score 11.101) (writing took 3.394886906724423 seconds)
2022-03-07 21:05:14 | INFO | fairseq_cli.train | end of epoch 188 (average epoch stats below)
2022-03-07 21:05:14 | INFO | train | epoch 188 | loss 2.948 | nll_loss 1.986 | ppl 3.96 | wps 20685.7 | ups 0.32 | wpb 65448 | bsz 127.8 | num_updates 36592 | lr 0.000165313 | gnorm 1.167 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 116597
2022-03-07 21:05:14 | INFO | fairseq.trainer | begin training epoch 189
2022-03-07 21:05:14 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:05:39 | INFO | train_inner | epoch 189:      8 / 196 loss=2.977, nll_loss=2.018, ppl=4.05, wps=20405.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=36600, lr=0.000165295, gnorm=1.18, loss_scale=16, train_wall=289, gb_free=19.9, wall=116622
2022-03-07 21:07:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:10:55 | INFO | train_inner | epoch 189:    109 / 196 loss=2.917, nll_loss=1.953, ppl=3.87, wps=20769, ups=0.32, wpb=65536, bsz=128, num_updates=36700, lr=0.00016507, gnorm=1.171, loss_scale=16, train_wall=293, gb_free=19.9, wall=116938
2022-03-07 21:14:40 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:15:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:15:31 | INFO | valid | epoch 189 | valid on 'valid' subset | loss 11.061 | nll_loss 10.456 | ppl 1404.78 | wps 40899 | wpb 510.9 | bsz 1 | num_updates 36786 | best_loss 7.384
2022-03-07 21:15:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 189 @ 36786 updates
2022-03-07 21:15:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 21:15:35 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 21:15:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 189 @ 36786 updates, score 11.061) (writing took 3.348270350135863 seconds)
2022-03-07 21:15:35 | INFO | fairseq_cli.train | end of epoch 189 (average epoch stats below)
2022-03-07 21:15:35 | INFO | train | epoch 189 | loss 2.945 | nll_loss 1.983 | ppl 3.95 | wps 20466 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 36786 | lr 0.000164876 | gnorm 1.181 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 117218
2022-03-07 21:15:35 | INFO | fairseq.trainer | begin training epoch 190
2022-03-07 21:15:35 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:16:19 | INFO | train_inner | epoch 190:     14 / 196 loss=2.965, nll_loss=2.006, ppl=4.02, wps=20199.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=36800, lr=0.000164845, gnorm=1.184, loss_scale=16, train_wall=292, gb_free=19.9, wall=117261
2022-03-07 21:21:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:21:34 | INFO | train_inner | epoch 190:    115 / 196 loss=2.918, nll_loss=1.954, ppl=3.88, wps=20766.6, ups=0.32, wpb=65536, bsz=128, num_updates=36900, lr=0.000164622, gnorm=1.156, loss_scale=16, train_wall=293, gb_free=19.9, wall=117577
2022-03-07 21:25:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:25:52 | INFO | valid | epoch 190 | valid on 'valid' subset | loss 11.107 | nll_loss 10.507 | ppl 1455.46 | wps 40560.2 | wpb 510.9 | bsz 1 | num_updates 36981 | best_loss 7.384
2022-03-07 21:25:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 190 @ 36981 updates
2022-03-07 21:25:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 21:25:55 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 21:25:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 190 @ 36981 updates, score 11.107) (writing took 3.4023673068732023 seconds)
2022-03-07 21:25:55 | INFO | fairseq_cli.train | end of epoch 190 (average epoch stats below)
2022-03-07 21:25:55 | INFO | train | epoch 190 | loss 2.943 | nll_loss 1.981 | ppl 3.95 | wps 20571 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 36981 | lr 0.000164441 | gnorm 1.161 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 117838
2022-03-07 21:25:55 | INFO | fairseq.trainer | begin training epoch 191
2022-03-07 21:25:55 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:26:55 | INFO | train_inner | epoch 191:     19 / 196 loss=2.965, nll_loss=2.005, ppl=4.01, wps=20402.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=37000, lr=0.000164399, gnorm=1.17, loss_scale=16, train_wall=289, gb_free=19.9, wall=117897
2022-03-07 21:28:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:32:10 | INFO | train_inner | epoch 191:    120 / 196 loss=2.919, nll_loss=1.956, ppl=3.88, wps=20771.7, ups=0.32, wpb=65536, bsz=128, num_updates=37100, lr=0.000164177, gnorm=1.163, loss_scale=16, train_wall=293, gb_free=19.9, wall=118213
2022-03-07 21:35:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:36:07 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:36:12 | INFO | valid | epoch 191 | valid on 'valid' subset | loss 11.13 | nll_loss 10.529 | ppl 1477.41 | wps 40618.1 | wpb 510.9 | bsz 1 | num_updates 37175 | best_loss 7.384
2022-03-07 21:36:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 191 @ 37175 updates
2022-03-07 21:36:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 21:36:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 21:36:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 191 @ 37175 updates, score 11.13) (writing took 3.6756129218265414 seconds)
2022-03-07 21:36:16 | INFO | fairseq_cli.train | end of epoch 191 (average epoch stats below)
2022-03-07 21:36:16 | INFO | train | epoch 191 | loss 2.938 | nll_loss 1.976 | ppl 3.93 | wps 20457.5 | ups 0.31 | wpb 65448.9 | bsz 127.8 | num_updates 37175 | lr 0.000164012 | gnorm 1.168 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 118459
2022-03-07 21:36:16 | INFO | fairseq.trainer | begin training epoch 192
2022-03-07 21:36:16 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:37:34 | INFO | train_inner | epoch 192:     25 / 196 loss=2.957, nll_loss=1.997, ppl=3.99, wps=20176.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=37200, lr=0.000163956, gnorm=1.172, loss_scale=16, train_wall=292, gb_free=19.9, wall=118537
2022-03-07 21:42:46 | INFO | train_inner | epoch 192:    125 / 196 loss=2.922, nll_loss=1.958, ppl=3.89, wps=20978.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=37300, lr=0.000163737, gnorm=1.173, loss_scale=32, train_wall=290, gb_free=19.9, wall=118849
2022-03-07 21:42:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:46:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:46:33 | INFO | valid | epoch 192 | valid on 'valid' subset | loss 11.128 | nll_loss 10.527 | ppl 1475.7 | wps 40869.5 | wpb 510.9 | bsz 1 | num_updates 37370 | best_loss 7.384
2022-03-07 21:46:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 192 @ 37370 updates
2022-03-07 21:46:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 21:46:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 21:46:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 192 @ 37370 updates, score 11.128) (writing took 3.37768415780738 seconds)
2022-03-07 21:46:36 | INFO | fairseq_cli.train | end of epoch 192 (average epoch stats below)
2022-03-07 21:46:36 | INFO | train | epoch 192 | loss 2.937 | nll_loss 1.975 | ppl 3.93 | wps 20574 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 37370 | lr 0.000163583 | gnorm 1.168 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 119079
2022-03-07 21:46:36 | INFO | fairseq.trainer | begin training epoch 193
2022-03-07 21:46:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:48:10 | INFO | train_inner | epoch 193:     30 / 196 loss=2.949, nll_loss=1.988, ppl=3.97, wps=20205.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=37400, lr=0.000163517, gnorm=1.163, loss_scale=16, train_wall=292, gb_free=19.9, wall=119173
2022-03-07 21:49:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:53:25 | INFO | train_inner | epoch 193:    131 / 196 loss=2.92, nll_loss=1.957, ppl=3.88, wps=20766.8, ups=0.32, wpb=65536, bsz=128, num_updates=37500, lr=0.000163299, gnorm=1.175, loss_scale=16, train_wall=293, gb_free=19.9, wall=119488
2022-03-07 21:56:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 21:56:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 21:56:53 | INFO | valid | epoch 193 | valid on 'valid' subset | loss 11.136 | nll_loss 10.534 | ppl 1482.66 | wps 40564.2 | wpb 510.9 | bsz 1 | num_updates 37564 | best_loss 7.384
2022-03-07 21:56:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 193 @ 37564 updates
2022-03-07 21:56:53 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 21:56:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 21:56:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 193 @ 37564 updates, score 11.136) (writing took 3.340810873080045 seconds)
2022-03-07 21:56:57 | INFO | fairseq_cli.train | end of epoch 193 (average epoch stats below)
2022-03-07 21:56:57 | INFO | train | epoch 193 | loss 2.934 | nll_loss 1.972 | ppl 3.92 | wps 20465.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 37564 | lr 0.00016316 | gnorm 1.18 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 119699
2022-03-07 21:56:57 | INFO | fairseq.trainer | begin training epoch 194
2022-03-07 21:56:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 21:58:49 | INFO | train_inner | epoch 194:     36 / 196 loss=2.946, nll_loss=1.985, ppl=3.96, wps=20202.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=37600, lr=0.000163082, gnorm=1.184, loss_scale=16, train_wall=292, gb_free=19.9, wall=119812
2022-03-07 22:03:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:04:05 | INFO | train_inner | epoch 194:    137 / 196 loss=2.923, nll_loss=1.959, ppl=3.89, wps=20769, ups=0.32, wpb=65532.4, bsz=128, num_updates=37700, lr=0.000162866, gnorm=1.165, loss_scale=16, train_wall=293, gb_free=19.9, wall=120127
2022-03-07 22:07:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:07:13 | INFO | valid | epoch 194 | valid on 'valid' subset | loss 11.134 | nll_loss 10.534 | ppl 1482.33 | wps 40887.4 | wpb 510.9 | bsz 1 | num_updates 37759 | best_loss 7.384
2022-03-07 22:07:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 194 @ 37759 updates
2022-03-07 22:07:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 22:07:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 22:07:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 194 @ 37759 updates, score 11.134) (writing took 4.860768793150783 seconds)
2022-03-07 22:07:18 | INFO | fairseq_cli.train | end of epoch 194 (average epoch stats below)
2022-03-07 22:07:18 | INFO | train | epoch 194 | loss 2.931 | nll_loss 1.968 | ppl 3.91 | wps 20528.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 37759 | lr 0.000162738 | gnorm 1.17 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 120321
2022-03-07 22:07:18 | INFO | fairseq.trainer | begin training epoch 195
2022-03-07 22:07:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:09:26 | INFO | train_inner | epoch 195:     41 / 196 loss=2.93, nll_loss=1.967, ppl=3.91, wps=20308.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=37800, lr=0.00016265, gnorm=1.181, loss_scale=16, train_wall=289, gb_free=19.9, wall=120449
2022-03-07 22:10:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:14:42 | INFO | train_inner | epoch 195:    142 / 196 loss=2.924, nll_loss=1.961, ppl=3.89, wps=20767.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=37900, lr=0.000162435, gnorm=1.163, loss_scale=16, train_wall=293, gb_free=19.9, wall=120765
2022-03-07 22:17:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:17:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:17:35 | INFO | valid | epoch 195 | valid on 'valid' subset | loss 11.163 | nll_loss 10.561 | ppl 1510.8 | wps 40922.7 | wpb 510.9 | bsz 1 | num_updates 37953 | best_loss 7.384
2022-03-07 22:17:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 195 @ 37953 updates
2022-03-07 22:17:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 22:17:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 22:17:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 195 @ 37953 updates, score 11.163) (writing took 3.3696065968833864 seconds)
2022-03-07 22:17:39 | INFO | fairseq_cli.train | end of epoch 195 (average epoch stats below)
2022-03-07 22:17:39 | INFO | train | epoch 195 | loss 2.928 | nll_loss 1.965 | ppl 3.9 | wps 20462.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 37953 | lr 0.000162322 | gnorm 1.175 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 120942
2022-03-07 22:17:39 | INFO | fairseq.trainer | begin training epoch 196
2022-03-07 22:17:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:20:06 | INFO | train_inner | epoch 196:     47 / 196 loss=2.93, nll_loss=1.967, ppl=3.91, wps=20204.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=38000, lr=0.000162221, gnorm=1.192, loss_scale=16, train_wall=292, gb_free=19.9, wall=121088
2022-03-07 22:24:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:25:21 | INFO | train_inner | epoch 196:    148 / 196 loss=2.925, nll_loss=1.962, ppl=3.9, wps=20785.6, ups=0.32, wpb=65532.4, bsz=128, num_updates=38100, lr=0.000162008, gnorm=1.173, loss_scale=16, train_wall=293, gb_free=19.9, wall=121404
2022-03-07 22:27:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:27:55 | INFO | valid | epoch 196 | valid on 'valid' subset | loss 11.099 | nll_loss 10.498 | ppl 1445.7 | wps 40625 | wpb 510.9 | bsz 1 | num_updates 38148 | best_loss 7.384
2022-03-07 22:27:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 196 @ 38148 updates
2022-03-07 22:27:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 22:27:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 22:27:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 196 @ 38148 updates, score 11.099) (writing took 3.322372400201857 seconds)
2022-03-07 22:27:59 | INFO | fairseq_cli.train | end of epoch 196 (average epoch stats below)
2022-03-07 22:27:59 | INFO | train | epoch 196 | loss 2.926 | nll_loss 1.962 | ppl 3.9 | wps 20585.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 38148 | lr 0.000161906 | gnorm 1.179 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 121561
2022-03-07 22:27:59 | INFO | fairseq.trainer | begin training epoch 197
2022-03-07 22:27:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:30:41 | INFO | train_inner | epoch 197:     52 / 196 loss=2.921, nll_loss=1.958, ppl=3.88, wps=20411.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=38200, lr=0.000161796, gnorm=1.178, loss_scale=16, train_wall=289, gb_free=19.9, wall=121724
2022-03-07 22:31:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:35:57 | INFO | train_inner | epoch 197:    153 / 196 loss=2.929, nll_loss=1.967, ppl=3.91, wps=20767.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=38300, lr=0.000161585, gnorm=1.179, loss_scale=16, train_wall=293, gb_free=19.9, wall=122039
2022-03-07 22:38:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:38:15 | INFO | valid | epoch 197 | valid on 'valid' subset | loss 11.155 | nll_loss 10.556 | ppl 1505.18 | wps 40672.4 | wpb 510.9 | bsz 1 | num_updates 38343 | best_loss 7.384
2022-03-07 22:38:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 197 @ 38343 updates
2022-03-07 22:38:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 22:38:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 22:38:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 197 @ 38343 updates, score 11.155) (writing took 3.3459790321066976 seconds)
2022-03-07 22:38:19 | INFO | fairseq_cli.train | end of epoch 197 (average epoch stats below)
2022-03-07 22:38:19 | INFO | train | epoch 197 | loss 2.922 | nll_loss 1.959 | ppl 3.89 | wps 20579.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 38343 | lr 0.000161494 | gnorm 1.181 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 122182
2022-03-07 22:38:19 | INFO | fairseq.trainer | begin training epoch 198
2022-03-07 22:38:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:38:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:41:20 | INFO | train_inner | epoch 198:     58 / 196 loss=2.916, nll_loss=1.952, ppl=3.87, wps=20213.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=38400, lr=0.000161374, gnorm=1.195, loss_scale=16, train_wall=292, gb_free=19.9, wall=122363
2022-03-07 22:45:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:46:36 | INFO | train_inner | epoch 198:    159 / 196 loss=2.925, nll_loss=1.962, ppl=3.9, wps=20727.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=38500, lr=0.000161165, gnorm=1.172, loss_scale=16, train_wall=293, gb_free=19.9, wall=122679
2022-03-07 22:48:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:48:37 | INFO | valid | epoch 198 | valid on 'valid' subset | loss 11.125 | nll_loss 10.526 | ppl 1474.63 | wps 40199.4 | wpb 510.9 | bsz 1 | num_updates 38537 | best_loss 7.384
2022-03-07 22:48:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 198 @ 38537 updates
2022-03-07 22:48:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 22:48:41 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 22:48:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 198 @ 38537 updates, score 11.125) (writing took 3.4532727622427046 seconds)
2022-03-07 22:48:41 | INFO | fairseq_cli.train | end of epoch 198 (average epoch stats below)
2022-03-07 22:48:41 | INFO | train | epoch 198 | loss 2.919 | nll_loss 1.955 | ppl 3.88 | wps 20410.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 38537 | lr 0.000161087 | gnorm 1.182 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 122804
2022-03-07 22:48:41 | INFO | fairseq.trainer | begin training epoch 199
2022-03-07 22:48:41 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:51:58 | INFO | train_inner | epoch 199:     63 / 196 loss=2.907, nll_loss=1.942, ppl=3.84, wps=20332.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=38600, lr=0.000160956, gnorm=1.17, loss_scale=16, train_wall=290, gb_free=19.9, wall=123000
2022-03-07 22:52:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 22:57:13 | INFO | train_inner | epoch 199:    164 / 196 loss=2.929, nll_loss=1.966, ppl=3.91, wps=20778, ups=0.32, wpb=65536, bsz=128, num_updates=38700, lr=0.000160748, gnorm=1.185, loss_scale=16, train_wall=293, gb_free=19.9, wall=123316
2022-03-07 22:58:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 22:58:58 | INFO | valid | epoch 199 | valid on 'valid' subset | loss 11.153 | nll_loss 10.555 | ppl 1504.68 | wps 40188.2 | wpb 510.9 | bsz 1 | num_updates 38732 | best_loss 7.384
2022-03-07 22:58:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 199 @ 38732 updates
2022-03-07 22:58:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 22:59:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 22:59:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 199 @ 38732 updates, score 11.153) (writing took 3.419293365906924 seconds)
2022-03-07 22:59:01 | INFO | fairseq_cli.train | end of epoch 199 (average epoch stats below)
2022-03-07 22:59:01 | INFO | train | epoch 199 | loss 2.917 | nll_loss 1.953 | ppl 3.87 | wps 20578.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 38732 | lr 0.000160681 | gnorm 1.175 | loss_scale 32 | train_wall 568 | gb_free 19.9 | wall 123424
2022-03-07 22:59:01 | INFO | fairseq.trainer | begin training epoch 200
2022-03-07 22:59:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 22:59:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:02:36 | INFO | train_inner | epoch 200:     69 / 196 loss=2.905, nll_loss=1.941, ppl=3.84, wps=20216.5, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=38800, lr=0.00016054, gnorm=1.174, loss_scale=16, train_wall=292, gb_free=19.9, wall=123639
2022-03-07 23:05:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:07:51 | INFO | train_inner | epoch 200:    170 / 196 loss=2.929, nll_loss=1.967, ppl=3.91, wps=20796.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=38900, lr=0.000160334, gnorm=1.189, loss_scale=16, train_wall=293, gb_free=19.9, wall=123954
2022-03-07 23:09:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:09:17 | INFO | valid | epoch 200 | valid on 'valid' subset | loss 11.136 | nll_loss 10.539 | ppl 1488 | wps 40574.8 | wpb 510.9 | bsz 1 | num_updates 38926 | best_loss 7.384
2022-03-07 23:09:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 200 @ 38926 updates
2022-03-07 23:09:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 23:09:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 23:09:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 200 @ 38926 updates, score 11.136) (writing took 3.4116564029827714 seconds)
2022-03-07 23:09:21 | INFO | fairseq_cli.train | end of epoch 200 (average epoch stats below)
2022-03-07 23:09:21 | INFO | train | epoch 200 | loss 2.914 | nll_loss 1.951 | ppl 3.87 | wps 20490.3 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 38926 | lr 0.00016028 | gnorm 1.183 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 124044
2022-03-07 23:09:21 | INFO | fairseq.trainer | begin training epoch 201
2022-03-07 23:09:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:12:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:13:15 | INFO | train_inner | epoch 201:     75 / 196 loss=2.901, nll_loss=1.935, ppl=3.82, wps=20220.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=39000, lr=0.000160128, gnorm=1.17, loss_scale=16, train_wall=292, gb_free=19.9, wall=124278
2022-03-07 23:18:27 | INFO | train_inner | epoch 201:    175 / 196 loss=2.923, nll_loss=1.96, ppl=3.89, wps=21002, ups=0.32, wpb=65532.4, bsz=128, num_updates=39100, lr=0.000159923, gnorm=1.2, loss_scale=16, train_wall=290, gb_free=19.9, wall=124590
2022-03-07 23:19:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:19:37 | INFO | valid | epoch 201 | valid on 'valid' subset | loss 11.136 | nll_loss 10.539 | ppl 1487.93 | wps 40627.4 | wpb 510.9 | bsz 1 | num_updates 39121 | best_loss 7.384
2022-03-07 23:19:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 201 @ 39121 updates
2022-03-07 23:19:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 23:19:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 23:19:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 201 @ 39121 updates, score 11.136) (writing took 3.48178009968251 seconds)
2022-03-07 23:19:40 | INFO | fairseq_cli.train | end of epoch 201 (average epoch stats below)
2022-03-07 23:19:40 | INFO | train | epoch 201 | loss 2.912 | nll_loss 1.947 | ppl 3.86 | wps 20593.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 39121 | lr 0.00015988 | gnorm 1.183 | loss_scale 32 | train_wall 567 | gb_free 19.9 | wall 124663
2022-03-07 23:19:40 | INFO | fairseq.trainer | begin training epoch 202
2022-03-07 23:19:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:19:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:23:50 | INFO | train_inner | epoch 202:     80 / 196 loss=2.891, nll_loss=1.925, ppl=3.8, wps=20218.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=39200, lr=0.000159719, gnorm=1.178, loss_scale=16, train_wall=292, gb_free=19.9, wall=124913
2022-03-07 23:26:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:29:05 | INFO | train_inner | epoch 202:    181 / 196 loss=2.932, nll_loss=1.97, ppl=3.92, wps=20798.8, ups=0.32, wpb=65536, bsz=128, num_updates=39300, lr=0.000159516, gnorm=1.173, loss_scale=16, train_wall=293, gb_free=19.9, wall=125228
2022-03-07 23:29:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:29:57 | INFO | valid | epoch 202 | valid on 'valid' subset | loss 11.151 | nll_loss 10.548 | ppl 1497.29 | wps 40604.6 | wpb 510.9 | bsz 1 | num_updates 39315 | best_loss 7.384
2022-03-07 23:29:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 202 @ 39315 updates
2022-03-07 23:29:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 23:30:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 23:30:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 202 @ 39315 updates, score 11.151) (writing took 3.3916503791697323 seconds)
2022-03-07 23:30:00 | INFO | fairseq_cli.train | end of epoch 202 (average epoch stats below)
2022-03-07 23:30:00 | INFO | train | epoch 202 | loss 2.908 | nll_loss 1.944 | ppl 3.85 | wps 20491.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 39315 | lr 0.000159485 | gnorm 1.177 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 125283
2022-03-07 23:30:00 | INFO | fairseq.trainer | begin training epoch 203
2022-03-07 23:30:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:34:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:34:28 | INFO | train_inner | epoch 203:     86 / 196 loss=2.886, nll_loss=1.919, ppl=3.78, wps=20219.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=39400, lr=0.000159313, gnorm=1.181, loss_scale=16, train_wall=292, gb_free=19.9, wall=125551
2022-03-07 23:39:41 | INFO | train_inner | epoch 203:    186 / 196 loss=2.93, nll_loss=1.968, ppl=3.91, wps=20997.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=39500, lr=0.000159111, gnorm=1.188, loss_scale=16, train_wall=290, gb_free=19.9, wall=125863
2022-03-07 23:40:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:40:16 | INFO | valid | epoch 203 | valid on 'valid' subset | loss 11.168 | nll_loss 10.57 | ppl 1520.61 | wps 40817.7 | wpb 510.9 | bsz 1 | num_updates 39510 | best_loss 7.384
2022-03-07 23:40:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 203 @ 39510 updates
2022-03-07 23:40:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 23:40:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 23:40:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 203 @ 39510 updates, score 11.168) (writing took 3.4453028440475464 seconds)
2022-03-07 23:40:20 | INFO | fairseq_cli.train | end of epoch 203 (average epoch stats below)
2022-03-07 23:40:20 | INFO | train | epoch 203 | loss 2.907 | nll_loss 1.943 | ppl 3.84 | wps 20591.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 39510 | lr 0.000159091 | gnorm 1.182 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 125903
2022-03-07 23:40:20 | INFO | fairseq.trainer | begin training epoch 204
2022-03-07 23:40:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:41:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:45:04 | INFO | train_inner | epoch 204:     91 / 196 loss=2.879, nll_loss=1.912, ppl=3.76, wps=20218.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=39600, lr=0.00015891, gnorm=1.168, loss_scale=16, train_wall=292, gb_free=19.9, wall=126187
2022-03-07 23:47:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:50:19 | INFO | train_inner | epoch 204:    192 / 196 loss=2.93, nll_loss=1.968, ppl=3.91, wps=20813, ups=0.32, wpb=65536, bsz=128, num_updates=39700, lr=0.00015871, gnorm=1.202, loss_scale=16, train_wall=292, gb_free=19.9, wall=126502
2022-03-07 23:50:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-07 23:50:36 | INFO | valid | epoch 204 | valid on 'valid' subset | loss 11.165 | nll_loss 10.569 | ppl 1518.65 | wps 40835.1 | wpb 510.9 | bsz 1 | num_updates 39704 | best_loss 7.384
2022-03-07 23:50:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 204 @ 39704 updates
2022-03-07 23:50:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 23:50:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-07 23:50:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 204 @ 39704 updates, score 11.165) (writing took 3.443876401055604 seconds)
2022-03-07 23:50:39 | INFO | fairseq_cli.train | end of epoch 204 (average epoch stats below)
2022-03-07 23:50:39 | INFO | train | epoch 204 | loss 2.902 | nll_loss 1.937 | ppl 3.83 | wps 20496.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 39704 | lr 0.000158702 | gnorm 1.185 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 126522
2022-03-07 23:50:39 | INFO | fairseq.trainer | begin training epoch 205
2022-03-07 23:50:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-07 23:54:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-07 23:55:42 | INFO | train_inner | epoch 205:     97 / 196 loss=2.873, nll_loss=1.905, ppl=3.75, wps=20224.1, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=39800, lr=0.000158511, gnorm=1.171, loss_scale=16, train_wall=292, gb_free=19.9, wall=126825
2022-03-08 00:00:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:00:55 | INFO | valid | epoch 205 | valid on 'valid' subset | loss 11.188 | nll_loss 10.592 | ppl 1543.17 | wps 40717.1 | wpb 510.9 | bsz 1 | num_updates 39899 | best_loss 7.384
2022-03-08 00:00:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 205 @ 39899 updates
2022-03-08 00:00:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 00:00:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 00:00:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 205 @ 39899 updates, score 11.188) (writing took 3.386820788960904 seconds)
2022-03-08 00:00:59 | INFO | fairseq_cli.train | end of epoch 205 (average epoch stats below)
2022-03-08 00:00:59 | INFO | train | epoch 205 | loss 2.901 | nll_loss 1.936 | ppl 3.83 | wps 20603.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 39899 | lr 0.000158314 | gnorm 1.177 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 127142
2022-03-08 00:00:59 | INFO | fairseq.trainer | begin training epoch 206
2022-03-08 00:00:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:01:02 | INFO | train_inner | epoch 206:      1 / 196 loss=2.929, nll_loss=1.967, ppl=3.91, wps=20425.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=39900, lr=0.000158312, gnorm=1.184, loss_scale=16, train_wall=289, gb_free=19.9, wall=127145
2022-03-08 00:01:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:06:17 | INFO | train_inner | epoch 206:    102 / 196 loss=2.867, nll_loss=1.899, ppl=3.73, wps=20801.1, ups=0.32, wpb=65536, bsz=128, num_updates=40000, lr=0.000158114, gnorm=1.189, loss_scale=16, train_wall=293, gb_free=19.9, wall=127460
2022-03-08 00:08:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:11:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:11:15 | INFO | valid | epoch 206 | valid on 'valid' subset | loss 11.217 | nll_loss 10.616 | ppl 1569.55 | wps 40671.6 | wpb 510.9 | bsz 1 | num_updates 40093 | best_loss 7.384
2022-03-08 00:11:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 206 @ 40093 updates
2022-03-08 00:11:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 00:11:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 00:11:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 206 @ 40093 updates, score 11.217) (writing took 3.3951546088792384 seconds)
2022-03-08 00:11:18 | INFO | fairseq_cli.train | end of epoch 206 (average epoch stats below)
2022-03-08 00:11:18 | INFO | train | epoch 206 | loss 2.898 | nll_loss 1.933 | ppl 3.82 | wps 20496.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 40093 | lr 0.00015793 | gnorm 1.188 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 127761
2022-03-08 00:11:18 | INFO | fairseq.trainer | begin training epoch 207
2022-03-08 00:11:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:11:40 | INFO | train_inner | epoch 207:      7 / 196 loss=2.926, nll_loss=1.964, ppl=3.9, wps=20228.6, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=40100, lr=0.000157917, gnorm=1.185, loss_scale=16, train_wall=292, gb_free=19.9, wall=127783
2022-03-08 00:15:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:16:55 | INFO | train_inner | epoch 207:    108 / 196 loss=2.868, nll_loss=1.9, ppl=3.73, wps=20783.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=40200, lr=0.00015772, gnorm=1.166, loss_scale=16, train_wall=293, gb_free=19.9, wall=128098
2022-03-08 00:21:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:21:35 | INFO | valid | epoch 207 | valid on 'valid' subset | loss 11.173 | nll_loss 10.573 | ppl 1523.6 | wps 40567.4 | wpb 510.9 | bsz 1 | num_updates 40288 | best_loss 7.384
2022-03-08 00:21:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 207 @ 40288 updates
2022-03-08 00:21:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 00:21:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 00:21:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 207 @ 40288 updates, score 11.173) (writing took 3.4078431581147015 seconds)
2022-03-08 00:21:38 | INFO | fairseq_cli.train | end of epoch 207 (average epoch stats below)
2022-03-08 00:21:38 | INFO | train | epoch 207 | loss 2.897 | nll_loss 1.932 | ppl 3.82 | wps 20590.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 40288 | lr 0.000157548 | gnorm 1.175 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 128381
2022-03-08 00:21:38 | INFO | fairseq.trainer | begin training epoch 208
2022-03-08 00:21:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:22:16 | INFO | train_inner | epoch 208:     12 / 196 loss=2.925, nll_loss=1.962, ppl=3.9, wps=20424.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=40300, lr=0.000157524, gnorm=1.191, loss_scale=16, train_wall=289, gb_free=19.9, wall=128418
2022-03-08 00:23:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:27:31 | INFO | train_inner | epoch 208:    113 / 196 loss=2.87, nll_loss=1.902, ppl=3.74, wps=20792.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=40400, lr=0.000157329, gnorm=1.174, loss_scale=16, train_wall=293, gb_free=19.9, wall=128734
2022-03-08 00:30:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:31:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:31:54 | INFO | valid | epoch 208 | valid on 'valid' subset | loss 11.148 | nll_loss 10.549 | ppl 1497.81 | wps 40848.8 | wpb 510.9 | bsz 1 | num_updates 40482 | best_loss 7.384
2022-03-08 00:31:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 208 @ 40482 updates
2022-03-08 00:31:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 00:32:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 00:32:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 208 @ 40482 updates, score 11.148) (writing took 5.511024903971702 seconds)
2022-03-08 00:32:00 | INFO | fairseq_cli.train | end of epoch 208 (average epoch stats below)
2022-03-08 00:32:00 | INFO | train | epoch 208 | loss 2.894 | nll_loss 1.929 | ppl 3.81 | wps 20424.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 40482 | lr 0.00015717 | gnorm 1.187 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 129002
2022-03-08 00:32:00 | INFO | fairseq.trainer | begin training epoch 209
2022-03-08 00:32:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:32:56 | INFO | train_inner | epoch 209:     18 / 196 loss=2.912, nll_loss=1.949, ppl=3.86, wps=20102.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=40500, lr=0.000157135, gnorm=1.188, loss_scale=16, train_wall=292, gb_free=19.9, wall=129059
2022-03-08 00:37:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:38:11 | INFO | train_inner | epoch 209:    119 / 196 loss=2.869, nll_loss=1.901, ppl=3.74, wps=20794.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=40600, lr=0.000156941, gnorm=1.178, loss_scale=16, train_wall=293, gb_free=19.9, wall=129374
2022-03-08 00:42:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:42:16 | INFO | valid | epoch 209 | valid on 'valid' subset | loss 11.149 | nll_loss 10.546 | ppl 1495.31 | wps 40756.4 | wpb 510.9 | bsz 1 | num_updates 40677 | best_loss 7.384
2022-03-08 00:42:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 209 @ 40677 updates
2022-03-08 00:42:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 00:42:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 00:42:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 209 @ 40677 updates, score 11.149) (writing took 3.4700641809031367 seconds)
2022-03-08 00:42:19 | INFO | fairseq_cli.train | end of epoch 209 (average epoch stats below)
2022-03-08 00:42:19 | INFO | train | epoch 209 | loss 2.892 | nll_loss 1.927 | ppl 3.8 | wps 20596.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 40677 | lr 0.000156793 | gnorm 1.177 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 129622
2022-03-08 00:42:19 | INFO | fairseq.trainer | begin training epoch 210
2022-03-08 00:42:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:43:31 | INFO | train_inner | epoch 210:     23 / 196 loss=2.915, nll_loss=1.951, ppl=3.87, wps=20421.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=40700, lr=0.000156748, gnorm=1.172, loss_scale=16, train_wall=289, gb_free=19.9, wall=129694
2022-03-08 00:44:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:48:46 | INFO | train_inner | epoch 210:    124 / 196 loss=2.877, nll_loss=1.91, ppl=3.76, wps=20790.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=40800, lr=0.000156556, gnorm=1.178, loss_scale=16, train_wall=293, gb_free=19.9, wall=130009
2022-03-08 00:50:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:52:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 00:52:35 | INFO | valid | epoch 210 | valid on 'valid' subset | loss 11.208 | nll_loss 10.606 | ppl 1559.05 | wps 40811.1 | wpb 510.9 | bsz 1 | num_updates 40871 | best_loss 7.384
2022-03-08 00:52:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 210 @ 40871 updates
2022-03-08 00:52:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 00:52:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 00:52:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 210 @ 40871 updates, score 11.208) (writing took 4.65157121187076 seconds)
2022-03-08 00:52:40 | INFO | fairseq_cli.train | end of epoch 210 (average epoch stats below)
2022-03-08 00:52:40 | INFO | train | epoch 210 | loss 2.889 | nll_loss 1.923 | ppl 3.79 | wps 20450.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 40871 | lr 0.00015642 | gnorm 1.178 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 130243
2022-03-08 00:52:40 | INFO | fairseq.trainer | begin training epoch 211
2022-03-08 00:52:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 00:54:11 | INFO | train_inner | epoch 211:     29 / 196 loss=2.899, nll_loss=1.934, ppl=3.82, wps=20147.2, ups=0.31, wpb=65367, bsz=127.7, num_updates=40900, lr=0.000156365, gnorm=1.182, loss_scale=16, train_wall=292, gb_free=19.9, wall=130334
2022-03-08 00:57:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 00:59:26 | INFO | train_inner | epoch 211:    130 / 196 loss=2.876, nll_loss=1.91, ppl=3.76, wps=20801.2, ups=0.32, wpb=65536, bsz=128, num_updates=41000, lr=0.000156174, gnorm=1.182, loss_scale=16, train_wall=293, gb_free=19.9, wall=130649
2022-03-08 01:02:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:02:56 | INFO | valid | epoch 211 | valid on 'valid' subset | loss 11.189 | nll_loss 10.592 | ppl 1543.67 | wps 40974.5 | wpb 510.9 | bsz 1 | num_updates 41066 | best_loss 7.384
2022-03-08 01:02:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 211 @ 41066 updates
2022-03-08 01:02:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 01:03:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 01:03:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 211 @ 41066 updates, score 11.189) (writing took 3.398129459004849 seconds)
2022-03-08 01:03:00 | INFO | fairseq_cli.train | end of epoch 211 (average epoch stats below)
2022-03-08 01:03:00 | INFO | train | epoch 211 | loss 2.888 | nll_loss 1.922 | ppl 3.79 | wps 20600.3 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 41066 | lr 0.000156048 | gnorm 1.188 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 130862
2022-03-08 01:03:00 | INFO | fairseq.trainer | begin training epoch 212
2022-03-08 01:03:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:04:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:04:50 | INFO | train_inner | epoch 212:     35 / 196 loss=2.889, nll_loss=1.924, ppl=3.79, wps=20177, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=41100, lr=0.000155984, gnorm=1.199, loss_scale=16, train_wall=292, gb_free=19.9, wall=130973
2022-03-08 01:10:04 | INFO | train_inner | epoch 212:    135 / 196 loss=2.881, nll_loss=1.915, ppl=3.77, wps=20836.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=41200, lr=0.000155794, gnorm=1.186, loss_scale=16, train_wall=290, gb_free=19.9, wall=131287
2022-03-08 01:11:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:13:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:13:21 | INFO | valid | epoch 212 | valid on 'valid' subset | loss 11.188 | nll_loss 10.585 | ppl 1536.36 | wps 40749.7 | wpb 510.9 | bsz 1 | num_updates 41260 | best_loss 7.384
2022-03-08 01:13:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 212 @ 41260 updates
2022-03-08 01:13:21 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 01:13:24 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 01:13:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 212 @ 41260 updates, score 11.188) (writing took 3.565296701621264 seconds)
2022-03-08 01:13:24 | INFO | fairseq_cli.train | end of epoch 212 (average epoch stats below)
2022-03-08 01:13:24 | INFO | train | epoch 212 | loss 2.884 | nll_loss 1.918 | ppl 3.78 | wps 20330.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 41260 | lr 0.000155681 | gnorm 1.189 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 131487
2022-03-08 01:13:24 | INFO | fairseq.trainer | begin training epoch 213
2022-03-08 01:13:24 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:15:29 | INFO | train_inner | epoch 213:     40 / 196 loss=2.888, nll_loss=1.922, ppl=3.79, wps=20128.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=41300, lr=0.000155606, gnorm=1.187, loss_scale=16, train_wall=292, gb_free=19.9, wall=131612
2022-03-08 01:18:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:20:44 | INFO | train_inner | epoch 213:    141 / 196 loss=2.879, nll_loss=1.913, ppl=3.76, wps=20800.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=41400, lr=0.000155417, gnorm=1.181, loss_scale=16, train_wall=293, gb_free=19.9, wall=131927
2022-03-08 01:23:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:23:40 | INFO | valid | epoch 213 | valid on 'valid' subset | loss 11.165 | nll_loss 10.564 | ppl 1513.42 | wps 40600.3 | wpb 510.9 | bsz 1 | num_updates 41455 | best_loss 7.384
2022-03-08 01:23:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 213 @ 41455 updates
2022-03-08 01:23:40 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 01:23:44 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 01:23:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 213 @ 41455 updates, score 11.165) (writing took 3.4041106747463346 seconds)
2022-03-08 01:23:44 | INFO | fairseq_cli.train | end of epoch 213 (average epoch stats below)
2022-03-08 01:23:44 | INFO | train | epoch 213 | loss 2.883 | nll_loss 1.917 | ppl 3.78 | wps 20601 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 41455 | lr 0.000155314 | gnorm 1.184 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 132106
2022-03-08 01:23:44 | INFO | fairseq.trainer | begin training epoch 214
2022-03-08 01:23:44 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:25:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:26:07 | INFO | train_inner | epoch 214:     46 / 196 loss=2.885, nll_loss=1.919, ppl=3.78, wps=20225.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=41500, lr=0.00015523, gnorm=1.197, loss_scale=16, train_wall=292, gb_free=19.9, wall=132250
2022-03-08 01:31:19 | INFO | train_inner | epoch 214:    146 / 196 loss=2.875, nll_loss=1.908, ppl=3.75, wps=21008.4, ups=0.32, wpb=65532.4, bsz=128, num_updates=41600, lr=0.000155043, gnorm=1.185, loss_scale=16, train_wall=290, gb_free=19.9, wall=132562
2022-03-08 01:32:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:33:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:34:00 | INFO | valid | epoch 214 | valid on 'valid' subset | loss 11.211 | nll_loss 10.613 | ppl 1565.8 | wps 40492.7 | wpb 510.9 | bsz 1 | num_updates 41649 | best_loss 7.384
2022-03-08 01:34:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 214 @ 41649 updates
2022-03-08 01:34:00 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 01:34:03 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 01:34:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 214 @ 41649 updates, score 11.211) (writing took 3.464123015291989 seconds)
2022-03-08 01:34:03 | INFO | fairseq_cli.train | end of epoch 214 (average epoch stats below)
2022-03-08 01:34:03 | INFO | train | epoch 214 | loss 2.88 | nll_loss 1.914 | ppl 3.77 | wps 20493 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 41649 | lr 0.000154952 | gnorm 1.189 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 132726
2022-03-08 01:34:03 | INFO | fairseq.trainer | begin training epoch 215
2022-03-08 01:34:03 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:36:43 | INFO | train_inner | epoch 215:     51 / 196 loss=2.882, nll_loss=1.916, ppl=3.77, wps=20214, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=41700, lr=0.000154857, gnorm=1.193, loss_scale=16, train_wall=292, gb_free=19.9, wall=132885
2022-03-08 01:39:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:41:58 | INFO | train_inner | epoch 215:    152 / 196 loss=2.876, nll_loss=1.909, ppl=3.76, wps=20797.1, ups=0.32, wpb=65536, bsz=128, num_updates=41800, lr=0.000154672, gnorm=1.189, loss_scale=16, train_wall=293, gb_free=19.9, wall=133200
2022-03-08 01:44:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:44:20 | INFO | valid | epoch 215 | valid on 'valid' subset | loss 11.186 | nll_loss 10.588 | ppl 1539.59 | wps 40735.4 | wpb 510.9 | bsz 1 | num_updates 41844 | best_loss 7.384
2022-03-08 01:44:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 215 @ 41844 updates
2022-03-08 01:44:20 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 01:44:23 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 01:44:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 215 @ 41844 updates, score 11.186) (writing took 3.420298925600946 seconds)
2022-03-08 01:44:23 | INFO | fairseq_cli.train | end of epoch 215 (average epoch stats below)
2022-03-08 01:44:23 | INFO | train | epoch 215 | loss 2.877 | nll_loss 1.911 | ppl 3.76 | wps 20588.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 41844 | lr 0.000154591 | gnorm 1.187 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 133346
2022-03-08 01:44:23 | INFO | fairseq.trainer | begin training epoch 216
2022-03-08 01:44:23 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:46:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:47:21 | INFO | train_inner | epoch 216:     57 / 196 loss=2.873, nll_loss=1.906, ppl=3.75, wps=20215.5, ups=0.31, wpb=65367, bsz=127.7, num_updates=41900, lr=0.000154487, gnorm=1.173, loss_scale=16, train_wall=292, gb_free=19.9, wall=133524
2022-03-08 01:52:33 | INFO | train_inner | epoch 216:    157 / 196 loss=2.888, nll_loss=1.922, ppl=3.79, wps=20996.9, ups=0.32, wpb=65536, bsz=128, num_updates=42000, lr=0.000154303, gnorm=1.18, loss_scale=16, train_wall=290, gb_free=19.9, wall=133836
2022-03-08 01:53:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 01:54:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 01:54:39 | INFO | valid | epoch 216 | valid on 'valid' subset | loss 11.198 | nll_loss 10.602 | ppl 1553.76 | wps 40580.3 | wpb 510.9 | bsz 1 | num_updates 42038 | best_loss 7.384
2022-03-08 01:54:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 216 @ 42038 updates
2022-03-08 01:54:39 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 01:54:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 01:54:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 216 @ 42038 updates, score 11.198) (writing took 3.3259224388748407 seconds)
2022-03-08 01:54:43 | INFO | fairseq_cli.train | end of epoch 216 (average epoch stats below)
2022-03-08 01:54:43 | INFO | train | epoch 216 | loss 2.875 | nll_loss 1.908 | ppl 3.75 | wps 20494.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 42038 | lr 0.000154234 | gnorm 1.175 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 133965
2022-03-08 01:54:43 | INFO | fairseq.trainer | begin training epoch 217
2022-03-08 01:54:43 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 01:57:56 | INFO | train_inner | epoch 217:     62 / 196 loss=2.865, nll_loss=1.897, ppl=3.72, wps=20235.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=42100, lr=0.00015412, gnorm=1.185, loss_scale=16, train_wall=292, gb_free=19.9, wall=134159
2022-03-08 02:00:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:03:11 | INFO | train_inner | epoch 217:    163 / 196 loss=2.882, nll_loss=1.916, ppl=3.77, wps=20792.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=42200, lr=0.000153937, gnorm=1.182, loss_scale=16, train_wall=293, gb_free=19.9, wall=134474
2022-03-08 02:04:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:04:59 | INFO | valid | epoch 217 | valid on 'valid' subset | loss 11.196 | nll_loss 10.598 | ppl 1549.66 | wps 40487.2 | wpb 510.9 | bsz 1 | num_updates 42233 | best_loss 7.384
2022-03-08 02:04:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 217 @ 42233 updates
2022-03-08 02:04:59 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 02:05:02 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 02:05:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 217 @ 42233 updates, score 11.196) (writing took 3.365020403172821 seconds)
2022-03-08 02:05:02 | INFO | fairseq_cli.train | end of epoch 217 (average epoch stats below)
2022-03-08 02:05:02 | INFO | train | epoch 217 | loss 2.873 | nll_loss 1.907 | ppl 3.75 | wps 20594.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 42233 | lr 0.000153877 | gnorm 1.185 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 134585
2022-03-08 02:05:02 | INFO | fairseq.trainer | begin training epoch 218
2022-03-08 02:05:02 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:07:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:08:35 | INFO | train_inner | epoch 218:     68 / 196 loss=2.858, nll_loss=1.889, ppl=3.7, wps=20225, ups=0.31, wpb=65367, bsz=127.7, num_updates=42300, lr=0.000153755, gnorm=1.18, loss_scale=16, train_wall=292, gb_free=19.9, wall=134797
2022-03-08 02:13:47 | INFO | train_inner | epoch 218:    168 / 196 loss=2.885, nll_loss=1.92, ppl=3.78, wps=20994.9, ups=0.32, wpb=65532.4, bsz=128, num_updates=42400, lr=0.000153574, gnorm=1.199, loss_scale=16, train_wall=290, gb_free=19.9, wall=135109
2022-03-08 02:14:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:15:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:15:19 | INFO | valid | epoch 218 | valid on 'valid' subset | loss 11.197 | nll_loss 10.599 | ppl 1551.13 | wps 40807.6 | wpb 510.9 | bsz 1 | num_updates 42427 | best_loss 7.384
2022-03-08 02:15:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 218 @ 42427 updates
2022-03-08 02:15:19 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 02:15:22 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 02:15:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 218 @ 42427 updates, score 11.197) (writing took 3.3687260351143777 seconds)
2022-03-08 02:15:22 | INFO | fairseq_cli.train | end of epoch 218 (average epoch stats below)
2022-03-08 02:15:22 | INFO | train | epoch 218 | loss 2.871 | nll_loss 1.903 | ppl 3.74 | wps 20491.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 42427 | lr 0.000153525 | gnorm 1.192 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 135205
2022-03-08 02:15:22 | INFO | fairseq.trainer | begin training epoch 219
2022-03-08 02:15:22 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:19:10 | INFO | train_inner | epoch 219:     73 / 196 loss=2.853, nll_loss=1.884, ppl=3.69, wps=20224.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=42500, lr=0.000153393, gnorm=1.187, loss_scale=16, train_wall=292, gb_free=19.9, wall=135433
2022-03-08 02:21:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:24:25 | INFO | train_inner | epoch 219:    174 / 196 loss=2.888, nll_loss=1.922, ppl=3.79, wps=20789.1, ups=0.32, wpb=65532.4, bsz=128, num_updates=42600, lr=0.000153213, gnorm=1.191, loss_scale=16, train_wall=293, gb_free=19.9, wall=135748
2022-03-08 02:25:33 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:25:38 | INFO | valid | epoch 219 | valid on 'valid' subset | loss 11.213 | nll_loss 10.617 | ppl 1570.12 | wps 40853.5 | wpb 510.9 | bsz 1 | num_updates 42622 | best_loss 7.384
2022-03-08 02:25:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 219 @ 42622 updates
2022-03-08 02:25:38 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 02:25:42 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 02:25:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 219 @ 42622 updates, score 11.213) (writing took 3.4025441422127187 seconds)
2022-03-08 02:25:42 | INFO | fairseq_cli.train | end of epoch 219 (average epoch stats below)
2022-03-08 02:25:42 | INFO | train | epoch 219 | loss 2.868 | nll_loss 1.901 | ppl 3.73 | wps 20592.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 42622 | lr 0.000153173 | gnorm 1.191 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 135825
2022-03-08 02:25:42 | INFO | fairseq.trainer | begin training epoch 220
2022-03-08 02:25:42 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:28:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:29:48 | INFO | train_inner | epoch 220:     79 / 196 loss=2.848, nll_loss=1.879, ppl=3.68, wps=20227.3, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=42700, lr=0.000153033, gnorm=1.196, loss_scale=16, train_wall=292, gb_free=19.9, wall=136071
2022-03-08 02:34:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:35:03 | INFO | train_inner | epoch 220:    180 / 196 loss=2.885, nll_loss=1.919, ppl=3.78, wps=20804.4, ups=0.32, wpb=65536, bsz=128, num_updates=42800, lr=0.000152854, gnorm=1.183, loss_scale=16, train_wall=293, gb_free=19.9, wall=136386
2022-03-08 02:35:52 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:35:58 | INFO | valid | epoch 220 | valid on 'valid' subset | loss 11.216 | nll_loss 10.621 | ppl 1574.55 | wps 40806.3 | wpb 510.9 | bsz 1 | num_updates 42816 | best_loss 7.384
2022-03-08 02:35:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 220 @ 42816 updates
2022-03-08 02:35:58 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 02:36:01 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 02:36:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 220 @ 42816 updates, score 11.216) (writing took 3.3986217961646616 seconds)
2022-03-08 02:36:01 | INFO | fairseq_cli.train | end of epoch 220 (average epoch stats below)
2022-03-08 02:36:01 | INFO | train | epoch 220 | loss 2.865 | nll_loss 1.897 | ppl 3.72 | wps 20497.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 42816 | lr 0.000152826 | gnorm 1.185 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 136444
2022-03-08 02:36:01 | INFO | fairseq.trainer | begin training epoch 221
2022-03-08 02:36:01 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:40:23 | INFO | train_inner | epoch 221:     84 / 196 loss=2.843, nll_loss=1.873, ppl=3.66, wps=20421, ups=0.31, wpb=65367, bsz=127.7, num_updates=42900, lr=0.000152676, gnorm=1.178, loss_scale=16, train_wall=289, gb_free=19.9, wall=136706
2022-03-08 02:41:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:45:38 | INFO | train_inner | epoch 221:    185 / 196 loss=2.886, nll_loss=1.921, ppl=3.79, wps=20799.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=43000, lr=0.000152499, gnorm=1.19, loss_scale=16, train_wall=293, gb_free=19.9, wall=137021
2022-03-08 02:46:12 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:46:17 | INFO | valid | epoch 221 | valid on 'valid' subset | loss 11.191 | nll_loss 10.596 | ppl 1548 | wps 40673 | wpb 510.9 | bsz 1 | num_updates 43011 | best_loss 7.384
2022-03-08 02:46:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 221 @ 43011 updates
2022-03-08 02:46:17 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 02:46:21 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 02:46:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 221 @ 43011 updates, score 11.191) (writing took 3.364737659227103 seconds)
2022-03-08 02:46:21 | INFO | fairseq_cli.train | end of epoch 221 (average epoch stats below)
2022-03-08 02:46:21 | INFO | train | epoch 221 | loss 2.863 | nll_loss 1.896 | ppl 3.72 | wps 20595.5 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 43011 | lr 0.000152479 | gnorm 1.189 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 137064
2022-03-08 02:46:21 | INFO | fairseq.trainer | begin training epoch 222
2022-03-08 02:46:21 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 02:48:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:51:02 | INFO | train_inner | epoch 222:     90 / 196 loss=2.838, nll_loss=1.868, ppl=3.65, wps=20219.6, ups=0.31, wpb=65367, bsz=127.7, num_updates=43100, lr=0.000152322, gnorm=1.191, loss_scale=16, train_wall=292, gb_free=19.9, wall=137345
2022-03-08 02:55:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 02:56:17 | INFO | train_inner | epoch 222:    191 / 196 loss=2.89, nll_loss=1.925, ppl=3.8, wps=20793.7, ups=0.32, wpb=65536, bsz=128, num_updates=43200, lr=0.000152145, gnorm=1.211, loss_scale=16, train_wall=293, gb_free=19.9, wall=137660
2022-03-08 02:56:32 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 02:56:37 | INFO | valid | epoch 222 | valid on 'valid' subset | loss 11.259 | nll_loss 10.663 | ppl 1621.4 | wps 40868.9 | wpb 510.9 | bsz 1 | num_updates 43205 | best_loss 7.384
2022-03-08 02:56:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 222 @ 43205 updates
2022-03-08 02:56:37 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 02:56:40 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 02:56:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 222 @ 43205 updates, score 11.259) (writing took 3.377634566742927 seconds)
2022-03-08 02:56:40 | INFO | fairseq_cli.train | end of epoch 222 (average epoch stats below)
2022-03-08 02:56:40 | INFO | train | epoch 222 | loss 2.862 | nll_loss 1.894 | ppl 3.72 | wps 20489.5 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43205 | lr 0.000152136 | gnorm 1.199 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 137683
2022-03-08 02:56:40 | INFO | fairseq.trainer | begin training epoch 223
2022-03-08 02:56:40 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:01:37 | INFO | train_inner | epoch 223:     95 / 196 loss=2.832, nll_loss=1.862, ppl=3.64, wps=20424.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=43300, lr=0.000151969, gnorm=1.177, loss_scale=16, train_wall=289, gb_free=19.9, wall=137980
2022-03-08 03:02:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:06:51 | INFO | train_inner | epoch 223:    196 / 196 loss=2.889, nll_loss=1.924, ppl=3.79, wps=20788.6, ups=0.32, wpb=65363.4, bsz=127.7, num_updates=43400, lr=0.000151794, gnorm=1.204, loss_scale=16, train_wall=292, gb_free=19.9, wall=138294
2022-03-08 03:06:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:06:57 | INFO | valid | epoch 223 | valid on 'valid' subset | loss 11.18 | nll_loss 10.585 | ppl 1535.93 | wps 40625.9 | wpb 510.9 | bsz 1 | num_updates 43400 | best_loss 7.384
2022-03-08 03:06:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 223 @ 43400 updates
2022-03-08 03:06:57 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 03:07:00 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 03:07:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 223 @ 43400 updates, score 11.18) (writing took 3.4196159639395773 seconds)
2022-03-08 03:07:00 | INFO | fairseq_cli.train | end of epoch 223 (average epoch stats below)
2022-03-08 03:07:00 | INFO | train | epoch 223 | loss 2.86 | nll_loss 1.892 | ppl 3.71 | wps 20594.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 43400 | lr 0.000151794 | gnorm 1.189 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 138303
2022-03-08 03:07:00 | INFO | fairseq.trainer | begin training epoch 224
2022-03-08 03:07:00 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:09:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:12:15 | INFO | train_inner | epoch 224:    101 / 196 loss=2.829, nll_loss=1.859, ppl=3.63, wps=20220.6, ups=0.31, wpb=65536, bsz=128, num_updates=43500, lr=0.00015162, gnorm=1.193, loss_scale=16, train_wall=293, gb_free=19.9, wall=138618
2022-03-08 03:16:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:17:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:17:16 | INFO | valid | epoch 224 | valid on 'valid' subset | loss 11.207 | nll_loss 10.612 | ppl 1565.51 | wps 40830.8 | wpb 510.9 | bsz 1 | num_updates 43594 | best_loss 7.384
2022-03-08 03:17:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 224 @ 43594 updates
2022-03-08 03:17:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 03:17:20 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 03:17:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 224 @ 43594 updates, score 11.207) (writing took 3.324885925743729 seconds)
2022-03-08 03:17:20 | INFO | fairseq_cli.train | end of epoch 224 (average epoch stats below)
2022-03-08 03:17:20 | INFO | train | epoch 224 | loss 2.857 | nll_loss 1.889 | ppl 3.7 | wps 20491.2 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43594 | lr 0.000151456 | gnorm 1.195 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 138923
2022-03-08 03:17:20 | INFO | fairseq.trainer | begin training epoch 225
2022-03-08 03:17:20 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:17:39 | INFO | train_inner | epoch 225:      6 / 196 loss=2.882, nll_loss=1.916, ppl=3.77, wps=20227, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=43600, lr=0.000151446, gnorm=1.197, loss_scale=16, train_wall=292, gb_free=19.9, wall=138941
2022-03-08 03:22:51 | INFO | train_inner | epoch 225:    106 / 196 loss=2.828, nll_loss=1.858, ppl=3.62, wps=20990.6, ups=0.32, wpb=65536, bsz=128, num_updates=43700, lr=0.000151272, gnorm=1.199, loss_scale=32, train_wall=290, gb_free=19.9, wall=139254
2022-03-08 03:22:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:27:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:27:36 | INFO | valid | epoch 225 | valid on 'valid' subset | loss 11.227 | nll_loss 10.632 | ppl 1587.41 | wps 40740.9 | wpb 510.9 | bsz 1 | num_updates 43789 | best_loss 7.384
2022-03-08 03:27:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 225 @ 43789 updates
2022-03-08 03:27:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 03:27:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 03:27:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 225 @ 43789 updates, score 11.227) (writing took 3.2575724818743765 seconds)
2022-03-08 03:27:39 | INFO | fairseq_cli.train | end of epoch 225 (average epoch stats below)
2022-03-08 03:27:39 | INFO | train | epoch 225 | loss 2.855 | nll_loss 1.887 | ppl 3.7 | wps 20594.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 43789 | lr 0.000151118 | gnorm 1.2 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 139542
2022-03-08 03:27:39 | INFO | fairseq.trainer | begin training epoch 226
2022-03-08 03:27:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:28:14 | INFO | train_inner | epoch 226:     11 / 196 loss=2.877, nll_loss=1.911, ppl=3.76, wps=20228.8, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=43800, lr=0.000151099, gnorm=1.2, loss_scale=16, train_wall=292, gb_free=19.9, wall=139577
2022-03-08 03:29:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:33:29 | INFO | train_inner | epoch 226:    112 / 196 loss=2.833, nll_loss=1.863, ppl=3.64, wps=20788.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=43900, lr=0.000150927, gnorm=1.2, loss_scale=16, train_wall=293, gb_free=19.9, wall=139892
2022-03-08 03:36:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:37:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:37:56 | INFO | valid | epoch 226 | valid on 'valid' subset | loss 11.258 | nll_loss 10.663 | ppl 1621.9 | wps 40670.5 | wpb 510.9 | bsz 1 | num_updates 43983 | best_loss 7.384
2022-03-08 03:37:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 226 @ 43983 updates
2022-03-08 03:37:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 03:37:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 03:37:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 226 @ 43983 updates, score 11.258) (writing took 3.326921802945435 seconds)
2022-03-08 03:37:59 | INFO | fairseq_cli.train | end of epoch 226 (average epoch stats below)
2022-03-08 03:37:59 | INFO | train | epoch 226 | loss 2.852 | nll_loss 1.883 | ppl 3.69 | wps 20487.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 43983 | lr 0.000150785 | gnorm 1.199 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 140162
2022-03-08 03:37:59 | INFO | fairseq.trainer | begin training epoch 227
2022-03-08 03:37:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:38:52 | INFO | train_inner | epoch 227:     17 / 196 loss=2.867, nll_loss=1.9, ppl=3.73, wps=20227.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=44000, lr=0.000150756, gnorm=1.195, loss_scale=16, train_wall=292, gb_free=19.9, wall=140215
2022-03-08 03:43:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:44:08 | INFO | train_inner | epoch 227:    118 / 196 loss=2.832, nll_loss=1.862, ppl=3.64, wps=20780, ups=0.32, wpb=65536, bsz=128, num_updates=44100, lr=0.000150585, gnorm=1.197, loss_scale=16, train_wall=293, gb_free=19.9, wall=140530
2022-03-08 03:48:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:48:16 | INFO | valid | epoch 227 | valid on 'valid' subset | loss 11.258 | nll_loss 10.667 | ppl 1626.35 | wps 40764.6 | wpb 510.9 | bsz 1 | num_updates 44178 | best_loss 7.384
2022-03-08 03:48:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 227 @ 44178 updates
2022-03-08 03:48:16 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 03:48:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 03:48:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 227 @ 44178 updates, score 11.258) (writing took 3.3427924159914255 seconds)
2022-03-08 03:48:19 | INFO | fairseq_cli.train | end of epoch 227 (average epoch stats below)
2022-03-08 03:48:19 | INFO | train | epoch 227 | loss 2.851 | nll_loss 1.882 | ppl 3.69 | wps 20587.8 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 44178 | lr 0.000150452 | gnorm 1.198 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 140782
2022-03-08 03:48:19 | INFO | fairseq.trainer | begin training epoch 228
2022-03-08 03:48:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 03:49:28 | INFO | train_inner | epoch 228:     22 / 196 loss=2.868, nll_loss=1.901, ppl=3.73, wps=20418.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=44200, lr=0.000150414, gnorm=1.196, loss_scale=16, train_wall=289, gb_free=19.9, wall=140851
2022-03-08 03:50:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:54:43 | INFO | train_inner | epoch 228:    123 / 196 loss=2.829, nll_loss=1.858, ppl=3.63, wps=20794.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=44300, lr=0.000150244, gnorm=1.186, loss_scale=16, train_wall=293, gb_free=19.9, wall=141166
2022-03-08 03:56:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 03:58:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 03:58:36 | INFO | valid | epoch 228 | valid on 'valid' subset | loss 11.21 | nll_loss 10.615 | ppl 1568.13 | wps 40503 | wpb 510.9 | bsz 1 | num_updates 44372 | best_loss 7.384
2022-03-08 03:58:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 228 @ 44372 updates
2022-03-08 03:58:36 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 03:58:39 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 03:58:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 228 @ 44372 updates, score 11.21) (writing took 3.3568818508647382 seconds)
2022-03-08 03:58:39 | INFO | fairseq_cli.train | end of epoch 228 (average epoch stats below)
2022-03-08 03:58:39 | INFO | train | epoch 228 | loss 2.848 | nll_loss 1.879 | ppl 3.68 | wps 20483.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 44372 | lr 0.000150122 | gnorm 1.193 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 141402
2022-03-08 03:58:39 | INFO | fairseq.trainer | begin training epoch 229
2022-03-08 03:58:39 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:00:06 | INFO | train_inner | epoch 229:     28 / 196 loss=2.861, nll_loss=1.894, ppl=3.72, wps=20208, ups=0.31, wpb=65367, bsz=127.7, num_updates=44400, lr=0.000150075, gnorm=1.216, loss_scale=16, train_wall=292, gb_free=19.9, wall=141489
2022-03-08 04:03:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:05:22 | INFO | train_inner | epoch 229:    129 / 196 loss=2.837, nll_loss=1.867, ppl=3.65, wps=20783.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=44500, lr=0.000149906, gnorm=1.198, loss_scale=16, train_wall=293, gb_free=19.9, wall=141805
2022-03-08 04:08:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:08:55 | INFO | valid | epoch 229 | valid on 'valid' subset | loss 11.242 | nll_loss 10.647 | ppl 1602.94 | wps 40751.9 | wpb 510.9 | bsz 1 | num_updates 44567 | best_loss 7.384
2022-03-08 04:08:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 229 @ 44567 updates
2022-03-08 04:08:56 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 04:08:59 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 04:08:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 229 @ 44567 updates, score 11.242) (writing took 3.37799385888502 seconds)
2022-03-08 04:08:59 | INFO | fairseq_cli.train | end of epoch 229 (average epoch stats below)
2022-03-08 04:08:59 | INFO | train | epoch 229 | loss 2.846 | nll_loss 1.878 | ppl 3.67 | wps 20585.7 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 44567 | lr 0.000149794 | gnorm 1.204 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 142022
2022-03-08 04:08:59 | INFO | fairseq.trainer | begin training epoch 230
2022-03-08 04:08:59 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:10:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:10:45 | INFO | train_inner | epoch 230:     34 / 196 loss=2.853, nll_loss=1.885, ppl=3.69, wps=20215.8, ups=0.31, wpb=65367, bsz=127.7, num_updates=44600, lr=0.000149738, gnorm=1.194, loss_scale=16, train_wall=292, gb_free=19.9, wall=142128
2022-03-08 04:15:57 | INFO | train_inner | epoch 230:    134 / 196 loss=2.834, nll_loss=1.864, ppl=3.64, wps=20985.4, ups=0.32, wpb=65536, bsz=128, num_updates=44700, lr=0.000149571, gnorm=1.178, loss_scale=16, train_wall=290, gb_free=19.9, wall=142440
2022-03-08 04:17:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:19:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:19:15 | INFO | valid | epoch 230 | valid on 'valid' subset | loss 11.248 | nll_loss 10.651 | ppl 1607.87 | wps 40594.9 | wpb 510.9 | bsz 1 | num_updates 44761 | best_loss 7.384
2022-03-08 04:19:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 230 @ 44761 updates
2022-03-08 04:19:15 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 04:19:19 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 04:19:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 230 @ 44761 updates, score 11.248) (writing took 3.3082224866375327 seconds)
2022-03-08 04:19:19 | INFO | fairseq_cli.train | end of epoch 230 (average epoch stats below)
2022-03-08 04:19:19 | INFO | train | epoch 230 | loss 2.844 | nll_loss 1.875 | ppl 3.67 | wps 20481.8 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 44761 | lr 0.000149469 | gnorm 1.189 | loss_scale 16 | train_wall 568 | gb_free 19.9 | wall 142642
2022-03-08 04:19:19 | INFO | fairseq.trainer | begin training epoch 231
2022-03-08 04:19:19 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:21:21 | INFO | train_inner | epoch 231:     39 / 196 loss=2.852, nll_loss=1.884, ppl=3.69, wps=20223.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=44800, lr=0.000149404, gnorm=1.198, loss_scale=16, train_wall=292, gb_free=19.9, wall=142763
2022-03-08 04:24:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:26:36 | INFO | train_inner | epoch 231:    140 / 196 loss=2.838, nll_loss=1.868, ppl=3.65, wps=20786.8, ups=0.32, wpb=65536, bsz=128, num_updates=44900, lr=0.000149237, gnorm=1.207, loss_scale=16, train_wall=293, gb_free=19.9, wall=143079
2022-03-08 04:29:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:29:35 | INFO | valid | epoch 231 | valid on 'valid' subset | loss 11.221 | nll_loss 10.626 | ppl 1580.01 | wps 40705.2 | wpb 510.9 | bsz 1 | num_updates 44956 | best_loss 7.384
2022-03-08 04:29:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 231 @ 44956 updates
2022-03-08 04:29:35 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 04:29:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 04:29:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 231 @ 44956 updates, score 11.221) (writing took 3.271366051863879 seconds)
2022-03-08 04:29:38 | INFO | fairseq_cli.train | end of epoch 231 (average epoch stats below)
2022-03-08 04:29:38 | INFO | train | epoch 231 | loss 2.844 | nll_loss 1.875 | ppl 3.67 | wps 20594.9 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 44956 | lr 0.000149144 | gnorm 1.205 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 143261
2022-03-08 04:29:38 | INFO | fairseq.trainer | begin training epoch 232
2022-03-08 04:29:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:31:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:31:59 | INFO | train_inner | epoch 232:     45 / 196 loss=2.846, nll_loss=1.877, ppl=3.67, wps=20228.2, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=45000, lr=0.000149071, gnorm=1.202, loss_scale=16, train_wall=292, gb_free=19.9, wall=143402
2022-03-08 04:37:11 | INFO | train_inner | epoch 232:    145 / 196 loss=2.836, nll_loss=1.867, ppl=3.65, wps=21004.8, ups=0.32, wpb=65532.4, bsz=128, num_updates=45100, lr=0.000148906, gnorm=1.195, loss_scale=16, train_wall=290, gb_free=19.9, wall=143714
2022-03-08 04:37:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:39:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:39:55 | INFO | valid | epoch 232 | valid on 'valid' subset | loss 11.254 | nll_loss 10.655 | ppl 1612.67 | wps 40943.5 | wpb 510.9 | bsz 1 | num_updates 45150 | best_loss 7.384
2022-03-08 04:39:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 232 @ 45150 updates
2022-03-08 04:39:55 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 04:39:58 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 04:39:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 232 @ 45150 updates, score 11.254) (writing took 3.279365301132202 seconds)
2022-03-08 04:39:58 | INFO | fairseq_cli.train | end of epoch 232 (average epoch stats below)
2022-03-08 04:39:58 | INFO | train | epoch 232 | loss 2.839 | nll_loss 1.869 | ppl 3.65 | wps 20494.6 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 45150 | lr 0.000148823 | gnorm 1.195 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 143881
2022-03-08 04:39:58 | INFO | fairseq.trainer | begin training epoch 233
2022-03-08 04:39:58 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:42:34 | INFO | train_inner | epoch 233:     50 / 196 loss=2.839, nll_loss=1.87, ppl=3.66, wps=20229.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=45200, lr=0.000148741, gnorm=1.204, loss_scale=16, train_wall=292, gb_free=19.9, wall=144037
2022-03-08 04:44:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:47:49 | INFO | train_inner | epoch 233:    151 / 196 loss=2.838, nll_loss=1.869, ppl=3.65, wps=20791, ups=0.32, wpb=65536, bsz=128, num_updates=45300, lr=0.000148577, gnorm=1.187, loss_scale=16, train_wall=293, gb_free=19.9, wall=144352
2022-03-08 04:50:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 04:50:14 | INFO | valid | epoch 233 | valid on 'valid' subset | loss 11.239 | nll_loss 10.645 | ppl 1601.76 | wps 40637.7 | wpb 510.9 | bsz 1 | num_updates 45345 | best_loss 7.384
2022-03-08 04:50:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 233 @ 45345 updates
2022-03-08 04:50:14 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 04:50:18 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 04:50:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 233 @ 45345 updates, score 11.239) (writing took 3.3296368760056794 seconds)
2022-03-08 04:50:18 | INFO | fairseq_cli.train | end of epoch 233 (average epoch stats below)
2022-03-08 04:50:18 | INFO | train | epoch 233 | loss 2.839 | nll_loss 1.869 | ppl 3.65 | wps 20592.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 45345 | lr 0.000148503 | gnorm 1.196 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 144501
2022-03-08 04:50:18 | INFO | fairseq.trainer | begin training epoch 234
2022-03-08 04:50:18 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 04:51:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:53:13 | INFO | train_inner | epoch 234:     56 / 196 loss=2.836, nll_loss=1.866, ppl=3.65, wps=20217.9, ups=0.31, wpb=65367, bsz=127.7, num_updates=45400, lr=0.000148413, gnorm=1.201, loss_scale=16, train_wall=292, gb_free=19.9, wall=144675
2022-03-08 04:58:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 04:58:28 | INFO | train_inner | epoch 234:    157 / 196 loss=2.84, nll_loss=1.871, ppl=3.66, wps=20784.7, ups=0.32, wpb=65532.4, bsz=128, num_updates=45500, lr=0.00014825, gnorm=1.208, loss_scale=16, train_wall=293, gb_free=19.9, wall=144991
2022-03-08 05:00:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:00:34 | INFO | valid | epoch 234 | valid on 'valid' subset | loss 11.235 | nll_loss 10.636 | ppl 1591.16 | wps 40589.1 | wpb 510.9 | bsz 1 | num_updates 45539 | best_loss 7.384
2022-03-08 05:00:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 234 @ 45539 updates
2022-03-08 05:00:34 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 05:00:38 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 05:00:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 234 @ 45539 updates, score 11.235) (writing took 3.4490679278969765 seconds)
2022-03-08 05:00:38 | INFO | fairseq_cli.train | end of epoch 234 (average epoch stats below)
2022-03-08 05:00:38 | INFO | train | epoch 234 | loss 2.835 | nll_loss 1.866 | ppl 3.64 | wps 20480.7 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 45539 | lr 0.000148186 | gnorm 1.205 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 145120
2022-03-08 05:00:38 | INFO | fairseq.trainer | begin training epoch 235
2022-03-08 05:00:38 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:03:48 | INFO | train_inner | epoch 235:     61 / 196 loss=2.824, nll_loss=1.853, ppl=3.61, wps=20420.7, ups=0.31, wpb=65367, bsz=127.7, num_updates=45600, lr=0.000148087, gnorm=1.194, loss_scale=16, train_wall=289, gb_free=19.9, wall=145311
2022-03-08 05:05:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:09:03 | INFO | train_inner | epoch 235:    162 / 196 loss=2.847, nll_loss=1.879, ppl=3.68, wps=20799.4, ups=0.32, wpb=65536, bsz=128, num_updates=45700, lr=0.000147925, gnorm=1.194, loss_scale=16, train_wall=293, gb_free=19.9, wall=145626
2022-03-08 05:10:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:10:54 | INFO | valid | epoch 235 | valid on 'valid' subset | loss 11.281 | nll_loss 10.69 | ppl 1651.8 | wps 40419.8 | wpb 510.9 | bsz 1 | num_updates 45734 | best_loss 7.384
2022-03-08 05:10:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 235 @ 45734 updates
2022-03-08 05:10:54 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 05:10:57 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 05:10:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 235 @ 45734 updates, score 11.281) (writing took 3.4201666340231895 seconds)
2022-03-08 05:10:57 | INFO | fairseq_cli.train | end of epoch 235 (average epoch stats below)
2022-03-08 05:10:57 | INFO | train | epoch 235 | loss 2.833 | nll_loss 1.863 | ppl 3.64 | wps 20602.4 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 45734 | lr 0.00014787 | gnorm 1.192 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 145740
2022-03-08 05:10:57 | INFO | fairseq.trainer | begin training epoch 236
2022-03-08 05:10:57 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:12:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:14:26 | INFO | train_inner | epoch 236:     67 / 196 loss=2.821, nll_loss=1.851, ppl=3.61, wps=20229.3, ups=0.31, wpb=65359.9, bsz=127.7, num_updates=45800, lr=0.000147764, gnorm=1.197, loss_scale=16, train_wall=292, gb_free=19.9, wall=145949
2022-03-08 05:18:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:19:41 | INFO | train_inner | epoch 236:    168 / 196 loss=2.845, nll_loss=1.876, ppl=3.67, wps=20801.2, ups=0.32, wpb=65536, bsz=128, num_updates=45900, lr=0.000147602, gnorm=1.21, loss_scale=16, train_wall=293, gb_free=19.9, wall=146264
2022-03-08 05:21:08 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:21:13 | INFO | valid | epoch 236 | valid on 'valid' subset | loss 11.283 | nll_loss 10.69 | ppl 1652.38 | wps 40736.9 | wpb 510.9 | bsz 1 | num_updates 45928 | best_loss 7.384
2022-03-08 05:21:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 236 @ 45928 updates
2022-03-08 05:21:13 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 05:21:16 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 05:21:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 236 @ 45928 updates, score 11.283) (writing took 3.433493123855442 seconds)
2022-03-08 05:21:17 | INFO | fairseq_cli.train | end of epoch 236 (average epoch stats below)
2022-03-08 05:21:17 | INFO | train | epoch 236 | loss 2.831 | nll_loss 1.861 | ppl 3.63 | wps 20495.4 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 45928 | lr 0.000147557 | gnorm 1.202 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 146359
2022-03-08 05:21:17 | INFO | fairseq.trainer | begin training epoch 237
2022-03-08 05:21:17 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:25:01 | INFO | train_inner | epoch 237:     72 / 196 loss=2.809, nll_loss=1.837, ppl=3.57, wps=20424.1, ups=0.31, wpb=65367, bsz=127.7, num_updates=46000, lr=0.000147442, gnorm=1.196, loss_scale=16, train_wall=289, gb_free=19.9, wall=146584
2022-03-08 05:25:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:30:16 | INFO | train_inner | epoch 237:    173 / 196 loss=2.847, nll_loss=1.878, ppl=3.68, wps=20794.5, ups=0.32, wpb=65532.4, bsz=128, num_updates=46100, lr=0.000147282, gnorm=1.199, loss_scale=16, train_wall=293, gb_free=19.9, wall=146899
2022-03-08 05:31:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:31:33 | INFO | valid | epoch 237 | valid on 'valid' subset | loss 11.265 | nll_loss 10.671 | ppl 1629.89 | wps 40692.1 | wpb 510.9 | bsz 1 | num_updates 46123 | best_loss 7.384
2022-03-08 05:31:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 237 @ 46123 updates
2022-03-08 05:31:33 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 05:31:36 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 05:31:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 237 @ 46123 updates, score 11.265) (writing took 3.3989533158019185 seconds)
2022-03-08 05:31:36 | INFO | fairseq_cli.train | end of epoch 237 (average epoch stats below)
2022-03-08 05:31:36 | INFO | train | epoch 237 | loss 2.829 | nll_loss 1.859 | ppl 3.63 | wps 20598.1 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 46123 | lr 0.000147245 | gnorm 1.196 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 146979
2022-03-08 05:31:36 | INFO | fairseq.trainer | begin training epoch 238
2022-03-08 05:31:36 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:33:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:35:40 | INFO | train_inner | epoch 238:     78 / 196 loss=2.81, nll_loss=1.838, ppl=3.57, wps=20226.3, ups=0.31, wpb=65367, bsz=127.7, num_updates=46200, lr=0.000147122, gnorm=1.204, loss_scale=16, train_wall=292, gb_free=19.9, wall=147222
2022-03-08 05:40:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:40:55 | INFO | train_inner | epoch 238:    179 / 196 loss=2.849, nll_loss=1.881, ppl=3.68, wps=20804.4, ups=0.32, wpb=65536, bsz=128, num_updates=46300, lr=0.000146964, gnorm=1.193, loss_scale=16, train_wall=293, gb_free=19.9, wall=147537
2022-03-08 05:41:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:41:52 | INFO | valid | epoch 238 | valid on 'valid' subset | loss 11.32 | nll_loss 10.729 | ppl 1696.69 | wps 40890.4 | wpb 510.9 | bsz 1 | num_updates 46317 | best_loss 7.384
2022-03-08 05:41:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 238 @ 46317 updates
2022-03-08 05:41:52 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 05:41:56 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 05:41:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 238 @ 46317 updates, score 11.32) (writing took 3.4316777251660824 seconds)
2022-03-08 05:41:56 | INFO | fairseq_cli.train | end of epoch 238 (average epoch stats below)
2022-03-08 05:41:56 | INFO | train | epoch 238 | loss 2.828 | nll_loss 1.858 | ppl 3.62 | wps 20496.9 | ups 0.31 | wpb 65448.9 | bsz 127.8 | num_updates 46317 | lr 0.000146937 | gnorm 1.199 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 147598
2022-03-08 05:41:56 | INFO | fairseq.trainer | begin training epoch 239
2022-03-08 05:41:56 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:46:15 | INFO | train_inner | epoch 239:     83 / 196 loss=2.804, nll_loss=1.832, ppl=3.56, wps=20432, ups=0.31, wpb=65367, bsz=127.7, num_updates=46400, lr=0.000146805, gnorm=1.202, loss_scale=16, train_wall=289, gb_free=19.9, wall=147857
2022-03-08 05:47:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:51:30 | INFO | train_inner | epoch 239:    184 / 196 loss=2.85, nll_loss=1.882, ppl=3.69, wps=20796.2, ups=0.32, wpb=65532.4, bsz=128, num_updates=46500, lr=0.000146647, gnorm=1.221, loss_scale=16, train_wall=293, gb_free=19.9, wall=148172
2022-03-08 05:52:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 05:52:12 | INFO | valid | epoch 239 | valid on 'valid' subset | loss 11.247 | nll_loss 10.652 | ppl 1608.65 | wps 40585.6 | wpb 510.9 | bsz 1 | num_updates 46512 | best_loss 7.384
2022-03-08 05:52:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 239 @ 46512 updates
2022-03-08 05:52:12 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 05:52:15 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 05:52:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 239 @ 46512 updates, score 11.247) (writing took 3.40424382686615 seconds)
2022-03-08 05:52:15 | INFO | fairseq_cli.train | end of epoch 239 (average epoch stats below)
2022-03-08 05:52:15 | INFO | train | epoch 239 | loss 2.827 | nll_loss 1.857 | ppl 3.62 | wps 20602.6 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 46512 | lr 0.000146628 | gnorm 1.212 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 148218
2022-03-08 05:52:15 | INFO | fairseq.trainer | begin training epoch 240
2022-03-08 05:52:15 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 05:53:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 05:56:53 | INFO | train_inner | epoch 240:     89 / 196 loss=2.804, nll_loss=1.831, ppl=3.56, wps=20233.7, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=46600, lr=0.00014649, gnorm=1.18, loss_scale=16, train_wall=292, gb_free=19.9, wall=148496
2022-03-08 06:00:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 06:02:08 | INFO | train_inner | epoch 240:    190 / 196 loss=2.85, nll_loss=1.882, ppl=3.69, wps=20799.8, ups=0.32, wpb=65536, bsz=128, num_updates=46700, lr=0.000146333, gnorm=1.227, loss_scale=16, train_wall=293, gb_free=19.9, wall=148811
2022-03-08 06:02:26 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 06:02:31 | INFO | valid | epoch 240 | valid on 'valid' subset | loss 11.248 | nll_loss 10.655 | ppl 1612.59 | wps 40473.8 | wpb 510.9 | bsz 1 | num_updates 46706 | best_loss 7.384
2022-03-08 06:02:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 240 @ 46706 updates
2022-03-08 06:02:31 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 06:02:34 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 06:02:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 240 @ 46706 updates, score 11.248) (writing took 3.2823884543031454 seconds)
2022-03-08 06:02:34 | INFO | fairseq_cli.train | end of epoch 240 (average epoch stats below)
2022-03-08 06:02:34 | INFO | train | epoch 240 | loss 2.824 | nll_loss 1.853 | ppl 3.61 | wps 20500.9 | ups 0.31 | wpb 65447.1 | bsz 127.8 | num_updates 46706 | lr 0.000146323 | gnorm 1.204 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 148837
2022-03-08 06:02:34 | INFO | fairseq.trainer | begin training epoch 241
2022-03-08 06:02:34 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 06:07:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 06:07:31 | INFO | train_inner | epoch 241:     95 / 196 loss=2.794, nll_loss=1.821, ppl=3.53, wps=20231.4, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=46800, lr=0.000146176, gnorm=1.204, loss_scale=16, train_wall=292, gb_free=19.9, wall=149134
2022-03-08 06:12:43 | INFO | train_inner | epoch 241:    195 / 196 loss=2.853, nll_loss=1.885, ppl=3.69, wps=21002.5, ups=0.32, wpb=65536, bsz=128, num_updates=46900, lr=0.00014602, gnorm=1.21, loss_scale=16, train_wall=290, gb_free=19.9, wall=149446
2022-03-08 06:12:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2022-03-08 06:12:51 | INFO | valid | epoch 241 | valid on 'valid' subset | loss 11.27 | nll_loss 10.673 | ppl 1632.87 | wps 40246.3 | wpb 510.9 | bsz 1 | num_updates 46901 | best_loss 7.384
2022-03-08 06:12:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 241 @ 46901 updates
2022-03-08 06:12:51 | INFO | fairseq.trainer | Saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 06:12:54 | INFO | fairseq.trainer | Finished saving checkpoint to /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt
2022-03-08 06:12:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /cluster/scratch/andriusb/checkpoints/w103_size_0.125_fp16-label_smoothing_0.04_#2/checkpoint_last.pt (epoch 241 @ 46901 updates, score 11.27) (writing took 3.4173762309364974 seconds)
2022-03-08 06:12:54 | INFO | fairseq_cli.train | end of epoch 241 (average epoch stats below)
2022-03-08 06:12:54 | INFO | train | epoch 241 | loss 2.822 | nll_loss 1.852 | ppl 3.61 | wps 20594.2 | ups 0.31 | wpb 65447.5 | bsz 127.8 | num_updates 46901 | lr 0.000146019 | gnorm 1.205 | loss_scale 16 | train_wall 567 | gb_free 19.9 | wall 149457
2022-03-08 06:12:54 | INFO | fairseq.trainer | begin training epoch 242
2022-03-08 06:12:54 | INFO | fairseq_cli.train | Start iterating over samples
2022-03-08 06:14:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2022-03-08 06:18:06 | INFO | train_inner | epoch 242:    100 / 196 loss=2.789, nll_loss=1.816, ppl=3.52, wps=20223, ups=0.31, wpb=65363.4, bsz=127.7, num_updates=47000, lr=0.000145865, gnorm=1.2, loss_scale=16, train_wall=292, gb_free=19.9, wall=149769
Traceback (most recent call last):
  File "/cluster/home/andriusb/fq/env/bin/fairseq-train", line 33, in <module>
    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 544, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/distributed/utils.py", line 369, in call_main
    main(cfg, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 207, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq_cli/train.py", line 328, in train
    log_output = trainer.train_step(samples)
  File "/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/trainer.py", line 754, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/tasks/fairseq_task.py", line 492, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/criterions/label_smoothed_cross_entropy.py", line 79, in forward
    net_output = model(**sample["net_input"])
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/fairseq_model.py", line 496, in forward
    return self.decoder(src_tokens, **kwargs)
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 216, in forward
    x, extra = self.extract_features(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 238, in extract_features
    return self.extract_features_scriptable(
  File "/cluster/home/andriusb/fq/fairseq/fairseq/models/transformer/transformer_decoder.py", line 340, in extract_features_scriptable
    x, layer_attn, _ = layer(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/transformer_layer.py", line 368, in forward
    x, attn = self.self_attn(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/home/andriusb/fq/fairseq/fairseq/modules/multihead_attention.py", line 170, in forward
    return F.multi_head_attention_forward(
  File "/cluster/apps/nss/gcc-6.3.0/python_gpu/3.8.5/torch/nn/functional.py", line 4314, in multi_head_attention_forward
    attn_output_weights = dropout(attn_output_weights, p=dropout_p, training=training)
KeyboardInterrupt
